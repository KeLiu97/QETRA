originalTitle,body,reformulatedTitle
How do you express binary literals in python?," How do you express an integer as a binary number with Python literals?I was easily able to find the answer for hex: and octal: How do you use literals to express binary in Python?Summary of AnswersPython 2.5 and earlier: can express binary using int('01010101111',2) but not with a literal.Python 2.5 and earlier: there is no way to express binary literals.Python 2.6 beta: You can do like so: 0b1100111 or 0B1100111.Python 2.6 beta: will also allow 0o27 or 0O27 (second character is the letter O) to represent an octal.Python 3.0 beta: Same as 2.6, but will no longer allow the older 027 syntax for octals. <code>  >>> 0x12AF4783>>> 0x100256 >>> 01267695>>> 010064",How do you express binary literals in Python?
"Is there a simple, elegant way to define Singletons in Python?", There seem to be many ways to define singletons in Python. Is there a consensus opinion on Stack Overflow? <code> ,"Is there a simple, elegant way to define singletons?"
"Is there a simple, elegant way to define singletons in Python?", There seem to be many ways to define singletons in Python. Is there a consensus opinion on Stack Overflow? <code> ,"Is there a simple, elegant way to define singletons?"
How do I add data to the database in Django," Currently, I am writing up a bit of a product-based CMS as my first project.Here is my question. How can I add additional data (products) to my Product model?I have added '/admin/products/add' to my urls.py, but I don't really know where to go from there. How would i build both my view and my template? Please keep in mind that I don't really know all that much Python, and i am very new to DjangoHow can I do this all without using this existing django admin interface. <code> ",How do I add data to an existing model in Django?
Generator Expressions vs. List Comprehension, When should you use generator expressions and when should you use list comprehensions in Python? <code>  # Generator expression(x*2 for x in range(256))# List comprehension[x*2 for x in range(256)],Generator expressions vs. list comprehensions
round() in Python doesn't seem to be rounding properly," The documentation for the round() function states that you pass it a number, and the positions past the decimal to round. Thus it should do this: But, in actuality, good old floating point weirdness creeps in and you get: For the purposes of UI, I need to display 5.6. I poked around the Internet and found some documentation that this is dependent on my implementation of Python. Unfortunately, this occurs on both my Windows dev machine and each Linux server I've tried. See here also.Short of creating my own round library, is there any way around this? <code>  n = 5.59round(n, 1) # 5.6 5.5999999999999996",round() doesn't seem to be rounding properly
Can you pre-compile regular expressions in python?," Each time a python file is imported that contains a large quantity of static regular expressions, cpu cycles are spent compiling the strings into their representative state machines in memory. Question: Is it possible to store these regular expressions in a cache on disk in a pre-compiled manner to avoid having to execute the regex compilations on each import?Pickling the object simply does the following, causing compilation to happen anyway: And re objects are unmarshallable: <code>  a = re.compile(""a.*b"")b = re.compile(""c.*d"")... >>> import pickle>>> import re>>> x = re.compile("".*"")>>> pickle.dumps(x)""cre\n_compile\np0\n(S'.*'\np1\nI0\ntp2\nRp3\n."" >>> import marshal>>> import re>>> x = re.compile("".*"")>>> marshal.dumps(x)Traceback (most recent call last): File ""<stdin>"", line 1, in <module>ValueError: unmarshallable object",Caching compiled regex objects in Python?
MAC-address from Python," I'd like to search for a given MAC address on my network, all from within a Python script. I already have a map of all the active IP addresses in the network but I cannot figure out how to glean the MAC address. Any ideas? <code> ",Search for host with MAC-address using Python
How do you send a HEAD HTTP request in Python?," What I'm trying to do here is get the headers of a given URL so I can determine the MIME type. I want to be able to see if http://somedomain/foo/ will return an HTML document or a JPEG image for example. Thus, I need to figure out how to send a HEAD request so that I can read the MIME type without having to download the content. Does anyone know of an easy way of doing this? <code> ",How do you send a HEAD HTTP request in Python 2?
Python signal woes: SIGQUIT handler delays execution if SIGQUIT recieved during execution of another signal handler?," The following program is very simple: it outputs a single dot each half a second. If it recieves a SIGQUIT, it proceeds to output ten Qs. If it recieves a SIGTSTP (Ctrl-Z), it outputs ten Zs.If it recieves a SIGTSTP while printing Qs, it will print ten Zs after it's done with the ten Qs. This is a good thing.However, if it recieves a SIGQUIT while printing Zs, it fails to print Qs after them. Instead, it prints them out only after I manually terminate execution via a KeyboardInterrupt. I want the Qs to be printed immediately after the Zs.This happens using Python2.3.What am I doing wrong? <code>  #!/usr/bin/pythonfrom signal import *from time import sleepfrom sys import stdoutdef write(text): stdout.write(text) stdout.flush()def process_quit(signum, frame): for i in range(10): write(""Q"") sleep(0.5)def process_tstp(signum, frame): for i in range(10): write(""Z"") sleep(0.5)signal(SIGQUIT, process_quit)signal(SIGTSTP, process_tstp)while 1: write('.') sleep(0.5)",Python signal woes: SIGQUIT handler delays execution if SIGQUIT received during execution of another signal handler?
Python Vs. Ruby for Metaprogramming," I'm currently primarily a D programmer and am looking to add another language to my toolbox, preferably one that supports the metaprogramming hacks that just can't be done in a statically compiled language like D.I've read up on Lisp a little and I would love to find a language that allows some of the cool stuff that Lisp does, but without the strange syntax, etc. of Lisp. I don't want to start a language flame war, and I'm sure both Ruby and Python have their tradeoffs, so I'll list what's important to me personally. Please tell me whether Ruby, Python, or some other language would be best for me.Important:Good metaprogramming. Ability to create classes, methods, functions, etc. at runtime. Preferably, minimal distinction between code and data, Lisp style.Nice, clean, sane syntax and consistent, intuitive semantics. Basically a well thought-out, fun to use, modern language.Multiple paradigms. No one paradigm is right for every project, or even every small subproblem within a project.An interesting language that actually affects the way one thinks about programming.Somewhat important:Performance. It would be nice if performance was decent, but when performance is a real priority, I'll use D instead.Well-documented. Not important:Community size, library availability, etc. None of these are characteristics of the language itself, and all can change very quickly.Job availability. I am not a full-time, professional programmer. I am a grad student and programming is tangentially relevant to my research.Any features that are primarily designed with very large projects worked on by a million code monkeys in mind. <code> ",Python vs. Ruby for metaprogramming
Looking to Read Bytes from File in Python," Similar to this question, I am trying to read in an ID3v2 tag header and am having trouble figuring out how to get individual bytes in python.I first read all ten bytes into a string. I then want to parse out the individual pieces of information.I can grab the two version number chars in the string, but then I have no idea how to take those two chars and get an integer out of them.The struct package seems to be what I want, but I can't get it to work.Here is my code so-far (I am very new to python btw...so take it easy on me): Printing out any value except is obviously crap because they are not formatted correctly. <code>  def __init__(self, ten_byte_string): self.whole_string = ten_byte_string self.file_identifier = self.whole_string[:3] self.major_version = struct.pack('x', self.whole_string[3:4]) #this self.minor_version = struct.pack('x', self.whole_string[4:5]) # and this self.flags = self.whole_string[5:6] self.len = self.whole_string[6:10]",How Does One Read Bytes from File in Python
What's the canonical way to check for type in python?, What is the best way to check whether a given object is of a given type? How about checking whether the object inherits from a given type?Let's say I have an object o. How do I check whether it's a str? <code> ,What's the canonical way to check for type in Python?
How to pacakge Twisted program with py2exe?," I tried to package a Twisted program with py2exe, but once I run the exe file I built, I got a ""No module named resource"" error. And I found the py2exe said: The following modules appear to be missing ['FCNTL', 'OpenSSL', 'email.Generator', 'email.Iterators', 'email.Utils', 'pkg_resources', 'pywintypes', 'resource', 'win32api', 'win32con', 'win32event', 'win32file', 'win32pipe', 'win32process', 'win32security']So how do I solve this problem?Thanks. <code> ",How to package Twisted program with py2exe?
Microcrophone access in Python, Can I access a users microphone in Python?Sorry I forgot not everyone is a mind reader:Windows at minimum XP but Vista support would be VERY good. <code> ,Microphone access in Python
Microphone access in Pypython, Can I access a users microphone in Python?Sorry I forgot not everyone is a mind reader:Windows at minimum XP but Vista support would be VERY good. <code> ,Microphone access in Python
How to avoid computation every time a python module is imported," I have a python module that makes use of a huge dictionary global variable, currently I put the computation code in the top section, every first time import or reload of the module takes more then one minute which is totally unacceptable. How can I save the computation result somewhere so that the next import/reload doesn't have to compute it? I tried cPickle, but loading the dictionary variable from a file(1.3M) takes approximately the same time as computation.To give more information about my problem,  <code>  FD = FreqDist(word for word in brown.words()) # this line of code takes 1 min",How to avoid computation every time a python module is reloaded
"What's the best soap client library for Python, and where is the documentation for it?"," I've never used SOAP before and I'm sort of new to Python. I'm doing this to get myself acquainted with both technologies. I've installed SOAPlib and I've tried to read their Client documentation, but I don't understand it too well. Is there anything else I can look into which is more suited for being a SOAP Client library for Python?Edit: Just in case it helps, I'm using Python 2.6. <code> ","What SOAP client libraries exist for Python, and where is the documentation for them?"
"What's the best SOAP client library for Python, and where is the documentation for it?"," I've never used SOAP before and I'm sort of new to Python. I'm doing this to get myself acquainted with both technologies. I've installed SOAPlib and I've tried to read their Client documentation, but I don't understand it too well. Is there anything else I can look into which is more suited for being a SOAP Client library for Python?Edit: Just in case it helps, I'm using Python 2.6. <code> ","What SOAP client libraries exist for Python, and where is the documentation for them?"
Python: Difference between class and instance attributes," Is there any meaningful distinction between: vs. If you're creating a lot of instances, is there any difference in performance or space requirements for the two styles? When you read the code, do you consider the meaning of the two styles to be significantly different? <code>  class A(object): foo = 5 # some default value class B(object): def __init__(self, foo=5): self.foo = foo",What is the difference between class and instance attributes?
Are there any IDE's that support Python 3?," I recently saw an announcement and article outlining the release of the first Python 3.0 release candidate. I was wondering whether there were any commercial, free, open source etc. IDE's that support its syntax. <code> ",Are there any IDE's that support Python 3 syntax?
Would python make a good substitute for the windows command line/batch scripts?," I've got some experience with Bash, which I don't mind, but now that I'm doing a lot of Windows development I'm needing to do basic stuff/write basic scripts using the Windows command-line language. For some reason said language really irritates me, so I was considering learning Python and using that instead.Is Python suitable for such things? Moving files around, creating scripts to do things like unzipping a backup and restoring a SQL database, etc. <code> ",Would Python make a good substitute for the Windows command-line/batch scripts?
Python snippet to remove C and C++ comments," I'm looking for Python code that removes C and C++ comments from a string. (Assume the string contains an entire C source file.)I realize that I could .match() substrings with a Regex, but that doesn't solve nesting /*, or having a // inside a /* */.Ideally, I would prefer a non-naive implementation that properly handles awkward cases. <code> ",Remove C and C++ comments using Python?
Most efficient way to search the last x lines of a file in python," I have a file and I don't know how big it's going to be (it could be quite large, but the size will vary greatly). I want to search the last 10 lines or so to see if any of them match a string. I need to do this as quickly and efficiently as possible and was wondering if there's anything better than: <code>  s = ""foo""last_bit = fileObj.readlines()[-10:]for line in last_bit: if line == s: print ""FOUND""",Most efficient way to search the last X lines of a file?
TkInter Invoke Event in Main Loop," How do you invoke a tkinter event from a separate object? I'm looking for something like wxWidgets wx.CallAfter. For example, If I create an object, and pass to it my Tk root instance, and then try to call a method of that root window from my object, my app locks up.The best I can come up with is to use the the after method and check the status from my separate object, but that seems wasteful. <code> ",Tkinter: invoke event in main loop
Is there a cross-platform way of getting information from Python's OSError," On a simple directory creation operation for example, I can make an OSError like this:(Ubuntu Linux) Now I can catch that error like this: Is there a cross-platform way that I can know that that the 17 or the 'File Exists' will always mean the same thing so that I can act differently depending on the situation?(This came up during another question.) <code>  >>> import os>>> os.mkdir('foo')>>> os.mkdir('foo')Traceback (most recent call last): File ""<stdin>"", line 1, in <module>OSError: [Errno 17] File exists: 'foo' >>> import os>>> os.mkdir('foo')>>> try:... os.mkdir('foo')... except OSError, e:... print e.args... (17, 'File exists')",Is there a cross-platform way of getting information from Python's OSError?
How do I script an OLE component using Python?," I would like to use Python to script an application that advertises itself as providing an OLE component. How should I get started?I don't yet know what methods I need to call on the COMponents I will be accessing. Should I use win32com to load those components, and then start pressing 'tab' in IPython? <code> ",How to script an OLE component using Python
Why don't xpaths work when processing an XHTML document with lxml (in python)?," I am testing against the following test document: If I parse the document using lxml.html, I can get the IMG with an xpath just fine: However, if I parse the document as XML and try to get the IMG tag, I get an empty result: I can navigate to the element directly: But of course that doesn't help me process arbitrary documents. I would also expect to be able to query etree to get an xpath expression that will directly identify this element, which, technically I can do: But that xpath is, again, obviously not useful for parsing arbitrary documents.Obviously I am missing some key issue here, but I don't know what it is. My best guess is that it has something to do with namespaces but the only namespace defined is the default and I don't know what else I might need to consider in regards to namespaces.So, what am I missing? <code>  <?xml version=""1.0"" encoding=""UTF-8""?><!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Strict//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd""><html xmlns=""http://www.w3.org/1999/xhtml""> <head> <title>hi there</title> </head>",Why doesn't xpath work when processing an XHTML document with lxml (in python)?
What do i use on linux to make a python program executable, I just installed a linux system (Kubuntu) and was wondering if there is a program to make python programs executable for linux. <code> ,What do I use on linux to make a python program executable
How do I randomly select an item from a list using Python?," Assume I have the following list: What is the simplest way to retrieve an item at random from this list? <code>  foo = ['a', 'b', 'c', 'd', 'e']",How can I randomly select an item from a list?
How to randomly select an item from a list?," Assume I have the following list: What is the simplest way to retrieve an item at random from this list? <code>  foo = ['a', 'b', 'c', 'd', 'e']",How can I randomly select an item from a list?
"Get Element value with minidom, Python"," I am creating a GUI frontend for the Eve Online API in Python.I have successfully pulled the XML data from their server.I am trying to grab the value from a node called ""name"": This seems to find the node, but the output is below: How could I get it to print the value of the node? <code>  from xml.dom.minidom import parsedom = parse(""C:\\eve.xml"")name = dom.getElementsByTagName('name')print name [<DOM Element: name at 0x11e6d28>]",Get Element value with minidom with Python
Making functions non overloadable," I know python functions are virtual by default. Let's say I have this: I don't want them to be able to do this: Is there a way to prevent users from overloading roo()? <code>  class Foo: def __init__(self, args): do some stuff def goo(): print ""You can overload me"" def roo(): print ""You cannot overload me"" class Aoo(Foo): def roo(): print ""I don't want you to be able to do this""",Making functions non override-able
Howto configure vim to not put comments at the beginning of lines while editing python files," When I add a # in insert mode on an empty line in Vim while editing python files, vim moves the # to the beginning of the line, but I would like the # to be inserted at the tab level where I entered it.For example, when writing this in vim the # does not stay there where I entered it.It is moved like so, by vim. Does anyone know of a configuration item in vim that would change this?If it helps, I am using Ubuntu 8.10. <code>  for i in range(10): # for i in range(10):#",How to configure vim to not put comments at the beginning of lines while editing python files
"In Python, how to I iterate over a dictionary in sorted order?"," There's an existing function that ends in the following, where d is a dictionary: that returns an unsorted iterator for a given dictionary. I would like to return an iterator that goes through the items sorted by key. How do I do that? <code>  return d.iteritems()","In Python, how do I iterate over a dictionary in sorted key order?"
"In Python, how do I iterate over a dictionary in sorted order?"," There's an existing function that ends in the following, where d is a dictionary: that returns an unsorted iterator for a given dictionary. I would like to return an iterator that goes through the items sorted by key. How do I do that? <code>  return d.iteritems()","In Python, how do I iterate over a dictionary in sorted key order?"
How to do this - python dictionary traverse and search," I have nested dictionaries: given an id - one of all the ids like 4130 to 4130-2-2.whats the easiest way to navigate to the correct dictionary?If the given id is 4130-2-1 then it should reach the dictionary with key=key5.No XML approaches please.Edit(1): The nesting is between 1 to 4 levels, but I know the nesting before I parse.Edit(2): Fixed the code.Edit(3): Fixed code again for string values of ids. Please excuse for the confusion created. This is final I hope :) <code>  {'key0': {'attrs': {'entity': 'p', 'hash': '34nj3h43b4n3', 'id': '4130'}, u'key1': {'attrs': {'entity': 'r', 'hash': '34njasd3h43b4n3', 'id': '4130-1'}, u'key2': {'attrs': {'entity': 'c', 'hash': '34njasd3h43bdsfsd4n3', 'id': '4130-1-1'}}}, u'key3': {'attrs': {'entity': 'r', 'hash': '34njasasasd3h43b4n3', 'id': '4130-2'}, u'key4': {'attrs': {'entity': 'c', 'hash': '34njawersd3h43bdsfsd4n3', 'id': '4130-2-1'}}, u'key5': {'attrs': {'entity': 'c', 'hash': '34njawersd3h43bdsfsd4n3', 'id': '4130-2-2'}}}}, 'someohterthing': 'someothervalue', 'something': 'somevalue'} ",How do I traverse and search a python dictionary?
python and regular expression with unicode," I need to delete some Unicode symbols from the string ' 'I know they exist here for sure. I tried: but it doesn't work. String stays the same. What am I doing wrong? <code>  re.sub('([\u064B-\u0652\u06D4\u0670\u0674\u06D5-\u06ED]+)', '', ' ')",Python and regular expression with Unicode
Extended slice that goes to beginning of sequence with reverse stride," Bear with me while I explain my question. Skip down to the bold heading if you already understand extended slice list indexing.In python, you can index lists using slice notation. Here's an example: You can also include a stride, which acts like a ""step"": The stride is also allowed to be negative, meaning the elements are retrieved in reverse order: But wait! I wanted to see [4, 3, 2, 1, 0]. Oh, I see, I need to decrement the start and end indices: What happened? It's interpreting -1 as being at the end of the array, not the beginning. I know you can achieve this as follows: But you can't use this in all cases. For example, in a method that's been passed indices. My question is:Is there any good pythonic way of using extended slices with negative strides and explicit start and end indices that include the first element of a sequence?This is what I've come up with so far, but it seems unsatisfying. <code>  >>> A = list(range(10))>>> A[0:5][0, 1, 2, 3, 4] >>> A[0:5:2][0, 2, 4] >>> A[5:0:-1][5, 4, 3, 2, 1] >>> A[4:-1:-1][] >>> A[4::-1][4, 3, 2, 1, 0] >>> A[0:5][::-1][4, 3, 2, 1, 0]",Extended slice that goes to beginning of sequence with negative stride
Reading and running a mathematical expression," Using Python, how would I go about reading in (be from a string, file or url) a mathematical expression (1 + 1 is a good start) and executing it? Aside from grabbing a string, file or url I have no idea of where to start with this. <code> ",Reading and running a mathematical expression in Python
What is the easiest way to export data from a live google app engine application?," I'm especially interested in solutions with source code available (Django independency is a plus, but I'm willing to hack my way through) <code> ",What is the easiest way to export data from a live Google App Engine application?
Python: Numpy array help.  Is there a function to return the index of something in an array?," I know there is a method for a Python list to return the first index of something: Is there something like that for NumPy arrays? <code>  >>> l = [1, 2, 3]>>> l.index(2)1",Is there a NumPy function to return the first index of something in an array?
Is there a function to return the index of something in an array?," I know there is a method for a Python list to return the first index of something: Is there something like that for NumPy arrays? <code>  >>> l = [1, 2, 3]>>> l.index(2)1",Is there a NumPy function to return the first index of something in an array?
Find the first index of something in a numpy array," I know there is a method for a Python list to return the first index of something: Is there something like that for NumPy arrays? <code>  >>> l = [1, 2, 3]>>> l.index(2)1",Is there a NumPy function to return the first index of something in an array?
Python: Numpy array help. Is there a function to return the first index of something in an array?," I know there is a method for a Python list to return the first index of something: Is there something like that for NumPy arrays? <code>  >>> l = [1, 2, 3]>>> l.index(2)1",Is there a NumPy function to return the first index of something in an array?
Is there a Numpy function to return the first index of something in an array?," I know there is a method for a Python list to return the first index of something: Is there something like that for NumPy arrays? <code>  >>> l = [1, 2, 3]>>> l.index(2)1",Is there a NumPy function to return the first index of something in an array?
I've got an existing Django app with a lack of database indexes. Is there a way to automatically generate a list of columns that need indexing?, The beauty of ORM lulled me into a soporific sleep. I've got an existing Django app with a lack of database indexes. Is there a way to automatically generate a list of columns that need indexing?I was thinking maybe some middleware that logs which columns are involved in WHERE clauses? but is there anything built into MySQL that might help? <code> ,Is there a way to automatically generate a list of columns that need indexing?
ReadInt() ReadByte() ReadString() etc in Python?," The functions ReadInt(), ReadByte(), and ReadString() (to name a few) exist in other languages for reading input from streams. I am trying to read from a socket, and I want to use functions like these. Are they tucked away in Python somewhere under a different way or has someone made a library for it?Also, there are Writedatatype() counterparts. <code> ","ReadInt(), ReadByte(), ReadString(), etc. in Python?"
Deployment options for script idea, I want to write a piece of software which is essentially a regex data scrubber. I am going to take a contact list in CSV and remove all non-word characters and such from the person's name.This project has Perl written all over it but my client base is largely non-technical and installing Perl on Windows would not be worth it for them.Any ideas on how I can use a Perl/Python/Ruby type language without all the headaches of getting the interpreter on their computer?Thought about web for a second but it would not work for business reasons. <code> ,How can I deploy a Perl/Python/Ruby script without installing an interpreter?
what's the difference between encode/decode? (python 2.x)," I've never been sure that I understand the difference between str/unicode decode and encode.I know that str().decode() is for when you have a string of bytes that you know has a certain character encoding, given that encoding name it will return a unicode string.I know that unicode().encode() converts unicode chars into a string of bytes according to a given encoding name.But I don't understand what str().encode() and unicode().decode() are for. Can anyone explain, and possibly also correct anything else I've gotten wrong above?EDIT:Several answers give info on what .encode does on a string, but no-one seems to know what .decode does for unicode. <code> ",What is the difference between encode/decode?
What is the difference between encode/decode? (python 2.x)," I've never been sure that I understand the difference between str/unicode decode and encode.I know that str().decode() is for when you have a string of bytes that you know has a certain character encoding, given that encoding name it will return a unicode string.I know that unicode().encode() converts unicode chars into a string of bytes according to a given encoding name.But I don't understand what str().encode() and unicode().decode() are for. Can anyone explain, and possibly also correct anything else I've gotten wrong above?EDIT:Several answers give info on what .encode does on a string, but no-one seems to know what .decode does for unicode. <code> ",What is the difference between encode/decode?
Python - why compile?," Why would you compile a Python script? You can run them directly from the .py file and it works fine, so is there a performance advantage or something? I also notice that some files in my application get compiled into .pyc while others do not, why is this? <code> ",Why compile Python code?
Shuffle an array with python, What's the easiest way to shuffle an array with python? <code> ,"Shuffle an array with python, randomize array item order with python"
How do you remove duplicates from a list in Python?," Given a list of strings, I want to sort it alphabetically and remove duplicates. I know I can do this: but I don't know how to retrieve the list members from the hash in alphabetical order.I'm not married to the hash, so any way to accomplish this will work. Also, performance is not an issue, so I'd prefer a solution that is expressed in code clearly to a fast but more opaque one. <code>  from sets import Set[...]myHash = Set(myList)",How to remove duplicates from Python list and keep order?
How do you remove duplicates from a list in Python if the item order is not important?," Given a list of strings, I want to sort it alphabetically and remove duplicates. I know I can do this: but I don't know how to retrieve the list members from the hash in alphabetical order.I'm not married to the hash, so any way to accomplish this will work. Also, performance is not an issue, so I'd prefer a solution that is expressed in code clearly to a fast but more opaque one. <code>  from sets import Set[...]myHash = Set(myList)",How to remove duplicates from Python list and keep order?
How do you remove duplicates from a list in Python whilst preserving order?," Is there a built-in that removes duplicates from list in Python, whilst preserving order? I know that I can use a set to remove duplicates, but that destroys the original order. I also know that I can roll my own like this: (Thanks to unwind for that code sample.)But I'd like to avail myself of a built-in or a more Pythonic idiom if possible.Related question: In Python, what is the fastest algorithm for removing duplicates from a list so that all elements are unique while preserving order? <code>  def uniq(input): output = [] for x in input: if x not in output: output.append(x) return output",How do you remove duplicates from a list whilst preserving order?
How do you remove duplicates from a list in whilst preserving order?," Is there a built-in that removes duplicates from list in Python, whilst preserving order? I know that I can use a set to remove duplicates, but that destroys the original order. I also know that I can roll my own like this: (Thanks to unwind for that code sample.)But I'd like to avail myself of a built-in or a more Pythonic idiom if possible.Related question: In Python, what is the fastest algorithm for removing duplicates from a list so that all elements are unique while preserving order? <code>  def uniq(input): output = [] for x in input: if x not in output: output.append(x) return output",How do you remove duplicates from a list whilst preserving order?
How can I use a Python -file to process a .txt -file? ," I have the following code in .py file: I have the data in .txt file as a sequence: How can I use the Python -file to process the .txt -file? I guess that we need a parameter in the .py file, so that we can use a syntax like in terminal: This question was raised by the post here. <code>  import reregex = re.compile( r""""""ULLAT:\ (?P<ullat>-?[\d.]+).*? ULLON:\ (?P<ullon>-?[\d.]+).*? LRLAT:\ (?P<lrlat>-?[\d.]+)"""""", re.DOTALL|re.VERBOSE) QUADNAME: rockport_colony_SDRESOLUTION: 10 ULLAT: 43.625 ULLON:-97.87527466 LRLAT: 43.5 LRLON: -97.75027466 HDATUM: 27 ZMIN: 361.58401489 ZMAX: 413.38400269 ZMEAN: 396.1293335 ZSIGMA: 12.36359215 PMETHOD: 5 QUADDATE: 20001001 $ py-file file-to-be-processed",How can I pass a filename as a parameter into my module?
django model question ( newbie )," First of all,I'm not into web programming. I bumped into django and read a bit about models. I was intrigued by the following code ( from djangoproject.com ) : By my understanding of python , first_name and last_name are class variables , right ? How is that used in code ( because I guess that setting Person.first_name or Person.last_name will affect all Person instances ) ? Why is it used that way ? <code>  class Person(models.Model): first_name = models.CharField(max_length=50) last_name = models.CharField(max_length=50) def __str__(self): # Note use of django.utils.encoding.smart_str() here because # first_name and last_name will be unicode strings. return smart_str('%s %s' % (self.first_name, self.last_name))",How do Django model fields work?
Traverse a list in reverse in Python, So I can start from collection[len(collection)-1] and end in collection[0].I also want to be able to access the loop index. <code> ,Traverse a list in reverse order in Python
Can you use a string to instantiate a class in python?," I'm using a builder pattern to seperate a bunch of different configuration possibilities. Basically, I have a bunch of classes that are named an ID (something like ID12345). These all inherit from the base builder class. In my script, I need to instantiate an instance for each class (about 50) every time this app runs. So, I'm trying to see if instead of doing something like this: Can I do something like this (I know this doesn't work): That way, when I need to add a new one in the future, all I have to do is add the id to the IDS list, rather than peppering the new ID throughout the code. EDITLooks like there are some different opinions based on where the data is coming from. These IDs are entered in a file that no one else has access to. I'm not reading the strings from the command line, and I'd like to be able to do as little alteration when adding a new ID in the future.  <code>  ProcessDirector = ProcessDirector()ID12345 = ID12345()ID01234 = ID01234()ProcessDirector.construct(ID12345)ProcessDirector.construct(ID01234)ID12345.run()ID01234.run() IDS = [""ID12345"", ""ID01234""]ProcessDirector = ProcessDirector()for id in IDS: builder = id() #some how instantiate class from string ProcessDirector.construct(builder) builder.run()",Can you use a string to instantiate a class?
What are the important language features (idioms) of Python to learn," I would be interested in knowing what the StackOverflow community thinks are the important language features (idioms) of Python. Features that would define a programmer as Pythonic.Python (pythonic) idiom - ""code expression"" that is natural or characteristic to the language Python. Plus, Which idioms should all Python programmers learn early on?Thanks in advanceRelated:Code Like a Pythonista: Idiomatic PythonPython: Am I missing something? <code> ",What are the important language features (idioms) of Python to learn early on
How do YOU deploy your WSGI application?," Deploying a WSGI application. There are many ways to skin this cat. I am currently using apache2 with mod-wsgi, but I can see some potential problems with this.So how can it be done?Apache Mod-wsgi (the other mod-wsgi's seem to not be worth it)Pure Python web server eg paste, cherrypy, Spawning, Twisted.webas 2 but with reverse proxy from nginx, apache2 etc, with good static file handlingConversion to other protocol such as FCGI with a bridge (eg Flup) and running in a conventional web server.More?I want to know how you do it, and why it is the best way to do it. I would absolutely love you to bore me with details about the whats and the whys, application specific stuff, etc. I will upvote any non-insane answer. <code> ",How do YOU deploy your WSGI application? (and why it is the best way)
What's the Python function like sum() but for multiplication?," Python's sum() function returns the sum of numbers in an iterable. I'm looking for the function that returns the product instead. I'm pretty sure such a function exists, but I can't find it. <code>  sum([3,4,5]) == 3 + 4 + 5 == 12 somelib.somefunc([3,4,5]) == 3 * 4 * 5 == 60",What's the function like sum() but for multiplication? product()?
What's the Python function like sum() but for multiplication? prod()?," Python's sum() function returns the sum of numbers in an iterable. I'm looking for the function that returns the product instead. I'm pretty sure such a function exists, but I can't find it. <code>  sum([3,4,5]) == 3 + 4 + 5 == 12 somelib.somefunc([3,4,5]) == 3 * 4 * 5 == 60",What's the function like sum() but for multiplication? product()?
What's the Python function like sum() but for multiplication? product()?," Python's sum() function returns the sum of numbers in an iterable. I'm looking for the function that returns the product instead. I'm pretty sure such a function exists, but I can't find it. <code>  sum([3,4,5]) == 3 + 4 + 5 == 12 somelib.somefunc([3,4,5]) == 3 * 4 * 5 == 60",What's the function like sum() but for multiplication? product()?
Looking for a Good Reference on Neural Nets," DuplicateWhat are some good resources for learning about Artificial Neural Networks?I'm looking for a good (beginner level) reference book (or website) on different types of Neural Nets/their applications/examples. I don't have any particular application in mind, I'm just curious as to how I can make use of them. I'm specifically interested in using them with Python, but any language, or even just theory would do fine. <code> ",Looking for a Good Reference on Neural Networks
how to detect whether a python variable is a function?," I have a variable, x, and I want to know whether it is pointing to a function or not.I had hoped I could do something like: But that gives me: The reason I picked that is because <code>  >>> isinstance(x, function) Traceback (most recent call last): File ""<stdin>"", line 1, in ?NameError: name 'function' is not defined >>> type(x)<type 'function'>",How do I detect whether a Python variable is a function?
Python - Intersection of two lists," I know how to get an intersection of two flat lists: or But when I have to find intersection for nested lists then my problems starts: In the end I would like to receive: Can you guys give me a hand with this?RelatedFlattening a shallow list in python <code>  b1 = [1,2,3,4,5,9,11,15]b2 = [4,5,6,7,8]b3 = [val for val in b1 if val in b2] def intersect(a, b): return list(set(a) & set(b)) print intersect(b1, b2) c1 = [1, 6, 7, 10, 13, 28, 32, 41, 58, 63]c2 = [[13, 17, 18, 21, 32], [7, 11, 13, 14, 28], [1, 5, 6, 8, 15, 16]] c3 = [[13,32],[7,13,28],[1,6]]",Find intersection of two nested lists?
Find intersection of two lists?," I know how to get an intersection of two flat lists: or But when I have to find intersection for nested lists then my problems starts: In the end I would like to receive: Can you guys give me a hand with this?RelatedFlattening a shallow list in python <code>  b1 = [1,2,3,4,5,9,11,15]b2 = [4,5,6,7,8]b3 = [val for val in b1 if val in b2] def intersect(a, b): return list(set(a) & set(b)) print intersect(b1, b2) c1 = [1, 6, 7, 10, 13, 28, 32, 41, 58, 63]c2 = [[13, 17, 18, 21, 32], [7, 11, 13, 14, 28], [1, 5, 6, 8, 15, 16]] c3 = [[13,32],[7,13,28],[1,6]]",Find intersection of two nested lists?
"Python, redirectinf the stream of Popen to a python function"," I'm new to python programming.I have this problem: I have a list of text files (both compressed and not) and I need to :- connect to the server and open them - after the opening of the file, I need to take his content and pass it to another python function that I wrote in order to elaborate the info contained in those files.The function has the aim of write in just 1 line the logs that are stored in those files using 3 lines ... The function is working fine on files read from my local machine but I cannot figure out how to connect to a remote server and create these one-line logs without storing the content of each file into a string and then working with the string ... The command that I use to connect to the remote machine is : retList[0] and retList[2] are the user@remote and the folder name that I have to accessThanks to all in advance !UPDATE:My problem is that I have to establish an ssh connection first : All the files that I need to open are stored in a list, fileList[], part of them are compressed (.gz) and part are just text files !! I have tried all the procedures that u showed before bot nothing worked ... I think that I mus modify the third argument of the Popen function but I cannot figure out how to do it ! Is there anyone that can help me ??? <code>  def readLogs (fileName):f = open (fileName, 'r')inStream = f.read()counter = 0inStream = re.split('\n', inStream) # Create a 'list of lines'out = """" # Will contain the outputlogInConst = """" # log In ConstructioncurLine = """" # Line that I am working onfor nextLine in inStream: logInConst += curLine curLine = nextLine # check if it is a start of a new log && check if the previous log is 'ready' if newLogRegExp.match(curLine) and logInConst != """": counter = counter + 1 out = logInConst logInConst = """" yield outyield logInConst + curLinedef checkFile (regExp, fileName): generatore = readLogs(fileName) listOfMatches=[] for i in generatore: #I'm now cycling through the logs # regExp must be a COMPILE regular expression if regExp.search(i): listOfMatches.append(i) return listOfMatches connection_out = Popen(['ssh', retList[0], 'cd '+retList[2]+'; cat'+fileName], stdout=PIPE).communicate()[0] pr1=Popen(['ssh', 'siatc@lgssp101', '*~/XYZ/AAAAA/log_archive/00/MSG_090308_162648.gz*' ], stdout=PIPE).communicate()[0]","Python, redirecting the stream of Popen to a python function"
How do I suppress scientific notation in Python?, Here's my code: My quotient displays as 1.00000e-05.Is there any way to suppress scientific notation and make it display as0.00001? I'm going to use the result as a string. <code>  x = 1.0y = 100000.0 print x/y,How to suppress scientific notation when printing float values?
Pagination of Date Based Generic Views in Django," I have a pretty simple question. I want to make some date-based generic views on a Django site, but I also want to paginate them. According to the documentation the object_list view has page and paginate_by arguments, but the archive_month view does not. What's the ""right"" way to do it? <code> ",Pagination of Date-Based Generic Views in Django
Can I transpose a file in vim?," I know I can use Awk, but I am on a Windows box, and I am making a function for others that may not have Awk. I also know I can write a C program, but I would love not to have something that requires compilation and maintenance for a little Vim utility I am making.The original file might be: and after the transposition, it should become: UpdateGolf rules apply to selecting correct answer.Python fans should check out Charles Duffys answer below. <code>  THE DAY WAS LONG THE WAY WAS FAST TTHHEEDWAAYYWWAASSLFOANSGT",How to transpose the contents of lines and columns in a file in Vim?
Can I transpose a file in Vim?," I know I can use Awk, but I am on a Windows box, and I am making a function for others that may not have Awk. I also know I can write a C program, but I would love not to have something that requires compilation and maintenance for a little Vim utility I am making.The original file might be: and after the transposition, it should become: UpdateGolf rules apply to selecting correct answer.Python fans should check out Charles Duffys answer below. <code>  THE DAY WAS LONG THE WAY WAS FAST TTHHEEDWAAYYWWAASSLFOANSGT",How to transpose the contents of lines and columns in a file in Vim?
Removing a specific items from Django's cache, I'm using site wide caching with memcached as the backend. I would like to invalidate pages in the cache when the underlying database object changes. If the page name changes then I would invalidate the whole cache (as it affects navigation on every page. Clumsy but sufficient for my needs.If just the page content changes then I'd like to invalidate the cache of just that page.Is there an easy way to do this?  <code> ,Removing specific items from Django's cache?
Python/Django pluggin for Dreamweaver, Does a plugin exists for Python/Django into Dreamweaver? Just wondering since Dreamweaver is a great web dev tool. <code> ,Python/Django plugin for Dreamweaver
How can I do unicode uppercase in python," I have this: What I need to do to print: (Where the 'a' gets its accute accent, but in uppercase.)I'm using Python 2.6. <code>  >>> print 'example'example>>> print 'exmple'exmple>>> print 'exmple'.upper()EXMPLE EXMPLE",How can I convert Unicode to uppercase to print it?
How can I do Unicode uppercase?," I have this: What I need to do to print: (Where the 'a' gets its accute accent, but in uppercase.)I'm using Python 2.6. <code>  >>> print 'example'example>>> print 'exmple'exmple>>> print 'exmple'.upper()EXMPLE EXMPLE",How can I convert Unicode to uppercase to print it?
Unserstanding python decorators," How can I make two decorators in Python that would do the following? ...which should return: I'm not trying to make HTML this way in a real application - just trying to understand how decorators and decorator chaining works. <code>  @makebold@makeitalicdef say(): return ""Hello"" ""<b><i>Hello</i></b>""",How to make function decorators and chain them together?
Understanding Python decorators," How can I make two decorators in Python that would do the following? ...which should return: I'm not trying to make HTML this way in a real application - just trying to understand how decorators and decorator chaining works. <code>  @makebold@makeitalicdef say(): return ""Hello"" ""<b><i>Hello</i></b>""",How to make function decorators and chain them together?
How can I make a chain of function decorators in Python?," How can I make two decorators in Python that would do the following? ...which should return: I'm not trying to make HTML this way in a real application - just trying to understand how decorators and decorator chaining works. <code>  @makebold@makeitalicdef say(): return ""Hello"" ""<b><i>Hello</i></b>""",How to make function decorators and chain them together?
How to make a chain of function decorators in Python?," How can I make two decorators in Python that would do the following? ...which should return: I'm not trying to make HTML this way in a real application - just trying to understand how decorators and decorator chaining works. <code>  @makebold@makeitalicdef say(): return ""Hello"" ""<b><i>Hello</i></b>""",How to make function decorators and chain them together?
How to make a chain of function decorators?," How can I make two decorators in Python that would do the following? ...which should return: I'm not trying to make HTML this way in a real application - just trying to understand how decorators and decorator chaining works. <code>  @makebold@makeitalicdef say(): return ""Hello"" ""<b><i>Hello</i></b>""",How to make function decorators and chain them together?
create function though mysqldb," How can I define a multi-statement function or procedure in using the MySQLdb lib in python?Example: Which creates the following traceback: If I copy the same SQL directly into a mysql shell client, it works as expected <code>  import MySQLdbdb = MySQLdb.connect(db='service')c = db.cursor()c.execute(""""""DELIMITER //CREATE FUNCTION trivial_func (radius float) RETURNS FLOAT BEGIN IF radius > 1 THEN RETURN 0.0; ELSE RETURN 1.0; END IF;END //DELIMITER ;"""""") Traceback (most recent call last): File ""proof.py"", line 21, in <module> DELIMITER ;"""""") File ""build/bdist.macosx-10.5-i386/egg/MySQLdb/cursors.py"", line 173, in execute File ""build/bdist.macosx-10.5-i386/egg/MySQLdb/connections.py"", line 35, in defaulterrorhandler_mysql_exceptions.ProgrammingError: (1064, ""You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'DELIMITER //\nCREATE FUNCTION trivial_func (radius float) \n RETURNS FLOAT\n\n ' at line 1"")",Create function through MySQLdb
Python: How to return something from a function that makes a dictionary," In my previous question, Andrew Jaffe writes: In addition to all of the other hints and tips, I think you're missing something crucial: your functions actually need to return something. When you create autoparts() or splittext(), the idea is that this will be a function that you can call, and it can (and should) give something back. Once you figure out the output that you want your function to have, you need to put it in a return statement. This function creates a dictionary, but it does not return something. However, since I added the print, the output of the function is shown when I run the function. What is the difference between returning something and printing it? <code>  def autoparts(): parts_dict = {} list_of_parts = open('list_of_parts.txt', 'r') for line in list_of_parts: k, v = line.split() parts_dict[k] = v print(parts_dict)>>> autoparts(){'part A': 1, 'part B': 2, ...}",How is returning the output of a function different from printing it?
How to return something from a function that makes a dictionary," In my previous question, Andrew Jaffe writes: In addition to all of the other hints and tips, I think you're missing something crucial: your functions actually need to return something. When you create autoparts() or splittext(), the idea is that this will be a function that you can call, and it can (and should) give something back. Once you figure out the output that you want your function to have, you need to put it in a return statement. This function creates a dictionary, but it does not return something. However, since I added the print, the output of the function is shown when I run the function. What is the difference between returning something and printing it? <code>  def autoparts(): parts_dict = {} list_of_parts = open('list_of_parts.txt', 'r') for line in list_of_parts: k, v = line.split() parts_dict[k] = v print(parts_dict)>>> autoparts(){'part A': 1, 'part B': 2, ...}",How is returning the output of a function different from printing it?
How is returning the output of a function different than printing it?," In my previous question, Andrew Jaffe writes: In addition to all of the other hints and tips, I think you're missing something crucial: your functions actually need to return something. When you create autoparts() or splittext(), the idea is that this will be a function that you can call, and it can (and should) give something back. Once you figure out the output that you want your function to have, you need to put it in a return statement. This function creates a dictionary, but it does not return something. However, since I added the print, the output of the function is shown when I run the function. What is the difference between returning something and printing it? <code>  def autoparts(): parts_dict = {} list_of_parts = open('list_of_parts.txt', 'r') for line in list_of_parts: k, v = line.split() parts_dict[k] = v print(parts_dict)>>> autoparts(){'part A': 1, 'part B': 2, ...}",How is returning the output of a function different from printing it?
django on Google Appengine," I came across 2 different modules for porting Django to App Engine:http://code.google.com/p/app-engine-patch/http://code.google.com/p/google-app-engine-django/Both seem to be compatible with Django 1.0,The featured download of the latter is in Aug 08, whereas the former is Feb 09.What are the relative merits?What if I don't use the database at all, would it matter? <code> ",2 different Django modules on Google App Engine
How to call a parent class's method from child class in python?," When creating a simple object hierarchy in Python, I'd like to be able to invoke methods of the parent class from a derived class. In Perl and Java, there is a keyword for this (super). In Perl, I might do this: In Python, it appears that I have to name the parent class explicitly from the child.In the example above, I'd have to do something like Foo::frotz(). This doesn't seem right since this behavior makes it hard to make deep hierarchies. If children need to know what class defined an inherited method, then all sorts of information pain is created. Is this an actual limitation in python, a gap in my understanding or both? <code>  package Foo;sub frotz { return ""Bamf"";}package Bar;@ISA = qw(Foo);sub frotz { my $str = SUPER::frotz(); return uc($str);}",How do I call a parent class's method from a child class in Python?
Call a parent class's method from child class in Python?," When creating a simple object hierarchy in Python, I'd like to be able to invoke methods of the parent class from a derived class. In Perl and Java, there is a keyword for this (super). In Perl, I might do this: In Python, it appears that I have to name the parent class explicitly from the child.In the example above, I'd have to do something like Foo::frotz(). This doesn't seem right since this behavior makes it hard to make deep hierarchies. If children need to know what class defined an inherited method, then all sorts of information pain is created. Is this an actual limitation in python, a gap in my understanding or both? <code>  package Foo;sub frotz { return ""Bamf"";}package Bar;@ISA = qw(Foo);sub frotz { my $str = SUPER::frotz(); return uc($str);}",How do I call a parent class's method from a child class in Python?
Call a parent class's method from child class?," When creating a simple object hierarchy in Python, I'd like to be able to invoke methods of the parent class from a derived class. In Perl and Java, there is a keyword for this (super). In Perl, I might do this: In Python, it appears that I have to name the parent class explicitly from the child.In the example above, I'd have to do something like Foo::frotz(). This doesn't seem right since this behavior makes it hard to make deep hierarchies. If children need to know what class defined an inherited method, then all sorts of information pain is created. Is this an actual limitation in python, a gap in my understanding or both? <code>  package Foo;sub frotz { return ""Bamf"";}package Bar;@ISA = qw(Foo);sub frotz { my $str = SUPER::frotz(); return uc($str);}",How do I call a parent class's method from a child class in Python?
How to call a Parent Class's method from Child Class in Python?," When creating a simple object hierarchy in Python, I'd like to be able to invoke methods of the parent class from a derived class. In Perl and Java, there is a keyword for this (super). In Perl, I might do this: In Python, it appears that I have to name the parent class explicitly from the child.In the example above, I'd have to do something like Foo::frotz(). This doesn't seem right since this behavior makes it hard to make deep hierarchies. If children need to know what class defined an inherited method, then all sorts of information pain is created. Is this an actual limitation in python, a gap in my understanding or both? <code>  package Foo;sub frotz { return ""Bamf"";}package Bar;@ISA = qw(Foo);sub frotz { my $str = SUPER::frotz(); return uc($str);}",How do I call a parent class's method from a child class in Python?
How can I check parity without converting to binary?," How can I get the number of ""1""s in the binary representation of a number without actually converting and counting ?e.g. <code>  def number_of_ones(n): # do something # I want to MAKE this FASTER (computationally less complex). c = 0 while n: c += n%2 n /= 2 return c>>> number_of_ones(5) 2>>> number_of_ones(4) 1",How can I check Hamming Weight without converting to binary?
django admin filter," How can I change the default filter choice from 'ALL'? I have a field named as status which has three values: activate, pending and rejected. When I use list_filter in Django admin, the filter is by default set to 'All' but I want to set it to pending by default. <code> ",Default filter in Django admin
item frequency count in python," Assume I have a list of words, and I want to find the number of times each word appears in that list.An obvious way to do this is: But I find this code not very good, because the program runs through the word list twice, once to build the set, and a second time to count the number of appearances.Of course, I could write a function to run through the list and do the counting, but that wouldn't be so Pythonic. So, is there a more efficient and Pythonic way? <code>  words = ""apple banana apple strawberry banana lemon""uniques = set(words.split())freqs = [(item, words.split().count(item)) for item in uniques]print(freqs)",Item frequency count in Python
Detect if a numpy array contains at least one non-numeric value?," I need to write a function which will detect if the input contains at least one value which is non-numeric. If a non-numeric value is found I will raise an error (because the calculation should only return a numeric value). The number of dimensions of the input array is not known in advance - the function should give the correct value regardless of ndim. As an extra complication the input could be a single float or numpy.float64 or even something oddball like a zero-dimensional array. The obvious way to solve this is to write a recursive function which iterates over every iterable object in the array until it finds a non-iterabe. It will apply the numpy.isnan() function over every non-iterable object. If at least one non-numeric value is found then the function will return False immediately. Otherwise if all the values in the iterable are numeric it will eventually return True. That works just fine, but it's pretty slow and I expect that NumPy has a much better way to do it. What is an alternative that is faster and more numpyish?Here's my mockup: <code>  def contains_nan( myarray ): """""" @param myarray : An n-dimensional array or a single float @type myarray : numpy.ndarray, numpy.array, float @returns: bool Returns true if myarray is numeric or only contains numeric values. Returns false if at least one non-numeric value exists Not-A-Number is given by the numpy.isnan() function. """""" return True",Detect if a NumPy array contains at least one non-numeric value?
retrieving a variable's name in python at runtime?," Is there a way to know, during run-time, a variable's name (from the code)?Or do variable's names forgotten during compilation (byte-code or not)?e.g.: Note: I'm talking about plain data-type variables (int, str, list etc.) <code>  >>> vari = 15>>> print vari.~~name~~()'vari'",How to retrieve a variable's name in python at runtime?
Write to utf-8 file in python," I'm really confused with the codecs.open function. When I do: It gives me the error UnicodeDecodeError: 'ascii' codec can't decode byte 0xef in position 0: ordinal not in range(128)If I do: It works fine.Question is why does the first method fail? And how do I insert the bom?If the second method is the correct way of doing it, what the point of using codecs.open(filename, ""w"", ""utf-8"")? <code>  file = codecs.open(""temp"", ""w"", ""utf-8"")file.write(codecs.BOM_UTF8)file.close() file = open(""temp"", ""w"")file.write(codecs.BOM_UTF8)file.close()",Write to UTF-8 file in Python
Python lambda function parameters are reference or value?," I need a callback function that is almost exactly the same for a series of gui events. The function will behave slightly differently depending on which event has called it. Seems like a simple case to me, but I cannot figure out this weird behavior of lambda functions.So I have the following simplified code below: The output of this code is: I expected: Why has using an iterator messed things up?I've tried using a deepcopy: But this has the same problem. <code>  def callback(msg): print msg#creating a list of function handles with an iteratorfuncList=[]for m in ('do', 're', 'mi'): funcList.append(lambda: callback(m))for f in funcList: f()#create one at a timefuncList=[]funcList.append(lambda: callback('do'))funcList.append(lambda: callback('re'))funcList.append(lambda: callback('mi'))for f in funcList: f() mimimidoremi doremidoremi import copyfuncList=[]for m in ('do', 're', 'mi'): funcList.append(lambda: callback(copy.deepcopy(m)))for f in funcList: f()",Scope of lambda functions and their parameters?
Scope of python lambda functions and their parameters," I need a callback function that is almost exactly the same for a series of gui events. The function will behave slightly differently depending on which event has called it. Seems like a simple case to me, but I cannot figure out this weird behavior of lambda functions.So I have the following simplified code below: The output of this code is: I expected: Why has using an iterator messed things up?I've tried using a deepcopy: But this has the same problem. <code>  def callback(msg): print msg#creating a list of function handles with an iteratorfuncList=[]for m in ('do', 're', 'mi'): funcList.append(lambda: callback(m))for f in funcList: f()#create one at a timefuncList=[]funcList.append(lambda: callback('do'))funcList.append(lambda: callback('re'))funcList.append(lambda: callback('mi'))for f in funcList: f() mimimidoremi doremidoremi import copyfuncList=[]for m in ('do', 're', 'mi'): funcList.append(lambda: callback(copy.deepcopy(m)))for f in funcList: f()",Scope of lambda functions and their parameters?
Python total memory used," Is there a way for a Python program to determine how much memory it's currently using? I've seen discussions about memory usage for a single object, but what I need is total memory usage for the process, so that I can determine when it's necessary to start discarding cached data. <code> ",Total memory used by Python process?
Help me port this NetHack function to Python please!," I am trying to write a Python function which returns the same moon phase value as in the game NetHack. This is found in hacklib.c.I have tried to simply copy the corresponding function from the NetHack code but I don't believe I am getting the correct results.The function which I have written is phase_of_the_moon().The functions position() and phase(), I found on the net, and I am using them as an indication of the success of my function. They are very accurate and give results which approximately match the nethack.alt.org server (see http://alt.org/nethack/moon/pom.txt). What I am after however is an exact replication of the original NetHack function, idiosyncrasies intact.I would expect my function and the 'control' function to give the same moon phase at least, but currently they do not and I'm not sure why!Here is the NetHack code: Here is the getlt() function (also in hacklib.c): Here is my Python code: <code>  /* * moon period = 29.53058 days ~= 30, year = 365.2422 days * days moon phase advances on first day of year compared to preceding year * = 365.2422 - 12*29.53058 ~= 11 * years in Metonic cycle (time until same phases fall on the same days of * the month) = 18.6 ~= 19 * moon phase on first day of year (epact) ~= (11*(year%19) + 29) % 30 * (29 as initial condition) * current phase in days = first day phase + days elapsed in year * 6 moons ~= 177 days * 177 ~= 8 reported phases * 22 * + 11/22 for rounding */intphase_of_the_moon() /* 0-7, with 0: new, 4: full */{ register struct tm *lt = getlt(); register int epact, diy, goldn; diy = lt->tm_yday; goldn = (lt->tm_year % 19) + 1; epact = (11 * goldn + 18) % 30; if ((epact == 25 && goldn > 11) || epact == 24) epact++; return( (((((diy + epact) * 6) + 11) % 177) / 22) & 7 );} static struct tm *getlt(){ time_t date;#if defined(BSD) && !defined(POSIX_TYPES) (void) time((long *)(&date));#else (void) time(&date);#endif#if (defined(ULTRIX) && !(defined(ULTRIX_PROTO) || defined(NHSTDC))) || (defined(BSD) && !defined(POSIX_TYPES)) return(localtime((long *)(&date)));#else return(localtime(&date));#endif} from datetime import datedef phase_of_the_moon(): lt = date.today() diy = (lt - date(lt.year, 1, 1)).days goldn = ((lt.year - 1900) % 19) + 1 epact = (11 * goldn + 18) % 30; if ((epact == 25 and goldn > 11) or epact == 24): epact += 1 return ( (((((diy + epact) * 6) + 11) % 177) / 22) & 7 )import math, decimal, datetimedec = decimal.Decimaldef position(now=None): if now is None: now = datetime.datetime.now() diff = now - datetime.datetime(2001, 1, 1) days = dec(diff.days) + (dec(diff.seconds) / dec(86400)) lunations = dec(""0.20439731"") + (days * dec(""0.03386319269"")) return lunations % dec(1)def phase(pos): index = (pos * dec(8)) + dec(""0.5"") index = math.floor(index) return { 0: ""New Moon"", 1: ""Waxing Crescent"", 2: ""First Quarter"", 3: ""Waxing Gibbous"", 4: ""Full Moon"", 5: ""Waning Gibbous"", 6: ""Last Quarter"", 7: ""Waning Crescent"" }[int(index) & 7]def phase2(pos): return { 0: ""New Moon"", 1: ""Waxing Crescent"", 2: ""First Quarter"", 3: ""Waxing Gibbous"", 4: ""Full Moon"", 5: ""Waning Gibbous"", 6: ""Last Quarter"", 7: ""Waning Crescent"" }[int(pos)]def main(): ## Correct output pos = position() phasename = phase(pos) roundedpos = round(float(pos), 3) print ""%s (%s)"" % (phasename, roundedpos) ## My output print ""%s (%s)"" % (phase2(phase_of_the_moon()), phase_of_the_moon())if __name__==""__main__"": main()",How to port this NetHack function to Python?
Help me to port this NetHack function to Python please!," I am trying to write a Python function which returns the same moon phase value as in the game NetHack. This is found in hacklib.c.I have tried to simply copy the corresponding function from the NetHack code but I don't believe I am getting the correct results.The function which I have written is phase_of_the_moon().The functions position() and phase(), I found on the net, and I am using them as an indication of the success of my function. They are very accurate and give results which approximately match the nethack.alt.org server (see http://alt.org/nethack/moon/pom.txt). What I am after however is an exact replication of the original NetHack function, idiosyncrasies intact.I would expect my function and the 'control' function to give the same moon phase at least, but currently they do not and I'm not sure why!Here is the NetHack code: Here is the getlt() function (also in hacklib.c): Here is my Python code: <code>  /* * moon period = 29.53058 days ~= 30, year = 365.2422 days * days moon phase advances on first day of year compared to preceding year * = 365.2422 - 12*29.53058 ~= 11 * years in Metonic cycle (time until same phases fall on the same days of * the month) = 18.6 ~= 19 * moon phase on first day of year (epact) ~= (11*(year%19) + 29) % 30 * (29 as initial condition) * current phase in days = first day phase + days elapsed in year * 6 moons ~= 177 days * 177 ~= 8 reported phases * 22 * + 11/22 for rounding */intphase_of_the_moon() /* 0-7, with 0: new, 4: full */{ register struct tm *lt = getlt(); register int epact, diy, goldn; diy = lt->tm_yday; goldn = (lt->tm_year % 19) + 1; epact = (11 * goldn + 18) % 30; if ((epact == 25 && goldn > 11) || epact == 24) epact++; return( (((((diy + epact) * 6) + 11) % 177) / 22) & 7 );} static struct tm *getlt(){ time_t date;#if defined(BSD) && !defined(POSIX_TYPES) (void) time((long *)(&date));#else (void) time(&date);#endif#if (defined(ULTRIX) && !(defined(ULTRIX_PROTO) || defined(NHSTDC))) || (defined(BSD) && !defined(POSIX_TYPES)) return(localtime((long *)(&date)));#else return(localtime(&date));#endif} from datetime import datedef phase_of_the_moon(): lt = date.today() diy = (lt - date(lt.year, 1, 1)).days goldn = ((lt.year - 1900) % 19) + 1 epact = (11 * goldn + 18) % 30; if ((epact == 25 and goldn > 11) or epact == 24): epact += 1 return ( (((((diy + epact) * 6) + 11) % 177) / 22) & 7 )import math, decimal, datetimedec = decimal.Decimaldef position(now=None): if now is None: now = datetime.datetime.now() diff = now - datetime.datetime(2001, 1, 1) days = dec(diff.days) + (dec(diff.seconds) / dec(86400)) lunations = dec(""0.20439731"") + (days * dec(""0.03386319269"")) return lunations % dec(1)def phase(pos): index = (pos * dec(8)) + dec(""0.5"") index = math.floor(index) return { 0: ""New Moon"", 1: ""Waxing Crescent"", 2: ""First Quarter"", 3: ""Waxing Gibbous"", 4: ""Full Moon"", 5: ""Waning Gibbous"", 6: ""Last Quarter"", 7: ""Waning Crescent"" }[int(index) & 7]def phase2(pos): return { 0: ""New Moon"", 1: ""Waxing Crescent"", 2: ""First Quarter"", 3: ""Waxing Gibbous"", 4: ""Full Moon"", 5: ""Waning Gibbous"", 6: ""Last Quarter"", 7: ""Waning Crescent"" }[int(pos)]def main(): ## Correct output pos = position() phasename = phase(pos) roundedpos = round(float(pos), 3) print ""%s (%s)"" % (phasename, roundedpos) ## My output print ""%s (%s)"" % (phase2(phase_of_the_moon()), phase_of_the_moon())if __name__==""__main__"": main()",How to port this NetHack function to Python?
What does the percentage sign mean in Python 3.1," In the tutorial there is an example for finding prime numbers: I understand that the double == is a test for equality, but I don't understand the if n % x part. Like I can verbally walk through each part and say what the statement does for the example. But I don't understand how the percentage sign falls in. What does if n % x actually say? <code>  >>> for n in range(2, 10):... for x in range(2, n):... if n % x == 0:... print(n, 'equals', x, '*', n//x)... break... else:... # loop fell through without finding a factor... print(n, 'is a prime number')...",What does the percentage sign mean in Python
Formatting date times provided as strings in Django.," In my Django application I get times from a webservice, provided as a string, that I use in my templates: This provides me with a date such as: These are obviously a bit ugly, and I'd like to present them in a nice format to my users. Django has a great built in date formatter, which would do exactly what I wanted: However this expects the value to be provided as a date object, and not a string. So I can't format it using this. After searching here on StackOverflow pythons strptime seems to do what I want, but being fairly new to Python I was wondering if anyone could come up with an easier way of getting date formatting using strings, without having to resort to writing a whole new custom strptime template tag?  <code>  {{date.string}} 2009-06-11 17:02:09+0000 {{ value|date:""D d M Y"" }}",Formatting date times provided as strings in Django
How to leave a python virtualenv?," I'm using virtualenv and the virtualenvwrapper. I can switch between virtualenv's just fine using the workon command. How do I exit all virtual environments and work on my system environment again? Right now, the only way I have of getting back to me@mymachine:~$ is to exit the shell and start a new one. That's kind of annoying. Is there a command to work on ""nothing"", and if so, what is it? If such a command does not exist, how would I go about creating it? <code>  me@mymachine:~$ workon env1(env1)me@mymachine:~$ workon env2(env2)me@mymachine:~$ workon env1(env1)me@mymachine:~$ ",How to leave/exit/deactivate a Python virtualenv
How to leave/exit/deactivate a python virtualenv?," I'm using virtualenv and the virtualenvwrapper. I can switch between virtualenv's just fine using the workon command. How do I exit all virtual environments and work on my system environment again? Right now, the only way I have of getting back to me@mymachine:~$ is to exit the shell and start a new one. That's kind of annoying. Is there a command to work on ""nothing"", and if so, what is it? If such a command does not exist, how would I go about creating it? <code>  me@mymachine:~$ workon env1(env1)me@mymachine:~$ workon env2(env2)me@mymachine:~$ workon env1(env1)me@mymachine:~$ ",How to leave/exit/deactivate a Python virtualenv
String Slicing Python," I have a string, example: Where a ',' (comma) will always be the 3rd to the last character, aka s[-3].I am thinking of ways to remove the ',' but can only think of converting the string into a list, deleting it, and converting it back to a string. This however seems a bit too much for simple task. How can I accomplish this in a simpler way? <code>  s = ""this is a string, a""",Ways to slice a string?
decimal alignment formatting in python," This should be easy.Here's my array (rather, a method of generating representative test arrays): I want a list of strings where '\n'.join(list_o_strings) would print: I want to space pad to the left and the right (but no more than necessary).I want a zero after the decimal if that is all that is after the decimal.I do not want scientific notation...and I do not want to lose any significant digits. (in 353.98000000000002 the 2 is not significant)Yeah, it's nice to want..Python 2.5's %g, %fx.x, etc. are either befuddling me, or can't do it.I have not tried import decimal yet. I can't see that NumPy does it either (although, the array.__str__ and array.__repr__ are decimal aligned (but sometimes return scientific).Oh, and speed counts. I'm dealing with big arrays here.My current solution approaches are:to str(a) and parse off NumPy's bracketsto str(e) each element in the array and split('.') then pad and reconstructto a.astype('S'+str(i)) where i is the max(len(str(a))), then padIt seems like there should be some off-the-shelf solution out there... (but not required)Top suggestion fails with when dtype is float64: <code>  >>> ri = numpy.random.randint>>> ri2 = lambda x: ''.join(ri(0,9,x).astype('S'))>>> a = array([float(ri2(x)+ '.' + ri2(y)) for x,y in ri(1,10,(10,2))])>>> aarray([ 7.99914000e+01, 2.08000000e+01, 3.94000000e+02, 4.66100000e+03, 5.00000000e+00, 1.72575100e+03, 3.91500000e+02, 1.90610000e+04, 1.16247000e+04, 3.53920000e+02]) 79.9914 20.8 394.0 4661.0 5.0 1725.751 391.519061.011624.7 353.92 >>> aarray([ 5.50056103e+02, 6.77383566e+03, 6.01001513e+05, 3.55425142e+08, 7.07254875e+05, 8.83174744e+02, 8.22320510e+01, 4.25076609e+08, 6.28662635e+07, 1.56503068e+02])>>> ut0 = re.compile(r'(\d)0+$')>>> thelist = [ut0.sub(r'\1', ""%12f"" % x) for x in a]>>> print '\n'.join(thelist) 550.056103 6773.835663601001.513355425141.8471707254.875038 883.174744 82.232051425076608.767662866263.55 156.503068",Decimal alignment formatting in Python
Python: Data Structure for Maintaing Tabular Data in Memory?," My scenario is as follows: I have a table of data (handful of fields, less than a hundred rows) that I use extensively in my program. I also need this data to be persistent, so I save it as a CSV and load it on start-up. I choose not to use a database because every option (even SQLite) is an overkill for my humble requirement (also - I would like to be able to edit the values offline in a simple way, and nothing is simpler than notepad).Assume my data looks as follows (in the file it's comma separated without titles, this is just an illustration): Notes:Row may be a ""real"" value written to the file or just an auto-generated value that represents the row number. Either way it exists in memory.Names are unique.Things I do with the data:Look-up a row based on either ID (iteration) or name (direct access).Display the table in different orders based on multiple field: I need to sort it e.g. by Priority and then Year, or Year and then Priority, etc.I need to count instances based on sets of parameters, e.g. how many rows have their year between 1997 and 2002, or how many rows are in 1998 and priority > 2, etc.I know this ""cries"" for SQL...I'm trying to figure out what's the best choice for data structure. Following are several choices I see:List of row lists: List of column lists (there will obviously be an API for add_row etc): Dictionary of columns lists (constants can be created to replace the string keys): Dictionary with keys being tuples of (Row, Field): And I'm sure there are other ways... However each way has disadvantages when it comes to my requirements (complex ordering and counting). What's the recommended approach?EDIT:To clarify, performance is not a major issue for me. Because the table is so small, I believe almost every operation will be in the range of milliseconds, which is not a concern for my application. <code>  Row | Name | Year | Priority------------------------------------ 1 | Cat | 1998 | 1 2 | Fish | 1998 | 2 3 | Dog | 1999 | 1 4 | Aardvark | 2000 | 1 5 | Wallaby | 2000 | 1 6 | Zebra | 2001 | 3 a = []a.append( [1, ""Cat"", 1998, 1] )a.append( [2, ""Fish"", 1998, 2] )a.append( [3, ""Dog"", 1999, 1] )... a = []a.append( [1, 2, 3, 4, 5, 6] )a.append( [""Cat"", ""Fish"", ""Dog"", ""Aardvark"", ""Wallaby"", ""Zebra""] )a.append( [1998, 1998, 1999, 2000, 2000, 2001] )a.append( [1, 2, 1, 1, 1, 3] ) a = {}a['ID'] = [1, 2, 3, 4, 5, 6]a['Name'] = [""Cat"", ""Fish"", ""Dog"", ""Aardvark"", ""Wallaby"", ""Zebra""] a['Year'] = [1998, 1998, 1999, 2000, 2000, 2001] a['Priority'] = [1, 2, 1, 1, 1, 3] Create constants to avoid string searchingNAME=1YEAR=2PRIORITY=3a={}a[(1, NAME)] = ""Cat""a[(1, YEAR)] = 1998a[(1, PRIORITY)] = 1a[(2, NAME)] = ""Fish""a[(2, YEAR)] = 1998a[(2, PRIORITY)] = 2...",Data structure for maintaining tabular data in memory?
Unable to understand Python-way to code, What is the difference between the following codes?code1: code2: I see no difference. This raises the following question.Why is the code1 used if we can use code2? <code>  var=2**2*3 var2=2*2*3,"What's the difference between ""2*2"" and ""2**2"" in Python?"
"What's the different between ""2*2"" and ""2**2"" in Python?", What is the difference between the following codes?code1: code2: I see no difference. This raises the following question.Why is the code1 used if we can use code2? <code>  var=2**2*3 var2=2*2*3,"What's the difference between ""2*2"" and ""2**2"" in Python?"
python decimal comparasion," python decimal comparison I was expecting it to convert 2.0 correctly, but after reading thru PEP 327 I understand there were some reason for not implictly converting float to Decimal, but shouldn't in that case it should raise TypeError as it does in this case so does all other operator / - % // etcso my questions areis this right behavior? (not to raise exception in cmp)What if I derive my own class andright a float converter basicallyDecimal(repr(float_value)), arethere any caveats? my use caseinvolves only comparison of pricesSystem details: Python 2.5.2 on Ubuntu 8.04.1 <code>  >>> from decimal import Decimal>>> Decimal('1.0') > 2.0True >>> Decimal('1.0') + 2.0Traceback (most recent call last): File ""<string>"", line 1, in <string>TypeError: unsupported operand type(s) for +: 'Decimal' and 'float'",python decimal comparison
Multiple Python classes in a single file," Possible Duplicate: How many Python classes should I put in one file? Coming from a C++ background I've grown accustomed to organizing my classes such that, for the most part, there's a 1:1 ratio between classes and files. By making it so that a single file contains a single class I find the code more navigable. As I introduce myself to Python I'm finding lots of examples where a single file contains multiple classes. Is that the recommended way of doing things in Python? If so, why?Am I missing this convention in the PEP8? <code> ",Are multiple classes in a single file recommended?
Are multiple Python classes in a single file recommended?," Possible Duplicate: How many Python classes should I put in one file? Coming from a C++ background I've grown accustomed to organizing my classes such that, for the most part, there's a 1:1 ratio between classes and files. By making it so that a single file contains a single class I find the code more navigable. As I introduce myself to Python I'm finding lots of examples where a single file contains multiple classes. Is that the recommended way of doing things in Python? If so, why?Am I missing this convention in the PEP8? <code> ",Are multiple classes in a single file recommended?
Event system in Python," I am aware of pydispatcher, but there must be other event-related packages around for Python.Which libraries are available?I'm not interested in event managers that are part of large frameworks, I'd rather use a small bare-bones solution that I can easily extend. <code> ",Which Python packages offer a stand-alone event system?
FFT-based convolution and correlation in Python," Is there a FFT-based 2D cross-correlation or convolution function built into scipy (or another popular library)?There are functions like these:scipy.signal.correlate2d - ""the direct method implemented by convolveND will beslow for large data""scipy.ndimage.correlate - ""The array is correlated with the given kernel usingexact calculation (i.e. not FFT).""scipy.fftpack.convolve.convolve, which I don't really understand, but seems wrongnumarray had a correlate2d() function with an fft=True switch, but I guess numarray was foldedinto numpy, and I can't find if this function was included. <code> ",FFT-based 2D convolution and correlation in Python
Library to read ELF file debug information, Any recommendations for a good cross-platform library for reading ELF file debug information in DWARF format? I'd like to read the DWARF debug info in a Python program. <code> ,Library to read ELF file DWARF debug information
How to parse dates with -0400 timezone string in python?," I have a date string of the form '2009/05/13 19:19:30 -0400'. It seems that previous versions of Python may have supported a %z format tag in strptime for the trailing timezone specification, but 2.6.x seems to have removed that.What's the right way to parse this string into a datetime object? <code> ",How to parse dates with -0400 timezone string in Python?
Find methods defined in a module (python)," Ok I know you can use the dir() method to list everything in a module, but is there any way to see only the functions that are defined in that module? For example, assume my module looks like this: Even if i use inspect() to filter out the builtins, I'm still left with anything that was imported. E.g I'll see:['date', 'datetime', 'test']Is there any way to exclude imports? Or another way to find out what's defined in a module? <code>  from datetime import date, datetimedef test(): return ""This is a real method""",Find functions explicitly defined in a module (python)
Find methods explicitly defined in a module (python)," Ok I know you can use the dir() method to list everything in a module, but is there any way to see only the functions that are defined in that module? For example, assume my module looks like this: Even if i use inspect() to filter out the builtins, I'm still left with anything that was imported. E.g I'll see:['date', 'datetime', 'test']Is there any way to exclude imports? Or another way to find out what's defined in a module? <code>  from datetime import date, datetimedef test(): return ""This is a real method""",Find functions explicitly defined in a module (python)
What is the most efficent way to store a list in the Django models?," Currently I have a lot of python objects in my code similar to the following: Now I want to turn this into a Django model, where self.myName is a string field, and self.myFriends is a list of strings. Since the list is such a common data structure in python, I sort of expected there to be a Django model field for it. I know I can use a ManyToMany or OneToMany relationship, but I was hoping to avoid that extra indirection in the code.Edit:I added this related question, which people may find useful. <code>  class MyClass(): def __init__(self, name, friends): self.myName = name self.myFriends = [str(x) for x in friends] from django.db import modelsclass myDjangoModelClass(): myName = models.CharField(max_length=64) myFriends = ??? # what goes here?",What is the most efficient way to store a list in the Django models?
Django persistent database connection.," I'm using django with apache and mod_wsgi and PostgreSQL (all on same host), and I need to handle a lot of simple dynamic page requests (hundreds per second). I faced with problem that the bottleneck is that a django don't have persistent database connection and reconnects on each requests (that takes near 5ms).While doing a benchmark I got that with persistent connection I can handle near 500 r/s while without I get only 50 r/s.Anyone have any advice? How can I modify Django to use a persistent connection or speed up the connection from Python to DB? <code> ",Django persistent database connection
Python inheritance and calling parent class constructor (newbie question...)," This is what I'm trying to do in Python: Which results in: What gives? Why doesn't this work as I expect? <code>  class BaseClass: def __init__(self): print 'The base class constructor ran!' self.__test = 42class ChildClass(BaseClass): def __init__(self): print 'The child class constructor ran!' BaseClass.__init__(self) def doSomething(self): print 'Test is: ', self.__testtest = ChildClass()test.doSomething() AttributeError: ChildClass instance has no attribute '_ChildClass__test'",Python inheritance and calling parent class constructor
PYTHON: Removing duplicates from list of lists," Can anyone suggest a good solution to remove duplicates from nested lists if wanting to evaluate duplicates based on first element of each nested list?The main list looks like this: If there is another list with the same element at first position [k][0] that had already occurred, then I'd like to remove that list and get this result: Can you suggest an algorithm to achieve this goal? <code>  L = [['14', '65', 76], ['2', '5', 6], ['7', '12', 33], ['14', '22', 46]] L = [['14', '65', 76], ['2', '5', 6], ['7', '12', 33]]",Removing duplicates from list of lists in Python
Removing duplicates from list of lists in python," Can anyone suggest a good solution to remove duplicates from nested lists if wanting to evaluate duplicates based on first element of each nested list?The main list looks like this: If there is another list with the same element at first position [k][0] that had already occurred, then I'd like to remove that list and get this result: Can you suggest an algorithm to achieve this goal? <code>  L = [['14', '65', 76], ['2', '5', 6], ['7', '12', 33], ['14', '22', 46]] L = [['14', '65', 76], ['2', '5', 6], ['7', '12', 33]]",Removing duplicates from list of lists in Python
chnage python file in place," I have a large xml file (40 Gb) that I need to split into smaller chunks. I am working with limited space, so is there a way to delete lines from the original file as I write them to new files?Thanks! <code> ",Change python file in place
Python *on* the iPhone?," Is there an online Python interpreter or some such that would allow me to try out simple python code from my iPhone?Something like try ruby! (in your browser) for Python, and works with the iPhone? <code> ",Online Python interpreter for use *from* the iPhone?
Automatically determine Natual Language used by a Webiste Page given it's URL," I'm looking for a way to automatically determine the natural language used by a website page, given its URL.In Python, a function like: Which returns a language specifier (e.g. 'en' for English, 'jp' for Japanese, etc...)Summary of Results:I have a reasonable solution working in Python using code from the PyPi for oice.langdet.It does a decent job in discriminating English vs. Non-English, which is all I require at the moment. Note that you have to fetch the html using Python urllib. Also, oice.langdet is GPL license.For a more general solution using Trigrams in Python as others have suggested, see this Python Cookbook Recipe from ActiveState.The Google Natural Language Detection API works very well (if not the best I've seen). However, it is Javascript and their TOS forbids automating its use. <code>  def LanguageUsed (url): #stuff",Automatically determine the natural language of a website page given its URL
why __builtins__ both module and dict," I am using the built-in module to insert a few instances, so they can be accessed globally for debugging purposes. The problem with the __builtins__ module is that it is a module in a main script and is a dict in modules, but as my script depending on cases can be a main script or a module, I have to do this: Is there a workaround, shorter than this? More importantly, why does __builtins__ behave this way?Here is a script to see this. Create a module a.py: Create a module b.py: Now run python a.py: <code>  if isinstance(__builtins__, dict): __builtins__['g_frame'] = 'xxx'else: setattr(__builtins__, 'g_frame', 'xxx') #module-aimport bprint 'a-builtin:',type(__builtins__) #module-bprint 'b-builtin:',type(__builtins__) $ python a.py b-builtin: <type 'dict'>a-builtin: <type 'module'>",why __builtins__ is both module and dict
good or bad practice in python: import in the middle of a file," Suppose I have a relatively long module, but need an external module or method only once.Is it considered OK to import that method or module in the middle of the module?Or should imports only be in the first part of the module.Example: Please justify your answer and add links to PEPs or relevant sources <code>  import string, pythis, pythat............def func(): blah blah blah from pysomething import foo foo() etc etc etc.........",Good or bad practice in Python: import in the middle of a file
How to make the wxNotebook to expand?," (I have tagged this question as Python as well since I understand Python code so examples in Python are also welcome!).I want to create a simple window in wxWidgets:I create a main panel which I add to a formI associate a boxsizer to the main panel (splitting it in two, horizontally).I add LeftPanel to the boxsizer,I add RightPanel to the boxsizer,I create a new boxsizer (vertical)I create another boxsizer (horizontal)I create a Notebook widgetI create a Panel and put it inside the Notebook (addpage)I add the notebook to the new boxsizer (vertical one)I add the vertical sizer in the horizontal oneI associate the horizontal sizer to the RightPanelI add the Left and Right panel to the main sizer.This doesn't work...Maybe I have missed something (mental block about sizers) but what I would like to do is to expand the notebook widget without the use of the vertical sizer inside the horizontal one (it doesn't work anyway).So my question is. Assuming I want to expand the Notebook widget inside the RightPanel to take up the rest of the right side area of the form, how would I go about doing that?For those that understand Erlang, This is what I have so far: <code>  mainwindow() -> %% Create new environment X = wx:new(), %% Create the main frame MainFrame = wxFrame:new(X, -1, ""Test""), MainPanel = wxPanel:new(MainFrame, [{winid, ?wxID_ANY}]), MainSizer = wxBoxSizer:new(?wxHORIZONTAL), wxWindow:setSizer(MainPanel, MainSizer), %% Left Panel... LeftPanel = wxPanel:new(MainPanel, [{winid, ?wxID_ANY}]), LeftPanelSizer = wxBoxSizer:new(?wxVERTICAL), wxWindow:setSizer(LeftPanel, LeftPanelSizer), wxWindow:setMinSize(LeftPanel, {152, -1}), %% Right Panel RightPanel = wxPanel:new(MainPanel, [{winid, ?wxID_ANY}]), RightPanelVerticalSizer = wxBoxSizer:new(?wxVERTICAL), RightPanelHorizontalSizer = wxBoxSizer:new(?wxHORIZONTAL), wxWindow:setBackgroundColour(RightPanel, {255,0,0}), Notebook = wxNotebook:new(RightPanel, ?wxID_ANY, [{size,{-1,-1}}]), TestPanel1 = wxPanel:new(Notebook, [{size,{-1,-1}},{winid, ?wxID_ANY}]), wxNotebook:addPage(Notebook, TestPanel1, ""Testpanel!""), TestPanel2 = wxPanel:new(Notebook, [{size,{-1,-1}},{winid, ?wxID_ANY}]), wxNotebook:addPage(Notebook, TestPanel2, ""Testpanel!""), wxSizer:add(RightPanelVerticalSizer, Notebook, [{border,0},{proportion,1}, {flag,?wxEXPAND}]), wxSizer:add(RightPanelHorizontalSizer, RightPanelVerticalSizer, [{proportion,1}, {flag,?wxEXPAND}]), wxWindow:setSizer(RightPanel, RightPanelHorizontalSizer), %% Main Sizer wxSizer:add(MainSizer, LeftPanel, [{border, 2}, {flag,?wxEXPAND bor ?wxALL}]), wxSizer:add(MainSizer, RightPanel, [{border, 2}, {flag,?wxEXPAND bor ?wxTOP bor ?wxRIGHT bor ?wxBOTTOM}]), %% Connect to events wxFrame:connect(MainFrame, close_window), wxWindow:center(MainFrame), wxWindow:show(MainFrame), ...",In erlang: How do I expand wxNotebook in a panel?
On python module benchmarking for FFT," Taking speed as an issue it may be better to choose another language, but what is your library/module/implementation of choice for doing a 1D fast Fourier transform (FFT) in Python? <code> ",What is the recommended Python module for fast Fourier transforms (FFT)?
PIL Image.resize() not resizing  the picture," I have some strange problem with PIL not resizing the image. This code runs without any errors and produces me image named mugshotv2.jpg in correct folder, but it does not resize it. It does something to it, because the size of the picture drops from 120 kb to 20 kb, but the dimensions remain the same. Perhaps you can also suggest way to crop images into squares with less code. I kinda thought that Image.thumbnail does it, but what it did was that it scaled my image to 150 px by its width, leaving height 100px. <code>  from PIL import Imageimg = Image.open('foo.jpg')width, height = img.sizeratio = floor(height / width)newheight = ratio * 150img.resize((150, newheight), Image.ANTIALIAS)img.save('mugshotv2.jpg', format='JPEG')",PIL Image.resize() not resizing the picture
How can I create a list of files in the current directory and its subdirectories with a given extension (.asp)?," I'm trying to generate a text file that has a list of all files in the current directory and all of its sub-directories with the extension "".asp"". What would be the best way to do this? <code> ",How can I create a list of files in the current directory and its subdirectories with a given extension?
Convert Python dict to object," I'm searching for an elegant way to get data using attribute access on a dict with some nested dicts and lists (i.e. javascript-style object syntax).For example: Should be accessible in this way: I think, this is not possible without recursion, but what would be a nice way to get an object style for dicts? <code>  >>> d = {'a': 1, 'b': {'c': 2}, 'd': [""hi"", {'foo': ""bar""}]} >>> x = dict2obj(d)>>> x.a1>>> x.b.c2>>> x.d[1].foobar",Convert nested Python dict to object?
Convert Python dict to object?," I'm searching for an elegant way to get data using attribute access on a dict with some nested dicts and lists (i.e. javascript-style object syntax).For example: Should be accessible in this way: I think, this is not possible without recursion, but what would be a nice way to get an object style for dicts? <code>  >>> d = {'a': 1, 'b': {'c': 2}, 'd': [""hi"", {'foo': ""bar""}]} >>> x = dict2obj(d)>>> x.a1>>> x.b.c2>>> x.d[1].foobar",Convert nested Python dict to object?
"Django: foreign keys, many-to-many relations, and through"," I want to store which user invited another user to a group... but django is telling me this is ambigous and against the rules (which makes sense). groups.group: Intermediary model Group_to_Member has more than one foreign key to User, which is ambiguous and is not permitted.So how do I do this correctly? Maybe a generic relation? might work but seems a bit convoluted... Here's how I was approaching it (with unrelated bits removed) SolutionOk so I did a little combination of the answers you guys provided (Thanks!) and things I found on the internet plus my own admittedly meager python-fu: <code>  from django.contrib.auth.models import Userclass UserGroup(models.Model): members = models.ManyToManyField(User, through='Group_to_Member')class UserGroup_to_Member(models.Model): group = models.ForeignKey(UserGroup) member = models.ForeignKey(User) invited_by = models.ForeignKey(User, related_name=""group_invited_users"") from django.contrib.auth.models import Userclass UserGroup(models.Model): # notice there is no member object here ... other model data def add_member(self, **kwargs): g2m = UserGroup_to_Member(group = self, **kwargs) g2m.save() def remove_member(self, member): g2m = UserGroup_to_Member.objects.get(group=self, member=member) g2m.delete() # This is not elegant at all, help please? I'm pretty sure it isn't # as bad on the database as it looks though. def get_members(self): g2ms = UserGroup_to_Member.objects.filter(group=self) member_ids = [g2m.member.id for g2m in g2ms] members = User.objects.none() for id in member_ids: members = members | User.objects.get(id=id) return membersclass UserGroup_to_Member(models.Model): group = models.ForeignKey(UserGroup) member = models.ForeignKey(User) invited_by = models.ForeignKey(User, related_name=""group_invited_users"")","Django many-to-many relations, and through"
"Django: many-to-many relations, and through"," I want to store which user invited another user to a group... but django is telling me this is ambigous and against the rules (which makes sense). groups.group: Intermediary model Group_to_Member has more than one foreign key to User, which is ambiguous and is not permitted.So how do I do this correctly? Maybe a generic relation? might work but seems a bit convoluted... Here's how I was approaching it (with unrelated bits removed) SolutionOk so I did a little combination of the answers you guys provided (Thanks!) and things I found on the internet plus my own admittedly meager python-fu: <code>  from django.contrib.auth.models import Userclass UserGroup(models.Model): members = models.ManyToManyField(User, through='Group_to_Member')class UserGroup_to_Member(models.Model): group = models.ForeignKey(UserGroup) member = models.ForeignKey(User) invited_by = models.ForeignKey(User, related_name=""group_invited_users"") from django.contrib.auth.models import Userclass UserGroup(models.Model): # notice there is no member object here ... other model data def add_member(self, **kwargs): g2m = UserGroup_to_Member(group = self, **kwargs) g2m.save() def remove_member(self, member): g2m = UserGroup_to_Member.objects.get(group=self, member=member) g2m.delete() # This is not elegant at all, help please? I'm pretty sure it isn't # as bad on the database as it looks though. def get_members(self): g2ms = UserGroup_to_Member.objects.filter(group=self) member_ids = [g2m.member.id for g2m in g2ms] members = User.objects.none() for id in member_ids: members = members | User.objects.get(id=id) return membersclass UserGroup_to_Member(models.Model): group = models.ForeignKey(UserGroup) member = models.ForeignKey(User) invited_by = models.ForeignKey(User, related_name=""group_invited_users"")","Django many-to-many relations, and through"
Raise exception vs. return None in Python functions," What's better practice in a user-defined function in Python: raise an exception or return None? For example, I have a function that finds the most recent file in a folder. Another option is leave the exception and handle it in the caller code, but I figure it's more clear to deal with a FileNotFoundError than an IndexError. Or is it bad form to re-raise an exception with a different name? <code>  def latestpdf(folder): # list the files and sort them try: latest = files[-1] except IndexError: # Folder is empty. return None # One possibility raise FileNotFoundError() # Alternative else: return somefunc(latest) # In my case, somefunc parses the filename",Raise exception vs. return None in functions?
How to unit test django views?  Or maybe and MVT question.," I want to begin integrating unit tests into my Django projects and I've discovered unit testing a view to be tricky because of the way Django implements views with functions. For example, each function is a view/page in Django if the function has a URL.How do I unit test Django views? <code> ",How do I unit test Django Views?
Python: List all base classes in a hierarchy," Given a class Foo (whether it is a new-style class or not), how do you generate all the base classes - anywhere in the inheritance hierarchy - it issubclass of? <code> ",List all base classes in a hierarchy of given class?
Python: List all base classes in a hierarchy of given class," Given a class Foo (whether it is a new-style class or not), how do you generate all the base classes - anywhere in the inheritance hierarchy - it issubclass of? <code> ",List all base classes in a hierarchy of given class?
manipulating pipe buffer size in C or python," I have a general question about popen (and all related functions), applicable to all operating systems, when I write a python script or some c code and run the resulting executable from the console (win or linux), i can immediately see the output from the process. However, if I run the same executable as a forked process with its stdout redirected into a pipe, the output buffers somewhere, usually up to 4096 bytes before it is written to the pipe where the parent process can read it.The following python script will generate output in chunks of 1024 bytes The following python script will execute the previous script and read the output as soon as it comes to the pipe, byte by byte Adjust the path for your operating system. When run in this configuration, the output will not appear in chunks of 1024 but chunks of 4096, despite the buffer size of the popen command being set to 0 (which is the default anyway). Can anyone tell me how to change this behaviour?, is there any way I can force the operating system to treat the output from the forked process in the same way as when it is run from the console?, ie, just feed the data through without buffering? <code>  import os, sys, timeif __name__ == ""__main__"": dye = '@'*1024 for i in range (0,8): print dye time.sleep(1) import os, sys, subprocess, time, threadif __name__ == ""__main__"": execArgs = [""c:\\python25\\python.exe"", ""C:\\Scripts\\PythonScratch\\byte_stream.py""] p = subprocess.Popen(execArgs, bufsize=0, stdout=subprocess.PIPE) while p.returncode == None: data = p.stdout.read(1) sys.stdout.write(data) p.poll()",Bypassing buffering of subprocess output with popen in C or Python
Python normal arugments vs. keyword arguments," How are ""keyword arguments"" different from regular arguments? Can't all arguments be passed as name=value instead of using positional syntax? <code> ",Normal arguments vs. keyword arguments
Python normal arguments vs. keyword arguments," How are ""keyword arguments"" different from regular arguments? Can't all arguments be passed as name=value instead of using positional syntax? <code> ",Normal arguments vs. keyword arguments
Python: Does python have an equivalent ot 'switch' in PHP?, I am trying to check each index in an 8 digit binary string. If it is '0' then it is 'OFF' otherwise it is 'ON'.Is there a more concise way to write this code with a switch-like feature? <code> ,Does Python have an equivalent to 'switch'?
Python: Does python have an equivalent to 'switch'?, I am trying to check each index in an 8 digit binary string. If it is '0' then it is 'OFF' otherwise it is 'ON'.Is there a more concise way to write this code with a switch-like feature? <code> ,Does Python have an equivalent to 'switch'?
do properties work on django model fields?," I think the best way to ask this question is with some code... can I do this: or do I have to do it like this: note: you can keep the column name as 'foo' in the database by passing a db_column to the model field. This is very helpful when you are working on an existing system and you don't want to have to do db migrations for no reason <code>  class MyModel(models.Model): foo = models.CharField(max_length = 20) bar = models.CharField(max_length = 20) def get_foo(self): if self.bar: return self.bar else: return self.foo def set_foo(self, input): self.foo = input foo = property(get_foo, set_foo) class MyModel(models.Model): _foo = models.CharField(max_length = 20, db_column='foo') bar = models.CharField(max_length = 20) def get_foo(self): if self.bar: return self.bar else: return self._foo def set_foo(self, input): self._foo = input foo = property(get_foo, set_foo)",Do properties work on Django model fields?
How to I parse an HTTP date-string in Python?," Is there an easy way to parse HTTP date-strings in Python? According to the standard, there are several ways to format HTTP date strings; the method should be able to handle this.In other words, I want to convert a string like ""Wed, 23 Sep 2009 22:15:29 GMT"" to a python time-structure. <code> ",How do I parse an HTTP date-string in Python?
Doing python like strip() in C, I started on a little toy project in C lately and have been scratching my head over the best way to mimic the strip() functionality that is part of the python string objects. Reading around for fscanf or sscanf says that the string is processed upto the first whitespace that is encountered.fgets doesn't help either as I still have newlines sticking around.I did try a strchr() to search for a whitespace and setting the returned pointer to '\0' explicitly but that doesn't seem to work. <code> ,Mimic Python's strip() function in C
Python Pypi: what is your process for releasing packages for different Python versions?," I've got several eggs I maintain on Pypi but up until now I've always focused on Python 2.5x.I'd like to release my eggs under both Python 2.5 & Python 2.6 in an automated fashion i.e.running tests generating docpreparing eggsuploading to PypiHow do you guys achieve this?A related question: how do I tag an egg to be ""version independent"" ? works under all version of Python? <code> ",Python Pypi: what is your process for releasing packages for different Python versions? (Linux)
Python: List Sorting with Multiple Attributes and Mixed Order," I have to sort a list with multiple attributes. I can do that in ascending order for ALL attributes easily with but the problem is, that I have to use mixed configurations for ascending/descending... I have to ""imitate"" a bit the SQL Order By where you can do something like name ASC, year DESC.Is there a way to do this easily in Python without having to implement a custom compare function? <code>  L.sort(key=operator.attrgetter(attribute))....",List sorting with multiple attributes and mixed order
About python's built in sort() method, What algorithm is the built in sort() method in Python using? Is it possible to have a look at the code for that method? <code> ,About Python's built in sort() method
Python: static instance variable?," I use @property to ensure that changes to an objects instance variables are wrapped by methods where I need to. What about when an instance has an variable that logically should not be changed? Eg, if I'm making a class for a Process, each Process instance should have a PID attribute that will frequently be accessed but should not be changed. What's the most Pythonic way to handle someone attempting to modify that instance variable? Simply trust the user not to try and changesomething they shouldn't? Use property but raise anexception if the instance variable ischanged? Something else? <code> ",Constant instance variables?
Python: Replace values in list," I have a list where I want to replace values with None where condition() returns True. For example, if condition checks bool(item%2) should return: What is the most efficient way to do this? <code>  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] [None, 1, None, 3, None, 5, None, 7, None, 9, None]",Replace values in list using Python
Simple AtomPub server daemon or library," What simple AtomPub server libraries with file- or DB-based backends can you recommend? Unix-style servers that ""do one thing, do it well"" are especially welcome.Maybe even libraries in Python? <code> ",Simple AtomPub server library
python html generator," I am looking for an easily implemented HTML generator for Python. I found HTML.py, but there is no way to add CSS elements (id, class) for table. <code> ",Python HTML generator
Sorting heterogeneous a list of objects in Python," I have some custom objects and dictionaries that I want to sort. I want to sort both the objects the dictionaries together. I want to sort the objects by an attribute and the dictionaries by a key. How do I sort this list using the object's name attribute and the dictionary's 'name' key? <code>  object.name = 'Jack'd = {'name':'Jill'}sort_me =[object, d]",Sorting a heterogeneous list of objects in Python
How to capitalize the first letter of each word in a string (Python)?, ...do something here...s should be: What's the easiest way to do this? <code>  s = 'the brown fox' 'The Brown Fox',How can I capitalize the first letter of each word in a string?
How to capitalize the first letter of each word in a string?, ...do something here...s should be: What's the easiest way to do this? <code>  s = 'the brown fox' 'The Brown Fox',How can I capitalize the first letter of each word in a string?
"How to assign a variable in IF, and then return it. (Python)"," This works: This does NOT work: Why doesn't the 2nd one work!? I want a 1-liner. Except, the 1st one will call the function TWICE.How to make it 1 liner, without calling the function twice? <code>  def isBig(x): if x > 4: return 'apple' else: return 'orange' if isBig(y): return isBig(y) if fruit = isBig(y): return fruit","How to assign a variable in an IF condition, and then return it?"
"How to assign a variable in IF, and then return it?"," This works: This does NOT work: Why doesn't the 2nd one work!? I want a 1-liner. Except, the 1st one will call the function TWICE.How to make it 1 liner, without calling the function twice? <code>  def isBig(x): if x > 4: return 'apple' else: return 'orange' if isBig(y): return isBig(y) if fruit = isBig(y): return fruit","How to assign a variable in an IF condition, and then return it?"
Please help install matplotlib. It won't work! (Python)," I downloaded the source and untarred it. And below are the errors I get. By the way, Numpy is installed. <code>  sudo python setup.py install src/_image.cpp:5:17: error: png.h: No such file or directorysrc/_image.cpp: In member function 'Py::Object Image::write_png(const Py::Tuple&)':src/_image.cpp:646: error: 'png_structp' was not declared in this scopesrc/_image.cpp:646: error: expected `;' before 'png_ptr'src/_image.cpp:647: error: 'png_infop' was not declared in this scopesrc/_image.cpp:647: error: expected `;' before 'info_ptr'src/_image.cpp:648: error: aggregate 'png_color_8_struct sig_bit' has incomplete type and cannot be definedsrc/_image.cpp:649: error: 'png_uint_32' was not declared in this scopesrc/_image.cpp:649: error: expected `;' before 'row'src/_image.cpp:652: error: 'png_bytep' was not declared in this scopesrc/_image.cpp:652: error: 'row_pointers' was not declared in this scopesrc/_image.cpp:652: error: expected type-specifier before 'png_bytep'src/_image.cpp:652: error: expected `;' before 'png_bytep'src/_image.cpp:654: error: 'row' was not declared in this scopesrc/_image.cpp:660: error: type '<type error>' argument given to 'delete', expected pointersrc/_image.cpp:665: error: 'png_ptr' was not declared in this scopesrc/_image.cpp:665: error: 'PNG_LIBPNG_VER_STRING' was not declared in this scopesrc/_image.cpp:665: error: 'png_create_write_struct' was not declared in this scopesrc/_image.cpp:669: error: type '<type error>' argument given to 'delete', expected pointersrc/_image.cpp:673: error: 'info_ptr' was not declared in this scopesrc/_image.cpp:673: error: 'png_create_info_struct' was not declared in this scopesrc/_image.cpp:677: error: 'png_destroy_write_struct' was not declared in this scopesrc/_image.cpp:678: error: type '<type error>' argument given to 'delete', expected pointersrc/_image.cpp:685: error: 'png_destroy_write_struct' was not declared in this scopesrc/_image.cpp:686: error: type '<type error>' argument given to 'delete', expected pointersrc/_image.cpp:690: error: 'png_init_io' was not declared in this scopesrc/_image.cpp:693: error: 'PNG_COLOR_TYPE_RGB_ALPHA' was not declared in this scopesrc/_image.cpp:693: error: 'PNG_INTERLACE_NONE' was not declared in this scopesrc/_image.cpp:694: error: 'PNG_COMPRESSION_TYPE_BASE' was not declared in this scopesrc/_image.cpp:694: error: 'PNG_FILTER_TYPE_BASE' was not declared in this scopesrc/_image.cpp:694: error: 'png_set_IHDR' was not declared in this scopesrc/_image.cpp:703: error: 'png_set_sBIT' was not declared in this scopesrc/_image.cpp:705: error: 'png_write_info' was not declared in this scopesrc/_image.cpp:706: error: 'png_write_image' was not declared in this scopesrc/_image.cpp:707: error: 'png_write_end' was not declared in this scopesrc/_image.cpp:708: error: 'png_destroy_write_struct' was not declared in this scopesrc/_image.cpp:711: error: type '<type error>' argument given to 'delete', expected pointersrc/_image.cpp: In member function 'Py::Object _image_module::readpng(const Py::Tuple&)':src/_image.cpp:860: error: 'png_byte' was not declared in this scopesrc/_image.cpp:860: error: expected `;' before 'header'src/_image.cpp:866: error: 'header' was not declared in this scopesrc/_image.cpp:868: error: 'header' was not declared in this scopesrc/_image.cpp:868: error: 'png_sig_cmp' was not declared in this scopesrc/_image.cpp:873: error: 'png_structp' was not declared in this scopesrc/_image.cpp:873: error: expected `;' before 'png_ptr'src/_image.cpp:875: error: 'png_ptr' was not declared in this scopesrc/_image.cpp:878: error: 'png_infop' was not declared in this scopesrc/_image.cpp:878: error: expected `;' before 'info_ptr'src/_image.cpp:879: error: 'info_ptr' was not declared in this scopesrc/_image.cpp:882: error: 'png_ptr' was not declared in this scopesrc/_image.cpp:882: error: 'png_jmpbuf' was not declared in this scopesrc/_image.cpp:885: error: 'png_ptr' was not declared in this scopesrc/_image.cpp:885: error: 'png_init_io' was not declared in this scopesrc/_image.cpp:886: error: 'png_set_sig_bytes' was not declared in this scopesrc/_image.cpp:888: error: 'info_ptr' was not declared in this scopesrc/_image.cpp:888: error: 'png_read_info' was not declared in this scopesrc/_image.cpp:890: error: 'png_uint_32' was not declared in this scopesrc/_image.cpp:890: error: expected `;' before 'width'src/_image.cpp:891: error: expected `;' before 'height'src/_image.cpp:894: error: 'PNG_COLOR_TYPE_GRAY' was not declared in this scopesrc/_image.cpp:895: error: 'PNG_COLOR_TYPE_GRAY_ALPHA' was not declared in this scopesrc/_image.cpp:896: error: 'png_set_gray_to_rgb' was not declared in this scopesrc/_image.cpp:897: error: 'PNG_COLOR_TYPE_PALETTE' was not declared in this scopesrc/_image.cpp:898: error: 'png_set_palette_to_rgb' was not declared in this scopesrc/_image.cpp:902: error: 'png_set_strip_16' was not declared in this scopesrc/_image.cpp:905: error: 'png_set_interlace_handling' was not declared in this scopesrc/_image.cpp:906: error: 'png_read_update_info' was not declared in this scopesrc/_image.cpp:908: error: 'PNG_COLOR_TYPE_RGBA' was not declared in this scopesrc/_image.cpp:909: error: 'PNG_COLOR_TYPE_RGB' was not declared in this scopesrc/_image.cpp:915: error: 'png_jmpbuf' was not declared in this scopesrc/_image.cpp:918: error: 'png_bytep' was not declared in this scopesrc/_image.cpp:918: error: 'row_pointers' was not declared in this scopesrc/_image.cpp:918: error: expected type-specifier before 'png_bytep'src/_image.cpp:918: error: expected `;' before 'png_bytep'src/_image.cpp:919: error: expected `;' before 'row'src/_image.cpp:921: error: 'row' was not declared in this scopesrc/_image.cpp:921: error: 'height' was not declared in this scopesrc/_image.cpp:922: error: expected type-specifier before 'png_byte'src/_image.cpp:922: error: expected `;' before 'png_byte'src/_image.cpp:924: error: 'png_read_image' was not declared in this scopesrc/_image.cpp:929: error: 'height' was not declared in this scopesrc/_image.cpp:930: error: 'width' was not declared in this scopesrc/_image.cpp:936: error: expected `;' before 'y'src/_image.cpp:936: error: 'y' was not declared in this scopesrc/_image.cpp:938: error: expected `;' before 'x'src/_image.cpp:938: error: 'x' was not declared in this scopesrc/_image.cpp:940: error: 'ptr' was not declared in this scopesrc/_image.cpp:951: error: 'png_read_end' was not declared in this scopesrc/_image.cpp:952: error: 'png_infopp_NULL' was not declared in this scopesrc/_image.cpp:952: error: 'png_destroy_read_struct' was not declared in this scopesrc/_image.cpp:956: error: type '<type error>' argument given to 'delete', expected pointererror: command 'gcc' failed with exit status 1",How to install matplotlib without gcc errors?
Python objects that monitors changes in objects," I want a Python object that will monitor whether other objects have changed since the last time they were checked in, probably by storing their hash and comparing. It should behave sort of like this: Do you know of anything like that? <code>  >>> library = Library()>>> library.is_changed(object1)False>>> object1.change_somehow()>>> library.is_changed(object1)True>>> library.is_changed(object1)False",Python object that monitors changes in objects
"Is there a way to perform ""if"" in python's lambda"," In Python 2.6, I want to do: This clearly isn't the syntax. Is it possible to perform an if in lambda and if so how to do it? <code>  f = lambda x: if x==2 print x else raise Exception()f(2) #should print ""2""f(3) #should throw an exception","Is there a way to perform ""if"" in python's lambda?"
python: read two variables in a single line," I am familiar with the input() function, to read a single variable from user input. Is there a similar easy way to read two variables? I'm looking for the equivalent of: One way I am able to achieve this is to use raw_input() and then split what was entered. Is there a more elegant way?This is not for live use. Just for learning.. <code>  scanf(""%d%d"", &i, &j); // accepts ""10 20\n""",Read two variables in a single line with Python
Is there any simple way to benchmark python script?," Usually I use shell command time. My purpose is to test if data is small, medium, large or very large set, how much time and memory usage will be.Any tools for Linux or just Python to do this? <code> ",Is there any simple way to benchmark Python script?
"python queue get(),task_done() question"," My consumer side of the queue: Questions:Does task_done() effectively pops m off the queue and release whatever locks the consumer has on the queue?I need to use m during the rest of the program. Is it safe, or do I need to copy it before I call task_done() or is m usable after task_done()?be happy <code>  m = queue.get()queue.task_done()<rest of the program>",Python Queue get()/task_done() issue
python Queue get()/task_done() issue," My consumer side of the queue: Questions:Does task_done() effectively pops m off the queue and release whatever locks the consumer has on the queue?I need to use m during the rest of the program. Is it safe, or do I need to copy it before I call task_done() or is m usable after task_done()?be happy <code>  m = queue.get()queue.task_done()<rest of the program>",Python Queue get()/task_done() issue
Using south to refactor a Django model with inheritence," I was wondering if the following migration is possible with Django south and still retain data.Before:I currently have two apps, one called tv, one called movies, each with a VideoFile model (simplified here):tv/models.py: movies/models.py: After:Because the two videofile objects are so similar I want to get rid of duplication and create a new model in a separate app called media that contains a generic VideoFile class and use inheritance to extend it:media/models.py: tv/models.py: movies/models.py: So my question is, how can I accomplish this with django-south and still maintain existing data?All three these apps are already managed by south migrations and according to the south documentation it is bad practice to combine a schema and data migration and they recommend it should be done in a few steps.I think it could be done using separate migrations like this (assuming media.VideoFile is already created)Schema migration to rename all fields in tv.VideoFile and movies.VideoFile that will move to the new media.VideoFile model, maybe to something like old_name, old_size, etcSchema migration to tv.VideoFile and movies.VideoFile to inherit from media.VideoFileData migration to copy old_name to name, old_size to size, etcScheme migration to remove old_ fieldsBefore I go through all that work, do you think that will work? Is there a better way?If you're interested, the project is hosted here: http://code.google.com/p/medianav/ <code>  class VideoFile(models.Model): show = models.ForeignKey(Show, blank=True, null=True) name = models.CharField(max_length=1024, blank=True) size = models.IntegerField(blank=True, null=True) ctime = models.DateTimeField(blank=True, null=True) class VideoFile(models.Model): movie = models.ForeignKey(Movie, blank=True, null=True) name = models.CharField(max_length=1024, blank=True) size = models.IntegerField(blank=True, null=True) ctime = models.DateTimeField(blank=True, null=True) class VideoFile(models.Model): name = models.CharField(max_length=1024, blank=True) size = models.IntegerField(blank=True, null=True) ctime = models.DateTimeField(blank=True, null=True) class VideoFile(media.models.VideoFile): show = models.ForeignKey(Show, blank=True, null=True) class VideoFile(media.models.VideoFile): movie = models.ForeignKey(Movie, blank=True, null=True)",Using south to refactor a Django model with inheritance
How to stop Python from referring to same list object when it is used in many places?," Have a look at this Python code: Notice how modifying one element of c modifies that everywhere. That is, if 99 is appended to c[0][0], it is also appended to c[1][1]. I am guessing this is because Python is cleverly referring to the same object for c[0][0] and c[1][1]. (That is their id() is the same.)Question: Is there something that can be done to c so that its list elements can be safely locally modified? Above is just an example, my real problem has a list much more complicated, but having a similar problem.(Sorry for the poorly formed question above. Python gurus please feel free to modify the question or tags to better express this query.) <code>  a = [1, 2, 3]b = [4, 5, 6]c = [[a, b], [b, a]] # [[[1, 2, 3], [4, 5, 6]], [[4, 5, 6], [1, 2, 3]]]c[0][0].append(99) # [[[1, 2, 3, 99], [4, 5, 6]], [[4, 5, 6], [1, 2, 3, 99]]]",How to make a completely unshared copy of a complicated list? (Deep copy is not enough)
Python: How to make a completely unshared copy of a complicated list? (Deep copy is not enough)," Have a look at this Python code: Notice how modifying one element of c modifies that everywhere. That is, if 99 is appended to c[0][0], it is also appended to c[1][1]. I am guessing this is because Python is cleverly referring to the same object for c[0][0] and c[1][1]. (That is their id() is the same.)Question: Is there something that can be done to c so that its list elements can be safely locally modified? Above is just an example, my real problem has a list much more complicated, but having a similar problem.(Sorry for the poorly formed question above. Python gurus please feel free to modify the question or tags to better express this query.) <code>  a = [1, 2, 3]b = [4, 5, 6]c = [[a, b], [b, a]] # [[[1, 2, 3], [4, 5, 6]], [[4, 5, 6], [1, 2, 3]]]c[0][0].append(99) # [[[1, 2, 3, 99], [4, 5, 6]], [[4, 5, 6], [1, 2, 3, 99]]]",How to make a completely unshared copy of a complicated list? (Deep copy is not enough)
How to make a python script run like a service or daemon in linux," I have written a Python script that checks a certain e-mail address and passes new e-mails to an external program. How can I get this script to execute 24/7, such as turning it into daemon or service in Linux. Would I also need a loop that never ends in the program, or can it be done by just having the code re executed multiple times? <code> ",How to make a Python script run like a service or daemon in Linux
How to launch and run extenal script in background?," I tried these two methods: Both approaches need to wait until test.py finishes which blocks main process. I know ""nohup"" can do the job. Is there a Python way to launch test.py or any other shell scripts and leave it running in background?Suppose test.py is like this: Both os.system() or subprocess.Popen() will block main program until 1000000 lines of output displayed. What I want is let test.py runs silently and display main program output only. Main program may quie while test.py is still running. <code>  os.system(""python test.py"")subprocess.Popen(""python test.py"", shell=True) for i in range(0, 1000000): print i",How to launch and run external script in background?
Adding docstrings to namedtuples in Python?," Is it possible to add a documentation string to a namedtuple in an easy manner?I tried but that doesn't cut it. Is it possible to do in some other way? <code>  from collections import namedtuplePoint = namedtuple(""Point"", [""x"", ""y""])""""""A point in 2D space""""""# Yet another test""""""A(nother) point in 2D space""""""Point2 = namedtuple(""Point2"", [""x"", ""y""])print Point.__doc__ # -> ""Point(x, y)""print Point2.__doc__ # -> ""Point2(x, y)""",Adding docstrings to namedtuples?
How to use C++ classes with ctype?, I'm just getting started with ctypes and would like to use a C++ class that I have exported in a dll file from within python using ctypes.So lets say my C++ code looks something like this: I would know create a .dll file that contains this class and then load the .dll file in python using ctypes.Now how would I create an Object of type MyClass and call its test function? Is that even possible with ctypes? Alternatively I would consider using SWIG or Boost.Python but ctypes seems like the easiest option for small projects. <code>  class MyClass { public: int test();...,How to use C++ classes with ctypes?
What is the pythonic way to detect the last element in a python 'for' loop?," I'd like to know the best way (more compact and ""pythonic"" way) to do a special treatment for the last element in a for loop. There is a piece of code that should be called only between elements, being suppressed in the last one.Here is how I currently do it: Is there any better way?Note: I don't want to make it with hacks such as using reduce. ;) <code>  for i, data in enumerate(data_list): code_that_is_done_for_every_element if i != len(data_list) - 1: code_that_is_done_between_elements",What is the pythonic way to detect the last element in a 'for' loop?
"Python, List running processes 64Bit Windows"," I amm writing a little python script that will grab information from VMs of Windows that I am running.At the moment I can list the processes on a 32bit XP machine with the following method:http://code.activestate.com/recipes/305279/Is it possible to somehow detect the version of windows running and excute a different method for getting the processes on a 64bit machine, I am trying to get the processes from a 64Bit Vista and 64bit Windows 7.Any ideas? <code> ",List running processes on 64-bit Windows
problem with the new lines when I use toprettyxml()," I'm currently using the toprettyxml() function of the xml.dom module in a Python script and I'm having some trouble with the newlines.If don't use the newl parameter or if I use toprettyxml(newl='\n') it displays several newlines instead of only one.For instance displayed: Does anyone know where the problem comes from and how I can use it? FYI I'm using Python 2.6.1 <code>  f = open(filename, 'w')f.write(dom1.toprettyxml(encoding='UTF-8'))f.close() <params> <param name=""Level"" value=""#LEVEL#""/> <param name=""Code"" value=""281""/></params>",Problem with newlines when I use toprettyxml()
proper usage of super built-in method in python," I get some error that I can't figure out. Any clue what is wrong with my sample code? I got the sample test code from help of 'super' built-in method. Here is the error: FYI, here is the help(super) from python itself: <code>  class B: def meth(self, arg): print argclass C(B): def meth(self, arg): super(C, self).meth(arg)print C().meth(1) Traceback (most recent call last): File ""./test.py"", line 10, in ? print C().meth(1) File ""./test.py"", line 8, in meth super(C, self).meth(arg)TypeError: super() argument 1 must be type, not classobj Help on class super in module __builtin__:class super(object) | super(type) -> unbound super object | super(type, obj) -> bound super object; requires isinstance(obj, type) | super(type, type2) -> bound super object; requires issubclass(type2, type) | Typical use to call a cooperative superclass method: | class C(B): | def meth(self, arg): | super(C, self).meth(arg) |","super() fails with error: TypeError ""argument 1 must be type, not classobj"" when parent does not inherit from object"
"super() fails with error: TypeError ""argument 1 must be type, not classobj"""," I get some error that I can't figure out. Any clue what is wrong with my sample code? I got the sample test code from help of 'super' built-in method. Here is the error: FYI, here is the help(super) from python itself: <code>  class B: def meth(self, arg): print argclass C(B): def meth(self, arg): super(C, self).meth(arg)print C().meth(1) Traceback (most recent call last): File ""./test.py"", line 10, in ? print C().meth(1) File ""./test.py"", line 8, in meth super(C, self).meth(arg)TypeError: super() argument 1 must be type, not classobj Help on class super in module __builtin__:class super(object) | super(type) -> unbound super object | super(type, obj) -> bound super object; requires isinstance(obj, type) | super(type, type2) -> bound super object; requires issubclass(type2, type) | Typical use to call a cooperative superclass method: | class C(B): | def meth(self, arg): | super(C, self).meth(arg) |","super() fails with error: TypeError ""argument 1 must be type, not classobj"" when parent does not inherit from object"
Python: Split unicode string on whitespace," I need to take a string, and shorten it to 140 characters.Currently I am doing: So this works great for English, and English like strings, but fails for a Chinese string because tweet.split() just returns one array: How should I do this so it handles I18N? Does this make sense in all languages?I'm on python 2.5.4 if that matters. <code>  if len(tweet) > 140: tweet = re.sub(r""\s+"", "" "", tweet) #normalize space footer = "" "" + utils.shorten_urls(post['url']) avail = 140 - len(footer) words = tweet.split() result = """" for word in words: word += "" "" if len(word) > avail: break result += word avail -= len(word) tweet = (result + footer).strip() assert len(tweet) <= 140 >>> s = u""104230"">>> su'\u7b80\u8baf\uff1a\u65b0\u83ef\u793e\u5831\u9053\uff0c\u7f8e\u570b\u7e3d\u7d71\u5967\u5df4\u99ac\u4e58\u5750\u7684\u300c\u7a7a\u8ecd\u4e00\u865f\u300d\u5c08\u6a5f\u665a\u4e0a10\u664242\u5206\u9032\u5165\u4e0a\u6d77\u7a7a\u57df\uff0c\u9810\u8a08\u7d0430\u5206\u9418\u5f8c\u62b5\u9054\u6d66\u6771\u570b\u969b\u6a5f\u5834\uff0c\u958b\u5c55\u4ed6\u4e0a\u4efb\u5f8c\u9996\u6b21\u8a2a\u83ef\u4e4b\u65c5\u3002'>>> s.split()[u'\u7b80\u8baf\uff1a\u65b0\u83ef\u793e\u5831\u9053\uff0c\u7f8e\u570b\u7e3d\u7d71\u5967\u5df4\u99ac\u4e58\u5750\u7684\u300c\u7a7a\u8ecd\u4e00\u865f\u300d\u5c08\u6a5f\u665a\u4e0a10\u664242\u5206\u9032\u5165\u4e0a\u6d77\u7a7a\u57df\uff0c\u9810\u8a08\u7d0430\u5206\u9418\u5f8c\u62b5\u9054\u6d66\u6771\u570b\u969b\u6a5f\u5834\uff0c\u958b\u5c55\u4ed6\u4e0a\u4efb\u5f8c\u9996\u6b21\u8a2a\u83ef\u4e4b\u65c5\u3002']",Python: Split unicode string on word boundaries
Python:: Turn string into operator," How can I turn a string such as ""+"" into the operator plus? <code> ",Turn string into operator
Can SQLAlchemy update the table structure ?," I am working on my first pylons + SQLAlchemy app (I'm new to both).As I change my mind on the table structure, I wish there was a similar function to metadata.create_all(), that checks if there are new columns definitions and create them in the database.Does such a function exist ? <code> ",Can SQLAlchemy update the table structure?
How to detect whether two files are indentical in Python," Is making system call to ""md5sum file1"" and ""md5sum file2"" and compare two return values enough in this case? <code> ",How to detect whether two files are identical in Python
How do I compile a shared library using Python's distutils?," I need to compile ICU using it's own build mechanism. Therefore the question:How can I run a Makefile from setup.py? Obviously, I only want it to run during the build process, not while installing. <code> ",How can I run a Makefile in setup.py?
How to do additional work in setup.py?," I need to compile ICU using it's own build mechanism. Therefore the question:How can I run a Makefile from setup.py? Obviously, I only want it to run during the build process, not while installing. <code> ",How can I run a Makefile in setup.py?
Swaping 1 with 0 and 0 with 1 in a pythonic way?," In some part of my Python program I have a val variable that can be 1 or 0. If it's 1 I must change to 0, if it's 0 I must change to 1.How do you do it in a Pythonic way? it's too long!I did: So I can use it: Other ideas? <code>  if val == 1: val = 0elif val == 0: val = 1 swap = {0: 1, 1:0} swap[val]",Swapping 1 with 0 and 0 with 1 in a Pythonic way
Swapping 1 with 0 and 0 with 1 in a pythonic way?," In some part of my Python program I have a val variable that can be 1 or 0. If it's 1 I must change to 0, if it's 0 I must change to 1.How do you do it in a Pythonic way? it's too long!I did: So I can use it: Other ideas? <code>  if val == 1: val = 0elif val == 0: val = 1 swap = {0: 1, 1:0} swap[val]",Swapping 1 with 0 and 0 with 1 in a Pythonic way
Python Decorator Problem with Docstrings," I have a problem using docstrings with decorators. Given the following example: Now the help doesn't show me the docstring of foo as expected, it shows: Without the decorator, the help is correct: I know, that the function foo is wrapped by the decorator, and so the function object is not the function foo any more. But what is a nice solution to get the docstring (and the help) as expected? <code>  def decorator(f): def _decorator(): print 'decorator active' f() return _decorator@decoratordef foo(): '''the magic foo function''' print 'this is function foo'help(foo) Help on function _decorator in module __main__:_decorator() Help on function foo in module __main__:foo() the magic foo function",Python decorator handling docstrings
is there a Java equivalent of Python's defaultdict?," In Python, the defaultdict class provides a convenient way to create a mapping from key -> [list of values], in the following example, Is there an equivalent to this in Java? <code>  from collections import defaultdictd = defaultdict(list)d[1].append(2)d[1].append(3)# d is now {1: [2, 3]}",Is there a Java equivalent of Python's defaultdict?
Python: parsing date with timezone from an email," I am trying to retrieve date from an email. At first it's easy: and I receive: But I need a nice datetime object, so I use: which raises ValueError, since %Z isn't format for +0100. But I can't find proper format for timezone in the documentation, there is only this %Z for zone. Can someone help me on that? <code>  message = email.parser.Parser().parse(file)date = message['Date']print date 'Mon, 16 Nov 2009 13:32:02 +0100' datetime.strptime('Mon, 16 Nov 2009 13:32:02 +0100', '%a, %d %b %Y %H:%M:%S %Z')",Parsing date with timezone from an email?
What is the fool proof way to convert some string (utf-8 or else) to a simple asci string in python," Inside my python scrip, I get some string back from a function which I didn't write. The encoding of it varies. I need to convert it to ascii format. Is there some fool-proof way of doing this? I don't mind replacing the non-ascii chars with blanks or something else... <code> ",What is the fool proof way to convert some string (utf-8 or else) to a simple ASCII string in python
What is the default content-type?," According to this answer: urllib2 read to UnicodeI have to get the content-type in order to change to Unicode. However, some websites don't have a ""charset"".For example, the ['content-type'] for this page is ""text/html"". I can't convert it to Unicode. Is there a default ""encoding"" (English, of course)...so that if nothing is found, I can just use that? <code>  encoding=urlResponse.headers['content-type'].split('charset=')[-1]htmlSource = unicode(htmlSource, encoding)TypeError: 'int' object is not callable",What is the default content-type/charset?
Using try vs if in python," Is there a rationale to decide which one of try or if constructs to use, when testing variable to have a value?For example, there is a function that returns either a list or doesn't return a value. I want to check result before processing it. Which of the following would be more preferable and why? or Related discussion:Checking for member existence in Python <code>  result = function();if (result): for r in result: #process items result = function();try: for r in result: # Process itemsexcept TypeError: pass;",Using 'try' vs. 'if' in Python
How can I correct corrupted $PYTHONPATH?," When trying to launch Mercurial(hg) after a restart in my Ubuntu 9.10 Linux Box I got following message: Mysteriously other Python programs don't find their modules, including django-admin, bzr, BUT surprisingly the Python interpreter itself is launching.Here you can find my current sys.path: Does anyone know how to resolve this issue?I know this is no programming question in specific, but it disallows me to program, so I beg your comprehension!Thanks in advance. <code>  abort: couldn't find mercurial libraries in [/usr/bin /usr/local/lib/python2.6/dist-packages/vipy-0.4-py2.6.egg /usr/local/lib/python2.6/dist-packages/nose-0.11.1-py2.6.egg /usr/local/lib/python2.6/dist-packages/rope-0.9.2-py2.6.egg /usr/local/lib/python2.6/dist-packages/Sphinx-0.6.3-py2.6.egg /usr/local/lib/python2.6/dist-packages/django_html-0.0.1-py2.6.egg /usr/local/lib/python2.6/dist-packages/html5lib-0.11.1-py2.6.egg /home/kenny /home/kenny/Projects/soclone-read-only /home/kenny/python/Django /home/kenny/python/pysmell /home/kenny/python/Django/ropemode /home/kenny/python/Django/rope /home/kenny/python/lib /usr/lib/python2.6 /usr/lib/python2.6/plat-linux2 /usr/lib/python2.6/lib-tk /usr/lib/python2.6/lib-old /usr/lib/python2.6/lib-dynload /usr/local/lib/python2.6/dist-packages] (check your install and PYTHONPATH) ['', '/usr/local/lib/python2.6/dist-packages/vipy-0.4-py2.6.egg', '/usr/local/lib/python2.6/dist-packages/nose-0.11.1-py2.6.egg', '/usr/local/lib/python2.6/dist-packages/rope-0.9.2-py2.6.egg', '/usr/local/lib/python2.6/dist-packages/Sphinx-0.6.3-py2.6.egg', '/usr/local/lib/python2.6/dist-packages/django_html-0.0.1-py2.6.egg', '/usr/local/lib/python2.6/dist-packages/html5lib-0.11.1-py2.6.egg', '/home/kenny', '/home/kenny/Projects/soclone-read-only', '/home/kenny/python/Django', '/home/kenny/python/pysmell', '/home/kenny/python/Django/ropemode', '/home/kenny/python/Django/rope', '/home/kenny/python/lib', '/usr/lib/python2.6', '/usr/lib/python2.6/plat-linux2', '/usr/lib/python2.6/lib-tk', '/usr/lib/python2.6/lib-old', '/usr/lib/python2.6/lib-dynload', '/usr/local/lib/python2.6/dist-packages']",How can I correct a corrupted $PYTHONPATH?
Supply arguments to optparse in a file," I've been using optparse for a while now, and would like to add the ability to load the arguments from a config file. So far the best I can think of is a wrapper batch script with the arguments hardcoded... seems clunky.What is the most elegant way to do this?  <code> ",Using a file to store optparse arguments
Use a file to store optparse arguments (optionally)," I've been using optparse for a while now, and would like to add the ability to load the arguments from a config file. So far the best I can think of is a wrapper batch script with the arguments hardcoded... seems clunky.What is the most elegant way to do this?  <code> ",Using a file to store optparse arguments
Use a file to store optparse arguments," I've been using optparse for a while now, and would like to add the ability to load the arguments from a config file. So far the best I can think of is a wrapper batch script with the arguments hardcoded... seems clunky.What is the most elegant way to do this?  <code> ",Using a file to store optparse arguments
How can I get optparse's OptionParser to ignore invalid arguments?," In python's OptionParser, how can I instruct it to ignore undefined options supplied to method parse_args?e.g.I've only defined option --foo for my OptionParser instance, but I call parse_args with list [ '--foo', '--bar' ] EDIT:I don't care if it filters them out of the original list. I just want undefined options ignored.The reason I'm doing this is because I'm using SCons' AddOption interface to add custom build options. However, some of those options guide the declaration of the targets. Thus I need to parse them out of sys.argv at different points in the script without having access to all the options. In the end, the top level Scons OptionParser will catch all the undefined options in the command line. <code> ",How can I get optparse's OptionParser to ignore invalid options?
convert string list to list in python," I was wondering what the simplest way is to convert a string representation of a list like the following to a list: Even in cases where the user puts spaces in between the commas, and spaces inside of the quotes, I need to handle that as well and convert it to: I know I can strip spaces with strip() and split() and check for non-letter characters. But the code was getting very kludgy. Is there a quick function that I'm not aware of? <code>  x = '[ ""A"",""B"",""C"" , "" D""]' x = [""A"", ""B"", ""C"", ""D""] ",How to convert string representation of list to a list?
convert string representation of list to list in python," I was wondering what the simplest way is to convert a string representation of a list like the following to a list: Even in cases where the user puts spaces in between the commas, and spaces inside of the quotes, I need to handle that as well and convert it to: I know I can strip spaces with strip() and split() and check for non-letter characters. But the code was getting very kludgy. Is there a quick function that I'm not aware of? <code>  x = '[ ""A"",""B"",""C"" , "" D""]' x = [""A"", ""B"", ""C"", ""D""] ",How to convert string representation of list to a list?
Convert string representation of list to list in Python," I was wondering what the simplest way is to convert a string representation of a list like the following to a list: Even in cases where the user puts spaces in between the commas, and spaces inside of the quotes, I need to handle that as well and convert it to: I know I can strip spaces with strip() and split() and check for non-letter characters. But the code was getting very kludgy. Is there a quick function that I'm not aware of? <code>  x = '[ ""A"",""B"",""C"" , "" D""]' x = [""A"", ""B"", ""C"", ""D""] ",How to convert string representation of list to a list?
Convert string representation of list to list," I was wondering what the simplest way is to convert a string representation of a list like the following to a list: Even in cases where the user puts spaces in between the commas, and spaces inside of the quotes, I need to handle that as well and convert it to: I know I can strip spaces with strip() and split() and check for non-letter characters. But the code was getting very kludgy. Is there a quick function that I'm not aware of? <code>  x = '[ ""A"",""B"",""C"" , "" D""]' x = [""A"", ""B"", ""C"", ""D""] ",How to convert string representation of list to a list?
Autoreload of modules in iPython, Is there a way to have IPython automatically reload all changed code? Either before each line is executed in the shell or failing that when it is specifically requested to. I'm doing a lot of exploratory programming using IPython and SciPy and it's quite a pain to have to manually reload each module whenever I change it. <code> ,Autoreload of modules in IPython
easiest way to parse xml in python," I have many rows in a database that contains XML and I'm trying to write a Python script to count instances of a particular node attribute.My tree looks like: How can I access the attributes ""1"" and ""2"" in the XML using Python? <code>  <foo> <bar> <type foobar=""1""/> <type foobar=""2""/> </bar></foo>",How to parse XML and count instances of a particular node attribute?
How do I parse XML in python?," I have many rows in a database that contains XML and I'm trying to write a Python script to count instances of a particular node attribute.My tree looks like: How can I access the attributes ""1"" and ""2"" in the XML using Python? <code>  <foo> <bar> <type foobar=""1""/> <type foobar=""2""/> </bar></foo>",How to parse XML and count instances of a particular node attribute?
How do I parse XML in Python?," I have many rows in a database that contains XML and I'm trying to write a Python script to count instances of a particular node attribute.My tree looks like: How can I access the attributes ""1"" and ""2"" in the XML using Python? <code>  <foo> <bar> <type foobar=""1""/> <type foobar=""2""/> </bar></foo>",How to parse XML and count instances of a particular node attribute?
How do I write good/correct __init__.py files," My package has the following structure: I'm not sure how the __init__.py files should be correctly written. The __init__.py #1 looks like: But how should for example __init__.py #2 look like? Mine is: When should be __all__ used?  <code>  mobilescouter/ __init__.py #1 mapper/ __init__.py #2 lxml/ __init__.py #3 vehiclemapper.py vehiclefeaturemapper.py vehiclefeaturesetmapper.py ... basemapper.py vehicle/ __init__.py #4 vehicle.py vehiclefeature.py vehiclefeaturemapper.py ... __all__ = ['mapper', 'vehicle']import mapperimport vehicle __all__ = ['basemapper', 'lxml']from basemaper import *import lxml",How do I write good/correct package __init__.py files
Is it possible to add PyQt4\PySide packages on a virtualenv sandbox?," I'm using Virtualenv with profit on my development environment with web.py, simplejson and other web oriented packages.I'm going to develop a simple python client using Qt to reuse some Api developed with web.py. Does anybody here had succesfully installed PyQt4 with Virtualenv?Is it possible?I've downloaded all the binaries and have PyQt4 installed globally on my python2.6 directory.If I don't use --no-site--packages option, Virtualenv correctly includes PyQt4 in my new sandbox but, obviously, with all the global packages that I don't need.Is there a clean way to prepare a new sandbox with --no-site--packages option and then add PyQt4 or PySide using pip, easy_install or some other magic trick? <code> ",Is it possible to add PyQt4/PySide packages on a Virtualenv sandbox?
 what is the Significance of  a  funciton no 'self' argument  in class., what is the Significance of bthanks and  <code>  class a: def b(): ... class a: @staticmethod def b(): return 1 def c(self): b()print a.b()print a().b()print a().c()#error class a: @staticmethod def b(): return 1 def c(self): return self.b()print a.b()print a().b()print a().c()#1#1#1,What is the significance of a function without a 'self' argument  insde a class?
Which are more fundamental: Python functions or Python object-methods?," I am trying to get a conceptual understanding of the nature of Python functions and methods. I get that functions are actually objects, with a method that is called when the function is executed. But is that function-object method actually another function?For example: If I look at dir(fred), I see it has an attribute named __call__. But dir(fred.__call__) also has an attribute named __call__. So does fred.__call__.__call__ and so on. The ids of this chain of __call__ objects suggest they are all distinct. Are they really objects or is this some low-level trick of the interpreter?Which is more fundamental: functions or object-methods? <code>  def fred(): pass",Which is more fundamental: Python functions or Python object-methods?
Django: signal when user logs in ?," In my Django app, I need to start running a few periodic background jobs when a user logs in and stop running them when the user logs out, so I am looking for an elegant way toget notified of a user login/logoutquery user login statusFrom my perspective, the ideal solution would bea signal sent by each django.contrib.auth.views.login and ... views.logouta method django.contrib.auth.models.User.is_logged_in(), analogous to ... User.is_active() or ... User.is_authenticated()Django 1.1.1 does not have that and I am reluctant to patch the source and add it (not sure how to do that, anyway).As a temporary solution, I have added an is_logged_in boolean field to the UserProfile model which is cleared by default, is set the first time the user hits the landing page (defined by LOGIN_REDIRECT_URL = '/') and is queried in subsequent requests. I added it to UserProfile, so I don't have to derive from and customize the builtin User model for that purpose only.I don't like this solution. If the user explicitely clicks the logout button, I can clear the flag, but most of the time, users just leave the page or close the browser; clearing the flag in these cases does not seem straight forward to me. Besides (that's rather data model clarity nitpicking, though), is_logged_in does not belong in the UserProfile, but in the User model.Can anyone think of alternate approaches ? <code> ",Django: signal when user logs in?
"Best way to save complex Ptyhon data structures across program sessions (pickle, json, xml, database, other)"," Looking for advice on the best technique for saving complex Python data structures across program sessions.Here's a list of techniques I've come up with so far:pickle/cpicklejsonjsonpicklexmldatabase (like SQLite)Pickle is the easiest and fastest technique, but my understanding is that there is no guarantee that pickle output will work across various versions of Python 2.x/3.x or across 32 and 64 bit implementations of Python.Json only works for simple data structures. Jsonpickle seems to correct this AND seems to be written to work across different versions of Python.Serializing to XML or to a database is possible, but represents extra effort since we would have to do the serialization ourselves manually.Thank you,Malcolm <code> ","Best way to save complex Python data structures across program sessions (pickle, json, xml, database, other)"
Why are Python closures read-only?," Closures are an incredibly useful language feature. They let us do clever things that would otherwise take a lot of code, and often enable us to write code that is more elegant and more clear. In Python 2.x, closures variable names cannot be rebound; that is, a function defined inside another lexical scope cannot do something like some_var = 'changed!' for variables outside of its local scope. Can someone explain why that is? There have been situations in which I would like to create a closure that rebinds variables in the outer scope, but it wasn't possible. I realize that in almost all cases (if not all of them), this behavior can be achieved with classes, but it is often not as clean or as elegant. Why can't I do it with a closure? Here is an example of a rebinding closure: This is the current behavior when you call it: What I'd like it to do instead is this:  <code>  def counter(): count = 0 def c(): count += 1 return count return c >>> c()Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 4, in cUnboundLocalError: local variable 'count' referenced before assignment >>> c()1>>> c()2>>> c()3",Read/Write Python Closures
execute (not import) python script from python promt," I need to execute a Python script from an already started Python session, as if it were launched from the command line. I'm thinking of similar to doing source in bash or sh. <code> ",How do I execute (not import) a python script from a python prompt?
python - os.makedirs don't understand ~ in my path?," I have a little problem with ~ in my paths.This code example creates some directories called ~/some_dir and do not understand that I wanted to create some_dir in my home directory. Note this is on a Linux-based system. <code>  my_dir = ""~/some_dir""if not os.path.exists(my_dir): os.makedirs(my_dir)","os.makedirs doesn't understand ""~"" in my path"
"Python's os.makedirs doesn't understand ""~"" in my path"," I have a little problem with ~ in my paths.This code example creates some directories called ~/some_dir and do not understand that I wanted to create some_dir in my home directory. Note this is on a Linux-based system. <code>  my_dir = ""~/some_dir""if not os.path.exists(my_dir): os.makedirs(my_dir)","os.makedirs doesn't understand ""~"" in my path"
Vim insert mode: Comments (#) go to start of line," Whenever I want to add a comment to an indented line in vim, I hit Shift-o (open a new row above the current, switch to insert mode) and start typing a Python comment (using #). That hash is then magically moved to the start of the line (no indentation) and I have to click tab a few times. Anyone know how to work around it? <code> ",Comments (#) go to start of line in the insert mode in Vim
Find system folder locations in Python 3.1," I am trying to find out the location of system folders with Python 3.1. For example ""My Documents"" = ""C:\Documents and Settings\User\My Documents"", ""Program Files"" = ""C:\Program Files"" etc etc. <code> ",Find system folder locations in Python
Changing global variables in a function through an exec() statement," Why can I not change global variables from inside a function, using exec()? It works fine when the assignment statement is outside of exec(). Here is an example of my problem: <code>  >>> myvar = 'test'>>> def myfunc():... global myvar... exec('myvar = ""changed!""')... print(myvar)... >>> myfunc()test>>> print(myvar)test",Cannot change global variables in a function through an exec() statement?
How to make a web interface to a script that takes 30 minutes to execute?," I wrote a python script to process some data from CSV files. The script takes between 3 to 30 minutes to complete, depending on the size of the CSV.Now I want to put in a web interface to this, so I can upload the CSV data files from anywhere. I wrote a basic HTTP POST upload page and used Python's CGI module - but the script just times out after some time. The script outputs HTTP headers at the start, and outputs bits of data after iterating over every line of the CSV. As an example, this print statement would trigger every 30 seconds or so. I assumed the browser would receive the headers, and wait since it keeps on receiving little bits of data. But what actually seems to happen is it doesn't receive any data at all, and Error 504 times out when given a CSV with lots of lines. Perhaps there's some caching happening somewhere? From the logs, What's the best way to resolve this, or, is it not appropriate to run such scripts in a browser?Edit:This is a script for my own use - I normally intend to use it on my computer, but I thought a web-based interface could come in handy while travelling, or for example from a phone. Also, there's really nothing to download - the script will most probably e-mail a report off at the end. <code>  # at the very top, with the 'import'sprint ""Content-type: text/html\n\n Processing ... <br />""# the really long loop.for currentRecord in csvRecords: count = count + 1 print ""On line "" + str(count) + "" <br />"" [Wed Jan 20 16:59:09 2010] [error] [client ::1] Script timed out before returning headers: datacruncher.py, referer: http://localhost/index.htm[Wed Jan 20 17:04:09 2010] [warn] [client ::1] Timeout waiting for output from CGI script /Library/WebServer/CGI-Executables/datacruncher.py, referer: http://localhost/index.htm",Making a web interface to a script that takes 30 minutes to execute
SqlAlchemy add new Field to class and create corresponding column in table ," I want to add a field to an existing mapped class, how would I update the sql table automatically. Does sqlalchemy provide a method to update the database with a new column, if a field is added to the class.  <code> ",SqlAlchemy add new Field to class and create corresponding column in table
most Pythonic way to concatenate strings," Given this harmless little list: My goal is to pythonically concatenate the little devils using one of the following ways:A. plain ol' string function to get the job done, short, no imports B. lambda, lambda, lambda C. globalization (do nothing, import everything) Please suggest other pythonic ways to achieve this magnanimous task.Please rank (pythonic level) and rate solutions giving concise explanations.In this case, is the most pythonic solution the best coding solution? <code>  >>> lst = ['o','s','s','a','m','a'] >>> ''.join(lst)'ossama' >>> reduce(lambda x, y: x + y, lst)'ossama' >>> import functools, operator>>> functools.reduce(operator.add, lst)'ossama'",Most Pythonic way to concatenate strings
"Python (2.6) , List Comprehension: why is this a syntax error ?"," Why is print(x) here not valid (SyntaxError) in the following list-comprehension? To contrast - the following doesn't give a syntax error: <code>  my_list=[1,2,3][print(my_item) for my_item in my_list] def my_func(x): print(x)[my_func(my_item) for my_item in my_list]",List Comprehension: why is this a syntax error?
Django: serializing list to json," I am sending information between client and Django server, and I would like to use JSON to this. I am sending simple information - list of strings. I tried using django.core.serializers, but when I did, I got It seems this can be used only for Django objects. How can I serialize simple, Python objects? <code>  AttributeError: 'str' object has no attribute '_meta'",Serializing list to JSON
Django: serializing list to JSON," I am sending information between client and Django server, and I would like to use JSON to this. I am sending simple information - list of strings. I tried using django.core.serializers, but when I did, I got It seems this can be used only for Django objects. How can I serialize simple, Python objects? <code>  AttributeError: 'str' object has no attribute '_meta'",Serializing list to JSON
how to change process name of python script running on windows machine," Windows Task Manager lists all running processes in the ""Processes"" tab.The image name of Python scripts is always python.exe, or pythonw.exe, or the name of the Python interpreter.Is there a nice way to change the image name of a Python script, other than changing the name of the Python interpreter? <code> ",Change process name of Python script
How to access url hash from a Django Request object," As in the title: How can I access the URL hash/fragment (the part following the hash #, or 'pound symbol' in US English) from a Django view and so, I suppose, from a Django Request object?I've not found enough information on the documentation here available: http://docs.djangoproject.com/en/dev/ref/request-response/P.S. Assume the fragment is included in the URL sent to the server. (I have verified this in my case, where I'm not actually using a browser.) <code> ",How to access url hash/fragment from a Django Request object
Getting correct string length in Python for strings with ASCII color codes," I've got some Python code that will automatically print a set of data in a nice column format, including putting in the appropriate ASCII escape sequences to color various pieces of the data for readability.I eventually end up with each line being represented as a list, with each item being a column that is space-padded so that the same columns on each line are always the same length. Unfortunately when I actually go to print this, not all the columns line up. I suspect this is to do with the ASCII escape sequences - because the len function doesn't seem to recognize these: And so while each column is the same length according to len, they are not actually the same length when printed on the screen. Is there any way (save for doing some hackery with regular expressions which I'd rather not do) to take the escaped string and find out what the printed length is so I can space pad appropriately? Maybe some way to just ""print"" it back to string and examine the length of that? <code>  >>> a = '\x1b[1m0.0\x1b[0m'>>> len(a)11>>> print a0.0",Getting correct string length in Python for strings with ANSI color codes
Python: Print only the message on warnings," I'm issuing lots of warnings in a validator, and I'd like to suppress everything in stdout except the message that is supplied to warnings.warn().I.e., now I see this: I'd like to see this: Edit 2: Overriding warnings.showwarning() turned out to work: <code>  ./file.py:123: UserWarning: My looong warning messagesome Python code My looong warning message def _warning( message, category = UserWarning, filename = '', lineno = -1): print(message)...warnings.showwarning = _warningwarnings.warn('foo')",Print only the message on warnings
Length of an integer in python," In Python, how do you find the number of digits in an integer? <code> ",How to find length of digits in an integer?
Length of an integer in Python," In Python, how do you find the number of digits in an integer? <code> ",How to find length of digits in an integer?
Why isn't there a do while flow control  statement in python?, Is there a good reason why there isn't a do while flow control statement in python?Why do people have to write while and break explicitly? <code> ,Why isn't there a do while flow control statement in python?
Inserting values into specific locations in a list in python," I am trying to do print all the possible outcomes of a given list and I was wondering how to put a value into various locations in the list. For example, if my list was [A, B], I want to insert X into all possible index of the list such that it would return this [X, A, B], [A, X, B], [A, B, X]. I was thinking about using range(len()) and a for loop but not sure how to start.  <code> ",Inserting a value into all possible locations in a list
Inserting values into specific locations in a list in Python," I am trying to do print all the possible outcomes of a given list and I was wondering how to put a value into various locations in the list. For example, if my list was [A, B], I want to insert X into all possible index of the list such that it would return this [X, A, B], [A, X, B], [A, B, X]. I was thinking about using range(len()) and a for loop but not sure how to start.  <code> ",Inserting a value into all possible locations in a list
how to get the same day of next month of some day in python using datetime, i know using datetime.timedelta i can get the date of some days away form given date but seems no datetime.timedelta(month=1) <code>  daysafter = datetime.date.today() + datetime.timedelta(days=5),how to get the same day of next month of a given day in python using datetime
How to clear the entry widget in a GUI after a button is pressed in python," I'm trying to clear the Entry widget after the user presses a button using Tkinter. I tried using ent.delete(0, END), but I got an error saying that strings don't have the attribute delete.Here is my code, where I'm getting error on real.delete(0, END): <code>  secret = randrange(1,100)print(secret)def res(real, secret): if secret==eval(real): showinfo(message='that is right!') real.delete(0, END)def guess(): ge = Tk() ge.title('guessing game') Label(ge, text=""what is your guess:"").pack(side=TOP) ent = Entry(ge) ent.pack(side=TOP) btn=Button(ge, text=""Enter"", command=lambda: res(ent.get(),secret)) btn.pack(side=LEFT) ge.mainloop()",How to clear the Entry widget after a button is pressed in Tkinter?
remove default apps from django-admin," By default, in Django-admin there is Users, Groups, and Sites apps. How can I remove Groups and Sites?I tried to remove admin.autodiscover() from root urls. Then, when I added something like admin.site.register(User, UserAdmin) somewhere in my app models I got an AlreadyRegistered exception (this is fairly right - models users already registered in django.contrib.auth). <code> ",Remove default apps from Django-admin
"Python, OpenOffice: Programatically Manipulating spreadsheets"," I have an spreadsheet-based automated report that needs to be created daily, with some charts, aggregating functions (e.g. SUM and AVERAGE) and formatted cells (Dates, percentage, etc.).I have tried to write these results directly to an Excel file, but Python's xlwt and xlrd don'y support charts and functions. Moreover, trying to open an existing, formatted Excel file and changing some cell's values ended up erasing all charts and functions in the existing file.Is there a way to write charts and functions to an OpenOffice spreadsheet, or at least change cells in an existing spreadsheet without erasing data? If there is a Pythonic way to do it, I can easily convert the OO file into an Excel file and deliver it. <code> ","Python, OpenOffice: Programmatically Manipulating spreadsheets"
What is the proper way to comment functions in python?, Is there a generally accepted way to comment functions in Python? Is the following acceptable? <code>  ########################################################## Create a new user#########################################################def add(self):,What is the proper way to comment functions in Python?
is Tkinter worth learning?," I generally make my desktop interfaces with Qt, but some recent TK screenshots convince me Tk isn't just ugly motif any more.Additionally Tkinter comes bundled with Python, which makes distribution easier. So is it worth learning or should I stick with Qt?(source: kb-creative.net)  <code> ",Is Tkinter worth learning?
subprocess.Popen doesn't work when args is sequence," I'm having a problem with subprocess.Popen when args parameter is given as sequence.For example: This works (it prints the correct size of /home/support/Maildir dir): But, this doesn't work (try it): What's wrong? <code>  import subprocessmaildir = ""/home/support/Maildir"" size = subprocess.Popen([""du -s -b "" + maildir], shell=True, stdout=subprocess.PIPE).communicate()[0].split()[0]print size size = subprocess.Popen([""du"", ""-s -b"", maildir], shell=True, stdout=subprocess.PIPE).communicate()[0].split()[0]print size",Why subprocess.Popen doesn't work when args is sequence?
Calling Python from Ruby," Would it be possible to integrate Python (and/or Perl) and Ruby? I've looked at http://www.goto.info.waseda.ac.jp/~fukusima/ruby/python/doc/ and http://code.google.com/p/ruby-perl/ , but they both seem rather outdated.Has someone generated a Ruby interface for Python's C API?Edit: Python can be integrated with many other languages according to http://wiki.python.org/moin/IntegratingPythonWithOtherLanguages . However, that list doesn't include Ruby. <code> ",Would it be possible to integrate Python or Perl with Ruby?
Calling Perl and/or Python from Ruby," Would it be possible to integrate Python (and/or Perl) and Ruby? I've looked at http://www.goto.info.waseda.ac.jp/~fukusima/ruby/python/doc/ and http://code.google.com/p/ruby-perl/ , but they both seem rather outdated.Has someone generated a Ruby interface for Python's C API?Edit: Python can be integrated with many other languages according to http://wiki.python.org/moin/IntegratingPythonWithOtherLanguages . However, that list doesn't include Ruby. <code> ",Would it be possible to integrate Python or Perl with Ruby?
Would it be possible to transparently integrate Python or Perl with Ruby?," Would it be possible to integrate Python (and/or Perl) and Ruby? I've looked at http://www.goto.info.waseda.ac.jp/~fukusima/ruby/python/doc/ and http://code.google.com/p/ruby-perl/ , but they both seem rather outdated.Has someone generated a Ruby interface for Python's C API?Edit: Python can be integrated with many other languages according to http://wiki.python.org/moin/IntegratingPythonWithOtherLanguages . However, that list doesn't include Ruby. <code> ",Would it be possible to integrate Python or Perl with Ruby?
Python Copy Through Assignment?," I would expect that the following code would just initialise the dict_a, dict_b and dict_c dictionaries. But it seams to have a copy through effect: As you can see the result is as follows: Why does that program give the previous result, When I would expect it to return: <code>  dict_a = dict_b = dict_c = {}dict_c['hello'] = 'goodbye'print dict_aprint dict_bprint dict_c {'hello': 'goodbye'}{'hello': 'goodbye'}{'hello': 'goodbye'} {}{}{'hello': 'goodbye'}",Does Python make a copy of objects on assignment?
close() implies flush()?," In Python, and in general - does a close() operation on a file object imply a flush() operation? <code> ",does close() imply flush() in Python?
Python named print?," I know it's a really simple question, but I have no idea how to google it.how can I do So that my_url is used twice? I assume I have to ""name"" the %s and then use a dict in the params, but I'm not sure of the proper syntax?just FYI, I'm aware I can just use my_url twice in the params, but that's not the point :) <code>  print '<a href=""%s"">%s</a>' % (my_url)",String formatting named parameters?
python library for converting plain text (ASCII) into GSM 7-bit character set?, Is there a python library for encoding ascii data to 7-bit GSM character set (for sending SMS)? <code> ,Python library for converting plain text (ASCII) into GSM 7-bit character set?
Will python 3 ever catch on?, I have been learning a bit of Python 2 and Python 3 and it seems like Python 2 is overall better than Python 3. So that's where my question comes in. Are there any good reasons to actually switch over to python 3? <code> ,Will Python 3 ever catch on?
"why the hell does x,y = zip(*zip(a,b)) work in Python?"," OK I love Python's zip() function. Use it all the time, it's brilliant. Every now and again I want to do the opposite of zip(), think ""I used to know how to do that"", then google python unzip, then remember that one uses this magical * to unzip a zipped list of tuples. Like this: What on earth is going on? What is that magical asterisk doing? Where else can it be applied and what other amazing awesome things in Python are so mysterious and hard to google? <code>  x = [1,2,3]y = [4,5,6]zipped = zip(x,y)unzipped_x, unzipped_y = zip(*zipped)unzipped_x Out[30]: (1, 2, 3)unzipped_y Out[31]: (4, 5, 6)","Why does x,y = zip(*zip(a,b)) work in Python?"
scoping problem in recursive closure, why does this work: but this does not: I get this error: <code>  def function1(): a = 10 def function2(): print a function2() def function1(): a = 10 def function2(): print a a -= 1 if a>0: function2() function2() UnboundLocalError: local variable 'a' referenced before assignment,scoping error in recursive closure
Are strings singletons in Python," Does Python have a pool of all strings and are they (strings) singletons there?More precise, in the following code, are one or two strings created in memory? <code>  a = str(num)b = str(num)",Are strings pooled in Python?
Are strings pooled in Python," Does Python have a pool of all strings and are they (strings) singletons there?More precise, in the following code, are one or two strings created in memory? <code>  a = str(num)b = str(num)",Are strings pooled in Python?
I need a simple command line program to transform XML using XSLT," I am on OSX Snow Leopard (10.6.2) I can install anything I need to. I would preferably like a Python or Java solution. I have searched on Google and found lots of information on writing my own program to do this, but this is a just a quick and dirty experiment so I don't want to invest a lot of time on writing a bunch of code to do this, I am sure someone else has done this already. This is off-topic now, do not use this question as an example of why your recommendations request is on topic, it is not. I apologize, but my Google-Foo was failing me the day I asked this 4 years ago! <code> ",I need a simple command line program to transform XML using an XSL Stylesheet
Help getting frame rate (fps) up in Python + Pygame," I am working on a little card-swapping world-travel game that I sort of envision as a cross between Bejeweled and the 10 Days geography board games. So far the coding has been going okay, but the frame rate is pretty bad... currently I'm getting low 20's on my Core 2 Duo. This is a problem since I'm creating the game for Intel's March developer competition, which is squarely aimed at netbooks packing underpowered Atom processors.Here's a screen from the game:(source: necessarygames.com) I am very new to Python and Pygame (this is the first thing I've used them for), and am sadly lacking in formal CS training... which is to say that I think there are probably A LOT of bad practices going on in my code, and A LOT that could be optimized. If some of you older Python hands wouldn't mind taking a look at my code and seeing if you can't find any obvious areas for optimization, I would be extremely grateful. You can download the full source code here (Python 2.6 + Pygame 1.9):http://www.necessarygames.com/my_games/betraveled/betraveled_src0328.zipCompiled exe here:www.necessarygames.com/my_games/betraveled/betraveled_src0328.zipOne thing I am concerned about is my event manager, which I feel may have some performance wholes in it, and another thing is my rendering... I'm pretty much just blitting everything to the screen all the time (see the render routines in my game_components.py below); I recently found out that you should only update the areas of the screen that have changed, but I'm still foggy on how that accomplished exactly... could this be a huge performance issue?Any thoughts are much appreciated! As usual, I'm happy to ""tip"" you for your time and energy via PayPal.JordanEDIT:Thanks to the advice below, I ran cprofile on my code. It would be great if anyone would be willing to look at this output and let me know what is and what is not to be expected.Here is the output of p.strip_dirs().sort_stats('cumulative').print_stats(): Here is the output of p.strip_dirs().sort_stats('time').print_stats(): Here are some bits of the source:Main.py event_manager.py rooms.py <code>  pydev debugger: startingSun Mar 28 04:46:16 2010 cprofile 8383715 function calls (8264821 primitive calls) in 157.732 CPU seconds Ordered by: cumulative time ncalls tottime percall cumtime percall filename:lineno(function) 1 0.000 0.000 157.732 157.732 <string>:1(<module>) 1 0.000 0.000 157.732 157.732 main.py:47(main) 1 0.074 0.074 157.280 157.280 event_manager.py:101(run)4911/2414 11.837 0.002 156.984 0.065 event_manager.py:32(post)4786/4681 0.238 0.000 94.852 0.020 rooms.py:251(notify) 2187 0.523 0.000 51.136 0.023 rooms.py:329(render)4911/2959 0.220 0.000 35.732 0.012 event_manager.py:54(notify) 2271 33.996 0.015 33.996 0.015 {pygame.display.update} 2271 0.060 0.000 23.664 0.010 app.py:178(paint)37347/2271 1.580 0.000 23.587 0.010 container.py:83(paint)70236/2271 3.609 0.000 23.448 0.010 theme.py:275(func) 1078950 16.926 0.000 16.926 0.000 {method 'blit' of 'pygame.Surface' objects} 2187 2.131 0.001 16.875 0.008 game_components.py:666(render)19635/17756 0.187 0.000 13.852 0.001 game_components.py:641(notify) 733820 7.710 0.000 13.643 0.000 game_components.py:1151(notify) 2271 12.254 0.005 12.254 0.005 {pygame.time.wait} 64112 3.174 0.000 11.252 0.000 game_components.py:1186(render) 9 0.002 0.000 10.151 1.128 game_components.py:286(deal_new_board) 1934 0.095 0.000 8.489 0.004 app.py:144(event)4359/2146 0.178 0.000 8.375 0.004 container.py:112(event)2335/2146 0.056 0.000 8.251 0.004 widget.py:346(_event)2335/2146 0.048 0.000 8.193 0.004 theme.py:320(func)4883/4691 0.018 0.000 8.049 0.002 widget.py:313(send) 229 0.139 0.001 8.017 0.035 game_components.py:632(refresh_darkness) 7258 0.499 0.000 7.844 0.001 game_components.py:963(test_close_to_trip) 2271 7.378 0.003 7.378 0.003 {pygame.display.set_caption} 19347 3.313 0.000 6.687 0.000 game_components.py:928(get_next_to) 2187 1.529 0.001 6.629 0.003 game_components.py:130(render) 2011729 5.938 0.000 5.938 0.000 {isinstance} 3 0.000 0.000 5.906 1.969 my_gui.py:274(clicked) 219456 3.060 0.000 5.680 0.000 surface.py:5(subsurface) 1290 0.506 0.000 5.416 0.004 game_components.py:683(__init__) 8748 2.891 0.000 5.340 0.001 theme.py:400(render) 70236 2.232 0.000 4.945 0.000 theme.py:186(box) 1357 0.097 0.000 4.391 0.003 game_components.py:742(set_image) 9 0.054 0.006 3.450 0.383 game_components.py:209(deal_connection_matrix) 8748 0.325 0.000 3.318 0.000 theme.py:479(paint) 1051 0.015 0.000 3.262 0.003 game_components.py:1232(__init__) 386467 3.148 0.000 3.148 0.000 {method 'fill' of 'pygame.Surface' objects} 245332 2.470 0.000 3.117 0.000 game_components.py:495(get_card) 55761 3.068 0.000 3.068 0.000 {pygame.draw.aaline} 2271 2.968 0.001 2.968 0.001 {pygame.event.get} 9 0.007 0.001 2.946 0.327 game_components.py:416(clone) 2187 0.089 0.000 2.717 0.001 misc.py:28(paint) 803701 2.483 0.000 2.497 0.000 weakref.py:302(iterkeys) 2739 2.441 0.001 2.441 0.001 {pygame.imageext.load_extended} 1470 1.108 0.001 2.432 0.002 game_components.py:450(set_pos) 29 0.029 0.001 2.270 0.078 game_components.py:433(clear) 2 0.000 0.000 2.158 1.079 main.py:22(set_room)4911/4862 0.027 0.000 2.141 0.000 main.py:37(notify) 1 0.000 0.000 2.099 2.099 my_gui.py:164(clicked) 1 0.001 0.001 2.099 2.099 rooms.py:117(__init__) 1120 0.039 0.000 1.978 0.002 game_components.py:484(place_card) 27 0.013 0.000 1.963 0.073 game_components.py:339(merge_board_stack) 311 0.018 0.000 1.939 0.006 game_components.py:443(remove) 125 0.007 0.000 1.848 0.015 rooms.py:18(notify) 224 0.003 0.000 1.806 0.008 game_components.py:721(clone) 219456 1.638 0.000 1.638 0.000 {method 'subsurface' of 'pygame.Surface' objects} 183 0.004 0.000 1.313 0.007 game_components.py:1240(__init__) 4374 0.106 0.000 1.224 0.000 button.py:97(paint) 46765 0.967 0.000 1.137 0.000 game_components.py:784(set_pos) 229 0.109 0.000 1.111 0.005 game_components.py:594(refresh_trip) 4076 1.076 0.000 1.076 0.000 {method 'convert_alpha' of 'pygame.Surface' objects} 9 0.015 0.002 1.018 0.113 game_components.py:542(displace_cards) 17496 0.360 0.000 0.953 0.000 basic.py:102(paint) 66299 0.948 0.000 0.948 0.000 {pygame.draw.rect} 1357 0.024 0.000 0.945 0.001 game_components.py:736(set_text) 1357 0.052 0.000 0.917 0.001 game_components.py:841(render_text_rec) 5815 0.455 0.000 0.881 0.000 game_components.py:1108(get_connections) 6610 0.869 0.000 0.869 0.000 {method 'replace' of 'pygame.PixelArray' objects} 56 0.001 0.000 0.861 0.015 game_components.py:1252(__init__) 10 0.001 0.000 0.857 0.086 game_components.py:377(__init__) 540 0.484 0.001 0.828 0.002 game_components.py:613(refresh_connections) 189431 0.730 0.000 0.730 0.000 game_components.py:500(test_index_on_board) 161329 0.632 0.000 0.727 0.000 matrix.py:33(__iter__) 309968 0.710 0.000 0.710 0.000 {method 'get_width' of 'pygame.Surface' objects} 308567 0.675 0.000 0.675 0.000 {method 'get_height' of 'pygame.Surface' objects} 109626 0.646 0.000 0.670 0.000 style.py:18(__getattr__) 241697 0.646 0.000 0.646 0.000 matrix.py:24(getitem) 21084 0.601 0.000 0.601 0.000 {method 'render' of 'pygame.font.Font' objects} 327 0.006 0.000 0.580 0.002 game_components.py:490(place_card_first_time) 26 0.001 0.000 0.561 0.022 game_components.py:1259(__init__) 166 0.002 0.000 0.536 0.003 game_components.py:608(get_longest_trip) 1491/166 0.075 0.000 0.534 0.003 game_components.py:989(get_longest_trip) 5702 0.533 0.000 0.533 0.000 {method 'size' of 'pygame.font.Font' objects} 1 0.000 0.000 0.478 0.478 game_components.py:141(__init__) 1 0.001 0.001 0.478 0.478 game_components.py:165(rebuild) 67 0.005 0.000 0.457 0.007 game_components.py:713(change_size) 1 0.001 0.001 0.420 0.420 game_components.py:347(change_card_size) 1210/166 0.044 0.000 0.412 0.002 game_components.py:982(add_to_trip) 7654 0.275 0.000 0.405 0.000 game_components.py:938(get_bounding_box) 14969 0.101 0.000 0.385 0.000 game_components.py:1305(render) 149709 0.341 0.000 0.341 0.000 {method 'append' of 'list' objects} 87958 0.341 0.000 0.341 0.000 {hasattr} 1362 0.068 0.000 0.336 0.000 textrect.py:12(render_textrect) 30 0.001 0.000 0.301 0.010 game_components.py:1282(__init__) 47795 0.257 0.000 0.257 0.000 game_components.py:1024(test_connection) 14849 0.098 0.000 0.251 0.000 my_gui.py:209(notify) 17498 0.158 0.000 0.242 0.000 basic.py:22(is_color) 87480 0.224 0.000 0.224 0.000 {method 'set_clip' of 'pygame.Surface' objects} 9 0.003 0.000 0.209 0.023 game_components.py:279(deal_to_blank_squares) 1 0.114 0.114 0.208 0.208 {pygame.display.set_mode} 56617 0.206 0.000 0.206 0.000 game_components.py:837(get_center) 84 0.005 0.000 0.206 0.002 rooms.py:110(render) Sun Mar 28 04:46:16 2010 cprofile 8383715 function calls (8264821 primitive calls) in 157.732 CPU seconds Ordered by: internal time ncalls tottime percall cumtime percall filename:lineno(function) 2271 33.996 0.015 33.996 0.015 {pygame.display.update} 1078950 16.926 0.000 16.926 0.000 {method 'blit' of 'pygame.Surface' objects} 2271 12.254 0.005 12.254 0.005 {pygame.time.wait}4911/2414 11.837 0.002 156.984 0.065 event_manager.py:32(post) 733820 7.710 0.000 13.643 0.000 game_components.py:1151(notify) 2271 7.378 0.003 7.378 0.003 {pygame.display.set_caption} 2011729 5.938 0.000 5.938 0.000 {isinstance}70236/2271 3.609 0.000 23.448 0.010 theme.py:275(func) 19347 3.313 0.000 6.687 0.000 game_components.py:928(get_next_to) 64112 3.174 0.000 11.252 0.000 game_components.py:1186(render) 386467 3.148 0.000 3.148 0.000 {method 'fill' of 'pygame.Surface' objects} 55761 3.068 0.000 3.068 0.000 {pygame.draw.aaline} 219456 3.060 0.000 5.680 0.000 surface.py:5(subsurface) 2271 2.968 0.001 2.968 0.001 {pygame.event.get} 8748 2.891 0.000 5.340 0.001 theme.py:400(render) 803701 2.483 0.000 2.497 0.000 weakref.py:302(iterkeys) 245332 2.470 0.000 3.117 0.000 game_components.py:495(get_card) 2739 2.441 0.001 2.441 0.001 {pygame.imageext.load_extended} 70236 2.232 0.000 4.945 0.000 theme.py:186(box) 2187 2.131 0.001 16.875 0.008 game_components.py:666(render) 219456 1.638 0.000 1.638 0.000 {method 'subsurface' of 'pygame.Surface' objects}37347/2271 1.580 0.000 23.587 0.010 container.py:83(paint) 2187 1.529 0.001 6.629 0.003 game_components.py:130(render) 1470 1.108 0.001 2.432 0.002 game_components.py:450(set_pos) 4076 1.076 0.000 1.076 0.000 {method 'convert_alpha' of 'pygame.Surface' objects} 46765 0.967 0.000 1.137 0.000 game_components.py:784(set_pos) 66299 0.948 0.000 0.948 0.000 {pygame.draw.rect} 6610 0.869 0.000 0.869 0.000 {method 'replace' of 'pygame.PixelArray' objects} 189431 0.730 0.000 0.730 0.000 game_components.py:500(test_index_on_board) 309968 0.710 0.000 0.710 0.000 {method 'get_width' of 'pygame.Surface' objects} 308567 0.675 0.000 0.675 0.000 {method 'get_height' of 'pygame.Surface' objects} 109626 0.646 0.000 0.670 0.000 style.py:18(__getattr__) 241697 0.646 0.000 0.646 0.000 matrix.py:24(getitem) 161329 0.632 0.000 0.727 0.000 matrix.py:33(__iter__) 21084 0.601 0.000 0.601 0.000 {method 'render' of 'pygame.font.Font' objects} 5702 0.533 0.000 0.533 0.000 {method 'size' of 'pygame.font.Font' objects} 2187 0.523 0.000 51.136 0.023 rooms.py:329(render) 1290 0.506 0.000 5.416 0.004 game_components.py:683(__init__) 7258 0.499 0.000 7.844 0.001 game_components.py:963(test_close_to_trip) 540 0.484 0.001 0.828 0.002 game_components.py:613(refresh_connections) 5815 0.455 0.000 0.881 0.000 game_components.py:1108(get_connections) 17496 0.360 0.000 0.953 0.000 basic.py:102(paint) 149709 0.341 0.000 0.341 0.000 {method 'append' of 'list' objects} 87958 0.341 0.000 0.341 0.000 {hasattr} 8748 0.325 0.000 3.318 0.000 theme.py:479(paint) 7654 0.275 0.000 0.405 0.000 game_components.py:938(get_bounding_box) 47795 0.257 0.000 0.257 0.000 game_components.py:1024(test_connection)4786/4681 0.238 0.000 94.852 0.020 rooms.py:251(notify) 87480 0.224 0.000 0.224 0.000 {method 'set_clip' of 'pygame.Surface' objects}4911/2959 0.220 0.000 35.732 0.012 event_manager.py:54(notify) 56617 0.206 0.000 0.206 0.000 game_components.py:837(get_center) 1 0.200 0.200 0.200 0.200 {pygame.base.quit}19635/17756 0.187 0.000 13.852 0.001 game_components.py:641(notify) 1 0.184 0.184 0.184 0.184 {pygame.base.init}4359/2146 0.178 0.000 8.375 0.004 container.py:112(event) 17498 0.158 0.000 0.242 0.000 basic.py:22(is_color) 1358 0.151 0.000 0.151 0.000 {pygame.transform.smoothscale} 229 0.139 0.001 8.017 0.035 game_components.py:632(refresh_darkness) 1 0.114 0.114 0.208 0.208 {pygame.display.set_mode} 37862 0.112 0.000 0.112 0.000 {method 'set_alpha' of 'pygame.Surface' objects} 6561 0.111 0.000 0.195 0.000 button.py:236(paint) 229 0.109 0.000 1.111 0.005 game_components.py:594(refresh_trip) 4374 0.106 0.000 1.224 0.000 button.py:97(paint) 14969 0.101 0.000 0.385 0.000 game_components.py:1305(render) 14849 0.098 0.000 0.251 0.000 my_gui.py:209(notify) 1357 0.097 0.000 4.391 0.003 game_components.py:742(set_image) 24072 0.096 0.000 0.096 0.000 game_components.py:913(test_pos_on_card) 4926 0.095 0.000 0.132 0.000 game_components.py:1292(__init__) 33286 0.095 0.000 0.095 0.000 {range} 1934 0.095 0.000 8.489 0.004 app.py:144(event) 2187 0.089 0.000 2.717 0.001 misc.py:28(paint) 2135 0.087 0.000 0.091 0.000 matrix.py:21(setitem) 4374 0.076 0.000 0.131 0.000 button.py:182(paint) 1491/166 0.075 0.000 0.534 0.003 game_components.py:989(get_longest_trip) 1 0.074 0.074 157.280 157.280 event_manager.py:101(run) 301 0.069 0.000 0.107 0.000 game_components.py:508(clear_selection) 1362 0.068 0.000 0.336 0.000 textrect.py:12(render_textrect) 2227 0.066 0.000 0.150 0.000 game_components.py:809(move) 5740 0.062 0.000 0.165 0.000 misc.py:34(__setattr__) 2271 0.060 0.000 23.664 0.010 app.py:178(paint)2335/2146 0.056 0.000 8.251 0.004 widget.py:346(_event) 4786 0.055 0.000 0.099 0.000 game_components.py:97(notify) 9 0.054 0.006 3.450 0.383 game_components.py:209(deal_connection_matrix) 1357 0.052 0.000 0.917 0.001 game_components.py:841(render_text_rec) 5 0.052 0.010 0.052 0.010 {method 'convert' of 'pygame.Surface' objects}2335/2146 0.048 0.000 8.193 0.004 theme.py:320(func) 1210/166 0.044 0.000 0.412 0.002 game_components.py:982(add_to_trip) 1120 0.039 0.000 1.978 0.002 game_components.py:484(place_card) 2871/465 0.037 0.000 0.070 0.000 container.py:77(reupdate) 11862 0.037 0.000 0.037 0.000 {method 'collidepoint' of 'pygame.Rect' objects}13602/13558 0.035 0.000 0.035 0.000 {len} 4493 0.035 0.000 0.047 0.000 button.py:71(__setattr__) #Remote importsimport pygamefrom pygame.locals import *#Local importsimport configimport roomsfrom event_manager import *from events import *class RoomController(object): """"""Controls which room is currently active (eg Title Screen)"""""" def __init__(self, screen, ev_manager): self.room = None self.screen = screen self.ev_manager = ev_manager self.ev_manager.register_listener(self) self.room = self.set_room(config.room) def set_room(self, room_const): #Unregister old room from ev_manager if self.room: self.room.ev_manager.unregister_listener(self.room) self.room = None #Set new room based on const if room_const == config.TITLE_SCREEN: return rooms.TitleScreen(self.screen, self.ev_manager) elif room_const == config.GAME_MODE_ROOM: return rooms.GameModeRoom(self.screen, self.ev_manager) elif room_const == config.GAME_ROOM: return rooms.GameRoom(self.screen, self.ev_manager) elif room_const == config.HIGH_SCORES_ROOM: return rooms.HighScoresRoom(self.screen, self.ev_manager) def notify(self, event): if isinstance(event, ChangeRoomRequest): if event.game_mode: config.game_mode = event.game_mode self.room = self.set_room(event.new_room) def render(self, surface): self.room.render(surface)#Run game def main(): pygame.init() screen = pygame.display.set_mode(config.screen_size) ev_manager = EventManager() spinner = CPUSpinnerController(ev_manager) room_controller = RoomController(screen, ev_manager) pygame_event_controller = PyGameEventController(ev_manager) spinner.run()# this runs the main function if this script is called to run.# If it is imported as a module, we don't run the main function.if __name__ == ""__main__"": main() #Remote importsimport pygamefrom pygame.locals import *#Local importsimport configfrom events import *def debug( msg ): print ""Debug Message: "" + str(msg)class EventManager: #This object is responsible for coordinating most communication #between the Model, View, and Controller. def __init__(self): from weakref import WeakKeyDictionary self.listeners = WeakKeyDictionary() self.eventQueue= [] self.gui_app = None #---------------------------------------------------------------------- def register_listener(self, listener): self.listeners[listener] = 1 #---------------------------------------------------------------------- def unregister_listener(self, listener): if listener in self.listeners: del self.listeners[listener] #---------------------------------------------------------------------- def post(self, event): if isinstance(event, MouseButtonLeftEvent): debug(event.name) #NOTE: copying the list like this before iterating over it, EVERY tick, is highly inefficient, #but currently has to be done because of how new listeners are added to the queue while it is running #(eg when popping cards from a deck). Should be changed. See: http://dr0id.homepage.bluewin.ch/pygame_tutorial08.html #and search for ""Watch the iteration"" for listener in list(self.listeners): #NOTE: If the weakref has died, it will be #automatically removed, so we don't have #to worry about it. listener.notify(event)#------------------------------------------------------------------------------class PyGameEventController: """"""..."""""" def __init__(self, ev_manager): self.ev_manager = ev_manager self.ev_manager.register_listener(self) self.input_freeze = False #---------------------------------------------------------------------- def notify(self, incoming_event): if isinstance(incoming_event, UserInputFreeze): self.input_freeze = True elif isinstance(incoming_event, UserInputUnFreeze): self.input_freeze = False elif isinstance(incoming_event, TickEvent): #Share some time with other processes, so we don't hog the cpu pygame.time.wait(5) #Handle Pygame Events for event in pygame.event.get(): #If this event manager has an associated PGU GUI app, notify it of the event if self.ev_manager.gui_app: self.ev_manager.gui_app.event(event) #Standard event handling for everything else ev = None if event.type == QUIT: ev = QuitEvent() elif event.type == pygame.MOUSEBUTTONDOWN and not self.input_freeze: if event.button == 1: #Button 1 pos = pygame.mouse.get_pos() ev = MouseButtonLeftEvent(pos) elif event.type == pygame.MOUSEMOTION: pos = pygame.mouse.get_pos() ev = MouseMoveEvent(pos) #Post event to event manager if ev: self.ev_manager.post(ev) #------------------------------------------------------------------------------ class CPUSpinnerController: def __init__(self, ev_manager): self.ev_manager = ev_manager self.ev_manager.register_listener(self) self.clock = pygame.time.Clock() self.cumu_time = 0 self.keep_going = True #---------------------------------------------------------------------- def run(self): if not self.keep_going: raise Exception('dead spinner') while self.keep_going: time_passed = self.clock.tick() fps = self.clock.get_fps() self.cumu_time += time_passed self.ev_manager.post(TickEvent(time_passed, fps)) if self.cumu_time >= 1000: self.cumu_time = 0 self.ev_manager.post(SecondEvent()) pygame.quit() #---------------------------------------------------------------------- def notify(self, event): if isinstance(event, QuitEvent): #this will stop the while loop from running self.keep_going = False #Remote importsimport pygame#Local importsimport configimport continentsfrom game_components import *from my_gui import *from pgu import highclass Room(object): def __init__(self, screen, ev_manager): self.screen = screen self.ev_manager = ev_manager self.ev_manager.register_listener(self) def notify(self, event): if isinstance(event, TickEvent): pygame.display.set_caption('FPS: ' + str(int(event.fps))) self.render(self.screen) pygame.display.update() def get_highs_table(self): fname = 'high_scores.txt' highs_table = None config.all_highs = high.Highs(fname) if config.game_mode == config.TIME_CHALLENGE: if config.difficulty == config.EASY: highs_table = config.all_highs['time_challenge_easy'] if config.difficulty == config.MED_DIF: highs_table = config.all_highs['time_challenge_med'] if config.difficulty == config.HARD: highs_table = config.all_highs['time_challenge_hard'] if config.difficulty == config.SUPER: highs_table = config.all_highs['time_challenge_super'] elif config.game_mode == config.PLAN_AHEAD: pass return highs_tableclass TitleScreen(Room): def __init__(self, screen, ev_manager): Room.__init__(self, screen, ev_manager) self.background = pygame.image.load('assets/images/interface/background.jpg').convert() #Initialize #--------------------------------------- self.gui_form = gui.Form() self.gui_app = gui.App(config.gui_theme) self.ev_manager.gui_app = self.gui_app c = gui.Container(align=0,valign=0) #Quit Button #--------------------------------------- b = StartGameButton(ev_manager=self.ev_manager) c.add(b, 0, 0) self.gui_app.init(c) def render(self, surface): surface.blit(self.background, (0, 0)) #GUI self.gui_app.paint(surface) class GameModeRoom(Room): def __init__(self, screen, ev_manager): Room.__init__(self, screen, ev_manager) self.background = pygame.image.load('assets/images/interface/background.jpg').convert() self.create_gui() #Create pgu gui elements def create_gui(self): #Setup #--------------------------------------- self.gui_form = gui.Form() self.gui_app = gui.App(config.gui_theme) self.ev_manager.gui_app = self.gui_app c = gui.Container(align=0,valign=-1) #Mode Relaxed Button #--------------------------------------- b = GameModeRelaxedButton(ev_manager=self.ev_manager) self.b = b print b.rect",How to get frame rate (fps) up in Python + Pygame?
"Why does date is calculated from January 1st, 1970 ?"," Is there any reason behind using date(January 1st, 1970) as default standard for time manipulation? I have seen this standard in Java as well as in Python. These two languages I am aware of. Are there other popular languages which follows the same standard?Please describe. <code> ","Why are dates calculated from January 1st, 1970 ?"
HOW TO: Draggable legend in matplotlib," I'm drawing a legend on an axes object in matplotlib but the default positioning which claims to place it in a smart place doesn't seem to work. Ideally, I'd like to have the legend be draggable by the user. How can this be done? <code> ",How to create a draggable legend in matplotlib?
how best do I find the intersection of multiple sets in python?," I have a list of sets: I want s1 s2 s3 ...I can write a function to do it by performing a series of pairwise s1.intersection(s2), etc.Is there a recommended, better, or built-in way? <code>  setlist = [s1,s2,s3...]",Best way to find the intersection of multiple sets?
"In Python, after I INSERT Into mysqldb, how do I get the ""id""?"," I execute an INSERT INTO statement and I want to get the primary key.My table has 2 columns: How do I get the ""id"", after I just inserted this?  <code>  cursor.execute(""INSERT INTO mytable(height) VALUES(%s)"",(height)) id primary, auto incrementheight this is the other column.","How do I get the ""id"" after INSERT into MySQL database with Python?"
Parse html and find data in the html," I am trying to use html5lib to parse an html page in to something I can query with xpath. html5lib has close to zero documentation and I've spent too much time trying to figure this problem out. Ultimate goal is to pull out the second row of a table: so lets try it: that looks good, lets see what else we have: LOL WUT?seriously. I was planning on using some xpath to get at the data I want, but that doesn't seem to work. So what can I do? I am willing to try different libraries and approaches. <code>  <html> <table> <tr><td>Header</td></tr> <tr><td>Want This</td></tr> </table></html> >>> doc = html5lib.parse('<html><table><tr><td>Header</td></tr><tr><td>Want This</td> </tr></table></html>', treebuilder='lxml')>>> doc<lxml.etree._ElementTree object at 0x1a1c290> >>> root = doc.getroot()>>> print(lxml.etree.tostring(root))<html:html xmlns:html=""http://www.w3.org/1999/xhtml""><html:head/><html:body><html:table><html:tbody><html:tr><html:td>Header</html:td></html:tr><html:tr><html:td>Want This</html:td></html:tr></html:tbody></html:table></html:body></html:html>","How can I parse HTML with html5lib, and query the parsed HTML with XPath?"
How can I parse HTML with html5lib and query the parsed HTML with XPath?," I am trying to use html5lib to parse an html page in to something I can query with xpath. html5lib has close to zero documentation and I've spent too much time trying to figure this problem out. Ultimate goal is to pull out the second row of a table: so lets try it: that looks good, lets see what else we have: LOL WUT?seriously. I was planning on using some xpath to get at the data I want, but that doesn't seem to work. So what can I do? I am willing to try different libraries and approaches. <code>  <html> <table> <tr><td>Header</td></tr> <tr><td>Want This</td></tr> </table></html> >>> doc = html5lib.parse('<html><table><tr><td>Header</td></tr><tr><td>Want This</td> </tr></table></html>', treebuilder='lxml')>>> doc<lxml.etree._ElementTree object at 0x1a1c290> >>> root = doc.getroot()>>> print(lxml.etree.tostring(root))<html:html xmlns:html=""http://www.w3.org/1999/xhtml""><html:head/><html:body><html:table><html:tbody><html:tr><html:td>Header</html:td></html:tr><html:tr><html:td>Want This</html:td></html:tr></html:tbody></html:table></html:body></html:html>","How can I parse HTML with html5lib, and query the parsed HTML with XPath?"
"Matplotlib, plotting discreet values "," I am trying to plot the following ! However, I keep getting a blank graph (?). Just to make sure that the program logic is correct I added the code print(x,y), just the confirm that (x,y) pairs are being generated. (x,y) pairs are being generated, but there is no plot, I keep getting a blank graph.Any help ?  <code>  from numpy import *from pylab import *import randomfor x in range(1,500): y = random.randint(1,25000) print(x,y) plot(x,y)show()",Matplotlib: plotting discrete values 
Python: Behavior of object in set operations," I'm trying to create a custom object that behaves properly in set operations.I've generally got it working, but I want to make sure I fully understand the implications. In particular, I'm interested in the behavior when there is additional data in the object that is not included in the equal / hash methods. It seems that in the 'intersection' operation, it returns the set of objects that are being compared to, where the 'union' operations returns the set of objects that are being compared.To illustrate: Is this behavior documented somewhere and deterministic? If so, what is the governing principle? <code>  class MyObject: def __init__(self,value,meta): self.value = value self.meta = meta def __eq__(self,other): return self.value == other.value def __hash__(self): return hash(self.value)a = MyObject('1','left')b = MyObject('1','right')c = MyObject('2','left')d = MyObject('2','right')e = MyObject('3','left')print a == b # Trueprint a == c # Falsefor i in set([a,c,e]).intersection(set([b,d])): print ""%s %s"" % (i.value,i.meta)#returns:#1 right#2 right for i in set([a,c,e]).union(set([b,d])): print ""%s %s"" % (i.value,i.meta)#returns:#1 left#3 left#2 left",Behavior of object in set operations
Displaying webcam feed using opencv and python," I have been trying to create a simple program with Python which uses OpenCV to get a video feed from my webcam and display it on the screen.I know I am partly there because the window is created and the light on my webcam flicks on, but it just doesn't seem to show anything in the window. Hopefully someone can explain what I'm doing wrong. On an unrelated note, I have noticed that my webcam sometimes changes its index number in cv.CaptureFromCAM, and sometimes I need to put in 0, 1 or 2 even though I only have one camera connected and I haven't unplugged it (I know because the light doesn't come on unless I change the index). Is there a way to get Python to determine the correct index? <code>  import cvcv.NamedWindow(""w1"", cv.CV_WINDOW_AUTOSIZE)capture = cv.CaptureFromCAM(0)def repeat(): frame = cv.QueryFrame(capture) cv.ShowImage(""w1"", frame)while True: repeat()",Displaying a webcam feed using OpenCV and Python
how to extract elements from a list in python ?," If you have a list in python, and want to extract elements at indices 1, 2 and 5 into a new list, how would you do this?This is how I did it, but I'm not very satisfied: Is there a better way?More in general, given a tuple of indices, how would you use this tuple to extract the corresponding elements from a list, even with duplication (e.g. tuple (1,1,2,1,5) produces [11,11,12,11,15]). <code>  >>> a[10, 11, 12, 13, 14, 15]>>> [x[1] for x in enumerate(a) if x[0] in [1,2,5]][11, 12, 15]",How to extract elements from a list using indices in Python?
How to extract elements from a list using indexes in Python?," If you have a list in python, and want to extract elements at indices 1, 2 and 5 into a new list, how would you do this?This is how I did it, but I'm not very satisfied: Is there a better way?More in general, given a tuple of indices, how would you use this tuple to extract the corresponding elements from a list, even with duplication (e.g. tuple (1,1,2,1,5) produces [11,11,12,11,15]). <code>  >>> a[10, 11, 12, 13, 14, 15]>>> [x[1] for x in enumerate(a) if x[0] in [1,2,5]][11, 12, 15]",How to extract elements from a list using indices in Python?
how to convert the Interger date format into YYYYMMDD?, Python and Matlab quite often have integer date representations as follows:733828.0733829.0733832.0733833.0733834.0733835.0733836.0733839.0733840.0733841.0these numbers correspond to some dates this year. Do you guys know which function can convert them back to YYYYMMDD format?thanks a million! <code> ,How to convert the integer date format into YYYYMMDD?
Read a file on App Eninge with Python?," Is it possible to open a file on GAE just to read its contents and get the last modified tag?I get a IOError: [Errno 13] file not accessible:I know that i cannot delete or update but i believe reading should be possibleHas anyone faced a similar problem? <code>  os.stat(f,'r').st_mtim",Read a file on App Engine with Python?
Count the number of files in a directory using python," I need to count the number of files in a directory using Python.I guess the easiest way is len(glob.glob('*')), but that also counts the directory itself as a file.Is there any way to count only the files in a directory? <code> ",How to count the number of files in a directory using Python
What is exactly binding means?," I always see people mention that ""Python binding"" and ""C Sharp binding"" etc. when I am actually using their C++ libraries. What does binding mean? If the library is written in C, and does Python binding means that they use SWIG kind of tool to mock a Python interface?Newbie in this field, and any suggestion will be welcomed.  <code> ",What does binding mean exactly?
how to run gwt application on private server?," I want to develop a GAE application using python, but I fear that Google will be the only company able to host the code. Is it possible to run a GAE app on a private server or other host?(Note that a previous version of the question incorrectly referred to GWT). <code> ",How can I run a GAE application on a private server?
sqlalchemy - Mapping self-referential relationship as one to many (declarative form)," I want to map a Tag entity using declarative method with SQLAlchemy. A tag can have a parent (another Tag).I have: How can I add the ""parent"" relationship? <code>  class Tag(Base): __tablename__ = 'tag' id = Column(Integer, primary_key=True) label = Column(String) def __init__(self, label, parentTag=None): self.label = label",SQLAlchemy - Mapping self-referential relationship as one to many (declarative form)
what is a good way to do countif in python," I want to count how many members of an iterable meet a given condition. I'd like to do it in a way that is clear and simple and preferably reasonably optimal.My current best ideas are: and The first one being iterator based is presumably faster for big lists. And it's the same form as you'd use for testing any and all. However it depends on the fact that int(True) == 1, which is somewhat ugly.The second one seems easier to read to me, but it is different from the any and all forms.Does anyone have any better suggestions? is there a library function somewhere that I am missing? <code>  sum(meets_condition(x) for x in my_list) len([x for x in my_list if meets_condition(x)])",What is a good way to do countif in Python
How to get REALLY fast python over a simple loop," I'm working on a SPOJ problem, INTEST. The goal is to specify the number of test cases (n) and a divisor (k), then feed your program n numbers. The program will accept each number on a newline of stdin and after receiving the nth number, will tell you how many were divisible by k.The only challenge in this problem is getting your code to be FAST because k can be anything up to 10^7 and n can be as high as 10^9.I'm trying to write it in Python and have trouble speeding it up. Any ideas?Edit 2: I finally got it to pass at 10.54 seconds. I used nearly all of your answers to get there, and thus it was hard to choose one as 'correct', but I believe the one I chose sums it up the best. Thanks to you all. Final passing code is below.Edit: I included some of the suggested updates in the included code.Extensions and third-party modules are not allowed. The code is also run by the SPOJ judge machine, so I do not have the option of changing interpreters. <code>  import sysimport psycopsyco.full()def main(): from sys import stdin, stdout first_in = stdin.readline() thing = first_in.split() n = int(thing[0]) k = int(thing[1]) total = 0 list = stdin.readlines() for item in list: if int(item) % k == 0: total += 1 stdout.write(str(total) + ""\n"")if __name__ == ""__main__"": main()",How to get REALLY fast Python over a simple loop
Why does python use 'magic methods'?," I've been playing around with Python recently, and one thing I'm finding a bit odd is the extensive use of 'magic methods', e.g. to make its length available, an object implements a method, def __len__(self), and then it is called when you write len(obj).I was just wondering why objects don't simply define a len(self) method and have it called directly as a member of the object, e.g. obj.len()? I'm sure there must be good reasons for Python doing it the way it does, but as a newbie I haven't worked out what they are yet. <code> ",Why does Python use 'magic methods'?
Twisted: how-to bind a server to a specified IP address?," I want to have a twisted service (started via twistd) which listens to TCP/POST request on a specified port on a specified IP address. By now I have a twisted application which listens to port 8040 on localhost. It is running fine, but I want it to only listen to a certain IP address, say 10.0.0.78.How-to manage that? This is a snippet of my code: <code>  application = service.Application('SMS_Inbound')smsInbound = resource.Resource()smsInbound.putChild('75sms_inbound',ReceiveSMS(application))smsInboundServer = internet.TCPServer(8001, webserver.Site(smsInbound))smsInboundServer.setName(""SMS Handling"")smsInboundServer.setServiceParent(application)",Twisted: how-to bind a server to a specified IP address? (solved)
Hot to log in to a website using installed twill?," I have just successfully installed TWILL on my computer with the help of one very supportive member of ""StackOverflow"" (you can check it out HERE) and have tried to run one of the simple examples on the twill documentation page (you can see that page HERE). Here is that example:(source: narod.ru) Let's say my username on www.slash.org is lynxye and my password is mammal. When I try to enter that example code into my Python prompt, I can only type and enter the first line of the code because when I click on ""Enter"" to start a new line, I get some error messages right away:(source: narod.ru) The same happens when I try to enter this code into my terminal:(source: narod.ru) I think I miss out on some basics here. Perhaps, I need to create a file that would contain that code and then run that file somehow, but I really don't know where I need to create that file and with what extensdion.Can anyone, please, help me with this? <code> ",How to log in to a website using installed twill?
In django changing the file name of uploading file.," Is it possible to change the file name of an uploaded file in django? I searched, but couldn't find any answer.My requirement is whenever a file is uploaded its file name should be changed in the following format. Thank you very much... <code>  format = userid + transaction_uuid + file_extension",How to change the file name of an uploaded file in Django?
In django changing the file name of uploading file," Is it possible to change the file name of an uploaded file in django? I searched, but couldn't find any answer.My requirement is whenever a file is uploaded its file name should be changed in the following format. Thank you very much... <code>  format = userid + transaction_uuid + file_extension",How to change the file name of an uploaded file in Django?
In django changing the file name of an uploaded file," Is it possible to change the file name of an uploaded file in django? I searched, but couldn't find any answer.My requirement is whenever a file is uploaded its file name should be changed in the following format. Thank you very much... <code>  format = userid + transaction_uuid + file_extension",How to change the file name of an uploaded file in Django?
Creating constant in Python," Is there a way to declare a constant in Python? In Java we can create constant values in this manner: What is the equivalent of the above Java constant declaration in Python? <code>  public static final String CONST_NAME = ""Name"";",How do I create a constant in Python?
How to create a constant in Python," Is there a way to declare a constant in Python? In Java we can create constant values in this manner: What is the equivalent of the above Java constant declaration in Python? <code>  public static final String CONST_NAME = ""Name"";",How do I create a constant in Python?
a simple smtp server," Could you please suggest a simple SMTP server with the very basic APIs (by very basic I mean, to read, write, delete email), that could be run on a linux box?I just need to convert the crux of the email into XML format and FTP it to another machine. <code> ",A simple SMTP server (in Python)
Get class of caller's method (via inspect) in Python," Trying to convert super(B, self).method() into a simple nice bubble() call.Did it, see below!Is it possible to get reference to class B in this example? Basically, C is child of B, B is child of A. Then we create c of type C. Then the call to c.test() actually calls B.test() (via inheritance), which calls to test2(). test2() can get the parent frame frame; code reference to method via frame.f_code; self via frame.f_locals['self']; but type(frame.f_locals['self']) is C (of course), but not B, where method is defined.Any way to get B? <code>  class A(object): passclass B(A): def test(self): test2()class C(B): passimport inspectdef test2(): frame = inspect.currentframe().f_back cls = frame.[?something here?] # cls here should == B (class)c = C()c.test()",super() in Python 2.x without args
"Get class of caller's method (via inspect) in Python; or: super(Class,self).method() replacement with bubble()"," Trying to convert super(B, self).method() into a simple nice bubble() call.Did it, see below!Is it possible to get reference to class B in this example? Basically, C is child of B, B is child of A. Then we create c of type C. Then the call to c.test() actually calls B.test() (via inheritance), which calls to test2(). test2() can get the parent frame frame; code reference to method via frame.f_code; self via frame.f_locals['self']; but type(frame.f_locals['self']) is C (of course), but not B, where method is defined.Any way to get B? <code>  class A(object): passclass B(A): def test(self): test2()class C(B): passimport inspectdef test2(): frame = inspect.currentframe().f_back cls = frame.[?something here?] # cls here should == B (class)c = C()c.test()",super() in Python 2.x without args
Get class of caller's method (via inspect) in Python (alt: super() emulator)," Trying to convert super(B, self).method() into a simple nice bubble() call.Did it, see below!Is it possible to get reference to class B in this example? Basically, C is child of B, B is child of A. Then we create c of type C. Then the call to c.test() actually calls B.test() (via inheritance), which calls to test2(). test2() can get the parent frame frame; code reference to method via frame.f_code; self via frame.f_locals['self']; but type(frame.f_locals['self']) is C (of course), but not B, where method is defined.Any way to get B? <code>  class A(object): passclass B(A): def test(self): test2()class C(B): passimport inspectdef test2(): frame = inspect.currentframe().f_back cls = frame.[?something here?] # cls here should == B (class)c = C()c.test()",super() in Python 2.x without args
super() in Python 2.x without args!," Trying to convert super(B, self).method() into a simple nice bubble() call.Did it, see below!Is it possible to get reference to class B in this example? Basically, C is child of B, B is child of A. Then we create c of type C. Then the call to c.test() actually calls B.test() (via inheritance), which calls to test2(). test2() can get the parent frame frame; code reference to method via frame.f_code; self via frame.f_locals['self']; but type(frame.f_locals['self']) is C (of course), but not B, where method is defined.Any way to get B? <code>  class A(object): passclass B(A): def test(self): test2()class C(B): passimport inspectdef test2(): frame = inspect.currentframe().f_back cls = frame.[?something here?] # cls here should == B (class)c = C()c.test()",super() in Python 2.x without args
Upgrading all packages with Pip, Is it possible to upgrade all Python packages at one time with pip?Note: that there is a feature request for this on the official issue tracker. <code> ,How to upgrade all Python packages with pip
Upgrading all packages with pip, Is it possible to upgrade all Python packages at one time with pip?Note: that there is a feature request for this on the official issue tracker. <code> ,How to upgrade all Python packages with pip
How to upgrade all python packages with pip?, Is it possible to upgrade all Python packages at one time with pip?Note: that there is a feature request for this on the official issue tracker. <code> ,How to upgrade all Python packages with pip
How to upgrade all Python packages with pip?, Is it possible to upgrade all Python packages at one time with pip?Note: that there is a feature request for this on the official issue tracker. <code> ,How to upgrade all Python packages with pip
how to create a CFuncType in python, I need to pass a callback function that is CFuncType (ctypes.CFUNCTYPE or ctypes.PYFUNCTYPE...).How can I cast a python function to CFuncType or how can I create a CFuncType function in python. <code> ,How to create a CFuncType in Python
Open source framework for distributed REST web queries?," I would like to create some sort of a distributed setup for running a ton of small/simple REST web queries in a production environment. For each 5-10 related queries which are executed from a node, I will generate a very small amount of derived data, which will need to be stored in a standard relational database (such as PostgreSQL).What platforms are built for this type of problem set? The nature, data sizes, and quantities seem to contradict the mindset of Hadoop. There are also more grid based architectures such as Condor and Sun Grid Engine, which I have seen mentioned. I'm not sure if these platforms have any recovery from errors though (checking if a job succeeds).What I would really like is a FIFO type queue that I could add jobs to, with the end result of my database getting updated.Any suggestions on the best tool for the job? <code> ",Solution for distributing MANY simple network tasks?
Can I compare a template variable to an integer in App Engine templates?," Using Django templates in Google App Engine (on Python), is it possible to compare a template variable to an integer in an {% if %} block?views.py: My template: This blows up with 'if' statement improperly formatted.What I was attempting to do in my template was build a simple if/elif/else tree to be grammatically correct to be able to state Browsing the Django template documents (this link provided in the GAE documentation appears to be for versions of Django far newer than what is supported on GAE), it appears as if I can only actually use boolean operators (if in fact boolean operators are supported in this older version of Django) with strings or other template variables.Is it not possible to compare variables to integers or non-strings with Django templates?I'm sure there is an easy way to workaround this - built up the message string on the Python side rather than within the template - but this seems like such a simple operation you ought to be able to handle in a template. It sounds like I should be switching to a more advanced templating engine, but as I am new to Django (templates or any part of it), I'd just like some confirmation first.  <code>  class MyHandler(webapp.RequestHandler): def get(self): foo_list = db.GqlQuery(...) ... template_values['foos'] = foo_list template_values['foo_count'] = len(foo_list) handler.response.out.write(template.render(...)) {% if foo_count == 1 %} There is one foo.{% endif %} #foo_count == 0:There are no foos.#foo_count == 1:There is one foo.#else:There are {{ foos|length }} foos.",Can I compare a template variable to an integer in Django/App Engine templates?
Muliple Models in a single django ModelForm?," Is it possible to have multiple models included in a single ModelForm in django? I am trying to create a profile edit form. So I need to include some fields from the User model and the UserProfile model. Currently I am using 2 forms like this Is there a way to consolidate these into one form or do I just need to create a form and handle the db loading and saving myself? <code>  class UserEditForm(ModelForm): class Meta: model = User fields = (""first_name"", ""last_name"")class UserProfileForm(ModelForm): class Meta: model = UserProfile fields = (""middle_name"", ""home_phone"", ""work_phone"", ""cell_phone"")",Multiple Models in a single django ModelForm?
Usabe of Python 3 super()," I wonder when to use what flavour of Python 3 super(). Until now I've used super() only without arguments and it worked as expected (by a Java developer). Questions:What does ""bound"" mean in this context? What is the difference between bound and unbound super object? When to use super(type, obj) and when super(type, type2)?Would it be better to name the super class like in Mother.__init__(...)? <code>  Help on class super in module builtins:class super(object) | super() -> same as super(__class__, <first argument>) | super(type) -> unbound super object | super(type, obj) -> bound super object; requires isinstance(obj, type) | super(type, type2) -> bound super object; requires issubclass(type2, type)",Which of the 4 ways to call super() in Python 3 to use?
Usage of Python 3 super()," I wonder when to use what flavour of Python 3 super(). Until now I've used super() only without arguments and it worked as expected (by a Java developer). Questions:What does ""bound"" mean in this context? What is the difference between bound and unbound super object? When to use super(type, obj) and when super(type, type2)?Would it be better to name the super class like in Mother.__init__(...)? <code>  Help on class super in module builtins:class super(object) | super() -> same as super(__class__, <first argument>) | super(type) -> unbound super object | super(type, obj) -> bound super object; requires isinstance(obj, type) | super(type, type2) -> bound super object; requires issubclass(type2, type)",Which of the 4 ways to call super() in Python 3 to use?
Tabulated log file format with the python logging system," I'm using the python logging module with the ""native""configuration file support (config.fileconfig) as describe in the documentation here :http://docs.python.org/library/logging.html (see the logging.conf file)I was wondering if it's possible to supply a tabulated data format in the configuration file:The sample configuration file is the following: I though that using the \t in the format would be enough but it doesn't: I tried a couple of things without success. I suppose it's really easy to do but I don't find it!How can I do that? <code>  [formatter_simpleFormatter] format=%(asctime)s - %(name)s - %(levelname)s - %(message)s format=%(asctime)s\t%(name)s\t%(levelname)s\t%(message)s\t ",using tabulation in Python logging format
how can I convert dictionary has string of keyword arguments?," we can convert the dictionary to kw using **kw but if I want kw as str(kw) not str(dict),as I want a string with keyword arguments for code_generator,if I pass I want a function to return the string like  <code>  obj.method(name='name', test='test', relation = [('id','=',1)]) ""name='name', test='test', relation = [('id','=',1)]""",how can I convert a dictionary to a string of keyword arguments?
Putting a simple if-then statement on one line," I'm just getting into Python and I really like the terseness of the syntax. However, is there an easier way of writing an if-then-else statement so it fits on one line?For example: Is there a simpler way of writing this? I mean, in Objective-C I would write this as: Is there something similar for Python?UpdateI know that in this instance I can use count == (count + 1) % N. I'm asking about the general syntax. <code>  if count == N: count = 0else: count = N + 1 count = count == N ? 0 : count + 1;",Putting a simple if-then-else statement on one line
Redirect logging output using custom logging handler," I'm using a module in my python app that writes a lot a of messages using the logging module. Initially I was using this in a console application and it was pretty easy to get the logging output to display on the console using a console handler. Now I've developed a GUI version of my app using wxPython and I'd like to display all the logging output to a custom control a multi-line textCtrl. Is there a way i could create a custom logging handler so i can redirect all the logging output there and display the logging messages wherever/however I want in this case, a wxPython app. <code> ",How can I redirect the logger to a wxPython textCtrl using a custom logging handler?
Python redirect logging output to wxPython textCtrl using custom logging handler," I'm using a module in my python app that writes a lot a of messages using the logging module. Initially I was using this in a console application and it was pretty easy to get the logging output to display on the console using a console handler. Now I've developed a GUI version of my app using wxPython and I'd like to display all the logging output to a custom control a multi-line textCtrl. Is there a way i could create a custom logging handler so i can redirect all the logging output there and display the logging messages wherever/however I want in this case, a wxPython app. <code> ",How can I redirect the logger to a wxPython textCtrl using a custom logging handler?
How to reverse a dictionary that it has repeated values (python)," I have a dictionary with almost 100,000 (key, value) pairs and the majority of the keys map to the same values. For example: What I want to do, is to reverse the dictionary so that each value in mydict is going to be a key at the reverse_dict and is going to map to a list of all the mydict.keys() that used to map to that value in mydict. So based on the example above I would get: I came up with a solution that is very expensive and I want to hear any ideas for doing this more efficiently than this: <code>  mydict = {'a': 1, 'c': 2, 'b': 1, 'e': 2, 'd': 3, 'h': 1, 'j': 3} reversed_dict = {1: ['a', 'b', 'h'], 2: ['c', 'e'] , 3: ['d', 'j']} reversed_dict = {}for value in mydict.values(): reversed_dict[value] = [] for key in mydict.keys(): if mydict[key] == value: if key not in reversed_dict[value]: reversed_dict[value].append(key)",How to reverse a dictionary that has repeated values
How to reverse a dictionary that it has repeated values," I have a dictionary with almost 100,000 (key, value) pairs and the majority of the keys map to the same values. For example: What I want to do, is to reverse the dictionary so that each value in mydict is going to be a key at the reverse_dict and is going to map to a list of all the mydict.keys() that used to map to that value in mydict. So based on the example above I would get: I came up with a solution that is very expensive and I want to hear any ideas for doing this more efficiently than this: <code>  mydict = {'a': 1, 'c': 2, 'b': 1, 'e': 2, 'd': 3, 'h': 1, 'j': 3} reversed_dict = {1: ['a', 'b', 'h'], 2: ['c', 'e'] , 3: ['d', 'j']} reversed_dict = {}for value in mydict.values(): reversed_dict[value] = [] for key in mydict.keys(): if mydict[key] == value: if key not in reversed_dict[value]: reversed_dict[value].append(key)",How to reverse a dictionary that has repeated values
Running a python script for a user-specified amount of time?," I've just started learning Python today. I've been reading a Byte of Python. Right now I have a project for Python that involves time. I can't find anything relating to time in Byte of Python, so I'll ask you:How can I run a block for a user specified amount of time and then break?For example (in some pseudo-code): or even better: <code>  time = int(raw_input('Enter the amount of seconds you want to run this: '))while there is still time left: #run this block import systime = sys.argv[1]while there is still time left: #run this block",Running a Python script for a user-specified amount of time
What's the fastest way to strip and replace a document of high ascii characters using Python?," I am looking to replace from a large document all high unicode characters, such as accented Es, left and right quotes, etc., with ""normal"" counterparts in the low range, such as a regular 'E', and straight quotes. I need to perform this on a very large document rather often. I see an example of this in what I think might be perl here: http://www.designmeme.com/mtplugins/lowdown.txtIs there a fast way of doing this in Python without using s.replace(...).replace(...).replace(...)...? I've tried this on just a few characters to replace and the document stripping became really slow.EDIT, my version of unutbu's code that doesn't seem to work: <code>  # -*- coding: iso-8859-15 -*-import unidecodedef ascii_map(): data={} for num in range(256): h=num filename='x{num:02x}'.format(num=num) try: mod = __import__('unidecode.'+filename, fromlist=True) except ImportError: pass else: for l,val in enumerate(mod.data): i=h<<8 i+=l if i >= 0x80: data[i]=unicode(val) return dataif __name__=='__main__': s = u'fancyfancy2' print(s.translate(ascii_map()))",What's the fastest way to strip and replace a document of high unicode characters using Python?
Best way to test class methods without running __init__," I've got a simple class that gets most of its arguments via init, which also runs a variety of private methods that do most of the work. Output is available either through access to object variables or public methods.Here's the problem - I'd like my unittest framework to directly call the private methods called by init with different data - without going through init. What's the best way to do this?So far, I've been refactoring these classes so that init does less and data is passed in separately. This makes testing easy, but I think the usability of the class suffers a little.EDIT: Example solution based on Ignacio's answer: <code>  import typesclass C(object): def __init__(self, number): new_number = self._foo(number) self._bar(new_number) def _foo(self, number): return number * 2 def _bar(self, number): print number * 10#--- normal execution - should print 160: -------MyC = C(8)#--- testing execution - should print 80 --------MyC = object.__new__(C)MyC._bar(8)",Best way to test instance methods without running __init__
"emacs: Inferior-mode python-shell appears ""lagged"""," I'm a Python(3.1.2)/emacs(23.2) newbie teaching myself tkinter using the pythonware tutorial found here. Relevant code is pasted below the question.Question: when I click the Hello button (which should call the say_hi function) why does the inferior python shell (i.e. the one I kicked off with C-c C-c) wait to execute the say_hi print function until I either a) click the Quit button or b) close the root widget down? When I try the same in IDLE, each click of the Hello button produces an immediate print in the IDLE python shell, even before I click Quit or close the root widget.Is there some quirk in the way emacs runs the Python shell (vs. IDLE) that causes this ""lagged"" behavior? I've noticed similar emacs lags vs. IDLE as I've worked through Project Euler problems, but this is the clearest example I've seen yet.FYI: I use python.el and have a relatively clean init.el...(setq python-python-command ""d:/bin/python31/python"")is the only line in my init.el.Thanks,Mike=== Begin Code=== <code>  from tkinter import *class App: def __init__(self,master): frame = Frame(master) frame.pack() self.button = Button(frame, text=""QUIT"", fg=""red"", command=frame.quit) self.button.pack(side=LEFT) self.hi_there = Button(frame, text=""Hello"", command=self.say_hi) self.hi_there.pack(side=LEFT) def say_hi(self): print(""hi there, everyone!"")root = Tk()app = App(root)root.mainloop()","Emacs: Inferior-mode python-shell appears ""lagged"""
Print each executed line in Python and Perl programs, I know that bash -x script.sh will execute script printing each line before actual execution.How to make Perl and Python interpreters do the same? <code> ,How can I make Perl and Python print each line of the program being executed?
regex-like processing on numeric data," Say I have some data like this: I want to process it looking for ""bumps"" that meet a certain pattern.Imagine I have my own customized regex language for working on numbers, where [[ >=5 ]] represents any number >= 5. I want to capture this case: In other words, I want to begin capturing any time I look ahead and see 3 or more values >= 5 in a row, and stop capturing any time I look ahead and see 2+ values < 3. So my output should be: Note that the first 7,8,... is ignored because it's not long enough, and that the capture ends before the 0,1,0....I'd also like a stream_processor object I can incrementally pass more data into in subsequent process calls, and return captured chunks as they're completed.I've written some code to do it, but it was hideous and state-machiney, and I can't help feeling like I'm missing something obvious. Any ideas to do this cleanly? <code>  number_stream = [0,0,0,7,8,0,0,2,5,6,10,11,10,13,5,0,1,0,...] ([[ >=5 ]]{3,})[[ <3 ]]{2,} >>> stream_processor.process(number_stream)[[5,6,10,11,10,13,5],...]",regex numeric data processing: match a series of numbers greater than X
Whats the deal with python?," My interests in programming lie mainly in algorithms, and lately I have seen many reputable researchers write a lot of their code in python. How easy and convenient is python for scientific computing? Does it have a library of algorithms that compares to matlab's? Is Python a scripting language or does it compile? Is it a great language for prototyping an algorithm? How long would it take me to learn enough of it to be productive provided I know C well and OO programming somewhat? Is it OO based?Sorry for the condensed format of questions, but I'm very curious and was hoping a more experienced programmer could help me out. <code> ",Is Python appropriate for algorithms focused on scientific computing?
Fuzzy string matching algorithm in Python," I'm trying to find some sort of a good, fuzzy string matching algorithm. Direct matching doesn't work for me this isn't too good because unless my strings are a 100% similar, the match fails. The Levenshtein method doesn't work too well for strings as it works on a character level. I was looking for something along the lines of word level matching e.g. String A: The quick brown fox. String B: The quick brown fox jumped over the lazy dog. These should match as all words in string A are in string B.Now, this is an oversimplified example but would anyone know a good, fuzzy string matching algorithm that works on a word level. <code> ",What is a simple fuzzy string matching algorithm in Python?
What's a good equivalent to python's subprocess.check_call that returns the contents of stdout?," I'd like a good method that matches the interface of subprocess.check_call -- ie, it throws CalledProcessError when it fails, is synchronous, &c -- but instead of returning the return code of the command (if it even does that) returns the program's output, either only stdout, or a tuple of (stdout, stderr).Does somebody have a method that does this? <code> ",What's a good equivalent to subprocess.check_call that returns the contents of stdout?
Kill process by name in python," I'm trying to kill a process (specifically iChat). On the command line, I use these commands: Then: However, I'm not exactly sure how to translate these commands over to Python. <code>  ps -A | grep iChat kill -9 PID",Kill process by name?
Kill process by name in Python," I'm trying to kill a process (specifically iChat). On the command line, I use these commands: Then: However, I'm not exactly sure how to translate these commands over to Python. <code>  ps -A | grep iChat kill -9 PID",Kill process by name?
Python: How best to parse a simple grammar?," Ok, so I've asked a bunch of smaller questions about this project, but I still don't have much confidence in the designs I'm coming up with, so I'm going to ask a question on a broader scale.I am parsing pre-requisite descriptions for a course catalog. The descriptions almost always follow a certain form, which makes me think I can parse most of them.From the text, I would like to generate a graph of course pre-requisite relationships. (That part will be easy, after I have parsed the data.)Some sample inputs and outputs: If the entire description is just a course, it is output directly.If the courses are conjoined (""and""), they are all output in the same listIf the course are disjoined (""or""), they are in separate listsHere, we have both ""and"" and ""or"".One caveat that makes it easier: it appears that the nesting of ""and""/""or"" phrases is never greater than as shown in example 3.What is the best way to do this? I started with PLY, but I couldn't figure out how to resolve the reduce/reduce conflicts. The advantage of PLY is that it's easy to manipulate what each parse rule generates: With PyParse, it's less clear how to modify the output of parseString(). I was considering building upon @Alex Martelli's idea of keeping state in an object and building up the output from that, but I'm not sure exactly how that is best done. For instance, to handle ""or"" cases: How does disjunctionCourses() know which smaller phrases to disjoin? All it gets is tokens, but what's been parsed so far is stored in result, so how can the function tell which data in result corresponds to which elements of token? I guess I could search through the tokens, then find an element of result with the same data, but that feel convoluted...Also, there are many descriptions that include misc text, like: But it isn't critical that I parse that text.What's a better way to approach this problem? <code>  ""CS 2110"" => (""CS"", 2110) # 0""CS 2110 and INFO 3300"" => [(""CS"", 2110), (""INFO"", 3300)] # 1""CS 2110, INFO 3300"" => [(""CS"", 2110), (""INFO"", 3300)] # 1""CS 2110, 3300, 3140"" => [(""CS"", 2110), (""CS"", 3300), (""CS"", 3140)] # 1""CS 2110 or INFO 3300"" => [[(""CS"", 2110)], [(""INFO"", 3300)]] # 2""MATH 2210, 2230, 2310, or 2940"" => [[(""MATH"", 2210), (""MATH"", 2230), (""MATH"", 2310)], [(""MATH"", 2940)]] # 3 def p_course(p): 'course : DEPT_CODE COURSE_NUMBER' p[0] = (p[1], int(p[2])) def addCourse(self, str, location, tokens): self.result.append((tokens[0][0], tokens[0][1])) def makeCourseList(self, str, location, tokens): dept = tokens[0][0] new_tokens = [(dept, tokens[0][1])] new_tokens.extend((dept, tok) for tok in tokens[1:]) self.result.append(new_tokens) def __init__(self): self.result = [] # ... self.statement = (course_data + Optional(OR_CONJ + course_data)).setParseAction(self.disjunctionCourses) def disjunctionCourses(self, str, location, tokens): if len(tokens) == 1: return tokens print ""disjunction tokens: %s"" % tokens ""CS 2110 or permission of instructor""""INFO 3140 or equivalent experience""""PYSCH 2210 and sophomore standing""",How best to parse a simple grammar?
Cross-compiling a Python script on Linux into a Windows executable," I have a Python script that I'd like to compile into a Windows executable. Now, py2exe works fine from Windows, but I'd like to be able to run this from Linux. I do have Windows on my development machine, but Linux is my primary dev platform and I'm getting kind of sick of rebooting into Windows just to create the .exe. Nor do I want to have to buy a second Windows license to run in a virtual machine such as VirtualBox. Any ideas?PS: I am aware that py2exe doesn't exactly compile the python file as much as package your script with the Python interpreter. But either way, the result is that you don't need Python installed to run the script. <code> ",Packaging a Python script on Linux into a Windows executable
Anything like bpython for Ruby?," IRb is pretty plain compared to bpython, even when using wirble.Is there any ruby equivalent of bpython? <code> ",Is there something like bpython for Ruby?
string count with overlapping occurances," What's the best way to count the number of occurrences of a given string, including overlap in Python? This is one way: This method returns 5.Is there a better way in Python? <code>  def function(string, str_to_search_for): count = 0 for x in xrange(len(string) - len(str_to_search_for) + 1): if string[x:x+len(str_to_search_for)] == str_to_search_for: count += 1 return countfunction('1011101111','11')",String count with overlapping occurrences
string count with overlapping occurrences," What's the best way to count the number of occurrences of a given string, including overlap in Python? This is one way: This method returns 5.Is there a better way in Python? <code>  def function(string, str_to_search_for): count = 0 for x in xrange(len(string) - len(str_to_search_for) + 1): if string[x:x+len(str_to_search_for)] == str_to_search_for: count += 1 return countfunction('1011101111','11')",String count with overlapping occurrences
pythonic way to do something N times," Every day I love python more and more. Today, I was writing some code like: I had to do something N times. But each time didn't depend on the value of i (index variable).I realized that I was creating a variable I never used (i), and I thought ""There surely is a more pythonic way of doing this without the need for that useless index variable.""So... the question is: do you know how to do this simple task in a more (pythonic) beautiful way? <code>  for i in xrange(N): do_something()",pythonic way to do something N times without an index variable?
List filtering: list comprehension vs. lambda + filter," I happened to find myself having a basic filtering need: I have a list and I have to filter it by an attribute of the items.My code looked like this: But then I thought, wouldn't it be better to write it like this? It's more readable, and if needed for performance the lambda could be taken out to gain something. Question is: are there any caveats in using the second way? Any performance difference? Am I missing the Pythonic Way entirely and should do it in yet another way (such as using itemgetter instead of the lambda)? <code>  my_list = [x for x in my_list if x.attribute == value] my_list = filter(lambda x: x.attribute == value, my_list)",List comprehension vs. lambda + filter
list comprehension vs. lambda + filter," I happened to find myself having a basic filtering need: I have a list and I have to filter it by an attribute of the items.My code looked like this: But then I thought, wouldn't it be better to write it like this? It's more readable, and if needed for performance the lambda could be taken out to gain something. Question is: are there any caveats in using the second way? Any performance difference? Am I missing the Pythonic Way entirely and should do it in yet another way (such as using itemgetter instead of the lambda)? <code>  my_list = [x for x in my_list if x.attribute == value] my_list = filter(lambda x: x.attribute == value, my_list)",List comprehension vs. lambda + filter
list comprehension vs. lambda + filter," I happened to find myself having a basic filtering need: I have a list and I have to filter it by an attribute of the items.My code looked like this: But then I thought, wouldn't it be better to write it like this? It's more readable, and if needed for performance the lambda could be taken out to gain something. Question is: are there any caveats in using the second way? Any performance difference? Am I missing the Pythonic Way entirely and should do it in yet another way (such as using itemgetter instead of the lambda)? <code>  my_list = [x for x in my_list if x.attribute == value] my_list = filter(lambda x: x.attribute == value, my_list)",List comprehension vs. lambda + filter
Python: Catching / blocking SIGINT during system call," I've written a web crawler that I'd like to be able to stop via the keyboard. I don't want the program to die when I interrupt it; it needs to flush its data to disk first. I also don't want to catch KeyboardInterruptedException, because the persistent data could be in an inconsistent state.My current solution is to define a signal handler that catches SIGINT and sets a flag; each iteration of the main loop checks this flag before processing the next url.However, I've found that if the system happens to be executing socket.recv() when I send the interrupt, I get this: and the process exits completely. Why does this happen? Is there a way I can prevent the interrupt from affecting the system call? <code>  ^CInterrupted; stopping... // indicates my interrupt handler ranTraceback (most recent call last): File ""crawler_test.py"", line 154, in <module> main() ... File ""/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/socket.py"", line 397, in readline data = recv(1)socket.error: [Errno 4] Interrupted system call",Catching / blocking SIGINT during system call
"Using an index to get an item, Python  "," I have a list in python ('A','B','C','D','E'), how do I get which item is under a particular index number?Example:Say it was given 0, it would return A.Given 2, it would return C.Given 4, it would return E. <code> ",Using an index to get an item
"Using an index to get an item, Python"," I have a list in python ('A','B','C','D','E'), how do I get which item is under a particular index number?Example:Say it was given 0, it would return A.Given 2, it would return C.Given 4, it would return E. <code> ",Using an index to get an item
"python: create a ""with"" block on several context managers"," Suppose you have three objects you acquire via context manager, for instance A lock, a db connection and an ip socket.You can acquire them by: But is there a way to do it in one block? something like Furthermore, is it possible, given an array of unknown length of objects that have context managers, is it possible to somehow do: If the answer is ""no"", is it because the need for such a feature implies bad design, or maybe I should suggest it in a pep? :-P <code>  with lock: with db_con: with socket: #do stuff with lock,db_con,socket: #do stuff a=[lock1, lock2, lock3, db_con1, socket, db_con2]with a as res: #now all objects in array are acquired","Create a ""with"" block on several context managers?"
Binomial test in Python," I need to do a binomial test in Python that allows calculation for 'n' numbers of the order of 10000.I have implemented a quick binomial_test function using scipy.misc.comb, however, it is pretty much limited around n = 1000, I guess because it reaches the biggest representable number while computing factorials or the combinatorial itself. Here is my function: How could I use a native python (or numpy, scipy...) function in order to calculate that binomial probability? If possible, I need scipy 0.7.2 compatible code.Many thanks! <code>  from scipy.misc import combdef binomial_test(n, k): """"""Calculate binomial probability """""" p = comb(n, k) * 0.5**k * 0.5**(n-k) return p",Binomial test in Python for very large numbers
Massage with BeatifulSoup or clean with Regex, Could someone tell me whats a better way to clean up bad HTML so BeautifulSoup can handle it - should one use the massage methods of BeautifulSoup or clean it up using regular expressions? <code> ,Massage with BeautifulSoup or clean with Regex
python ? (conditional/tenery) operator for assignments," C and many other languages have a conditional (AKA ternary) operator. This allows you to make very terse choices between two values based on the truth of a condition, which makes expressions, including assignments, very concise.I miss this because I find that my code has lots of conditional assignments that take four lines in Python: Whereas in C it'd be: Once or twice in a file is fine, but if you have lots of conditional assignments, the number of lines explode, and worst of all the eye is drawn to them.I like the terseness of the conditional operator, because it keeps things I deem un-strategic from distracting me when skimming the code.So, in Python, is there a trick you can use to get the assignment onto a single line to approximate the advantages of the conditional operator as I outlined them? <code>  if condition: var = somethingelse: var = something_else var = condition ? something : something_else;",Python ? (conditional/ternary) operator for assignments
python ? (conditional/ternary) operator for assignments," C and many other languages have a conditional (AKA ternary) operator. This allows you to make very terse choices between two values based on the truth of a condition, which makes expressions, including assignments, very concise.I miss this because I find that my code has lots of conditional assignments that take four lines in Python: Whereas in C it'd be: Once or twice in a file is fine, but if you have lots of conditional assignments, the number of lines explode, and worst of all the eye is drawn to them.I like the terseness of the conditional operator, because it keeps things I deem un-strategic from distracting me when skimming the code.So, in Python, is there a trick you can use to get the assignment onto a single line to approximate the advantages of the conditional operator as I outlined them? <code>  if condition: var = somethingelse: var = something_else var = condition ? something : something_else;",Python ? (conditional/ternary) operator for assignments
"python ctypes.WinDLL error , _dlopen(self._name, mode) cant be found"," How can I solve it? I found the _dlopen in C:\Python26\lib\ctypes\__init__.py, but I really don't know how to solve it. <code>  ctypes.WinDLL(""C:\Program Files\AHSDK\bin\ahscript.dll"")Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""C:\Python26\lib\ctypes\__init__.py"", line 353, in __init__ self._handle = _dlopen(self._name, mode)WindowsError: [Error 126] The specified module could not be found","Python ctypes.WinDLL error , _dlopen(self._name, mode) can't be found"
"Conquering Complexity, Eckel on Java and Python"," In the introduction to Bruce Eckel's Thinking In Java, he says, in 1998: Programming is about managing complexity: the complexity of the problem you want to solve, laid upon the complexity of the machine in which it is solved. Because of this complexity, most of our programming projects fail. And yet, of all the programming languages of which I am aware, none of them have gone all-out and decided that their main design goal would be to conquer the complexity of developing and maintaining programs.In the second and later editions he adds this footnote (circa 2003): I take this back on the 2nd edition: I believe that the Python language comes closest to doing exactly that. See www.Python.org.I am a dabbler with java, with a background in Delphi (Pascal), C, C++, and Python.Here is what I want to know:What exactly did Eckel consider when he called Python 'better' at conquering complexity, and are his thoughts on track with others who have used both? What do you think about conquering complexity? Is the shorter and more terse syntax of Python a key way to conquer complexity (and thus, for instance, Jython might be a nice bridge of Java's great libraries, and Python's terse syntax), or is the strong-typing-mentality of Java, which inherits this idea from C++, which inherited that idea from simula, I think it was, a key to conquering complexity? Or is it the Rapid Application Designer (think Delphi, or for Java, the excellent free NetBeans window/form designer tools) or components, or beans, or J2EE? What conquers all, for you?This is already tagged subjective. [edit]Note: More on Bruce's thoughts, on why he loves Python are found here. A key quote from the article: Bruce Eckel: They say you can hold seven plus or minus two pieces of information in your mind. I can't remember how to open files in Java. I've written chapters on it. I've done it a bunch of times, but it's too many steps. And when I actually analyze it, I realize these are just silly design decisions that they made. Even if they insisted on using the Decorator pattern in java.io, they should have had a convenience constructor for opening files simply. Because we open files all the time, but nobody can remember how. It is too much information to hold in your mind.So, chunk theory. By the chunk theory metric, Python kills everybody else dead. I'll grant him that. But what is the metric you use? I would like to particularly invite people to stand up for Java, and oppose Bruce, if you care to.[Please do not vote to re-open, this subject is inherently incendiary, and my gaffes have made it more-so. I agree with the moderators.]  <code> ","Conquering Complexity, Eckel on Java and Python and Chunk Theory"
Google app engine bulkloader problem when using yaml autogenerated configuration and enities with numeric ID," My application uses Django non-rel. I don't have access to model. I have my bulkloader.yaml file autogenerated by appcfg.py create_bulkloader_config. Problem is entities numeric ID's are being imported as string key names. So if I export entity with int ID of, for example, '62', it gets imported as entity with string key name of '61' which screws up Django.Revelant bulkloader.yaml Fragment: I'm trying to setup download/upload od data using bulkloader, and I want to have data as easy to understand format (like .csv) --- so using bulkloader.py --dump (...) is not a viable option since it gives me sqlite3 files that have entities contents pickled as a single row. EDITI tried doing what @Nick suggested and I got an exception: Does this mean that I have to stick to bulkloader.py (that uses that werid sqlite format) or I messed something? ;)Header of Transformer: Whole Stacktrace:  <code>  property_map: - property: __key__ external_name: key export_transform: transform.key_id_or_name_as_string ErrorOnTransform: Numeric keys are not supported on input at this time. - kind: auth_user connector: csv connector_options: encoding: utf-8 skip_import_header_row: True print_export_header_row: True property_map: - property: __key__ external_name: key export_transform: transform.key_id_or_name_as_string import_transform: transform.create_foreign_key('auth_user', key_is_id=True) Traceback (most recent call last): File ""/opt/google/appengine/google/appengine/tools/adaptive_thread_pool.py"", line 150, in WorkOnItems status, instruction = item.PerformWork(self.__thread_pool) File ""/opt/google/appengine/google/appengine/tools/bulkloader.py"", line 693, in PerformWork transfer_time = self._TransferItem(thread_pool) File ""/opt/google/appengine/google/appengine/tools/bulkloader.py"", line 848, in _TransferItem self.content = self.request_manager.EncodeContent(self.rows) File ""/opt/google/appengine/google/appengine/tools/bulkloader.py"", line 1269, in EncodeContent entity = loader.create_entity(values, key_name=key, parent=parent) File ""/opt/google/appengine/google/appengine/ext/bulkload/bulkloader_config.py"", line 385, in create_entity return self.dict_to_entity(input_dict, self.bulkload_state) File ""/opt/google/appengine/google/appengine/ext/bulkload/bulkloader_config.py"", line 131, in dict_to_entity instance = self.__create_instance(input_dict, bulkload_state_copy) File ""/opt/google/appengine/google/appengine/ext/bulkload/bulkloader_config.py"", line 209, in __create_instance 'Numeric keys are not supported on input at this time.')",Google App Engine bulkloader issue when using yaml autogenerated configuration and entities with numeric ID
Google App Engine bulkloader issue when using yaml autogenerated configuration and enities with numeric ID," My application uses Django non-rel. I don't have access to model. I have my bulkloader.yaml file autogenerated by appcfg.py create_bulkloader_config. Problem is entities numeric ID's are being imported as string key names. So if I export entity with int ID of, for example, '62', it gets imported as entity with string key name of '61' which screws up Django.Revelant bulkloader.yaml Fragment: I'm trying to setup download/upload od data using bulkloader, and I want to have data as easy to understand format (like .csv) --- so using bulkloader.py --dump (...) is not a viable option since it gives me sqlite3 files that have entities contents pickled as a single row. EDITI tried doing what @Nick suggested and I got an exception: Does this mean that I have to stick to bulkloader.py (that uses that werid sqlite format) or I messed something? ;)Header of Transformer: Whole Stacktrace:  <code>  property_map: - property: __key__ external_name: key export_transform: transform.key_id_or_name_as_string ErrorOnTransform: Numeric keys are not supported on input at this time. - kind: auth_user connector: csv connector_options: encoding: utf-8 skip_import_header_row: True print_export_header_row: True property_map: - property: __key__ external_name: key export_transform: transform.key_id_or_name_as_string import_transform: transform.create_foreign_key('auth_user', key_is_id=True) Traceback (most recent call last): File ""/opt/google/appengine/google/appengine/tools/adaptive_thread_pool.py"", line 150, in WorkOnItems status, instruction = item.PerformWork(self.__thread_pool) File ""/opt/google/appengine/google/appengine/tools/bulkloader.py"", line 693, in PerformWork transfer_time = self._TransferItem(thread_pool) File ""/opt/google/appengine/google/appengine/tools/bulkloader.py"", line 848, in _TransferItem self.content = self.request_manager.EncodeContent(self.rows) File ""/opt/google/appengine/google/appengine/tools/bulkloader.py"", line 1269, in EncodeContent entity = loader.create_entity(values, key_name=key, parent=parent) File ""/opt/google/appengine/google/appengine/ext/bulkload/bulkloader_config.py"", line 385, in create_entity return self.dict_to_entity(input_dict, self.bulkload_state) File ""/opt/google/appengine/google/appengine/ext/bulkload/bulkloader_config.py"", line 131, in dict_to_entity instance = self.__create_instance(input_dict, bulkload_state_copy) File ""/opt/google/appengine/google/appengine/ext/bulkload/bulkloader_config.py"", line 209, in __create_instance 'Numeric keys are not supported on input at this time.')",Google App Engine bulkloader issue when using yaml autogenerated configuration and entities with numeric ID
How do I get Monitor resolution in Python? , What is the simplest way to get monitor resolution (preferably in a tuple)?  <code> ,How do I get monitor resolution in Python?
How do i pipe output of file to a variable in python?, How do I pipe the output of file to a variable in Python?Is it possible? Say to pipe the output of netstat to a variable x in Python? <code> ,How do I pipe the output of file to a variable in Python?
concatenating bytes with a string," I'm working on a python project in 2.6 that also has future support for python 3 being worked in. Specifically I'm working on a digest-md5 algorithm. In python 2.6 without running this import: I am able to write a piece of code such as this: Without any issues, my authentication works fine. When I try the same line of code with the unicode_literals imported I get an exception: UnicodeDecodeError: 'utf8' codec can't decode byte 0xa8 in position 0: unexpected code byte Now I'm relatively new to python so I'm a bit stuck in figuring this out. if I replace the %s in the formatting string as %r I am able to concatenate the string, but the authentication doesn't work. The digest-md5 spec that I had read says that the 16 octet binary digest must be appended to these other strings.Any thoughts? <code>  from __future__ import unicode_literals a1 = hashlib.md5(""%s:%s:%s"" % (self.username, self.domain, self.password)).digest() a1 = ""%s:%s:%s"" %(a1, challenge[""nonce""], cnonce )",Python: concatenating bytes with a string
"Is there a Python version of Lisp's format ""~r""?"," How do pythonistas print a number as words, like the equivalent of the Common Lisp code: <code>  [3]> (format t ""~r"" 1e25)nine septillion, nine hundred and ninety-nine sextillion, nine hundred and ninety-nine quintillion, seven hundred and seventy-eight quadrillion, one hundred and ninety-six trillion, three hundred and eight billion, three hundred and sixty-one million, two hundred and sixteen thousand",Verbally format a number in Python
How often dos python flush to a file?," How often does Python flush to a file?How often does Python flush to stdout?I'm unsure about (1).As for (2), I believe Python flushes to stdout after every new line. But, if you overload stdout to be to a file, does it flush as often? <code> ",How often does python flush to a file?
Emulate SSH sever for testing purposes," I have to write test for deployment script which uploads files through SSH, but I'd like to have it not depending on external servers configuration. This is how i see it:Create 2 SSH daemons without authentication on different ports of loopback interface.Run the deployment script on these two portsThe only question is how to run these dummy SSH daemons.I use Python and Fabric. <code> ",Emulate SSH server for testing purposes
How exactly does evaluation work in a python interactive shell?," What happens internally when I press Enter?My motivation for asking, besides plain curiosity, is to figure out what happens when you and enter an expression. How does it go from Enter to calling in sympy.core.decorators? (That's the first place winpdb took me when I tried inspecting an evaluation.) I would guess that there is some built-in eval function that gets called normally, and is overridden when you import sympy? <code>  from sympy import * __sympifyit_wrapper(a,b)","How does sympy work?  How does it interact with the interactive Python shell, and how does the interactive Python shell work?"
Python: Searching/reading binary data," I'm reading in a binary file (a jpg in this case), and need to find some values in that file. For those interested, the binary file is a jpg and I'm attempting to pick out its dimensions by looking for the binary structure as detailed here. I need to find FFC0 in the binary data, skip ahead some number of bytes, and then read 4 bytes (this should give me the image dimensions).What's a good way of searching for the value in the binary data? Is there an equivalent of 'find', or something like re? <code> ",Searching/reading binary data in Python
Django: Attaching additional information to form fields, I'm trying to pass on additional information to fields of a Django form to be displayed in a template. I tried to override the constructor and add another property to the field like this: but in the template this: didn't print anything. Does anyone know how to add additional information to a field without rewriting/inheriting the forms field classes? <code>  self.fields['field_name'].foo = 'bar' {{ form.field_name.foo }},Attaching additional information to form fields
how to check variable against 2 possible values python," I have a variable s which contains a one letter string Depending on the value of that variable, I want to return different things. So far I am doing something along the lines of this: Is there a better way to write this? A more Pythonic way? Or is this the most efficient?Previously, I incorrectly had something like this: Obviously that doesn't work and was pretty dumb of me. I know of conditional assignment and have tried this: I guess my question is specifically to is there a way you can compare a variable to two values without having to type something == something or something == something <code>  s = 'a' if s == 'a' or s == 'b': return 1elif s == 'c' or s == 'd': return 2else: return 3 if s == 'a' or 'b': ... return 1 if s == 'a' or s == 'b' ...",How to check variable against 2 possible values?
"wxPython: ""Super"" wx.SpinCtrl"," wx.SpinCtrl is limited to spinning across integers, and not floats. Therefore, I am building a wx.TextCtrl + wx.SpinButton combo class which enables me to spin across floats. I am able to size and layout both of them programmatically so that the combo looks exactly the same as an ordinary wx.SpinCtrl.I am subclassing this combo from the wx.TextCtrl because I want its parent panel to catch wx.EVT_TEXT events. I would appreciate if you can improve on this argument of mine.The wx.EVT_SPIN_UP and wx.EVT_SPIN_DOWN events from the wx.SpinButton are both internal implementations and the parent frame doesn't care about these events.Now, I just hit a brick wall. My combo class doesn't work well with sizers. After .Add()ing the combo class to a wx.GridBagSizer, only the wx.TextCtrl is laid out within the wx.GridBagSizer. The wx.SpinButton is left on its own by itself. The wx.EVT_SPIN* bindings work very well, though.My problem is the layout. How should I write the class if I want the wx.GridBagSizer to treat it as one widget?Here is my combo class code: <code>  class SpinnerSuper(wx.TextCtrl): def __init__(self, parent, max): wx.TextCtrl.__init__(self, parent=parent, size=(48, -1)) spin = wx.SpinButton(parent=parent, style=wx.SP_VERTICAL, size=(-1, 21)) self.OnInit() self.layout(spin) self.internalBindings(spin) self.SizerFlag = wx.ALIGN_CENTER self.min = 0 self.max = max def OnInit(self): self.SetValue(u""0.000"") def layout(self, spin): pos = self.GetPosition() size = self.GetSize() RightEdge = pos[0] + size[0] TopEdge = pos[1] - (spin.GetSize()[1]/2 - size[1]/2) spin.SetPosition((RightEdge, TopEdge)) def internalBindings(self, spin): spin.Bind(wx.EVT_SPIN_UP, self.handlerSpinUp(self), spin) spin.Bind(wx.EVT_SPIN_DOWN, self.handlerSpinDown(self), spin) def handlerSpinUp(CallerObject, *args): def handler(CallerObject, *data): text = data[0] prev = text.GetValue() next = float(prev) + 0.008 text.SetValue(""{0:0.3f}"".format(next)) return lambda event: handler(CallerObject, *args) def handlerSpinDown(CallerObject, *args): def handler(CallerObject, *data): text = data[0] prev = text.GetValue() next = float(prev) - 0.008 text.SetValue(""{0:0.3f}"".format(next)) return lambda event: handler(CallerObject, *args)","wxPython: ""Super"" wx.SpinCtrl with float values, layout inside sizer"
How does python's super() work with multiple inheritance?," I'm pretty much new in Python object oriented programming and I have troubleunderstanding the super() function (new style classes) especially when it comes to multiple inheritance.For example if you have something like: What I don't get is: will the Third() class inherit both constructor methods? If yes, then which one will be run with super() and why?And what if you want to run the other one? I know it has something to do with Python method resolution order (MRO). <code>  class First(object): def __init__(self): print ""first""class Second(object): def __init__(self): print ""second""class Third(First, Second): def __init__(self): super(Third, self).__init__() print ""that's it""",How does Python's super() work with multiple inheritance?
Comparing dates in Python," Here's a little snippet that I'm trying execute: I can't seem to compare the date and the datetime values. What would be the best way to compare these? Should I convert the datetime to date or vice-versa? How do i convert between them.(A small question but it seems to be a little confusing.) <code>  >>> from datetime import *>>> item_date = datetime.strptime('7/16/10', ""%m/%d/%y"")>>> from_date = date.today()-timedelta(days=3)>>> print type(item_date)<type 'datetime.datetime'>>>> print type(from_date)<type 'datetime.date'>>>> if item_date > from_date:... print 'item is newer'...Traceback (most recent call last): File ""<stdin>"", line 1, in <module>TypeError: can't compare datetime.datetime to datetime.date",How can I compare a date and a datetime in Python?
"Download a spreadsheet from Google Docs using Python (should be simple, right?)"," Can you produce a Python example of how to download a Google Docs spreadsheet given its key and worksheet ID (gid)? I can't.I've scoured versions 1, 2 and 3 of the API. I'm having no luck, I can't figure out their compilcated ATOM-like feeds API, the gdata.docs.service.DocsService._DownloadFile private method says that I'm unauthorized, and I don't want to write an entire Google Login authentication system myself. I'm about to stab myself in the face due to frustration.I have a few spreadsheets and I want to access them like so: Please save my face.Update 1: I've tried the following, but no combination of Download() or Export() seems to work. (Docs for DocsService here) <code>  username = 'mygooglelogin@gmail.com'password = getpass.getpass()def get_spreadsheet(key, gid=0): ... (help!) ...for row in get_spreadsheet('5a3c7f7dcee4b4f'): cell1, cell2, cell3 = row ... import gdata.docs.serviceimport getpassimport osimport tempfileimport csvdef get_csv(file_path): return csv.reader(file(file_path).readlines())def get_spreadsheet(key, gid=0): gd_client = gdata.docs.service.DocsService() gd_client.email = 'xxxxxxxxx@gmail.com' gd_client.password = getpass.getpass() gd_client.ssl = False gd_client.source = ""My Fancy Spreadsheet Downloader"" gd_client.ProgrammaticLogin() file_path = tempfile.mktemp(suffix='.csv') uri = 'http://docs.google.com/feeds/documents/private/full/%s' % key try: entry = gd_client.GetDocumentListEntry(uri) # XXXX - The following dies with RequestError ""Unauthorized"" gd_client.Download(entry, file_path) return get_csv(file_path) finally: try: os.remove(file_path) except OSError: pass",Download a spreadsheet from Google Docs using Python
creating an event filter," I am trying to enable the delete key in my treeview. This is what I have so far: I connect the eventfilter like this: Why do I need to pass self.dataTreeview when I create the filter? It doesn't work without it. <code>  class delkeyFilter(QObject): delkeyPressed = pyqtSignal() def eventFilter(self, obj, event): if event.type() == QEvent.KeyPress: if event.key() == Qt.Key_Delete: self.delkeyPressed.emit() print 'delkey pressed' return True return False filter = delkeyFilter(self.dataTreeView) self.dataTreeView.installEventFilter(filter)",Creating an event filter
Maximum recursion depth?," I have this tail recursive function here: It works up to n=997, then it just breaks and spits out a RecursionError: maximum recursion depth exceeded in comparison. Is this just a stack overflow? Is there a way to get around it? <code>  def recursive_function(n, sum): if n < 1: return sum else: return recursive_function(n-1, sum+n)c = 998print(recursive_function(c, 0))","What is the maximum recursion depth in Python, and how to increase it?"
Why Python is not full object-oriented ?," I want to know why Python is not fully object-oriented. For example, it does not support private, public, protected access level modifiers.What are the advantages and disadvantages of this? By these expressions, Python is suitable for what applications (Desktop, Scientific, Web or other)? <code> ",Why is Python not fully object-oriented?
Using Python class as a data container," Sometimes it makes sense to cluster related data together. I tend to do so with a dict, e.g., One of my colleagues prefers to create a class Note that we are not defining any class methods.I like to use a dict because I like to minimize the number of lines of code. My colleague thinks the code is more readable if you use a class, and it makes it easier to add methods to the class in the future.Which do you prefer and why? <code>  self.group = dict(a=1, b=2, c=3)print self.group['a'] class groupClass(object): def __init__(a, b, c): self.a = a self.b = b self.c = cself.group = groupClass(1, 2, 3)print self.group.a",Whether to use a class or a dict as a data container
Using a class as a data container," Sometimes it makes sense to cluster related data together. I tend to do so with a dict, e.g., One of my colleagues prefers to create a class Note that we are not defining any class methods.I like to use a dict because I like to minimize the number of lines of code. My colleague thinks the code is more readable if you use a class, and it makes it easier to add methods to the class in the future.Which do you prefer and why? <code>  self.group = dict(a=1, b=2, c=3)print self.group['a'] class groupClass(object): def __init__(a, b, c): self.a = a self.b = b self.c = cself.group = groupClass(1, 2, 3)print self.group.a",Whether to use a class or a dict as a data container
python dictionary is thread safe ?, Some stated that python dictionary is thread safe. Does it mean I can or cannot modify the items in a dictionary while iterating over it? <code> ,python dictionary is thread safe?
where to host a periodically running python or java service? ," I'm going to build a little service which monitors an IMAP email account and acts on the read messages. For this it just has to run every say 10 min, no external trigger required, but I want to host this service externally (so that I don't need to worry about up times.)To be machine independent I could write the service in Java or Python. What are good hosting providers for this? and which of the two languages is better supported? The service has either to run the whole time (and must do the waiting itself) or it has to be kicked off every 10 min. I guess most (web) hosts are geared towards request driven code (e.g. JSP) and I assume they shut down processes which run forever. Who offers hosting for user-written services like the one mentioned above? <code> ",Where to host a periodically running Python or Java service? 
trasform negative elements to zero withou loop," If I have an array like I want to set all the negative elements to zero: [2, 3, 0, 0, 3]. How to do it with numpy without an explicit for? I need to use the modified a in a computation, for example where b is another array with the same length of the original aConclusion 1.386299848560.516846179962 <- faster a.clip(min=0);0.6154260635380.94455790519751.7364809513 <code>  a = np.array([2, 3, -1, -4, 3]) c = a * b import numpy as npfrom time import timea = np.random.uniform(-1, 1, 20000000)t = time(); b = np.where(a>0, a, 0); print (""1. "", time() - t)a = np.random.uniform(-1, 1, 20000000)t = time(); b = a.clip(min=0); print (""2. "", time() - t)a = np.random.uniform(-1, 1, 20000000)t = time(); a[a < 0] = 0; print (""3. "", time() - t)a = np.random.uniform(-1, 1, 20000000)t = time(); a[np.where(a<0)] = 0; print (""4. "", time() - t)a = np.random.uniform(-1, 1, 20000000)t = time(); b = [max(x, 0) for x in a]; print (""5. "", time() - t)",How to transform negative elements to zero without a loop?
Transform negative elements to zero withou loop," If I have an array like I want to set all the negative elements to zero: [2, 3, 0, 0, 3]. How to do it with numpy without an explicit for? I need to use the modified a in a computation, for example where b is another array with the same length of the original aConclusion 1.386299848560.516846179962 <- faster a.clip(min=0);0.6154260635380.94455790519751.7364809513 <code>  a = np.array([2, 3, -1, -4, 3]) c = a * b import numpy as npfrom time import timea = np.random.uniform(-1, 1, 20000000)t = time(); b = np.where(a>0, a, 0); print (""1. "", time() - t)a = np.random.uniform(-1, 1, 20000000)t = time(); b = a.clip(min=0); print (""2. "", time() - t)a = np.random.uniform(-1, 1, 20000000)t = time(); a[a < 0] = 0; print (""3. "", time() - t)a = np.random.uniform(-1, 1, 20000000)t = time(); a[np.where(a<0)] = 0; print (""4. "", time() - t)a = np.random.uniform(-1, 1, 20000000)t = time(); b = [max(x, 0) for x in a]; print (""5. "", time() - t)",How to transform negative elements to zero without a loop?
Calling a python script from command line with out typing python first," Question: In command line, how do I call a python script without having to type python in front of the script's name? Is this even possible?Info:I wrote a handy script for accessing sqlite databases from command line, but I kind of don't like having to type ""python SQLsap args"" and would rather just type ""SQLsap args"". I don't know if this is even possible, but it would be good to know if it is. For more than just this program. <code> ","Calling a python script from command line without typing ""python"" first"
Hyperlink In Tkinter Text Widget?," I am re designing a portion of my current software project, and want to use hyperlinks instead of Buttons. I really didn't want to use a Text widget, but that is all I could find when I googled the subject. Anyway, I found an example of this, but keep getting this error: When I add this line of code (using the IDLE) The code for the module is located here and the code for the script is located hereAnyone have any ideas? The part that is giving problems says foreground=""blue"", which is known as a color in Tkinter, isn't it? <code>  TclError: bitmap ""blue"" not defined hyperlink = tkHyperlinkManager.HyperlinkManager(text)",Hyperlink in Tkinter Text widget?
Multiple character  replace with python," I need to replace some characters as follows: & \&, # \#, ...I coded as follows, but I guess there should be some better way. Any hints? <code>  strs = strs.replace('&', '\&')strs = strs.replace('#', '\#')...",Best way to replace multiple characters in a string?
Multiple character replace with Python," I need to replace some characters as follows: & \&, # \#, ...I coded as follows, but I guess there should be some better way. Any hints? <code>  strs = strs.replace('&', '\&')strs = strs.replace('#', '\#')...",Best way to replace multiple characters in a string?
Python: most pythonic way to check if an object is a number," Given an arbitrary python object, what's the best way to determine whether it is a number? Here is is defined as acts like a number in certain circumstances.For example, say you are writing a vector class. If given another vector, you want to find the dot product. If given a scalar, you want to scale the whole vector.Checking if something is int, float, long, bool is annoying and doesn't cover user-defined objects that might act like numbers. But, checking for __mul__, for example, isn't good enough because the vector class I just described would define __mul__, but it wouldn't be the kind of number I want. <code> ",What is the most pythonic way to check if an object is a number?
exception for notifying that subclass should implement a method in python," Suppose I want to create an abstract class in Python with some methods to be implemented by subclasses, for example: I'd like that if the base class is instantiated and its f() method called, when self.g() is called, that throws an exception telling you that a subclass should have implemented method g().What's the usual thing to do here? Should I raise a NotImplementedError? or is there a more specific way of doing it? <code>  class Base(): def f(self): print ""Hello."" self.g() print ""Bye!"" class A(Base): def g(self): print ""I am A""class B(Base): def g(self): print ""I am B""",Which Exception for notifying that subclass should implement a method?
[Numpy] read csv into record array?," I wonder if there is a direct way to import the contents of a CSV file into a record array, much in the way that R's read.table(), read.delim(), and read.csv() family imports data to R's data frame?Or is the best way to use csv.reader() and then apply something like numpy.core.records.fromrecords()? <code> ",How do I read CSV data into a record array in NumPy?
How to read csv into record array in numpy?," I wonder if there is a direct way to import the contents of a CSV file into a record array, much in the way that R's read.table(), read.delim(), and read.csv() family imports data to R's data frame?Or is the best way to use csv.reader() and then apply something like numpy.core.records.fromrecords()? <code> ",How do I read CSV data into a record array in NumPy?
how do i load this in python as hex string," i need to load the third column of this text file as a hex stringhttp://www.netmite.com/android/mydroid/1.6/external/skia/emoji/gmojiraw.txt how do i open the file so that i can get the third column as hex string: i also tried binary mode and hex mode, with no success. <code>  >>> open('gmojiraw.txt').read().split('\n')[0].split('\t')[2]'\\xF3\\xBE\\x80\\x80' '\xF3\xBE\x80\x80'",Converting a hex-string representation to actual bytes in Python
how in python to generate a random list of fixed length of values from given range?," How to generate a random (but unique and sorted) list of a fixed given length out of numbers of a given range in Python?Something like that: <code>  >>> list_length = 4>>> values_range = [1, 30]>>> random_list(list_length, values_range)[1, 6, 17, 29]>>> random_list(list_length, values_range)[5, 6, 22, 24]>>> random_list(3, [0, 11])[0, 7, 10]",How to generate a random list of fixed length of values from given range?
static method vs modeule function in python," So I have a class in a module that has some static methods. A couple of these static methods just do crc checks and stuff, and they're not really useful outside of the class (I would just make them private static methods in java or C++). I'm wondering if I should instead make them global class functions (outside of the class). Is there any benefit for doing it either way? The class is being imported by from module import class so I'm not worried about having those modules pulled in as well. But should I just make them class methods so that from module import * is safer or something? <code> ",Static method vs module function in python
Why doesn't this Boost ASIO code work?," This code is identical to the original udp async echo server, but with a different socket.The response is transmitted and showing in wireshark, but then an ICMP Port Unreachable error is sent back to the server. I'm trying to understand why because everything looks correct.You can copy this code directly into a source file e.g. server.cpp. and then compile with gcc server.cpp -lboost_systemRun it with a command like: ./a.out 35200 The reason I need this is because I have multiple threads receiving data from an input queue that is fed with a UDP server. Now I want those threads to be able to send responses directly but I can't get it working.If I use the original socket (i.e. socket_) in the async_send_to call then it works.Ok... here is the test client that doesn't work with the code above (but works with the original version from the asio examples). It's got me baffled. But at least I can use the C++ UDP client and it works. <code>  #include <cstdlib>#include <iostream>#include <boost/bind.hpp>#include <boost/asio.hpp>using boost::asio::ip::udp;class server{public: server(boost::asio::io_service& io_service, short port) : io_service_(io_service), socket_(io_service, udp::endpoint(udp::v4(), port)), socket2_(io_service, udp::endpoint(udp::v4(),0)) { socket_.async_receive_from( boost::asio::buffer(data_, max_length), sender_endpoint_, boost::bind(&server::handle_receive_from, this, boost::asio::placeholders::error, boost::asio::placeholders::bytes_transferred)); } void handle_receive_from(const boost::system::error_code& error, size_t bytes_recvd) { if (!error && bytes_recvd > 0) { // use a different socket... random source port. socket2_.async_send_to( boost::asio::buffer(data_, bytes_recvd), sender_endpoint_, boost::bind(&server::handle_send_to, this, boost::asio::placeholders::error, boost::asio::placeholders::bytes_transferred)); } else { socket_.async_receive_from( boost::asio::buffer(data_, max_length), sender_endpoint_, boost::bind(&server::handle_receive_from, this, boost::asio::placeholders::error, boost::asio::placeholders::bytes_transferred)); } } void handle_send_to(const boost::system::error_code& /*error*/, size_t /*bytes_sent*/) { // error_code shows success when checked here. But wireshark shows // an ICMP response with destination unreachable, port unreachable when run on // localhost. Haven't tried it across a network. socket_.async_receive_from( boost::asio::buffer(data_, max_length), sender_endpoint_, boost::bind(&server::handle_receive_from, this, boost::asio::placeholders::error, boost::asio::placeholders::bytes_transferred)); }private: boost::asio::io_service& io_service_; udp::socket socket_; udp::socket socket2_; udp::endpoint sender_endpoint_; enum { max_length = 1024 }; char data_[max_length];};int main(int argc, char* argv[]){ try { if (argc != 2) { std::cerr << ""Usage: async_udp_echo_server <port>\n""; return 1; } boost::asio::io_service io_service; using namespace std; // For atoi. server s(io_service, atoi(argv[1])); io_service.run(); } catch (std::exception& e) { std::cerr << ""Exception: "" << e.what() << ""\n""; } return 0;} #!/usr/bin/pythonimport socket, sys, time, structtextport = ""35200""host = ""localhost""if len(sys.argv) > 1: host = sys.argv[1]print ""Sending Data""s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)port = int(textport)s.connect((host, port))s.sendall(""Hello World"")#s.shutdown(1)print ""Looking for replies; press Ctrl-C or Ctrl-Break to stop.""while 1: buf = s.recv(1200) if not len(buf): break print ""Received: %s"" % buf",Why doesn't this Boost ASIO code work with this python client?
"Sending POST data items with the same name, using AppEngine"," I try to send POST data to a server using urlfetch in AppEngine. Some of these POST-data items has the same name, but with different values. However, in this example, the server seems to receieve only one item named data, with the value bar. How could I solve this problem? <code>  form_fields = { ""data"": ""foo"", ""data"": ""bar""}form_data = urllib.urlencode(form_fields)result = urlfetch.fetch(url=""http://www.foo.com/"", payload=form_data, method=urlfetch.POST, headers={'Content-Type': 'application/x-www-form-urlencoded'})","Sending multiple POST data items with the same name, using AppEngine"
How to make it shorter (Pythonic)? Python.," I have to check a lot of worlds if they are in string... code looks like: how to make it more readable and more clear?  <code>  if ""string_1"" in var_string or ""string_2"" in var_string or ""string_3"" in var_string or ""string_n"" in var_string: do_something()",How to make it shorter (Pythonic)?
"Python / Django, dynamic generation of a CSS file?"," I'm wondering if there is anything like Django's HTML templating system, for for CSS.. my searches on this aren't turning up anything of use. I am aware of things like SASS and CleverCSS but, as far as I can tell, these still don't solve my issue as I want to dynamically generate a CSS file based on certain conditions, so that a different CSS file would be served based on a specific user session...I want to minimize the use of javascript / AJAX for some things (since its for a legacy system running in some hospital where they're still using IE 6 ), also I have an interest in possibly minimizing javascript for other projects as well... so it would be where there is 1 CSS file, but that it may need to be changed based on the situation (which would be done with CleverCSS), however the problem is that if I just write the changes to 1 file, then this would be served to everyone, even though they may have a different ""state"" of the CSS file depending on their use of the application, so I want to remove the physical association of a CSS file and rather have it dynamically generated each time (so that its unique to a specific user's session), the way that Django's HTML templating system works.. <code> ",CSS Templating system for Django / Python?
"Python newbie: ""if X == Y and Z"" syntax"," Does this: mean the same as this: If so, I'm totally confused about example 5.14 in Dive Into Python. How can key be equal to both ""name"" and item? On the other hand, does ""and item"" simply ask whether or not item exists as a variable? <code>  if key == ""name"" and item: if key == ""name"" and if key == ""item"":","Python ""if X == Y and Z"" syntax"
how to executing modules as scripts," I am learn python now, and today, i met a problemin http://docs.python.org/release/2.5.4/tut/node8.html 6.1.1 Executing modules as scripts When you run a Python module with python fibo.py <arguments> the code in the module will be executed, just as if you imported it, but with the __name__ set to ""__main__"". That means that by adding this code at the end of your module: you can make the file usable as a script as well as an importable module, because the code that parses the command line only runs if the module is executed as the ""main"" file: $ python fibo.py 50 1 1 2 3 5 8 13 21 34but when i do this in shell, i got how to execute script correctly?fibo.py is <code>  if __name__ == ""__main__"": import sys` fib(int(sys.argv[1])) File ""<input>"", line 1python fibo.py 222SyntaxError: invalid syntax def fib(n): a,b=0,1 while b<n: print b, a,b = b,a+bdef fib2(n): result=[] a,b=0,1 while b<n: result.append(b) a,b=b,a+b return resultif __name__ ==""__main__"": import sys fib(int(sys.argv[1]))",Executing modules as scripts
python:how to executing modules as scripts," I am learn python now, and today, i met a problemin http://docs.python.org/release/2.5.4/tut/node8.html 6.1.1 Executing modules as scripts When you run a Python module with python fibo.py <arguments> the code in the module will be executed, just as if you imported it, but with the __name__ set to ""__main__"". That means that by adding this code at the end of your module: you can make the file usable as a script as well as an importable module, because the code that parses the command line only runs if the module is executed as the ""main"" file: $ python fibo.py 50 1 1 2 3 5 8 13 21 34but when i do this in shell, i got how to execute script correctly?fibo.py is <code>  if __name__ == ""__main__"": import sys` fib(int(sys.argv[1])) File ""<input>"", line 1python fibo.py 222SyntaxError: invalid syntax def fib(n): a,b=0,1 while b<n: print b, a,b = b,a+bdef fib2(n): result=[] a,b=0,1 while b<n: result.append(b) a,b=b,a+b return resultif __name__ ==""__main__"": import sys fib(int(sys.argv[1]))",Executing modules as scripts
find time difference in seconds as an interger with python, I need to find the time difference in seconds with python. I know I can get the difference like this: how do I get difference in total seconds? <code>  from datetime import datetimenow = datetime.now()............later = datetime.now()difference = later-now,find time difference in seconds as an integer with python
Add a margin to a tkinter window," So I have so far a simple python tkinter window and i'm adding text, buttons, etc.snippet: the problem is, I want to have about a 15-20 px margin around the window, I looked everywhere, and I couldn't find a solution. Also doesn't work. Any possible solutions? <code>  class Cfrm(Frame): def createWidgets(self): self.text = Text(self, width=50, height=10) self.text.insert('1.0', 'some text will be here') self.text.tag_configure('big', font=('Verdana', 24, 'bold')) self.text[""state""] = ""disabled"" self.text.grid(row=0, column=1) self.quitw = Button(self) self.quitw[""text""] = ""exit"", self.quitw[""command""] = self.quit self.quitw.grid(row=1, column=1) def __init__(self, master=None): Frame.__init__(self, master) self.pack() self.createWidgets() self.text.tag_configure('big', font=('Verdana', 24, 'bold'))",How to add a margin to a tkinter window
How to add a margin to a tkinter window?," So I have so far a simple python tkinter window and i'm adding text, buttons, etc.snippet: the problem is, I want to have about a 15-20 px margin around the window, I looked everywhere, and I couldn't find a solution. Also doesn't work. Any possible solutions? <code>  class Cfrm(Frame): def createWidgets(self): self.text = Text(self, width=50, height=10) self.text.insert('1.0', 'some text will be here') self.text.tag_configure('big', font=('Verdana', 24, 'bold')) self.text[""state""] = ""disabled"" self.text.grid(row=0, column=1) self.quitw = Button(self) self.quitw[""text""] = ""exit"", self.quitw[""command""] = self.quit self.quitw.grid(row=1, column=1) def __init__(self, master=None): Frame.__init__(self, master) self.pack() self.createWidgets() self.text.tag_configure('big', font=('Verdana', 24, 'bold'))",How to add a margin to a tkinter window
"Django / Python, how to check if user is logged in (how to properly use: user.is_authenticated )?"," I am looking over this website but just can't seem to figure out how to do this as it's not working. I need to check if the current site user is logged in (authenticated), and am trying: despite being sure that the user is logged in, it returns just: I'm able to do other requests (from the first section in the url above), such as: which returns a successful response. <code>  request.user.is_authenticated > request.user.is_active",How to check if a user is logged in (how to properly use user.is_authenticated)?
What is the purpose of __str__ and __repr__ in Python? ," I really don't understand where are __str__ and __repr__ used in Python. I mean, I get that __str__ returns the string representation of an object. But why would I need that? In what use case scenario? Also, I read about the usage of __repr__But what I don't understand is, where would I use them? <code> ",What is the purpose of __str__ and __repr__?
Trying to understand python with statement and context managers," I am trying to understand the with statement. I understand that it is supposed to replace the try/except block.Now suppose I do something like this: How do I replace this with a context manager? <code>  try: name = ""rubicon"" / 2 # to raise an exceptionexcept Exception as e: print(""No, not possible."")finally: print(""OK, I caught you."")",Understanding the Python with statement and context managers
Python: Initialize a datetime object with seconds since epoch," The time module can be initialized using seconds since epoch: Is there an elegant way to initialize a datetime.datetime object in the same way? <code>  >>> import time>>> t1=time.gmtime(1284286794)>>> t1time.struct_time(tm_year=2010, tm_mon=9, tm_mday=12, tm_hour=10, tm_min=19, tm_sec=54, tm_wday=6, tm_yday=255, tm_isdst=0)","In Python, how do you convert seconds since epoch to a `datetime` object?"
Initialize a datetime object with seconds since epoch," The time module can be initialized using seconds since epoch: Is there an elegant way to initialize a datetime.datetime object in the same way? <code>  >>> import time>>> t1=time.gmtime(1284286794)>>> t1time.struct_time(tm_year=2010, tm_mon=9, tm_mday=12, tm_hour=10, tm_min=19, tm_sec=54, tm_wday=6, tm_yday=255, tm_isdst=0)","In Python, how do you convert seconds since epoch to a `datetime` object?"
how to reverse color map image to scalar values," How do I invert a color mapped image?I have a 2D image which plots data on a colormap. I'd like to read the image in and 'reverse' the color map, that is, look up a specific RGB value, and turn it into a float. For example:using this image: http://matplotlib.sourceforge.net/_images/mri_demo.pngI should be able to get a 440x360 matrix of floats, knowing the colormap was cm.jet <code>  from pylab import imreadimport matplotlib.cm as cma=imread('mri_demo.png')b=colormap2float(a,cm.jet) #<-tricky part",How to reverse a color map image to scalar values?
Getting pdb in emacs to use python process from current virtualenv," I am debugging some python code in emacs using pdb and getting some import issues. The dependencies are installed in one of my bespoked virtualenv environments. Pdb is stubbornly using /usr/bin/python and not the python process from my virtualenv. I use virtualenv.el to support switching of environments within emacs and via the postactivate hooks described in http://jesselegg.com/archives/2010/03/14/emacs-python-programmers-2-virtualenv-ipython-daemon-mode/This works well when running M-x python-shell This points to all of my virtualenv libraries indicating that the python-shell is that of my virtualenv.This is contradicted however by M-! which python, which gives /usr/bin/pythonDoes anyone know how I can tell M-x pdb to adopt the python process from the currently active virtualenv?  <code>  >>> import sys>>> print sys.path ",Getting pdb in Emacs to use Python process from current virtualenv
Is there a Python equivalent to Ruby symbols? , Is there a Python equivalent to Ruby symbols? If so then what is it?If not then are we stuck with using strings as our keys in dictionaries only? <code> ,Is there a Python equivalent to Ruby symbols?
Declare function at end of file in Python.," Is it possible to call a function without first fully defining it? When attempting this I get the error: ""function_name is not defined"". I am coming from a C++ background so this issue stumps me. Declaring the function before works: However, attempting to call the function without first defining it gives trouble: In C++, you can declare a function after the call once you place its header before it.Am I missing something here? <code>  def Kerma(): return ""energy / mass"" print Kerma() print Kerma()def Kerma(): return ""energy / mass""",Declare function at end of file in Python
Is there a way to run a python script who its inside a zip file in bash?," I know there is a way to import modules which are in a zip file with python. I created kind of custom python package library in a zip file. I would like to put as well my ""task"" script in this package, those are using the library. Then, with bash, I would like to call the desired script in the zip file without extracting the zip.The goal is to have only one zip to move in a specified folder when I want to run my scripts. <code> ",Is there a way to run a python script that is inside a zip file from bash?
How to sort with lambda," In Python, I am trying to sort by date with lambda. I can't understand my error message. The message is: The line I have is <code>  <lambda>() takes exactly 1 argument (2 given) a = sorted(a, lambda x: x.modified, reverse=True)",How to sort with lambda in Python
Python: Advanced Nested List Comprehension Syntax," I was playing around with list comprehensions to get a better understanding of them and I ran into some unexpected output that I am not able to explain. I haven't found this question asked before, but if it /is/ a repeat question, I apologize.I was essentially trying to write a generator which generated generators. A simple generator that uses list comprehension would look like this: What I was trying to do was write a generator that generated two generators - the first of which generated the even numbers in range(10) and the second of which generated the odd numbers in range(10). For this, I did: I don't understand why 'i' is being referenced before assignmentI thought it might have had something to do with i in range(2), so I did: This didn't make sense to me, so I thought it best to try something simpler first. So I went back to lists and tried: which I expected to be the same as: But when I tried it on a hunch, this worked: So I thought it might be a problem with what level of scope the if statement operates in. So I tried this: And now I'm thoroughly confused. Can someone please explain this behavior. I don't understand why my list comprehensions seem to be malformed, nor do I understand how the scoping of the if statements work.PS: While proof-reading the question, I realized that this does look a bit like a homework question - it is not. <code>  (x for x in range(10) if x%2==0) # generates all even integers in range(10) >>> (x for x in range(10) if x%2==i for i in range(2))<generator object <genexpr> at 0x7f6b90948f00>>>> for i in g.next(): print i... Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 1, in <genexpr>UnboundLocalError: local variable 'i' referenced before assignment>>> g.next()Traceback (most recent call last): File ""<stdin>"", line 1, in <module>StopIteration>>> g = (x for x in range(10) if x%2==i for i in range(2))>>> g<generator object <genexpr> at 0x7f6b90969730>>>> g.next()Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 1, in <genexpr> UnboundLocalError: local variable 'i' referenced before assignment >>> g = (x for x in range(10) if x%2==i for i in [0.1])>>> g<generator object <genexpr> at 0x7f6b90948f00>>>> g.next()Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 1, in <genexpr>UnboundLocalError: local variable 'i' referenced before assignment >>> [x for x in range(10) if x%2==i for i in range(2)][1, 1, 3, 3, 5, 5, 7, 7, 9, 9] >>> l = []>>> for i in range(2):... for x in range(10):... if x%2==i:... l.append(x)... >>> l[0, 2, 4, 6, 8, 1, 3, 5, 7, 9] # so where is my list comprehension malformed? >>> [[x for x in range(10) if x%2==i] for i in range(2)][[0, 2, 4, 6, 8], [1, 3, 5, 7, 9]] # so nested lists in nested list comprehension somehow affect the scope of if statements? :S >>> [x for x in range(10) for i in range(2) if x%2==i][0, 1, 2, 3, 4, 5, 6, 7, 8, 9]",Advanced Nested List Comprehension Syntax
"Python, os.system for command-line call (linux) not returning what it should?"," I need to make some command line calls to linux and get the return from this, however doing it as below is just returning 0 when it should return a time value, like 00:08:19, I am testing the exact same call in regular command line and it returns the time value 00:08:19 so I am confused as to what I am doing wrong as I thought this was how to do it in python. <code>  import osretvalue = os.system(""ps -p 2993 -o time --no-headers"")print retvalue",Linux command-line call not returning what it should from os.system?
How to do a Python split() on languages (like Chinese) that don't use whtespace as word separator?," I want to split a sentence into a list of words.For English and European languages this is easy, just use split() But I also need to deal with sentences in languages such as Chinese that don't use whitespace as word separator. Obviously that doesn't work.How do I split such a sentence into a list of words?UPDATE:So far the answers seem to suggest that this requires natural language processing techniques and that the word boundaries in Chinese are ambiguous. I'm not sure I understand why. The word boundaries in Chinese seem very definite to me. Each Chinese word/character has a corresponding unicode and is displayed on screen as an separate word/character.So where does the ambiguity come from. As you can see in my Python console output Python has no problem telling that my example sentence is made up of 5 characters: So obviously Python has no problem telling the word/character boundaries. I just need those words/characters in a list. <code>  >>> ""This is a sentence."".split()['This', 'is', 'a', 'sentence.'] >>> u"""".split()[u'\u8fd9\u662f\u4e00\u4e2a\u53e5\u5b50'] - u8fd9 - u662f - u4e00 - u4e2a - u53e5 - u5b50",How to do a Python split() on languages (like Chinese) that don't use whitespace as word separator?
Python: Recursively access dict (get and set)," I'm building some Python code to read and manipulate deeply nested dicts (ultimately for interacting with JSON services, however it would be great to have for other purposes) I'm looking for a way to easily read/set/update values deep within the dict, without needing a lot of code. @see also Python: Recursively access dict via attributes as well as index access? -- Curt Hagenlocher's ""DotDictify"" solution is pretty eloquent. I also like what Ben Alman presents for JavaScript in http://benalman.com/projects/jquery-getobject-plugin/ It would be great to somehow combine the two.Building off of Curt Hagenlocher and Ben Alman's examples, it would be great in Python to have a capability like: Any idea if this is possible, and if so, how to go about modifying the DotDictify solution?Alternatively, the get method could be made to accept a dot notation (and a complementary set method added) however the object notation sure is cleaner. This type of interaction would be great to have for dealing with deeply nested dicts. Does anybody know another strategy (or sample code snippet/library) to try? <code>  >>> my_obj = DotDictify()>>> my_obj.a.b.c = {'d':1, 'e':2}>>> print my_obj{'a': {'b': {'c': {'d': 1, 'e': 2}}}}>>> print my_obj.a.b.c.d1>>> print my_obj.a.b.c.xNone>>> print my_obj.a.b.c.d.xNone>>> print my_obj.a.b.c.d.x.y.zNone >>> my_obj = DotDictify()>>> my_obj.set('a.b.c', {'d':1, 'e':2})>>> print my_obj{'a': {'b': {'c': {'d': 1, 'e': 2}}}}>>> print my_obj.get('a.b.c.d')1>>> print my_obj.get('a.b.c.x')None>>> print my_obj.get('a.b.c.d.x')None>>> print my_obj.get('a.b.c.d.x.y.z')None",Python: Easily access deeply nested dict (get and set)
Most elegant way to find node's parents with networkX ," I'm working on a graphical model project with python using NetworkX. NetworkX provides simple and good functionality using dictionaries: I want to use directed graphs because I am coding dependencies that have directions (in the above example I have the closed form for 'b' conditional on 'a', not the other way around). For a given node, I want to find the predecessors of that node. For the above example, par('b') should return ['a']. NetworkX does have a successor function, which finds the children of any node. Obviously, by going through all the nodes and finding those that have 'b' as a child will work, but it will be (n) in the number of nodes (which will be too expensive for my application).I cannot imagine that something this simple would be left out of this well-made package, but can't find anything. One efficient option is to store a directed and an undirected version of the graph; all undirected edges are essentially implemented by adding both directed edges, and so it would be possible to take the set-wise difference between the adjacent nodes and the children (which would be the predecessor). The trouble is I'm not sure of the most pythonic way to wrap the existing networkx DiGraph and Graph class to accomplish this. Really I just want to end up with a class PGraph that behaves exactly like the networkx DiGraph class but has a predecessors(node) function in addition to the successors(node) function. Should PGraph inherit from DiGraph and encapsulate Graph (for use in the predecessors function)? How then should I force all nodes and edges to be added to both the directed and undirected graphs that it contains? Should I just reimplement the functions for adding and removing nodes and edges in PGraph (so that they are added and removed from both the directed and undirected version)? I worry that if I miss something obscure I'll be in for a headache later, which may not imply good design.Or (and please let this be True) is there simply an easy way to get a node's predecessors in a networkx.DiGraph and I've completely missed it?Thanks a lot for your help.EDIT:I think this does the job. PGraph inherits from DiGraph and encapsulates another DiGraph (this one reversed). I've overridden the methods to add & remove nodes & edges. What do you think about this solution? It might double the memory usage, but I think that's acceptable. Is it too complicated? Is this good design? <code>  import networkx as nxG = nx.DiGraph() # a directed graphG.add_edge('a', 'b')print G['a'] # prints {'b': {}}print G['b'] # prints {} import networkx as nxclass PGraph(nx.DiGraph): def __init__(self): nx.DiGraph.__init__(self) self.reversed_graph = nx.DiGraph() def add_node(self, n, attr_dict=None, **attr): nx.DiGraph.add_node(self, n, attr_dict, **attr) self.reversed_graph.add_node(n, attr_dict, **attr) def add_nodes_from(self, ns, attr_dict=None, **attr): nx.DiGraph.add_nodes_from(self, ns, attr_dict, **attr) self.reversed_graph.add_nodes_from(ns, attr_dict, **attr) def add_edge(self, a, b, attr_dict=None, **attr): nx.DiGraph.add_edge(self, a, b, attr_dict, **attr) self.reversed_graph.add_edge(b, a, attr_dict, **attr) def add_edges_from(self, es, attr_dict=None, **attr): nx.DiGraph.add_edges_from(self, es, attr_dict, **attr) self.reversed_graph.add_edges_from(es, attr_dict, **attr) def remove_node(self, n): nx.DiGraph.remove_node(self, n) self.reversed_graph.remove_node(n) def remove_nodes_from(self, ns): nx.DiGraph.remove_nodes_from(self, ns) self.reversed_graph.remove_nodes_from(ns) def remove_edge(self, a, b): nx.DiGraph.remove_edge(self, b, a) self.reversed_graph.remove_edge(a, b) def remove_edges_from(self, es): nx.DiGraph.remove_edges_from(self, es) self.reversed_graph.remove_edges_from([ (b,a) for a,b in es])# the predecessors function I wanted def predecessors(self, n): return self.reversed_graph.successors(n)",Most elegant way to find node's predecessors with networkX 
"why we need sys.setdefaultencoding(""utf-8"") in py scipt "," I have seen few py scripts which use this at the top of the script. In what cases one should use it? <code>  import sysreload(sys)sys.setdefaultencoding(""utf-8"")","Why should we NOT use sys.setdefaultencoding(""utf-8"") in a py script?"
"Why we need sys.setdefaultencoding(""utf-8"") in a py script?"," I have seen few py scripts which use this at the top of the script. In what cases one should use it? <code>  import sysreload(sys)sys.setdefaultencoding(""utf-8"")","Why should we NOT use sys.setdefaultencoding(""utf-8"") in a py script?"
Django-Python: Generate XML file from model data," I need to write model data (CharFields only) to an XML file to contain the data for a flash file. I am new to this, and the process is a little unclear to me for doing this in django. I am creating an xml file, and then writing the text data to the file (as is done with the csv module, but to xml). A very simplified xml file should result for the flash file to read, ie: 1. I am using a serializer to write the xml data from the model: 2. I then create file using core.files: 3. Write File data and close: This works so far, although the xml output contains the fields for the object ""django-objects"" etc, and I will have to see if I can interpret this in ActionScript easily for the flash file. I would prefer to define the xml field names manually like in the csv module. As I am new to django and python, I am wondering if there is an easier, simpler way to do this? Note: In serializer I use filter on the model objects because using get for the model instance returns an object not iterable error. In fact I filter it twice to get a single instance, seems like there must be a better way.  <code>  <?xml version=""1.0"" encoding=""UTF-8""?><textFields> <textField id=""0"" text=""HELLO WORLD"" /> <textField id=""1"" text=""HELLO EARTH"" /> ...</textFields> from django.core import serializersdata = serializers.serialize('xml', myModel.objects.filter(instanceIwantTowrite), fields=('fieldName')) from django.core.files import File f = open('/path/to/new/dir/content.xml', 'w')myfile = File(f) myfile.write(data)myfile.close()",Generate XML file from model data
easy_install force a version," I'm trying to install lxml. I've had a look at the website, and version 2.2.8 looked reasonable to me but when I did easy_install lxml, it installed version 2.3.beta1 which is not really what I want I presume.What is the best way to fix this and how can I force easy_install to install a particular version?(Mac os x 10.6) <code> ",Install particular version with easy_install
How do I specify a range of odd, How do I specify a range of unicode characters from ' ' (space) to \u00D7FF?I have a regular expression like r'[\u0020-\u00D7FF]' and it won't compile saying that it's a bad range. I am new to Unicode regular expressions so I haven't had this problem before. Is there a way to make this compile or a regular expression that I'm forgetting or haven't learned yet? <code> ,How do I specify a range of unicode characters
How can I check if 2 lines intersect ? ," How can I check if 2 segments intersect?I've the following data: I need to write a small algorithm in Python to detect if the 2 lines are intersecting. <code>  Segment1 [ {x1,y1}, {x2,y2} ]Segment2 [ {x1,y1}, {x2,y2} ] ",How can I check if two segments intersect?
How can I check if 2 segments intersect ? ," How can I check if 2 segments intersect?I've the following data: I need to write a small algorithm in Python to detect if the 2 lines are intersecting. <code>  Segment1 [ {x1,y1}, {x2,y2} ]Segment2 [ {x1,y1}, {x2,y2} ] ",How can I check if two segments intersect?
SWIG passing argument to python callback fuction," So I'm almost done. Now I have working code which calls python callback function.Only thing I need now is how to pass argument to the python callback function.My callback.c is: My callback.i is: It works well. What should I do so I can pass argument to my_callback?Any help will be greatly appreciated! <code>  #include <stdio.h>typedef void (*CALLBACK)(void);CALLBACK my_callback = 0;void set_callback(CALLBACK c);void test(void);void set_callback(CALLBACK c) { my_callback = c;}void test(void) { printf(""Testing the callback function\n""); if (my_callback) (*my_callback)(); else printf(""No callback registered\n"");} // An entirely different mechanism for handling a callback%module callback%{typedef void (*CALLBACK)(void);extern CALLBACK my_callback;extern void set_callback(CALLBACK c);extern void my_set_callback(PyObject *PyFunc);extern void test(void);%}extern CALLBACK my_callback;extern void set_callback(CALLBACK c);extern void my_set_callback(PyObject *PyFunc);extern void test(void);%{static PyObject *my_pycallback = NULL;static void PythonCallBack(void){ PyObject *func, *arglist; PyObject *result; func = my_pycallback; /* This is the function .... */ arglist = Py_BuildValue(""()""); /* No arguments needed */ result = PyEval_CallObject(func, arglist); Py_DECREF(arglist); Py_XDECREF(result); return /*void*/;}void my_set_callback(PyObject *PyFunc){ Py_XDECREF(my_pycallback); /* Dispose of previous callback */ Py_XINCREF(PyFunc); /* Add a reference to new callback */ my_pycallback = PyFunc; /* Remember new callback */ set_callback(PythonCallBack);}%}%typemap(python, in) PyObject *PyFunc { if (!PyCallable_Check($input)) { PyErr_SetString(PyExc_TypeError, ""Need a callable object!""); return NULL; } $1 = $input;}",SWIG passing argument to python callback function
Ruby equivalent of Python None or JavaScript undefined," How does Ruby's nil manifest in code? For example, in Python you might use None for a default argument when it refers to another argument, but in Ruby you can refer to other arguments in the arg list (see this question). In JS, undefined pops up even more because you can't specify default arguments at all. Can you give an example of how RubyNone pops up and how it's dealt with?I'm not looking for just an example using nil. Preferably it would be a real code snippet which had to use nil for some reason or other. <code> ","Ruby use case for nil, equivalent to Python None or JavaScript undefined"
Django ManyToManyField ordering using through?," Here is a snippet of how my models are setup: Then when I access the Accounts, how can I sort by 'number' in the intermediary ProfileAccounts model, rather than the default 'name' in the Accounts Model?: This does not work, but is the gist of how I want it to access this data: <code>  class Profile(models.Model): name = models.CharField(max_length=32) accout = models.ManyToManyField( 'project.Account', through='project.ProfileAccount' ) def __unicode__(self) return self.nameclass Accounts(models.Model): name = models.CharField(max_length=32) type = models.CharField(max_length=32) class Meta: ordering = ('name',) def __unicode__(self) return self.nameclass ProfileAccounts(models.Model): profile = models.ForeignKey('project.Profile') account = models.ForeignKey('project.Accounts') number = models.PositiveIntegerField() class Meta: ordering = ('number',) for acct_number in self.profile.accounts.all(): pass for scanline_field in self.scanline_profile.fields.all().order_by('number'): pass",Django ManyToManyField ordering using through
Performance comparison of string concatenation between Java and Python," UPDATES: thanks a lot to Gabe and Glenn for the detailed explanation. The test is wrote not for language comparison benchmark, just for my studying on VM optimization technologies. I did a simple test to understand the performance of string concatenation between Java and Python. The test is target for the default immutable String object/type in both languages. So I don't use StringBuilder/StringBuffer in Java test.The test simply adds strings for 100k times. Java consumes ~32 seconds to finish, while Python only uses ~13 seconds for Unicode string and 0.042 seconds for non Unicode string.I'm a bit surprise about the results. I thought Java should be faster than Python. What optimization technology does Python leverage to achieve better performance? Or String object is designed too heavy in Java?OS: Ubuntu 10.04 x64JDK: Sun 1.6.0_21Python: 2.6.5Java test did use -Xms1024m to minimize GC activities.Java code: }Python code: <code>  public class StringConcateTest {public static void test(int n) { long start = System.currentTimeMillis(); String a = """"; for (int i = 0; i < n; i++) { a = a.concat(String.valueOf(i)); } long end = System.currentTimeMillis(); System.out.println(a.length() + "", time:"" + (end - start));}public static void main(String[] args) { for (int i = 0; i < 10; i++) { test(1000 * 100); }} import timedef f(n): start = time.time() a = u'' #remove u to use non Unicode string for i in xrange(n): a = a + str(i) print len(a), 'time', (time.time() - start)*1000.0for j in xrange(10): f(1000 * 100)",Performance comparison of immutable string concatenation between Java and Python
Python function local name binding from an outer scope," I need a way to ""inject"" names into a function from an outer code block, so they are accessible locally and they don't need to be specifically handled by the function's code (defined as function parameters, loaded from *args etc.)The simplified scenario: providing a framework within which the users are able to define (with as little syntax as possible) custom functions to manipulate other objects of the framework (which are not necessarily global).Ideally, the user defines Here Cat, Mouse and Cheese are framework objects that, for good reasons, cannot be bounded to the global namespace.I want to write a wrapper for this function to behave like this: Then this wrapper could be applied to all user-defined functions (as a decorator, by the user himself or automatically, although I plan to use a metaclass). I am aware of the Python 3's nonlocal keyword, but I still consider ugly (from the framework's user perspective) to add an additional line: and to worry about adding every object he needs to this line.Any suggestion is greatly appreciated. <code>  def user_func(): Mouse.eat(Cheese) if Cat.find(Mouse): Cat.happy += 1 def framework_wrap(user_func): # this is a framework internal and has name bindings to Cat, Mouse and Cheese def f(): inject(user_func, {'Cat': Cat, 'Mouse': Mouse, 'Cheese': Cheese}) user_func() return f @framework_wrapdef user_func(): nonlocal Cat, Mouse, Cheese",Function local name binding from an outer scope
Find oldest/youngest dateime object in a list," I've got a list of datetime objects, and I want to find the oldest or youngest one. Some of these dates might be in the future. What's the most optimal way to do so? I was thinking of comparing datetime.now() to each one of those. <code>  from datetime import datetimedatetime_list = [ datetime(2009, 10, 12, 10, 10), datetime(2010, 10, 12, 10, 10), datetime(2010, 10, 12, 10, 10), datetime(2011, 10, 12, 10, 10), #future datetime(2012, 10, 12, 10, 10), #future]",Find oldest/youngest datetime object in a list
How to list an image sequence in an efficient way?," I have a directory of 9 images: I would like to be able to list them in an efficient way, like this (4 entries for 9 images): Is there a way in python, to return a directory of images, in a short/clear way (without listing every file)?So, possibly something like this:list all images, sort numerically, create a list (counting each image in sequence from start).When an image is missing (create a new list), continue until original file list is finished.Now I should just have some lists that contain non broken sequences.I'm trying to make it easy to read/describe a list of numbers. If I had a sequence of 1000 consecutive files It could be clearly listed as file[0001-1000] rather than file['0001','0002','0003' etc...]Edit1(based on suggestion): Given a flattened list, how would you derive the glob patterns?Edit2 I'm trying to break the problem down into smaller pieces. Here is an example of part of the solution:data1 works, data2 returns 0010 as 64, data3 (the realworld data) doesn't work: returns: <code>  image_0001, image_0002, image_0003image_0010, image_0011image_0011-1, image_0011-2, image_0011-3image_9999 (image_000[1-3], image_00[10-11], image_0011-[1-3], image_9999) # Find runs of consecutive numbers using groupby. The key to the solution# is differencing with a range so that consecutive numbers all appear in# same group.from operator import itemgetterfrom itertools import *data1=[01,02,03,10,11,100,9999]data2=[0001,0002,0003,0010,0011,0100,9999]data3=['image_0001','image_0002','image_0003','image_0010','image_0011','image_0011-2','image_0011-3','image_0100','image_9999']list1 = []for k, g in groupby(enumerate(data1), lambda (i,x):i-x): list1.append(map(itemgetter(1), g))print 'data1'print list1list2 = []for k, g in groupby(enumerate(data2), lambda (i,x):i-x): list2.append(map(itemgetter(1), g))print '\ndata2'print list2 data1[[1, 2, 3], [10, 11], [100], [9999]]data2[[1, 2, 3], [8, 9], [64], [9999]]",How to list an image sequence in an efficient way? Numercial sequence comparison in Python
How to list an image sequence in an efficient way? Numercial string comparison," I have a directory of 9 images: I would like to be able to list them in an efficient way, like this (4 entries for 9 images): Is there a way in python, to return a directory of images, in a short/clear way (without listing every file)?So, possibly something like this:list all images, sort numerically, create a list (counting each image in sequence from start).When an image is missing (create a new list), continue until original file list is finished.Now I should just have some lists that contain non broken sequences.I'm trying to make it easy to read/describe a list of numbers. If I had a sequence of 1000 consecutive files It could be clearly listed as file[0001-1000] rather than file['0001','0002','0003' etc...]Edit1(based on suggestion): Given a flattened list, how would you derive the glob patterns?Edit2 I'm trying to break the problem down into smaller pieces. Here is an example of part of the solution:data1 works, data2 returns 0010 as 64, data3 (the realworld data) doesn't work: returns: <code>  image_0001, image_0002, image_0003image_0010, image_0011image_0011-1, image_0011-2, image_0011-3image_9999 (image_000[1-3], image_00[10-11], image_0011-[1-3], image_9999) # Find runs of consecutive numbers using groupby. The key to the solution# is differencing with a range so that consecutive numbers all appear in# same group.from operator import itemgetterfrom itertools import *data1=[01,02,03,10,11,100,9999]data2=[0001,0002,0003,0010,0011,0100,9999]data3=['image_0001','image_0002','image_0003','image_0010','image_0011','image_0011-2','image_0011-3','image_0100','image_9999']list1 = []for k, g in groupby(enumerate(data1), lambda (i,x):i-x): list1.append(map(itemgetter(1), g))print 'data1'print list1list2 = []for k, g in groupby(enumerate(data2), lambda (i,x):i-x): list2.append(map(itemgetter(1), g))print '\ndata2'print list2 data1[[1, 2, 3], [10, 11], [100], [9999]]data2[[1, 2, 3], [8, 9], [64], [9999]]",How to list an image sequence in an efficient way? Numercial sequence comparison in Python
How to list an image sequence in an efficient way? Numerical sequence comparison in Python," I have a directory of 9 images: I would like to be able to list them in an efficient way, like this (4 entries for 9 images): Is there a way in python, to return a directory of images, in a short/clear way (without listing every file)?So, possibly something like this:list all images, sort numerically, create a list (counting each image in sequence from start).When an image is missing (create a new list), continue until original file list is finished.Now I should just have some lists that contain non broken sequences.I'm trying to make it easy to read/describe a list of numbers. If I had a sequence of 1000 consecutive files It could be clearly listed as file[0001-1000] rather than file['0001','0002','0003' etc...]Edit1(based on suggestion): Given a flattened list, how would you derive the glob patterns?Edit2 I'm trying to break the problem down into smaller pieces. Here is an example of part of the solution:data1 works, data2 returns 0010 as 64, data3 (the realworld data) doesn't work: returns: <code>  image_0001, image_0002, image_0003image_0010, image_0011image_0011-1, image_0011-2, image_0011-3image_9999 (image_000[1-3], image_00[10-11], image_0011-[1-3], image_9999) # Find runs of consecutive numbers using groupby. The key to the solution# is differencing with a range so that consecutive numbers all appear in# same group.from operator import itemgetterfrom itertools import *data1=[01,02,03,10,11,100,9999]data2=[0001,0002,0003,0010,0011,0100,9999]data3=['image_0001','image_0002','image_0003','image_0010','image_0011','image_0011-2','image_0011-3','image_0100','image_9999']list1 = []for k, g in groupby(enumerate(data1), lambda (i,x):i-x): list1.append(map(itemgetter(1), g))print 'data1'print list1list2 = []for k, g in groupby(enumerate(data2), lambda (i,x):i-x): list2.append(map(itemgetter(1), g))print '\ndata2'print list2 data1[[1, 2, 3], [10, 11], [100], [9999]]data2[[1, 2, 3], [8, 9], [64], [9999]]",How to list an image sequence in an efficient way? Numercial sequence comparison in Python
Iterator (iter()) function in Python. ," For dictionary, I can use iter() for iterating over keys of the dictionary. When I have the iterator as follows, Why can't I use it this way nor but this way? What's the usage of iter() function?ADDEDI guess this can be one of the ways of how iter() is used. <code>  y = {""x"":10, ""y"":20}for val in iter(y): print val class Counter: def __init__(self, low, high): self.current = low self.high = high def __iter__(self): return self def next(self): if self.current > self.high: raise StopIteration else: self.current += 1 return self.current - 1 x = Counter(3,8)for i in x: print x x = Counter(3,8)for i in iter(x): print x for c in Counter(3, 8): print c class Counter: def __init__(self, low, high): self.current = low self.high = high def __iter__(self): return self def next(self): if self.current > self.high: raise StopIteration else: self.current += 1 return self.current - 1class Hello: def __iter__(self): return Counter(10,20)x = iter(Hello())for i in x: print i",Iterator (iter()) function in Python.
dose python urllib2 will automaticly uncompress gzip data from fetch webpage, I'm using I want to know:How can I tell if the data at a URL is gzipped?Does urllib2 automatically uncompress the data if it is gzipped? Will the data always be a string? <code>  data=urllib2.urlopen(url).read(),Does python urllib2 automatically uncompress gzip data fetched from webpage?
Does python urllib2 will automaticly uncompress gzip data from fetch webpage, I'm using I want to know:How can I tell if the data at a URL is gzipped?Does urllib2 automatically uncompress the data if it is gzipped? Will the data always be a string? <code>  data=urllib2.urlopen(url).read(),Does python urllib2 automatically uncompress gzip data fetched from webpage?
How do you select between two dates with Django," I am looking to make a query that selects between dates with Django.I know how to do this with raw SQL pretty easily, but how could this be achieved using the Django ORM?This is where I want to add the between dates of 30 days in my query: <code>  start_date = datetime.datetime.now() + datetime.timedelta(-30)context[self.varname] = self.model._default_manager.filter( current_issue__isnull=True ).live().order_by('-created_at')",Select between two dates with Django
How to delete record from table? (Python with Sqlite3)," I have a problem with deleting a record from my SQLite3 database: Everything is OK, no errors, but the delete function doesn't work!Does anyone have an idea? <code>  conn = sqlite3.connect('databaza.db')c = conn.cursor()data3 = str(input('Please enter name: '))mydata = c.execute('DELETE FROM Zoznam WHERE Name=?', (data3,))conn.commit()c.close",How to delete a record from table?
How to delete record from table?," I have a problem with deleting a record from my SQLite3 database: Everything is OK, no errors, but the delete function doesn't work!Does anyone have an idea? <code>  conn = sqlite3.connect('databaza.db')c = conn.cursor()data3 = str(input('Please enter name: '))mydata = c.execute('DELETE FROM Zoznam WHERE Name=?', (data3,))conn.commit()c.close",How to delete a record from table?
Hot calculate a sigmoid function in Python?, This is a logistic sigmoid function:I know x. How can I calculate F(x) in Python now?Let's say x = 0.458.F(x) = ? <code> ,How to calculate a logistic sigmoid function in Python?
How to calculate a sigmoid function in Python?, This is a logistic sigmoid function:I know x. How can I calculate F(x) in Python now?Let's say x = 0.458.F(x) = ? <code> ,How to calculate a logistic sigmoid function in Python?
Longest increasing subsequence.," Given an input sequence, what is the best way to find the longest (not necessarily continuous) increasing subsequence I'm looking for the best algorithm. If there is code, Python would be nice, but anything is alright. <code>  [0, 8, 4, 12, 2, 10, 6, 14, 1, 9, 5, 13, 3, 11, 7, 15] # input[1, 9, 13, 15] # an example of an increasing subsequence (not the longest)[0, 2, 6, 9, 13, 15] # longest increasing subsequence (not a unique answer)[0, 2, 6, 9, 11, 15] # another possible solution",Longest increasing subsequence
Python: generate random integers between 0 and 9," How can I generate random integers between 0 and 9 (inclusive) in Python?For example, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 <code> ",Generate random integers between 0 and 9
generate random integers between 0 and 9," How can I generate random integers between 0 and 9 (inclusive) in Python?For example, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 <code> ",Generate random integers between 0 and 9
"Python 2.7/Windows: Create resizable, multiline tkinter/ttk labels with word wrap"," Is it possible to create a multi-line label with word wrap that resizes in sync with the width of its parent? In other words the wordwrap behavior of Notepad as you change the width of the NotePad window.The use case is a dialog that needs to present a block of multi-line text (instructions) in its entirety without having the text clipped or resorting to scrollbars. The parent container will have enough vertical space to accomodate narrow widths.I've been experimenting with Tkinter Label and Message widgets and the ttk Label widget without success. It seems that I need to hard code a pixel wraplength value vs. have these controls auto wordwrap when their text reaches the right edge of their containers. Certainly Tkinters geometry managers can help me auto-resize my labels and update their wraplength values accordingly?Should I be looking at the Text widget instead? If so, is it possible to hide the border of a Text widget so I can use it as a multi-line label with wordwrap?Here's a prototype of how one might do what I described above. It was inspired by Bryan Oakley's tip to use the Text widget and the following post on Stackoverflow:In python's tkinter, how can I make a Label such that you can select the text with the mouse? <code>  from Tkinter import *master = Tk()text = """"""If tkinter is 8.5 or above you'll want the selection background to appear like it does when the widget is activated. Comment this out for older versions of Tkinter.This is even more text.The final line of our auto-wrapping label that supports clipboard copy."""""".strip()frameLabel = Frame( master, padx=20, pady=20 )frameLabel.pack()w = Text( frameLabel, wrap='word', font='Arial 12 italic' )w.insert( 1.0, text )w.pack()# - have selection background appear like it does when the widget is activated (Tkinter 8.5+)# - have label background color match its parent background color via .cget('bg')# - set relief='flat' to hide Text control borders# - set state='disabled' to block changes to text (while still allowing selection/clipboard copy)w.configure( bg=master.cget('bg'), relief='flat', state='disabled' )mainloop()",Create resizable/multiline Tkinter/ttk Labels with word wrap
Is it OK to use SQLalchemy with gevent?," I know that some database drivers and other libraries providing connection to external services are incompatible with coroutine-based network libraries. However, I couldn't find out if SQLAlchemy can be safely used with such libraries (namely, gevent), and if any workarounds should be applied to exclude possible errors.Can you either tell me that or point me to an article where I can read this information? An additional 'thank you' if the provided answer also contains explanation of the mechanics of why it is okay or why it isn't. :-) <code> ",Is it safe to use SQLalchemy with gevent?
Python verify url exists and if not send email, I have a list of urls (1000+) which have been stored for over a year now. I want to run through and verify them all to see if they still exist. What is the best / quickest way to check them all and return a list of ones which do not return a site? <code> ,Python verify url goes to a page
Handle a blocking import in Python," I'm working with the Gnuradio framework. I handle flowgraphs I generate to send/receive signals. These flowgraphs initialize and start, but they don't return the control flow to my application:I imported time The problem is: only the first if statement invokes the flow-graph (that interacts with the hardware). I'm stuck in this. I could use a Thread, but I'm unexperienced how to timeout threads in Python. I doubt that this is possible, because it seems killing threads isn't within the APIs. This script only has to work on Linux...How do you handle blocking functions with Python properly - without killing the whole program. Another more concrete example for this problem is: How do I still get print ""hallo"". ;)Thanks,Marius <code>  while time.time() < endtime: # invoke GRC flowgraph for 1st sequence if not seq1_sent: tb = send_seq_2.top_block() tb.Run(True) seq1_sent = True if time.time() < endtime: break # invoke GRC flowgraph for 2nd sequence if not seq2_sent: tb = send_seq_2.top_block() tb.Run(True) seq2_sent = True if time.time() < endtime: break import signal, osdef handler(signum, frame): # print 'Signal handler called with signal', signum #raise IOError(""Couldn't open device!"") import time print ""wait"" time.sleep(3)def foo(): # Set the signal handler and a 5-second alarm signal.signal(signal.SIGALRM, handler) signal.alarm(3) # This open() may hang indefinitely fd = os.open('/dev/ttys0', os.O_RDWR) signal.alarm(0) # Disable the alarmfoo()print ""hallo""",Handle a blocking function call in Python
zeromq persistent patents," Who has to manages the persistent in the ZeroMQ?When we use the ZeroMQ clients in Python language, what are the plug-ins/modules available to manage the persistent?I would like to know the patterns to use the ZeroMQ. <code> ",zeromq persistence patterns
Python infinite yield problem," Here is my simple code It's absolutely obvious, but...WHY interpreter is executing block Forever?I specified in generator,that I want only elements<10 IMHO, it's a little bit misunderstandingAny way to prevent infinite execution without re-writing ""series""?  <code>  class Fibonacci: @staticmethod def series(): fprev = 1 fnext = 1 yield fnext while True: yield fnext fprev,fnext = fnext,fprev+fnextunder10 = (i for i in Fibonacci.series() if i<10)for i in under10 : print i while True: yield fnext fprev,fnext = fnext,fprev+fnext under10 = (i for i in Fibonacci.series() if i<10)",Infinite yield problem
"return a list of weekdays, python"," My task is to dene a function weekdays(weekday) that returns a list of weekdays, starting with the given weekday. It should work like this: So far I've come up with this one: But this prints the input day only: What am I doing wrong? <code>  >>> weekdays('Wednesday')['Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'Monday', 'Tuesday'] def weekdays(weekday): days = ('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday') result = """" for day in days: if day == weekday: result += day return result >>> weekdays(""Sunday"")'Sunday'","Return a list of weekdays, starting with given weekday"
Return a list of weekdays," My task is to dene a function weekdays(weekday) that returns a list of weekdays, starting with the given weekday. It should work like this: So far I've come up with this one: But this prints the input day only: What am I doing wrong? <code>  >>> weekdays('Wednesday')['Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'Monday', 'Tuesday'] def weekdays(weekday): days = ('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday') result = """" for day in days: if day == weekday: result += day return result >>> weekdays(""Sunday"")'Sunday'","Return a list of weekdays, starting with given weekday"
Questions about python package import," I wasn't clear how to correctly name this question.Case 1Assume that I have the following directory structure. If I have How do I know which bar (bar.py or bar/__init__.py) is being imported? Is there any easy way to automatically detect this from occurring?Case 2 If other.py has the line How do I know which foo (foo or foo.foo) is being imported? Again, is tehre any easy way to automatically detect this from occurring? <code>  foo|+- bar/__init__.py|+- bar.py from foo import bar foo|+- foo.py|+- other.py import foo",Python import precedence: packages or modules?
How to clamp an integer to some range? (in Python)," I have the following code: Basically, I calculate a new index and use that to find some element from a list. In order to make sure the index is inside the bounds of the list, I needed to write those 2 if statements spread into 4 lines. That's quite verbose, a bit ugly... Dare I say, it's quite un-pythonic.Is there any other simpler and more compact solution? (and more pythonic)Yes, i know I can use if else in one line, but it is not readable: I also know I can chain max() and min() together. It's more compact, but I feel it's kinda obscure, more difficult to find bugs if I type it wrong. In other words, I don't find it very straightforward. <code>  new_index = index + offsetif new_index < 0: new_index = 0if new_index >= len(mylist): new_index = len(mylist) - 1return mylist[new_index] new_index = 0 if new_index < 0 else len(mylist) - 1 if new_index >= len(mylist) else new_index new_index = max(0, min(new_index, len(mylist)-1))",How to clamp an integer to some range?
Find in a dynamic pythonic way the minimum elements in a partially ordered set," Let Os be a partially ordered set, and given any two objects O1 and O2 in Os, F(O1,O2) will return 1 if O1 is bigger than O2, -1 if O1 is smaller than O2, 2 if they are incomparable, and 0 if O1 is equal to O2.I need to find the subset Mn of elements is Os that are the minimum. That is for each A in Mn, and for each B in Os, F(A,B) is never equal to 1.It is not hard to do, but I am convinced it could be done in a more pythonic way.The fast and dirty way is: In particular I am not happy with the fact that I am essentially going through all the elements N^2 times. I wonder if there could be a dynamic way. By ""dynamic"" I don't mean merely fast, but also such that once something is discovered being not a possible in the minimum, maybe it could be taken off. And doing all this in a pythonic, elegant way <code>  def GetMinOs(Os): Mn=set([]) NotMn=set([]) for O1 in Os: for O2 in Os: rel=f(O1,O2) if rel==1: NotMn|=set([O1]) elif rel==-1: NotMn|=set([O2]) Mn=Os-NotMn return Mn",Find in a dynamic pythonic way the minimum elements in a partially ordered set
Find in a fast pythonic way the minimum elements in a partially ordered set," Let Os be a partially ordered set, and given any two objects O1 and O2 in Os, F(O1,O2) will return 1 if O1 is bigger than O2, -1 if O1 is smaller than O2, 2 if they are incomparable, and 0 if O1 is equal to O2.I need to find the subset Mn of elements is Os that are the minimum. That is for each A in Mn, and for each B in Os, F(A,B) is never equal to 1.It is not hard to do, but I am convinced it could be done in a more pythonic way.The fast and dirty way is: In particular I am not happy with the fact that I am essentially going through all the elements N^2 times. I wonder if there could be a dynamic way. By ""dynamic"" I don't mean merely fast, but also such that once something is discovered being not a possible in the minimum, maybe it could be taken off. And doing all this in a pythonic, elegant way <code>  def GetMinOs(Os): Mn=set([]) NotMn=set([]) for O1 in Os: for O2 in Os: rel=f(O1,O2) if rel==1: NotMn|=set([O1]) elif rel==-1: NotMn|=set([O2]) Mn=Os-NotMn return Mn",Find in a dynamic pythonic way the minimum elements in a partially ordered set
Why does the python-pip yum package provide pip-python instead of pip?," I installed python-pip package via yum (using Fedora's updates repo). It does not add the pip script to my PATH though. It does, however, create pip-python: I was considering making pip a symbolic link to pip-python but is there a reason the executable is named pip-python to begin with? <code>  $ which pip/usr/bin/which: no pip in (/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/bin:/bin: /usr/bin:/usr/local/sbin:/usr/sbin:/sbin) $ rpm -ql python-pip/usr/bin/pip-python[...snip...]$ which pip-python/usr/bin/pip-python",python-pip yum package provides pip-python instead of pip
Python: Nexting counters," For my customers, iterating through multiple counters is turning into a recurring task.The most straightforward way would be something like this: The number of counters can be anywhere from 3 on up and those nested for loops start taking up real estate.Is there a Pythonic way to do something like this? I keep thinking that something in itertools would fit this bill, but I'm just not familiar enough with itertools to make sense of the options. Is there already a solution such as itertools, or do I need to roll my own?Thanks,j <code>  cntr1 = range(0,2)cntr2 = range(0,5)cntr3 = range(0,7)for li in cntr1: for lj in cntr2: for lk in cntr3: print li, lj, lk for li, lj, lk in mysteryfunc(cntr1, cntr2, cntr3): print li, lj, lk",Python: Nesting counters
python: single vs double quotes in JSON," My code: #1 definition is wrong#2 definition is rightI heard that in Python that single and double quote can be interchangable. Can anyone explain this to me? <code>  import simplejson as jsons = ""{'username':'dfdsfdsf'}"" #1#s = '{""username"":""dfdsfdsf""}' #2j = json.loads(s)",Single vs double quotes in JSON
accessing variables in the dubugging session with ipython and %pdb on," I'm new to ipython and I am trying to use ipython to debug my code. I did: and then and in the code, I have 1/0 so it raises an exception and will automatically goes into the debug session. So I can access variables. But when I do the following: But this works: <code>  [1]: %pdbAutomatic pdb calling has been turned ON In [2]: %run mycode.py ZeroDivisionError: float divisionipdb> variablearray([ 0.00704313, -1.34700666, -2.81474391]) ipdb> b = variable*** The specified object '= variable' is not a function or was not found along sys.path. ipdb> b = self.X",accessing variables in the debugging session with ipython and %pdb on
How are google-app-engine model classes stored?," I'm in doubt how the objects are stored. Say I have a class defined like: My guess is that the only thing stored in the data store is the name/value/type of some_number together with the fully qualified name of the class (SomeEntity). However I have not stumbled upon any information that confirms this. 1) Can anyone confirm this? I would like to confirm that I can change (and add/remove) methods without somehow affecting the data is stored. 2) Furthermore, what happens to existing objects if I add a new property to the class (and what if that property has required=true)? <code>  class SomeEntity(db.Model): some_number = db.IntegerProperty(required=True) def calculate_something(self): return self.some_number * 2",How are Google App Engine model classes stored?
An example using LibSVM in python, I am in dire need of a classification task example using LibSVM in python. I don't know how the Input should look like and which function is responsible for training and which one for testingThanks <code> ,"An example using python bindings for SVM library, LIBSVM"
Python: Call to operating system to open url?, What can I use to call the OS to open a URL in whatever browser the user has as default? Not worried about cross-OS compatibility; if it works in linux thats enough for me! <code> ,Call to operating system to open url?
Python: BaseHTTPRequestHandler post variables," given the simplest HTTP server, how do I get post variables in a BaseHTTPRequestHandler? I would simply like to able to get the values of param1 and param2. Thanks! <code>  from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServerclass Handler(BaseHTTPRequestHandler): def do_POST(self): # post variables?!server = HTTPServer(('', 4444), Handler)server.serve_forever()# test with:# curl -d ""param1=value1&param2=value2"" http://localhost:4444",Python: How do I get key/value pairs from the BaseHTTPRequestHandler HTTP POST handler?
Splitting audio/video stream for Adaptive Streaming in HTML5 chat/video application," We are currently working on a chat + (file sharing +) video conference application using HTML5 websockets. To make our application more accessible we want to implement Adaptive Streaming, using the following sequence:Raw audio/video data client goes to serverStream is split into 1 second chunksEncode stream into varying bandwidthsClient receives manifest file describing available segmentsDownloads one segment using normal HTTPBandwidth next segment chosen on performance of previous oneClient may select from a number of different alternate streams at a variety of data ratesSo.. How do we split our audio/video data in chunks with Python?We know Microsoft already build the Expression Encoder 2 which enables Adaptive Streaming, but it only supports Silverlight and that's not what we want.Edit:There's also an solution called FFmpeg (and for Python a PyFFmpeg wrapper), but it only supports Apple Adaptive streaming. <code> ",Chopping media stream in HTML5 websocket server for webbased chat/video conference application
Splitting audio/video stream (in Python) for Adaptive Streaming in webbased chat/video app.," We are currently working on a chat + (file sharing +) video conference application using HTML5 websockets. To make our application more accessible we want to implement Adaptive Streaming, using the following sequence:Raw audio/video data client goes to serverStream is split into 1 second chunksEncode stream into varying bandwidthsClient receives manifest file describing available segmentsDownloads one segment using normal HTTPBandwidth next segment chosen on performance of previous oneClient may select from a number of different alternate streams at a variety of data ratesSo.. How do we split our audio/video data in chunks with Python?We know Microsoft already build the Expression Encoder 2 which enables Adaptive Streaming, but it only supports Silverlight and that's not what we want.Edit:There's also an solution called FFmpeg (and for Python a PyFFmpeg wrapper), but it only supports Apple Adaptive streaming. <code> ",Chopping media stream in HTML5 websocket server for webbased chat/video conference application
How to show the stardand popup menu in python tkinter text widget when mouse right button pressed?," I'm working on Windows XP, with Python 2.6.x and TKinter. Using a text widget in the app, but the standard popup menu (cut, copy, paste, delete, select all) is missing. How to make it appear? <code> ",Stardand context menu in Python TKinter text widget when mouse right button is pressed
Python: Memory Limit?," Is there a limit to memory for python? I've been using a python script to calculate the average values from a file which is a minimum of 150mb big.Depending on the size of the file I sometimes encounter a MemoryError.Can more memory be assigned to the python so I don't encounter the error?EDIT: Code now belowNOTE: The file sizes can vary greatly (up to 20GB) the minimum size of the a file is 150mb <code>  file_A1_B1 = open(""A1_B1_100000.txt"", ""r"")file_A2_B2 = open(""A2_B2_100000.txt"", ""r"")file_A1_B2 = open(""A1_B2_100000.txt"", ""r"")file_A2_B1 = open(""A2_B1_100000.txt"", ""r"")file_write = open (""average_generations.txt"", ""w"")mutation_average = open(""mutation_average"", ""w"")files = [file_A2_B2,file_A2_B2,file_A1_B2,file_A2_B1]for u in files: line = u.readlines() list_of_lines = [] for i in line: values = i.split('\t') list_of_lines.append(values) count = 0 for j in list_of_lines: count +=1 for k in range(0,count): list_of_lines[k].remove('\n') length = len(list_of_lines[0]) print_counter = 4 for o in range(0,length): total = 0 for p in range(0,count): number = float(list_of_lines[p][o]) total = total + number average = total/count print average if print_counter == 4: file_write.write(str(average)+'\n') print_counter = 0 print_counter +=1file_write.write('\n')",Upper memory limit?
Formatted String Dynamically Adjustable," If I want to make my formatted string dynamically adjustable, I can change the following code from to However, it seems that string concatenation is cumbersome here. Any other way to simplify things? <code>  print '%20s : %20s' % (""Python"", ""Very Good"") width = 20print ('%' + str(width) + 's : %' + str(width) + 's') % (""Python"", ""Very Good"")",Format string dynamically
Should i use Pylons or pyramid," I was planning to move from Django to Pylons, but then I bumped into Pyramid.What are the differences between Pylons and Pyramid?I read some text in PylonsBook, which currently covers Pylons 0.9.7, and wonder if it is a to start for Pylons and Pyramid. <code> ",Should I use Pylons or Pyramid?
python -m SimpleHTTPServer - Page Not Found, After cding to my folder I enter and get in reply. But when I hit http://0.0.0.0:8000/test.html I get a page not found error.I've also tried taken from this questionWhen I hit ls I can see the file and the directory. Anyone know what I'm doing wrong? <code>  python -m SimpleHTTPServer Serving HTTP on 0.0.0.0 port 8000 ... pushd /path/you/want/to/serve; python -m SimpleHTTPServer; popd,"python -m SimpleHTTPServer - Listening on 0.0.0.0:8000 but http://0.0.0.0:8000/test.html gives ""Page Not Found"""
"BeautifulSoup: How do I start at the first H3 in the document, then pull all <li>'s until the first <h2> tag?"," I'm a newbie programmer trying to jump in to Python by building a script that scrapes http://en.wikipedia.org/wiki/2000s_in_film and extracts a list of ""Movie Title (Year)"".My HTML source looks like: I'd like all the li tags following the first h3 tag and stopping at the next h2 tag, including all nested li tags. ...correctly finds the place I'd like to start. ...gives me a list uls, each with li contents that I need.Excerpt of the uls list: But I'm unsure of where to go from here.Update:Final Code: The if...break throws out the LI's that contain UL's since the nested LI's are now duplicated.Print output is now:102 Dalmatians(2000)10th & Wolf(2006)11:14(2006)12:08 East of Bucharest(2006)13 Going on 30(2004)1408(2007)... <code>  <h3>Header3 (Start here)</h3><ul> <li>List items</li> <li>Etc...</li></ul><h3>Header 3</h3><ul> <li>List items</li> <ul> <li>Nested list items</li> <li>Nested list items</li></ul> <li>List items</li></ul><h2>Header 2 (end here)</h2> firstH3 = soup.find('h3') firstH3 = soup.find('h3') # Start hereuls = []for nextSibling in firstH3.findNextSiblings(): if nextSibling.name == 'h2': break if nextSibling.name == 'ul': uls.append(nextSibling) <ul>... <li><i><a href=""/wiki/Agent_Cody_Banks"" title=""Agent Cody Banks"">Agent Cody Banks</a></i> (2003)</li> <li><i><a href=""/wiki/Agent_Cody_Banks_2:_Destination_London"" title=""Agent Cody Banks 2: Destination London"">Agent Cody Banks 2: Destination London</a></i> (2004)</li> <li>Air Bud series: <ul> <li><i><a href=""/wiki/Air_Bud:_World_Pup"" title=""Air Bud: World Pup"">Air Bud: World Pup</a></i> (2000)</li> <li><i><a href=""/wiki/Air_Bud:_Seventh_Inning_Fetch"" title=""Air Bud: Seventh Inning Fetch"">Air Bud: Seventh Inning Fetch</a></i> (2002)</li> <li><i><a href=""/wiki/Air_Bud:_Spikes_Back"" title=""Air Bud: Spikes Back"">Air Bud: Spikes Back</a></i> (2003)</li> <li><i><a href=""/wiki/Air_Buddies"" title=""Air Buddies"">Air Buddies</a></i> (2006)</li> </ul> </li> <li><i><a href=""/wiki/Akeelah_and_the_Bee"" title=""Akeelah and the Bee"">Akeelah and the Bee</a></i> (2006)</li>...</ul> lis = [] for ul in uls: for li in ul.findAll('li'): if li.find('ul'): break lis.append(li) for li in lis: print li.text.encode(""utf-8"")",BeautifulSoup: How do I extract all the <li>s from a list of <ul>s that contains some nested <ul>s?
How to see function signature in python?," Is there a way to introspect a function so that it shows me information on the arguments it takes (like number of args, type if possible, name of arguments if named) and the return value? dir() doesn't seem to do what I want. The __doc__ string sometimes includes method/function arguments, but often doesn't. <code> ",How to see function signature in Python?
Python - HEAD request with urllib2," I'm trying to do a HEAD request of a page using Python 2.I am trying with misc_urllib2.py containing But I am getting If I just do then it works fine <code>  import misc_urllib2.....opender = urllib2.build_opener([misc_urllib2.MyHTTPRedirectHandler(), misc_urllib2.HeadRequest()]) class HeadRequest(urllib2.Request): def get_method(self): return ""HEAD""class MyHTTPRedirectHandler(urllib2.HTTPRedirectHandler): def __init__ (self): self.redirects = [] def http_error_301(self, req, fp, code, msg, headers): result = urllib2.HTTPRedirectHandler.http_error_301( self, req, fp, code, msg, headers) result.redirect_code = code return result http_error_302 = http_error_303 = http_error_307 = http_error_301 TypeError: __init__() takes at least 2 arguments (1 given) opender = urllib2.build_opener(misc_urllib2.MyHTTPRedirectHandler())",Making HTTP HEAD request with urllib2 from Python 2
How do I modify the session in the Django test framwork," My site allows individuals to contribute content in the absence of being logged in by creating a User based on the current session_keyI would like to setup a test for my view, but it seems that it is not possible to modify the request.session:I'd like to do this: But I get the error: Thoughts on how to modify the client session before making get requests?I have seen this and it doesn't seem to work <code>  from django.contrib.sessions.models import Sessions = Session()s.expire_date = '2010-12-05's.session_key = 'my_session_key's.save()self.client.session = sresponse = self.client.get('/myview/') AttributeError: can't set attribute",How do I modify the session in the Django test framework
Does Django have a template tag that can detect URL's and turn them into hyperlinks?," When someone writes a post and copies and pastes a url in it, can Django detect it and render it as a hyperlink rather than plain text? <code> ",Does Django have a template tag that can detect URLs and turn them into hyperlinks?
OpenGL Shadow Mapping Problem," I am trying to get shadow mapping working using GLSL. Unfortunately my depth render results are unusable even I have a pretty decent depth buffer precision. It is rendering like wireframe, following image may be a better description.I am also including a test case(single file including shader), only dependency is pyopengl.  <code>  # shadow mapping test# utkualtinkaya at gmail # shader is from http://www.fabiensanglard.net/shadowmapping/index.phpfrom OpenGL.GL import *from OpenGL.GLU import * from OpenGL.GLUT import *from OpenGL.GL.shaders import *from OpenGL.GL.framebufferobjects import *import mathclass Camera: def __init__(self): self.rotx, self.roty = math.pi/4, math.pi/4 self.distance = 100 self.moving = False self.ex, self.ey = 0, 0 self.size = (800, 600) def load_matrices(self): glViewport(0, 0, *self.size) y = math.cos(self.roty) * self.distance x = math.sin(self.roty) * math.cos(self.rotx) * self.distance z = math.sin(self.roty) * math.sin(self.rotx) * self.distance glMatrixMode(GL_PROJECTION) glLoadIdentity() gluPerspective(45.0, self.size[0]/float(self.size[1]), 1, 1000) glMatrixMode(GL_MODELVIEW) glLoadIdentity() gluLookAt(x,y,z, 0,0,0, 0,1,0) def on_mouse_button (self, b, s, x, y): self.moving = not s self.ex, self.ey = x, y if b in [3, 4]: dz = (1 if b == 3 else -1) self.distance += self.distance/15.0 * dz; def on_mouse_move(self, x, y, z = 0): if self.moving: self.rotx += (x-self.ex) / 300.0 self.roty += -(y-self.ey) / 300.0 self.ex, self.ey = x, y def set_size(self, w, h): self.size = w, hclass Shader(): def __init__(self): self.is_built = False self.uniforms = {} def build(self): self.program = compileProgram( compileShader(''' uniform mat4 camMatrix; uniform mat4 shadowMatrix; varying vec4 depthProjection; uniform bool useShadow; void main() { gl_Position = camMatrix * gl_ModelViewMatrix * gl_Vertex; depthProjection = shadowMatrix * gl_ModelViewMatrix * gl_Vertex; gl_FrontColor = gl_Color; } ''',GL_VERTEX_SHADER), compileShader(''' varying vec4 depthProjection; uniform sampler2D shadowMap; uniform bool useShadow; void main () { float shadow = 1.0; if (useShadow) { vec4 shadowCoord = depthProjection / depthProjection.w ; // shadowCoord.z -= 0.0003; float distanceFromLight = texture2D(shadowMap, shadowCoord.st).z; if (depthProjection .w > 0.0) shadow = distanceFromLight < shadowCoord.z ? 0.5 : 1.0 ; } gl_FragColor = shadow * gl_Color; } ''',GL_FRAGMENT_SHADER),) self.is_built = True self.uniforms['camMatrix'] = glGetUniformLocation(self.program, 'camMatrix') self.uniforms['shadowMatrix'] = glGetUniformLocation(self.program, 'shadowMatrix') self.uniforms['shadowMap'] = glGetUniformLocation(self.program, 'shadowMap') self.uniforms['useShadow'] = glGetUniformLocation(self.program, 'useShadow') print self.uniforms def use(self): if not self.is_built: self.build() glUseProgram(self.program)class Test: def __init__(self): glutInit(sys.argv) glutInitDisplayMode(GLUT_RGBA | GLUT_DOUBLE | GLUT_ALPHA | GLUT_DEPTH) glutInitWindowSize(800, 600) glutInitWindowPosition(1120/2, 100) self.window = glutCreateWindow(""Shadow Test"") self.cam = Camera() self.light = Camera() self.cam.set_size(800, 600) self.light.set_size(2048, 2048) self.light.distance = 100 self.shader = Shader() self.initialized = False def setup(self): self.initialized = True glClearColor(0,0,0,1.0); glDepthFunc(GL_LESS) glEnable(GL_DEPTH_TEST) self.fbo = glGenFramebuffers(1); self.shadowTexture = glGenTextures(1) glBindFramebuffer(GL_FRAMEBUFFER, self.fbo) w, h = self.light.size glActiveTexture(GL_TEXTURE5) glBindTexture(GL_TEXTURE_2D, self.shadowTexture) glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST); glTexParameterf( GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP ); glTexParameterf( GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP ); glTexImage2D( GL_TEXTURE_2D, 0, GL_DEPTH_COMPONENT, w, h, 0, GL_DEPTH_COMPONENT, GL_UNSIGNED_INT, None) glDrawBuffer(GL_NONE) glReadBuffer(GL_NONE) glFramebufferTexture2D(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_TEXTURE_2D, self.fbo, 0) FBOstatus = glCheckFramebufferStatus(GL_FRAMEBUFFER) if FBOstatus != GL_FRAMEBUFFER_COMPLETE: print (""GL_FRAMEBUFFER_COMPLETE_EXT failed, CANNOT use FBO\n""); glBindFramebuffer(GL_FRAMEBUFFER, 0) #glActiveTexture(GL_TEXTURE0) def draw(self): glPushMatrix() glTranslate(0, 10 ,0) glColor4f(0, 1, 1, 1) glutSolidCube(5) glPopMatrix() glPushMatrix() glColor4f(0.5, 0.5, .5, 1) glScale(100, 1, 100) glutSolidCube(1) glPopMatrix() def apply_camera(self, cam): cam.load_matrices() model_view = glGetDoublev(GL_MODELVIEW_MATRIX); projection = glGetDoublev(GL_PROJECTION_MATRIX); glMatrixMode(GL_MODELVIEW) glLoadIdentity() glMultMatrixd(projection) glMultMatrixd(model_view) glUniformMatrix4fv(self.shader.uniforms['camMatrix'], 1, False, glGetFloatv(GL_MODELVIEW_MATRIX)) glLoadIdentity() def shadow_pass(self): glUniform1i(self.shader.uniforms['useShadow'], 0) glBindFramebuffer(GL_FRAMEBUFFER, self.fbo) glClear(GL_DEPTH_BUFFER_BIT) glCullFace(GL_FRONT) self.apply_camera(self.light) self.draw() glBindFramebuffer(GL_FRAMEBUFFER, 0) def final_pass(self): glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT) self.light.load_matrices() model_view = glGetDoublev(GL_MODELVIEW_MATRIX); projection = glGetDoublev(GL_PROJECTION_MATRIX); glMatrixMode(GL_MODELVIEW) glLoadIdentity() bias = [ 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.5, 0.5, 0.5, 1.0] glLoadMatrixd(bias) glMultMatrixd(projection) glMultMatrixd(model_view) glUniformMatrix4fv(self.shader.uniforms['shadowMatrix'], 1, False, glGetFloatv(GL_MODELVIEW_MATRIX)) glActiveTexture(GL_TEXTURE5) glBindTexture(GL_TEXTURE_2D, self.shadowTexture) glUniform1i(self.shader.uniforms['shadowMap'], 5) glUniform1i(self.shader.uniforms['useShadow'], 1); self.apply_camera(self.cam) glLoadIdentity() glCullFace(GL_BACK) self.draw() def render(self): if not self.initialized: self.setup() self.shader.use() self.shadow_pass() self.final_pass() glutSwapBuffers() def mouse_move(self, *args): self.cam.on_mouse_move(*args) self.light.on_mouse_move(*args) def mouse_button(self, b, *args): if b==0: self.light.on_mouse_button(b, *args) else: self.cam.on_mouse_button(b, *args) def main(self): glutDisplayFunc(self.render) glutIdleFunc(self.render) glutMouseFunc(self.mouse_button) glutMotionFunc(self.mouse_move) glutReshapeFunc(self.cam.set_size) #self.setup() glutMainLoop()if __name__ == '__main__': test = Test() test.main()",OpenGL Shadow Mapping using GLSL
Combine user with prefix error with setup.py install," I was trying to install Python packages a system I recently gained access to. I was trying to take advantage of Python's relatively new per user site-packages directory, and the new option --user. (The option is currently undocumented, however it exists for Python 2.6+; you can see the help by running python setup.py install --help.)When I tried running on any package I downloaded, I always got the following error: The error was extremely perplexing because, as you can see, I wasn't providing the --prefix, --exec-prefix, --install-base, or --install-platbase flags as command line options. I wasted a lot of time trying to figure out what the problem was. I document my answer below, in hopes to spare some other poor soul a few hours of yak shaving. <code>  python setup.py install --user error: can't combine user with with prefix/exec_prefix/home or install_(plat)base",Combine --user with --prefix error with setup.py install
print python whithout \n," I have this code: I want to output 'a', without ' ' like this: Is it possible? <code>  >>> for i in xrange(20):... print 'a',... a a a a a a a a a a a a a a a a a a a a aaaaaaaaaaaaaaaaaaaa","Printing without newline (print 'a',) prints a space, how to remove?"
print python without space," I have this code: I want to output 'a', without ' ' like this: Is it possible? <code>  >>> for i in xrange(20):... print 'a',... a a a a a a a a a a a a a a a a a a a a aaaaaaaaaaaaaaaaaaaa","Printing without newline (print 'a',) prints a space, how to remove?"
"Check if module exists, if not install it in python"," I want to check if a module exists, if it doesn't I want to install it.How should I do this?So far I have this code which correctly prints f if the module doesn't exist. <code>  try: import keyringexcept ImportError: print 'f'","Check if module exists, if not install it"
"Python: How to get a value of datetime.today() that is ""timezone aware""?"," I am trying to subtract one date value from the value of datetime.datetime.today() to calculate how long ago something was. But it complains: The value datetime.datetime.today() doesn't seem to be ""timezone aware"", while my other date value is. How do I get a value of datetime.datetime.today() that is timezone aware?Right now, it's giving me the time in local time, which happens to be PST, i.e. UTC - 8 hours. Worst case, is there a way I can manually enter a timezone value into the datetime object returned by datetime.datetime.today() and set it to UTC-8?Of course, the ideal solution would be for it to automatically know the timezone. <code>  TypeError: can't subtract offset-naive and offset-aware datetimes","How do I get a value of datetime.today() in Python that is ""timezone aware""?"
"What's the logical value of ""string"" in python?"," I erroneously wrote this code in Python: It always prints out ""That is a nice name"" even when the input is neither ""Kamran"" nor ""Samaneh"". Am I correct in saying that it considers ""Samaneh"" as a true? Why?By the way, I already noticed my mistake. The correct form is: <code>  name = input(""what is your name?"")if name == ""Kamran"" or ""Samaneh"": print(""That is a nice name"")else: print(""You have a boring name ;)"") if name == ""Kamran"" or name == ""Samaneh"":","What's the logical value of ""string"" in Python?"
How can you get the call tree with python profilers?," I used to use a nice Apple profiler that is built into the System Monitor application. As long as your C++ code was compiled with debug information, you could sample your running application and it would print out an indented tree telling you what percent of the parent function's time was spent in this function (and the body vs. other function calls).For instance, if main called function_1 and function_2, function_2 calls function_3, and then main calls function_3: I would see this and think, ""Something is taking a long time in the code in the body of function_2. If I want my program to be faster, that's where I should start.""How can I most easily get this exact profiling output for a Python program?I've seen people say to do this: But it's quite messy compared to that elegant call tree. Please let me know if you can easily do this, it would help quite a bit. <code>  main (100%, 1% in function body): function_1 (9%, 9% in function body): function_2 (90%, 85% in function body): function_3 (100%, 100% in function body) function_3 (1%, 1% in function body) import cProfile, pstatsprof = cProfile.Profile()prof = prof.runctx(""real_main(argv)"", globals(), locals())stats = pstats.Stats(prof)stats.sort_stats(""time"") # Or cumulativestats.print_stats(80) # 80 = how many to print",How can you get the call tree with Python profilers?
Python regex parse stream., Is there any way to use regex match on a stream in python?like And I don't want to do this by getting the value of the whole string. I want to know if there's any way to match regex on a srtream(on-the-fly). <code>  reg = re.compile(r'\w+')reg.match(StringIO.StringIO('aa aaa aa')),Python regex parse stream
python import question," What is the difference between these two lines? The first line is ""import QtGui class from module PyQt4"".But what does second line means? ""Import everything from QtGui of module PyQt4"".Is not it the same? <code>  from PyQt4 import QtGuifrom PyQt4.QtGui import *",`from x import y` vs. `from x.y import *`
Add to integers in a list., I have a list of integers and I was wondering if it would be possible to add to individual integers in this list.  <code> ,Add to integers in a list
how to connect to sql server 2008  via an IP address usign pyodbc in python," I am able to connect to server with mssql management studio but not able to connect using python I think some problem in connection string please help below is string I am using. error is like below <code>  import pyodbc as pconnStr = ( r'DRIVER={SQL Server};Server=ip; Network=DBMSSOCN;Initial Catalog=' + database + ';User ID=' + id +';Password=' + pass1 +';Trusted_Connection=True' +';')conn = p.connect(connStr) conn = p.connect(connStr)pyodbc.Error: ('08001', '[08001] [Microsoft][ODBC SQL Server Driver][TCP/IP Sockets]SQL Server does not exist or access denied. (17) (SQLDriverConnectW); [01000] [Microsoft][ODBC SQL Server Driver][TCP/IP Sockets]ConnectionOpen (Connect()). (10060); [01S00] [Microsoft][ODBC SQL Server Driver]Invalid connection string attribute (0)')",How to connect to sql server 2008 via an IP address using pyodbc in Python
Call perl script from python," I've got a Perl script that I want to invoke from a Python script. I've been looking all over, and haven't been successful. I'm basically trying to call the Perl script sending 1 variable to it, but don't need the output of the Perl script, as it is a self contained program. What I've come up with so far is: Only just started Python programming, so I'm sure the above is total nonsense. Any help would be much appreciated. <code>  var = ""/some/file/path/""pipe = subprocess.Popen([""./uireplace.pl"", var], stdin=subprocess.PIPE)pipe.stdin.write(var)pipe.stdin.close()",Call Perl script from Python
Plotting system of (implicit) equations in matpotlib," First off, I'm new to python and matplotlib. I need to plot several systems of implicit equations in one figure.The equations are in form of: Is there an easy way to plot these, other than first making the equations explicit (i.e. y=...)? <code>  3x+2y=1",Plotting system of (implicit) equations in matplotlib
Efficient way to iterate throught xml elements," I have a xml like this: I need to iterate through all <a> and <b> tags, but I don't know how many of them are in document. So I use xpath to handle that: It works, but I have pretty big files, and cProfile shows me that xpath is very expensive to use.I wonder, maybe there is there more efficient way to iterate through indefinitely number of xml-elements? <code>  <a> <b>hello</b> <b>world</b></a><x> <y></y></x><a> <b>first</b> <b>second</b> <b>third</b></a> from lxml import etreedoc = etree.fromstring(xml)atags = doc.xpath('//a')for a in atags: btags = a.xpath('b') for b in btags: print b",Efficient way to iterate through xml elements
recursive search function in python," In a program I'm writing I have Python use the re.search() function to find matches in a block of text and print the results. However, the program exits once it finds the first match in the block of text.How do I do this repeatedly where the program doesn't stop until ALL matches have been found? Is there a separate function to do this? <code> ",How can I find all matches to a regular expression in Python?
Is there a way in Python to return value via output parameter?," Some languages have the feature to return values using parameters also like C#.Lets take a look at an example: So is there anything similar in Python to get a value using parameter, too? <code>  class OutClass{ static void OutMethod(out int age) { age = 26; } static void Main() { int value; OutMethod(out value); // value is now 26 }}",Is there a way in Python to return a value via an output parameter?
How do you append to file in python?, How do you append to the file instead of overwriting it? Is there a special function that appends to the file? <code> ,How do you append to a file?
How do you append to a file in Python?, How do you append to the file instead of overwriting it? Is there a special function that appends to the file? <code> ,How do you append to a file?
How do you append to a file?, How do you append to the file instead of overwriting it? Is there a special function that appends to the file? <code> ,How do you append to a file?
How do you append to a file with newline?, How do you append to the file instead of overwriting it? Is there a special function that appends to the file? <code> ,How do you append to a file?
Splitting text files using a delimiter in python.," OK so I have a string that has this: I'm trying to make so I can use python and split it using | as the delimiter. Here's what I have so far: and that results into this: I know it's incomplete but what do I have to do to finish it? Yes, I tried googling this problem... but it's not happening. :(I want so that I can choose specifically which one from the delimiter so if I was dan.split('|')[1] .. it would pick warrior. See my point? <code>  Dan|warrior|54 #!/usr/bin/env pythondan = 'dan|warrior|54'print dan.split('|') ['dan', 'warrior', '54']",Splitting strings using a delimiter in python.
Proccessing a Django UploadedFile as UTF-8 with universal newlines," In my django application, I provide a form that allows users to upload a file. The file can be in a variety of formats (Excel, CSV), come from a variety of platforms (Mac, Linux, Windows), and be encoded in a variety of encodings (ASCII, UTF-8).For the purpose of this question, let's assume that I have a view which is receiving request.FILES['file'], which is an instance of InMemoryUploadedFile, called file. My problem is that InMemoryUploadedFile objects (like file):Do not support UTF-8 encoding (I see a \xef\xbb\xbf at the beginning of the file, which as I understand is a flag meaning 'this file is UTF-8').Do not support universal newlines (which probably the majority of the files uploaded to this system will need).Complicating the issue is that I wish to pass the file in to the python csv module, which does not natively support Unicode. I will happily accept answers that avoid this issue - once I get django playing nice with UTF-8 I'm sure I can bludgeon csv into doing the same. (Similarly, please ignore the requirement to support Excel - I am waiting until CSV works before I tackle parsing Excel files.)I have tried using StringIO,mmap,codec, and any of a wide variety of ways of accessing the data in an InMemoryUploadedFile object. Each approach has yielded differing errors, none so far have been perfect. This shows some of the code that I feel came the closest: Please note that I haven't spent too much time on the actual parsing algorithm so it may be wildly inefficient, right now I'm more concerned with getting encoding to work as expected.The problem is that the results are also not encoded, despite being wrapped in the Unicode codecs.EncodedFile file wrapper.EDIT: It turns out, the above code does in fact work. codecs.EncodedFile(file,""utf-8"") is the ticket. It turns out the reason I thought it didn't work was that the terminal I was using does not support UTF-8. Live and learn! <code>  import csvimport codecsclass CSVParser: def __init__(self,file): # 'file' is assumed to be an InMemoryUploadedFile object. dialect = csv.Sniffer().sniff(codecs.EncodedFile(file,""utf-8"").read(1024)) file.open() # seek to 0 self.reader = csv.reader(codecs.EncodedFile(file,""utf-8""), dialect=dialect) try: self.field_names = self.reader.next() except StopIteration: # The file was empty - this is not allowed. raise ValueError('Unrecognized format (empty file)') if len(self.field_names) <= 1: # This probably isn't a CSV file at all. # Note that the csv module will (incorrectly) parse ALL files, even # binary data. This will catch most such files. raise ValueError('Unrecognized format (too few columns)') # Additional methods snipped, unrelated to issue",Processing a Django UploadedFile as UTF-8 with universal newlines
list sorting case insensitive in python using operator.attrgetter," hi i have list of dictionaries .. and i want to sort it case insensitive here sort_by have attribute name by whom i want to sort.Now how can i sort with case insensitive ??Please help.. <code>  members = Person.objects.filter(person=person_id)members_list = list(members)members_list.sort( key = operator.attrgetter( sort_by ), reverse = False )",list sorting case insensitive using operator.attrgetter
Python Documentation Question," The principal challenge of multi-threaded applications is coordinating threads that share data or other resources. To that end, the threading module provides a number of synchronization primitives including locks, events, condition variables, and semaphores. While those tools are powerful, minor design errors can result in problems that are difficult to reproduce. So, the preferred approach to task coordination is to concentrate all access to a resource in a single thread and then use the Queue module to feed that thread with requests from other threads. Applications using Queue.Queue objects for inter-thread communication and coordination are easier to design, more readable, and more reliable.It, basically, states to use Queue.Queue for inter-thread communication and coordination, instead of the powerful tools such as semaphores, locks, etc.My question is, what's the drawback of the suggested method? When should one use the more ""powerful tools"" instead, and why?EditTo be clear, I know what semaphores are. I was just wondering why the Python documentation suggests to use the Queue.Queue method instead of the ""powerful tools"" -- I'm simply using the documentation's own verbiage, not coming up with my own. <code> ","Queue.Queue vs semaphores, locks, etc. in multithreaded Python code"
Python - Convert UTC datetime string to local datetime," I've never had to convert time to and from UTC. Recently had a request to have my app be timezone aware, and I've been running myself in circles. Lots of information on converting local time to UTC, which I found fairly elementary (maybe I'm doing that wrong as well), but I can not find any information on easily converting the UTC time to the end-users timezone.In a nutshell, and android app sends me (appengine app) data and within that data is a timestamp. To store that timestamp to utc time I am using: That seems to be working. When my app stores the data, it is being store as 5 hours ahead (I am EST -5)The data is being stored on appengine's BigTable, and when retrieved it comes out as a string like so: How do I convert this string to a DateTime in the users correct time zone?Also, what is the recommended storage for a users timezone information? (How do you typically store tz info ie: ""-5:00"" or ""EST"" etc etc ?) I'm sure the answer to my first question might contain a parameter the answers the second.  <code>  datetime.utcfromtimestamp(timestamp) ""2011-01-21 02:37:21""",Convert UTC datetime string to local datetime
Convert UTC datetime string to local datetime with Python," I've never had to convert time to and from UTC. Recently had a request to have my app be timezone aware, and I've been running myself in circles. Lots of information on converting local time to UTC, which I found fairly elementary (maybe I'm doing that wrong as well), but I can not find any information on easily converting the UTC time to the end-users timezone.In a nutshell, and android app sends me (appengine app) data and within that data is a timestamp. To store that timestamp to utc time I am using: That seems to be working. When my app stores the data, it is being store as 5 hours ahead (I am EST -5)The data is being stored on appengine's BigTable, and when retrieved it comes out as a string like so: How do I convert this string to a DateTime in the users correct time zone?Also, what is the recommended storage for a users timezone information? (How do you typically store tz info ie: ""-5:00"" or ""EST"" etc etc ?) I'm sure the answer to my first question might contain a parameter the answers the second.  <code>  datetime.utcfromtimestamp(timestamp) ""2011-01-21 02:37:21""",Convert UTC datetime string to local datetime
poll object of select module reports an event that does not go away," The problemI expected the script below to print at most one event and then stop (it's written only to illustrate the problem). However, it prints about 70000 events per second. Why?BackgroundI've written a class that uses the pyudev.Monitor class internally. Amongst other things, it polls the fileno supplied by the fileno() method for changes using a poll object.Now I'm trying to write an unit test for my class (I realize I'm supposed to write the unit test first, so no need to point it out), and therefore I need to write my own fileno() method for my mock pyudev.Monitor object, and I need to control it so that I can trigger the poll object to report an event. As the above code demonstrates, I can't make it stop reporting seemingly non-existent events!I can find no acknowledge_event() or similar in the poll class to make the event go away (I suspect there's just one event that's somehow stuck), searching google and this site has yielded nothing. I'm using python 2.6.6 on Ubuntu 10.10. <code>  #!/usr/bin/env pythonfrom select import poll, POLLINfilename = ""test.tmp""# make sure file existsopen(filename, ""a"").close()file = open(filename, ""r+"")p = poll()p.register(file.fileno(), POLLIN)while True: events = p.poll(100) for e in events: print e # Read data, so that the event goes away? file.read()",How does polling a file for changes work?
Python: How do I format a date in Jinja2?," Using Jinja2, how do I format a date field? I know in Python I can simply do this: But how do I format the date in Jinja2? <code>  print(car.date_of_manufacture.strftime('%Y-%m-%d'))",How do I format a date in Jinja2?
python 3: parameterized class factory," I am trying to create a function that is passed a parameter x and returns a new class C. C should be a subclass of a fixed base class A, with only one addition: a certain class attribute is added and is set to equal x.In other words: Is this easy to do? Are there any issues I should be aware of? <code>  class C(A): C.p = x # x is the parameter passed to the factory function","python 3: class ""template"" (function that returns a parameterized class)"
Python: how to do a basic data manipulation tasks?," I have been working with R for several years. R is very strong in data manipulation. I'm learning python and I would like to know how to manipulate data using python. Basically my data sets are organized as data frames (e.g excel sheet). I would like to know (by example) how this kind of basic data manipulation task can be done using python? Your help and example is most appreciated!  <code>  1. Read csv file like the followingvar1, var2, var31, 2, 34, 5, 6 7, 8, 92. Subset data where var2 in ('5', '8') 3. Make a new variable --> var4 = var3 * 34. Transpose this data5. Write to csv file",Python: how to do basic data manipulation like in R?
File Input and Processing - Simple," I was approached by a friend a few days ago - who has very little programming experience, and he has a project that he asked for some help with.Basically - this is what he is attempting to accomplish: He knows a small amount of Python (enough to write the processing script), but he has no idea where to go from here. I made a quick example for him using an ASP page that read in a file, and used IronPython to pass the parameters into a script file and output the results, which worked just as I had expected it.However - for him I was hoping to guide him in the right direction of developing a much simpler application to perform this and was hoping to find some suggestions or hear some thoughts on different approaches. I figure due to his lack of experience the simpler, the better.Thanks guys. <code>  1.) Create a website that can accept text files as input.2.) Read said file and pass the parameters contained in the file to a python script.3.) Output these results of the script on the same webpage upon completion.",File Upload and Processing using Python
Using Python 2.6 how do I get the day of the month as an integer?," I need to write a method that returns the day of the month as an integer. For example, if it's Feb 8th 2011, I want to have method like this: where day would be given the integer value 8 <code>  >>> day = get_day_of_month()","Using Python, get the day of the current month as an integer"
"Using Python, get the day of the month as an integer"," I need to write a method that returns the day of the month as an integer. For example, if it's Feb 8th 2011, I want to have method like this: where day would be given the integer value 8 <code>  >>> day = get_day_of_month()","Using Python, get the day of the current month as an integer"
chart ticks smart calculator," Whatever I'm using matplotlib, Open-Flash-Charts or other charts frameworks I always end needing to find a way to set x/y scales limits and intervals since builtins are not enough smart (or not at all...)just try this in pylab (ipyhton -pylab) to understand what I mean: you'll see just and empty frame grid hiding the 2 horizontal lines under it's its top and bottom borders. I wonder if there is some algorithm around (that I can port to python) to set smartly top and bottom y limits and steps and also calculate every how many values show the x thick.For example, let's say I have 475 measures as (datetime, temperature) as (x, y) with (one every 5 minutes) and My suggestion for this particular case could be to set: 26.4 <= y_scale <= 28.4 with a thick every .2and a tick on x_scale every 12 items (once per hour).But what about if I have just 20 measures over 20 days with -21.5 < temperature < 38.7, and so on? Is there a standardized method around? <code>  In [1]: a, b, x = np.zeros(10), np.ones(10), np.arange(10)In [2]: plot(x, a); plot(x, b) 2011-01-15 10:45:00 < datetime < 2011-01-17 02:20:00 26.5 < temperature < 28.3",Intelligently calculating chart tick positions
Pass many datas from python to C program," I have a Python script and a C program and I need to pass large quantities of data from Python script that call many times the C program. Right now I let the user choose between passing them with an ASCII file or a binary file, but both are quite slow and useless (I mean files are useful if you want to store the data, but I delete these files at the end of the script).os.system doesn't work, the arguments are too much as the C program too uses files to return data to Python, but this is much less data.I wonder what I can use to make this exchange fast. Writing the files to a RAM disk? If so, how can I do this?I heard is possible to call functions from DLL using ctypes, but don't know how to compile my program as a DLL (I use wxdevc+ on Windows 7 64).Or wrap it, but still don't know if it can work and if it is efficient.The data are vertices of a 3D mesh.I'm running the Python script inside another program (blender (open source), and is called many times (usually more than 500 times) because it's inside a cycle. The script send vertices information (1 int index and 3 float coords) to the program, and the program should return many vertices (only int index, because I can find the corresponding vertices with Python).So this is not interactive, it's more like a function (but it's wrote in C). The script + C program (that are add-ons of blender) that I'm writing should be cross-platform because it will be redistributed.The program is actually wrote in C, and from Python I can know the address in memory of the struct that contains the vertices data. If only I know how to do this, should be better to pass to the C program only an address, and from there find all the other vertices (are stored in list).But as far as I know, I can't access to the memory space of another program, and I don't know if calling the program with pipes or whatever initialize a new thread or is run inside the script (that is actually run under the Blender thread)Here is the source and blender/source/blender/makesdna/DNA_meshdata_types.h should be the struct definition <code> ",Pass many pieces of data from Python to C program
What is the most efficient way to traverse a tree in python?," Assuming I have a list of objects that have the following fieldsparentvalueand this defines a tree structure, similar to a directory tree.I want to traverse the list in a pre-order fashion. What's the most efficient way?Normally, in other (more imperative) languages I would iterate the values, finding the ones with no parents, then for each, iterating again for every object whose parent is the one I'm currently looking at and so forth, but is there a cleverer way to do this in Python? <code> ",What is the most efficient way to traverse a tree in Python?
Why does refs increases 2 for every new object in Python?," It is a little weird to me that the refs number in the interactive environment increases 2 after a new object is defined. I created only one object, isn't it? <code>  >>> vTraceback (most recent call last): File ""<stdin>"", line 1, in <module>NameError: name 'v' is not defined[41830 refs]>>> v = ""v""[41832 refs]",Why does refs increase 2 for every new object in Python?
change file creation date on windows from python, How do I change the file creation date of a Windows file from Python? <code> ,How do I change the file creation date of a Windows file?
How do I change the file creation date on windows from python?, How do I change the file creation date of a Windows file from Python? <code> ,How do I change the file creation date of a Windows file?
How do I change the file creation date of a Windows file from Python?, How do I change the file creation date of a Windows file from Python? <code> ,How do I change the file creation date of a Windows file?
How to get python library path ( not sys.executable )," I need to get Path for PyQt library in python program. Program is run as a script from another application, therefore my and I need my actual python path (and path for PyQt library module) im trying with but I'm not sure if it can be robust.Regards!PS. I need it to be able to plug plugins: <code>  sys.executable = 'D:/program files/visum/exe/visum115.exe Path = C:\Python25\Lib\site-packages\PyQt4\plugins os.environ['PYTHONPATH'] qApp.addLibaryPath('C:\Python25\Lib\site-packages\PyQt4\plugins')",How to get path of a python module ( not sys.executable )
how to query seed used by math.random()?," Is there any way to find out what seed Python used to seed its random number generator?I know I can specify my own seed, but I'm quite happy with Python managing it. But, I do want to know what seed it used, so that if I like the results I'm getting in a particular run, I could reproduce that run later. If I had the seed that was used then I could.If the answer is I can't, then what's the best way to generate a seed myself? I want them to always be different from run to run---I just want to know what was used.UPDATE: yes, I mean random.random()! mistake... [title updated] <code> ",how to query seed used by random.random()?
sqlachemy: cascade delete," I must be missing something trivial with SQLAlchemy's cascade options because I cannot get a simple cascade delete to operate correctly -- if a parent element is a deleted, the children persist, with null foreign keys. I've put a concise test case here: Output: There is a simple, one-to-many relationship between Parent and Child. The script creates a parent, adds 3 children, then commits. Next, it deletes the parent, but the children persist. Why? How do I make the children cascade delete? <code>  from sqlalchemy import Column, Integer, ForeignKeyfrom sqlalchemy.orm import relationshipfrom sqlalchemy import create_enginefrom sqlalchemy.orm import sessionmakerfrom sqlalchemy.ext.declarative import declarative_baseBase = declarative_base()class Parent(Base): __tablename__ = ""parent"" id = Column(Integer, primary_key = True)class Child(Base): __tablename__ = ""child"" id = Column(Integer, primary_key = True) parentid = Column(Integer, ForeignKey(Parent.id)) parent = relationship(Parent, cascade = ""all,delete"", backref = ""children"")engine = create_engine(""sqlite:///:memory:"")Base.metadata.create_all(engine)Session = sessionmaker(bind=engine)session = Session()parent = Parent()parent.children.append(Child())parent.children.append(Child())parent.children.append(Child())session.add(parent)session.commit()print ""Before delete, children = {0}"".format(session.query(Child).count())print ""Before delete, parent = {0}"".format(session.query(Parent).count())session.delete(parent)session.commit()print ""After delete, children = {0}"".format(session.query(Child).count())print ""After delete parent = {0}"".format(session.query(Parent).count())session.close() Before delete, children = 3Before delete, parent = 1After delete, children = 3After delete parent = 0",SQLAlchemy: cascade delete
Methods with the same name in one class in python?," How can I declare a few methods with the same name, but with different numbers of parameters or different types in one class?What must I change in the following class? <code>  class MyClass: """""""""""" #---------------------------------------------------------------------- def __init__(self): """"""Constructor"""""" def my_method(self,parameter_A_that_Must_Be_String): print parameter_A_that_Must_Be_String def my_method(self,parameter_A_that_Must_Be_String,parameter_B_that_Must_Be_String): print parameter_A_that_Must_Be_String print parameter_B_that_Must_Be_String def my_method(self,parameter_A_that_Must_Be_String,parameter_A_that_Must_Be_Int): print parameter_A_that_Must_Be_String * parameter_A_that_Must_Be_Int",Methods with the same name in one class in Python
Help with __add__," I am trying to understand how __add__ works: If I put them in a list this works But this does not: What am I doing wrong? How can I get the sum() to work?My problem is how to use the sum on a list of objects that support the __add__, need to keep it as generic as possible. <code>  class MyNum: def __init__(self,num): self.num=num def __add__(self,other): return MyNum(self.num+other.num) def __str__(self): return str(self.num) d=[MyNum(i) for i in range(10)] t=MyNum(0)for n in d: t=t+nprint t print sum(d)TypeError: unsupported operand type(s) for +: 'int' and 'instance'",TypeError after overriding the __add__ method
Embedded a Low Performance Scripting Language in Python," I have a web-application. As part of this, I need users of the app to be able to write (or copy and paste) very simple scripts to run against their data.The scripts really can be very simple, and performance is only the most minor issue. And example of the sophistication of script I mean are something like: where price and cost are a global variables (something I can feed into the environment and access after the computation). I do, however, need to guarantee some stuff.Any scripts run cannot get access to the environment of Python. They cannot import stuff, call methods I don't explicitly expose for them, read or write files, spawn threads, etc. I need total lockdown.I need to be able to put a hard-limit on the number of 'cycles' that a script runs for. Cycles is a general term here. could be VM instructions if the language byte-compiled. Apply-calls for an Eval/Apply loop. Or just iterations through some central processing loop that runs the script. The details aren't as important as my ability to stop something running after a short time and send an email to the owner and say ""your scripts seems to be doing more than adding a few numbers together - sort them out.""It must run on Vanilla unpatched CPython.So far I've been writing my own DSL for this task. I can do that. But I wondered if I could build on the shoulders of giants. Is there a mini-language available for Python that would do this?There are plenty of hacky Lisp-variants (Even one I wrote on Github), but I'd prefer something with more non-specialist syntax (more C or Pascal, say), and as I'm considering this as an alternative to coding one myself I'd like something a bit more mature.Any ideas? <code>  ratio = 1.2345678minimum = 10def convert(money) return money * ratioendif price < minimum cost = convert(minimum)else cost = convert(price)end",Embedding a Low Performance Scripting Language in Python
setDaemon function in thread," I am a newbie in python programming, what I understand is that a process can be a daemon, but a thread in a daemon mode, I couldn't understand the usecase of this, I would request the python gurus to help me in understanding this.  <code> ",setDaemon() method of threading.Thread
Python subprocess.Popen slow under uWSGI/Cherokee," I've set up a development server running Cherokee on Fedora 14, using uWSGI to interface with my WSGI application.When the application is hit with the first request, I spawn a process like so: The first request takes 10-15 seconds to complete (subsequent ones take less than a second).Without the creation of the Popen object, the first request only takes about 2-3 seconds to complete. When I execute the same Popen request from a Python shell, it's instantaneous.What could be causing this behaviour? Have I missed something obvious? <code>  from subprocess import PopenPopen(['bash']) # bash is just an example; the problem happens with all programs",Python subprocess.Popen slow under uWSGI
Python find current directory and file's directory," In Python, what commands can I use to find:the current directory (where I was in the terminal when I ran the Python script), andwhere the file I am executing is? <code> ",Find the current directory and file's directory
Find current directory and file's directory," In Python, what commands can I use to find:the current directory (where I was in the terminal when I ran the Python script), andwhere the file I am executing is? <code> ",Find the current directory and file's directory
"What is the use of ""assert"" in Python?", I have been reading some source code and in several places I have seen the usage of assert. What does it mean exactly? What is its usage? <code> ,"What is the use of ""assert"" in Python?"
"What is the use of ""assert""?", I have been reading some source code and in several places I have seen the usage of assert. What does it mean exactly? What is its usage? <code> ,"What is the use of ""assert"" in Python?"
django paginarion and RawQuerySet," is there a way to paginate a rawqueryset using django's inbuilt pagination?when i cast it to a list , it throws an error in my face ...TypeError: expected string or Unicode object, NoneType found. Is there a way around this? <code> ",django pagination and RawQuerySet
Python: get datetime for '3 years ago today'?," In Python, how do I get a datetime object for '3 years ago today'?UPDATE: FWIW, I don't care hugely about accuracy... i.e. it's Feb 29th today, I don't care whether I'm given Feb 28th or March 1st in my answer. Concision is more important than configurability, in this case. <code> ",Python: get datetime for '3 years ago today'
Python Print String To Text File," I'm using Python to open a text document: I want to substitute the value of a string variable TotalAmount into the text document. Can someone please let me know how to do this? <code>  text_file = open(""Output.txt"", ""w"")text_file.write(""Purchase Amount: "" 'TotalAmount')text_file.close()",Print string to text file
python itertools.chain to chain an iter list?," This works to print the combinations. But this: will not, it will print out I have to rewrite code to make it works as demanded. Here is my question: What is the different between them? According to documentationthe itertools.chain does: So, why does itertools.chain(iter, iter, iter) also works here?Does it means iter, iter, iter = *(iter, iter, iter) ? <code>  import itertoolsdef _yield_sample(): it = iter(itertools.combinations('ABCD', 2)) it2 = iter(itertools.combinations('EFGH', 3)) itc = itertools.chain(it,it2) for x in itc: yield xdef main(): for x in _yield_sample(): print x >>> ('A', 'B')('A', 'C')('A', 'D')... def __position_combination(_count = [2,3,4,5]): its = [] for ct in _count: it = iter(itertools.combinations('ABCDEFG', ct)) its.append(it) itc = itertools.chain(its) for x in itc: yield xdef main(): for x in __position_combination(): print x >>> <itertools.combinations object at 0x02179210><itertools.combinations object at 0x02179240><itertools.combinations object at 0x02179270> itc = itertools.chain(*its) itertools.chain(iter, iter, iter) vs itertools.chain(*[iter,iter,iter]) def chain(*iterables): # chain('ABC', 'DEF') --> A B C D E F for it in iterables: for element in it: yield element",itertools.chain to chain an iter list?
Google App Engine Pyton, Can you recommend me book to Google App Engine in Python?English/French books are both accepted. <code> ,Looking for a good book for Google App Engine Python
How to see the real SQL query in Python cursor.execute," I use the following code in Python (with pyodbc for a MS-Access base). It's Ok but, for maintenance purposes, I need to know the complete and exact SQL string send to the database.Is it possible and how ? <code>  cursor.execute(""select a from tbl where b=? and c=?"", (x, y))",How to see the real SQL query in Python cursor.execute using pyodbc and MS-Access
python assign multiple variables at once," I'm aware I can assign multiple variables to multiple values at once with: And have foo = 1, bar = 2, and so on.But how could I make the names of the variables more dynamic? Ie, And have the same?  <code>  (foo, bar, baz) = 1, 2, 3 somefunction(data,tupleofnames): (return that each name mapped to a single datum)somefunction((1,2,3),(foo,bar,baz)) ",Assign multiple variables at once with dynamic variable names
load javascript in app engine," I got so confused loading JavaScript in Google App Engine. I am using the Django template.First, in my base HTML file, I can't load my downloaded jQuery code from local say, d:/jquery.js, like This line is in my base HTML file. It works when I load jQuery from remote. Like I don't know why.Second, I can't load my own-created JavaScript code to my HTML file. Say I create a JavaScript file, like layout.js, and I try to load it like this in my child HTML file, which, by the way, inherits from the base HTML. And it doesn't work at all. The only way it works I have tried is when I put the actual JavaScript code in the body of my base HTML file. Like I don't know why either... How do I fix it? <code>  <script src=""d:\jquery.js"" type=""text/javascript"" ></script></head>, <script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.5.1/jquery.min.js""type=""text/javascript"" ></script></head> <body><script src=""layout.js"" type=""text/javascript""></script></body> <body><script> $(document).ready( $(""#yes"").click(function() { $(""#no"").hide(""slow""); }));</script>",Load JavaScript in Google App Engine
Load javascript in app engine," I got so confused loading JavaScript in Google App Engine. I am using the Django template.First, in my base HTML file, I can't load my downloaded jQuery code from local say, d:/jquery.js, like This line is in my base HTML file. It works when I load jQuery from remote. Like I don't know why.Second, I can't load my own-created JavaScript code to my HTML file. Say I create a JavaScript file, like layout.js, and I try to load it like this in my child HTML file, which, by the way, inherits from the base HTML. And it doesn't work at all. The only way it works I have tried is when I put the actual JavaScript code in the body of my base HTML file. Like I don't know why either... How do I fix it? <code>  <script src=""d:\jquery.js"" type=""text/javascript"" ></script></head>, <script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.5.1/jquery.min.js""type=""text/javascript"" ></script></head> <body><script src=""layout.js"" type=""text/javascript""></script></body> <body><script> $(document).ready( $(""#yes"").click(function() { $(""#no"").hide(""slow""); }));</script>",Load JavaScript in Google App Engine
object has no attrubute _state," I'm developing Django application, and I have following error My models are constructed like this What I must do? <code>  'Sheep' object has no attribute _state class Animal(models.Model): aul = models.ForeignKey(Aul) weight = models.IntegerField() quality = models.IntegerField() age = models.IntegerField() def __init__(self,aul): self.aul=aul self.weight=3 self.quality=10 self.age=0 def __str__(self): return self.ageclass Sheep(Animal): wool = models.IntegerField() def __init__(self,aul): Animal.__init__(self,aul)",Object has no attribute _state
Generating cyclic permutations / reduced Latin Squares in Python," Was just wondering what's the most efficient way of generating all the circular shifts of a list in Python. In either direction. For example, given a list [1, 2, 3, 4], I want to generate either: where the next permutation is generated by moving the last element to the front, or: where the next permutation is generated by moving the first element to the back.The second case is slightly more interesting to me because it results in a reduced Latin square (the first case also gives a Latin square, just not reduced), which is what I'm trying to use to do experimental block design. It actually isn't that different from the first case since they're just re-orderings of each other, but order does still matter.The current implementation I have for the first case is: For the second case its: The first case seems like it should be reasonably efficient to me, since it uses pop(), but you can't do that in the second case, so I'd like to hear ideas about how to do this more efficiently. Maybe there's something in itertools that will help? Or maybe a double-ended queue for the second case? <code>  [[1, 2, 3, 4], [4, 1, 2, 3], [3, 4, 1, 2], [2, 3, 4, 1]] [[1, 2, 3, 4], [2, 3, 4, 1], [3, 4, 1, 2], [4, 1, 2, 3]] def gen_latin_square(mylist): tmplist = mylist[:] latin_square = [] for i in range(len(mylist)): latin_square.append(tmplist[:]) tmplist = [tmplist.pop()] + tmplist return latin_square def gen_latin_square(mylist): tmplist = mylist[:] latin_square = [] for i in range(len(mylist)): latin_square.append(tmplist[:]) tmplist = tmplist[1:] + [tmplist[0]] return latin_square",Generating circular shifts / reduced Latin Squares in Python
"Task queue created and shown in admin console, but UnknownQueueError when adding tasks."," Updated: Originally I didn't realize this only fails when run from unit tests.I have a working task queue in AppEngine with Python. - When calling a view manually, the task is added to the queue and runs- When called from unit tests, adding the task to the queue fails with an UnknownQueueError. When reading about others who've encountered this issue, there have been some suggestions of overriding taskqueue_stub to fix this. But I'm not sure exactly how this should be done or why. <code> ","Task queue works from view, but UnknownQueueError when run from unit tests."
"Task queue works from view, but UnknownQueueError when when run from unit tests."," Updated: Originally I didn't realize this only fails when run from unit tests.I have a working task queue in AppEngine with Python. - When calling a view manually, the task is added to the queue and runs- When called from unit tests, adding the task to the queue fails with an UnknownQueueError. When reading about others who've encountered this issue, there have been some suggestions of overriding taskqueue_stub to fix this. But I'm not sure exactly how this should be done or why. <code> ","Task queue works from view, but UnknownQueueError when run from unit tests."
Python: how to know function return type and argument types?," While I am aware of the duck-typing concept of Python, I sometimes struggle with the type of arguments of functions, or the type of the return value of the function.Now, if I wrote the function myself, I DO know the types. But what if somebody wants to use and call my functions, how is he/she expected to know the types? I usually put type information in the function's docstring (like: ""...the id argument should be an integer..."" and ""... the function will return a (string, [integer]) tuple."")But is looking up the information in the docstring (and putting it there, as a coder) really the way it is supposed to be done? Edit: While the majority of answers seem to direct towards ""yes, document!"" I feel this is not always very easy for 'complex' types. For example: how to describe concisely in a docstring that a function returns a list of tuples, with each tuple of the form (node_id, node_name, uptime_minutes) and that the elements are respectively a string, string and integer?The docstring PEP documentation doesn't give any guidelines on that.I guess the counterargument will be that in that case classes should be used, but I find python very flexible because it allows passing around these things using lists and tuples, i.e. without classes. <code> ",How to know function return type and argument types?
Programatically Save Draft in Gmail drafts folder," Preferably using Python or Java, I want to compose an email and save it into gmail drafts without user intervention, <code> ",Programmatically Save Draft in Gmail drafts folder
"Python: list to dictionary, multilpe values per key"," I have a Python list which holds pairs of key/value: I want to convert the list into a dictionary, where multiple values per key would be aggregated into a tuple: The iterative solution is trivial: Is there a more elegant, Pythonic solution for this task? <code>  l = [[1, 'A'], [1, 'B'], [2, 'C']] {1: ('A', 'B'), 2: ('C',)} l = [[1, 'A'], [1, 'B'], [2, 'C']]d = {}for pair in l: if pair[0] in d: d[pair[0]] = d[pair[0]] + tuple(pair[1]) else: d[pair[0]] = tuple(pair[1])print(d){1: ('A', 'B'), 2: ('C',)}",list to dictionary conversion with multiple values per key?
"Python: list to dictionary, multiple values per key"," I have a Python list which holds pairs of key/value: I want to convert the list into a dictionary, where multiple values per key would be aggregated into a tuple: The iterative solution is trivial: Is there a more elegant, Pythonic solution for this task? <code>  l = [[1, 'A'], [1, 'B'], [2, 'C']] {1: ('A', 'B'), 2: ('C',)} l = [[1, 'A'], [1, 'B'], [2, 'C']]d = {}for pair in l: if pair[0] in d: d[pair[0]] = d[pair[0]] + tuple(pair[1]) else: d[pair[0]] = tuple(pair[1])print(d){1: ('A', 'B'), 2: ('C',)}",list to dictionary conversion with multiple values per key?
How to setup Postgresql Database in Django?," I'm new to Python and Django.I'm configuring a Django project using a PostgreSQL database engine backend, But I'm getting errors on each database operation. For example when I run manage.py syncdb, I'm getting: Can someone give me a clue on what is going on? <code>  C:\xampp\htdocs\djangodir>python manage.py syncdbTraceback (most recent call last): File ""manage.py"", line 11, in <module> execute_manager(settings) File ""C:\Python27\lib\site-packages\django\core\management\__init__.py"", line438, in execute_manager utility.execute() File ""C:\Python27\lib\site-packages\django\core\management\__init__.py"", line379, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File ""C:\Python27\lib\site-packages\django\core\management\__init__.py"", line261, in fetch_command klass = load_command_class(app_name, subcommand) File ""C:\Python27\lib\site-packages\django\core\management\__init__.py"", line67, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File ""C:\Python27\lib\site-packages\django\utils\importlib.py"", line 35, in import_module __import__(name) File ""C:\Python27\lib\site-packages\django\core\management\commands\syncdb.py"", line 7, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File ""C:\Python27\lib\site-packages\django\core\management\sql.py"", line 6, in <module> from django.db import models File ""C:\Python27\lib\site-packages\django\db\__init__.py"", line 77, in <module> connection = connections[DEFAULT_DB_ALIAS] File ""C:\Python27\lib\site-packages\django\db\utils.py"", line 92, in __getitem__ backend = load_backend(db['ENGINE']) File ""C:\Python27\lib\site-packages\django\db\utils.py"", line 33, in load_backend return import_module('.base', backend_name) File ""C:\Python27\lib\site-packages\django\utils\importlib.py"", line 35, in import_module __import__(name) File ""C:\Python27\lib\site-packages\django\db\backends\postgresql\base.py"", line 23, in <module> raise ImproperlyConfigured(""Error loading psycopg module: %s"" % e)django.core.exceptions.ImproperlyConfigured: Error loading psycopg module: No module named psycopg",How to set up a PostgreSQL database in Django
How to setup PostgreSQL Database in Django?," I'm new to Python and Django.I'm configuring a Django project using a PostgreSQL database engine backend, But I'm getting errors on each database operation. For example when I run manage.py syncdb, I'm getting: Can someone give me a clue on what is going on? <code>  C:\xampp\htdocs\djangodir>python manage.py syncdbTraceback (most recent call last): File ""manage.py"", line 11, in <module> execute_manager(settings) File ""C:\Python27\lib\site-packages\django\core\management\__init__.py"", line438, in execute_manager utility.execute() File ""C:\Python27\lib\site-packages\django\core\management\__init__.py"", line379, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File ""C:\Python27\lib\site-packages\django\core\management\__init__.py"", line261, in fetch_command klass = load_command_class(app_name, subcommand) File ""C:\Python27\lib\site-packages\django\core\management\__init__.py"", line67, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File ""C:\Python27\lib\site-packages\django\utils\importlib.py"", line 35, in import_module __import__(name) File ""C:\Python27\lib\site-packages\django\core\management\commands\syncdb.py"", line 7, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File ""C:\Python27\lib\site-packages\django\core\management\sql.py"", line 6, in <module> from django.db import models File ""C:\Python27\lib\site-packages\django\db\__init__.py"", line 77, in <module> connection = connections[DEFAULT_DB_ALIAS] File ""C:\Python27\lib\site-packages\django\db\utils.py"", line 92, in __getitem__ backend = load_backend(db['ENGINE']) File ""C:\Python27\lib\site-packages\django\db\utils.py"", line 33, in load_backend return import_module('.base', backend_name) File ""C:\Python27\lib\site-packages\django\utils\importlib.py"", line 35, in import_module __import__(name) File ""C:\Python27\lib\site-packages\django\db\backends\postgresql\base.py"", line 23, in <module> raise ImproperlyConfigured(""Error loading psycopg module: %s"" % e)django.core.exceptions.ImproperlyConfigured: Error loading psycopg module: No module named psycopg",How to set up a PostgreSQL database in Django
How to read/make sense of a serialised data string in python," A legacy database I'm accessing via Django has a table column that stores serialised data in the following string format: Is there any way I can use python/python-library to change it into a list or any other friendly python data type for further processing of individual values?Note: These values were written to the database via PHP. <code>  a:5:{i:1;s:4:""1869"";i:2;s:4:""1859"";i:3;s:4:""1715"";i:4;s:1:""0"";i:5;s:1:""0"";}",How to read/make sense of a PHP serialised data string in python
Python - How to check if input is a number (given that input always returns strings)," How do I check if a user's string input is a number (e.g. -1, 0, 1, etc.)? The above won't work since input always returns a string. <code>  user_input = input(""Enter something:"")if type(user_input) == int: print(""Is a number"")else: print(""Not a number"")",How to check if string input is a number?
Python: how to check if input is a number?," How do I check if a user's string input is a number (e.g. -1, 0, 1, etc.)? The above won't work since input always returns a string. <code>  user_input = input(""Enter something:"")if type(user_input) == int: print(""Is a number"")else: print(""Not a number"")",How to check if string input is a number?
How to check if input is a number?," How do I check if a user's string input is a number (e.g. -1, 0, 1, etc.)? The above won't work since input always returns a string. <code>  user_input = input(""Enter something:"")if type(user_input) == int: print(""Is a number"")else: print(""Not a number"")",How to check if string input is a number?
Best way to remove an item from a Python dictionary?," What is the best way to remove an item from a dictionary by value, i.e. when the item's key is unknown? Here's a simple approach: Are there better ways? Is there anything wrong with mutating (deleting items) from the dictionary while iterating it? <code>  for key, item in some_dict.items(): if item is item_to_remove: del some_dict[key]",Remove an item from a dictionary when its key is unknown
get bbox in data coordinates in matplotlib," I have the bbox of a matplotlib.patches.Rectangle object (a bar from a bar graph) in display coordinates, like this: But I would like that not in display coordinates but data coordinates. I'm pretty sure this requires a transform. What's the method for doing this? <code>  Bbox(array([[ 0., 0.],[ 1., 1.]])",Get bbox in data coordinates in matplotlib
Making an numpy array subclass were all shape changing operations return a normal array.," I have an array subclass, where some of the extra attributes are only valid for the object's original shape. Is there a way to make sure that all array shape changing operations return a normal numpy array instead of an instance of my class?I've already written array_wrap, but this doesn't seem to have any effect on operations like np.mean, np.sum or np.rollaxis. These all just return an instance of my class. I figure I have to do something in __new__ or __array_finalize__, but I haven't a clue what.Update:After carefully reading the numpy documentation on subclassing (http://docs.scipy.org/doc/numpy/user/basics.subclassing.html), all array shape changing operations are doing the 'new from template' operation. So the question becomes, how do you make the 'new from template' operation return ndarray instances instead of instances of my class. As far as I can tell, __new__ is never called within these functions.Alternative:Assuming the above is not possible, how do I at least identify in __array_finalize__ the new from template operation (as opposed to view casting)? This would at least let me dereference some attributes that are copied by reference. I could also set a flag or something telling the new instance that its shape is invalid. <code>  import numpy as np class NewArrayClass(np.ndarray): __array_priority__ = 3.0 def __array_wrap__(self, out_arr, context=None): if out_arr.shape == self.shape: out = out_arr.view(new_array) # Do a bunch of class dependant initialization and attribute copying. # ... return out else: return np.asarray(out_arr)A = np.arange(10)A.shape = (5, 2)A = arr.view(NewArrayClass)# Would like this to be np.ndarray, but get new_array_class.print type(np.sum(A, 0))",Making an numpy array subclass where all shape changing operations return a normal array
How do I get the application ID at runtime," I need the Application ID to construct the correct <applicationid>.appspotmail.com address, and I'm having a hard time finding out how to retrieve it at runtime. <code> ",How do I get the Application ID at runtime
Creating a List from a Binary Search Tree - Python," I'm trying to make a list of all items in a binary search tree. I understand the recursion but I don't know how to make it return each value and then append it into a list. I want to create a function called makeList() that will return a list of all the items in my tree. All the functions in my programs work except the makeList() function and are included to make sure everyone understands the basic structure of how I set up my tree. Looking at my makeList() function I can see why it doesn't work but I don't know how to make it work.EDITOk, I got it! And I even got two answers which are: and And looking back I can see that I do not understand recursion very well so it's time to hit the books! Anyone have any good resources on recursion?Another question, so say I call my makeList() function. When Python goes through makeList(), when it gets to the self.makeList(aNode.lChild, a) does it begin running the function again while it's still finishing up the makeList() function or does everything stop and it just starts over with it's new aNode?I hope that makes sense. <code>  class Node(object): def __init__(self, data): self.data = data self.lChild = None self.rChild = Noneclass Tree(object): def __init__(self): self.root = None def __str__(self): current = self.root def isEmpty(self): if self.root == None: return True else: return False def insert (self, item): newNode = Node (item) current = self.root parent = self.root if self.root == None: self.root = newNode else: while current != None: parent = current if item < current.data: current = current.lChild else: current = current.rChild if item < parent.data: parent.lChild = newNode else: parent.rChild = newNode def inOrder(self, aNode): if aNode == None: pass if aNode != None: self.inOrder(aNode.lChild) print aNode.data self.inOrder(aNode.rChild) def makeList(self, aNode): a = [] self.inOrder(aNode) a += [aNode.data] print an = Tree()for i in [4,7,2,9,1]: n.insert(i)n.makeList(n.root) def makeList(self, aNode, a = []): if aNode != None: self.makeList(aNode.lChild, a) a += [aNode.data] self.makeList(aNode.rChild, a) return a def makeList2(self, aNode): if aNode is None: return [] return self.makeList2(aNode.lChild) + [aNode.data] + self.makeList2(aNode.rChild)",Creating a List from a Binary Search Tree
the python try .. except vs if ...else ?, I want to restrict user input so that a provided N obeys N >0 or N < 100.Should I use if... else or try... except? Could you provide examples of both approaches? <code> ,How do I use try .. except or if ...else to validate user input?
Tkinter Option Menu Widget Changing Widths," I have a snippet which creates an OptionMenu widget. One problem I've encountered is every time a new option is selected, the width of the widget changes. I believe this is due to the text within the widget changing widths. How do I make the widget hold a consistent width?  <code>  ...options = ('White', 'Grey', 'Black', 'Red', 'Orange', 'Yellow', 'Green', 'Blue', 'Cyan', 'Purple')var = StringVar()optionmenu = OptionMenu(par, var, *options)optionmenu.grid(column=column, row=row)...",How to make a OptionMenu maintain the same width?
Python: string.replace vs re.sub," For Python 2.5, 2.6, should I be using string.replace or re.sub for basic text replacements?In PHP, this was explicitly stated but I can't find a similar note for Python. <code> ",Use Python's string.replace vs re.sub
Most elegant way to modify Python list elements inplace.," I have a 2D list that looks like this: I want to change the last three elements to integers, but the code below feels very ugly: But I'd rather have something that looks like: I think there should be a way to write the code above, but the slice creates an iterator/new list that's separate from the original, so the references don't carry over. Is there some way to get a more Pythonic solution? <code>  table = [['donkey', '2', '1', '0'], ['goat', '5', '3', '2']] for row in table: for i in range(len(row)-1): row[i+1] = int(row[i+1]) for row in table: for col in row[1:]: col = int(col)",Most elegant way to modify elements of nested lists in place
Most elegant way to modify Python list elements inplace," I have a 2D list that looks like this: I want to change the last three elements to integers, but the code below feels very ugly: But I'd rather have something that looks like: I think there should be a way to write the code above, but the slice creates an iterator/new list that's separate from the original, so the references don't carry over. Is there some way to get a more Pythonic solution? <code>  table = [['donkey', '2', '1', '0'], ['goat', '5', '3', '2']] for row in table: for i in range(len(row)-1): row[i+1] = int(row[i+1]) for row in table: for col in row[1:]: col = int(col)",Most elegant way to modify elements of nested lists in place
Django pre_save triggered twicce," I am using django signals for data denormalization. Here is my code: I cannot understand why, but when my Vote instance is being saved, update_post_votes_on_save() is being called twice. I thought there was a bug in my code, but saving through admin interface gives the same result.Docs say something about using dispatch_uid to prevent duplicate calls, but I cannot understand if this is the case. How to use dispatch_uid? I've tried this, but with no luck: Any ideas why function is being called twice and how to avoid it? <code>  # vote was saved@receiver(pre_save, sender=Vote)def update_post_votes_on_save(sender, instance, **kwargs): """""" Update post rating """""" # is vote is being updated, then we must remove previous value first if instance.id: old_vote = Vote.objects.get(pk=instance.id) instance.post.rating -= old_vote.value # now adding the new vote instance.post.rating += instance.value instance.post.save() @receiver(pre_save, sender=Vote, dispatch_uid=""my_unique_identifier"")",Django pre_save triggered twice
Python - Help using pdfminer as a library," I am trying to get text data from a pdf using pdfminer. I am able to extract this data to a .txt file successfully with the pdfminer command line tool pdf2txt.py. I currently do this and then use a python script to clean up the .txt file. I would like to incorporate the pdf extract process into the script and save myself a step. I thought I was on to something when I found this link, but I didn't have success with any of the solutions. Perhaps the function listed there needs to be updated again because I am using a newer version of pdfminer. I also tried the function shown here, but it also did not work. Another approach I tried was to call the script within a script using os.system. This was also unsuccessful.I am using Python version 2.7.1 and pdfminer version 20110227. <code> ",How do I use pdfminer as a library
an expression for an infinite generator?," Is there a straight-forward expression that can produce an infinite iterator?This is a purely theoretical question. No need for a ""practical"" answer here :)For example, it is easy to use a generator expression to make a finite iterator: However, to make an infinite one I need to ""pollute"" my namespace with a bogus function: Doing things in a separate file and import-ing later doesn't count.I also know that itertools.repeat does exactly this. I'm curious if there is a one-liner solution without that. <code>  my_gen = (0 for i in xrange(42)) def _my_gen(): while True: yield 0my_gen = _my_gen()",Is there an expression for an infinite iterator?
Is there an expression for an infinite generator?," Is there a straight-forward expression that can produce an infinite iterator?This is a purely theoretical question. No need for a ""practical"" answer here :)For example, it is easy to use a generator expression to make a finite iterator: However, to make an infinite one I need to ""pollute"" my namespace with a bogus function: Doing things in a separate file and import-ing later doesn't count.I also know that itertools.repeat does exactly this. I'm curious if there is a one-liner solution without that. <code>  my_gen = (0 for i in xrange(42)) def _my_gen(): while True: yield 0my_gen = _my_gen()",Is there an expression for an infinite iterator?
Is there a namespace-package friendly alternative to find_module?," BackgroundI've grown tired of the issue with pylint not being able to import files when you use namespace packages and divide your code-base into separate folders. As such I started digging into the astNG source-code which has been identified as the source of the trouble (see bugreport 8796 on astng). At the heart of the issue seems to be the use of pythons own imp.find_module in the process of finding imports.What happens is that the import's first (sub)package - a in import a.b.c - is fed to find_module with a None path. Whatever path comes back is then fed into find_module the next pass in the look up loop where you try to find b in the previous example. Pseudo-code from logilab.common.modutils: The ProblemThis is what's broken: you only get the first best hit from find_module, which may or may not have your subpackages in it. If you DON'T find the subpackages, you have no way to back out and try the next one. I tried explicitly using sys.path instead of None, so that the result could be removed from the path list and a second attempt be made, but python's module finder is clever enough that there doesn't have to be an exact match in the paths, making this approach unusable - to the best of my knowledge anyway.Teary-eyed PleaIs there an alternative to find_modules which will return ALL possible matches or take an exclude list? I'm also open to completely different solutions. Preferably not patching python by hand, but it wouldn't be impossible - at least for a local solution.(Caveat emptor: I'm running python 2.6 and for reasons of current company policy can't upgrade, suggestions for p3k etc won't get marked as accepted unless it's the only answer.) <code>  path = Nonewhile import_as_list: try: _, found_path, etc = find_module(import_as_list[0], path) #exception handling and checking for a better version in the .egg files path = [found_path] import_as_list.pop(0)",Alternatives to imp.find_module?
Render unicode as ASCII in django template," I understand that this is a little perverse, but I have a legacy database with some entries as formatted html. I'd like to just push this into my django templates as ASCII and let the browser display it. Django kindly converts my fields to unicode, and so the browser displays the entire text <p> </p> etc. intact.I was hoping that there might be a template flag {{ obj.text|ascii }} or something that might fix this for me - does anyone have any ideas? Thanks. <code> ",Render HTML in django template (not unicode but ASCII) 
iterate through pairs of items in python list," Possible Duplicates: Iterate a list as pair (current, next) in Python Iterating over every two elements in a list Is it possible to iterate a list in the following way in Python (treat this code as pseudocode)? And it should produce <code>  a = [5, 7, 11, 4, 5]for v, w in a: print [v, w] [5, 7][7, 11][11, 4][4, 5]",Iterate through pairs of items in a Python list
Should I set any Windows paths when using virtualenv?, I'm new to virtualenv and not sure how to set up paths. My paths have been set to something like this: Should I remove those paths for virtualenv's activate script to work correctly? If I can keep my paths then how do I call scripts for an env when it has been activated? Do I call scripts by explicitly running them with python.exe instead of simply typing the script name alone? Not sure how to handle the paths and I would appreciate a little guidance. <code>  PYTHONPATH=C:\Python27\PYTHONSTARTUP=C:\Python27\Scripts\startup.pyPATH=%PYTHONPATH%;...;%PYTHONPATH%\Scripts python myscript.py,Python Windows: correct virtualenv paths
Python Windows Paths when using virtualenv, I'm new to virtualenv and not sure how to set up paths. My paths have been set to something like this: Should I remove those paths for virtualenv's activate script to work correctly? If I can keep my paths then how do I call scripts for an env when it has been activated? Do I call scripts by explicitly running them with python.exe instead of simply typing the script name alone? Not sure how to handle the paths and I would appreciate a little guidance. <code>  PYTHONPATH=C:\Python27\PYTHONSTARTUP=C:\Python27\Scripts\startup.pyPATH=%PYTHONPATH%;...;%PYTHONPATH%\Scripts python myscript.py,Python Windows: correct virtualenv paths
Full text searching XML data with Python," TaskI want to use Python for doing full text searches of XML data.Example data Basic functionalityThe most basic functionality I want is that a search for ""other"" in an XPath (""/elements/elem"") returns at least the value of the ID attribute for the matching element (elem 2) and nested element (elem 3, nested 1) or the matching XPaths.Ideal functionalityThe solution should be flexible and scalable. I am looking for possible combinations of these features:search nested elements (infinite depth)search attributessearch for sentences and paragraphssearch using wildcardssearch using fuzzy matchingreturn precise matching infogood search speed for large XML filesQuestionI don't expect a solution with all of the ideal functionality, I'll have to combine different existing functionalities and code the rest myself. But first I would like to know more about what there is out there, which libraries and approaches you would usually use for this, what their pros and cons are.EDIT: Thanks for the answers so far, I added detail and started a bounty. <code>  <elements> <elem id=""1"">some element</elem> <elem id=""2"">some other element</elem> <elem id=""3"">some element <nested id=""1""> other nested element </nested> </elem></elements>","Full text searching XML data with Python: best practices, pros & cons"
using numpy.argmax() on multidemionsal arrays," I have a 4 dimensional array, i.e., data.shape = (20,30,33,288). I am finding the index of the closest array to n using I would like to use data[index] = ""values"" with values.shape = (20,33,288), but data[index] returns the error ""index (8) out of range (0<=index<1) in dimension 0"" or this operation takes a relatively long time to compute and returns a matrix with a shape that doesn't seem to make sense.How do I return a array of correct values? i.e., This seems like a simple problem, is there a simple answer?I would eventually like to find index2 = abs(data - n2).argmin(axis = 1), so I can perform an operation, say sum data at index to data at index2 without looping through the variables. Is this possible?I am using python2.7 and numpy version 1.5.1. <code>  index = abs(data - n).argmin(axis = 1), soindex.shape = (20,33,288) with the indices varying. data[index] = ""values"" with values.shape = (20,33,288)",Using numpy.argmax() on multidimensional arrays
Gui development with ironpython and visual studio 2010," I'm teaching an introductory class to programming and GUI development using Python, and have found that the least overwhelming solution for students new to programming is to use Visual Studio for GUI development.While the GUI development experience with C# and VB is pleasant, I couldn't find a way to do the same with IronPython. I installed IronPython 2.7.1 which includes the Visual Studio tools, and created a WPF IronPython project.I can use the WPF form designer just like VB and C#, however, I couldn't find a convenient way (i.e., comprehensible to the students) in which the GUI elements could be accessed. For example, with VB, you can refer to elements based on their name and then you can modify properties within them. The best I could do with IronPython (which I don't plan to show to the students) is the following: I noticed that the GUI elements do not get a name and Visual Studio crashes whenever I try to manually modify the XAML to name elements. Here is the generated XAML for a simple frame with a button and text area: Any assistance in making this easier on the students would be appreciated. I'm also open to other suggestions for Python GUI development which offer an experience similar to Visual Studio. <code>  import wpffrom System.Windows import Application, Windowclass MyWindow(Window): def __init__(self): wpf.LoadComponent(self, 'WpfApplication3.xaml') def Button_Click(self, sender, e): #This is the only way I could find in which I can #access an element and modify its properties self.Content.Children[1].Text += 'Hello World\n'if __name__ == '__main__': Application().Run(MyWindow()) <Window xmlns=""http://schemas.microsoft.com/winfx/2006/xaml/presentation"" xmlns:x=""http://schemas.microsoft.com/winfx/2006/xaml"" Title=""WpfApplication3"" Height=""300"" Width=""300""> <Grid> <Button Content=""Button"" Height=""23"" HorizontalAlignment=""Left"" Margin=""103,226,0,0"" VerticalAlignment=""Top"" Width=""75"" Click=""Button_Click"" /> <TextBox Height=""182"" HorizontalAlignment=""Left"" Margin=""24,21,0,0"" VerticalAlignment=""Top"" Width=""237"" /> </Grid></Window> ",GUI development with IronPython and Visual Studio 2010
"Can anyone explain me the source code of python ""import this""?"," If you open a Python interpreter, and type ""import this"", as you know, it prints: The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than right now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! In the python source(Lib/this.py) this text is generated by a curious piece of code: <code>  s = """"""Gur Mra bs Clguba, ol Gvz CrgrefOrnhgvshy vf orggre guna htyl.Rkcyvpvg vf orggre guna vzcyvpvg.Fvzcyr vf orggre guna pbzcyrk.Pbzcyrk vf orggre guna pbzcyvpngrq.Syng vf orggre guna arfgrq.Fcnefr vf orggre guna qrafr.Ernqnovyvgl pbhagf.Fcrpvny pnfrf nera'g fcrpvny rabhtu gb oernx gur ehyrf.Nygubhtu cenpgvpnyvgl orngf chevgl.Reebef fubhyq arire cnff fvyragyl.Hayrff rkcyvpvgyl fvyraprq.Va gur snpr bs nzovthvgl, ershfr gur grzcgngvba gb thrff.Gurer fubhyq or bar-- naq cersrenoyl bayl bar --boivbhf jnl gb qb vg.Nygubhtu gung jnl znl abg or boivbhf ng svefg hayrff lbh'er Qhgpu.Abj vf orggre guna arire.Nygubhtu arire vf bsgra orggre guna *evtug* abj.Vs gur vzcyrzragngvba vf uneq gb rkcynva, vg'f n onq vqrn.Vs gur vzcyrzragngvba vf rnfl gb rkcynva, vg znl or n tbbq vqrn.Anzrfcnprf ner bar ubaxvat terng vqrn -- yrg'f qb zber bs gubfr!""""""d = {}for c in (65, 97): for i in range(26): d[chr(i+c)] = chr((i+13) % 26 + c)print """".join([d.get(c, c) for c in s])","What is the source code of the ""this"" module doing?"
Reverse a string in Python two characters at a time," Say you have this string: And you want to reverse it so that it becomes: What would be the most efficient / pythonic solution? I've tried a few different things but they all look horrible...Thanks in advance!Update:In case anyone's interested, this wasn't for homework. I had a script that was processing data from a network capture and returning it as a string of hex bytes. The problem was the data was still in network order. Due to the way the app was written, I didn't want to go back through and try to use say socket.htons, I just wanted to reverse the string. Unfortunately my attempts seemed so hideous, I knew there must be a better way (a more pythonic solution) - hence my question here.  <code>  ABCDEFGH GHEFCDAB",Reverse a string in Python two characters at a time (Network byte order)
Stopwatch In Python," I'm trying to create a simple game where the point is to collect as many blocks as you can in a certain amount of time, say 10 seconds. How can I get a timer to begin ticking at the start of the program and when it reaches 10 seconds, do something (in this case, exit a loop)? <code> ",Timer for Python game
BeautifulSoup and Searching By Attribute," Possible Duplicate: Beautiful Soup cannot find a CSS class if the object has other classes, too I'm using BeautifulSoup to find tables in the HTML. The problem I am currently running into is the use of spaces in the class attribute. If my HTML reads <html><table class=""wikitable sortable"">blah</table></html>, I can't seem to extract it with the following (where I was to be able to find tables with both wikipedia and wikipedia sortable for the class): This will find the table if my HTML is just <html><table class=""wikitable"">blah</table></html> though. Likewise, I have tried using ""wikitable sortable"" in my regular expression, and that won't match either. Any ideas? <code>  BeautifulSoup(html).findAll(attrs={'class':re.compile(""wikitable( sortable)?"")})",BeautifulSoup and Searching By Class
Using Variables in Python Regular Expression, I'm parsing a file and looking in the lines for username-# where the username will change and there can be any number of digits [0-9] after the dash.I have tried nearly every combination trying to use the variable username in the regular expression. Am I even close with something like re.compile('%s-\d*'%user)? <code> ,Using variables in Python regular expression
how to defined a funciton from a string using python," this is my code : but it shows error : so what can i do ,thanks <code>  a = \'''def fun():\n print 'bbb''''eval(a)fun() Traceback (most recent call last): File ""c.py"", line 8, in <module> eval(a) File ""<string>"", line 1 def fun(): ^SyntaxError: invalid syntax",how to define a function from a string using python
"combing plt.plot(x,y) with plt.boxplot()"," I'm trying to combine a normal matplotlib.pyplot plt.plot(x,y) with variable y as a function of variable x with a boxplot. However, I only want a boxplot on certain (variable) locations of x but this does not seem to work in matplotlib? <code> ","Combining plt.plot(x,y) with plt.boxplot()"
Deleting a line form a file in Python," I'm trying to delete a specific line that contains a specific string.I've a file called numbers.txt with the following content: peter tom tom1 yanWhat I want to delete is that tom from the file, so I made this function: The output is: peter yanAs you can see, the problem is that the function delete tom and tom1, but I don't want to delete tom1. I want to delete just tom. This is the output that I want to have: peter tom1 yanAny ideas to change the function to make this correctly? <code>  def deleteLine():fn = 'numbers.txt'f = open(fn)output = []for line in f: if not ""tom"" in line: output.append(line)f.close()f = open(fn, 'w')f.writelines(output)f.close()",Deleting a line from a file in Python
Why zeromq don't work on localhost," This code works great: But this code doesn't* work: It raises this error: ZMQError: No such deviceWhy, can't zeromq use localhost interfaces?Does it only work on IPC on the same machine? <code>  import zmq, json, timedef main(): context = zmq.Context() subscriber = context.socket(zmq.SUB) subscriber.bind(""ipc://test"") subscriber.setsockopt(zmq.SUBSCRIBE, '') while True: print subscriber.recv()def main(): context = zmq.Context() publisher = context.socket(zmq.PUB) publisher.connect(""ipc://test"") while True: publisher.send( ""hello world"" ) time.sleep( 1 ) import zmq, json, timedef recv(): context = zmq.Context() subscriber = context.socket(zmq.SUB) subscriber.bind(""tcp://localhost:5555"") subscriber.setsockopt(zmq.SUBSCRIBE, '') while True: print subscriber.recv()def send(): context = zmq.Context() publisher = context.socket(zmq.PUB) publisher.connect(""tcp://localhost:5555"") while True: publisher.send( ""hello world"" ) time.sleep( 1 )",Why doesn't zeromq work on localhost?
Boxplotting MaskeArray's, How can i boxplot only the non-masked values of a MaskedArray ? I tought this would happen automatically by boxplot(ma) but this seems to boxplot the non-masked array. <code> ,Boxplotting Masked Arrays
How do I check if cartesian coordinates make up a retangle efficiently?," The situation is as follows:There are N arrays.In each array (0..N-1) there are (x,y) tuples (cartesian coordinates) storedThe length of each array can be differentI want to extract the subset of coordinate combinations which make up a completeretangle of size N. In other words; all the cartesian coordinates are adjacent to each other.Example: yields the following: No two points can come from the same set.I first just calculated the cartesian product, but this quickly becomes infeasible (my use-case at the moment has 18 arrays of points with each array roughly containing 10 different coordinates). <code>  findRectangles({ {*(1,1), (3,5), (6,9)}, {(9,4), *(2,2), (5,5)}, {(5,1)}, {*(1,2), (3,6)}, {*(2,1), (3,3)}}) [(1,1),(1,2),(2,1),(2,2)],..., ...(other solutions)...",How do I check if cartesian coordinates make up a rectangle efficiently?
Why dictionary values get fuc*** up in a weird magic sense i can't understand.," When i declare a list 1,2,3,4 and i do something with it , even just print i get back the same sequence 1,2,3,4. But when i do anything with dictionaries , they always change number sequence , like it is being sorted in a twisted way i can't understand . How in the world did 'a' become the first element and 'c' , even if it alphabetically sorted the dictionary it should have been 1,2,3,4 or a,b,c,d not 1,3,2,4 . wT?F @!$!@$#@!So how do i print , get values from dictionary without changing the positions of the elements .? <code>  test1 = [4,1,2,3,6,5]print test1test2 = {""c"":3,""a"":1,""b"":2,""d"":4}print test2 [4, 1, 2, 3, 6, 5]{'a': 1, 'c': 3, 'b': 2, 'd': 4}",Why dictionary values aren't in the inserted order?
how to get a complete exception stack trace in python," The following snippet: Produces this output: What should I use if I want the complete stack trace including the call to a?If it matters I have Python 2.6.6edit: What I'd like to get is the same information I'd get if I left the try except out and let the exception propagate to the top level. This snippet for example: Produces this output: <code>  import tracebackdef a(): b()def b(): try: c() except: traceback.print_exc()def c(): assert Falsea() Traceback (most recent call last): File ""test.py"", line 8, in b c() File ""test.py"", line 13, in c assert FalseAssertionError def a(): b()def b(): c()def c(): assert Falsea() Traceback (most recent call last): File ""test.py"", line 10, in <module> a() File ""test.py"", line 2, in a b() File ""test.py"", line 5, in b c() File ""test.py"", line 8, in c assert FalseAssertionError",How to get a complete exception stack trace in Python
windows + virtualenv + pip + numpy (problems when installing numpy)," On Windows, I normally just use the binary installer, but I would like to install NumPy only in a virtualenv this time, so I created a virtual env: Then I tried to install NumPy And I get an error. My pip.log is pasted below: <code>  virtualenv --no-site-packages --distribute summary_pythoncd summary_python/Scriptsactivate.bat pip install numpy Downloading/unpacking numpy Running setup.py egg_info for package numpy non-existing path in 'numpy\\distutils': 'site.cfg' F2PY Version 2 blas_opt_info: blas_mkl_info: libraries mkl,vml,guide not found in c:\Users\fname.lname\Documents\summary_python\lib libraries mkl,vml,guide not found in C:\ NOT AVAILABLE atlas_blas_threads_info: Setting PTATLAS=ATLAS libraries ptf77blas,ptcblas,atlas not found in c:\Users\fname.lname\Documents\summary_python\lib libraries ptf77blas,ptcblas,atlas not found in C:\ NOT AVAILABLE atlas_blas_info: libraries f77blas,cblas,atlas not found in c:\Users\fname.lname\Documents\summary_python\lib libraries f77blas,cblas,atlas not found in C:\ NOT AVAILABLE blas_info: libraries blas not found in c:\Users\fname.lname\Documents\summary_python\lib libraries blas not found in C:\ NOT AVAILABLE blas_src_info: NOT AVAILABLE NOT AVAILABLE lapack_opt_info: lapack_mkl_info: mkl_info: libraries mkl,vml,guide not found in c:\Users\fname.lname\Documents\summary_python\lib libraries mkl,vml,guide not found in C:\ NOT AVAILABLE NOT AVAILABLE atlas_threads_info: Setting PTATLAS=ATLAS libraries ptf77blas,ptcblas,atlas not found in c:\Users\fname.lname\Documents\summary_python\lib libraries lapack_atlas not found in c:\Users\fname.lname\Documents\summary_python\lib libraries ptf77blas,ptcblas,atlas not found in C:\ libraries lapack_atlas not found in C:\ numpy.distutils.system_info.atlas_threads_info NOT AVAILABLE atlas_info: libraries f77blas,cblas,atlas not found in c:\Users\fname.lname\Documents\summary_python\lib libraries lapack_atlas not found in c:\Users\fname.lname\Documents\summary_python\lib libraries f77blas,cblas,atlas not found in C:\ libraries lapack_atlas not found in C:\ numpy.distutils.system_info.atlas_info NOT AVAILABLE lapack_info: libraries lapack not found in c:\Users\fname.lname\Documents\summary_python\lib libraries lapack not found in C:\ NOT AVAILABLE lapack_src_info: NOT AVAILABLE NOT AVAILABLE running egg_info running build_src build_src building py_modules sources building library ""npymath"" sources No module named msvccompiler in numpy.distutils; trying from distutils Running from numpy source directory.c:\Users\fname.lname\Documents\summary_python\build\numpy\numpy\distutils\system_info.py:531: UserWarning: Specified path is invalid. warnings.warn('Specified path %s is invalid.' % d) c:\Users\fname.lname\Documents\summary_python\build\numpy\numpy\distutils\system_info.py:1417: UserWarning: Atlas (http://math-atlas.sourceforge.net/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [atlas]) or by setting the ATLAS environment variable. warnings.warn(AtlasNotFoundError.__doc__) c:\Users\fname.lname\Documents\summary_python\build\numpy\numpy\distutils\system_info.py:1426: UserWarning: Blas (http://www.netlib.org/blas/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [blas]) or by setting the BLAS environment variable. warnings.warn(BlasNotFoundError.__doc__) c:\Users\fname.lname\Documents\summary_python\build\numpy\numpy\distutils\system_info.py:1429: UserWarning: Blas (http://www.netlib.org/blas/) sources not found. Directories to search for the sources can be specified in the numpy/distutils/site.cfg file (section [blas_src]) or by setting the BLAS_SRC environment variable. warnings.warn(BlasSrcNotFoundError.__doc__) c:\Users\fname.lname\Documents\summary_python\build\numpy\numpy\distutils\system_info.py:1333: UserWarning: Atlas (http://math-atlas.sourceforge.net/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [atlas]) or by setting the ATLAS environment variable. warnings.warn(AtlasNotFoundError.__doc__) c:\Users\fname.lname\Documents\summary_python\build\numpy\numpy\distutils\system_info.py:1344: UserWarning: Lapack (http://www.netlib.org/lapack/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [lapack]) or by setting the LAPACK environment variable. warnings.warn(LapackNotFoundError.__doc__) c:\Users\fname.lname\Documents\summary_python\build\numpy\numpy\distutils\system_info.py:1347: UserWarning: Lapack (http://www.netlib.org/lapack/) sources not found. Directories to search for the sources can be specified in the numpy/distutils/site.cfg file (section [lapack_src]) or by setting the LAPACK_SRC environment variable. warnings.warn(LapackSrcNotFoundError.__doc__) error: Unable to find vcvarsall.bat Complete output from command python setup.py egg_info: non-existing path in 'numpy\\distutils': 'site.cfg'F2PY Version 2blas_opt_info:blas_mkl_info: libraries mkl,vml,guide not found in c:\Users\fname.lname\Documents\summary_python\lib libraries mkl,vml,guide not found in C:\ NOT AVAILABLEatlas_blas_threads_info:Setting PTATLAS=ATLAS libraries ptf77blas,ptcblas,atlas not found in c:\Users\fname.lname\Documents\summary_python\lib libraries ptf77blas,ptcblas,atlas not found in C:\ NOT AVAILABLEatlas_blas_info: libraries f77blas,cblas,atlas not found in c:\Users\fname.lname\Documents\summary_python\lib libraries f77blas,cblas,atlas not found in C:\ NOT AVAILABLEblas_info: libraries blas not found in c:\Users\fname.lname\Documents\summary_python\lib libraries blas not found in C:\ NOT AVAILABLEblas_src_info: NOT AVAILABLE NOT AVAILABLElapack_opt_info:lapack_mkl_info:mkl_info: libraries mkl,vml,guide not found in c:\Users\fname.lname\Documents\summary_python\lib libraries mkl,vml,guide not found in C:\ NOT AVAILABLE NOT AVAILABLEatlas_threads_info:Setting PTATLAS=ATLAS libraries ptf77blas,ptcblas,atlas not found in c:\Users\fname.lname\Documents\summary_python\lib libraries lapack_atlas not found in c:\Users\fname.lname\Documents\summary_python\lib libraries ptf77blas,ptcblas,atlas not found in C:\ libraries lapack_atlas not found in C:\numpy.distutils.system_info.atlas_threads_info NOT AVAILABLEatlas_info: libraries f77blas,cblas,atlas not found in c:\Users\fname.lname\Documents\summary_python\lib libraries lapack_atlas not found in c:\Users\fname.lname\Documents\summary_python\lib libraries f77blas,cblas,atlas not found in C:\ libraries lapack_atlas not found in C:\numpy.distutils.system_info.atlas_info NOT AVAILABLElapack_info: libraries lapack not found in c:\Users\fname.lname\Documents\summary_python\lib libraries lapack not found in C:\ NOT AVAILABLElapack_src_info: NOT AVAILABLE NOT AVAILABLErunning egg_inforunning build_srcbuild_srcbuilding py_modules sourcesbuilding library ""npymath"" sourcesNo module named msvccompiler in numpy.distutils; trying from distutilsRunning from numpy source directory.c:\Users\fname.lname\Documents\summary_python\build\numpy\numpy\distutils\system_info.py:531: UserWarning: Specified path is invalid. warnings.warn('Specified path %s is invalid.' % d)c:\Users\fname.lname\Documents\summary_python\build\numpy\numpy\distutils\system_info.py:1417: UserWarning: Atlas (http://math-atlas.sourceforge.net/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [atlas]) or by setting the ATLAS environment variable. warnings.warn(AtlasNotFoundError.__doc__)c:\Users\fname.lname\Documents\summary_python\build\numpy\numpy\distutils\system_info.py:1426: UserWarning: Blas (http://www.netlib.org/blas/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [blas]) or by setting the BLAS environment variable. warnings.warn(BlasNotFoundError.__doc__)c:\Users\fname.lname\Documents\summary_python\build\numpy\numpy\distutils\system_info.py:1429: UserWarning: Blas (http://www.netlib.org/blas/) sources not found. Directories to search for the sources can be specified in the numpy/distutils/site.cfg file (section [blas_src]) or by setting the BLAS_SRC environment variable. warnings.warn(BlasSrcNotFoundError.__doc__)c:\Users\fname.lname\Documents\summary_python\build\numpy\numpy\distutils\system_info.py:1333: UserWarning: Atlas (http://math-atlas.sourceforge.net/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [atlas]) or by setting the ATLAS environment variable. warnings.warn(AtlasNotFoundError.__doc__)c:\Users\fname.lname\Documents\summary_python\build\numpy\numpy\distutils\system_info.py:1344: UserWarning: Lapack (http://www.netlib.org/lapack/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [lapack]) or by setting the LAPACK environment variable. warnings.warn(LapackNotFoundError.__doc__)c:\Users\fname.lname\Documents\summary_python\build\numpy\numpy\distutils\system_info.py:1347: UserWarning: Lapack (http://www.netlib.org/lapack/) sources not found. Directories to search for the sources can be specified in the numpy/distutils/site.cfg file (section [lapack_src]) or by setting the LAPACK_SRC environment variable. warnings.warn(LapackSrcNotFoundError.__doc__)error: Unable to find vcvarsall.bat----------------------------------------Command python setup.py egg_info failed with error code 1Exception information:Traceback (most recent call last): File ""c:\Users\fname.lname\Documents\summary_python\lib\site-packages\pip-1.0.1-py2.7.egg\pip\basecommand.py"", line 126, in main self.run(options, args) File ""c:\Users\fname.lname\Documents\summary_python\lib\site-packages\pip-1.0.1-py2.7.egg\pip\commands\install.py"", line 223, in run requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle) File ""c:\Users\fname.lname\Documents\summary_python\lib\site-packages\pip-1.0.1-py2.7.egg\pip\req.py"", line 986, in prepare_files req_to_install.run_egg_info() File ""c:\Users\fname.lname\Documents\summary_python\lib\site-packages\pip-1.0.1-py2.7.egg\pip\req.py"", line 222, in run_egg_info command_desc='python setup.py egg_info') File ""c:\Users\fname.lname\Documents\summary_python\lib\site-packages\pip-1.0.1-py2.7.egg\pip\__init__.py"", line 255, in call_subprocess % (command_desc, proc.returncode))InstallationError: Command python setup.py egg_info failed with error code 1",Windows + virtualenv + pip + NumPy (problems when installing NumPy)
Python replace multiple strings," I would like to use the .replace function to replace multiple strings.I currently have but would like to have something like although that does not feel like good syntaxwhat is the proper way to do this? kind of like how in grep/regex you can do \1 and \2 to replace fields to certain search strings <code>  string.replace(""condition1"", """") string.replace(""condition1"", """").replace(""condition2"", ""text"")",How to replace multiple substrings of a string?
How to alternate around directories usgin subprocess," I want to change the current directory using subprocess.For example: I think that this should work like a command line unix But it doesn't happen..How must I do to change the current dir?Thanks. <code>  import os, sys, subprocessos.environ['a'] = '/home'os.environ['b'] = '/'subprocess.call('cd $a', shell=True)subprocess.call('ls', shell=True)subprocess.call('cd $b', shell=True)subprocess.call('ls', shell=True) $ export a='/home'$ export b='/'$ cd $a$ ls$ cd $b$ ls",How to alternate around directories using subprocess
Celery and Python's logging inside tasks," I'm wondering how to setup a more specific logging system. All my tasks use as a module-wide logger.I want celery to log to ""celeryd.log"" and my tasks to ""tasks.log"" but I got no idea how to get this working. Using CELERYD_LOG_FILE from django-celery I can route all celeryd related log messages to celeryd.log but there is no trace of the log messages created in my tasks. <code>  logger = logging.getLogger(__name__)",Send log messages from all celery tasks to a single file
Celery and Python's logging," I'm wondering how to setup a more specific logging system. All my tasks use as a module-wide logger.I want celery to log to ""celeryd.log"" and my tasks to ""tasks.log"" but I got no idea how to get this working. Using CELERYD_LOG_FILE from django-celery I can route all celeryd related log messages to celeryd.log but there is no trace of the log messages created in my tasks. <code>  logger = logging.getLogger(__name__)",Send log messages from all celery tasks to a single file
Celery and Python's logging," I'm wondering how to setup a more specific logging system. All my tasks use as a module-wide logger.I want celery to log to ""celeryd.log"" and my tasks to ""tasks.log"" but I got no idea how to get this working. Using CELERYD_LOG_FILE from django-celery I can route all celeryd related log messages to celeryd.log but there is no trace of the log messages created in my tasks. <code>  logger = logging.getLogger(__name__)",Send log messages from all celery tasks to a single file
pip unusable. how to fix DistributionNotFound error?," Whenever i try to use pip I get an error. For exampple: I feel tempted to change the value of into pip==0.8.2.. but I dont feel dealing with the consequences of 'hacking' up my installation...I'm running python 2.7 and pip is at version 0.8.2. <code>  $ sudo pip install gevent-websocketTraceback (most recent call last): File ""/usr/local/bin/pip"", line 5, in <module>from pkg_resources import load_entry_pointFile ""/usr/lib/python2.7/dist-packages/pkg_resources.py"", line 2675, in <module>parse_requirements(__requires__), Environment()File ""/usr/lib/python2.7/dist-packages/pkg_resources.py"", line 552, in resolveraise DistributionNotFound(req)pkg_resources.DistributionNotFound: pip==0.8.1",pip broke. how to fix DistributionNotFound error?
Youtube Download Link Generator in Python, How to convert something like into something like http://something.flv [the downloadable file of corresponding video]in Python? <code>  http://www.youtube.com/watch?v=ZYffV7qhvTc,YouTube download link generator in Python
The best way to store an python list to a database?," What would be the best way of storing a python list of numbers (such as [4, 7, 10, 39, 91]) to a database? I am using the Pyramid framework with SQLAlchemy to communicate to a database.Thanks! <code> ",The best way to store a python list to a database?
How to make a python dictionary that returns key for keys missing from the dictionary instead of raising KeyError?, I want to create a python dictionary that returns me the key value for the keys are missing from the dictionary.Usage example: <code>  dic = smart_dict()dic['a'] = 'one a'print(dic['a'])# >>> one aprint(dic['b'])# >>> b,How to make a dictionary that returns key for keys missing from the dictionary instead of raising KeyError?
Logging Uncaught Exceptions in Python," How do you cause uncaught exceptions to output via the logging module rather than to stderr?I realize the best way to do this would be: But my situation is such that it would be really nice if logging.exception(...) were invoked automatically whenever an exception isn't caught. <code>  try: raise Exception, 'Throwing a boring exception'except Exception, e: logging.exception(e)",Logging uncaught exceptions in Python
Check if an integer got decimals in Python," I would like to determine if a numeric value in Python is a whole number. For example, given: I want to distinguish between values of x which are evenly divisible by 3 those which are not. <code>  y = x / 3",Determining whether an value is a whole number in Python
How can I see the details of an exception in pythons debugger?," Sometimes while I'm debugging an exception will be raised.For example, consider this code: While debugging from the some_function() call, if I issue a next command I will see the following details about the exception that was raised [and caught]: Here's a straight copy / paste from the terminal I was working in: It would be useful to see the entire exception message. How can I do this in pdb? <code>  def some_function(): # Pretend this function is in a library... # ...and deep within the library is an exception: raise Exception('An exception message with valuable information.')import pdb; pdb.set_trace()try: some_function() # Pretend I am debugging from this point using pdb.except: pass Exception: Exceptio...ation.',) > /tmp/test.py(7)<module>()-> some_function() # Pretend I am debugging from this point using pdb.(Pdb) nextException: Exceptio...ation.',)> /tmp/test.py(7)<module>()-> some_function() # Pretend I am debugging from this point using pdb.(Pdb) ",How can I see the details of an exception in Python's debugger?
"How Are Deques in Python Implemented, and When are They Worse Than Lists?"," I've recently gotten into investigating how various data structures are implemented in Python in order to make my code more efficient. In investigating how lists and deques work, I found that I can get benefits when I want to shift and unshift reducing the time from O(n) in lists to O(1) in deques (lists being implemented as fixed-length arrays that have to be copied completely each time something is inserted at the front, etc...). What I can't seem to find are the specifics of how a deque is implemented, and the specifics of its downsides v.s. lists. Can someone enlighten me on these two questions? <code> ","How are deques in Python implemented, and when are they worse than lists?"
matplotlib: Drawing R-style axis ticks that extend outward from the axes," Because they are drawn inside the plot area, axis ticks are obscured by the data in many matplotlib plots. A better approach is to draw the ticks extending from the axes outward, as is the default in ggplot, R's plotting system.In theory, this can be done by redrawing the tick lines with the TICKDOWN and TICKLEFT line-styles for the x-axis and y-axis ticks respectively: But in practice, that's only half the solution because the get_xticklines and get_yticklines methods return only the major tick lines. The minor ticks remain pointing inward.What's the work-around for the minor ticks? <code>  import matplotlib.pyplot as pltimport matplotlib.ticker as mpltickerimport matplotlib.lines as mpllines# Create everything, plot some data stored in `x` and `y`fig = plt.figure()ax = fig.gca()plt.plot(x, y)# Set position and labels of major and minor ticks on the y-axis# Ignore the details: the point is that there are both major and minor ticksax.yaxis.set_major_locator(mplticker.MultipleLocator(1.0))ax.yaxis.set_minor_locator(mplticker.MultipleLocator(0.5))ax.xaxis.set_major_locator(mplticker.MultipleLocator(1.0))ax.xaxis.set_minor_locator(mplticker.MultipleLocator(0.5))# Try to set the tick markers to extend outward from the axes, R-stylefor line in ax.get_xticklines(): line.set_marker(mpllines.TICKDOWN)for line in ax.get_yticklines(): line.set_marker(mpllines.TICKLEFT)# In real life, we would now move the tick labels farther from the axes so our# outward-facing ticks don't cover them upplt.show()","In matplotlib, how do you draw R-style axis ticks that point outward from the axes?"
removing dictonary entries with no values- Python," If I have a dictionary, and I want to remove the entries in which the value is an empty list [] how would I go about doing that?I tried: but that didn't work. <code>  for x in dict2.keys(): if dict2[x] == []: dict2.keys().remove(x)",removing dictionary entries with no values- Python
regular expressions Python error with Unicode - difference with sub vs findall," I am having difficulty trying to figure out a bug in my Python (2.7) script. I am getting an difference with using sub and findall in recognizing special characters.Here is the code: When I use findall, it correctly sees as an alphabetic character, but when I use sub it replaces this--treating it as a non-alphabetic character.I've been able to get the correct functionality using findall with string.replace, but this seems like a bad solution. Also, I want to use re.split, and I'm having the same problems as with re.sub.Thanks in advance for the help. <code>  >>> re.sub(ur""[^-' ().,\w]+"", '' , u'Castaeda', re.UNICODE)u'Castaeda'>>> re.findall(ur""[^-' ().,\w]+"", u'Castaeda', re.UNICODE)[]",Regular expressions and Unicode in Python: difference between sub and findall
Access USB using Python and pyserial," How do I access the USB port using pyserial? I have seen an example with: I used to access the serial port from MATLAB on Windows and using the appropriate syntax, /dev/ttyUSB0 would be replaced by COM1 or any other COM port.I'm on a Mac and I tried using the serial port scanners on the pyserial documentation to no avail. I think I should write it like this: How do I find out what name should be on a Mac?EDIT: In response to an answer below, I'd like to find out how to access both USB to RS232 converters as well as pure USB ports. <code>  import serialser = serial.Serial('/dev/ttyUSB0') import serialname = ? # Names of serial ports on Mac OS Xser = serial.Serial(name)",Access USB serial ports using Python and pyserial
creating elevation/height field gdal numpy pyton," I would like to create some elevation/heightfield rasters using python, gdal and numpy. I'm stuck on numpy (and probably python and gdal.)In numpy, I've been attempting the following: from osgeo import gdalfrom gdalconst import * I figure I'm missing something simple and look forward to your advice.Thanks,Chris(continued later)terragendataset.cpp,v 1.2*Project: Terragen(tm) TER DriverPurpose: Reader for Terragen TER documentsAuthor: Ray Gardener, Daylon Graphics Ltd.*Portions of this module derived from GDAL drivers by Frank Warmerdam, see http://www.gdal.orgMy apologies in advance to Ray Gardener and Frank Warmerdam.Terragen format notes:For writing: SCAL = gridpost distance in meters hv_px = hv_m / SCAL span_px = span_m / SCAL offset = see TerragenDataset::write_header() scale = see TerragenDataset::write_header() physical hv = (hv_px - offset) * 65536.0/scaleWe tell callers that: This tells me that prior to my above WriteArray(somearray) I have to set both the GeoTransformand SetProjection and SetUnitType for things to work (potentially)From the GDAL API Tutorial:Python import osr import numpy My apologies for creating an overly long post and a confession. If and when I get this right, it would be nice to have all the notes in one place (the long post). The Confession:I have previously taken a picture (jpeg), converted it to a geotiff, and imported it as tiles into a PostGIS database. I am now attempting to create elevation raster over which to drape the picture. This probably seems silly, but there is an artist whom I wish to honor, while at thesame time not offending those who have worked assiduously to create and improve these great tools. The artist is Belgian so meters would be in order. She works in lower Manhattan, NY so, UTM 18. Now some reasonable SetGeoTransform. The above mentioned picture is w=3649/h=2736, I'll have to give some thought to this. Another try: Clearly getting closer but unclear if the SetUTM(18,1) was picked up. Is this a 4x4 in theHudson River or a Local_CS(coordinate system)? What's a silent fail?More Reading 4x4 meters is a pretty small logical span.So, perhaps this is as good as it gets. The SetGeoTransform gets the units right, sets the scale and you have your Terragen World Space.Final Thought: I don't program, but to an extent I can follow along. That said, I kinda wondered first if and then what the data looked like in my little Terragen World Space(the following thanks to http://www.gis.usu.edu/~chrisg/python/2009/ week 4): So this is gratifying. I imagine the difference between the above used numpy cand this result goes to the actions of applying Float16 across this very smalllogical span.And on to 'hf2': Nearly completely gratifying, though it looks like I'm in La Concordia Peru. So I haveto figure out how to say- like more north, like New York North. Does SetUTM take a third element suggesting 'how far' north or south. Seems I ran across a UTM chart yesterday that had letter label zones, something like C at the equator and maybe T or S for New York area. I actually thought the SetGeoTransform was essentially establishing the top left northing and easting and this was influencing the that 'how far north/south' part of SetUTM. Off to gdal-dev.And later still:Paddington Bear went to Peru because he had a ticket. I got there because I said that's where I wanted to be. Terragen, working as it does, gave me my pixel space. The subsequent calls to srs acted upon the hf2 (SetUTM), but the easting and northing were established under Terragen so the UTM 18 got set, but in a bounding box at the equator. Good enough.gdal_translate got me to New York. I'm in windows so a command line. and the result; So, we're back in NY. There are probably better ways to approach all this. I had to have a target that accepted Create as I was postulating/improvising a dataset from numpy as well. I need to look at other formats that allow create. Elevation in GeoTiff is a possibility (I think.)My thanks to all Comments, suggestions, and gentle nudges toward appropriate reading. Making maps in python is fun!Chris <code>  >>> a= numpy.linspace(4,1,4, endpoint=True)>>> b= numpy.vstack(a)>>> c=numpy.repeat(b,4,axis=1)>>> carray([[ 4., 4., 4., 4.], [ 3., 3., 3., 3.], [ 2., 2., 2., 2.], [ 1., 1., 1., 1.]]) #This is the elevation data I want >>> format = ""Terragen"">>> driver = gdal.GetDriverByName(format) >>> dst_ds = driver.Create('test.ter', 4,4,1,gdal.GDT_Float32, ['MINUSERPIXELVALUE = 1', 'MAXUSERPIXELVALUE= 4']) >>> raster = numpy.zeros((4,4), dtype=numpy.float32) #this is where I'm messing up>>> dst_ds.GetRasterBand(1).WriteArray(raster) # gives the null elevation data I asked for in (raster) 0>>> dst_ds = None Elevations are Int16 when reading, and Float32 when writing. We need logical elevations when writing so that we can encode them with as much precision as possible when going down to physical 16-bit ints. Implementing band::SetScale/SetOffset won't work because it requires callers to know format write details. So we've added two Create() options that let the caller tell us the span's logical extent, and with those two values we can convert to physical pixels. ds::SetGeoTransform() lets us establish the size of ground pixels. ds::SetProjection() lets us establish what units ground measures are in (also needed to calc the size of ground pixels). band::SetUnitType() tells us what units the given Float32 elevations are in. dst_ds.SetGeoTransform( [ 444720, 30, 0, 3751320, 0, -30 ] )srs = osr.SpatialReference()srs.SetUTM( 11, 1 )srs.SetWellKnownGeogCS( 'NAD27' )dst_ds.SetProjection( srs.ExportToWkt() )raster = numpy.zeros( (512, 512), dtype=numpy.uint8 ) #we've seen this before dst_ds.GetRasterBand(1).WriteArray( raster )# Once we're done, close properly the datasetdst_ds = None >>> format = ""Terragen"">>> driver = gdal.GetDriverByName(format)>>> dst_ds = driver.Create('test_3.ter', 4,4,1, gdal.GDT_Float32, ['MINUSERPIXELVALUE=1','MAXUSERPIXELVALUE-4']) >>> type(dst_ds)<class 'osgeo.gdal.Dataset'>>>> import osr>>> dst_ds.SetGeoTransform([582495, 1, 0.5, 4512717, 0.5, -1]) #x-res 0.5, y_res 0.5 a guess0>>> type(dst_ds)<class 'osgeo.gdal.Dataset'>>>> srs = osr.SpatialReference()>>> srs.SetUTM(18,1)0>>> srs.SetWellKnownGeogCS('WGS84')0>>> dst_ds.SetProjection(srs.ExportToWkt())0>>> raster = c.astype(numpy.float32)>>> dst_ds.GetRasterBand(1).WriteArray(raster)0>>> dst_ds = None>>> from osgeo import gdalinfo>>> gdalinfo.main(['foo', 'test_3.ter'])Driver: Terragen/Terragen heightfieldFiles: test_3.terSize is 4, 4Coordinate System is:LOCAL_CS[""Terragen world space"", UNIT[""m"",1]]Origin = (0.000000000000000,0.000000000000000)Pixel Size = (1.000000000000000,1.000000000000000)Metadata: AREA_OR_POINT=PointCorner Coordinates:Upper Left ( 0.0000000, 0.0000000) Lower Left ( 0.0000000, 4.0000000) Upper Right ( 4.0000000, 0.0000000) Lower Right ( 4.0000000, 4.0000000) Center ( 2.0000000, 2.0000000) Band 1 Block=4x1 Type=Int16, ColorInterp=Undefined Unit Type: mOffset: 2, Scale:7.62939453125e-050 // Terragen files aren't really georeferenced, but // we should get the projection's linear units so // that we can scale elevations correctly.// Increase the heightscale until the physical span// fits within a 16-bit range. The smaller the logical span,// the more necessary this becomes. >>> fn = 'test_3.ter'>>> ds = gdal.Open(fn, GA_ReadOnly)>>> band = ds.GetRasterBand(1)>>> data = band.ReadAsArray(0,0,1,1)>>> dataarray([[26214]], dtype=int16)>>> data = band.ReadAsArray(0,0,4,4)>>> dataarray([[ 26214, 26214, 26214, 26214], [ 13107, 13107, 13107, 13107], [ 0, 0, 0, 0], [-13107, -13107, -13107, -13107]], dtype=int16)>>> >>> src_ds = gdal.Open('test_3.ter')>>> dst_ds = driver.CreateCopy('test_6.hf2', src_ds, 0)>>> dst_ds.SetGeoTransform([582495,1,0.5,4512717,0.5,-1])0>>> srs = osr.SpatialReference()>>> srs.SetUTM(18,1)0>>> srs.SetWellKnownGeogCS('WGS84')0>>> dst_ds.SetProjection( srs.ExportToWkt())0>>> dst_ds = None>>> src_ds = None>>> gdalinfo.main(['foo','test_6.hf2'])Driver: HF2/HF2/HFZ heightfield rasterFiles: test_6.hf2 test_6.hf2.aux.xmlSize is 4, 4Coordinate System is:PROJCS[""UTM Zone 18, Northern Hemisphere"", GEOGCS[""WGS 84"", DATUM[""WGS_1984"", SPHEROID[""WGS 84"",6378137,298.257223563, AUTHORITY[""EPSG"",""7030""]], TOWGS84[0,0,0,0,0,0,0], AUTHORITY[""EPSG"",""6326""]], PRIMEM[""Greenwich"",0, AUTHORITY[""EPSG"",""8901""]], UNIT[""degree"",0.0174532925199433, AUTHORITY[""EPSG"",""9108""]], AUTHORITY[""EPSG"",""4326""]],PROJECTION[""Transverse_Mercator""],PARAMETER[""latitude_of_origin"",0],PARAMETER[""central_meridian"",-75],PARAMETER[""scale_factor"",0.9996],PARAMETER[""false_easting"",500000],PARAMETER[""false_northing"",0],UNIT[""Meter"",1]]Origin = (0.000000000000000,0.000000000000000)Pixel Size = (1.000000000000000,1.000000000000000)Metadata: VERTICAL_PRECISION=1.000000Corner Coordinates:Upper Left ( 0.0000000, 0.0000000) ( 79d29'19.48""W, 0d 0' 0.01""N)Lower Left ( 0.0000000, 4.0000000) ( 79d29'19.48""W, 0d 0' 0.13""N)Upper Right ( 4.0000000, 0.0000000) ( 79d29'19.35""W, 0d 0' 0.01""N)Lower Right ( 4.0000000, 4.0000000) ( 79d29'19.35""W, 0d 0' 0.13""N)Center ( 2.0000000, 2.0000000) ( 79d29'19.41""W, 0d 0' 0.06""N)Band 1 Block=256x1 Type=Float32, ColorInterp=UndefinedUnit Type: m0>>> C:\Program Files\GDAL>gdalinfo c:/python27/test_9.hf2Driver: HF2/HF2/HFZ heightfield rasterFiles: c:/python27/test_9.hf2Size is 4, 4Coordinate System is:PROJCS[""UTM Zone 18, Northern Hemisphere"",GEOGCS[""NAD83"", DATUM[""North_American_Datum_1983"", SPHEROID[""GRS 1980"",6378137,298.257222101, AUTHORITY[""EPSG"",""7019""]], TOWGS84[0,0,0,0,0,0,0], AUTHORITY[""EPSG"",""6269""]], PRIMEM[""Greenwich"",0, AUTHORITY[""EPSG"",""8901""]], UNIT[""degree"",0.0174532925199433, AUTHORITY[""EPSG"",""9122""]], AUTHORITY[""EPSG"",""4269""]],PROJECTION[""Transverse_Mercator""],PARAMETER[""latitude_of_origin"",0],PARAMETER[""central_meridian"",-75],PARAMETER[""scale_factor"",0.9996],PARAMETER[""false_easting"",500000],PARAMETER[""false_northing"",0],UNIT[""Meter"",1]]Origin = (583862.000000000000000,4510893.000000000000000)Pixel Size = (-1.000000000000000,-1.000000000000000)Metadata:VERTICAL_PRECISION=0.010000Corner Coordinates:Upper Left ( 583862.000, 4510893.000) ( 74d 0'24.04""W, 40d44'40.97""N)Lower Left ( 583862.000, 4510889.000) ( 74d 0'24.04""W, 40d44'40.84""N)Upper Right ( 583858.000, 4510893.000) ( 74d 0'24.21""W, 40d44'40.97""N)Lower Right ( 583858.000, 4510889.000) ( 74d 0'24.21""W, 40d44'40.84""N)Center ( 583860.000, 4510891.000) ( 74d 0'24.13""W, 40d44'40.91""N)Band 1 Block=256x1 Type=Float32, ColorInterp=UndefinedUnit Type: mC:\Program Files\GDAL>",creating elevation/height field gdal numpy python
"Differences between distribute, distutils and setuptools?"," The SituationIm trying to port an open-source library to Python 3. (SymPy, if anyone is wondering.) So, I need to run 2to3 automatically when building for Python 3. To do that, I need to use distribute. Therefore, I need to port the current system, which (according to the doctest) is distutils. The ProblemUnfortunately, Im not sure whats the difference between these modulesdistutils, distribute, setuptools. The documentation is sketchy as best, as they all seem to be a fork of one another, intended to be compatible in most circumstances (but actually, not all)and so on, and so forth. The QuestionCould someone explain the differences? What am I supposed to use? What is the most modern solution? (As an aside, Id also appreciate some guide on porting to Distribute, but thats a tad beyond the scope of the question) <code> ","Differences between distribute, distutils, setuptools and distutils2?"
Python: Dictionary merge by updating but not overwrting if value exists," If I have 2 dicts as follows: In order to 'merge' them: results in But what should I do if I would like to compare each value of the two dictionaries and only update d2 into d1 if values in d1 are empty/None/''?When the same key exists, I would like to only maintain the numerical value (either from d1 or d2) instead of the empty value. If both values are empty, then no problems maintaining the empty value. If both have values, then d1-value should stay.i.e. should result in where 8 is not overwritten by ''. <code>  d1 = {'a': 2, 'b': 4}d2 = {'a': 2, 'b': ''} dict(d1.items() + d2.items()) {'a': 2, 'b': ''} d1 = {'a': 2, 'b': 8, 'c': ''}d2 = {'a': 2, 'b': '', 'c': ''} {'a': 2, 'b': 8, 'c': ''}",Dictionary merge by updating but not overwriting if value exists
Python: Dictionary merge by updating but not overwriting if value exists," If I have 2 dicts as follows: In order to 'merge' them: results in But what should I do if I would like to compare each value of the two dictionaries and only update d2 into d1 if values in d1 are empty/None/''?When the same key exists, I would like to only maintain the numerical value (either from d1 or d2) instead of the empty value. If both values are empty, then no problems maintaining the empty value. If both have values, then d1-value should stay.i.e. should result in where 8 is not overwritten by ''. <code>  d1 = {'a': 2, 'b': 4}d2 = {'a': 2, 'b': ''} dict(d1.items() + d2.items()) {'a': 2, 'b': ''} d1 = {'a': 2, 'b': 8, 'c': ''}d2 = {'a': 2, 'b': '', 'c': ''} {'a': 2, 'b': 8, 'c': ''}",Dictionary merge by updating but not overwriting if value exists
[Python] Music Analysis and Visualization," I'm interested in programming a music visualizer in Python.The first problem is how to get the information from the music? Like volume, frequency, rpm, etc.And from where? From the soundcard or the actual music file?My guess is from the soundcard, but how do I access the soundcard and the wanted information? Preferably in a platform-independed way(Linux is a must).I already read a bit about fourier transformation, but I'm not sure if that's the best way to go.I thought about using OpenGL for the visualization, but I'm still open for suggestions.I've already looked at those wikipages, but didn't find an answer yet:http://wiki.python.org/moin/Audio/ http://wiki.python.org/moin/PythonInMusic <code> ",Music Analysis and Visualization
Nested Python class needs to access variable in parent/owner class," I've seen a few ""solutions"" to this, but the solution every time seems to be ""Don't use nested classes, define the classes outside and then use them normally"". I don't like that answer, because it ignores the primary reason I chose nested classes, which is, to have a pool of constants (associated with the base class) accessible to all sub-class instances which are created.Here is example code: All classes are passed a single param, which is a custom bitstream class. My intention is to have a solution that does not require me to read the idx value for ChildClass while still in the ParentClass. All child-class stream reading should be done in the child class.This example is over simplified. The constant pool is not the only variable i need available to all subclasses. The idx variable is not the only thing read from the stream reader.Is this even possible in python? Is there no way to access the parent's information? <code>  class ParentClass: constant_pool = [] children = [] def __init__(self, stream): self.constant_pool = ConstantPool(stream) child_count = stream.read_ui16() for i in range(0, child_count): children.append(ChildClass(stream)) class ChildClass: name = None def __init__(self, stream): idx = stream.read_ui16() self.name = constant_pool[idx]",Nested Python class needs to access variable in enclosing class
Get a list from a set in python," How do I get the contents of a set() in list[] form in Python?I need to do this because I need to save the collection in Google App Engine and Entity property types can be lists, but not sets. I know I can just iterate over the whole thing, but it seems like there should be a short-cut, or ""best practice"" way to do this. <code> ",How to get a list from a set?
cannot properly install lxml for python," 30 jun 2011 -- I am awarding @Pablo for this question, because of his answer. I am still unable to properly install lxml 2.3 for reasons discussed in his comments. I gather for a little bit of work I could, but I have already spent a ridiculous amount of time on this problem. I have, however, written the code I needed and successfully installed lxml 2.2.8. The code functions with this version. Better yet, Pablo was the only one to properly diagnose the error. Which was libxslt needed to be updated to a version with support for exsltMathXpathCtxtRegister I appreciate everyones help on this question.29 jun 2011 -- updating this question to reflect comments and to greater document my attemptsI should begin by saying I have tried every possible solution and install scenario imaginable. Yes, there are similar questions with this topic but their specific problem and solution are not my own. I have spent about 10-15 hours on this and I only continue to become more perplexed.My Main ConcernIn short, after installing lxml-2.3 from source or with easy_install-2.7 lxml for python2.7 on centOs5.6, an alternate install of python, I cannot import the module properly. It will install without any obvious error, but It returns the following error when trying to import etree: What I have TriedThe two most common suggestions I have encountered are to make sure libxml2 && libxml2-dev and libxslt1 && libxslt1-devThey are. I have installed them through yum. $ yum list libxslt libxslt-devel libxml2 libxml2-devel Loaded plugins: fastestmirror Installed Packages libxml2.i386 2.6.26-2.1.2.8.el5_5.1 installed libxml2.x86_64 2.6.26-2.1.2.8.el5_5.1 installed libxml2-devel.i386 2.6.26-2.1.2.8.el5_5.1 installed libxml2-devel.x86_64 2.6.26-2.1.2.8.el5_5.1 installed libxslt.i386 1.1.17-2.el5_2.2 installed libxslt.x86_64 1.1.17-2.el5_2.2 installed libxslt-devel.i386 1.1.17-2.el5_2.2 installed libxslt-devel.x86_64 1.1.17-2.el5_2.2 installedRe-installed and Confirmed that zlib && zlib-devel are installed. $ yum list zlib zlib-devel Loaded plugins: fastestmirror Installed Packages zlib.i386 1.2.3-3 installed zlib.x86_64 1.2.3-3 installed zlib-devel.i386 1.2.3-3 installed zlib-devel.x86_64 1.2.3-3 installedConfirmed python-devel is installed. I think. a. According to several things I have read a way to check if python-devel is installed is to import distutils. regoogling this question brings this up quickly.b. @Keith suggested I tried to 'install' it anyhow, using this, however, I encountered an error so, I created the dir, and it has since installed. But with no success. I can, however, import xml.etree.cElementTree as etree and/or import xml.etree.ElementTree as etree, but I do require some specific functionality from lxml.etreeOddly, If I try to install lxml under the assumption the dependencies not installed and try something like the following sudo STATIC_DEPS=true easy_install-2.7 lxml 2>&1 | tee -a ~/.lxmlit fails with the outputting the error below. I should tersely note, for the unfamiliar, this seems to solve many people's issues as it will go out and retrieve any dependencies and install them for install lxml.tail -100 ~/.lxml gcc -DHAVE_CONFIG_H -I. -I./include -I./include -D_REENTRANT -I/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/include -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -c runsuite.c gcc -DHAVE_CONFIG_H -I. -I./include -I./include -D_REENTRANT -I/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/include -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -c testchar.c testapi.c: In function test_xmlBufferSetAllocationScheme: testapi.c:18773: warning: comparison of distinct pointer types lacks a cast gcc -DHAVE_CONFIG_H -I. -I./include -I./include -D_REENTRANT -I/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/include -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -c testdict.c gcc -DHAVE_CONFIG_H -I. -I./include -I./include -D_REENTRANT -I/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/include -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -c runxmlconf.c gcc -DHAVE_CONFIG_H -I. -I./include -I./include -D_REENTRANT -I/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/include -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -c testrecurse.c sed -e 's?\@XML_LIBDIR\@?-L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib?g' \ -e 's?\@XML_INCLUDEDIR\@?-I/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/include/libxml2 -I/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/include?g' \ -e 's?\@VERSION\@?2.7.8?g' \ -e 's?\@XML_LIBS\@?-lxml2 -lz -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -liconv -lm ?g' \ < ./xml2Conf.sh.in > xml2Conf.tmp \ && mv xml2Conf.tmp xml2Conf.sh /bin/sh ./libtool --tag=CC --mode=link gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -module -no-undefined -avoid-version -rpath /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -o testdso.la testdso.lo testapi.c: At top level: testapi.c:17989: warning: gen_xmlSchematronPtr defined but not used testapi.c:17992: warning: des_xmlSchematronPtr defined but not used testapi.c:18009: warning: gen_xmlSchematronParserCtxtPtr defined but not used testapi.c:18012: warning: des_xmlSchematronParserCtxtPtr defined but not used testapi.c:34157: warning: gen_xmlSAXHandlerPtr_ptr defined but not used testapi.c:34160: warning: des_xmlSAXHandlerPtr_ptr defined but not used libtool: link: ar cru .libs/testdso.a testdso.o libtool: link: ranlib .libs/testdso.a libtool: link: ( cd "".libs"" && rm -f ""testdso.la"" && ln -s ""../testdso.la"" ""testdso.la"" ) /bin/sh ./libtool --tag=CC --mode=link gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -version-info 9:8:7 -ldl -o libxml2.la -rpath /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib SAX.lo entities.lo encoding.lo error.lo parserInternals.lo parser.lo tree.lo hash.lo list.lo xmlIO.lo xmlmemory.lo uri.lo valid.lo xlink.lo HTMLparser.lo HTMLtree.lo debugXML.lo xpath.lo xpointer.lo xinclude.lo nanohttp.lo nanoftp.lo DOCBparser.lo catalog.lo globals.lo threads.lo c14n.lo xmlstring.lo xmlregexp.lo xmlschemas.lo xmlschemastypes.lo xmlunicode.lo xmlreader.lo relaxng.lo dict.lo SAX2.lo xmlwriter.lo legacy.lo chvalid.lo pattern.lo xmlsave.lo xmlmodule.lo schematron.lo -lz -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -liconv -lm libtool: link: ar cru .libs/libxml2.a SAX.o entities.o encoding.o error.o parserInternals.o parser.o tree.o hash.o list.o xmlIO.o xmlmemory.o uri.o valid.o xlink.o HTMLparser.o HTMLtree.o debugXML.o xpath.o xpointer.o xinclude.o nanohttp.o nanoftp.o DOCBparser.o catalog.o globals.o threads.o c14n.o xmlstring.o xmlregexp.o xmlschemas.o xmlschemastypes.o xmlunicode.o xmlreader.o relaxng.o dict.o SAX2.o xmlwriter.o legacy.o chvalid.o pattern.o xmlsave.o xmlmodule.o schematron.o libtool: link: ranlib .libs/libxml2.a libtool: link: ( cd "".libs"" && rm -f ""libxml2.la"" && ln -s ""../libxml2.la"" ""libxml2.la"" ) /bin/sh ./libtool --tag=CC --mode=link gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o xmllint xmllint.o ./libxml2.la -lz -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -liconv -lm /bin/sh ./libtool --tag=CC --mode=link gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o xmlcatalog xmlcatalog.o ./libxml2.la -lz -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -liconv -lm /bin/sh ./libtool --tag=CC --mode=link gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o testSchemas testSchemas.o ./libxml2.la -lz -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -liconv -lm /bin/sh ./libtool --tag=CC --mode=link gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o testRelax testRelax.o ./libxml2.la -lz -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -liconv -lm /bin/sh ./libtool --tag=CC --mode=link gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o testSAX testSAX.o ./libxml2.la -lz -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -liconv -lm libtool: link: gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o testSAX testSAX.o ./.libs/libxml2.a -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -ldl -lz /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib/libiconv.a -lm libtool: link: gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o testRelax testRelax.o ./.libs/libxml2.a -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -ldl -lz /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib/libiconv.a -lm libtool: link: gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o xmlcatalog xmlcatalog.o ./.libs/libxml2.a -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -ldl -lz /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib/libiconv.a -lm libtool: link: gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o testSchemas testSchemas.o ./.libs/libxml2.a -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -ldl -lz /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib/libiconv.a -lm libtool: link: gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o xmllint xmllint.o ./.libs/libxml2.a -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -ldl -lz /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib/libiconv.a -lm ./..libs/libxml2.a(/.xmlIO.o):libs In function xmlGzfileOpenW': //tmp/libxml2.aeasy_install-(Y2MKTgxmlIO.o/):lxml -2.3/In build/function tmp/libxml2xmlGzfileOpenW-': 2.7.8//xmlIO.c:tmp1247/:easy_install -Y2MKTg/lxml-2.3/build/tmp/undefined libxml2-reference2.7.8/xmlIO.c:1247 :to undefined`gzopen64' ./reference.libs/libxml2.a( xmlIO.o): In functionto ``xmlGzfileOpen_real':gzopen64' /./tmp./libs/easy_installlibxml2.a-(xmlIO.oY2MKTg):/lxml- In 2.3function/build /tmp/libxml2xmlGzfileOpen_real': -/2.7.8tmp//xmlIO.ceasy_install:1175-Y2MKTg/:lxml- 2.3/build/undefinedtmp /referencelibxml2-2.7.8 /toxmlIO.c :1175: gzopen64undefined' reference to gzopen64' collect2: collect2: ld returned 1 exit status ld returned 1 exit status make[2]: *** [testRelax] Error 1 make[2]: *** Waiting for unfinished jobs.... make[2]: *** [testSAX] Error 1 .././.libs/libslibxml2.a/(libxml2.axmlIO.o(xmlIO.o)):: In function In function xmlGzfileOpenW'xmlGzfileOpenW':: / tmp/easy_install-Y2MKTg//tmp/lxmleasy_install--Y2MKTg/lxml-2.32.3//build/tmp/buildlibxml2-2.7.8//xmlIO.ctmp:/libxml2-1247: undefined reference to 2.7.8gzopen64/xmlIO.c:1247: undefined' .reference/ .to libs/gzopen64libxml2.a' (./.xmlIO.olibs/libxml2.a(xmlIO.o)): : In Infunction function xmlGzfileOpen_real '`: xmlGzfileOpen_real/': tmp//tmp/easy_install-Y2MKTgeasy_install-/Y2MKTg/lxml-lxml2.3-2.3//build/tmp/build/libxml2tmp/libxml2--2.7.82.7.8/xmlIO.c:/1175:xmlIO.c :1175: undefinedundefined reference reference toto ``gzopen64' gzopen64'collect2: ld returned 1 exit status collect2: ld returned 1 exit status ./.libs/libxml2.a(xmlIO.o): In function xmlGzfileOpenW': /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2-2.7.8/xmlIO.c:1247: undefined reference togzopen64' ./.libs/libxml2.a(xmlIO.o): In function xmlGzfileOpen_real': /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2-2.7.8/xmlIO.c:1175: undefined reference togzopen64' collect2: ld returned 1 exit status make2: * [testSchemas] Error 1 make2: [xmlcatalog] Error 1 make2: [xmllint] Error 1 make2: Leaving directory /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2-2.7.8' make[1]: *** [all-recursive] Error 1 make[1]: Leaving directory /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2-2.7.8' make: * [all] Error 2 Traceback (most recent call last): File ""/usr/local/bin/easy_install-2.7"", line 8, in load_entry_point('setuptools==0.6c11', 'console_scripts', 'easy_install-2.7')() File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 1712, in main File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 1700, in with_ei_usage File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 1716, in File ""/usr/local/lib/python2.7/distutils/core.py"", line 152, in setup dist.run_commands() File ""/usr/local/lib/python2.7/distutils/dist.py"", line 953, in run_commands self.run_command(cmd) File ""/usr/local/lib/python2.7/distutils/dist.py"", line 972, in run_command cmd_obj.run() File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 211, in run File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 446, in easy_install File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 476, in install_item File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 655, in install_eggs File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 930, in build_and_install File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 919, in run_setup File ""build/bdist.linux-i686/egg/setuptools/sandbox.py"", line 62, in run_setup File ""build/bdist.linux-i686/egg/setuptools/sandbox.py"", line 105, in run File ""build/bdist.linux-i686/egg/setuptools/sandbox.py"", line 64, in File ""setup.py"", line 130, in File ""/tmp/easy_install-Y2MKTg/lxml-2.3/setupinfo.py"", line 56, in ext_modules File ""/tmp/easy_install-Y2MKTg/lxml-2.3/buildlibxml.py"", line 311, in build_libxml2xslt File ""/tmp/easy_install-Y2MKTg/lxml-2.3/buildlibxml.py"", line 253, in cmmi File ""/tmp/easy_install-Y2MKTg/lxml-2.3/buildlibxml.py"", line 236, in call_subprocess Exception: Command ""make -j6"" returned code 2there was considerably more, and if you are interested you can check it out hereI have tried several other methods of installation. including @agilevic's which returned an error virtually identical to the one above. Most others install effortlessly, but all fail on from lxml import etreeWhy not use a yum? Well, I was able to install lxml effortlessly through yum install python-lxml, but that was centOs' default installation of python2.4. At this point it is probably easier to retool my existing project to work with 2.4. In the long run, however, I would truly like to resolve this issue. For myself, and anyone else who may encounter a similar problem in the future.  <code>  Python 2.7.2 (default, Jun 16 2011, 11:53:48) [GCC 4.1.2 20080704 (Red Hat 4.1.2-50)] on linux2Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import lxml>>> from lxml import etreeTraceback (most recent call last): File ""<stdin>"", line 1, in <module>ImportError: /usr/local/lib/python2.7/site-packages/lxml-2.3-py2.7-linux-x86_64.egg/lxml/etree.so: undefined symbol: exsltMathXpathCtxtRegister > error: cannot create %sourcedir /usr/src/redhat/SOURCES","[Updated 30 june] (still) cannot properly install lxml 2.3 for python, but at least 2.2.8 works"
[Updated 29 june] (still) cannot properly install lxml for python," 30 jun 2011 -- I am awarding @Pablo for this question, because of his answer. I am still unable to properly install lxml 2.3 for reasons discussed in his comments. I gather for a little bit of work I could, but I have already spent a ridiculous amount of time on this problem. I have, however, written the code I needed and successfully installed lxml 2.2.8. The code functions with this version. Better yet, Pablo was the only one to properly diagnose the error. Which was libxslt needed to be updated to a version with support for exsltMathXpathCtxtRegister I appreciate everyones help on this question.29 jun 2011 -- updating this question to reflect comments and to greater document my attemptsI should begin by saying I have tried every possible solution and install scenario imaginable. Yes, there are similar questions with this topic but their specific problem and solution are not my own. I have spent about 10-15 hours on this and I only continue to become more perplexed.My Main ConcernIn short, after installing lxml-2.3 from source or with easy_install-2.7 lxml for python2.7 on centOs5.6, an alternate install of python, I cannot import the module properly. It will install without any obvious error, but It returns the following error when trying to import etree: What I have TriedThe two most common suggestions I have encountered are to make sure libxml2 && libxml2-dev and libxslt1 && libxslt1-devThey are. I have installed them through yum. $ yum list libxslt libxslt-devel libxml2 libxml2-devel Loaded plugins: fastestmirror Installed Packages libxml2.i386 2.6.26-2.1.2.8.el5_5.1 installed libxml2.x86_64 2.6.26-2.1.2.8.el5_5.1 installed libxml2-devel.i386 2.6.26-2.1.2.8.el5_5.1 installed libxml2-devel.x86_64 2.6.26-2.1.2.8.el5_5.1 installed libxslt.i386 1.1.17-2.el5_2.2 installed libxslt.x86_64 1.1.17-2.el5_2.2 installed libxslt-devel.i386 1.1.17-2.el5_2.2 installed libxslt-devel.x86_64 1.1.17-2.el5_2.2 installedRe-installed and Confirmed that zlib && zlib-devel are installed. $ yum list zlib zlib-devel Loaded plugins: fastestmirror Installed Packages zlib.i386 1.2.3-3 installed zlib.x86_64 1.2.3-3 installed zlib-devel.i386 1.2.3-3 installed zlib-devel.x86_64 1.2.3-3 installedConfirmed python-devel is installed. I think. a. According to several things I have read a way to check if python-devel is installed is to import distutils. regoogling this question brings this up quickly.b. @Keith suggested I tried to 'install' it anyhow, using this, however, I encountered an error so, I created the dir, and it has since installed. But with no success. I can, however, import xml.etree.cElementTree as etree and/or import xml.etree.ElementTree as etree, but I do require some specific functionality from lxml.etreeOddly, If I try to install lxml under the assumption the dependencies not installed and try something like the following sudo STATIC_DEPS=true easy_install-2.7 lxml 2>&1 | tee -a ~/.lxmlit fails with the outputting the error below. I should tersely note, for the unfamiliar, this seems to solve many people's issues as it will go out and retrieve any dependencies and install them for install lxml.tail -100 ~/.lxml gcc -DHAVE_CONFIG_H -I. -I./include -I./include -D_REENTRANT -I/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/include -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -c runsuite.c gcc -DHAVE_CONFIG_H -I. -I./include -I./include -D_REENTRANT -I/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/include -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -c testchar.c testapi.c: In function test_xmlBufferSetAllocationScheme: testapi.c:18773: warning: comparison of distinct pointer types lacks a cast gcc -DHAVE_CONFIG_H -I. -I./include -I./include -D_REENTRANT -I/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/include -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -c testdict.c gcc -DHAVE_CONFIG_H -I. -I./include -I./include -D_REENTRANT -I/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/include -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -c runxmlconf.c gcc -DHAVE_CONFIG_H -I. -I./include -I./include -D_REENTRANT -I/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/include -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -c testrecurse.c sed -e 's?\@XML_LIBDIR\@?-L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib?g' \ -e 's?\@XML_INCLUDEDIR\@?-I/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/include/libxml2 -I/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/include?g' \ -e 's?\@VERSION\@?2.7.8?g' \ -e 's?\@XML_LIBS\@?-lxml2 -lz -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -liconv -lm ?g' \ < ./xml2Conf.sh.in > xml2Conf.tmp \ && mv xml2Conf.tmp xml2Conf.sh /bin/sh ./libtool --tag=CC --mode=link gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -module -no-undefined -avoid-version -rpath /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -o testdso.la testdso.lo testapi.c: At top level: testapi.c:17989: warning: gen_xmlSchematronPtr defined but not used testapi.c:17992: warning: des_xmlSchematronPtr defined but not used testapi.c:18009: warning: gen_xmlSchematronParserCtxtPtr defined but not used testapi.c:18012: warning: des_xmlSchematronParserCtxtPtr defined but not used testapi.c:34157: warning: gen_xmlSAXHandlerPtr_ptr defined but not used testapi.c:34160: warning: des_xmlSAXHandlerPtr_ptr defined but not used libtool: link: ar cru .libs/testdso.a testdso.o libtool: link: ranlib .libs/testdso.a libtool: link: ( cd "".libs"" && rm -f ""testdso.la"" && ln -s ""../testdso.la"" ""testdso.la"" ) /bin/sh ./libtool --tag=CC --mode=link gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -version-info 9:8:7 -ldl -o libxml2.la -rpath /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib SAX.lo entities.lo encoding.lo error.lo parserInternals.lo parser.lo tree.lo hash.lo list.lo xmlIO.lo xmlmemory.lo uri.lo valid.lo xlink.lo HTMLparser.lo HTMLtree.lo debugXML.lo xpath.lo xpointer.lo xinclude.lo nanohttp.lo nanoftp.lo DOCBparser.lo catalog.lo globals.lo threads.lo c14n.lo xmlstring.lo xmlregexp.lo xmlschemas.lo xmlschemastypes.lo xmlunicode.lo xmlreader.lo relaxng.lo dict.lo SAX2.lo xmlwriter.lo legacy.lo chvalid.lo pattern.lo xmlsave.lo xmlmodule.lo schematron.lo -lz -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -liconv -lm libtool: link: ar cru .libs/libxml2.a SAX.o entities.o encoding.o error.o parserInternals.o parser.o tree.o hash.o list.o xmlIO.o xmlmemory.o uri.o valid.o xlink.o HTMLparser.o HTMLtree.o debugXML.o xpath.o xpointer.o xinclude.o nanohttp.o nanoftp.o DOCBparser.o catalog.o globals.o threads.o c14n.o xmlstring.o xmlregexp.o xmlschemas.o xmlschemastypes.o xmlunicode.o xmlreader.o relaxng.o dict.o SAX2.o xmlwriter.o legacy.o chvalid.o pattern.o xmlsave.o xmlmodule.o schematron.o libtool: link: ranlib .libs/libxml2.a libtool: link: ( cd "".libs"" && rm -f ""libxml2.la"" && ln -s ""../libxml2.la"" ""libxml2.la"" ) /bin/sh ./libtool --tag=CC --mode=link gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o xmllint xmllint.o ./libxml2.la -lz -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -liconv -lm /bin/sh ./libtool --tag=CC --mode=link gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o xmlcatalog xmlcatalog.o ./libxml2.la -lz -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -liconv -lm /bin/sh ./libtool --tag=CC --mode=link gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o testSchemas testSchemas.o ./libxml2.la -lz -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -liconv -lm /bin/sh ./libtool --tag=CC --mode=link gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o testRelax testRelax.o ./libxml2.la -lz -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -liconv -lm /bin/sh ./libtool --tag=CC --mode=link gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o testSAX testSAX.o ./libxml2.la -lz -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -liconv -lm libtool: link: gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o testSAX testSAX.o ./.libs/libxml2.a -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -ldl -lz /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib/libiconv.a -lm libtool: link: gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o testRelax testRelax.o ./.libs/libxml2.a -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -ldl -lz /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib/libiconv.a -lm libtool: link: gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o xmlcatalog xmlcatalog.o ./.libs/libxml2.a -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -ldl -lz /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib/libiconv.a -lm libtool: link: gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o testSchemas testSchemas.o ./.libs/libxml2.a -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -ldl -lz /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib/libiconv.a -lm libtool: link: gcc -g -O2 -pedantic -W -Wformat -Wunused -Wimplicit -Wreturn-type -Wswitch -Wcomment -Wtrigraphs -Wformat -Wchar-subscripts -Wuninitialized -Wparentheses -Wshadow -Wpointer-arith -Wcast-align -Wwrite-strings -Waggregate-return -Wstrict-prototypes -Wmissing-prototypes -Wnested-externs -Winline -Wredundant-decls -o xmllint xmllint.o ./.libs/libxml2.a -L/tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib -ldl -lz /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2/lib/libiconv.a -lm ./..libs/libxml2.a(/.xmlIO.o):libs In function xmlGzfileOpenW': //tmp/libxml2.aeasy_install-(Y2MKTgxmlIO.o/):lxml -2.3/In build/function tmp/libxml2xmlGzfileOpenW-': 2.7.8//xmlIO.c:tmp1247/:easy_install -Y2MKTg/lxml-2.3/build/tmp/undefined libxml2-reference2.7.8/xmlIO.c:1247 :to undefined`gzopen64' ./reference.libs/libxml2.a( xmlIO.o): In functionto ``xmlGzfileOpen_real':gzopen64' /./tmp./libs/easy_installlibxml2.a-(xmlIO.oY2MKTg):/lxml- In 2.3function/build /tmp/libxml2xmlGzfileOpen_real': -/2.7.8tmp//xmlIO.ceasy_install:1175-Y2MKTg/:lxml- 2.3/build/undefinedtmp /referencelibxml2-2.7.8 /toxmlIO.c :1175: gzopen64undefined' reference to gzopen64' collect2: collect2: ld returned 1 exit status ld returned 1 exit status make[2]: *** [testRelax] Error 1 make[2]: *** Waiting for unfinished jobs.... make[2]: *** [testSAX] Error 1 .././.libs/libslibxml2.a/(libxml2.axmlIO.o(xmlIO.o)):: In function In function xmlGzfileOpenW'xmlGzfileOpenW':: / tmp/easy_install-Y2MKTg//tmp/lxmleasy_install--Y2MKTg/lxml-2.32.3//build/tmp/buildlibxml2-2.7.8//xmlIO.ctmp:/libxml2-1247: undefined reference to 2.7.8gzopen64/xmlIO.c:1247: undefined' .reference/ .to libs/gzopen64libxml2.a' (./.xmlIO.olibs/libxml2.a(xmlIO.o)): : In Infunction function xmlGzfileOpen_real '`: xmlGzfileOpen_real/': tmp//tmp/easy_install-Y2MKTgeasy_install-/Y2MKTg/lxml-lxml2.3-2.3//build/tmp/build/libxml2tmp/libxml2--2.7.82.7.8/xmlIO.c:/1175:xmlIO.c :1175: undefinedundefined reference reference toto ``gzopen64' gzopen64'collect2: ld returned 1 exit status collect2: ld returned 1 exit status ./.libs/libxml2.a(xmlIO.o): In function xmlGzfileOpenW': /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2-2.7.8/xmlIO.c:1247: undefined reference togzopen64' ./.libs/libxml2.a(xmlIO.o): In function xmlGzfileOpen_real': /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2-2.7.8/xmlIO.c:1175: undefined reference togzopen64' collect2: ld returned 1 exit status make2: * [testSchemas] Error 1 make2: [xmlcatalog] Error 1 make2: [xmllint] Error 1 make2: Leaving directory /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2-2.7.8' make[1]: *** [all-recursive] Error 1 make[1]: Leaving directory /tmp/easy_install-Y2MKTg/lxml-2.3/build/tmp/libxml2-2.7.8' make: * [all] Error 2 Traceback (most recent call last): File ""/usr/local/bin/easy_install-2.7"", line 8, in load_entry_point('setuptools==0.6c11', 'console_scripts', 'easy_install-2.7')() File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 1712, in main File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 1700, in with_ei_usage File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 1716, in File ""/usr/local/lib/python2.7/distutils/core.py"", line 152, in setup dist.run_commands() File ""/usr/local/lib/python2.7/distutils/dist.py"", line 953, in run_commands self.run_command(cmd) File ""/usr/local/lib/python2.7/distutils/dist.py"", line 972, in run_command cmd_obj.run() File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 211, in run File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 446, in easy_install File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 476, in install_item File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 655, in install_eggs File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 930, in build_and_install File ""build/bdist.linux-i686/egg/setuptools/command/easy_install.py"", line 919, in run_setup File ""build/bdist.linux-i686/egg/setuptools/sandbox.py"", line 62, in run_setup File ""build/bdist.linux-i686/egg/setuptools/sandbox.py"", line 105, in run File ""build/bdist.linux-i686/egg/setuptools/sandbox.py"", line 64, in File ""setup.py"", line 130, in File ""/tmp/easy_install-Y2MKTg/lxml-2.3/setupinfo.py"", line 56, in ext_modules File ""/tmp/easy_install-Y2MKTg/lxml-2.3/buildlibxml.py"", line 311, in build_libxml2xslt File ""/tmp/easy_install-Y2MKTg/lxml-2.3/buildlibxml.py"", line 253, in cmmi File ""/tmp/easy_install-Y2MKTg/lxml-2.3/buildlibxml.py"", line 236, in call_subprocess Exception: Command ""make -j6"" returned code 2there was considerably more, and if you are interested you can check it out hereI have tried several other methods of installation. including @agilevic's which returned an error virtually identical to the one above. Most others install effortlessly, but all fail on from lxml import etreeWhy not use a yum? Well, I was able to install lxml effortlessly through yum install python-lxml, but that was centOs' default installation of python2.4. At this point it is probably easier to retool my existing project to work with 2.4. In the long run, however, I would truly like to resolve this issue. For myself, and anyone else who may encounter a similar problem in the future.  <code>  Python 2.7.2 (default, Jun 16 2011, 11:53:48) [GCC 4.1.2 20080704 (Red Hat 4.1.2-50)] on linux2Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import lxml>>> from lxml import etreeTraceback (most recent call last): File ""<stdin>"", line 1, in <module>ImportError: /usr/local/lib/python2.7/site-packages/lxml-2.3-py2.7-linux-x86_64.egg/lxml/etree.so: undefined symbol: exsltMathXpathCtxtRegister > error: cannot create %sourcedir /usr/src/redhat/SOURCES","[Updated 30 june] (still) cannot properly install lxml 2.3 for python, but at least 2.2.8 works"
Why Python is not best in multiprocessing or multithreading applications than Java?," Since Python has some issues with GIL, Java is better for developing multiprocessing applications. Could you please justify the exact reasoning of java's effective processing than python in your way? <code> ",Why Python is not better in multiprocessing or multithreading applications than Java?
Translate Exif Geolocation with Python," I am using the following code to extract the geolocation of an image taken with an iPhone: This is the result that I am returned: So, I am wondering how I convert the GPSInfo values (DMS) to Decimal Degrees for actual coordinates? Also, there seems to be two Wests listed . . . ? <code>  from PIL import Imagefrom PIL.ExifTags import TAGSdef get_exif(fn): ret = {} i = Image.open(fn) info = i._getexif() for tag, value in info.items(): decoded = TAGS.get(tag, tag) ret[decoded] = value return reta = get_exif('photo2.jpg')print a { 'YResolution': (4718592, 65536), 41986: 0, 41987: 0, 41990: 0, 'Make': 'Apple', 'Flash': 32, 'ResolutionUnit': 2, 'GPSInfo': { 1: 'N', 2: ((32, 1), (4571, 100), (0, 1)), 3: 'W', 4: ((117, 1), (878, 100), (0, 1)), 7: ((21, 1), (47, 1), (3712, 100)) }, 'MeteringMode': 1, 'XResolution': (4718592, 65536), 'ExposureProgram': 2, 'ColorSpace': 1, 'ExifImageWidth': 1600, 'DateTimeDigitized': '2011:03:01 13:47:39', 'ApertureValue': (4281, 1441), 316: 'Mac OS X 10.6.6', 'SensingMethod': 2, 'FNumber': (14, 5), 'DateTimeOriginal': '2011:03:01 13:47:39', 'ComponentsConfiguration': '\x01\x02\x03\x00', 'ExifOffset': 254, 'ExifImageHeight': 1200, 'Model': 'iPhone 3G', 'DateTime': '2011:03:03 10:37:32', 'Software': 'QuickTime 7.6.6', 'Orientation': 1, 'FlashPixVersion': '0100', 'YCbCrPositioning': 1, 'ExifVersion': '0220'}",Translate Exif DMS to DD Geolocation with Python
How can I get a list of all the Python standard library modules," I want something like sys.builtin_module_names except for the standard library. Other things that didn't work:sys.modules - only shows modules that have already been loadedsys.prefix - a path that would include non-standard library modules and doesn't seem to work inside a virtualenv.The reason I want this list is so that I can pass it to the --ignore-module or --ignore-dir command line options of trace.So ultimately, I want to know how to ignore all the standard library modules when using trace or sys.settrace. <code> ",How to get a list of all the Python standard library modules?
How do i get a number random of items but in the same order of a sorted list ?," I have a sorted list, let say: (its not really just numbers, its a list of objects that are sorted with a complicated time consuming algorithm) Is there some python function that will give me N of the items, but will keep the order?Example: etc... <code>  mylist = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ,9 , 10 ] randomList = getRandom(mylist,4)# randomList = [ 3 , 6 ,7 , 9 ]randomList = getRandom(mylist,4)# randomList = [ 1 , 2 , 4 , 8 ]",Get random sample from list while maintaining ordering of items?
"Python, compute list difference"," In Python, what is the best way to compute the difference between two lists?example <code>  A = [1,2,3,4]B = [2,5]A - B = [1,3,4]B - A = [5]",Compute list difference
"How do you pick ""x"" number of unique numbers from a list in python?"," I need to pick out ""x"" number of non-repeating, random numbers out of a list. For example: How do I pick out a list like [2, 11, 15] and not [3, 8, 8]? <code>  all_data = [1, 2, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 11, 12, 13, 14, 15, 15]","How do you pick ""x"" number of unique numbers from a list in Python?"
what is the neat way to divide huge nested loops to 8(or more) threads using Python?," this time i'm facing a ""design"" problem. Using Python, I have a implement a mathematical algorithm which uses 5 parameters. To find the best combination of these 5 parameters, i used 5-layer nested loop to enumerate all possible combinations in a given range. The time it takes to finish appeared to be beyond my expectation. So I think it's the time to use multithreading... The task in the core of nested loops are calculation and saving. In current code, result from every calculation is appended to a list and the list will be written to a file at the end of program. since I don't have too much experience of multithreading in any language, not to mention Python, I would like to ask for some hints on what should the structure be for this problem. Namely, how should the calculations be assigned to the threads dynamically and how should the threads save results and later combine all results into one file. I hope the number of threads can be adjustable.Any illustration with code will be very helpful. thank you very much for your time, I appreciate it.#update of 2nd Day:thanks for all helpful answers, now I know that it is multiprocessing instead of multithreading. I always confuse with these two concepts because I think if it is multithreaded then the OS will automatically use multiple processor to run it when available. I will find time to have some hands-on with multiprocessing tonight. <code> ",what is the neat way to divide huge nested loops to 8(or more) processes using Python?
__init__ as a ctor?," Dive into Python - It would be tempting but incorrect to call this the constructor of the class. It's tempting, because it looks like a constructor (by convention, __init__ is the first method defined for the class), acts like one (it's the first piece of code executed in a newly created instance of the class), and even sounds like one (init certainly suggests a constructor-ish nature). Incorrect, because the object has already been constructed by the time __init__ is called, and you already have a valid reference to the new instance of the class.Quote suggests it is incorrect to call __init__ as a constructor because the object is already constructed by the time __init__ is called. But! I have always been under the impression that the constructor is called only after the object is constructed because it is essentially used to initialized the data members of the instance which wouldn't make sense if the object didn't exist by the time constructor was called? (coming from C++/Java background) <code> ",__init__ as a constructor?
How to read numbers from file in Python?," I'd like to read numbers from file into two dimensional array. File contents:line containing w, hh lines containing w integers separated with spaceFor example: <code>  4 31 2 3 42 3 4 56 7 8 9",How to read integers from a file?
Python @property versus getters and setters," Here is a pure Python-specific design question: and Python lets us to do it either way. If you would design a Python program, which approach would you use and why? <code>  class MyClass(object): ... def get_my_attr(self): ... def set_my_attr(self, value): ... class MyClass(object): ... @property def my_attr(self): ... @my_attr.setter def my_attr(self, value): ...",Using @property versus getters and setters
Assignment in While Loop in Python?," I just came across this piece of code and thought, there must be a better way to do this, than using an infinite loop with break.So I tried: and, obviously, got an error.Is there any way to avoid using a break in that situation?Edit:Ideally, you'd want to avoid saying readline twice... IMHO, repeating is even worse than just a break, especially if the statement is complex. <code>  while 1: line = data.readline() if not line: break #... while line = data.readline(): #...",Assign variable in while loop condition in Python?
Programmagically `git checkout .` with dulwich," Having this code I end up with the commit in the history, BUT the created file is pending for deletion (git status says so).A git checkout . fixes it.My question is: how to do git checkout . programmatically with dulwich? <code>  from dulwich.objects import Blob, Tree, Commit, parse_timezonefrom dulwich.repo import Repofrom time import timerepo = Repo.init(""myrepo"", mkdir=True)blob = Blob.from_string(""my file content\n"")tree = Tree()tree.add(""spam"", 0100644, blob.id)commit = Commit()commit.tree = tree.idauthor = ""Flav <foo@bar.com>""commit.author = commit.committer = authorcommit.commit_time = commit.author_time = int(time())tz = parse_timezone('+0200')[0]commit.commit_timezone = commit.author_timezone = tzcommit.encoding = ""UTF-8""commit.message = ""initial commit""o_sto = repo.object_storeo_sto.add_object(blob)o_sto.add_object(tree)o_sto.add_object(commit)repo.refs[""HEAD""] = commit.id",Programmatically `git checkout .` with dulwich
Python list multiplication unexpected bug," Why this is happening? I don't really understand: <code>  >>> P = [ [()]*3 ]*3>>> P[[(), (), ()], [(), (), ()], [(), (), ()]]>>> P[0][0]=1>>> P[[1, (), ()], [1, (), ()], [1, (), ()]]",Python list multiplication: [[...]]*3 makes 3 lists which mirror each other when modified
split elements of a list in python," I have a list: How can I delete the \t and everything after to get this result: <code>  my_list = ['element1\t0238.94', 'element2\t2.3904', 'element3\t0139847'] ['element1', 'element2', 'element3']",How to split elements of a list?
Configuring django-celery monitoring with database broker," I'm using django-celery using the database as both broker and results store. The events are getting processed, but the monitoring (celerycam or celeryev) is not working. My settings.py includes This is enough to get manage.py celeryd to process tasks from cbridge.tasks that get inserted into the queue from anywhere and sometimes get the results back to the thread which started the task. (I'll leave that sometimes alone for now.) So the system is set up enough to get the tasks to actually run, which is great and all, but...My question is how to get I get any monitoring of the tasks? Nothing ever shows up in the djcelery_ tables.I run manage.py celerycam or manage.py celeryev and they start up just fine but never do anything. I'm guessing there are yet more configuration variables I need to set. But what are they? I've tried setting But they don't make any difference. I would expect celeryev and celerycam to read the same config settings as celeryd. Why wouldn't they? <code>  import djcelerydjcelery.setup_loader()BROKER_BACKEND = ""django""CELERY_IMPORTS = ( ""cbridge.tasks"", )INSTALLED_APPS += [ 'djcelery', 'djkombu', ] CELERY_RESULT_BACKEND = ""database"" CELERY_RESULT_DBURI = ""mysql://root:@localhost/cbridge""",Celery monitoring not working with database broker
Parenthese pairing ({}[]()<>) problem. Python," I want to be able to pair up all parentheses in a string, if they aren't paired then then they get their index number and False. It seems like it is repeating some values over and over, i.e cl == pop[1]. I have tried to see where the problem is but I can't see it no matter how hard I try. So I'm asking if anyone help me to locate the error and maybe even improve my code ;) <code>  def check_parentheses(string): pending = 0 brackets = [] '''Checks if parens are paired, otherwise they are bad.''' parenstack = collections.deque() for ch in string: if ch in lrmap: try: cl = string.index(ch, pending) pending = cl + 1 except: cl = False if ch in lparens: parenstack.append([ch, cl]) print parenstack elif ch in rparens: try: pop = parenstack.pop() if lrmap[pop[0]] != ch: print 'wrong type of parenthesis popped from stack',\ pop[0], ch, pop[1], cl brackets.append([pop[1], False]) brackets.append([cl, False]) else: brackets.append([pop[1], cl]) except IndexError: print 'no opening parenthesis left in stack' brackets.append([cl, False]) # if we are not out of opening parentheses, we have a mismatch for p in parenstack: brackets.append([p[1],False]) return brackets",Parentheses pairing ({}[]()<>) issue
Parentheses pairing ({}[]()<>) problem. Python," I want to be able to pair up all parentheses in a string, if they aren't paired then then they get their index number and False. It seems like it is repeating some values over and over, i.e cl == pop[1]. I have tried to see where the problem is but I can't see it no matter how hard I try. So I'm asking if anyone help me to locate the error and maybe even improve my code ;) <code>  def check_parentheses(string): pending = 0 brackets = [] '''Checks if parens are paired, otherwise they are bad.''' parenstack = collections.deque() for ch in string: if ch in lrmap: try: cl = string.index(ch, pending) pending = cl + 1 except: cl = False if ch in lparens: parenstack.append([ch, cl]) print parenstack elif ch in rparens: try: pop = parenstack.pop() if lrmap[pop[0]] != ch: print 'wrong type of parenthesis popped from stack',\ pop[0], ch, pop[1], cl brackets.append([pop[1], False]) brackets.append([cl, False]) else: brackets.append([pop[1], cl]) except IndexError: print 'no opening parenthesis left in stack' brackets.append([cl, False]) # if we are not out of opening parentheses, we have a mismatch for p in parenstack: brackets.append([p[1],False]) return brackets",Parentheses pairing ({}[]()<>) issue
Python autocompletion on object instances in VIM," I have found a strange behavior in Vim when I attempt to use autocompletion on objects. If I instantiate the objects on a module level, the Vim autocompletion will work on the instance I create:If I try the same from within a function or class, it is no longer working:Does anyone know how to fix this, or is there a way to get omnicompletion to work on instances in a non module-scope? <code> ",How to fix absent Python autocompletion on object instances in Vim?
Python: Iterating through constructor's arguments," I often find myself writing class constructors like this: This can obviously become a pain if the number of arguments (and class attributes) gets high. I'm looking for the most pythonic way to loop through the constructor's arguments list and assign attributes accordingly. I'm working with Python 2.7, so ideally I'm looking for help with that version.  <code>  class foo: def __init__(self, arg1, arg2, arg3): self.arg1 = arg1 self.arg2 = arg2 self.arg3 = arg3",Iterating through constructor's arguments
Manage.py Runserver Error: [Errno 10013], I am having some problems running django. When I use the command manage.py runserver I receive an error that says: Error: [Errno 10013] An attempt was made to access a socket in a way forbidden by access permissionsI use postgreSQL as my database. Edit: I run Windows Vista <code> ,manage.py runserver Error: [Errno 10013]
Appending a list to itself in pyton," I want to attach a list to itself and I thought this would work: I wanted to get back [1,2,1,2] but all I get back is the builtin None. What am I doing wrong? I'm using Python v2.6 <code>  x = [1,2]y = x.extend(x)print y",Appending a list to itself in Python
Appending a list to itself in python," I want to attach a list to itself and I thought this would work: I wanted to get back [1,2,1,2] but all I get back is the builtin None. What am I doing wrong? I'm using Python v2.6 <code>  x = [1,2]y = x.extend(x)print y",Appending a list to itself in Python
force mysql select to prefix all column names," Mysql allows duplicate column names in the results of a query. So, in the terminal, none of the column names are prefixed using the above query.However, I'm using mysqldb in python with the DictCursor. Results are a list of dictionaries where the column names are the keys. Sometimes, the dict cursor automatically prefixes the column name with the table name. As far as I can tell, it does this for the second of two ambiguous column names, but only if the second value is unique. Anyways, I'd like to force the cursor to prefix ALL keys with the table name.From the mysqldb docs on the fetch.row() function... The second parameter (how) tells it how the row should be represented. By default, it is zero which means, return as a tuple. how=1 means, return it as a dictionary, where the keys are the column names, or table.column if there are two columns with the same name (say, from a join). how=2 means the same as how=1 except that the keys are always table.column; this is for compatibility with the old Mysqldb module.So, it seems doable, but I'm not using the fetch.row() function directly... so the question is, how can I cause the mysqldb dict cursor to always use how=2 when it fetches rows? <code>  SELECT * FROM a, b WHERE ...",force mysqldb dict cursor to return prefix all column names with table name
'Rolling window' iterator in Python," I need a rolling window (aka sliding window) iterable over a sequence/iterator/generator. Default Python iteration can be considered a special case, where the window length is 1. I'm currently using the following code. Does anyone have a more Pythonic, less verbose, or more efficient method for doing this? <code>  def rolling_window(seq, window_size): it = iter(seq) win = [it.next() for cnt in xrange(window_size)] # First window yield win for e in it: # Subsequent windows win[:-1] = win[1:] win[-1] = e yield winif __name__==""__main__"": for w in rolling_window(xrange(6), 3): print w""""""Example output: [0, 1, 2] [1, 2, 3] [2, 3, 4] [3, 4, 5]""""""",Rolling or sliding window iterator?
Rolling or sliding window iterator in Python," I need a rolling window (aka sliding window) iterable over a sequence/iterator/generator. Default Python iteration can be considered a special case, where the window length is 1. I'm currently using the following code. Does anyone have a more Pythonic, less verbose, or more efficient method for doing this? <code>  def rolling_window(seq, window_size): it = iter(seq) win = [it.next() for cnt in xrange(window_size)] # First window yield win for e in it: # Subsequent windows win[:-1] = win[1:] win[-1] = e yield winif __name__==""__main__"": for w in rolling_window(xrange(6), 3): print w""""""Example output: [0, 1, 2] [1, 2, 3] [2, 3, 4] [3, 4, 5]""""""",Rolling or sliding window iterator?
Get a random boolean in python.," I am looking for the best way (fast and elegant) to get a random boolean in python (flip a coin).For the moment I am using random.randint(0, 1) or random.getrandbits(1).Are there better choices that I am not aware of? <code> ",Get a random boolean in python?
Should a validate method throw an exception, I've implemented a little validation library which is used like this: validate() performs the checks and populates a list called errors.I know from other validation libraries that they throw exception when validation is performed unsuccessfully. Error messages would be passed as an exception property.What approach is better? Is it advantageous to throw validation exceptions? <code>  domain_object.validate()# handle validation errors in some way ...if domain_object.errors: for error in domain_object.errors: print(error),Should a validate method throw an exception?
How to convert escaped characters in Python?," I want to convert strings containing escaped characters to their normal form, the same way Python's lexical parser does: Of course the boring way will be to replace all known escaped characters one by one:http://docs.python.org/reference/lexical_analysis.html#string-literalsHow would you implement normalize_str() in the above code? <code>  >>> escaped_str = 'One \\\'example\\\''>>> print(escaped_str)One \'Example\'>>> normal_str = normalize_str(escaped_str)>>> print(normal_str)One 'Example'",How to convert escaped characters?
Tkinter Entry widget width," I noticed that the width argument for the Tkinter entry widget is in characters, not pixels. Is it possible to adjust the width in pixels?  <code> ",How do I set the width of an Tkinter Entry widget in pixels?
Regex to find urls in string in Python," Possible Duplicate: What is the best regular expression to check if a string is a valid URL? Considering a string as follows: How could I, with Python, extract the urls, inside the anchor tag's href? Something like: Thanks! <code>  string = ""<p>Hello World</p><a href=""http://example.com"">More Examples</a><a href=""http://example2.com"">Even More Examples</a>"" >>> url = getURLs(string)>>> url['http://example.com', 'http://example2.com']",Regex to extract URLs from href attribute in HTML with Python
should i put #! (shebang) in python scripts, Should I put the shebang in my Python scripts? In what form? or Are these equally portable? Which form is used most?Note: the tornado project uses the shebang. On the other hand the Django project doesn't. <code>  #!/usr/bin/env python #!/usr/local/bin/python,"Should I put #! (shebang) in Python scripts, and what form should it take?"
should I put #! (shebang) in python scripts, Should I put the shebang in my Python scripts? In what form? or Are these equally portable? Which form is used most?Note: the tornado project uses the shebang. On the other hand the Django project doesn't. <code>  #!/usr/bin/env python #!/usr/local/bin/python,"Should I put #! (shebang) in Python scripts, and what form should it take?"
"Should I put #! (shebang) in python scripts, and what form should it take?", Should I put the shebang in my Python scripts? In what form? or Are these equally portable? Which form is used most?Note: the tornado project uses the shebang. On the other hand the Django project doesn't. <code>  #!/usr/bin/env python #!/usr/local/bin/python,"Should I put #! (shebang) in Python scripts, and what form should it take?"
How to get the N maximum values in a numpy array?," NumPy proposes a way to get the index of the maximum value of an array via np.argmax.I would like a similar thing, but returning the indexes of the N maximum values.For instance, if I have an array, [1, 3, 2, 4, 5], function(array, n=3) would return the indices [4, 3, 1] which correspond to the elements [5, 4, 3]. <code> ",How do I get indices of N maximum values in a NumPy array?
How to get indices of N maximum values in a numpy array?," NumPy proposes a way to get the index of the maximum value of an array via np.argmax.I would like a similar thing, but returning the indexes of the N maximum values.For instance, if I have an array, [1, 3, 2, 4, 5], function(array, n=3) would return the indices [4, 3, 1] which correspond to the elements [5, 4, 3]. <code> ",How do I get indices of N maximum values in a NumPy array?
Python - How to convert decimal to scientific notation, How can I display this:Decimal('40800000000.00000000000000') as '4.08E+10'?I've tried this: But it has those extra 0's. <code>  >>> '%E' % Decimal('40800000000.00000000000000')'4.080000E+10',Display a decimal in scientific notation
interprocess communication in python," What is a good way to communicate between two separate Python runtimes? Thing's I've tried:reading/writing on named pipes e.g. os.mkfifo (feels hacky)dbus services (worked on desktop, but too heavyweight for headless)sockets (seems too low-level; surely there's a higher level module to use?)My basic requirement is to be able to run python listen.py like a daemon, able to receive messages from python client.py. The client should just send a message to the existing process and terminate, with return code 0 for success and nonzero for failure (i.e. a two-way communication will be required.) <code> ",How to communicate between two separated runtimes on the same host?
Interprocess communication in Python," What is a good way to communicate between two separate Python runtimes? Thing's I've tried:reading/writing on named pipes e.g. os.mkfifo (feels hacky)dbus services (worked on desktop, but too heavyweight for headless)sockets (seems too low-level; surely there's a higher level module to use?)My basic requirement is to be able to run python listen.py like a daemon, able to receive messages from python client.py. The client should just send a message to the existing process and terminate, with return code 0 for success and nonzero for failure (i.e. a two-way communication will be required.) <code> ",How to communicate between two separated runtimes on the same host?
[python]: how can I pass arguments to Tkinter button' callback command?," I got 2 buttons, respectively named 'ButtonA', 'ButtonB'.I want the program to print 'hello, ButtonA' and 'hello, ButtonB' if any button is clicked.My code is as follows: When I click ButtonA, error occurs, text not defined.I understand this error, but how can I pass ButtonA's text to lambda? <code>  def sayHi(name): print 'hello,', nameroot = Tk()btna = Button(root, text = 'ButtonA', command = lambda: text)btna.pack()",How can I pass arguments to Tkinter button's callback command?
What tool to use to excract text from images(OCR)?, I need to recognize text in images (OCR). What library can I use in Python to extract text from the images? <code> ,What library to use to extract text from images (OCR)?
What tool to use to extract text from images (OCR)?, I need to recognize text in images (OCR). What library can I use in Python to extract text from the images? <code> ,What library to use to extract text from images (OCR)?
Why is None returned instead of tkinter.Entry object," I'm new to python, poking around and I noticed this: outputs contain:'txtTest1': None'txtTest2': <tkinter.Entry object at 0x00EADD70>Why does test1 have a None instead of <tkinter.Entry object at ...?I'm using python 3.2 and PyScripter. <code>  from tkinter import *def test1(): root = Tk() txtTest1 = Entry(root).place(x=10, y=10) print(locals())def test2(): root = Tk() txtTest2 = Entry(root) txtTest2.place(x=10, y=10)#difference is this line print(locals())test1()test2()",Why is `None` returned instead of tkinter.Entry object?
Is in this case list comprehension appropriate?," I have to append elements to a list only if the current iterated element is not already in the list. vs The list comprehension gives the result is what I want, just the returned list is useless. Is this a good use case for list comprehensions?The iteration is a good solution, but I'm wondering if there is a more idiomatic way to do this? <code>  >>> l = [1, 2]>>> for x in (2, 3, 4):... if x not in l:... l.append(x)... >>> l[1, 2, 3, 4] >>> l = [1, 2]>>> [l.append(i) for i in (2, 3, 4) if i not in l][None, None]>>> l[1, 2, 3, 4]",Is list comprehension appropriate here?
Word Net - Word Synonmyms & realted word constructs - Java or Python," I am looking to use WordNet to look for a collection of like terms from a base set of terms. For example, the word 'discouraged' - potential synonyms could be: daunted, glum, deterred, pessimistic. I also wanted to identify potential bi-grams such as; beat down, put off, caved in etc.How do I go about extracting this information using Java or Python? Are there any hosted WordNet databases/web interfaces which would allow such querying?Thanks! <code> ",Word Net - Word Synonyms & related word constructs - Java or Python
Python safe serialization library that correctly handles unicode?," Apart from PyYAML, are there any safe Python data serialization libraries which correctly handle unicode/str?For example: Note that I want the serializers to be safe (so pickle and marshel are out), and PyYAML is an option, but I dislike the complexity of YAML, so I'd like to know if there are other options.Edit: it appears that there is some confusion about the nature of my data. Some of them are Unicode (ex, names) and some of them are binary (ex, images) So a serialization library which confuses unicode and str is just as useless to me as a library which confuses ""42"" and 42. <code>  >>> json.loads(json.dumps([u""x"", ""x""]))[u'x', u'x'] # Both unicode>>> msgpack.loads(msgpack.dumps([u""x"", ""x""]))['x', 'x'] # Neither are unicode>>> bson.loads(bson.dumps({""x"": [u""x"", ""x""]})){u'x': [u'x', 'x']} # Dict keys become unicode>>> pyamf.decode(pyamf.encode([u""x"", ""x""])).next()[u'x', u'x'] # Both are unicode",Python save serialization that correctly handles str/unicode?
how to set default integer field value in django models?, I'm trying to set a default value for the integer field in Django model using Why isn't it working? <code>  models.PositiveSmallIntegerField(default='0'),Setting default value for integer field in django models
"Python: simple idiom to break an n-long list into k-long chunks, when n % k > 0 ?"," In Python, it is easy to break an n-long list into k-size chunks if n is a multiple of k (IOW, n % k == 0). Here's my favorite approach (straight from the docs): (The trick is that [iter(x)] * k produces a list of k references to the same iterator, as returned by iter(x). Then zip generates each chunk by calling each of the k copies of the iterator exactly once. The * before [iter(x)] * k is necessary because zip expects to receive its arguments as ""separate"" iterators, rather than a list of them.)The main shortcoming I see with this idiom is that, when n is not a multiple of k (IOW, n % k > 0), the left over entries are just left out; e.g.: There's an alternative idiom that is slightly longer to type, produces the same result as the one above when n % k == 0, and has a more acceptable behavior when n % k > 0: At least, here the left over entries are retained, but the last chunk gets padded with None. If one just wants a different value for the padding, then itertools.izip_longest solves the problem.But suppose the desired solution is one in which the last chunk is left unpadded, i.e. Is there a simple way to modify the map(None, *[iter(x)]*k) idiom to produce this result?(Granted, it is not difficult to solve this problem by writing a function (see, for example, the many fine replies to How do you split a list into evenly sized chunks? or What is the most ""pythonic"" way to iterate over a list in chunks?). Therefore, a more accurate title for this question would be ""How to salvage the map(None, *[iter(x)]*k) idiom?"", but I think it would baffle a lot of readers.)I was struck by how easy it is to break a list into even-sized chunks, and how difficult (in comparison!) it is to get rid of the unwanted padding, even though the two problems seem of comparable complexity. <code>  >>> k = 3>>> n = 5 * k>>> x = range(k * 5)>>> zip(*[iter(x)] * k)[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, 14)] >>> zip(*[iter(x)] * (k + 1))[(0, 1, 2, 3), (4, 5, 6, 7), (8, 9, 10, 11)] >>> map(None, *[iter(x)] * k)[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, 14)]>>> map(None, *[iter(x)] * (k + 1))[(0, 1, 2, 3), (4, 5, 6, 7), (8, 9, 10, 11), (12, 13, 14, None)] [(0, 1, 2, 3), (4, 5, 6, 7), (8, 9, 10, 11), (12, 13, 14)]","Simple idiom to break an n-long list into k-long chunks, when n % k > 0?"
"Python: simple idiom to break an n-long list into k-long chunks, when n % k > 0?"," In Python, it is easy to break an n-long list into k-size chunks if n is a multiple of k (IOW, n % k == 0). Here's my favorite approach (straight from the docs): (The trick is that [iter(x)] * k produces a list of k references to the same iterator, as returned by iter(x). Then zip generates each chunk by calling each of the k copies of the iterator exactly once. The * before [iter(x)] * k is necessary because zip expects to receive its arguments as ""separate"" iterators, rather than a list of them.)The main shortcoming I see with this idiom is that, when n is not a multiple of k (IOW, n % k > 0), the left over entries are just left out; e.g.: There's an alternative idiom that is slightly longer to type, produces the same result as the one above when n % k == 0, and has a more acceptable behavior when n % k > 0: At least, here the left over entries are retained, but the last chunk gets padded with None. If one just wants a different value for the padding, then itertools.izip_longest solves the problem.But suppose the desired solution is one in which the last chunk is left unpadded, i.e. Is there a simple way to modify the map(None, *[iter(x)]*k) idiom to produce this result?(Granted, it is not difficult to solve this problem by writing a function (see, for example, the many fine replies to How do you split a list into evenly sized chunks? or What is the most ""pythonic"" way to iterate over a list in chunks?). Therefore, a more accurate title for this question would be ""How to salvage the map(None, *[iter(x)]*k) idiom?"", but I think it would baffle a lot of readers.)I was struck by how easy it is to break a list into even-sized chunks, and how difficult (in comparison!) it is to get rid of the unwanted padding, even though the two problems seem of comparable complexity. <code>  >>> k = 3>>> n = 5 * k>>> x = range(k * 5)>>> zip(*[iter(x)] * k)[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, 14)] >>> zip(*[iter(x)] * (k + 1))[(0, 1, 2, 3), (4, 5, 6, 7), (8, 9, 10, 11)] >>> map(None, *[iter(x)] * k)[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, 14)]>>> map(None, *[iter(x)] * (k + 1))[(0, 1, 2, 3), (4, 5, 6, 7), (8, 9, 10, 11), (12, 13, 14, None)] [(0, 1, 2, 3), (4, 5, 6, 7), (8, 9, 10, 11), (12, 13, 14)]","Simple idiom to break an n-long list into k-long chunks, when n % k > 0?"
Python - Any reason NOT to always use keyword arguments?," Before jumping into python, I had started with some Objective-C / Cocoa books. As I recall, most functions required keyword arguments to be explicitly stated. Until recently I forgot all about this, and just used positional arguments in Python. But lately, I've ran into a few bugs which resulted from improper positions - sneaky little things they were. Got me thinking - generally speaking, unless there is a circumstance that specifically requires non-keyword arguments - is there any good reason NOT to use keyword arguments? Is it considered bad style to always use them, even for simple functions? I feel like as most of my 50-line programs have been scaling to 500 or more lines regularly, if I just get accustomed to always using keyword arguments, the code will be more easily readable and maintainable as it grows. Any reason this might not be so? UPDATE:The general impression I am getting is that its a style preference, with many good arguments that they should generally not be used for very simple arguments, but are otherwise consistent with good style. Before accepting I just want to clarify though - is there any specific non-style problems that arise from this method - for instance, significant performance hits?  <code> ",Any reason NOT to always use keyword arguments?
Python ValueError: to many values to unpack," I am getting that exception from this code: The for line is the one throwing the exception. The ms are Material objects. Anybody have any ideas why? <code>  class Transaction: def __init__ (self): self.materials = {} def add_material (self, m): self.materials[m.type + m.purity] = m def serialize (self): ser_str = 'transaction_start\n' for k, m in self.materials: ser_str += m.serialize () sert += 'transaction_end\n' return ser_str",Python ValueError: too many values to unpack
is there any best way to generate all possible three letters keywords," I am generating all possible three letters keywords e.g. aaa, aab, aac.... zzy, zzz below is my code: Can this functionality be achieved in a more sleek and efficient way? <code>  alphabets = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']keywords = []for alpha1 in alphabets: for alpha2 in alphabets: for alpha3 in alphabets: keywords.append(alpha1+alpha2+alpha3)",What is the best way to generate all possible three letter strings?
Standard way to create debian packages for distributing python programs?," There is a ton of information on how to do this, but since ""there is more than one way to skin a cat"", and all the tutorials/manuals that cover a bit of the process seem to make certain assumptions which are different from other tutorials, I still didn't manage to grasp it.So far this is what I think I understood.My final goal should be that of creating a ""binary"" .deb package. Such package will be platform-independent (32/64 bit) as all Python programs are such.To create a ""binary"" package I need first to create a source package.To create the source package I can use either CDBS or debhelper. Debhelper is the recommended way for beginners.The core of creating a source package is populating the DEBIAN directory in the source directory with a number of files clarifying where files need to be copied, what copyright and licensing scheme they are subject to, what dependencies they have, etc...Step #4 can be largely automated the dh_makecommand if the Python source also comes with a distutils' setup.py script.Now my questions:Is my understanding of the process correct? Is there anything I am missing, or anything that I got wrong?Step #5 is really the more confusing to me: specifically the two points that remains most obscure to me are:How do I write a setup.py script that install a stand-alone programme? EDIT: By standalone programme I mean a program intended to be used by a desktop user (as opposed to a module which I understand like a collection of functionality to be used by other software after having been imported). In my specific case I would actually need two such ""programs"": the main software and a separate utility (in effect a second ""program"" that should be in the same package with the other one).What are the specificities of such a script for DEB packages? The official documentation only seems to deal with RPM and Windows stuff...BTW: These are the best sources of information that I could find myself so far. If you have anything better than this, please share! :)Ubuntu's Python packaging guideCreating a .deb package from a python setup.py (it shows the steps, but it doesn't explain them enough for me to follow along)ShowMeDo video on ""creating a .deb package out of a pythonprogram"" (it doesn't seem up-to-date and - if I got it right - will produce packages for personal use, without dependenciesand without a signed changelog and other key data that will make it incompatible with the Debian policy). <code> ",Is there a standard way to create Debian packages for distributing Python programs?
Standard way to create debian packages for distributing Python programs?," There is a ton of information on how to do this, but since ""there is more than one way to skin a cat"", and all the tutorials/manuals that cover a bit of the process seem to make certain assumptions which are different from other tutorials, I still didn't manage to grasp it.So far this is what I think I understood.My final goal should be that of creating a ""binary"" .deb package. Such package will be platform-independent (32/64 bit) as all Python programs are such.To create a ""binary"" package I need first to create a source package.To create the source package I can use either CDBS or debhelper. Debhelper is the recommended way for beginners.The core of creating a source package is populating the DEBIAN directory in the source directory with a number of files clarifying where files need to be copied, what copyright and licensing scheme they are subject to, what dependencies they have, etc...Step #4 can be largely automated the dh_makecommand if the Python source also comes with a distutils' setup.py script.Now my questions:Is my understanding of the process correct? Is there anything I am missing, or anything that I got wrong?Step #5 is really the more confusing to me: specifically the two points that remains most obscure to me are:How do I write a setup.py script that install a stand-alone programme? EDIT: By standalone programme I mean a program intended to be used by a desktop user (as opposed to a module which I understand like a collection of functionality to be used by other software after having been imported). In my specific case I would actually need two such ""programs"": the main software and a separate utility (in effect a second ""program"" that should be in the same package with the other one).What are the specificities of such a script for DEB packages? The official documentation only seems to deal with RPM and Windows stuff...BTW: These are the best sources of information that I could find myself so far. If you have anything better than this, please share! :)Ubuntu's Python packaging guideCreating a .deb package from a python setup.py (it shows the steps, but it doesn't explain them enough for me to follow along)ShowMeDo video on ""creating a .deb package out of a pythonprogram"" (it doesn't seem up-to-date and - if I got it right - will produce packages for personal use, without dependenciesand without a signed changelog and other key data that will make it incompatible with the Debian policy). <code> ",Is there a standard way to create Debian packages for distributing Python programs?
Python: How to suppress the output of os.system," In Python , If I am using ""wget"" to download a file using os.system(""wget ), it shows on the screen like: etc on the screen.What can I do to save this output in some file rather than showing it on the screen ?Currently I am running the command as follows: <code>  Resolving... Connecting to ... HTTP request sent, awaiting response... 100%[====================================================================================================================================================================>] 19,535,176 8.10M/s in 2.3s theurl = ""< file location >""downloadCmd = ""wget ""+theurlos.system(downloadCmd)",Python: How to save the output of os.system
Can Python unittest automatically reattempt a unittest / suite?," Short QuestionIs it possible to re-attempt a unittest upon failure / error N number of times OR based on a predefined function. (Like a user's prompt) BackgroundTo avoid retyping an entire page of system information, please see SO question on passing data to unittests and on auto test discovery for more details on my physical set up. Regarding the question at hand, I know I can do it by re-writting my test cases to loop until it gets the required results (see pseudo code below) then assert based off of this. However I rather not go and rewrite 100's of test cases. I there will be someone that will point out that if a unittest fails, it should just fail and be done. I agree with this 100% if human error could be removed. This is a physical system that I am connected to and there are many times the leads from the digital multimeter are not connected well and it could fail because of a loose connection. Pseudo Workaround EDIT 8/22/11 3:30 PM CSTI do know that I am violating the definition of the unittest in this use-case. These issues / comments are also addressed in a few of my other SO questions. One of the design goals we chose was to leverage an existing framework to avoid having to ""reinvent the wheel"". The fact that we chose python's unittest was not based on it's definition, but it's flexibility and robustness to execute and display a series of tests. Going into to this project, I knew there would be some things that would require workarounds because this module was not intended for this use. At this point in time, I still believe that these workarounds have been easier / cheaper than rewriting my own test runner. EDIT 8/22/11 5:22 PM CSTI am not dead set on using unittest for future projects in this manner, however I fairly set on using an existing frame work to avoid duplicating someone else's efforts. A comment below is an example of this pycopia-QA appears to be a good fit for this project. The only drawback for my current project is it is that I have written hundreds unittest test-cases, if I were to rewrite them it would be a very large undertaking (noting that it will also be a non-funded effort)EDIT 8/24/11 11:00 AM CSTIt may be clear for future projects to switch to a more tailored frame work for this type of testing. However I still have projects running with unittest so a solution using only unittest (or nose + 3rd addon) is still needed. <code>  class Suite_VoltageRegulator(unittest.TestCase): def test_voltage_5v_regulator(self): keep_running = 'y' error_detected = False print '\n' # Display User Test Configuration msg = \ '1) Connect Front DMM GND(black) to the TP_COM\n' +\ '2) Connect Front DMM POS(red) to the TP-A\n' +\ '3) Ensure the DMM terminal button indicates FRONT' continue_test = prompt.Prompt_Ok_Cancel('User Action Required!', msg) if not continue_test: self.assertTrue(False, 'User Canceled Test!') while(keep_running == 'y'): try: # Run the test results = measure_voltage_from_system() # Analyze Results test_status = pf.value_within_range(results, REGULATOR_5V_LOW, REGULATOR_5V_HIGH) catch: error_detected = True # Retest Failed Cards if(test_status == False): keep_running = rawinput('Test FAILED: Retry Test? (y/n):') elif(error_detected == True): keep_running = rawinput('Test ERROR: Retry Test? (y/n):') else: keep_running = 'n' # Inform the user on the test results self.assertTrue(test_status, 'FAIL: 5V Regulator (' +str(results)+ ') Out of Range!')",Can Python unittest automatically reattempt a failed testcase / suite?
Python - How to validate a url in python ? (Malformed or not), I have url from the user and I have to reply with the fetched HTML.How can I check for the URL to be malformed or not?For example : <code>  url = 'google' # Malformedurl = 'google.com' # Malformedurl = 'http://google.com' # Validurl = 'http://google' # Malformed,How to validate a url in Python? (Malformed or not)
python: Dictionaries of dictionaries merge," I need to merge multiple dictionaries, here's what I have for instance: With A B C and D being leaves of the tree, like {""info1"":""value"", ""info2"":""value2""}There is an unknown level(depth) of dictionaries, it could be {2:{""c"":{""z"":{""y"":{C}}}}}In my case it represents a directory/files structure with nodes being docs and leaves being files.I want to merge them to obtain: I'm not sure how I could do that easily with Python. <code>  dict1 = {1:{""a"":{A}}, 2:{""b"":{B}}}dict2 = {2:{""c"":{C}}, 3:{""d"":{D}} dict3 = {1:{""a"":{A}}, 2:{""b"":{B},""c"":{C}}, 3:{""d"":{D}}}",How to merge dictionaries of dictionaries?
Dictionaries of dictionaries merge," I need to merge multiple dictionaries, here's what I have for instance: With A B C and D being leaves of the tree, like {""info1"":""value"", ""info2"":""value2""}There is an unknown level(depth) of dictionaries, it could be {2:{""c"":{""z"":{""y"":{C}}}}}In my case it represents a directory/files structure with nodes being docs and leaves being files.I want to merge them to obtain: I'm not sure how I could do that easily with Python. <code>  dict1 = {1:{""a"":{A}}, 2:{""b"":{B}}}dict2 = {2:{""c"":{C}}, 3:{""d"":{D}} dict3 = {1:{""a"":{A}}, 2:{""b"":{B},""c"":{C}}, 3:{""d"":{D}}}",How to merge dictionaries of dictionaries?
Which widget do you use for a excel like table in tkinter?, I want a Excel like table widget in tkinter for a gui I am writing. Do you have any suggestions? <code> ,Which widget do you use for a Excel like table in tkinter?
Why no-semicolon in Python while use them in JavaScript?," Python and JavaScript both allow developers to use or to omit semicolons. However, I've often seen it suggested (in books and blogs) that I should not use semicolons in Python, while I should always use them in JavaScript.Is there a technical difference between how the languages use semicolons or is this just a cultural difference? <code> ",What is the difference between semicolons in JavaScript and in Python?
Why would you avoid semicolons in Python and use them in JavaScript?," Python and JavaScript both allow developers to use or to omit semicolons. However, I've often seen it suggested (in books and blogs) that I should not use semicolons in Python, while I should always use them in JavaScript.Is there a technical difference between how the languages use semicolons or is this just a cultural difference? <code> ",What is the difference between semicolons in JavaScript and in Python?
replace text to tag," I have some lxml element: I need to replace the word BREAK with an HTML break tag—<br />. I've tried to do simple text replacing: but it inserts the tag with escaped symbols, like &lt;br/&gt;. How do I solve this problem? <code>  >> lxml_element.text 'hello BREAK world' lxml_element.text.replace('BREAK', '<br />')",Replace text with HTML tag in LXML text element
how to get everything after last slash in a URL?," How can I extract whatever follows the last slash in a URL in Python? For example, these URLs should return the following: I've tried urlparse, but that gives me the full path filename, such as page/page/12345. <code>  URL: http://www.test.com/TEST1returns: TEST1URL: http://www.test.com/page/TEST2returns: TEST2URL: http://www.test.com/page/page/12345returns: 12345",How to get everything after last slash in a URL?
Python list append behavior," Suppose I do the following: Why does 1 get appended to both lists? <code>  >>> l = [[]]*2>>> l[[], []]>>> l[0].append(1)>>> l[[1], [1]]",Why does appending to one list also append to all other lists in my list of lists?
Sphinx floating point formating," I'm using Sphinx to generate documentation from code. Does anyone know if there is a way to control the formatting of floating point numbers generated from default arguments. For example if I have the following function: The generated documentation ends up looking like: Obviously this is a floating point precision issue, but is there a way to make the documentation not look so ugly? <code>  def f(x = 0.97): return x+1 foo(x = 0.96999999999997)",Sphinx floating point formatting
Python: try/except for specific error of type Exception," I have a certain function which does the following in certain cases:raise Exception, 'someError'and may raise other exceptions in other cases.I want to treat differently the cases when the function raises Exception, 'someError' and the cases where the function raises other exceptions.For example, I tried the following, but it didn't work as I expected. This prints 'second case'... <code>  try: raise Exception, 'someError'except Exception('someError'): print('first case')except: print ('second case')",Try/except for specific error of type Exception
Get Source of WebElement in Selenium WebDriver (Python)," I'm using the Python bindings to run Selenium WebDriver: I know I can grab a webelement like so: And I know I can get the full page source with... But is there a way to get the ""element source""? The Selenium WebDriver documentation for Python are basically non-existent and I don't see anything in the code that seems to enable that functionality.What is the best way to access the HTML of an element (and its children)? <code>  from selenium import webdriverwd = webdriver.Firefox() elem = wd.find_element_by_css_selector('#my-id') wd.page_source elem.source # <-- returns the HTML as a string",Get HTML source of WebElement in Selenium WebDriver using Python
Get HTML Source of WebElement in Selenium WebDriver (Python)," I'm using the Python bindings to run Selenium WebDriver: I know I can grab a webelement like so: And I know I can get the full page source with... But is there a way to get the ""element source""? The Selenium WebDriver documentation for Python are basically non-existent and I don't see anything in the code that seems to enable that functionality.What is the best way to access the HTML of an element (and its children)? <code>  from selenium import webdriverwd = webdriver.Firefox() elem = wd.find_element_by_css_selector('#my-id') wd.page_source elem.source # <-- returns the HTML as a string",Get HTML source of WebElement in Selenium WebDriver using Python
Get HTML Source of WebElement in Selenium WebDriver," I'm using the Python bindings to run Selenium WebDriver: I know I can grab a webelement like so: And I know I can get the full page source with... But is there a way to get the ""element source""? The Selenium WebDriver documentation for Python are basically non-existent and I don't see anything in the code that seems to enable that functionality.What is the best way to access the HTML of an element (and its children)? <code>  from selenium import webdriverwd = webdriver.Firefox() elem = wd.find_element_by_css_selector('#my-id') wd.page_source elem.source # <-- returns the HTML as a string",Get HTML source of WebElement in Selenium WebDriver using Python
Get HTML Source of WebElement in Selenium WebDriver using Python," I'm using the Python bindings to run Selenium WebDriver: I know I can grab a webelement like so: And I know I can get the full page source with... But is there a way to get the ""element source""? The Selenium WebDriver documentation for Python are basically non-existent and I don't see anything in the code that seems to enable that functionality.What is the best way to access the HTML of an element (and its children)? <code>  from selenium import webdriverwd = webdriver.Firefox() elem = wd.find_element_by_css_selector('#my-id') wd.page_source elem.source # <-- returns the HTML as a string",Get HTML source of WebElement in Selenium WebDriver using Python
How to reload a method in a Python module?," Following up on this question regarding reloading a module, how do I reload a specific function from a changed module?pseudo-code: <code>  from foo import barif foo.py has changed: reload bar",How to reload a module's function in Python?
Python: Get relative path from comparing two absolute paths," Say, I have two absolute paths. I need to check if the location referring to by one of the paths is a descendant of the other. If true, I need to find out the relative path of the descendant from the ancestor. What's a good way to implement this in Python? Any library that I can benefit from? <code> ",Get relative path from comparing two absolute paths
South won't generate or apply migrations," I'm using South to generate and apply migrations, rather than managing that myself. Unfortunately, South is refusing to actually do anything. Transcript below: As you can see, South thinks there is nothing to do. However, the last three models are completely new, and have no table in the database.Is there anything I can do, short of zapping the database to get South working again?I have no intention of manually writing migrations for the rest of the project, but if it would help, I would write one migration.  <code>  [graffias:~/testing.tustincommercial.com/oneclickcos]$ python ./manage.py schemamigration mainapp --autoYou cannot use --auto on an app with no migrations. Try --initial.[graffias:~/testing.tustincommercial.com/oneclickcos]$ python ./manage.py schemamigration mainapp --initial + Added model mainapp.CompanyUK + Added model mainapp.CompanyName + Added model mainapp.Individual + Added model mainapp.Director + Added model mainapp.DirectorsIndividual + Added model mainapp.DirectorsCorporate + Added model mainapp.ShareCapitalClass + Added model mainapp.Member + Added model mainapp.MembersIndividual + Added model mainapp.MemberGeneric + Added model mainapp.CompanyManager + Added model mainapp.PendingRegistration + Added model mainapp.PendingAuthorisationCreated 0001_initial.py. You can now apply this migration with: ./manage.py migrate mainapp[graffias:~/testing.tustincommercial.com/oneclickcos]$ python ./manage.py migrate mainappRunning migrations for mainapp:- Nothing to migrate. - Loading initial data for mainapp.No fixtures found.[graffias:~/testing.tustincommercial.com/oneclickcos]$ ","South won't generate or apply migrations for existing app, with changes to migrate"
"Python: Why won't my ""return list.sort"" return None"," I've been able to verify that the findUniqueWords does result in a sorted list. However, it does not return the list. Why? <code>  def findUniqueWords(theList): newList = [] words = [] # Read a line at a time for item in theList: # Remove any punctuation from the line cleaned = cleanUp(item) # Split the line into separate words words = cleaned.split() # Evaluate each word for word in words: # Count each unique word if word not in newList: newList.append(word) answer = newList.sort() return answer","Why does ""return list.sort()"" return None, not the list?"
"Why won't my ""return list.sort"" return None"," I've been able to verify that the findUniqueWords does result in a sorted list. However, it does not return the list. Why? <code>  def findUniqueWords(theList): newList = [] words = [] # Read a line at a time for item in theList: # Remove any punctuation from the line cleaned = cleanUp(item) # Split the line into separate words words = cleaned.split() # Evaluate each word for word in words: # Count each unique word if word not in newList: newList.append(word) answer = newList.sort() return answer","Why does ""return list.sort()"" return None, not the list?"
"Why won't my ""return list.sort"" return None"," I've been able to verify that the findUniqueWords does result in a sorted list. However, it does not return the list. Why? <code>  def findUniqueWords(theList): newList = [] words = [] # Read a line at a time for item in theList: # Remove any punctuation from the line cleaned = cleanUp(item) # Split the line into separate words words = cleaned.split() # Evaluate each word for word in words: # Count each unique word if word not in newList: newList.append(word) answer = newList.sort() return answer","Why does ""return list.sort()"" return None, not the list?"
How to tell wether sys.stdout has been flushed in python," I'm trying to debug some code I wrote, which involves a lot of parallel processes. And have some unwanted behaviour involving output to sys.stdout and certain messages being printed twice. For debugging purposes it would be very useful to know whether at a certain point sys.stdout has been flushed or not. I wonder if this is possible and if so, how?Ps. I don't know if it matters but I'm using OS X (at least some sys commands depend on the operating system). <code> ",How to tell whether sys.stdout has been flushed in Python
preferred way to implement 'yield' in Scala?," I am doing writing code for PhD research and starting to use Scala. I often have to do text processing. I am used to Python, whose 'yield' statement is extremely useful for implementing complex iterators over large, often irregularly structured text files. Similar constructs exist in other languages (e.g. C#), for good reason.Yes I know there have been previous threads on this. But they look like hacked-up (or at least badly explained) solutions that don't clearly work well and often have unclear limitations. I would like to write code something like this: I'd like to see the code that implements generate() and give(). BTW give() should be named yield() but Scala has taken that keyword already.I gather that, for reasons I don't understand, Scala continuations may not work inside a for statement. If so, generate() should supply an equivalent function that works as close as possible to a for statement, because iterator code with yield almost inevitably sits inside a for loop.Please, I would prefer not to get any of the following answers:'yield' sucks, continuations are better. (Yes, in general you can do more with continuations. But they are hella hard to understand, and 99% of the time an iterator is all you want or need. If Scala provides lots of powerful tools but they're too hard to use in practice, the language won't succeed.)This is a duplicate. (Please see my comments above.)You should rewrite your code using streams, continuations, recursion, etc. etc. (Please see #1. I will also add, technically you don't need for loops either. For that matter, technically you can do absolutely everything you ever need using SKI combinators.)Your function is too long. Break it up into smaller pieces and you won't need 'yield'. You'd have to do this in production code, anyway. (First, ""you won't need 'yield'"" is doubtful in any case. Second, this isn't production code. Third, for text processing like this, very often, breaking the function into smaller pieces -- especially when the language forces you to do this because it lacks the useful constructs -- only makes the code harder to understand.)Rewrite your code with a function passed in. (Technically, yes you can do this. But the result is no longer an iterator, and chaining iterators is much nicer than chaining functions. In general, a language should not force me to write in an unnatural style -- certainly, the Scala creators believe this in general, since they provide shitloads of syntactic sugar.)Rewrite your code in this, that, or the other way, or some other cool, awesome way I just thought of. <code>  import generator._def yield_values(file:String) = { generate { for (x <- Source.fromFile(file).getLines()) { # Scala is already using the 'yield' keyword. give(""something"") for (field <- "":"".r.split(x)) { if (field contains ""/"") { for (subfield <- ""/"".r.split(field)) { give(subfield) } } else { // Scala has no 'continue'. IMO that should be considered // a bug in Scala. // Preferred: if (field.startsWith(""#"")) continue // Actual: Need to indent all following code if (!field.startsWith(""#"")) { val some_calculation = { ... do some more stuff here ... } if (some_calculation && field.startsWith(""r"")) { give(""r"") give(field.slice(1)) } else { // Typically there will be a good deal more code here to handle different cases give(field) } } } } } }}",What is the preferred way to implement 'yield' in Scala?
"I want to return a value AND raise an exception, which probably means I'm doing something wrong"," I have a number of functions that parse data from files, usually returning a list of results.If I encounter a dodgy line in the file, I want to soldier on and process the valid lines, and return them. But I also want to report the error to the calling function. The reason I want to report it is so that the calling function can notify the user that the file needs looking at. I don't want to start doing GUI things in the parse function, as that seems to be a big violation of separation of concerns. The parse function does not have access to the console I'm writing error messages to anyway.This leaves me wanting to return the successful data, but also raise an exception because of the error, which clearly I can't do.Consider this code: Obviously the last three lines make no sense, but I can't figure out a nice way to do this. I guess I could do return (err, result), and call it like But returning error codes seems un-pythonic enough, let alone returning tuples of error codes and actual result values.The fact that I feel like I want to do something so strange in an application that shouldn't really be terribly complicated, is making me think I'm probably doing something wrong. Is there a better solution to this problem? Or is there some way that I can use finally to return a value and raise an exception at the same time? <code>  try: parseResult = parse(myFile)except MyErrorClass, e: HandleErrorsSomehow(str(e))def parse(file): #file is a list of lines from an actual file err = False result = [] for lines in file: processedLine = Process(line) if not processedLine: err = True else result.append(processedLine) return result if err: raise MyErrorClass(""Something went wrong"") parseErr, parseResult = parse(file)if parseErr: HandleErrorsSomehow()","I want to return a value AND raise an exception, does this mean I'm doing something wrong?"
Is there a Python module where I could easily convert mixed numbers into a float?," I'm just wondering if I could easily convert a mixed number (entered as a number or a string) into a floating point number or an integer. I've looked at the fractions module but it seems like it couldn't do what I want, or I didn't read well.Just wanted to know if something already exists before I write my own function. Here's what I'm looking for, btw: or Thanks. <code>  convert(1 1/2) convert('1 1/2')",Is there a Python module where I could easily convert mixed fractions into a float?
Python - Writing a folder and it's contents to a ZipFile," Is it possible to write a folder and its contents to an existing ZipFile? I've been messing around with this for a while and can only manage to write the folder structure to the archive, anything inside the folder isn't copied. I don't want to point to a specific file, because the idea is that the folder contents can change and the program will just copy the whole folder into the archive no matter what is inside.Currently I have, but I want the contents of 'Another Folder' to be copied as well not just the empty folderHopefully you understand what I mean. <code>  myzipfile.write('A Folder\\Another Folder\\') ",Python - Writing a folder and its contents to a ZipFile
threading.Condition vs threading.Event?," I have yet to find a clear explanation of the differences between Condition and Event classes in the threading module. Is there a clear use case where one would be more helpful than the other? All the examples I can find use a producer-consumer model as an example, where queue.Queue would be the more straightforward solution. <code> ",threading.Condition vs threading.Event
Removing duplicates in each row of a numpy array," I have a (N,3) array of numpy values: I'd like to remove rows from the array that have a duplicate value. For example, the result for the above array should be: I'm not sure how to do this efficiently with numpy without looping (the array could be quite large). Anyone know how I could do this? <code>  >>> vals = numpy.array([[1,2,3],[4,5,6],[7,8,7],[0,4,5],[2,2,1],[0,0,0],[5,4,3]])>>> valsarray([[1, 2, 3], [4, 5, 6], [7, 8, 7], [0, 4, 5], [2, 2, 1], [0, 0, 0], [5, 4, 3]]) >>> duplicates_removedarray([[1, 2, 3], [4, 5, 6], [0, 4, 5], [5, 4, 3]])",Removing rows with duplicates in a NumPy array
python - Count number of occurences of each number," I have a long string of numbers separated by commas. I can search and count the number of occurrences of most numbers, or more accurately, 2 digit numbers.IF I have a number sequences like:1,2,3,4,5,1,6,7,1,8,9,10,11,12,1,1,2and I want to count how many times the number 1 appears I should really get 5.However, because it is counting the 1 in 10,11 and 12, I am getting 9.Does anyone know how to make the below code match ONLY whole ""strings""? <code>  def mostfreq(numString): import json maxNum=45 count=1 list={} while count <= maxNum: list[count] = 0 count+=1 #numString is the array with all the numbers in it count=1 topTen = """" while count <= maxNum: list[count]=numString.count(str(count)) topTen = topTen+json.dumps( {count: list[count]}, sort_keys=True, indent=4)+"","" count+=1 response_generator = ( ""[""+topTen[:-1]+""]"" ) return HttpResponse(response_generator)",python - Count number of occurrences of each number
"pyinstaller, NameError: global name 'quit' is not defined"," I have a python script which runs just fine, however after running pyinstaller, I get the following when I use a quit() or exit() command:Makespec file: Here is what I see after I run the app: <code>  # -*- mode: python -*-a = Analysis([os.path.join(HOMEPATH,'support/_mountzlib.py'), os.path.join(HOMEPATH,'support/useUnicode.py'), 'icinga.py'], pathex=['/home/user/projects/icinga_python/releases/onefile_v1.0'])pyz = PYZ(a.pure)exe = EXE( pyz, a.scripts, a.binaries, a.zipfiles, a.datas, name=os.path.join('dist', 'icinga'), debug=False, strip=False, upx=True, console=1 ) Traceback (most recent call last): File ""<string>"", line 222, in <module> File ""<string>"", line 207, in main File ""<string>"", line 66, in icinga_mysql File ""<string>"", line 45, in checksanityNameError: global name 'quit' is not defined","Pyinstaller, NameError: global name 'quit' is not defined"
Flask subdomain routing?," I want to have my top-level domain as a portal for various subdomains that correspond to different sections of my site. example.com should route to a welcome.html template. eggs.example.com should route to an ""eggs"" subsection or application of the site. How would I achieve this in Flask? <code> ",Flask app that routes based on domains
Flask app that routes based on subdomain," I want to have my top-level domain as a portal for various subdomains that correspond to different sections of my site. example.com should route to a welcome.html template. eggs.example.com should route to an ""eggs"" subsection or application of the site. How would I achieve this in Flask? <code> ",Flask app that routes based on domains
disable or lock mouse and keybord in python?, Is there a way of disabling or locking mouse and keyboard using python? I want to freeze the mouse and disable the keyboard. <code> ,disable or lock mouse and keyboard in Python?
Tutorial or Guide for Scripting XCode Build Phases," I would like to add some files to the Compile Sources build phase using a script in Xcode, which pulls from some folder references. I haven't been able to find much documentation so far.Where is the general documentation (or a good tutorial) for scripting Xcode build phases?How can I add files to the Compile Sources phase?How can I discover information about the project and the folder references within it?Are there any special considerations if I want to script in Ruby or Python vs. bash scripting? <code> ",Tutorial or Guide for Scripting Xcode Build Phases
Workin whit lists," Possible Duplicate: A Transpose/Unzip Function in Python I have a list that looks like this: I want to separate the list in 2 lists. I can do it for example with: But I want to know if there is a more elegant solution. <code>  list = (('1','a'),('2','b'),('3','c'),('4','d')) list1 = ('1','2','3','4')list2 = ('a','b','c','d') list1 = []list2 = []for i in list: list1.append(i[0]) list2.append(i[1])",Unpacking a list / tuple of pairs into two lists / tuples
Working with lists," Possible Duplicate: A Transpose/Unzip Function in Python I have a list that looks like this: I want to separate the list in 2 lists. I can do it for example with: But I want to know if there is a more elegant solution. <code>  list = (('1','a'),('2','b'),('3','c'),('4','d')) list1 = ('1','2','3','4')list2 = ('a','b','c','d') list1 = []list2 = []for i in list: list1.append(i[0]) list2.append(i[1])",Unpacking a list / tuple of pairs into two lists / tuples
Working with lists," Possible Duplicate: A Transpose/Unzip Function in Python I have a list that looks like this: I want to separate the list in 2 lists. I can do it for example with: But I want to know if there is a more elegant solution. <code>  list = (('1','a'),('2','b'),('3','c'),('4','d')) list1 = ('1','2','3','4')list2 = ('a','b','c','d') list1 = []list2 = []for i in list: list1.append(i[0]) list2.append(i[1])",Unpacking a list / tuple of pairs into two lists / tuples
Python: How to make a list of n numbers and randomly select any number?," I have taken a count of something and it came out to N.Now I would like to have a list, containing 1 to N numbers in it.Example:N = 5then, count_list = [1, 2, 3, 4, 5]Also, once I have created the list, I would like to randomly select a number from that list and use that number.After that I would like to select another number from the remaining numbers of the list (N-1) and then use that also.This goes on it the list is empty. <code> ",How to make a list of n numbers in Python and randomly select any number?
"Python hashlib problem ""TypeError: Unicode-objects must be encoded before hashing"""," I have this error: when I try to execute this code in Python 3.2.2: <code>  Traceback (most recent call last): File ""python_md5_cracker.py"", line 27, in <module> m.update(line)TypeError: Unicode-objects must be encoded before hashing import hashlib, sysm = hashlib.md5()hash = """"hash_file = input(""What is the file name in which the hash resides? "")wordlist = input(""What is your wordlist? (Enter the file name) "")try: hashdocument = open(hash_file, ""r"")except IOError: print(""Invalid file."") raw_input() sys.exit()else: hash = hashdocument.readline() hash = hash.replace(""\n"", """")try: wordlistfile = open(wordlist, ""r"")except IOError: print(""Invalid file."") raw_input() sys.exit()else: passfor line in wordlistfile: # Flush the buffer (this caused a massive problem when placed # at the beginning of the script, because the buffer kept getting # overwritten, thus comparing incorrect hashes) m = hashlib.md5() line = line.replace(""\n"", """") m.update(line) word_hash = m.hexdigest() if word_hash == hash: print(""Collision! The word corresponding to the given hash is"", line) input() sys.exit()print(""The hash given does not correspond to any supplied word in the wordlist."")input()sys.exit()",How to correct TypeError: Unicode-objects must be encoded before hashing?
"""TypeError: Unicode-objects must be encoded before hashing"""," I have this error: when I try to execute this code in Python 3.2.2: <code>  Traceback (most recent call last): File ""python_md5_cracker.py"", line 27, in <module> m.update(line)TypeError: Unicode-objects must be encoded before hashing import hashlib, sysm = hashlib.md5()hash = """"hash_file = input(""What is the file name in which the hash resides? "")wordlist = input(""What is your wordlist? (Enter the file name) "")try: hashdocument = open(hash_file, ""r"")except IOError: print(""Invalid file."") raw_input() sys.exit()else: hash = hashdocument.readline() hash = hash.replace(""\n"", """")try: wordlistfile = open(wordlist, ""r"")except IOError: print(""Invalid file."") raw_input() sys.exit()else: passfor line in wordlistfile: # Flush the buffer (this caused a massive problem when placed # at the beginning of the script, because the buffer kept getting # overwritten, thus comparing incorrect hashes) m = hashlib.md5() line = line.replace(""\n"", """") m.update(line) word_hash = m.hexdigest() if word_hash == hash: print(""Collision! The word corresponding to the given hash is"", line) input() sys.exit()print(""The hash given does not correspond to any supplied word in the wordlist."")input()sys.exit()",How to correct TypeError: Unicode-objects must be encoded before hashing?
What is -m switch for in Python?," Could you explain to me what the difference is between calling and It seems in both cases mymod1.py is called and sys.argv is So what is the -m switch for? <code>  python -m mymod1 mymod2.py args python mymod1.py mymod2.py args ['mymod1.py', 'mymod2.py', 'args']",What is the purpose of the -m switch?
What is the -m switch for in Python?," Could you explain to me what the difference is between calling and It seems in both cases mymod1.py is called and sys.argv is So what is the -m switch for? <code>  python -m mymod1 mymod2.py args python mymod1.py mymod2.py args ['mymod1.py', 'mymod2.py', 'args']",What is the purpose of the -m switch?
python program to read a matrix from a given file," I have a text file which contains matrix of N * M dimensions.For example the input.txt file contains the following: I need to write python script where in I can import the matrix.My current python script is: the output list comes like this I need to fetch the values in int form . If I try to type cast, it throws errors. <code>  0,0,0,0,0,0,0,0,0,00,0,0,0,0,0,0,0,0,00,0,0,0,0,0,0,0,0,00,0,0,0,0,0,0,0,0,00,0,0,0,0,0,0,0,0,00,0,0,0,0,0,0,0,0,00,0,2,1,0,2,0,0,0,00,0,2,1,1,2,2,0,0,10,0,1,2,2,1,1,0,0,21,0,1,1,1,2,1,0,2,1 f = open ( 'input.txt' , 'r')l = []l = [ line.split() for line in f]print l [['0,0,0,0,0,0,0,0,0,0'], ['0,0,0,0,0,0,0,0,0,0'], ['0,0,0,0,0,0,0,0,0,0'], ['0,0,0,0,0,0,0,0,0,0'], ['0,0,0,0,0,0,0,0,0,0'], ['0,0,0,0,0,0,0,0,0,0'], ['0,0,2,1,0,2,0,0,0,0'], ['0,0,2,1,1,2,2,0,0,1'], ['0,0,1,2,2,1,1,0,0,2'], ['1,0,1,1,1,2,1,0,2,1']]",How to to read a matrix from a given file?
Numpy array: how to find index of first occurrence of item, How can I find the index of the first occurrence of a number in a Numpy array?Speed is important to me. I am not interested in the following answers because they scan the whole array and don't stop when they find the first occurrence: Note 1: none of the answers from that question seem relevant Is there a Numpy function to return the first index of something in an array?Note 2: using a C-compiled method is preferred to a Python loop. <code>  itemindex = numpy.where(array==item)[0][0]nonzero(array == item)[0][0],Numpy: find first index of value fast
using packages installed in virtualenv," I created a new virtual environment: Then, I activate the virtual environment and install packages: This all works just fine. I then install my own project called Hermes which uses termcolor: But when I run the executable that's installed to the virtualenv's bin directory, I get an error: How do I install termcolor? <code>  $ virtualenv --no-site-packages venv --python=python3.2 $ source venv/bin/activate$ pip install termcolor$ python -m termcolor $ python setup.py install ImportError: no module named termcolor",Running a python script produces: ImportError: no module named termcolor
Using packages installed in virtualenv," I created a new virtual environment: Then, I activate the virtual environment and install packages: This all works just fine. I then install my own project called Hermes which uses termcolor: But when I run the executable that's installed to the virtualenv's bin directory, I get an error: How do I install termcolor? <code>  $ virtualenv --no-site-packages venv --python=python3.2 $ source venv/bin/activate$ pip install termcolor$ python -m termcolor $ python setup.py install ImportError: no module named termcolor",Running a python script produces: ImportError: no module named termcolor
Get the name of the current timezone?," How do I get the Olson timezone name (such as Australia/Sydney) corresponding to the value given by C's localtime call?This is the value overridden via TZ, by symlinking /etc/localtime, or setting a TIMEZONE variable in time-related system configuration files. <code> ",Get the Olson TZ name for the local timezone?
How to approach number guessing game(with a twist) algorithm?," Update(July 2020): Question is 9 years old but still one that I'm deeply interested in. In the time since, machine learning(RNN's, CNN's, GANS,etc), new approaches and cheap GPU's have risen that enable new approaches. I thought it would be fun to revisit this question to see if there are new approaches.I am learning programming (Python and algorithms) and was trying to work on a project that I find interesting. I have created a few basic Python scripts, but Im not sure how to approach a solution to a game I am trying to build.Heres how the game will work:Users will be given items with a value. For example, They will then get a chance to choose any combo of them they like (i.e. 100 apples, 20 pears, and one orange). The only output the computer gets is the total value (in this example, it's currently $143). The computer will try to guess what they have. Which obviously it wont be able to get correctly the first turn. The next turn the user can modify their numbers but no more than 5% of the total quantity (or some other percent we may chose. Ill use 5% for example.). The prices of fruit can change(at random) so the total value may change based on that also (for simplicity I am not changing fruit prices in this example). Using the above example, on day 2 of the game, the user returns a value of $152 and $164 on day 3. Here's an example: *(I hope the tables show up right, I had to manually space them so hopefully it's not just doing it on my screen, if it doesn't work let me know and I'll try to upload a screenshot.)I am trying to see if I can figure out what the quantities are over time (assuming the user will have the patience to keep entering numbers). I know right now my only restriction is the total value cannot be more than 5% so I cannot be within 5% accuracy right now so the user will be entering it forever.What I have done so farHeres my solution so far (not much). Basically, I take all the values and figure out all the possible combinations of them (I am done this part). Then I take all the possible combos and put them in a database as a dictionary (so for example for $143, there could be a dictionary entry {apple:143, Pears:0, Oranges :0}..all the way to {apple:0, Pears:1, Oranges :47}. I do this each time I get a new number so I have a list of all possibilities.Heres where Im stuck. In using the rules above, how can I figure out the best possible solution? I think Ill need a fitness function that automatically compares the two days data and removes any possibilities that have more than 5% variance of the previous days data.Questions:So my question with user changing the total and me having a list of all the probabilities, how should I approach this? What do I need to learn? Is there any algorithms out there or theories that I can use that are applicable? Or, to help me understand my mistake, can you suggest what rules I can add to make this goal feasible (if it's not in its current state. I was thinking adding more fruits and saying they must pick at least 3, etc..)? Also, I only have a vague understanding of genetic algorithms, but I thought I could use them here, if is there something I can use?I'm very very eager to learn so any advice or tips would be greatly appreciated (just please don't tell me this game is impossible).UPDATE: Getting feedback that this is hard to solve. So I thought I'd add another condition to the game that won't interfere with what the player is doing (game stays the same for them) but everyday the value of the fruits change price (randomly). Would that make it easier to solve? Because within a 5% movement and certain fruit value changes, only a few combinations are probable over time.Day 1, anything is possible and getting a close enough range is almost impossible, but as the prices of fruits change and the user can only choose a 5% change, then shouldn't (over time) the range be narrow and narrow. In the above example, if prices are volatile enough I think I could brute force a solution that gave me a range to guess in, but I'm trying to figure out if there's a more elegant solution or other solutions to keep narrowing this range over time.UPDATE2: After reading and asking around, I believe this is a hidden Markov/Viterbi problem that tracks the changes in fruit prices as well as total sum (weighting the last data point the heaviest). I'm not sure how to apply the relationship though. I think this is the case and could be wrong but at the least I'm starting to suspect this is a some type of machine learning problem.Update 3: I am created a test case (with smaller numbers) and a generator to help automate the user generated data and I am trying to create a graph from it to see what's more likely.Here's the code, along with the total values and comments on what the users actually fruit quantities are. <code>  Apple = 1Pears = 2Oranges = 3 Value quantity(day1) value(day1)Apple 1 100 100Pears 2 20 40Orange 3 1 3Total 121 143 Quantity (day2) %change (day2) Value (day2) Quantity (day3) %change (day3) Value(day3) 104 104 106 106 21 42 23 46 2 6 4 12 127 4.96% 152 133 4.72% 164 #!/usr/bin/env pythonimport itertools# Fruit price datafruitPriceDay1 = {'Apple':1, 'Pears':2, 'Oranges':3}fruitPriceDay2 = {'Apple':2, 'Pears':3, 'Oranges':4}fruitPriceDay3 = {'Apple':2, 'Pears':4, 'Oranges':5}# Generate possibilities for testing (warning...will not scale with large numbers)def possibilityGenerator(target_sum, apple, pears, oranges): allDayPossible = {} counter = 1 apple_range = range(0, target_sum + 1, apple) pears_range = range(0, target_sum + 1, pears) oranges_range = range(0, target_sum + 1, oranges) for i, j, k in itertools.product(apple_range, pears_range, oranges_range): if i + j + k == target_sum: currentPossible = {} #print counter #print 'Apple', ':', i/apple, ',', 'Pears', ':', j/pears, ',', 'Oranges', ':', k/oranges currentPossible['apple'] = i/apple currentPossible['pears'] = j/pears currentPossible['oranges'] = k/oranges #print currentPossible allDayPossible[counter] = currentPossible counter = counter +1 return allDayPossible# Total sum being returned by user for value of fruitstotalSumDay1=26 # Computer does not know this but users quantities are apple: 20, pears 3, oranges 0 at the current prices of the daytotalSumDay2=51 # Computer does not know this but users quantities are apple: 21, pears 3, oranges 0 at the current prices of the daytotalSumDay3=61 # Computer does not know this but users quantities are apple: 20, pears 4, oranges 1 at the current prices of the daygraph = {}graph['day1'] = possibilityGenerator(totalSumDay1, fruitPriceDay1['Apple'], fruitPriceDay1['Pears'], fruitPriceDay1['Oranges'] )graph['day2'] = possibilityGenerator(totalSumDay2, fruitPriceDay2['Apple'], fruitPriceDay2['Pears'], fruitPriceDay2['Oranges'] )graph['day3'] = possibilityGenerator(totalSumDay3, fruitPriceDay3['Apple'], fruitPriceDay3['Pears'], fruitPriceDay3['Oranges'] )# Sample of dict = 1 : {'oranges': 0, 'apple': 0, 'pears': 0}..70 : {'oranges': 8, 'apple': 26, 'pears': 13}print graph",How to approach a number guessing game (with a twist) algorithm?
State machine in a statically typed language?," I implemented a simple state machine in Python: I wanted to port it to C, because it wasn't fast enough. But C doesn't let me make a function that returns a function of the same type. I tried making the function of this type: typedef *fn(fn)(), but it doesn't work, so I had to use a structure instead. Now the code is very ugly! So I figured it's a problem with C's broken type system. So I used a language with a real type system (Haskell), but the same problem happens. I can't just do something like: I get the error, Cycle in type synonym declarations.So I have to make some wrapper the same way I did for the C code like this: Why is it so hard to make a state machine in a statically typed language? I have to make unnecessary overhead in statically typed languages as well. Dynamically typed languages don't have this problem. Is there an easier way to do it in a statically typed language? <code>  import timedef a(): print ""a()"" return bdef b(): print ""b()"" return cdef c(): print ""c()"" return aif __name__ == ""__main__"": state = a while True: state = state() time.sleep(1) #include <stdio.h>#include <stdlib.h>#include <unistd.h>typedef struct fn { struct fn (*f)(void);} fn_t;fn_t a(void);fn_t b(void);fn_t c(void);fn_t a(void){ fn_t f = {b}; (void)printf(""a()\n""); return f;}fn_t b(void){ fn_t f = {c}; (void)printf(""b()\n""); return f;}fn_t c(void){ fn_t f = {a}; (void)printf(""c()\n""); return f;}int main(void){ fn_t state = {a}; for(;; (void)sleep(1)) state = state.f(); return EXIT_SUCCESS;} type Fn = IO Fna :: Fna = print ""a()"" >> return bb :: Fnb = print ""b()"" >> return cc :: Fnc = print ""c()"" >> return a import Control.Monadimport System.Posixdata Fn = Fn (IO Fn)a :: IO Fna = print ""a()"" >> return (Fn b)b :: IO Fnb = print ""b()"" >> return (Fn c)c :: IO Fnc = print ""c()"" >> return (Fn a)run = foldM (\(Fn f) () -> sleep 1 >> f) (Fn a) (repeat ())",Clean and type-safe state machine implementation in a statically typed language?
clean and type-safe state machine implementation in a statically typed language ?," I implemented a simple state machine in Python: I wanted to port it to C, because it wasn't fast enough. But C doesn't let me make a function that returns a function of the same type. I tried making the function of this type: typedef *fn(fn)(), but it doesn't work, so I had to use a structure instead. Now the code is very ugly! So I figured it's a problem with C's broken type system. So I used a language with a real type system (Haskell), but the same problem happens. I can't just do something like: I get the error, Cycle in type synonym declarations.So I have to make some wrapper the same way I did for the C code like this: Why is it so hard to make a state machine in a statically typed language? I have to make unnecessary overhead in statically typed languages as well. Dynamically typed languages don't have this problem. Is there an easier way to do it in a statically typed language? <code>  import timedef a(): print ""a()"" return bdef b(): print ""b()"" return cdef c(): print ""c()"" return aif __name__ == ""__main__"": state = a while True: state = state() time.sleep(1) #include <stdio.h>#include <stdlib.h>#include <unistd.h>typedef struct fn { struct fn (*f)(void);} fn_t;fn_t a(void);fn_t b(void);fn_t c(void);fn_t a(void){ fn_t f = {b}; (void)printf(""a()\n""); return f;}fn_t b(void){ fn_t f = {c}; (void)printf(""b()\n""); return f;}fn_t c(void){ fn_t f = {a}; (void)printf(""c()\n""); return f;}int main(void){ fn_t state = {a}; for(;; (void)sleep(1)) state = state.f(); return EXIT_SUCCESS;} type Fn = IO Fna :: Fna = print ""a()"" >> return bb :: Fnb = print ""b()"" >> return cc :: Fnc = print ""c()"" >> return a import Control.Monadimport System.Posixdata Fn = Fn (IO Fn)a :: IO Fna = print ""a()"" >> return (Fn b)b :: IO Fnb = print ""b()"" >> return (Fn c)c :: IO Fnc = print ""c()"" >> return (Fn a)run = foldM (\(Fn f) () -> sleep 1 >> f) (Fn a) (repeat ())",Clean and type-safe state machine implementation in a statically typed language?
"Python: produce PDF files, draw polygons with rounded corners"," What's the right tool for the job if I want to write a Python script that produces vector graphics in PDF format? In particular, I need to draw filled polygons with rounded corners (i.e., plane figures that are composed of straight lines and circular arcs).It seems that matplotlib makes it fairly easy to draw rectangles with rounded corners and general polygons with sharp corners. However, to draw polygons with rounded corners, it seems that I have to first compute a Bzier curve that approximates the shape.Is there anything more straightforward available? Or is there another library that I can use to compute the Bzier curve that approximates the shape that I want to produce? Ideally, I would simply specify the (location, corner radius) pair for each vertex.Here is an example: I would like to specify the red polygon (+ the radius of each corner) and the library would output the grey figure:(For convex polygons I could cheat and use a thick pen to draw the outline of the polygon. However, this does not work in the non-convex case.) <code> ","Produce PDF files, draw polygons with rounded corners"
Parse JSON in Python," My project is currently receiving a JSON message in python which I need to get bits of information out of. For the purposes of this, let's set it to some simple JSON in a string: So far I've been generating JSON requests using a list and then json.dumps, but to do the opposite of this I think I need to use json.loads. However I haven't had much luck with it. Could anyone provide me a snippet that would return ""2"" with the input of ""two"" in the above example? <code>  jsonStr = '{""one"" : ""1"", ""two"" : ""2"", ""three"" : ""3""}'",How to parse data in JSON format?
How to parse data in JSON?," My project is currently receiving a JSON message in python which I need to get bits of information out of. For the purposes of this, let's set it to some simple JSON in a string: So far I've been generating JSON requests using a list and then json.dumps, but to do the opposite of this I think I need to use json.loads. However I haven't had much luck with it. Could anyone provide me a snippet that would return ""2"" with the input of ""two"" in the above example? <code>  jsonStr = '{""one"" : ""1"", ""two"" : ""2"", ""three"" : ""3""}'",How to parse data in JSON format?
How to parse JSON in Python?," My project is currently receiving a JSON message in python which I need to get bits of information out of. For the purposes of this, let's set it to some simple JSON in a string: So far I've been generating JSON requests using a list and then json.dumps, but to do the opposite of this I think I need to use json.loads. However I haven't had much luck with it. Could anyone provide me a snippet that would return ""2"" with the input of ""two"" in the above example? <code>  jsonStr = '{""one"" : ""1"", ""two"" : ""2"", ""three"" : ""3""}'",How to parse data in JSON format?
cannout urllib.urlencode a URL in python," Why am I getting this error when trying to urlencode this string <code>  >>> callback = ""http://localhost/application/authtwitter?twitterCallback"" >>> urllib.urlencode(callback) Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/usr/lib/python2.7/urllib.py"", line 1261, in urlencode raise TypeError TypeError: not a valid non-string sequence or mapping object",cannot urllib.urlencode a URL in python
Daemonizing a python script in debian using virutalenv," I've seen a lot of scripts for daemonizing a python script in linux, but not much information about how to use them. Could anyone guide me on this?I currently have a lengthy python script that listens on a socket for an incoming message, if it's the correct format accepts it and then stores it into the database. The script itself just opens the socket and then listens on a while true (which does the job!) and does all the work in there.To daemonize it, would I have to modify my current script or call it from a separate script? I've seen examples of both but got neither to work.Also, I'm using virtualenv which might the root of my problems, any hints on using this with daemonized scripts? <code> ",Daemonizing a python script in debian using virtualenv
"Add an item (') to a list ('A,B,C') in Python?"," Given a string that is a sequence of several values separated by a commma: How do I convert the string to a list? <code>  mStr = 'A,B,C,D,E' mList = ['A', 'B', 'C', 'D', 'E']",How to convert comma-delimited string to list in Python?
What programming languages are well suited for developing a live coding framework with?," I would like to build a ""live coding framework"".I should explain what is meant by ""live coding framework"". I'll do so by comparing live coding to traditional coding.Generally put, in traditional programming you write code, sometimes compile it, then launch an executable or open a script in some sort of interpreter. If you want to modify your application you must repeat this process. A live coding framework enables code to be updated while the application is running and reloaded on demand. Perhaps this reloading happens each time a file containing code is changed or by some other action. Changes in the code are then reflected in the application as it is running. There is no need to close the program and to recompile and relaunch it. In this case, the application is a windowed app that has an update/draw loop, is most likely using OpenGL for graphics, an audio library for sound processing ( SuperCollider? ) and ideally a networking lib.Of course I have preferred languages, though I'm not certain that any of them would be well suited for this kind of architecture. Ideally I would use Python, Lua, Ruby or another higher level language. However, a friend recently suggested Clojure as a possibility, so I am considering it as well.I would like to know not only what languages would be suitable for this kind of framework but, generally, what language features would make a framework such as this possible. <code> ",What programming language features are well suited for developing a live coding framework?
why operator module has no function for logical or?," In Python 3, operator.or_ is equivalent to the bitwise |, not the logical or. Why is there no operator for the logical or? <code> ",Why doesn't the operator module have a function for logical or?
Writing to a new dictory in Python without changing directory," Currently, I have the following code... My goal is to have python write the file (under file_name) to a file in the ""address"" folder in the ""feed"" folder in the current directory (IE, where the python script is saved)I've looked into the os module, but I don't want to change my current directory and these directories do not already exist. <code>  file_name = content.split('=')[1].replace('""', '') #file, gotten previouslyfileName = ""/"" + self.feed + ""/"" + self.address + ""/"" + file_name #add folders output = open(file_name, 'wb')output.write(url.read())output.close()",Writing to a new directory in Python without changing directory
openning a file with a python application," I am trying to figure out how to make a python program open a file when a user right clicks on the file and selects ""Open With"". For example, I want a user to be able right click on a text file and to select my program so that my program can process the text file. Is the name of the text file passed into my program someway? Thanks. <code> ","""Open with..."" a file on Windows, with a python application"
python - matplotlib - setting aspect ratio," I'm trying to make a square plot (using imshow), i.e. aspect ratio of 1:1, but I can't. None of these work: It seems like the calls are just being ignored (a problem I often seem to have with matplotlib). <code>  import matplotlib.pyplot as pltax = fig.add_subplot(111,aspect='equal')ax = fig.add_subplot(111,aspect=1.0)ax.set_aspect('equal')plt.axes().set_aspect('equal')",How can I set the aspect ratio in matplotlib?
Python - Using strip on lists," I have to take a large list of words in the form: and then using the strip function, turn it into: I thought that what I had written would work, but I keep getting an error saying: ""'list' object has no attribute 'strip'""Here is the code that I tried: <code>  ['this\n', 'is\n', 'a\n', 'list\n', 'of\n', 'words\n'] ['this', 'is', 'a', 'list', 'of', 'words'] strip_list = []for lengths in range(1,20): strip_list.append(0) #longest word in the text file is 20 characters longfor a in lines: strip_list.append(lines[a].strip())",Remove trailing newline from the elements of a string list
Using strip on lists," I have to take a large list of words in the form: and then using the strip function, turn it into: I thought that what I had written would work, but I keep getting an error saying: ""'list' object has no attribute 'strip'""Here is the code that I tried: <code>  ['this\n', 'is\n', 'a\n', 'list\n', 'of\n', 'words\n'] ['this', 'is', 'a', 'list', 'of', 'words'] strip_list = []for lengths in range(1,20): strip_list.append(0) #longest word in the text file is 20 characters longfor a in lines: strip_list.append(lines[a].strip())",Remove trailing newline from the elements of a string list
Strip all the elements of a string list," I have to take a large list of words in the form: and then using the strip function, turn it into: I thought that what I had written would work, but I keep getting an error saying: ""'list' object has no attribute 'strip'""Here is the code that I tried: <code>  ['this\n', 'is\n', 'a\n', 'list\n', 'of\n', 'words\n'] ['this', 'is', 'a', 'list', 'of', 'words'] strip_list = []for lengths in range(1,20): strip_list.append(0) #longest word in the text file is 20 characters longfor a in lines: strip_list.append(lines[a].strip())",Remove trailing newline from the elements of a string list
what does this django regex mean?, I have the following regular expression (regex) in my urls.py and I'd like to know what it means. Specifically the (?P<category_slug> portion of the regex. <code>  r'^category/(?P<category_slug>[-\w]+)/$,What does this Django regular expression mean? `?P`
what does this django regex mean? `?P`, I have the following regular expression (regex) in my urls.py and I'd like to know what it means. Specifically the (?P<category_slug> portion of the regex. <code>  r'^category/(?P<category_slug>[-\w]+)/$,What does this Django regular expression mean? `?P`
"How to read large file, line by line in python"," I want to iterate over each line of an entire file. One way to do this is by reading the entire file, saving it to a list, then going over the line of interest. This method uses a lot of memory, so I am looking for an alternative.My code so far: Executing this code gives an error message: device active.Any suggestions?The purpose is to calculate pair-wise string similarity, meaning for each line in file, I want to calculate the Levenshtein distance with every other line. <code>  for each_line in fileinput.input(input_file): do_something(each_line) for each_line_again in fileinput.input(input_file): do_something(each_line_again)",How to read a large file - line by line?
"How to a read large file, line by line in python"," I want to iterate over each line of an entire file. One way to do this is by reading the entire file, saving it to a list, then going over the line of interest. This method uses a lot of memory, so I am looking for an alternative.My code so far: Executing this code gives an error message: device active.Any suggestions?The purpose is to calculate pair-wise string similarity, meaning for each line in file, I want to calculate the Levenshtein distance with every other line. <code>  for each_line in fileinput.input(input_file): do_something(each_line) for each_line_again in fileinput.input(input_file): do_something(each_line_again)",How to read a large file - line by line?
"How to a read large file, line by line in Python"," I want to iterate over each line of an entire file. One way to do this is by reading the entire file, saving it to a list, then going over the line of interest. This method uses a lot of memory, so I am looking for an alternative.My code so far: Executing this code gives an error message: device active.Any suggestions?The purpose is to calculate pair-wise string similarity, meaning for each line in file, I want to calculate the Levenshtein distance with every other line. <code>  for each_line in fileinput.input(input_file): do_something(each_line) for each_line_again in fileinput.input(input_file): do_something(each_line_again)",How to read a large file - line by line?
"How to read a large file, line by line, in Python"," I want to iterate over each line of an entire file. One way to do this is by reading the entire file, saving it to a list, then going over the line of interest. This method uses a lot of memory, so I am looking for an alternative.My code so far: Executing this code gives an error message: device active.Any suggestions?The purpose is to calculate pair-wise string similarity, meaning for each line in file, I want to calculate the Levenshtein distance with every other line. <code>  for each_line in fileinput.input(input_file): do_something(each_line) for each_line_again in fileinput.input(input_file): do_something(each_line_again)",How to read a large file - line by line?
How to read a large file line by line," I want to iterate over each line of an entire file. One way to do this is by reading the entire file, saving it to a list, then going over the line of interest. This method uses a lot of memory, so I am looking for an alternative.My code so far: Executing this code gives an error message: device active.Any suggestions?The purpose is to calculate pair-wise string similarity, meaning for each line in file, I want to calculate the Levenshtein distance with every other line. <code>  for each_line in fileinput.input(input_file): do_something(each_line) for each_line_again in fileinput.input(input_file): do_something(each_line_again)",How to read a large file - line by line?
How can I inform sqlalchemy to raise error whenever MySQL throws warning?," I want to modify sqlalchemy settings to raise an exception on warnings.For example, when I insert a larger amount of data in a field than the defined column length, mysql will truncate the data, insert the truncated data into the field and issue a warning.Instead of the warning, I would like sqlalchemy to raise an appropriate error on behalf of mysql. <code> ",How can I inform sqlalchemy to raise an error whenever MySQL throws a warning?
how to use python closing context manager," The standard library open function works both as a function: or as a context manager: I am trying to mimic this behaviour using contextlib.closing, where File is my custom file I/O class: this works as expected as a context manager: but of course if I call directly, I get back the closing object instead of my object: So, how do I implement myopen so that it both works as a context manager and returns my File object when called directly?Full working example on github: https://gist.github.com/1352573 <code>  f = open('file.txt')print(type(f))<type 'file'> with open('file.txt') as f: print(type(f))<type 'file'> def myopen(filename): f = File(filename) f.open() return closing(f) with myopen('file.txt') as f: print(type(f))<class '__main__.File'> f = myopen(filename)print(type(f))<class 'contextlib.closing'>",How to use Python closing context manager
content of infobox of wikipedia," I need to get the content of an infobox of any movie. I know the name of the movie. One way is to get the complete content of a Wikipedia page and then parse it until I find {{Infobox and then get the content of the infobox.Is there any other way for the same using some API or parser?I am using Python and the pywikipediabot API.I am also familiar with the wikitools API. So instead of pywikipedia if someone has solution related to the wikitools API, please mention that as well. <code> ",Content of infobox of Wikipedia
Why Python is so slow for a simple loop," We are making some kNN and SVD implementations in Python. Others picked Java. Our execution times are very different. I used cProfile to see where I make mistakes but everything is quite fine actually. Yes, I use numpy also. But I would like to ask simple question. This snippet takes 31.40s on my computer.Java version of this code takes 1 second or less on the same computer. Type checking is a main problem for this code, I suppose. But I should make lots of operation like this for my project and I think 9999*9999 is not so big number.I think I am making mistakes because I know Python is used by lots of scientific projects. But why is this code so slow and how can I handle problems bigger than this?Should I use a JIT compiler such as Psyco?EDITI also say that this loop problem is only an example. The code is not as simple as like this and It may be tough to put into practice your improvements/code samples.Another question is that can I implement lots of data mining & machine learning algorithms with numpy and scipy if I use it correctly?  <code>  total = 0.0for i in range(9999): # xrange is slower according for j in range(1, 9999): #to my test but more memory-friendly. total += (i / j)print total",Why Python is so slow for a simple for loop?
Multiple Inheritence with same Base Classes in Python," I'm trying to wrap my head around multiple inheritance in python.Suppose I have the following base class: And let's say I have two classes that inherit from it: Finally, a create a class that uses multiple inheritance: Then, I create a SchoolHouse object and run build on it: So I'm wondering -- what happened to the School class? Is there any way to get Python to run both somehow?I'm wondering specifically because there are a fair number of Django packages out there that provide custom Managers for models. But there doesn't appear to be a way to combine them without writing one or the other of the Managers as inheriting from the other one. It'd be nice to just import both and use both somehow, but looks like it can't be done?Also I guess it'd just help to be pointed to a good primer on multiple inheritance in Python. I have done some work with Mixins before and really enjoy using them. I guess I just wonder if there is any elegant way to combine functionality from two different classes when they inherit from the same base class.Yup, silly me. It was a typo all along. I feel very dumb. I promise, I always put the right class in when I super in real life, it was only when I was cutting and pasting to try this out that I messed up. <code>  class Structure(object): def build(self, *args): print ""I am building a structure!"" self.components = args class House(Structure): def build(self, *args): print ""I am building a house!"" super(House, self).build(*args)class School(Structure): def build(self, type=""Elementary"", *args): print ""I am building a school!"" super(School, self).build(*args) class SchoolHouse(School, House): def build(self, *args): print ""I am building a schoolhouse!"" super(School, self).build(*args) >>> sh = SchoolHouse()>>> sh.build(""roof"", ""walls"")I am building a schoolhouse!I am building a house!I am building a structure!",Multiple Inheritance with same Base Classes in Python
"Is there a way to ""compile"" python code onto an arduino (Uno)?"," I have a robotics type project with an Arduino Uno, and to make a long story short, I am experimenting with some AI algorithms. However, I need to implement some high level matrix algorithms that would be quite simple using NumPy/SciPy, but they are an utter nightmare in C or C++. Even with the libraries out there, this is just getting ridiculous.Is there any way I can do this project in Python? I think I heard something about the Mega having this capability, but I have an Uno, and replacing it is not an option at this point (that would set the project back quite a bit.) Also, I heard somethings about using Python to communicate to the Arduino via USB, but I cannot have the USB cable in while the thing is running. I need to be able to upload the program and be done with it.Are there any options out there, or have I just reached a dead end? <code> ","Is there a way to ""compile"" Python code onto an Arduino (Uno)?"
pyserial not talking to arduino," Python version: 2.6.6PySerial version: 2.5Arduino board: Duemilanove 328I have written some code to simulate some hardware I'm working with and uploaded it to the Arduino board. This code works. I know this, because I get the expected response from HyperTerminal.However, when I try to connect using PySerial the connection does not error, but I get no response to the commands I send.Why might this be?Python code NB: the code on the Arduino sends back \r\n at the end of a response to a command.HyperTerminal configuration:EditI have found that if I increase the timeout to 10 seconds and add a sp.readline() before I send anything, then I get responses to both commands.How long is the hardware handshake usually between PySerial and an Arduino or USB RS-232 ports? <code>  import serialdef main(): sp = serial.Serial() sp.port = 'COM4' sp.baudrate = 19200 sp.parity = serial.PARITY_NONE sp.bytesize = serial.EIGHTBITS sp.stopbits = serial.STOPBITS_ONE sp.timeout = 0.5 sp.xonxoff = False sp.rtscts = False sp.dsrdtr = False sp.open() sp.write(""GV\r\n"".encode('ascii')) value = sp.readline() print value sp.write(""GI\r\n"".encode('ascii')) value = sp.readline() print value sp.close() if __name__ == ""__main__"": main()",PySerial not talking to Arduino
i thought in python everything was passed by reference... was I wrong?," Take the following code I thought input would now be 10. Why is this not the case? <code>  #module functions.pydef foo(input, new_val): input = new_val#module main.pyinput = 5functions.foo(input, 10)print input",I thought Python passed everything by reference?
Streaming stdio/stdout in Python," I'm trying to stream a bash shell to/from a simple WebSockets UI, but I'm having trouble redirecting the IO. I want to start an instance of bash and connect stdout and stdin to write_message() and on_message() functions that interact with my web UI. Here's a simplified version of what I'm trying to do: While bash appears to start and on_message does get called, I don't get any output. readline() remains blocking. I've tried stdout.read(), stdout.read(1), and various buffer modifications, but still no output. I've also tried hardcoding commands with a trailing '\n' in on_message to isolate the issue, but I still don't get any output from readline().Ideally I want to stream each byte written to stdout in realtime, without waiting for EOL or any other characters, but I'm having a hard time finding the right API. Any pointers would be appreciated. <code>  class Handler(WebSocketHandler): def open(self): print ""New connection opened."" self.app = subprocess.Popen([""/bin/bash"", ""--norc"", ""-i""], stdout=subprocess.PIPE, stdin=subprocess.PIPE, shell=False) thread.start_new_thread(self.io_loop, ()) def on_message(self, message): self.app.stdin.write(message) def on_close(self): self.app.terminate() def io_loop(self): while self.app.poll() is None: line = self.app.stdout.readline() if line: self.write_message(line)",Streaming stdin/stdout in Python
in python how will you multiply individual elements of an array with a floting point or integer number," Here S is an arrayHow will I multiply this and get the value? <code>  S = [22, 33, 45.6, 21.6, 51.8]P = 2.45 SP = [53.9, 80.85, 111.72, 52.92, 126.91]",How to multiply individual elements of a list with a number?
In Python how will you multiply individual elements of an array with a floating point or integer number?," Here S is an arrayHow will I multiply this and get the value? <code>  S = [22, 33, 45.6, 21.6, 51.8]P = 2.45 SP = [53.9, 80.85, 111.72, 52.92, 126.91]",How to multiply individual elements of a list with a number?
In Python how will you multiply individual elements of a list with a floating point or integer number?," Here S is an arrayHow will I multiply this and get the value? <code>  S = [22, 33, 45.6, 21.6, 51.8]P = 2.45 SP = [53.9, 80.85, 111.72, 52.92, 126.91]",How to multiply individual elements of a list with a number?
Does Django Session is Thread safe?," I am storing a dictionary in a Django Session which is accessible by multiple threads. All threads can update that dictionary, threads also get values from dictionary in order to run the process. I want to know does the Django Session is thread safe or I have to use locks or semaphores?Typical example: Does there is a chance that when Point2 update thread dictionary in session just after it updates Point1 also update it, then my stop to quit thread is lost.More InfoAn ajax request start four threads which download samples from the 4 different urls. Why i used threads? because i want to show the user which samples are currently being downloaded and which are left. All threads will update its state in dictionary within session. After threads started i make ajax request after every two seconds and take the dictionary from session and read the current state of threads. But this idea failed because threads are independent of request and their session. Each ajax request definately have its session but i can not pass that session to threads because when they once begin they are independent of rest of world (May be i can pass it but i may not pass it as fast the processsing is being done by the threads). so to tackle this problem i choose cache framework instead of session. as cache is accessable from any where. Threads store their state in dictionary and put back in to cache and after every two seconds i take dictionary from cache and read the state. And one more thing according to my experience cache is not thread safe. So for four threads i used four dictionaries sepratelly. <code>  Thread1:threadDict = request.session.get('threadDict', None)if threadDict['stop']: #break the for loop exit the threadelse: #do some processing and update some values in thread dictionary threadDict['abc'] = 0 request.session['threadDict'] = threadDict (Point1)def someFunction(): #this function is used to send stop signal to thread threadDict = request.session.get('threadDict', None) threadDict['stop'] = True request.session['threadDict'] = threadDict (Point2)",Is a Django session thread safe?
How to use timeit correctly," I understand the concept of what timeit does but I am not sure how to implement it in my code.How can I compare two functions, say insertion_sort and tim_sort, with timeit? <code> ",How to use timeit module
How can improve the efficiency of this numpy loop," I've got a numpy array containing labels. I'd like to get calculate a number for each label based on its size and bounding box. How can I write this more efficiently so that it's realistic to use on large arrays (~15000 labels)? <code>  A = array([[ 1, 1, 0, 3, 3], [ 1, 1, 0, 0, 0], [ 1, 0, 0, 2, 2], [ 1, 0, 2, 2, 2]] )B = zeros( 4 )for label in range(1, 4): # get the bounding box of the label label_points = argwhere( A == label ) (y0, x0), (y1, x1) = label_points.min(0), label_points.max(0) + 1 # assume I've computed the size of each label in a numpy array size_A B[ label ] = myfunc(y0, x0, y1, x1, size_A[label])",How can I improve the efficiency of this numpy loop
Dividing Python module in to multiple regions, In C# we can create regions by using Is there any way to format python code in similar fashion so that I can keep all my relevant methods in one block? <code>  #region// some methods#endregion,Dividing Python module into multiple regions
Matlibplot : Comma separated number format for axis, I am attempting to change the format of my axis to be comma seperated in Matplotlib running under Python 2.7 but am unable to do so.I suspect that I need to use a FuncFormatter but I am at a bit of a loss.Can anyone help? <code> ,Matplotlib : Comma separated number format for axis
emacs python-mode python/ipython: multiple instances of python-shell?, Is there a way to force a new instance of python-shell while running Emacs? It would be convenient when working on multiple projects with separate working directories (and different sets of modules).Any attempt to invoke python-shell will only pull up the current instance. <code> ,Using multiple Python shells in Emacs 'python-mode' with Python or IPython
Call Javascript function from Python," I am working on a web-scraping project. One of the websites I am working with has the data coming from Javascript.There was a suggestion on one of my earlier questions that I can directly call the Javascript from Python, but I'm not sure how to accomplish this.For example: If a JavaScript function is defined as: add_2(var,var2)How would I call that JavaScript function from Python? <code> ",How do I call a Javascript function from Python?
"convert ASCII chars to ""corresponding"" UNICODE wide char?"," Can you easily convert between ASCII characters and their Asian full-width Unicode wide characters?Like: to <code>  0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~ \\",Convert ASCII chars to Unicode FULLWIDTH latin letters in Python?
"Convert ASCII chars to ""corresponding"" UNICODE wide chars in Python?"," Can you easily convert between ASCII characters and their Asian full-width Unicode wide characters?Like: to <code>  0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~ \\",Convert ASCII chars to Unicode FULLWIDTH latin letters in Python?
Python CSV Write Pproblem," I got this sample code: When I open the CSV in excel, I got all values in same column(A). I want to put values in A1, B1 and C1. How can I do that?MS Office Excel 2007 SP2Thank you.SOLUTION:w = csv.writer(file(r'test.csv','wb'), delimiter=';') <code>  import csvw = csv.writer(file(r'test.csv','wb'), dialect='excel')some_values=[(1,2,3)]w.writerows(some_values)",Python CSV Write column
How to import python module when module name has a '-' dash or hyphen in it?," I want to import foo-bar.py. This works: This does not: My question: Is there any way that I can use the above format i.e., from ""foo-bar"" import * to import a module that has a - in it? <code>  foobar = __import__(""foo-bar"") from ""foo-bar"" import *",How to import module when module name has a '-' dash or hyphen in it?
Unittest setUp/tearDown for serveral tests," Is there a function that is fired at the beginning/end of a scenario of tests? The functions setUp and tearDown are fired before/after every single test.I typically would like to have this: For now, these setUp and tearDown are unit tests and spread in all my scenarios (containing many tests), one is the first test, the other is the last test. <code>  class TestSequenceFunctions(unittest.TestCase): def setUpScenario(self): start() #launched at the beginning, once def test_choice(self): element = random.choice(self.seq) self.assertTrue(element in self.seq) def test_sample(self): with self.assertRaises(ValueError): random.sample(self.seq, 20) for element in random.sample(self.seq, 5): self.assertTrue(element in self.seq) def tearDownScenario(self): end() #launched at the end, once",Unittest setUp/tearDown for several tests
PyGame in a virtualenv?," Can't quite figure out how to install PyGame in a virtualenv on OSX Lion. I'd really like to keep things contained in the virtualenv, if at all possible. I've tried downloading the source for PyGame and running the included setup.py in the virtualenv, but it seems to be having a bunch of issues finding the SDL requirements even after installing those via Homebrew.Any tips from someone that's set this up? <code> ",PyGame in a virtualenv on OS X with brew?
how do i write to a python subprocess' stdin," I'm trying to write a Python script that starts a subprocess, and writes to the subprocess stdin. I'd also like to be able to determine an action to be taken if the subprocess crashes.The process I'm trying to start is a program called nuke which has its own built-in version of Python which I'd like to be able to submit commands to, and then tell it to quit after the commands execute. So far I've worked out that if I start Python on the command prompt like and then start nuke as a subprocess then I can type in commands to nuke, but I'd like to be able to put this all in a script so that the master Python program can start nuke and then write to its standard input (and thus into its built-in version of Python) and tell it to do snazzy things, so I wrote a script that starts nuke like this: Then nothing happens because nuke is waiting for user input. How would I now write to standard input?I'm doing this because I'm running a plugin with nuke that causes it to crash intermittently when rendering multiple frames. So I'd like this script to be able to start nuke, tell it to do something and then if it crashes, try again. So if there is a way to catch a crash and still be OK then that'd be great. <code>  subprocess.call([""C:/Program Files/Nuke6.3v5/Nuke6.3"", ""-t"", ""E:/NukeTest/test.nk""])",How do I write to a Python subprocess' stdin?
Is Python code with classes slower?," When I started learning Python, I created a few applications just using functions and procedural code. However, now I know classes and realized that the code can be much readable (and subjectively easier to understand) if I rewrite it with classes.How much slower the equivalent classes may get compared to the functions in general? Will the initializer, methods in the classes make any considerable difference in speed? <code> ",How much slower python classes are compared to their equivalent functions?
multiprocessing.pool.map and function with two arugments," I am using multiprocessing.Pool()here is what i want to Pool: I want to pass 2 arguments What i want to do is to initialize only 4 DB connections (here will try to create connection on every function call so possibly millions of them and cause IO Freezed to death) . if i can create 4 db connections and 1 for each processes it will be ok.Is there any solution for Pool ? or should i abandon it ?EDIT:From help of both of you i got this by doing this: So here it how it gonna work , i gonna move DB connection code out to the main level and do this: Yeah , i going to test it out and let you guys know. <code>  def insert_and_process(file_to_process,db): db = DAL(""path_to_mysql"" + db) #Table Definations db.table.insert(**parse_file(file_to_process)) return Trueif __name__==""__main__"": file_list=os.listdir(""."") P = Pool(processes=4) P.map(insert_and_process,file_list,db) # here having problem. args=zip(f,cycle(dbs))Out[-]: [('f1', 'db1'), ('f2', 'db2'), ('f3', 'db3'), ('f4', 'db4'), ('f5', 'db1'), ('f6', 'db2'), ('f7', 'db3'), ('f8', 'db4'), ('f9', 'db1'), ('f10', 'db2'), ('f11', 'db3'), ('f12', 'db4')] def process_and_insert(args): #Table Definations args[1].table.insert(**parse_file(args[0])) return Trueif __name__==""__main__"": file_list=os.listdir(""."") P = Pool(processes=4) dbs = [DAL(""path_to_mysql/database"") for i in range(0,3)] args=zip(file_list,cycle(dbs)) P.map(insert_and_process,args) # here having problem.",multiprocessing.pool.map and function with two arguments
Python - Is there a way to access __dict__ (or something like it) that includes base classes?," Suppose we have the following class hierarchy: If I explore __dict__ on ClassB like so, I only see the bar attribute: Output is barI can roll my own means to get attributes on not only the specified type but its ancestors. However, my question is whether there's already a way in python for me to do this without re-inventing a wheel. Running my code as follows... ... gives back both bar and foo.Note that I'm simplifying some things: name collisions, using items() when for this example I could use dict, skipping over anything that starts with __, ignoring the possibility that two ancestors themselves have a common ancestor, etc.EDIT1 - I tried to keep the example simple. But I really want both the attribute name and the attribute reference for each class and ancestor class. One of the answers below has me on a better track, I'll post some better code when I get it to work.EDIT2 - This does what I want and is very succinct. It's based on Eli's answer below. It gives back both the attribute names and their references.EDIT3 - One of the answers below suggested using inspect.getmembers. This appears very useful because it's like dict only it operates on ancestor classes as well.Since a large part of what I was trying to do was find attributes marked with a particular descriptor, and include ancestors classes, here is some code that would help do that in case it helps anyone: <code>  class ClassA: @property def foo(self): return ""hello""class ClassB(ClassA): @property def bar(self): return ""world"" for name,_ in ClassB.__dict__.items(): if name.startswith(""__""): continue print(name) def return_attributes_including_inherited(type): results = [] return_attributes_including_inherited_helper(type,results) return resultsdef return_attributes_including_inherited_helper(type,attributes): for name,attribute_as_object in type.__dict__.items(): if name.startswith(""__""): continue attributes.append(name) for base_type in type.__bases__: return_attributes_including_inherited_helper(base_type,attributes) for attribute_name in return_attributes_including_inherited(ClassB): print(attribute_name) def get_attributes(type): attributes = set(type.__dict__.items()) for type in type.__mro__: attributes.update(type.__dict__.items()) return attributes class MyCustomDescriptor: # This is greatly oversimplified def __init__(self,foo,bar): self._foo = foo self._bar = bar pass def __call__(self,decorated_function): return self def __get__(self,instance,type): if not instance: return self return 10class ClassA: @property def foo(self): return ""hello"" @MyCustomDescriptor(foo=""a"",bar=""b"") def bar(self): pass @MyCustomDescriptor(foo=""c"",bar=""d"") def baz(self): passclass ClassB(ClassA): @property def something_we_dont_care_about(self): return ""world"" @MyCustomDescriptor(foo=""e"",bar=""f"") def blah(self): pass# This will get attributes on the specified type (class) that are of matching_attribute_type. It just returns the attributes themselves, not their names.def get_attributes_of_matching_type(type,matching_attribute_type): return_value = [] for member in inspect.getmembers(type): member_name = member[0] member_instance = member[1] if isinstance(member_instance,matching_attribute_type): return_value.append(member_instance) return return_value# This will return a dictionary of name & instance of attributes on type that are of matching_attribute_type (useful when you're looking for attributes marked with a particular descriptor)def get_attribute_name_and_instance_of_matching_type(type,matching_attribute_type): return_value = {} for member in inspect.getmembers(ClassB): member_name = member[0] member_instance = member[1] if isinstance(member_instance,matching_attribute_type): return_value[member_name] = member_instance return return_value",Is there a way to access __dict__ (or something like it) that includes base classes?
Generate an MP3 file with a 15Khz tone, I'm playing around with high-pitched sounds. I'd like to generate an MP3 file with a 1 second 15Khz burst. Is there a simple way to do this from C or Python? I don't want to use MATLAB. <code> ,Generate a Sound file with a 15Khz tone
is there a quiet version of subprocess.call?," Is there a variant of subprocess.call that can run the command without printing to standard out, or a way to block out it's standard out messages?  <code> ",Is there a quiet version of subprocess.call?
Python imports being overridden by the standard library," I have a python package that I'm writing and I'm having an issue where the standard library is being imported instead of my files because of name clashes.For example, a file structure like below:package/__init__.py package/module.py package/signal.py I get the following results when I run this: I would like to get: The actual question:So, when I run module.py, it's import signal goes to the stdlib version. How am I able to force module.py to import signal.py instead?As noted in the tags, this needs to be able to run on python-2.4.3. While that is an old version, it's what is included in RHEL 5.Some additional informationJust for a bit more information, I explicitly have the following setup: Please note that when I ran ./package/module.py that the print statement in ./package/signal.py was not fired. This implies that the signal that was loaded is the one from the stdlib. <code>  # No data in this file #!/usr/bin/env pythonprint 'Loading module.py'import signal #!/usr/bin/env pythonprint 'Loading signal.py' $ ./module.pyLoading module.py $ ./module.pyLoading module.pyLoading signal.py [10:30pm][~/test] tree ..|-- package| |-- __init__.py| |-- module.py| `-- signal.py`-- script[10:30pm][~/test] cat script#!/usr/bin/env pythonfrom package import signal[10:30pm][~/test] cat package/__init__.py[10:30pm][~/test] cat package/module.py #!/usr/bin/env pythonprint ""Loading module.py""import signal[10:30pm][~/test] cat package/signal.py #!/usr/bin/env pythonprint ""Loading signal.py""[10:30pm][~/test] python ./scriptLoading signal.py[10:32pm][~/test] python ./package/module.py Loading module.py[10:32pm][~/test] python -m package.modulepython: module package.module not found",Python imports being overridden by the standard library (Python 2.4)
Compile fortran module with f2py," I have a Fortran module which I am trying to compile with f2py (listed below). When I remove the module declaration and leave the subroutine in the file by itself, everything works fine. However, if the module is declared as shown below, I get the following results: What is different about compiling a module or subroutine in f2py? Have I left something important out in the module that causes f2py to have trouble? Note that the module compiles fine when I use gfortran alone.Software: Windows 7; gcc, gfortran 4.6.1 (MinGW); python 3.2.2; f2py v2itimes-s2.f: <code>  > f2py.py -c -m its --compiler=mingw itimes-s2.f...Reading fortran codes... Reading file 'itimes-s2.f' (format:fix,strict)crackline: groupcounter=1 groupname={0: '', 1: 'module', 2: 'interface', 3: 'subroutine'}crackline: Mismatch of blocks encountered. Trying to fix it by assuming ""end"" statement....c:\users\astay13\appdata\local\temp\tmpgh5ag8\Release\users\astay13\appdata\local\temp\tmpgh5ag8\src.win32-3.2\itsmodule.o:itsmodule.c:(.data+0xec): undefined reference to `itimes_'collect2: ld returned 1 exit status module its contains subroutine itimes(infile,outfile) implicit none ! Constants integer, parameter :: dp = selected_real_kind(15) ! Subroutine Inputs character(*), intent(in) :: infile character(*), intent(in) :: outfile ! Internal variables real(dp) :: num integer :: inu integer :: outu integer :: ios inu = 11 outu = 22 open(inu,file=infile,action='read') open(outu,file=outfile,action='write',access='append') do read(inu,*,IOSTAT=ios) num if (ios < 0) exit write(outu,*) num**2 end do end subroutine itimes end module its",Compile Fortran module with f2py
employing dynamic data for graphs," I am aiming to build a site that will contain a lot of user generated data, hopefully. I'm in my first year of self learning programming: Python, Django, MySQL, HTML and Javascript.I can chart dummy data on a table just fine, but I'm now looking at turning that data into nice colorful looking graphs.I am in my first day of investigation into finding out how to do this. But before I continue, I would like to ask a few questions.There seems to be many JavaScript frameworks for building charts, such as Google charts and jquery charts, and some object orientated programs for building charts, such as Cairo Plot and matplotlib.The Javascript frameworks seem initially like a nice easy way to do it. However, whereas with tables, where you can enter variable data tags in the body of an HTML page, and have Javascript make it look pretty, the data of a graph goes in the scripting area, where the variable data tags don't quite seem to work the same way. I'm using Django, so a variable tag looks like: Q1. Should this work and am I just doing it wrong, or am I right in thinking variable tags can't go in the scripting area?Q2. Can you have Javascript frameworks produce graphs from data outside the <script> area?Q3. I've read that Javascript frameworks are getting more powerful, but because I'll be potentially using large amounts of dynamic data, should I be concentrating on using OO style graph programs like Cairo Plot and matplotlib, which to me don't seem to have the same levels of support?Just looking for a nudge in the right direction. <code>  {{ uniquenum }} ",Employing dynamic data for graphs
How to only keep nodes in networkx-graph which 2+ outgoing edges or 0 outgoing edges?, I have Directed Graph in networkx. I want to only keep those nodes which have two or more than two outgoing edges or no outgoing edge at all. How do I do this?or How do I removes nodes which have exactly one outgoing edge in a networkx graph. <code> ,How to only keep nodes in networkx-graph with 2+ outgoing edges or 0 outgoing edges?
What would you use a heapq Python module for in real life?," After reading Guido's Sorting a million 32-bit integers in 2MB of RAM using Python, I discovered the heapq module, but the concept is pretty abstract to me.One reason is that I don't understand the concept of a heap completely, but I do understand how Guido used it.Now, beside his kinda crazy example, what would you use the heapq module for?Must it always be related to sorting or minimum value? Is it only something you use because it's faster than other approaches? Or can you do really elegant things that you can't do without? <code> ",What would you use the heapq Python module for in real life?
Qt - Get the pixel length of a string in a QLabel," I have a QLabel of a fixed width.I need to check (periodically) that the entire string fits inside the QLabel at its current width, so I can resize it appropriately.To do this, I need to obtain the 'pixel length' of the string.(The total amount of horizontal pixels required to display the string).It should be noted that the point size of the QLabel never changes.I can not simply check the amount of characters present, since some characters are subscript / superscript and contribute differently to the width of the entire string.(This is to say there is no simple relationship between pixel width and the amount of characters)Is there any abstracted, super conveniant function for this? Specs:Python 2.7.1PyQt4Windows 7 <code> ",Qt - How to get the pixel length of a string in a QLabel?
Python several variables in an if statement," I am using Python and I would like to have an if statement with many variables in it.Such as: I realize that this is not the correct syntax and that is exactly the question I am asking what is the correct Python syntax for this type of an if statement? <code>  if A, B, C, and D >= 2: print (A, B, C, and D)",How to compare multiple variables to the same value?
"Compare multiple variables to the same value in ""if"" in Python?"," I am using Python and I would like to have an if statement with many variables in it.Such as: I realize that this is not the correct syntax and that is exactly the question I am asking what is the correct Python syntax for this type of an if statement? <code>  if A, B, C, and D >= 2: print (A, B, C, and D)",How to compare multiple variables to the same value?
"Compare multiple variables to the same value in ""if""?"," I am using Python and I would like to have an if statement with many variables in it.Such as: I realize that this is not the correct syntax and that is exactly the question I am asking what is the correct Python syntax for this type of an if statement? <code>  if A, B, C, and D >= 2: print (A, B, C, and D)",How to compare multiple variables to the same value?
Boost python overloads," How do I expose the following class using Boost.Python? I tried something like this: But, it raises an exception in Python (SystemError: initialization of libdistributions raised unreported exception). If I remove one of the overloads from the bp::class_, then everything works fine. I know that Boost.Python can automatically deal with overloaded constructors, so I was wondering if there was something like that for static methods.Answer <code>  class C { public: static void F(int) {} static void F(double) {}}; bp::class_<C>(""C"") .def(""F"", (void (C::*)(int))&C::F).staticmethod(""F"") .def(""F"", (void (C::*)(double))&C::F).staticmethod(""F""); bp::class_<C>(""C"") .def(""F"", (void (C::*)(int))&C::F) // Note missing staticmethod call! .def(""F"", (void (C::*)(double))&C::F).staticmethod(""F"");",Boost.Python static method overloads
How to handle urllib's timeout in Python 3," First off, my problem is quite similar to this one. I would like a timeout of urllib.urlopen() to generate an exception that I can handle.Doesn't this fall under URLError? The error message: resp = urllib.request.urlopen(req, timeout=10).read().decode('utf-8') File ""/usr/lib/python3.2/urllib/request.py"", line 138, in urlopen return opener.open(url, data, timeout) File ""/usr/lib/python3.2/urllib/request.py"", line 369, in open response = self._open(req, data) File ""/usr/lib/python3.2/urllib/request.py"", line 387, in _open '_open', req) File ""/usr/lib/python3.2/urllib/request.py"", line 347, in _call_chain result = func(*args) File ""/usr/lib/python3.2/urllib/request.py"", line 1156, in http_open return self.do_open(http.client.HTTPConnection, req) File ""/usr/lib/python3.2/urllib/request.py"", line 1141, in do_open r = h.getresponse() File ""/usr/lib/python3.2/http/client.py"", line 1046, in getresponse response.begin() File ""/usr/lib/python3.2/http/client.py"", line 346, in begin version, status, reason = self._read_status() File ""/usr/lib/python3.2/http/client.py"", line 308, in _read_status line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"") File ""/usr/lib/python3.2/socket.py"", line 276, in readinto return self._sock.recv_into(b) socket.timeout: timed out There was a major change from in Python 3 when they re-organised the urllib and urllib2 modules into urllib. Is it possible that there was a change then that causes this? <code>  try: response = urllib.request.urlopen(url, timeout=10).read().decode('utf-8')except (HTTPError, URLError) as error: logging.error( 'Data of %s not retrieved because %s\nURL: %s', name, error, url)else: logging.info('Access successful.')",How to handle urllib's timeout in Python 3?
"Python - How to check if a key modifier is pressed (shift, ctrl, alt)?", I am building a UI with Qt Creator and I want buttons to perform different actions with different modifiers. So I thought I could call functions with dynamic string properties that would perform the action depending on the modifier.Is there a simpler way to do this? <code> ,"How can I check if a keyboard modifier is pressed (Shift, Ctrl, or Alt)?"
"How to check if a key modifier is pressed (shift, ctrl, alt)?", I am building a UI with Qt Creator and I want buttons to perform different actions with different modifiers. So I thought I could call functions with dynamic string properties that would perform the action depending on the modifier.Is there a simpler way to do this? <code> ,"How can I check if a keyboard modifier is pressed (Shift, Ctrl, or Alt)?"
"How to check if a keyboard modifier is pressed (shift, ctrl, alt)?", I am building a UI with Qt Creator and I want buttons to perform different actions with different modifiers. So I thought I could call functions with dynamic string properties that would perform the action depending on the modifier.Is there a simpler way to do this? <code> ,"How can I check if a keyboard modifier is pressed (Shift, Ctrl, or Alt)?"
MySQL: Order BY a function of two columns," I have two integer fields A and B in table T .I want to do something like ""SELECT * FROM T ORDER BY f(A,B) DESC""where f(A,B) is a linear combination of A and B ... i.e f(A,B) = mA + nB, where m and n are numbers.What is the right syntax? <code> ",MySQL: Order by a function of two columns
Performance of len(set) vs. set.__len__() in Python 3," While profiling my Python's application, I've discovered that len() seems to be a very expensive one when using sets. See the below code: According to profiler's stats below, lenA() seems to be 14 times slower than lenB(): Am I missing something? Currently I use __len__() instead of len(), but the code looks dirty :( <code>  import cProfiledef lenA(s): for i in range(1000000): len(s);def lenB(s): for i in range(1000000): s.__len__();def main(): s = set(); lenA(s); lenB(s);if __name__ == ""__main__"": cProfile.run(""main()"",""stats""); ncalls tottime percall cumtime percall filename:lineno(function) 1 1.986 1.986 3.830 3.830 .../lentest.py:5(lenA)1000000 1.845 0.000 1.845 0.000 {built-in method len} 1 0.273 0.273 0.273 0.273 .../lentest.py:9(lenB)",Profiled performance of len(set) vs. set.__len__() in Python 3
"Does it make sense to use Hungarian notation prefixes in weakly-typed, interpreted languages?"," First of all, I have taken a look at the following posts to avoid duplicate question.https://stackoverflow.com/questions/1184717/hungarian-notationWhy shouldn't I use ""Hungarian Notation""?Are variable prefixes (Hungarian notation) really necessary anymore?Do people use the Hungarian Naming Conventions in the real world?Now, all of these posts are related to C#, C++, Java - strongly typed languages.I do understand that there is no need for the prefixes when the type is known before compilation.Nevertheless, my question is: Is it worthwhile to use the prefixes in interpreter based languages, considering the fact that you cant see the type of the object before runtime?Edit: If someone can make this post a community wiki, please do. I am hardly interested in the reputation (or negative reputation) from this post. <code> ",Does it make sense to use Hungarian notation prefixes in interpreted languages?
How to join links in Python to get a cycle," I have a list of links and want to know the joined path/cycle.My links look like this: And I want the answer to be a cycle like that (or any other matching cycle): So you take the first element of the first sublist, then you take the second element and you look for the next sublist starting with this element, and you start all over again.Is there an elegant way to accomplish this? I tried the reduce function but then the links have to be sorted in a way that the links match. <code>  [[0, 3], [1, 0], [3, 1]] [0,3,1]",How to join links in Python to get a cycle?
pip freeze > requirements.txt error, I am getting the following error with that command: This is my requirements.txt file beforehand: <code>  $pip freeze > requirements.txt Warning: cannot find svn location for distribute==0.6.16dev-r0 Django==1.3django-registration==0.7,Warning: cannot find svn location for distribute==0.6.16dev-r0
loop through kwargs in python," In the code below, I want to read obj.subject and place it into var subject, also read obj.body and place it into body. First I want to read the kwargs variables and search for keywords within the string to replace, if none exists then move on.How can I iterate through kwargs in Python? <code>  for key in kwargs: subject = str(obj.subject).replace('[%s]' % upper(key), kwargs[key])for key in kwargs: body = str(obj.body).replace('[%s]' % upper(key), kwargs[key])return (subject, body, obj.is_html)",How do I loop through **kwargs in Python?
local var referenced before assignment," For the following Python 2.7 code: I get the following error: But when I comment out the line c += 3 in funcB, I get the following output: Isn't c being accessed in both cases of += in funcB and = in funcC? Why doesn't it throw error for one but not for the other?I don't have a choice of making c a global variable and then declaring global c in funcB. Anyway, the point is not to get c incremented in funcB but why it's throwing error for funcB and not for funcC while both are accessing a variable that's either local or global. <code>  #!/usr/bin/pythondef funcA(): print ""funcA"" c = 0 def funcB(): c += 3 print ""funcB"", c def funcC(): print ""funcC"", c print ""c"", c funcB() c += 2 funcC() c += 2 funcB() c += 2 funcC() print ""end""funcA() File ""./a.py"", line 9, in funcB c += 3UnboundLocalError: local variable 'c' referenced before assignment funcAc 0funcB 0funcC 2funcB 4funcC 6end","Assigning to variable from parent function: ""Local variable referenced before assignment"""
Error message: Local variable referenced before assignment," For the following Python 2.7 code: I get the following error: But when I comment out the line c += 3 in funcB, I get the following output: Isn't c being accessed in both cases of += in funcB and = in funcC? Why doesn't it throw error for one but not for the other?I don't have a choice of making c a global variable and then declaring global c in funcB. Anyway, the point is not to get c incremented in funcB but why it's throwing error for funcB and not for funcC while both are accessing a variable that's either local or global. <code>  #!/usr/bin/pythondef funcA(): print ""funcA"" c = 0 def funcB(): c += 3 print ""funcB"", c def funcC(): print ""funcC"", c print ""c"", c funcB() c += 2 funcC() c += 2 funcB() c += 2 funcC() print ""end""funcA() File ""./a.py"", line 9, in funcB c += 3UnboundLocalError: local variable 'c' referenced before assignment funcAc 0funcB 0funcC 2funcB 4funcC 6end","Assigning to variable from parent function: ""Local variable referenced before assignment"""
Translating utf8 national letters to standard ascii letters (like  to n and  to a)," I'm looking for a fast and possibly convenient way in Python 3 to translate strings with non-ascii letters to words with only ascii letters.Examples!w => zolwmdek => mozdzekd => lodzand so on...There are many letters in national alphabets that can be turned into ASCII letters (like to n). I can do it manually for my language (Polish), by specifying how to translate each letter. But is there any automated way to do that? Or some library which would do what I need?Pythons str.encode() won't do, because ""w"".encode('ascii', 'replace') == ""???w"" and ""w"".encode('ascii', 'ignore') == ""w""...I can do such translation for polish letters but I don't want to do it for every other language: The above code does what I want with polish letters, but it's ugly :< What is the best way to do this?Of course such translations will change the meanings of some words, but thats ok. <code>  >>> utf8_letters = ['','','','','','','','','']>>> ascii_letters = ['a','e','c','z','z','o','l','n','s']>>> trans_dict = dict(zip(utf8_letters,ascii_letters))>>> turtle = ""w"">>> out = []>>> for l in turtle:... out.append(trans_dict[l] if l in trans_dict else l)>>> result = ''.join(out)>>> result'zolw'",Translating letters not in 7bit ASCII to ASCII (like  to n and  to a)
How can I access amazon's dynamodb via python?," I'm currently using hbase with my Python apps and wanted to try out Amazon DynamoDB. Is there a way to use Python to read, write and query data? <code> ",How can I access Amazon DynamoDB via Python?
Python: Attribute Error - 'NoneType' object has no attribute 'lifetime'," I keep getting an error that says The code I have is too long to post here. What general scenarios would cause this AttributeError, what is NoneType supposed to mean and how can I narrow down what's going on? <code>  AttributeError: 'NoneType' object has no attribute 'something'",Why do I get AttributeError: 'NoneType' object has no attribute 'something'?
Python: Attribute Error - 'NoneType' object has no attribute 'something'," I keep getting an error that says The code I have is too long to post here. What general scenarios would cause this AttributeError, what is NoneType supposed to mean and how can I narrow down what's going on? <code>  AttributeError: 'NoneType' object has no attribute 'something'",Why do I get AttributeError: 'NoneType' object has no attribute 'something'?
Don't understand what this AttributeError means," I keep getting an error that says The code I have is too long to post here. What general scenarios would cause this AttributeError, what is NoneType supposed to mean and how can I narrow down what's going on? <code>  AttributeError: 'NoneType' object has no attribute 'something'",Why do I get AttributeError: 'NoneType' object has no attribute 'something'?
Shorthand Python for and if statements, Hello I am trying to write these python lines in a single line but getting some errors due to the dictionary modifications the code is doing. the general syntax I believe is Would it be possible for someone to tell me how this might work considering that I am adding 1 to the value in a dictionaryThanks <code>  for i in range(len(string)): if string[i] in dict: dict[string[i]] += 1 abc = [i for i in len(x) if x[i] in array],Best way to count char occurences in a string
Python iterator into chunks," Can you think of a nice way (maybe with itertools) to split an iterator into chunks of given size?Therefore l=[1,2,3,4,5,6,7] with chunks(l,3) becomes an iterator [1,2,3], [4,5,6], [7]I can think of a small program to do that but not a nice way with maybe itertools. <code> ",Iterate an iterator by chunks (of n) in Python?
python dictionary sort bt key," What would be a nice way to go from {2:3, 1:89, 4:5, 3:0} to {1:89, 2:3, 3:0, 4:5}?I checked some posts but they all use the ""sorted"" operator that returns tuples.  <code> ",How can I sort a dictionary by key?
python dictionary sort by key," What would be a nice way to go from {2:3, 1:89, 4:5, 3:0} to {1:89, 2:3, 3:0, 4:5}?I checked some posts but they all use the ""sorted"" operator that returns tuples.  <code> ",How can I sort a dictionary by key?
How can I sort a Python dictionary sort by key?," What would be a nice way to go from {2:3, 1:89, 4:5, 3:0} to {1:89, 2:3, 3:0, 4:5}?I checked some posts but they all use the ""sorted"" operator that returns tuples.  <code> ",How can I sort a dictionary by key?
exchange two int sequences elements such that the difference of the sum of seq1 and sum of seq 2 is minimized," An interview question: Given two non-ordered integer sequences a and b, their size is n, all numbers are randomly chosen: Exchange the elements of a and b, such that the sum of the elements of a minus the sum of the elements of b is minimal.Given the example: The result is (1 + 2 + 3) - (4 + 5 + 9) = -12.My algorithm: Sort them together and then put the first smallest n ints in a and left in b. It is O(n lg n) in time and O(n) in space. I do not know how to improve it to an algorithm with O(n) in time and O(1) in space. O(1) means that we do not need more extra space except seq 1 and 2 themselves. Any ideas ? An alternative question would be: What if we need to minimize the absolute value of the differences (minimize |sum(a) - sum(b)|)?A python or C++ thinking is preferred. <code>  a = [ 5 1 3 ]b = [ 2 4 9 ]","Swap the elements of two sequences, such that the difference of the element-sums gets minimal."
How do I call a function twice or more times consecutively in python?, Is there a short way to call a function twice or more consecutively in Python? For example: maybe like: <code>  do()do()do() 3*do(),How do I call a function twice or more times consecutively?
Tkinter gui : window doesn't appear," This is the file I launch with python : The program is in execution, but I don't see any window appearing.I have to close the terminal window to stop, I don't get why. <code>  from Tkinter import *# Esempio di GUIdef main(): w1 = Tk() w1.title(""Finestra 1"") f1 = Frame(w1) f1.pack() w1.mainloop()main()",Window doesn't appear using Tkinter
3d plots using matplotlib," I have to plot data which is in the following format : x = range(6)y = range(11) and z depends on x, yFor each value of x, there should be a continuous curve that shows the variation of z w.r.t y and the curves for different values of x must be disconnected I am using mplot3d and it is not very clear how to plot disconnected curves.This is what it looks like using bar plots. <code> ",3D plots using maplot3d from matplotlib-
python: reusing variables," In Python, I often reuse variables in manner analogous to this: I like this technique because it helps me cut on the number of variables I need to track.Never had any problems but I am wondering if I am missing potential downsides e.g. performance etc. <code>  files = files[:batch_size]",Reusing variables in Python
(Python) Use a library locally instead of installing it," I've written a script in python that occasionally sends tweets to TwitterIt only uses one library called tweepy. After installing the library it works.Problem:I would like to host the script on a server where I do not have privileges to install anythingIt would be great if I can just include it locally from the folder I've got it in.As of right now, all I need to include at the top of my file is: the tweepy folder (DOES have a __init__.py file which I believe is important.Question:How can I use this library without installing it?basically I want to replace: import tweepy with import local_folder/tweepy/* <code>  import tweepy",Use a library locally instead of installing it
beautiful soup extract element with no class attribute," I need to navigate to an html element of a particular type.However, there are many such elements of that type on the page, with many different classes.I need one which does not have any class attribute.Should I look for one with class == '', or is there some other way? <code> ",Extract element with no class attribute
Interpereting Strings as Other Data Types in Python," I'm reading a file into python 2.4 that's structured like this: The idea is to parse it into a dictionary that takes fieldfoo as the key and whatever comes after the colon as the value.I want to convert whatever is after the colon to it's ""actual"" data type, that is, '7' should be converted to an int, ""Hello, world!"" to a string, etc. The only data types that need to be parsed are ints, floats and strings. Is there a function in the python standard library that would allow one to make this conversion easily?The only things this should be used to parse were written by me, so (at least in this case) safety is not an issue. <code>  field1: 7field2: ""Hello, world!""field3: 6.2",Interpreting Strings as Other Data Types in Python
Django cooking receipts site," I'm working on Django website that should give a possibility to select cooking recipes containing the ingredients provided by user. So in brief, the idea of the site is ""things you could make from the food in your refrigerator"".So I made 2 models Let's imagine, that I have as list ['egg','bread','meat','onion'].Now I need select all Recipes that could be made from that list on ingredients. The problem is, that some recipes may have only some of ingredients from the list.For example:Egg Toast = egg + breadMeat Egg Toast = meat + egg + breadMeat with Onion = meat + onionand etc...So my question is: could it be possible to select all recipes that could be made from the list of the ingredients AND select the closest recipes that could be made from the list of ingredients + some ingredients from the shop?For example: recipes has 3 elements from 4, so we add it to the result. <code>  class Recipe (models.Model): name = models.CharField(max_length=255) ingredients = models.ManyToManyField(Ingredient)class Ingredient (models.Model): name = models.CharField(max_length=255)",Django cooking recipes site model structure
Django cooking recipes site," I'm working on Django website that should give a possibility to select cooking recipes containing the ingredients provided by user. So in brief, the idea of the site is ""things you could make from the food in your refrigerator"".So I made 2 models Let's imagine, that I have as list ['egg','bread','meat','onion'].Now I need select all Recipes that could be made from that list on ingredients. The problem is, that some recipes may have only some of ingredients from the list.For example:Egg Toast = egg + breadMeat Egg Toast = meat + egg + breadMeat with Onion = meat + onionand etc...So my question is: could it be possible to select all recipes that could be made from the list of the ingredients AND select the closest recipes that could be made from the list of ingredients + some ingredients from the shop?For example: recipes has 3 elements from 4, so we add it to the result. <code>  class Recipe (models.Model): name = models.CharField(max_length=255) ingredients = models.ManyToManyField(Ingredient)class Ingredient (models.Model): name = models.CharField(max_length=255)",Django cooking recipes site model structure
Django cooking recipes site model structure," I'm working on Django website that should give a possibility to select cooking recipes containing the ingredients provided by user. So in brief, the idea of the site is ""things you could make from the food in your refrigerator"".So I made 2 models Let's imagine, that I have as list ['egg','bread','meat','onion'].Now I need select all Recipes that could be made from that list on ingredients. The problem is, that some recipes may have only some of ingredients from the list.For example:Egg Toast = egg + breadMeat Egg Toast = meat + egg + breadMeat with Onion = meat + onionand etc...So my question is: could it be possible to select all recipes that could be made from the list of the ingredients AND select the closest recipes that could be made from the list of ingredients + some ingredients from the shop?For example: recipes has 3 elements from 4, so we add it to the result. <code>  class Recipe (models.Model): name = models.CharField(max_length=255) ingredients = models.ManyToManyField(Ingredient)class Ingredient (models.Model): name = models.CharField(max_length=255)",Django cooking recipes site model structure
Python: wrapper to write to multiple streams," In python, is there an easy way to set up a file-like object for writing that is actually backed by multiple output streams? For instance, I want something like this: So what I'm looking for is OStreamWrapper. I know it'd be pretty easy to write my own, but if there's an existing one, I'd rather use that and not have to worry about finding and covering edge cases. <code>  file1 = open(""file1.txt"", ""w"")file2 = open(""file2.txt"", ""w"")ostream = OStreamWrapper(file1, file2, sys.stdout)#Write to both files and stdout at once:ostream.write(""ECHO!"")",Wrapper to write to multiple streams
Flipping a picture across the vertical axis in python, I am trying to flip a picture across its vertical axis in python but have no idea how to start. any suggestion would be appreciated. Thanks. <code> ,How can I flip an image along the vertical axis with python?
Numpy: Can you tell if an array is a view of another?," Do numpy arrays keep track of their ""view status""? What I am looking for is numpy.isview() or something.I want this for code profiling to be sure that I am doing things correctly and getting views when I think I am. <code>  import numpya = numpy.arange(100)b = a[0:10]b[0] = 100print a[0]# 100 comes out as it is a viewb is a[0:10]# False (hmm how to ask?)",Can you tell if an array is a view of another?
Combining predicates in Python," I have some predicates, e.g.: and want to logically combine them as in: The question is now: Can such combination be written in a pointfree style, such as: This has of course not the desired effect because the truth value of lambda functions is True and and and or are short-circuiting operators. The closest thing I came up with was to define a class P which is a simple predicate container that implements __call__() and has the methods and_() and or_() to combine predicates. The definition of P is as follows: With P I can now create a new predicate that is a combination of predicates like this: which is equivalent to the above lambda function. This comes closer to what I'd like to have, but it is also not pointfree (the points are now the predicates itself instead of their arguments). Now the second question is: Is there a better or shorter way (maybe without parentheses and dots) to combine predicates in Python than using classes like P and without using (lambda) functions? <code>  is_divisible_by_13 = lambda i: i % 13 == 0is_palindrome = lambda x: str(x) == str(x)[::-1] filter(lambda x: is_divisible_by_13(x) and is_palindrome(x), range(1000,10000)) filter(is_divisible_by_13 and is_palindrome, range(1000,10000)) import copyclass P(object): def __init__(self, predicate): self.pred = predicate def __call__(self, obj): return self.pred(obj) def __copy_pred(self): return copy.copy(self.pred) def and_(self, predicate): pred = self.__copy_pred() self.pred = lambda x: pred(x) and predicate(x) return self def or_(self, predicate): pred = self.__copy_pred() self.pred = lambda x: pred(x) or predicate(x) return self P(is_divisible_by_13).and_(is_palindrome)",Pointfree function combination in Python
"File iteration, checking line existance"," I am reading through a file using a for loop like this... except for each line I read i need to take an item from the line ahead of it and put it in the current line. What is the best way to do this? Is there a way to read the next line or get some item from it without reading it? <code>  f = open(""somefile.txt"")for line in f: do stuff","File iteration, checking line existence"
Trying to parse json in python. ValueError: Expecting property name," I am trying to parse a JSON object into a Python dict. I've never done this before. When I googled this particular error, (What is wrong with the first char?), other posts have said that the string being loaded is not actually a JSON string. I'm pretty sure this is, though.In this case, eval() works fine, but I'm wondering if there is a more appropriate way?Note: This string comes directly from Twitter, via ptt tools. <code>  >>> import json>>> line = '{u\'follow_request_sent\': False, u\'profile_use_background_image\': True, u\'default_profile_image\': False, u\'verified\': False, u\'profile_sidebar_fill_color\': u\'DDEEF6\', u\'profile_text_color\': u\'333333\', u\'listed_count\': 0}'>>> json.loads(line)Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/__init__.py"", line 326, in loads return _default_decoder.decode(s) File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/decoder.py"", line 366, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/decoder.py"", line 382, in raw_decode obj, end = self.scan_once(s, idx) ValueError: Expecting property name: line 1 column 1 (char 1)",Trying to parse JSON in Python. ValueError: Expecting property name
Thoroughput differences when using coroutines vs threading," A few days ago I has asked a question on SO about helping me design a paradigm for structuring multiple HTTP requestsHere's the scenario. I would like a have a multi-producer, multi-consumer system. My producers crawl and scrape a few sites and add the links that it finds into a queue. Since I'll be crawling multiple sites, I would like to have multiple producers/crawlers.The consumers/workers feed off this queue, make TCP/UDP requests to these links and saves the results to my Django DB. I would also like to have multiple-workers as each queue item is totally independent of each other.People suggested that use a coroutine library for this i.e. Gevent or Eventlet. Having never worked with coroutines, I read that even though the programming paradigm is similar to threaded paradigms, only one thread is actively executing but when blocking calls occur - such as I/O calls - the stacks are switched in-memory and the other green thread takes over until it encounters some sort of a blocking I/O call. Hopefully I got this right? Here's the code from one of my SO posts: This works well because the sleep calls are blocking calls and when a sleep event occurs, another green thread takes over. This is a lot faster than sequential execution.As you can see, I don't have any code in my program that purposely yields the execution of one thread to another thread. I fail to see how this fits into scenario above as I would like to have all the threads executing simultaneously.All works fine, but I feel the throughput that I've achieved using Gevent/Eventlets is higher than the original sequentially running program but drastically lower than what could be achieved using real-threading. If I were to re-implement my program using threading mechanisms, each of my producers and consumers could simultaneously be working without the need to swap stacks in and out like coroutines.Should this be re-implemented using threading? Is my design wrong? I've failed to see the real benefits of using coroutines.Maybe my concepts are little muddy but this is what I've assimilated. Any help or clarification of my paradigm and concepts would be great.Thanks <code>  import geventfrom gevent.queue import *import timeimport randomq = JoinableQueue()workers = []producers = []def do_work(wid, value): gevent.sleep(random.randint(0,2)) print 'Task', value, 'done', widdef worker(wid): while True: item = q.get() try: print ""Got item %s"" % item do_work(wid, item) finally: print ""No more items"" q.task_done()def producer(): while True: item = random.randint(1, 11) if item == 10: print ""Signal Received"" return else: print ""Added item %s"" % item q.put(item)for i in range(4): workers.append(gevent.spawn(worker, random.randint(1, 100000)))# This doesn't work.for j in range(2): producers.append(gevent.spawn(producer))# Uncommenting this makes this script work.# producer()q.join()",Throughput differences when using coroutines vs threading
using class objects as celery tasks," I'm trying to use the methods of class as the django-celery tasks, marking it up using @task decorator. The same situation is discribed here, asked by Anand Jeyahar. It's something like this The problem is even if i use class instance like this a.foo.delay(bar) it says, that foo needs at least two arguments, which meens that self pointer misses.More information:I can't convert class to module because of inheritanceMethods are strongly depended on class members, so i can't make them staticMarking class as the task with @task decorator makes the class a task itself, and it could be possible to execute the methods from run() method, using some argument as a key for method selection, but it's not exactly what i want.Creating an instance of class and passing it as self argument to methods changes the way i execute the methods not as celery taks, but as usual methods (i.e. while testing)I've tried to find out how i can register the task dinamically, from constructor for example, but celery shares the code between the workers, so that's why it seems to be impossible.Thanks for your help! <code>  class A: @task def foo(self, bar): ...def main(): a = A() ... # what i need a.foo.delay(bar) # executes as celery task a.foo(bar) # executes locally",using class methods as celery tasks
What exception class to use for badly formatted file?," I'm writing a parser for a certain file format. If a file is not correctly formatted (and can not be parsed) then the parser throws an exception.What exception class, in the Python 2 exception hierarchy, should I use? <code> ",What exception class to use for file parsing error?
"Why is this False?: 1 in [1,0] == True"," When I was looking at answers to this question, I found I didn't understand my own answer.I don't really understand how this is being parsed. Why does the second example return False? Thanks for any help. I think I must be missing something really obvious.I think this is subtly different to the linked duplicate:Why does the expression 0 < 0 == 0 return False in Python?.Both questions are to do with human comprehension of the expression. There seemed to be two ways (to my mind) of evaluating the expression. Of course neither were correct, but in my example, the last interpretation is impossible.Looking at 0 < 0 == 0 you could imagine each half being evaluated and making sense as an expression: So the link answers why this evaluates False: But with my example 1 in ([1,0] == True) doesn't make sense as an expression, so instead of there being two (admittedly wrong) possible interpretations, only one seems possible: <code>  >>> 1 in [1,0] # This is expectedTrue>>> 1 in [1,0] == True # This is strangeFalse>>> (1 in [1,0]) == True # This is what I wanted it to beTrue>>> 1 in ([1,0] == True) # But it's not just a precedence issue! # It did not raise an exception on the second example.Traceback (most recent call last): File ""<pyshell#4>"", line 1, in <module> 1 in ([1,0] == True)TypeError: argument of type 'bool' is not iterable >>> (0 < 0) == 0True>>> 0 < (0 == 0)True >>> 0 < 0 == 0False >>> (1 in [1,0]) == True","Why does (1 in [1,0] == True) evaluate to False?"
what's the difference between `groups` and `group` in Python's `re` module," Here it is: Why groups() gives me nothing, but group(0) yields some? What is the difference?Follow UpCode is as follows findall can get me all the -\w+ \w+ substrings, but look at this: Why can't search give me all the substrings?Follow Up AgainIf s = 'abc -j k -l m -k o, and <code>  import re>>>s = 'abc -j k -l m'>>>m = re.search('-\w+ \w+', s)>>>m.groups()()>>> m.group(0)'-j k' >>>re.findall('(-\w+ \w+)', s)['-j k', '-l m', '-n o'] >>>m = re.search('(-\w+ \w+)+', s)>>>m.groups()('-j k',) >>>m = re.search(r'(-\w+ \w+ )+', s)>>>m.groups()('-l m ',) # why just one substring?>>>m.group(0)'-j k -l m ' # as I expected",What's the difference between groups and group in the re module?
How Can an Implementation of a Language in the Same Language Be Faster Than The Language?," If I make a JVM in Java, for example, is it possible to make the implementation I made actually faster than the original implementation I used to build this implementation, even though my implementation is built on top of the original implementation and may even be dependant on that implementation? ( Confusing... )Look at PyPy. It's a JIT Compiler for Python made in Python. That's alright, but how can it claim to be faster than the original implementation of Python which it is using and is dependent on? <code> ",How can an implementation of a language in the same language be faster than the language?
Python ast module traversing object heirarchies," The following is a Python code snippet using the ast and symtablepackages. I am trying to parse the code and check the types. But I don't understand how to traverse objects to get to the actual variable being referenced.The following code implements a NodeVisitor, and a function is presented to the compiler and parsed by the compiler and the ast walked. The function being analyzed (eval_types) is passed a couple objects. Below are the code chunks that make up the example. I have added some comments for each chunk. To run the code, the ""chunks"" need to be reassembled.The imports and a function to un-indent a block of code for parsing. The following is the node visitor, it has the generic unhandled and name visitor overloads. The following are some simple classes that will be used in the function that will be parsed and analyzed with the AST. The following is the test function, the eval_types function is the function that will be analyzed with the AST. The code to execute the example, compile the function and analyze. The following is an example output up to the first name visit. Creating the node visitor doesn't seem to bad but I can't figure outhow to traverse an object hierarchy. In the general case the variable being accessed could be buried deep in a object. How to get to the actual variable being accessed from the ast visitor? I only see that an object is at the node but no additional information what the result variable access is. <code>  import inspectimport astimport symtablefrom tokenize import generate_tokens, untokenize, INDENTfrom cStringIO import StringIO# _dedent borrowed from the myhdl package (www.myhdl.org)def _dedent(s): """"""Dedent python code string."""""" result = [t[:2] for t in generate_tokens(StringIO(s).readline)] # set initial indent to 0 if any if result[0][0] == INDENT: result[0] = (INDENT, '') return untokenize(result) class NodeVisitor(ast.NodeVisitor): def __init__(self, SymbolTable): self.symtable = SymbolTable for child in SymbolTable.get_children(): self.symtable = child print(child.get_symbols()) def _visit_children(self, node): """"""Determine if the node has children and visit"""""" for _, value in ast.iter_fields(node): if isinstance(value, list): for item in value: if isinstance(item, ast.AST): print(' visit item %s' % (type(item).__name__)) self.visit(item) elif isinstance(value, ast.AST): print(' visit value %s' % (type(value).__name__)) self.visit(value) def generic_visit(self, node): print(type(node).__name__) self._visit_children(node) def visit_Name(self, node): print(' variable %s type %s' % (node.id, self.symtable.lookup(node.id))) print(dir(self.symtable.lookup(node.id))) class MyObj(object): def __init__(self): self.val = Noneclass MyObjFloat(object): def __init__(self): self.x = 1.class MyObjInt(object): def __init__(self): self.x = 1class MyObjObj(object): def __init__(self): self.xi = MyObjInt() self.xf = MyObjFloat() def testFunc(x,y,xo,z): def eval_types(): z.val = x + y + xo.xi.x + xo.xf.x return eval_types if __name__ == '__main__': z = MyObj() print(z.val) f = testFunc(1, 2, MyObjObj(), z) f() print(z.val) s = inspect.getsource(f) s = _dedent(s) print(type(s)) print(s) SymbolTable = symtable.symtable(s,'string','exec') tree = ast.parse(s) v = NodeVisitor(SymbolTable) v.visit(tree) Module visit item FunctionDefFunctionDef visit value argumentsarguments visit item AssignAssign visit item AttributeAttribute visit value Name variable z type <symbol 'z'>['_Symbol__flags', '_Symbol__name', '_Symbol__namespaces', '_Symbol__scope', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'get_name', 'get_namespace', 'get_namespaces', 'is_assigned', 'is_declared_global', 'is_free', 'is_global', 'is_imported', 'is_local', 'is_namespace', 'is_parameter', 'is_referenced']",Python ast package: traversing object hierarchies
Python ast package : traversing object heirarchies," The following is a Python code snippet using the ast and symtablepackages. I am trying to parse the code and check the types. But I don't understand how to traverse objects to get to the actual variable being referenced.The following code implements a NodeVisitor, and a function is presented to the compiler and parsed by the compiler and the ast walked. The function being analyzed (eval_types) is passed a couple objects. Below are the code chunks that make up the example. I have added some comments for each chunk. To run the code, the ""chunks"" need to be reassembled.The imports and a function to un-indent a block of code for parsing. The following is the node visitor, it has the generic unhandled and name visitor overloads. The following are some simple classes that will be used in the function that will be parsed and analyzed with the AST. The following is the test function, the eval_types function is the function that will be analyzed with the AST. The code to execute the example, compile the function and analyze. The following is an example output up to the first name visit. Creating the node visitor doesn't seem to bad but I can't figure outhow to traverse an object hierarchy. In the general case the variable being accessed could be buried deep in a object. How to get to the actual variable being accessed from the ast visitor? I only see that an object is at the node but no additional information what the result variable access is. <code>  import inspectimport astimport symtablefrom tokenize import generate_tokens, untokenize, INDENTfrom cStringIO import StringIO# _dedent borrowed from the myhdl package (www.myhdl.org)def _dedent(s): """"""Dedent python code string."""""" result = [t[:2] for t in generate_tokens(StringIO(s).readline)] # set initial indent to 0 if any if result[0][0] == INDENT: result[0] = (INDENT, '') return untokenize(result) class NodeVisitor(ast.NodeVisitor): def __init__(self, SymbolTable): self.symtable = SymbolTable for child in SymbolTable.get_children(): self.symtable = child print(child.get_symbols()) def _visit_children(self, node): """"""Determine if the node has children and visit"""""" for _, value in ast.iter_fields(node): if isinstance(value, list): for item in value: if isinstance(item, ast.AST): print(' visit item %s' % (type(item).__name__)) self.visit(item) elif isinstance(value, ast.AST): print(' visit value %s' % (type(value).__name__)) self.visit(value) def generic_visit(self, node): print(type(node).__name__) self._visit_children(node) def visit_Name(self, node): print(' variable %s type %s' % (node.id, self.symtable.lookup(node.id))) print(dir(self.symtable.lookup(node.id))) class MyObj(object): def __init__(self): self.val = Noneclass MyObjFloat(object): def __init__(self): self.x = 1.class MyObjInt(object): def __init__(self): self.x = 1class MyObjObj(object): def __init__(self): self.xi = MyObjInt() self.xf = MyObjFloat() def testFunc(x,y,xo,z): def eval_types(): z.val = x + y + xo.xi.x + xo.xf.x return eval_types if __name__ == '__main__': z = MyObj() print(z.val) f = testFunc(1, 2, MyObjObj(), z) f() print(z.val) s = inspect.getsource(f) s = _dedent(s) print(type(s)) print(s) SymbolTable = symtable.symtable(s,'string','exec') tree = ast.parse(s) v = NodeVisitor(SymbolTable) v.visit(tree) Module visit item FunctionDefFunctionDef visit value argumentsarguments visit item AssignAssign visit item AttributeAttribute visit value Name variable z type <symbol 'z'>['_Symbol__flags', '_Symbol__name', '_Symbol__namespaces', '_Symbol__scope', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'get_name', 'get_namespace', 'get_namespaces', 'is_assigned', 'is_declared_global', 'is_free', 'is_global', 'is_imported', 'is_local', 'is_namespace', 'is_parameter', 'is_referenced']",Python ast package: traversing object hierarchies
Is there any way to check if a function should return in python?," I'm wondering if anyone can think up a way to check if a function needs to return a meaningful value in Python. That is, to check whether the return value will be used for anything. I'm guessing the answer is no, and it is better to restructure my program flow. The function in question pulls its return values from a network socket. If the return value is not going to get used, I don't want to waste the resources fetching the result.I tried already to use tracebacks to discover the calling line, but that didn't work. Here's an example of what I had in mind: The function ""knows"" that its return value is being assigned.Here is my current workaround: <code>  >>> def func():... print should_return()...>>> func()False>>> ret = func()True >>> def func(**kwargs):... should_return = kwargs.pop('_wait', False)... print should_return...>>> func()False>>> ret = func(_wait=True)True","Check if there's something ""waiting for"" the return value of a function"
How to fix an smtp error where Recipient addressed refused when using postmix and python?," I'm getting this error: raise SMTPRecipientsRefused(senderrs) smtplib.SMTPRecipientsRefused: {'example@hotmail.com': (550, '5.1.1 : Recipient address rejected: hotmail.com')}when trying to run my python script.Regardless of what recipient address I put in, it will still give me the same error. I have postfix's configuration installed as local and it properly recognizes 'localhost' but not any of the sender addresses. This is my code: And this is the main.cf file for postfix. Looking at it now,mydestination is only set to local addresses, could that be the issue? Thank you in advance <code>  import smtplib def sendEmail(addressFrom, addressTo, msg): server = smtplib.SMTP('localhost') server.set_debuglevel(1) server.sendmail(addressFrom, addressTo, msg) server.quit() msg = ""This is the content of the email"" addressFrom = """" addressTo = ""example@hotmail.com"" sendEmail(addressFrom, addressTo, msg) # See /usr/share/postfix/main.cf.dist for a commented, more complete version# Debian specific: Specifying a file name will cause the first# line of that file to be used as the name. The Debian default# is /etc/mailname.#myorigin = /etc/mailnamesmtpd_banner = $myhostname ESMTP $mail_name (Ubuntu)biff = no# appending .domain is the MUA's job.append_dot_mydomain = no# Uncomment the next line to generate ""delayed mail"" warnings#delay_warning_time = 4hreadme_directory = no# TLS parameterssmtpd_tls_cert_file=/etc/ssl/certs/ssl-cert-snakeoil.pemsmtpd_tls_key_file=/etc/ssl/private/ssl-cert-snakeoil.keysmtpd_use_tls=yessmtpd_tls_session_cache_database = btree:${data_directory}/smtpd_scachesmtp_tls_session_cache_database = btree:${data_directory}/smtp_scache# See /usr/share/doc/postfix/TLS_README.gz in the postfix-doc package for# information on enabling SSL in the smtp client.myhostname = user-desktop**mydomain = hotmail.com**alias_maps = hash:/etc/aliasesalias_database = hash:/etc/aliases**mydestination = user-desktop, localhost.$mydomain www.$mydomain**relayhost = mynetworks = 127.0.0.0/8 [::ffff:127.0.0.0]/104 [::1]/128mailbox_size_limit = 0recipient_delimiter = +inet_interfaces = loopback-onlydefault_transport = errorrelay_transport = errorinet_protocols = ipv4","SMTP error: ""Recipient addressed refused"" when trying to send an email using python and postfix"
How to copy python class?, deepcopy from copy does not copy a class: Is it only way? <code>  >>> class A(object):>>> ARG = 1>>> B = deepcopy(A)>>> A().ARG>>> 1>>> B().ARG>>> 1>>> A.ARG = 2>>> B().ARG>>> 2 B(A): pass,How to copy a python class?
How to copy a python class instance / object?, deepcopy from copy does not copy a class: Is it only way? <code>  >>> class A(object):>>> ARG = 1>>> B = deepcopy(A)>>> A().ARG>>> 1>>> B().ARG>>> 1>>> A.ARG = 2>>> B().ARG>>> 2 B(A): pass,How to copy a python class?
How to copy a python class object?, deepcopy from copy does not copy a class: Is it only way? <code>  >>> class A(object):>>> ARG = 1>>> B = deepcopy(A)>>> A().ARG>>> 1>>> B().ARG>>> 1>>> A.ARG = 2>>> B().ARG>>> 2 B(A): pass,How to copy a python class?
Sort list of tuples considering locale," Apparently PostgreSQL 8.4 and Ubuntu 10.04 cannot handle the updated way to sort W and V for Swedish alphabet. That is, it's still ordering them as the same letter like this (old definition for Swedish ordering):WaVbWcVdit should be (new definition for Swedish ordering):VbVdWaWcI need to order this correctly for a Python/Django website I'm building. I have tried various ways to just order a list of tuples created from a Django QuerySet using *values_list*. But since it's Swedish also , and letters needs to be correctly ordered. Now I have either one or the other way both not both.. The examples gives: Now, What I want is a combination of these so that both V/W and ,, ordering are correct. To be more precise. I want Ordering One to respect locale. By then using the second item (object id) in each tuple I could fetch the correct object in Django. I'm starting to doubt that this will be possible? Would upgrading PostgreSQL to a newer version that handles collations better and then use raw SQL in Django be possible? <code>  list_of_tuples = [(u'Wa', 1), (u'Vb',2), (u'Wc',3), (u'Vd',4), (u'a',5), (u'a',6), (u'a',7)]print '########## Ordering One ##############'ordered_list_one = sorted(list_of_tuples, key=lambda t: tuple(t[0].lower()))for item in ordered_list_one: print item[0]print '########## Ordering Two ##############'locale.setlocale(locale.LC_ALL, ""sv_SE.utf8"")list_of_names = [u'Wa', u'Vb', u'Wc', u'Vd', u'a', u'a', u'a']ordered_list_two = sorted(list_of_names, cmp=locale.strcoll)for item in ordered_list_two: print item ########## Ordering One ##############VbVdWaWcaaa########## Ordering Two ##############WaVbWcVdaaa",Sort list of tuples considering locale (swedish ordering)
what is the equivalence in Python 3 of letters in Python 2?," In Python 2 you get But in Python 3, you get It's not defined, whereas digits and whitespace are.What is the equivalence of letters from the string module in Python 3? <code>  >>> from string import *>>> letters'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ' >>> from string import *>>> lettersTraceback (most recent call last): File ""<stdin>"", line 1, in <module>NameError: name 'letters' is not defined",What is the equivalence in Python 3 of letters in Python 2?
Find String index from last in Python," I want to find the position (or index) of the last occurrence of a certain substring in given input string str.For example, suppose the input string is str = 'hello' and the substring is target = 'l', then it should output 3.How can I do this? <code> ",Find index of last occurrence of a substring in a string
"How to use ""range_key_condition"" to query on table with boto dynamodb"," To get range_key between (0,9999), can I do it this way? with boto v2.2.2-dev, I always get empty resultsEDIT: This is a another error sample: It's OK without ""range_key_condition"" above <code>  conn = boto.connect_dynamodb()table = conn.get_table(""mytable"")...result = table.query( hash_key = ""66"", range_key_condition = {""0"":""GE"", ""9999"":""LE""} ) In [218]: qa = taa.query(hash_key = ""1"")In [219]: qa.next()Out[219]: {u'attra': u'this is attra', u'key': u'1', u'range': 1.1} In [220]: qa = taa.query(hash_key = ""1"", range_key_condition = {0.1: ""GE""})In [221]: qa.next()---------------------------------------------------------------------------AttributeError Traceback (most recent call last)/home/user/python/enva/<ipython-input-221-dba0a498b6e1> in <module>()----> 1 qa.next()/home/user/python/enva/local/lib/python2.7/site-packages/boto-2.2.2_dev-py2.7.egg/boto/dynamodb/layer2.pycin query(self, table, hash_key, range_key_condition,attributes_to_get, request_limit, max_results, consistent_read,scan_index_forward, exclusive_start_key, item_class) 559 """""" 560 if range_key_condition:--> 561 rkc = self.dynamize_range_key_condition(range_key_condition) 562 else: 563 rkc = None/home/user/python/enva/local/lib/python2.7/site-packages/boto-2.2.2_dev-py2.7.egg/boto/dynamodb/layer2.pycin dynamize_range_key_condition(self, range_key_condition) 83 structure required by Layer1. 84 """"""---> 85 return range_key_condition.to_dict() 86 87 def dynamize_scan_filter(self, scan_filter):AttributeError: 'dict' object has no attribute 'to_dict'","How to use ""range_key_condition"" to query a DynamoDB table with boto?"
stable eye corner detection," For those who find it too long, just read the bold lines.My project of gaze estimation based screen cursor moving HCI is now dependent on one last thing - gaze estimation, for which i'm using eye corners as a reference stable point relative to which i will detect the movement of the pupil and calculate the gaze.But i haven't been able to stably detect eye corners from live webcam feed. I've been using cv.CornerHarris() and GFTT - cv.GoodFeaturesToTrack() functions for corner detection. I tried FAST demo (the executable from their website) directly on my eye images but that wasn't good.These are some results of my so far corner detections for images.Using GFTT:Using Harris:what happens in video:The green cirlces are the corners, the others (in pink, smaller circles) are the other cornersI used a certain heuristic - that the corners will be in the left or right extremeties and around the middle if thinking vertically.I've done that because after taking many snapshots in many conditions, except for less than 5% of the images, rest are like these, and for them the above heuristics hold.But these eye corner detections are for snapshots - not from the webcam feed.When i use methodologies (harris and GFTT) for webcam feed, i just don't get 'em.My code for eye corner detection using cv.CornerHarrisEye corners using GFTTNow the parameters i use in both methods - they don't show results for different lighting conditions and obviously. But in the same lighting condition as the one in which these snapshots were taken, i'm still not getting the result for the frames i queried from webcam video These parameters from GFTT work good for average lighting conditions whereas these : worked good for the static image displayed above minDistance = 30 because obviously the corners would have atleast that much distance, again, something of a trend i saw from my snaps. But i lowered it for the webcam feed version of GFTT because then i wasn't getting any corners at all.Also, for the live feed version of GFTT, there's a small change i had to accomodate: whereas for the still image version (code on pastebin) i used: Pay attention to the depths.Would that change any quality of detection?? The eye image i was passing the GFTT method didn't have a depth of 32F so i had to change it and according the rest of the temporary images (eignenimg, tempimg ,etc)Bottom line: I've to finish gaze estimation but without stable eye corner detection i can't progress.. and i've to get on to blink detection and template matching based pupil tracking (or do you know better?). Put simply, i want to know if i'm making any rookie mistakes or not doing things which are stopping me from getting the near perfect eye corner detection in my webcam video stream, which i got in my snaps i posted here.Anyways thanks for giving this a view. Any idea how i could perform eye corner detection for various lighting conditions would be very helpful Okay, if you didn't get what i'm doing in my code (how i'm getting the left and right corners), i'll explain: maxLP and maxRP will store the (x,y) for left and right corners of the eye respectively.What i'm doing here is, taking a variable for left and right corner detection, maxL and maxR respectively, which will be comparedto the x-values of the corners detected. Now simply, for maxL, it has to be something more than 0; I assigned it 20 because ifthe left corner is at (x,y) where x<20, then maxL will be = x, or if say, ie, the leftest corner's X-ordinate is found this way. Similarly for rightest corner.I tried for maxL = 50 too (but that would mean that the left corner is almost in the middle of the eye region) to get more candidates for the webcam feed - in which i'm not getting any corners at allAlso, max_dist stores the maximum distance between the so far seen X-ordinates, and thus gives a measure of which pair of corners would be left and right eye corners - the one with the maximum distance = max_distAlso, i've seen from my snapshots that the eye corners' Y-ordinates fall in between 40-70 so i used that too to minimize the candidate pool <code>  cornerCount = 100qualityLevel = 0.1minDistance = 5 cornerCount = 500 qualityLevel = 0.005 minDistance = 30 cv.CreateImage((colorImage.width, colorImage.height), 8,1) cv.CreateImage(cv.GetSize(grayImage), cv.IPL_DEPTH_32F, 1) max_dist = 0maxL = 20maxR = 0lc =0rc =0maxLP =(0,0)maxRP =(0,0)for point in cornerMem: center = int(point[0]), int(point[1]) x = point[0] y = point[1] if ( x<colorImage.width/5 or x>((colorImage.width/4)*3) ) and (y>40 and y<70): #cv.Circle(image,(x,y),2,cv.RGB(155, 0, 25)) if maxL > x: maxL = x maxLP = center if maxR < x: maxR = x maxRP = center dist = maxR-maxL if max_dist<dist: max_dist = maxR-maxL lc = maxLP rc = maxRP cv.Circle(colorImage, (center), 1, (200,100,255)) #for every cornercv.Circle(colorImage,maxLP,3,cv.RGB(0, 255, 0)) # for left eye cornercv.Circle(colorImage,maxRP,3,cv.RGB(0,255,0)) # for right eye corner",how to perform stable eye corner detection?
Reversing a Stack in Python," I'm doing some practise questions. This one needs to reverse a stack without using any other data structures except another stack.I know I will need a helper function that appends the pop-ed numbers once the original stack is empty.Can somebody get me started? I'm stuck here Thanks!The Stack class has pop, push and is_empty functions. <code>  def flip_stack(s): if not s.is_empty(): temp = s.pop flip_stack(s)",Reversing a Stack in Python using recursion
Patching a class method used in a different class," I'm trying to patch a class method using mock as described in the documentation. The Mock object itself works fine, but its methods don't: For example, their attributes like call_count aren't updated, even though the method_calls attribute of the class Mock object is. More importantly, their return_value attribute is ignored: What am I doing wrong here?EDIT: Passing a class Mock via the constructor doesn't work either, so this is not really related to the patch function. <code>  class Lib: """"""In my actual program, a module that I import"""""" def method(self): return ""real""class User: """"""The class I want to test"""""" def run(self): l = Lib() return l.method()with patch(""__main__.Lib"") as mock: #mock.return_value = ""bla"" # This works mock.method.return_value = ""mock"" u = User() print(u.run())>>> mock<MagicMock name='Lib().method()' id='39868624'>",Mocking a class method that is used via an instance
Mocking a class method used in a different class," I'm trying to patch a class method using mock as described in the documentation. The Mock object itself works fine, but its methods don't: For example, their attributes like call_count aren't updated, even though the method_calls attribute of the class Mock object is. More importantly, their return_value attribute is ignored: What am I doing wrong here?EDIT: Passing a class Mock via the constructor doesn't work either, so this is not really related to the patch function. <code>  class Lib: """"""In my actual program, a module that I import"""""" def method(self): return ""real""class User: """"""The class I want to test"""""" def run(self): l = Lib() return l.method()with patch(""__main__.Lib"") as mock: #mock.return_value = ""bla"" # This works mock.method.return_value = ""mock"" u = User() print(u.run())>>> mock<MagicMock name='Lib().method()' id='39868624'>",Mocking a class method that is used via an instance
Mocking a class method used by a different class," I'm trying to patch a class method using mock as described in the documentation. The Mock object itself works fine, but its methods don't: For example, their attributes like call_count aren't updated, even though the method_calls attribute of the class Mock object is. More importantly, their return_value attribute is ignored: What am I doing wrong here?EDIT: Passing a class Mock via the constructor doesn't work either, so this is not really related to the patch function. <code>  class Lib: """"""In my actual program, a module that I import"""""" def method(self): return ""real""class User: """"""The class I want to test"""""" def run(self): l = Lib() return l.method()with patch(""__main__.Lib"") as mock: #mock.return_value = ""bla"" # This works mock.method.return_value = ""mock"" u = User() print(u.run())>>> mock<MagicMock name='Lib().method()' id='39868624'>",Mocking a class method that is used via an instance
"In sqlalchemy, how to check if a model is attached on sessioin?"," In sqlalchemy, how to check if one object of model is attached on session?and how to get attached session of one model object. <code> ","In sqlalchemy, how to check if a model is attached on session?"
Post JSON using Python Request," I need to POST a JSON from a client to a server. I'm using Python 2.7.1 and simplejson. The client is using Requests. The server is CherryPy. I can GET a hard-coded JSON from the server (code not shown), but when I try to POST a JSON to the server, I get ""400 Bad Request"".Here is my client code: Here is the server code. Any ideas? <code>  data = {'sender': 'Alice', 'receiver': 'Bob', 'message': 'We did it!'}data_json = simplejson.dumps(data)payload = {'json_payload': data_json}r = requests.post(""http://localhost:8080"", data=payload) class Root(object): def __init__(self, content): self.content = content print self.content # this works exposed = True def GET(self): cherrypy.response.headers['Content-Type'] = 'application/json' return simplejson.dumps(self.content) def POST(self): self.content = simplejson.loads(cherrypy.request.body.read())",How to POST JSON data with Python Requests?
Post JSON using Python Requests," I need to POST a JSON from a client to a server. I'm using Python 2.7.1 and simplejson. The client is using Requests. The server is CherryPy. I can GET a hard-coded JSON from the server (code not shown), but when I try to POST a JSON to the server, I get ""400 Bad Request"".Here is my client code: Here is the server code. Any ideas? <code>  data = {'sender': 'Alice', 'receiver': 'Bob', 'message': 'We did it!'}data_json = simplejson.dumps(data)payload = {'json_payload': data_json}r = requests.post(""http://localhost:8080"", data=payload) class Root(object): def __init__(self, content): self.content = content print self.content # this works exposed = True def GET(self): cherrypy.response.headers['Content-Type'] = 'application/json' return simplejson.dumps(self.content) def POST(self): self.content = simplejson.loads(cherrypy.request.body.read())",How to POST JSON data with Python Requests?
Restriction on class attribute names in Python?," There is a restriction on the syntax of attribute access, in Python (at least in the CPython 2.7.2 implementation): My question is twofold:Is there a fundamental reason why using Python keyword attribute names (as in o.if = 123) is forbidden?Is/where is the above restriction on attribute names documented?It would make sense to do o.class = , in one of my programs, and I am a little disappointed to not be able to do it (o.class_ would work, but it does not look as simple).PS: The problem is obviously that if and class are Python keywords. The question is why using keywords as attribute names would be forbidden (I don't see any ambiguity in the expression o.class = 123), and whether this is documented. <code>  >>> class C(object): pass>>> o = C()>>> o.x = 123 # Works>>> o.if = 123 o.if = 123 ^SyntaxError: invalid syntax",Why can't attribute names be Python keywords?
How can i return two values from a function in python," I would like to return two values from a function in two separate variables. For example: And I want to be able to use these values separately. When I tried to use return i, card, it returns a tuple and this is not what I want.  <code>  def select_choice(): loop = 1 row = 0 while loop == 1: print('''Choose from the following options?: 1. Row 1 2. Row 2 3. Row 3''') row = int(input(""Which row would you like to move the card from?: "")) if row == 1: i = 2 card = list_a[-1] elif row == 2: i = 1 card = list_b[-1] elif row == 3: i = 0 card = list_c[-1] return i return card",How can I return two values from a function in Python?
Rounding up numbers in Python, In Python floor() and ceil() round to the next higher or lower integer.How to round up any value between 1.01 - 1.5 to 1.5 and 1.51 - 2.0 to 2.0 etc.? <code> ,How can I round up numbers to the next half-integer?
Python pdb: Iterating dict & getting nexpected EOF while parsing," I have a pdb trace set inside a GET request. I want to print all the attributes of the request object. I am trying the following, in pdb: I am sure there is something fundamental I am missing here. <code>  (Pdb) request<GET /foo HTTP/1.1>(Pdb) for d in dir(request):*** SyntaxError: unexpected EOF while parsing (<stdin>, line 1)","""SyntaxError: unexpected EOF while parsing"" while iterating a dictionary in PDB"
python member variable of instance works weird," i'm python newbie, and member variable of class works weird in my python code.some works like normal variable, but some works like static variable! result but the last result of the test is different from what i expected in last test.There should be no ""A"" in the 'list_value' of the instance of 'b'. It was just created, and never have been added 'A' before.I added 'A' to the instance of 'a', not 'b'. But the result show me that there are also 'A' in 'b'More over, the 'list_value' and the 'value' in the class works differently. It looks like the both have same syntax. why do they work differently? <code>  class Chaos: list_value = [] value = ""default"" def set_value(self, word): self.list_value.append(word) self.value = word def show(self, num): print(str(num) + ""===="") print(""value : "" + self.value) for st in self.list_value: sys.stdout.write(st) print(""\n=====\n"")a = Chaos()a.show(0)a.set_value(""A"")a.show(1)b = Chaos()a.show(2)b.show(3) 0====value : default=====1====value : AA=====2====value : AA=====3====value : defaultA=====","python member variable of instance works like member variable, and some works like static variable"
Extract files from zip file and retain creation date- Python 2.7.1 on Windows 7," I'm trying to extract files from a zip file using Python 2.7.1 (on Windows, fyi) and each of my attempts shows extracted files with Modified Date = time of extraction (which is incorrect). I also tried using the .extractall method, with the same results. Can anyone tell me what I'm doing wrong?I'd like to think this is possible without having to post-correct the modified time per How do I change the file creation date of a Windows file?. <code>  import os,zipfileoutDirectory = 'C:\\_TEMP\\'inFile = 'test.zip'fh = open(os.path.join(outDirectory,inFile),'rb') z = zipfile.ZipFile(fh)for name in z.namelist(): z.extract(name,outDirectory)fh.close() import os,zipfileoutDirectory = 'C:\\_TEMP\\'inFile = 'test.zip'zFile = zipfile.ZipFile(os.path.join(outDirectory,inFile)) zFile.extractall(outDirectory)",Extract files from zip file and retain mod date?
Extract files from zip file and retain mod date- Python 2.7.1 on Windows 7," I'm trying to extract files from a zip file using Python 2.7.1 (on Windows, fyi) and each of my attempts shows extracted files with Modified Date = time of extraction (which is incorrect). I also tried using the .extractall method, with the same results. Can anyone tell me what I'm doing wrong?I'd like to think this is possible without having to post-correct the modified time per How do I change the file creation date of a Windows file?. <code>  import os,zipfileoutDirectory = 'C:\\_TEMP\\'inFile = 'test.zip'fh = open(os.path.join(outDirectory,inFile),'rb') z = zipfile.ZipFile(fh)for name in z.namelist(): z.extract(name,outDirectory)fh.close() import os,zipfileoutDirectory = 'C:\\_TEMP\\'inFile = 'test.zip'zFile = zipfile.ZipFile(os.path.join(outDirectory,inFile)) zFile.extractall(outDirectory)",Extract files from zip file and retain mod date?
How to render equatation using python with matplotlib without drawing figure?," I want to render equations to PNG files and embed them in the HTML documentation of my library. I am already using pylab (matplotlib) in other projects.I have not found any clues in http://matplotlib.sourceforge.net/users/usetex.html and http://matplotlib.sourceforge.net/users/mathtext.htmlWhen I do I get a titled empty figure with axes.Update:After doing some research I found, that the easiest way of rendering LaTeX to png is using mathext ( http://code.google.com/p/mathtex/ ).Suprisingly, I had all requiered libraries to build it from source.Anyway, thanks to everyone for replies.Update 2:I did some testing of mathtex and found it does not support matrices (\begin{pmatrix}) and some other things I need.So, I'm going to install LaTex (MikTeX).Update 3:I installed proTeXt. It's huge, but easy-to-use and fast. IMHO, for now it's the only way of rendering equations. <code>  plt.title(r'$\alpha > \beta$')plt.show()",Render equation to .png file using Python
Render equation using matplotlib without drawing a figure," I want to render equations to PNG files and embed them in the HTML documentation of my library. I am already using pylab (matplotlib) in other projects.I have not found any clues in http://matplotlib.sourceforge.net/users/usetex.html and http://matplotlib.sourceforge.net/users/mathtext.htmlWhen I do I get a titled empty figure with axes.Update:After doing some research I found, that the easiest way of rendering LaTeX to png is using mathext ( http://code.google.com/p/mathtex/ ).Suprisingly, I had all requiered libraries to build it from source.Anyway, thanks to everyone for replies.Update 2:I did some testing of mathtex and found it does not support matrices (\begin{pmatrix}) and some other things I need.So, I'm going to install LaTex (MikTeX).Update 3:I installed proTeXt. It's huge, but easy-to-use and fast. IMHO, for now it's the only way of rendering equations. <code>  plt.title(r'$\alpha > \beta$')plt.show()",Render equation to .png file using Python
Python newbie: How do I know what type of exception occured?," I have a function called by the main program: but in the middle of the execution of the function it raises exception, so it jumps to the except part.How can I see exactly what happened in the someFunction() that caused the exception to happen? <code>  try: someFunction()except: print ""exception happened!""",python: How do I know what type of exception occurred?
How do I know what type of exception occured?," I have a function called by the main program: but in the middle of the execution of the function it raises exception, so it jumps to the except part.How can I see exactly what happened in the someFunction() that caused the exception to happen? <code>  try: someFunction()except: print ""exception happened!""",python: How do I know what type of exception occurred?
python: How do I know what type of exception occured?," I have a function called by the main program: but in the middle of the execution of the function it raises exception, so it jumps to the except part.How can I see exactly what happened in the someFunction() that caused the exception to happen? <code>  try: someFunction()except: print ""exception happened!""",python: How do I know what type of exception occurred?
fast way of counting bits in python," I need a fast way to count the number of bits in an integer in python.My current solution is but I am wondering if there is any faster way of doing this?PS: (i am representing a big 2D binary array as a single list of numbers and doing bitwise operations, and that brings the time down from hours to minutes. and now I would like to get rid of those extra minutes.Edit: 1. it has to be in python 2.7 or 2.6and optimizing for small numbers does not matter that much since that would not be a clear bottleneck, but I do have numbers with 10 000 + bits at some placesfor example this is a 2000 bit case: <code>  bin(n).count(""1"") 12448057941136394342297748548545082997815840357634948550739612798732309975923280685245876950055614362283769710705811182976142803324242407017104841062064840113262840137625582646683068904149296501029754654149991842951570880471230098259905004533869130509989042199261339990315125973721454059973605358766253998615919997174542922163484086066438120268185904663422979603026066685824578356173882166747093246377302371176167843247359636030248569148734824287739046916641832890744168385253915508446422276378715722482359321205673933317512861336054835392844676749610712462818600179225635467147870208L",Fast way of counting non-zero bits in positive integer
fast way of counting non-zero bits in python," I need a fast way to count the number of bits in an integer in python.My current solution is but I am wondering if there is any faster way of doing this?PS: (i am representing a big 2D binary array as a single list of numbers and doing bitwise operations, and that brings the time down from hours to minutes. and now I would like to get rid of those extra minutes.Edit: 1. it has to be in python 2.7 or 2.6and optimizing for small numbers does not matter that much since that would not be a clear bottleneck, but I do have numbers with 10 000 + bits at some placesfor example this is a 2000 bit case: <code>  bin(n).count(""1"") 12448057941136394342297748548545082997815840357634948550739612798732309975923280685245876950055614362283769710705811182976142803324242407017104841062064840113262840137625582646683068904149296501029754654149991842951570880471230098259905004533869130509989042199261339990315125973721454059973605358766253998615919997174542922163484086066438120268185904663422979603026066685824578356173882166747093246377302371176167843247359636030248569148734824287739046916641832890744168385253915508446422276378715722482359321205673933317512861336054835392844676749610712462818600179225635467147870208L",Fast way of counting non-zero bits in positive integer
Maximum value for LONG in python," How can I assign the maximum value for a long integer to a variable, similar, for example, to C++'s LONG_MAX. <code> ",Maximum value for long integer
Choose m of n elements deterministicly ," I have a vector/array of n elements. I want to choose m elements.The choices must be fair / deterministic -- equally many from each subsection.With m=10, n=20 it is easy: just take every second element.But how to do it in the general case? Do I have to calculate the LCD? <code> ",Choose m evenly spaced elements from a sequence of length n
If I return inside a with block is the file guaranteed to close?," Consider the following: Will the file be closed properly, or does using return somehow bypass the context manager? <code>  with open(path, mode) as f: return [line for line in f if condition]","In Python, if I return inside a ""with"" block, will the file still close?"
"In Python, if I return inside a ""with"" block, is the file guaranteed to close?"," Consider the following: Will the file be closed properly, or does using return somehow bypass the context manager? <code>  with open(path, mode) as f: return [line for line in f if condition]","In Python, if I return inside a ""with"" block, will the file still close?"
Adding different size/shape displaced numpy matrices," In short: I have two matrices (or arrays): I have the displacement of block_2 in the block_1 element coordinate system. I want to be able to add them (quickly), to get: In long: I would like a fast way to add two different shape matrices together, where one of the matrices can be displaced. The resulting matrix must have the shape of the first matrix, and the overlapping elements between the two matrices are summed. If there is no overlap, just the first matrix is returned unmutated.I have a function that works fine, but it's kind of ugly, and elementwise: Can broadcasting or slicing perhaps do this?I feel like maybe I'm missing something obvious. <code>  import numpyblock_1 = numpy.matrix([[ 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0]])block_2 = numpy.matrix([[ 1, 1, 1], [ 1, 1, 1], [ 1, 1, 1], [ 1, 1, 1]]) pos = (1,1) [[0 0 0 0 0] [0 1 1 1 0] [0 1 1 1 0] [0 1 1 1 0]] def add_blocks(block_1, block_2, pos): for i in xrange(0, block_2.shape[0]): for j in xrange(0, block_2.shape[1]): if (i + pos[1] >= 0) and (i + pos[1] < block_1.shape[0]) and (j + pos[0] >= 0) and (j + pos[0] < block_1.shape[1]): block_1[pos[1] + i, pos[0] + j] += block_2[i,j] return block_1",Adding different sized/shaped displaced NumPy matrices
Adding tuple to a list," Python not support adding a tuple to a list: What are the disadvantages for providing such a support in the language? Note that I would expect this to be symmetric: [1, 2] + (3, 4) and (1, 2) + [3, 4] would both evaluate to a brand-new list [1, 2, 3, 4]. My rationale is that once someone applied operator + to a mix of tuples and lists, they are likely to do it again (very possibly in the same expression), so we might as well provide the list to avoid extra conversions.Here's my motivation for this question.It happens quite often that I have small collections that I prefer to store as tuples to avoid accidental modification and to help performance. I then need to combine such tuples with lists, and having to convert each of them to list makes for very ugly code.Note that += or extend may work in simple cases. But in general, when I have an expression I don't know which of these are tuples and which are lists. So I either have to convert everything to lists: Or use itertools: Both of these solutions are uglier than a simple sum; and the chain may also be slower (since it must iterate through the inputs an element at a time). <code>  >>> [1,2,3] + (4,5,6)Traceback (most recent call last): File ""<stdin>"", line 1, in <module>TypeError: can only concatenate list (not ""tuple"") to list columns = default_columns + columns_from_user + calculated_columns columns = list(default_columns) + list(columns_from_user) + list(calculated_columns) columns = list(itertools.chain(default_columns, columns_from_user, calculated_columns))",Why can't I add a tuple to a list with the '+' operator in Python?
How do I rewrite this URL in Django?," I have a Django based website. I would like to redirect URLs with the pattern servertest in them to the same URL except servertest should be replaced by server-test.So for example the following URLs would be mapped be redirected as shown below: I can get the first example working using the following line in urls.py: Not sure how to do it for the others so only the servetest part of the URL is replaced. <code>  http://acme.com/servertest/ => http://acme.com/server-test/ http://acme.com/servertest/www.example.com => http://acme.com/server-test/www.example.comhttp://acme.com/servertest/www.example.com:8833 => http://acme.com/server-test/www.example.com:8833 ('^servertest/$', 'redirect_to', {'url': '/server-test/'}),",How do I redirect by URL pattern in Django?
Simple(?) t-statistic flub," I'm trying to write my own Python code to compute t-statistics and p-values for one and two tailed independent t tests. I can use the normal approximation, but for the moment I am trying to just use the t-distribution. I've been unsuccessful in matching the results of SciPy's stats library on my test data. I could use a fresh pair of eyes to see if I'm just making a dumb mistake somewhere.Note, this is cross-posted from Cross-Validated because it's been up for a while over there with no responses, so I thought it can't hurt to also get some software developer opinions. I'm trying to understand if there's an error in the algorithm I'm using, which should reproduce SciPy's result. This is a simple algorithm, so it's puzzling why I can't locate the mistake.My code: Update:After reading a bit more on the Welch's t-test, I saw that I should be using the Welch-Satterthwaite formula to calculate degrees of freedom. I updated the code above to reflect this.With the new degrees of freedom, I get a closer result. My two-sided p-value is off by about 0.008 from the SciPy version's... but this is still much too big an error so I must still be doing something incorrect (or SciPy distribution functions are very bad, but it's hard to believe they are only accurate to 2 decimal places).Second update:While continuing to try things, I thought maybe SciPy's version automatically computes the Normal approximation to the t-distribution when the degrees of freedom are high enough (roughly > 30). So I re-ran my code using the Normal distribution instead, and the computed results are actually further away from SciPy's than when I use the t-distribution.Bonus question :)(More statistical theory related; feel free to ignore)Also, the t-statistic is negative. I was just wondering what this means for the one-sided t-test. Does this typically mean that I should be looking in the negative axis direction for the test? In my test data, population 1 is a control group who did not receive a certain employment training program. Population 2 did receive it, and the measured data are wage differences before/after treatment. So I have some reason to think that the mean for population 2 will be larger. But from a statistical theory point of view, it doesn't seem right to concoct a test this way. How could I have known to check (for the one-sided test) in the negative direction without relying on subjective knowledge about the data? Or is this just one of those frequentist things that, while not philosophically rigorous, needs to be done in practice? <code>  import numpy as npimport scipy.stats as stdef compute_t_stat(pop1,pop2): num1 = pop1.shape[0]; num2 = pop2.shape[0]; # The formula for t-stat when population variances differ. t_stat = (np.mean(pop1) - np.mean(pop2))/np.sqrt( np.var(pop1)/num1 + np.var(pop2)/num2 ) # ADDED: The Welch-Satterthwaite degrees of freedom. df = ((np.var(pop1)/num1 + np.var(pop2)/num2)**(2.0))/( (np.var(pop1)/num1)**(2.0)/(num1-1) + (np.var(pop2)/num2)**(2.0)/(num2-1) ) # Am I computing this wrong? # It should just come from the CDF like this, right? # The extra parameter is the degrees of freedom. one_tailed_p_value = 1.0 - st.t.cdf(t_stat,df) two_tailed_p_value = 1.0 - ( st.t.cdf(np.abs(t_stat),df) - st.t.cdf(-np.abs(t_stat),df) ) # Computing with SciPy's built-ins # My results don't match theirs. t_ind, p_ind = st.ttest_ind(pop1, pop2) return t_stat, one_tailed_p_value, two_tailed_p_value, t_ind, p_ind",Tracking down the assumptions made by SciPy's `ttest_ind()` function
Python: Calculating Mean of arrays with different lengths," Is it possible to calculate the mean of multiple arrays, when they may have different lengths? I am using numpy. So let's say I have: Now I want to calculate the mean, but ignoring elements that are 'missing' (Naturally, I can not just append zeros as this would mess up the mean)Is there a way to do this without iterating through the arrays?PS. These arrays are all 2-D, but will always have the same amount of coordinates for that array. I.e. the 1st array is 5 and 5, 2nd is 6 and 6, 3rd is 4 and 4.An example: This must give And graphically: Now imagine that these 2-D arrays are placed on top of each other with coordinates overlapping contributing to that coordinate's mean. <code>  numpy.array([[1, 2, 3, 4, 8], [3, 4, 5, 6, 0]])numpy.array([[5, 6, 7, 8, 7, 8], [7, 8, 9, 10, 11, 12]])numpy.array([[1, 2, 3, 4], [5, 6, 7, 8]]) np.array([[1, 2], [3, 4]])np.array([[1, 2, 3], [3, 4, 5]])np.array([[7], [8]]) (1+1+7)/3 (2+2)/2 3/1(3+3+8)/3 (4+4)/2 5/1 [1, 2] [1, 2, 3] [7][3, 4] [3, 4, 5] [8]",Calculating Mean of arrays with different lengths
Why the creation of class in Python is slow?," I found that creation of a class is way slower than instantiation of a class. Yeah, create 9000 classes took 16 secs, and becomes even slower in the subsequent calls.And this: gives similar results.But instantiation don't suffer: 5000000 instances in less than one sec.What makes the creation this expensive?And why the creation process become slower?EDIT:How to reproduce:start a fresh python process, the initial several ""calc(10000)""s give a number of 0.5 on my machine. And try some bigger values, calc(100000), it can't end in even 10secs, interrupt it, and calc(10000), gives a 15sec.EDIT:Additional fact:If you gc.collect() after 'calc' becomes slow, you can get the 'normal' speed at beginning, but the timing will increasing in subsequent calls <code>  >>> from timeit import Timer as T>>> def calc(n):... return T(""class Haha(object): pass"").timeit(n)<<After several these 'calc' things, at least one of them have a big number, eg. 100000>>>>> calc(9000)15.947055101394653>>> calc(9000)17.39099097251892>>> calc(9000)18.824054956436157>>> calc(9000)20.33335590362549 >>> T(""type('Haha', b, d)"", ""b = (object, ); d = {}"").timeit(9000) >>> T(""Haha()"", ""class Haha(object): pass"").timeit(5000000)0.8786070346832275 >>> from a import calc>>> calc(10000)0.4673938751220703>>> calc(10000)0.4300072193145752>>> calc(10000)0.4270968437194824>>> calc(10000)0.42754602432250977>>> calc(10000)0.4344758987426758>>> calc(100000)^CTraceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""a.py"", line 3, in calc return T(""class Haha(object): pass"").timeit(n) File ""/usr/lib/python2.7/timeit.py"", line 194, in timeit timing = self.inner(it, self.timer) File ""<timeit-src>"", line 6, in innerKeyboardInterrupt>>> import gc>>> gc.collect()234204>>> calc(10000)0.4237039089202881>>> calc(10000)1.5998330116271973>>> calc(10000)4.136359930038452>>> calc(10000)6.625348806381226",Why is creating a class in Python so much slower than instantiating a class?
"Advice needed, mixin classes should also be a standalone class"," Say I have a class A, B and C.Class A and B are both mixin classes for Class C. This will not work when instantiating class C. I would have to remove object from class C to make it work. (Else you'll get MRO problems). TypeError: Error when calling the metaclass bases Cannot create a consistent method resolution order (MRO) for bases B, object, AHowever, my case is a bit more complicated. In my case class C is a server where A and B will be plugins that are loaded on startup. These are residing in their own folder.I also have a Class named Cfactory. In Cfactory I have a __new__ method that will create a fully functional object C. In the __new__ method I search for plugins, load them using __import__, and then assign them to C.__bases__ += (loadedClassTypeGoesHere, )So the following is a possibility: (made it quite abstract) This again will not work, and will give the MRO errors again: TypeError: Cannot create a consistent method resolution order (MRO) for bases object, AAn easy fix for this is removing the object baseclass from A and B. However this will make them old-style objects which should be avoided when these plugins are being run stand-alone (which should be possible, UnitTest wise)Another easy fix is removing object from C but this will also make it an old-style class and C.__bases__ will be unavailable thus I can't add extra objects to the base of CWhat would be a good architectural solution for this and how would you do something like this? For now I can live with old-style classes for the plugins themselves. But I rather not use them. <code>  class A( object ): passclass B( object ): passclass C( object, A, B ): pass class A( object ): def __init__( self ): pass def printA( self ): print ""A""class B( object ): def __init__( self ): pass def printB( self ): print ""B""class C( object ): def __init__( self ): passclass Cfactory( object ): def __new__( cls ): C.__bases__ += ( A, ) C.__bases__ += ( B, ) return C()",How do I dynamically add mixins as base classes without getting MRO errors?
How do I dynamically add mixines as base classes without getting MRO errors?," Say I have a class A, B and C.Class A and B are both mixin classes for Class C. This will not work when instantiating class C. I would have to remove object from class C to make it work. (Else you'll get MRO problems). TypeError: Error when calling the metaclass bases Cannot create a consistent method resolution order (MRO) for bases B, object, AHowever, my case is a bit more complicated. In my case class C is a server where A and B will be plugins that are loaded on startup. These are residing in their own folder.I also have a Class named Cfactory. In Cfactory I have a __new__ method that will create a fully functional object C. In the __new__ method I search for plugins, load them using __import__, and then assign them to C.__bases__ += (loadedClassTypeGoesHere, )So the following is a possibility: (made it quite abstract) This again will not work, and will give the MRO errors again: TypeError: Cannot create a consistent method resolution order (MRO) for bases object, AAn easy fix for this is removing the object baseclass from A and B. However this will make them old-style objects which should be avoided when these plugins are being run stand-alone (which should be possible, UnitTest wise)Another easy fix is removing object from C but this will also make it an old-style class and C.__bases__ will be unavailable thus I can't add extra objects to the base of CWhat would be a good architectural solution for this and how would you do something like this? For now I can live with old-style classes for the plugins themselves. But I rather not use them. <code>  class A( object ): passclass B( object ): passclass C( object, A, B ): pass class A( object ): def __init__( self ): pass def printA( self ): print ""A""class B( object ): def __init__( self ): pass def printB( self ): print ""B""class C( object ): def __init__( self ): passclass Cfactory( object ): def __new__( cls ): C.__bases__ += ( A, ) C.__bases__ += ( B, ) return C()",How do I dynamically add mixins as base classes without getting MRO errors?
Extract a part of the filepath (a directory) in python, I need to extract the name of the parent directory of a certain path. This is what it looks like: I would like to extract directory_i_need. <code>  C:\stuff\directory_i_need\subdir\file.jpg,Extract a part of the filepath (a directory) in Python
Load CSV data into MySQL in Python," Not sure what I'm missing here but this code runs without any error message, but there's nothing in the table. I'm loading a CSV values in three columns into mysql table Would appreciate if someone else could have a look. <code>  import csvimport MySQLdbmydb = MySQLdb.connect(host='localhost', user='root', passwd='', db='mydb')cursor = mydb.cursor()csv_data = csv.reader(file('students.csv'))for row in csv_data: cursor.execute('INSERT INTO testcsv(names, \ classes, mark )' \ 'VALUES(""%s"", ""%s"", ""%s"")', row)#close the connection to the database.cursor.close()print ""Done""",Load CSV data into MySQL in Python
How To Do This in Python," I want to find the info about a webpage using curl, but in Python, so far I have this: If I run that, it prints out: What I want to do, is be able to match the 200 in it using a regex (i don't need help with that), but, I can't find a way to convert all the text above into a string. How do I do that? I tried: info = os.system(""curl --head www.google.com"") but info was just 0. <code>  os.system(""curl --head www.google.com"") HTTP/1.1 200 OKDate: Sun, 15 Apr 2012 00:50:13 GMTExpires: -1Cache-Control: private, max-age=0Content-Type: text/html; charset=ISO-8859-1Set-Cookie: PREF=ID=3e39ad65c9fa03f3:FF=0:TM=1334451013:LM=1334451013:S=IyFnmKZh0Ck4xfJ4; expires=Tue, 15-Apr-2014 00:50:13 GMT; path=/; domain=.google.comSet-Cookie: NID=58=Giz8e5-6p4cDNmx9j9QLwCbqhRksc907LDDO6WYeeV-hRbugTLTLvyjswf6Vk1xd6FPAGi8VOPaJVXm14TBm-0Seu1_331zS6gPHfFp4u4rRkXtSR9Un0hg-smEqByZO; expires=Mon, 15-Oct-2012 00:50:13 GMT; path=/; domain=.google.com; HttpOnlyP3P: CP=""This is not a P3P policy! See http://www.google.com/support/accounts/bin/answer.py?hl=en&answer=151657 for more info.""Server: gwsX-XSS-Protection: 1; mode=blockX-Frame-Options: SAMEORIGINTransfer-Encoding: chunked",How To Capture Output of Curl from Python script
How to pass extra arguments to python decorator?," I have a decorator like below. I want to enhance this decorator to accept another argument like below But this code gives the error, TypeError: myDecorator() takes exactly 2 arguments (1 given)Why is the function not automatically passed? How do I explicitly pass the function to the decorator function? <code>  def myDecorator(test_func): return callSomeWrapper(test_func)def callSomeWrapper(test_func): return test_func@myDecoratordef someFunc(): print 'hello' def myDecorator(test_func,logIt): if logIt: print ""Calling Function: "" + test_func.__name__ return callSomeWrapper(test_func)@myDecorator(False)def someFunc(): print 'Hello'",How do I pass extra arguments to a Python decorator?
Python: convert string to float --> error," I have some Python code that pulls strings out of a text file: Python code: This gives an error: Why can't [2.974717463860223e-06 be converted to a float? <code>  [2.467188005806714e-05, 0.18664554919828535, 0.5026880460053854, ....] v = string[string.index('['):].split(',')for elem in v: new_list.append(float(elem)) ValueError: could not convert string to float: [2.974717463860223e-06",Python error: could not convert string to float
How to remove convexity defects in sudoku square," I was doing a fun project: Solving a Sudoku from an input image using OpenCV (as in Google goggles etc). And I have completed the task, but at the end I found a little problem for which I came here.I did the programming using Python API of OpenCV 2.3.1.Below is what I did :Read the imageFind the contours Select the one with maximum area, ( and also somewhat equivalent to square).Find the corner points.e.g. given below:(Notice here that the green line correctly coincides with the true boundary of the Sudoku, so the Sudoku can be correctly warped. Check next image)warp the image to a perfect squareeg image:Perform OCR ( for which I used the method I have given in Simple Digit Recognition OCR in OpenCV-Python )And the method worked well.Problem:Check out this image.Performing the step 4 on this image gives the result below:The red line drawn is the original contour which is the true outline of sudoku boundary.The green line drawn is approximated contour which will be the outline of warped image.Which of course, there is difference between green line and red line at the top edge of sudoku. So while warping, I am not getting the original boundary of the Sudoku.My Question : How can I warp the image on the correct boundary of the Sudoku, i.e. the red line OR how can I remove the difference between red line and green line? Is there any method for this in OpenCV? <code> ",How to remove convexity defects in a Sudoku square?
pandas aggregated data to a numpy array : data conversion," I have aggregated data using pandas data frame. Below is some actual data shown and how I aggregated it.fdf.groupby(['row',col'])['percent'].sum()http://pastebin.com/R8XWpgtUWhat I would like to do is create a 2d numpy array of this (rows = row, columns = col). Any slick way to do this ?Another way I did something similar was create a pivot tablepivot_table(fdf,values='percent',rows='row',cols='col', aggfunc=np.sum)In this case I want to convert this pivot table to 2d numpy array. Is there a way for me to index into each cell of this table. If so then I probably will be Ok with the table itself.  <code> ",pandas aggregated data to a numpy array : data structure conversion
linux - write a script that displays result similar to top," I am looking for some sort of packages / APIs in Linux that can display output in columns similar to how top does. For example, keep cleaning and rewriting the output to a full screen at a given interval (I guess watch probably does this good enough. But I am hoping to have some APIs that wrap on top of it). Sort by columns easily. Particularly if I sort by column A, then when next time I re-print everything, it remembers to sort by that column A every time the output is refreshed. And of course, ideally it can handle the keyboard input for me as well. All in all, I am looking for packages or APIs that can help me organize my output in a way ""top"" organizes it. Just to clarity: What I display might be completely unrelated to the system statistics. I just like the way top organizes the content. For example, My output content might be (and it's constantly changing, which is why it needs to be cleaned and rewritten): Time Col1 Col2 12 4 13 13 5 19 14 5 15I can hit a key say ""A"" then it sorts by Time. If I hit a key B then it sorts by Col1. If I hit a key say C then it sorts by Col2, etc, etc. And of course this output content can be entirely in memory, organized in any data structures.  <code> ","scripting full-screen, curses-style, updating tabular output (a la top) on unixen"
"Iterating multidimensional list in Python, is there a generic way?"," I would like to implement a function, that would multiply all elements in list by two. However my problem is that lists can have different amount of dimensions.Is there a general way to loop/iterate multidimensional list and for example multiply each value by two?EDIT1: Thanks for the fast answers. For this case, I don't want to use numpy. The recursion seems good, and it doesn't even need to make copy of the list, which could be quite large actually. <code>  # 2x3 dimensional listmultidim_list = [ [1,2,3], [4,5,6], ]# 2x3x2 dimensional listmultidim_list2 = [ [ [1,2,3], [4,5,6], ], [ [7,8,9], [10,11,12], ] ]def multiply_list(list): ...","Python: Iterating lists with different amount of dimensions, is there a generic way?"
Python: most concise way to find difference of keys between 2 dictionaries?," I needed to compare 2 dictionaries to find the set of keys in one dictionary which was not in the other. I know that Python set objects support: but I can't do: or: (I was a little surprised by the last point, because in Java the keys are a Set object.) One solution is: is there a better or more concise way to do this? <code>  set3=set1-set2 dict3=dict1-dict2 missingKeys=dict1.keys()-dict2.keys() missingKeys=set(dict1.keys())-set(dict2.keys())","Concise way to find ""key"" difference between 2 dictionaries?"
"Concise way to find ""key"" differences between 2 dictionaries?"," I needed to compare 2 dictionaries to find the set of keys in one dictionary which was not in the other. I know that Python set objects support: but I can't do: or: (I was a little surprised by the last point, because in Java the keys are a Set object.) One solution is: is there a better or more concise way to do this? <code>  set3=set1-set2 dict3=dict1-dict2 missingKeys=dict1.keys()-dict2.keys() missingKeys=set(dict1.keys())-set(dict2.keys())","Concise way to find ""key"" difference between 2 dictionaries?"
Django - Create User," I'm trying to create a new User in a Django project by the following code, but the highlighted line fires an exception. Any help? <code>  def createUser(request): userName = request.REQUEST.get('username', None) userPass = request.REQUEST.get('password', None) userMail = request.REQUEST.get('email', None) # TODO: check if already existed **user = User.objects.create_user(userName, userMail, userPass)** user.save() return render_to_response('home.html', context_instance=RequestContext(request))",How to create a user in Django?
How to create an user in Django?," I'm trying to create a new User in a Django project by the following code, but the highlighted line fires an exception. Any help? <code>  def createUser(request): userName = request.REQUEST.get('username', None) userPass = request.REQUEST.get('password', None) userMail = request.REQUEST.get('email', None) # TODO: check if already existed **user = User.objects.create_user(userName, userMail, userPass)** user.save() return render_to_response('home.html', context_instance=RequestContext(request))",How to create a user in Django?
Is there a way to get an item from a Python set in linear time?," Possible Duplicate: Python: Retrieve items from a set Consider the following code: So new_item is in s because it is equivalent to one of its items, but it is a different object.What I want is to get item1 from s given new_item is in s.One solution I have come up with is straightforward but not very efficient: Another solution seems more efficient but actually does not work: Nor this one: Because intersection works in an undefined way: If this matters, I am using Python 2.7 x64 on Windows 7, but I need a cross-platform solution.Thanks to everyone. I came up with the following temporary solution: which will be replaced in future with the following solution (which is very incomplete right now): <code>  >>> item1 = (1,)>>> item2 = (2,)>>> s = set([item1, item2])>>> sset([(2,), (1,)])>>> new_item = (1,)>>> new_item in sTrue>>> new_item == item1True>>> new_item is item1False def get_item(s, new_item): for item in s: if item == new_item: return item>>> get_item(s, new_item) is new_itemFalse>>> get_item(s, new_item) is item1True def get_item_using_intersection1(s, new_item): return set([new_item]).intersection(s).pop() def get_item_using_intersection2(s, new_item): return s.intersection(set([new_item])).pop() >>> get_item_using_intersection1(s, new_item) is new_itemTrue>>> get_item_using_intersection1(s, new_item) is item1False>>> get_item_using_intersection2(s, new_item) is new_itemTrue>>> get_item_using_intersection2(s, new_item) is item1False class SearchableSet(set): def find(self, item): for e in self: if e == item: return e class SearchableSet(object): def __init__(self, iterable=None): self.__data = {} if iterable is not None: for e in iterable: self.__data[e] = e def __iter__(self): return iter(self.__data) def __len__(self): return len(self.__data) def __sub__(self, other): return SearchableSet(set(self).__sub__(set(other))) def add(self, item): if not item in self: self.__data[item] = item def find(self, item): return self.__data.get(item)",Is there a way to get an item from a set in O(1) time?
Is there a way to get an item from a set in linear time?," Possible Duplicate: Python: Retrieve items from a set Consider the following code: So new_item is in s because it is equivalent to one of its items, but it is a different object.What I want is to get item1 from s given new_item is in s.One solution I have come up with is straightforward but not very efficient: Another solution seems more efficient but actually does not work: Nor this one: Because intersection works in an undefined way: If this matters, I am using Python 2.7 x64 on Windows 7, but I need a cross-platform solution.Thanks to everyone. I came up with the following temporary solution: which will be replaced in future with the following solution (which is very incomplete right now): <code>  >>> item1 = (1,)>>> item2 = (2,)>>> s = set([item1, item2])>>> sset([(2,), (1,)])>>> new_item = (1,)>>> new_item in sTrue>>> new_item == item1True>>> new_item is item1False def get_item(s, new_item): for item in s: if item == new_item: return item>>> get_item(s, new_item) is new_itemFalse>>> get_item(s, new_item) is item1True def get_item_using_intersection1(s, new_item): return set([new_item]).intersection(s).pop() def get_item_using_intersection2(s, new_item): return s.intersection(set([new_item])).pop() >>> get_item_using_intersection1(s, new_item) is new_itemTrue>>> get_item_using_intersection1(s, new_item) is item1False>>> get_item_using_intersection2(s, new_item) is new_itemTrue>>> get_item_using_intersection2(s, new_item) is item1False class SearchableSet(set): def find(self, item): for e in self: if e == item: return e class SearchableSet(object): def __init__(self, iterable=None): self.__data = {} if iterable is not None: for e in iterable: self.__data[e] = e def __iter__(self): return iter(self.__data) def __len__(self): return len(self.__data) def __sub__(self, other): return SearchableSet(set(self).__sub__(set(other))) def add(self, item): if not item in self: self.__data[item] = item def find(self, item): return self.__data.get(item)",Is there a way to get an item from a set in O(1) time?
Getting inverse (1/x) elements of a numpy array," My question is very simple, suppose that I have an array like and I'd like to get an array like However, if you write something like or it won't work.The only way I've found so far is to write something like: But I'm absolutely certains that there is a better way to do that. Does anyone have any idea? <code>  array = np.array([1, 2, 3, 4]) [1, 0.5, 0.3333333, 0.25] 1/array np.divide(1.0, array) print np.divide(np.ones_like(array)*1.0, array)",Numpy array element-wise division (1/x)
is there a for python to read a .txt file and store each line to memory?," I am making a little program that will read and display text from a document. I have got a test file which looks like this: and so on. Now I would like Python to read each line and store it to memory, so when you select to display the data, it will display it in the shell as such: and so on. How can I do this? <code>  12,12,1212,31,121,5,3... 1. 12,12,122. 12,31,12...",Is there a way to read a .txt file and store each line to memory?
Is there a way for python to read a .txt file and store each line to memory?," I am making a little program that will read and display text from a document. I have got a test file which looks like this: and so on. Now I would like Python to read each line and store it to memory, so when you select to display the data, it will display it in the shell as such: and so on. How can I do this? <code>  12,12,1212,31,121,5,3... 1. 12,12,122. 12,31,12...",Is there a way to read a .txt file and store each line to memory?
Python: How do I remove all punctuation that follows a string?," It's for a game in which the user can input a value like ""Iced tea.."" I would like to manipulate the string to return ""Iced tea"" without the trailing punctuation marks. Looking for most elegant / simplest python solution. Tried which works if there's only one punctuation mark at the end. But it's not all-encompassing. Found a Java solution: <code>  def last_character(word): if word.endswith('.' or ','): word = word[:-1] return word String resultString = subjectString.replaceAll(""([a-z]+)[?:!.,;]*"", ""$1"");",How do I remove all punctuation that follows a string?
weird behaviour : python lambda's binding to local values," The following code spits out 1 twice, but I expect to see 0 and then 1. I expected python lambdas to bind to the reference a local variable is pointing to, behind the scenes. However that does not seem to be the case. I have encountered this problem in a large system where the lambda is doing modern C++'s equivalent of a bind ('boost::bind' for example) where in such case you would bind to a smart ptr or copy construct a copy for the lambda.So, how do I bind a local variable to a lambda function and have it retain the correct reference when used? I'm quite gobsmacked with the behaviour since I would not expect this from a language with a garbage collector. <code>  def pv(v) : print vx = []for v in range(2): x.append(lambda : pv(v))for xx in x: xx()",Python lambda's binding to local values
How to ompare inheritance with several classes?," I want to check if an object is an instance of any class in a list/group of Classes, but I can't find if there is even a pythonic way of doing so without doing I mean, comparing class by class.It would be more likely to use some function similar to isinstance that would receive n number of Classes to compare against if that even exists.Thanks in advance for your help!! :) <code>  if isinstance(obj, Class1) or isinstance(obj, Class2) ... or isinstance(obj, ClassN): # proceed with some logic",How to compare inheritance with several classes?
concatinating tuple," Suppose I have a list: Now I want to convert this list into a tuple. I thought coding something like this would do: and it gave an error. It's quite obvious why, I am trying to concatenate an integer with a tuple.But tuples don't have the same functions as lists do, like insert or append.So how can I add elements through looping? Same thing with dictionaries, I feel as if I have a missing link. <code>  a=[1,2,3,4,5] state=() for i in a: state=state+i",Concatenating Tuple
concatenating tuple," Suppose I have a list: Now I want to convert this list into a tuple. I thought coding something like this would do: and it gave an error. It's quite obvious why, I am trying to concatenate an integer with a tuple.But tuples don't have the same functions as lists do, like insert or append.So how can I add elements through looping? Same thing with dictionaries, I feel as if I have a missing link. <code>  a=[1,2,3,4,5] state=() for i in a: state=state+i",Concatenating Tuple
find indexes of sequence in list in python," I am quite new and I hope it's not too obvious, but I just can't seem to find a short and precise answer to the following problem.I have two lists: I would like to find when all the indexes of the second list (b) are in the first list (a), so that I get something like this:indexes of b in a: 3, 4, 5 or b = a[3:6] <code>  a = [2,3,5,2,5,6,7,2]b = [2,5,6]",Find indexes of sequence in list in python
Can I create separate files for separate classes in Python?," I am a Java programmer and I have always created separate files for Classes, I am attempting to learn python and I want to learn it right.Is it costly in python to put Classes in different files, meaning one file contains only one class. I read in a blog that it is costly because resolution of . operator happens at runtime in python (It happens at compile time for Java).Note: I did read in other posts that we can put them in separate files but they don't mention if they are costlier in any way <code> ",Is it costly in Python to put classes in different files?
Is it costly in Python to put classes in different files?," I am a Java programmer and I have always created separate files for Classes, I am attempting to learn python and I want to learn it right.Is it costly in python to put Classes in different files, meaning one file contains only one class. I read in a blog that it is costly because resolution of . operator happens at runtime in python (It happens at compile time for Java).Note: I did read in other posts that we can put them in separate files but they don't mention if they are costlier in any way <code> ",Is it costly in Python to put classes in different files?
Python extending a class using super(): python 3 vs python 2," Originally I wanted to ask this question, but then I found it was already thought of before...Googling around I found this example of extending configparser. The following works with Python 3: But not with Python 2: Then I read a little bit on Python New Class vs. Old Class styles (e.g. here.And now I am wondering, I can do: But, shouldn't I call init? Is this in Python 2 the equivalent: <code>  $ python3Python 3.2.3rc2 (default, Mar 21 2012, 06:59:51) [GCC 4.6.3] on linux2>>> from configparser import SafeConfigParser>>> class AmritaConfigParser(SafeConfigParser):... def __init__(self):... super().__init__()... >>> cfg = AmritaConfigParser() >>> class AmritaConfigParser(SafeConfigParser):... def __init__(self):... super(SafeConfigParser).init()... >>> cfg = AmritaConfigParser()Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 3, in __init__TypeError: must be type, not classob class MyConfigParser(ConfigParser.ConfigParser): def Write(self, fp): """"""override the module's original write funcition"""""" .... def MyWrite(self, fp): """"""Define new function and inherit all others"""""" class AmritaConfigParser(ConfigParser.SafeConfigParser): #def __init__(self): # super().__init__() # Python3 syntax, or rather, new style class syntax ... # # is this the equivalent of the above ? def __init__(self): ConfigParser.SafeConfigParser.__init__(self)",Python extending with - using super() Python 3 vs Python 2
Python extending with - using super() python 3 vs python 2," Originally I wanted to ask this question, but then I found it was already thought of before...Googling around I found this example of extending configparser. The following works with Python 3: But not with Python 2: Then I read a little bit on Python New Class vs. Old Class styles (e.g. here.And now I am wondering, I can do: But, shouldn't I call init? Is this in Python 2 the equivalent: <code>  $ python3Python 3.2.3rc2 (default, Mar 21 2012, 06:59:51) [GCC 4.6.3] on linux2>>> from configparser import SafeConfigParser>>> class AmritaConfigParser(SafeConfigParser):... def __init__(self):... super().__init__()... >>> cfg = AmritaConfigParser() >>> class AmritaConfigParser(SafeConfigParser):... def __init__(self):... super(SafeConfigParser).init()... >>> cfg = AmritaConfigParser()Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 3, in __init__TypeError: must be type, not classob class MyConfigParser(ConfigParser.ConfigParser): def Write(self, fp): """"""override the module's original write funcition"""""" .... def MyWrite(self, fp): """"""Define new function and inherit all others"""""" class AmritaConfigParser(ConfigParser.SafeConfigParser): #def __init__(self): # super().__init__() # Python3 syntax, or rather, new style class syntax ... # # is this the equivalent of the above ? def __init__(self): ConfigParser.SafeConfigParser.__init__(self)",Python extending with - using super() Python 3 vs Python 2
How to iterate over values in dictionary Python," I have: I want to iterate over this dictionary, but over the values instead of the keys, so I can use the values in another function.For example, I want to test which dictionary values are greater than 6, and then store their keys in a list. My code looks like this: and then, in a perfect world, list would feature all the keys whose value is greater than 6.However, my for loop is only iterating over the keys; I would like to change that to the values!Any help is greatly appreciated.thank you <code>  dictionary = {""foo"":12, ""bar"":2, ""jim"":4, ""bob"": 17} list = []for c in dictionary: if c > 6: list.append(dictionary[c])print list",How to filter dictionary keys based on its corresponding values
Readin data blocks from a file in Python," I'm new to python and am trying to read ""blocks"" of data from a file. The file is written something like: Now I want to be able specify and read only one block of data out of these many blocks. I'm using numpy.loadtxt('filename',comments='#') to read the data but it loads the whole file in one go. I searched online and someone has created a patch for the numpy io routine to specify reading blocks but it's not in mainstream numpy. It's much easier to choose blocks of data in gnuplot but I'd have to write the routine to plot the distribution functions. If I can figure out reading specific blocks, it would be much easier in python. Also, I'm moving all my visualization codes to python from IDL and gnuplot, so it'll be nice to have everything in python instead of having things scattered around in multiple packages.I thought about calling gnuplot from within python, plotting a block to a table and assigning the output to some array in python. But I'm still starting and I could not figure out the syntax to do it.Any ideas, pointers to solve this problem would be of great help. <code>  # Some comment# 4 cols of data --x,vx,vy,vz# nsp, nskip = 2 10# 0 0.0000000# 1 4 0.5056E+03 0.8687E-03 -0.1202E-02 0.4652E-02 0.3776E+03 0.8687E-03 0.1975E-04 0.9741E-03 0.2496E+03 0.8687E-03 0.7894E-04 0.8334E-03 0.1216E+03 0.8687E-03 0.1439E-03 0.6816E-03# 2 4 0.5056E+03 0.8687E-03 -0.1202E-02 0.4652E-02 0.3776E+03 0.8687E-03 0.1975E-04 0.9741E-03 0.2496E+03 0.8687E-03 0.7894E-04 0.8334E-03 0.1216E+03 0.8687E-03 0.1439E-03 0.6816E-03# 500 0.99999422# 1 4 0.5057E+03 0.7392E-03 -0.6891E-03 0.4700E-02 0.3777E+03 0.9129E-03 0.2653E-04 0.9641E-03 0.2497E+03 0.9131E-03 0.7970E-04 0.8173E-03 0.1217E+03 0.9131E-03 0.1378E-03 0.6586E-03and so on",Reading data blocks from a file in Python
"Python/Django: synonym for field ""type"" in database model (reserved keyword)"," I created a django project. It contains a model class with a ""type"" attribute. I think that ""type"" is the most appropriate term to describe that field, because it defines the kind of the entry. I do not like ""kind"" or ""category"" that much because they are not as generic as ""type"".The problem This is a warning, so is that a problem?If yes, what choices do i have?Do you have a good alternative to the term ""type""? <code>  class Vehicle(models.Model): TYPE = ( (u'car', u'Car'), (u'motorcycle', u'Motorcycle'), (u'airplane', u'Airplane'), ) type = models.CharField(max_length=100, choices=TYPE) Assignment to reserved built-in symbol: type","Python/Django: synonym for field ""type"" in database model (reserved built-in symbol)"
"Install pip and virtualenv, a chickend and egg dilemma?"," I've already been using pip and virtualenv (and actually sometimes still prefer a well organized combination through an SVN repository, wise usage of svn:externals, and dynamic sys.path).But this time for a new server installation I'd like to do things in the right way.So I go to the pip installation page and it says: The recommended way to use pip is within virtualenv, since every virtualenv has pip installed in it automatically. This does not require root access or modify your system Python installation. [...]Then I go to the virtualenv installation page and it suggests: You can install virtualenv with pip install virtualenv, or the latest development version with pip install virtualenv==dev. You can also use easy_install [...]And pip is supposed to replace easy_install, of course :)Granted, they both explain all alternative ways to install.But... which one should go first? And should I favor systemwide pip or not?I see a main reason to ponder over, but there might be othersdo I want to facilitate life for all users of the box, or is this a server targeted to one single user running some services?If I want everybody to have a virtual env available I might just install a system wide pip (eg. with ubuntu do sudo aptitude install python-pip then use it to install virtualenv sudo pip install virtualenv).edit another reason to ponder over: virtualenvwrapper install instructions (but not the docs) say: Note In order to use virtualenvwrapper you must install virtualenv separately.not exactly sure what ""separately"" mean there (i never noticed).Otherwise, which one should go first, and does it really make a difference or not?Related:The closest question (and answers) is the first of the following (in particular see @elarson answer), the second looks overly complicated:What is the official ""preferred"" way to install pip and virtualenv systemwide?What's the proper way to install pip, virtualenv, and distribute for Python?Step by step setting up python with pip and virtualenv?System PIP instead of virtualenv PIP by default?but I feel it all fail at answering my question in full: systemwide vs. local, but also should pip or virtualenv go first (and why do they send each one to the other to start with!!!) <code> ","Install pip and virtualenv, a chicken and the egg dilemma?"
How do you select a sprite image from a sprite sheet in python?, I want to import a sprite sheet and select one sprite. How would I do this in Python/pygame? <code> ,How do you select a sprite image from a sprite sheet in Python?
How to append multiple Paths to PYTHONPATH programatically," I have 4 directories: I have another directory with tests having the file testall.pyow, how can I append PATHS, for test1 thru test4 to PYTHONPATH so that I can access the files under test1 thru 4.btw, test1 thru 4 have multiple directories under them where the python files are located.I tried: did not seem to workalso: did not work.basically:from test1.common.api import GenericAPIshould work <code>  /home/user/test1/home/user/test2/home/user/test3/home/user/test4 /home/user/testing import sysimport osPROJECT_ROOT = os.path.dirname(__file__)sys.path.insert(0,os.path.join(PROJECT_ROOT,""test1""))sys.path.insert(1,os.path.join(PROJECT_ROOT,""test2""))sys.path.insert(2,os.path.join(PROJECT_ROOT,""test3""))sys.path.insert(3,os.path.join(PROJECT_ROOT,""test4"")) import syssys.path.append('/home/user/test1','/home/user/test2','/home/user/test3','/home/kahmed/test4')from test1.common.api import GenericAPI",How to append multiple Paths to PYTHONPATH programmatically
Format Nanoseconds in Python, I have some log files with times in the format HH:MM::SS.nano_seconds (e.g. 01:02:03.123456789). I would like to create a datetime in python so I can neatly do math on the time (e.g. take time differences). strptime works well for microseconds using %f. Do the Python datetime and time modules really not support nanoseconds? <code> ,Parsing datetime strings containing nanoseconds
Is it possible to draw a plot vertical with python matplotlib?, I need to draw a plot looks like this:Is it possible? How can I do this? <code> ,Is it possible to draw a plot vertically with python matplotlib?
Tkinter: get Entry content with get()," I'm trying to use an Entry field to get manual input, and then work with that data.All sources I've found claim I should use the get() function, but I haven't found a simple working mini example yet, and I can't get it to work.I hope someone can tel me what I'm doing wrong. Here's a mini file: This gives me an Entry field I can type in, but I can't do anything with the data once it's typed in.I suspect my code doesn't work because initially, entry is empty. But then how do I access input data once it has been typed in? <code>  from tkinter import *master = Tk()Label(master, text=""Input: "").grid(row=0, sticky=W)entry = Entry(master)entry.grid(row=0, column=1)content = entry.get()print(content) # does not workmainloop()",Why is Tkinter Entry's get function returning nothing?
Get content of a Tkinter Entry widget using the get function," I'm trying to use an Entry field to get manual input, and then work with that data.All sources I've found claim I should use the get() function, but I haven't found a simple working mini example yet, and I can't get it to work.I hope someone can tel me what I'm doing wrong. Here's a mini file: This gives me an Entry field I can type in, but I can't do anything with the data once it's typed in.I suspect my code doesn't work because initially, entry is empty. But then how do I access input data once it has been typed in? <code>  from tkinter import *master = Tk()Label(master, text=""Input: "").grid(row=0, sticky=W)entry = Entry(master)entry.grid(row=0, column=1)content = entry.get()print(content) # does not workmainloop()",Why is Tkinter Entry's get function returning nothing?
"Tkinter Entry ""get"" function is returning nothing"," I'm trying to use an Entry field to get manual input, and then work with that data.All sources I've found claim I should use the get() function, but I haven't found a simple working mini example yet, and I can't get it to work.I hope someone can tel me what I'm doing wrong. Here's a mini file: This gives me an Entry field I can type in, but I can't do anything with the data once it's typed in.I suspect my code doesn't work because initially, entry is empty. But then how do I access input data once it has been typed in? <code>  from tkinter import *master = Tk()Label(master, text=""Input: "").grid(row=0, sticky=W)entry = Entry(master)entry.grid(row=0, column=1)content = entry.get()print(content) # does not workmainloop()",Why is Tkinter Entry's get function returning nothing?
Do CSRF attack worries apply to API's?," I'm writing a Django RESTful API to back an iOS application, and I keep running into Django's CSRF protections whenever I write methods to deal with POST requests.My understanding is that cookies managed by iOS are not shared by applications, meaning that my session cookies are safe, and no other application can ride on them. Is this true? If so, can I just mark all my API functions as CSRF-exempt? <code> ",Do CSRF attacks apply to API's?
numpy: frequency counts for unique values in an array," In numpy / scipy, is there an efficient way to get frequency counts for unique values in an array?Something along these lines: ( For you, R users out there, I'm basically looking for the table() function ) <code>  x = array( [1,1,1,2,2,2,5,25,1,1] )y = freq_count( x )print y>> [[1, 5], [2,3], [5,1], [25,1]]",numpy: most efficient frequency counts for unique values in an array
How to install python3 version of package via pip?, I have both python2.7 and python3.2 installed in Ubuntu 12.04.The symbolic link python links to python2.7.When I type: It will default install python2 version of package-name.Some package supports both python2 and python3.How to install python3 version of package-name via pip? <code>  sudo pip install package-name,How to install python3 version of package via pip on Ubuntu?
why does python return 0 for simple calculation with division, Why does this simple calculation return 0 while this actually calculates correctly? What is wrong with the first example? <code>  >>> 25/100*50 0 >>> .25*5012.5>>> 10/2*2 10,Why does Python return 0 for simple division calculation?
Is it possible to multiprocess(Python) a function that returns something?," In Python I have seen many examples where multiprocessing is called but the target just prints something. I have a scenario where the target returns 2 variables, which I need to use later. For example: Now what? I can do .start and .join, but how do I retrieve the individual results? I need to catch the return a,b for all the jobs I execute and then work on it. <code>  def foo(some args): a = someObject b = someObject return a,bp1=multiprocess(target=foo,args(some args))p2=multiprocess(target=foo,args(some args))p3=multiprocess(target=foo,args(some args))",Is it possible to multiprocess a function that returns something in Python?
Django - Change default locale," Trying to understand L10N implementation into Django, Here are my settings If I try It will give me 'Wed May 30 15:30:00 2012' that is the EN locale. However the doc is saying: [...] Two users accessing the same content, but in different language, will see date and number fields formatted in different ways, depending on the format for their current locale [...]Are they talking about the locale set for their respective browser ?If not, how can I set it to french by default for example ? <code>  LANGUAGE_CODE = 'fr-FR'USE_L10N = True >>> datetime.datetime.strptime('2012-05-30 15:30', '%Y-%m-%d %H:%M') .strftime('%c')",System date formatting not using django locale
System date formatting not picking up django locale," Trying to understand L10N implementation into Django, Here are my settings If I try It will give me 'Wed May 30 15:30:00 2012' that is the EN locale. However the doc is saying: [...] Two users accessing the same content, but in different language, will see date and number fields formatted in different ways, depending on the format for their current locale [...]Are they talking about the locale set for their respective browser ?If not, how can I set it to french by default for example ? <code>  LANGUAGE_CODE = 'fr-FR'USE_L10N = True >>> datetime.datetime.strptime('2012-05-30 15:30', '%Y-%m-%d %H:%M') .strftime('%c')",System date formatting not using django locale
"Is Python open(file,r) supposed to update atime?"," Whenever I open() a file with Python, the last access time is not updated, that's very odd :If I open with r/rb nothing changes if I stat the fileIf I open with w/r+ or a the ctime and mtime update properly but not atimeIt doesn't look like it is a filesystem problem (which is ext3 in this case) because if I touch or cat the file it does update properly.I haven't been able to find a lot of information about it; is it supposed to behave this way or is there something wrong? <code> ","Is Python open(file,vr) supposed to update atime?"
Creating Python Email (recieving) server :: really basic," I am trying to produce a simple python script for a Linux VPS that will allow me to receive mail, (and then I can do stuff to it in python, like print it to stdout). Nothing more complex than that.I don't want to use a 'heavy' solution or server program, I am really just after a simple python script that I can run, and is capable of receiving mail.Will Pythons' smtpd module suffice for this task? I have heard conflicting opinions thus far. If not, what else would you suggest? Perhaps you have hacked together some code yourself?At this stage, even projects like lamson seem too heavy (though this may be unavoidable if I cannot find a better solution). <code> ",Creating Python Email (receiving) server
Creating Python Email (receiving) server :: really basic," I am trying to produce a simple python script for a Linux VPS that will allow me to receive mail, (and then I can do stuff to it in python, like print it to stdout). Nothing more complex than that.I don't want to use a 'heavy' solution or server program, I am really just after a simple python script that I can run, and is capable of receiving mail.Will Pythons' smtpd module suffice for this task? I have heard conflicting opinions thus far. If not, what else would you suggest? Perhaps you have hacked together some code yourself?At this stage, even projects like lamson seem too heavy (though this may be unavoidable if I cannot find a better solution). <code> ",Creating Python Email (receiving) server
Python : How do you find the CPU consumption for a piece of code?," BackgroundI have a Django application, it works and responds pretty well on low load, but on high load like 100 users/sec, it consumes 100% CPU and then due to lack of CPU slows down.Problem:Profiling the application gives me time taken by functions.This time increases on high load.Time consumed may be due to complex calculation or for waiting for CPU.So, how to find the CPU cycles consumed by a piece of code ?Since reducing the CPU consumption will increase the response time.I might have written extremely efficient code and need to add more CPU power ORI might have some stupid code taking the CPU and causing the slow down ?UpdateI am using Jmeter to profile my web app, it gives me a throughput of 2 requests/sec. [ 100 users]I get a average time of 36 seconds on 100 request vs 1.25 sec time on 1 request.More InfoConfiguration Nginx + Uwsgi with 4 workersNo database used, using a responses from a REST APIOn 1st hit the response of REST API gets cached, therefore doesn't makes a difference.Using ujson for json parsing.Curious to know:Python-Django is used by so many orgs for so many big sites, then there must be some high end Debug / Memory-CPU analysis tools.All those I found were casual snippets of code that perform profiling. <code> ",How do you find the CPU consumption for a piece of Python?
"Script gets interrupted by exception ""Message: u'Modal dialog present'"""," I'm fairly new at Python/JS and also automated testing with Selenium/WebDriver, but I have made some progress!Now I'm stuck at one point and it's really frustrating.The website I am testing sells products. I managed to make my script navigate randomly and get to the payment page, fill in dummy data, submit data by using: Usually, there is a Pay now button and clicking that element results in the same exception and there was no way for me to click OK/Cancel on it via WebDriver (no WebElement), but I figured out that executing this JS code I can get past it. My newly loaded page (after submitting data and confirming the posting of it) with a confirmation and all correct data loads, but the Python script is interrupted and I can't continue the test.Is there a workaround for this? What I want it to do is to ignore that modal dialog, wait for the next confirmation page to load and then continue locating elements, printing their values, storing them etc.Tried using: but the script gets interrupted. Sorry if this has been answered, but I couldn't find it, and also I am a newbie!Thanks in advance!EDIT:Did it! In my case what worked is I just modified my code a little Note for newbies that you will need to import Alert. <code>  browser.execute_script(""document.Form.submit(); return true;"")browser.execute_script(""processPayment(); return true;"") wait = ui.WebDriverWait(browser,10)wait.until(lambda browser: browser.title.lower().startswith('Your Receipt'))print(browser.title) browser.execute_script(""document.roomBookingForm.submit(); return true;"")alert = browser.switch_to_alert()alert.dismiss()browser.execute_script(""processPayment(); return true;"") from selenium.webdriver.common.alert import Alert","Selenium/WebDriver script gets interrupted by alert - exception ""Message: u'Modal dialog present'"""
"Selenium/WebDriver script gets interrupted by exception ""Message: u'Modal dialog present'"""," I'm fairly new at Python/JS and also automated testing with Selenium/WebDriver, but I have made some progress!Now I'm stuck at one point and it's really frustrating.The website I am testing sells products. I managed to make my script navigate randomly and get to the payment page, fill in dummy data, submit data by using: Usually, there is a Pay now button and clicking that element results in the same exception and there was no way for me to click OK/Cancel on it via WebDriver (no WebElement), but I figured out that executing this JS code I can get past it. My newly loaded page (after submitting data and confirming the posting of it) with a confirmation and all correct data loads, but the Python script is interrupted and I can't continue the test.Is there a workaround for this? What I want it to do is to ignore that modal dialog, wait for the next confirmation page to load and then continue locating elements, printing their values, storing them etc.Tried using: but the script gets interrupted. Sorry if this has been answered, but I couldn't find it, and also I am a newbie!Thanks in advance!EDIT:Did it! In my case what worked is I just modified my code a little Note for newbies that you will need to import Alert. <code>  browser.execute_script(""document.Form.submit(); return true;"")browser.execute_script(""processPayment(); return true;"") wait = ui.WebDriverWait(browser,10)wait.until(lambda browser: browser.title.lower().startswith('Your Receipt'))print(browser.title) browser.execute_script(""document.roomBookingForm.submit(); return true;"")alert = browser.switch_to_alert()alert.dismiss()browser.execute_script(""processPayment(); return true;"") from selenium.webdriver.common.alert import Alert","Selenium/WebDriver script gets interrupted by alert - exception ""Message: u'Modal dialog present'"""
google.appengine.db.UnindexedProperty strange code," Please help me understand this:On v1.6.6 it's in line 2744 of google/appengine/ext/db/__init__.py: After they constrained the indexed parameter to be False - They set it to True! <code>  class UnindexedProperty(Property): """"""A property that isn't indexed by either built-in or composite indices. TextProperty and BlobProperty derive from this class. """""" def __init__(self, *args, **kwds): """"""Construct property. See the Property class for details. Raises: ConfigurationError if indexed=True. """""" self._require_parameter(kwds, 'indexed', False) kwds['indexed'] = True super(UnindexedProperty, self).__init__(*args, **kwds) . . .",App Engine's UnindexedProperty contains strange code
Python - Slice Assignment with a String in a List," I did quite a bit of perusing, but I don't have a definite answer for the concept that I'm trying to understand.In Python, if I take a list, such as: And then attempted to replace the first pointer to an object in the list, namely 'muffins' by using the code: I would get a list L1: Yet if I took the same list and performed the operation (now with the 4 elements from the string cake): I get the output I initially desired: Can anyone explain why that is, exactly?I'm assuming that when I take cake initially without it being in a ""list"", it breaks the string into its individual characters to be stored as references to those characters as opposed to a single reference to a string...But I'm not entirely sure. <code>  L1=['muffins', 'brownies','cookies'] L1[0:1] = 'cake' ['c', 'a', 'k', 'e', 'brownies', 'cookies'] L1[0:4] = ['cake'] # presumably, it's now passing the string cake within a list? (it passed into the modified list shown above) ['cake', 'brownies', 'cookies']",Slice Assignment with a String in a List
What is introspection based code completion?, I'm looking at various IDEs for python. Looking at the official list the IDEs are categorized based on 'introspection based code completion'. What does introspection based code completion mean? Thanks.  <code> ,"What is ""Introspection-based code completion""?"
why can't I use string functions inside map()?," The following example shows the error I am getting when trying to use a string function inside a function call to map. I need help with why this happening. Thanks. though split() is an in-built function, but it still throws this error? <code>  >>> s=[""this is a string"",""python python python"",""split split split""]>>> map(split,s)Traceback (most recent call last): File ""<pyshell#16>"", line 1, in <module> map(split,s)NameError: name 'split' is not defined",Why can't I use string functions inside map()?
Python scipy machine learning (sklearn) model parameters really unavailable?," I am doing machine learning using scikit-learn as recommended in this question. To my surprise, it does not appear to provide access to the actual models it trains. For example, if I create an SVM, linear classifier or even a decision tree, it doesn't seem to provide a way for me to see the parameters selected for the actual trained model. Seeing the actual model is useful if the model is being created partly to get a clearer picture of what features it is using (e.g., decision trees). Seeing the model is also a significant issue if one wants to use Python to train the model and some other code to actually implement it.Am I missing something in scikit-learn or is there some way to get at this in scikit-learn? If not, what is the a good free machine learning workbench, not necessarily in python, in which models are transparently available?  <code> ",Scikit-learn model parameters unavailable? If so what ML workbench alternative?
Scikits.learn model parameters unavailable? If not what ML workbench alternative?," I am doing machine learning using scikit-learn as recommended in this question. To my surprise, it does not appear to provide access to the actual models it trains. For example, if I create an SVM, linear classifier or even a decision tree, it doesn't seem to provide a way for me to see the parameters selected for the actual trained model. Seeing the actual model is useful if the model is being created partly to get a clearer picture of what features it is using (e.g., decision trees). Seeing the model is also a significant issue if one wants to use Python to train the model and some other code to actually implement it.Am I missing something in scikit-learn or is there some way to get at this in scikit-learn? If not, what is the a good free machine learning workbench, not necessarily in python, in which models are transparently available?  <code> ",Scikit-learn model parameters unavailable? If so what ML workbench alternative?
Scikits.learn model parameters unavailable? If so what ML workbench alternative?," I am doing machine learning using scikit-learn as recommended in this question. To my surprise, it does not appear to provide access to the actual models it trains. For example, if I create an SVM, linear classifier or even a decision tree, it doesn't seem to provide a way for me to see the parameters selected for the actual trained model. Seeing the actual model is useful if the model is being created partly to get a clearer picture of what features it is using (e.g., decision trees). Seeing the model is also a significant issue if one wants to use Python to train the model and some other code to actually implement it.Am I missing something in scikit-learn or is there some way to get at this in scikit-learn? If not, what is the a good free machine learning workbench, not necessarily in python, in which models are transparently available?  <code> ",Scikit-learn model parameters unavailable? If so what ML workbench alternative?
"Simple python regex, match after semicolon"," I have a simple regex question that's driving me crazy.I have a variable x = ""field1: XXXX field2: YYYY"".I want to retrieve YYYY (note that this is an example value).My approach was as follows: It's not matching anything. Can I get some help with this? Thanks! <code>  values = re.match('field2:\s(.*)', x)print values.groups()","Simple python regex, match after colon"
remove pytz timezone," Is there a simple way to remove the timezone from a pytz datetime object?e.g. reconstructing dt from dt_tz in this example: <code>  >>> import datetime>>> import pytz>>> dt = datetime.datetime.now()>>> dtdatetime.datetime(2012, 6, 8, 9, 27, 32, 601000)>>> dt_tz = pytz.utc.localize(dt)>>> dt_tzdatetime.datetime(2012, 6, 8, 9, 27, 32, 601000, tzinfo=<UTC>)",How can I remove a pytz timezone from a datetime object?
Use python to generate html list based on directory tree," I am having some problems using Python to generate an html document. I am attempting to create an HTML list of a directory tree. This is what I have so far: It seems to work well if there is only the root directory, one level of sub-directories and files. However, adding another level of sub-directories causes there to be problems (because the close tag isn't input enough times at the end I think). But I'm having a hard time getting my head around it.If it can't be done this way, is there an easier way to do it? I'm using Flask but I'm very inexperienced with templates so perhaps I'm missing something. <code>  def list_files(startpath): for root, dirs, files in os.walk(startpath): level = root.replace(startpath, '').count(os.sep) if level <= 1: print('<li>{}<ul>'.format(os.path.basename(root))) else: print('<li>{}'.format(os.path.basename(root))) for f in files: last_file = len(files)-1 if f == files[last_file]: print('<li>{}</li></ul>'.format(f)) elif f == files[0] and level-1 > 0: print('<ul><li>{}</li>'.format(f)) else: print('<li>{}</li>'.format(f)) print('</li></ul>')",How to generate an html directory list using Python
"Tkinter, opening and reading a file"," I have the following code where I'm trying to allow the user to open a text file and once the user has selected it, I would like the code to read it (this isn't a finished block of code, just to show what I'm after).However, I'm having difficulties either using tkFileDialog.askopenfilename and adding 'mode='rb'' or using the code like below and using read where it produces an error.Does anyone know how I can arrange to do this as I don't wish to have to type Tkinter.'module' for each item such as Menu and Listbox. Beginner to Tkinter and a bit confused! Thanks for the help! Obviously the error I'm getting here is: I don't understand how to use the askopen and also be able to read the file I'm opening. <code>  import sysfrom Tkinter import *import tkFileDialogfrom tkFileDialog import askopenfilename # Open dialog boxfen1 = Tk() # Create windowfen1.title(""Optimisation"") #menu1 = Menu(fen1)def open(): filename = askopenfilename(filetypes=[(""Text files"",""*.txt"")]) txt = filename.read() print txt filename.close()fen1.mainloop() AttributeError: 'unicode' object has no attribute 'read'",Opening and reading a file with askopenfilename
'classmethod' object is not callable (Python)," I have this code: When I run this I keep getting the error: I am unable to figure out what is wrong with this and would appreciate your help. <code>  class SomeClass: @classmethod def func1(cls,arg1): #---Do Something--- @classmethod def func2(cls,arg1): #---Do Something--- # A 'function map' that has function name as its keys and the above function # objects as values func_map={'func1':func1,'func2':func2} @classmethod def func3(cls,arg1): # following is a dict(created by reading a config file) that # contains func names as keys and boolean as values that tells # the program whether or not to run that function global funcList for func in funcList: if funcList[func]==True: cls.func_map[func](arg1) #TROUBLING PART!!! if _name__='main' SomeClass.func3('Argumentus-Primus') Exception TypeError: ""'classmethod' object is not callable""",Various errors in code that tries to call classmethods
'classmethod' object is not callable," I have this code: When I run this I keep getting the error: I am unable to figure out what is wrong with this and would appreciate your help. <code>  class SomeClass: @classmethod def func1(cls,arg1): #---Do Something--- @classmethod def func2(cls,arg1): #---Do Something--- # A 'function map' that has function name as its keys and the above function # objects as values func_map={'func1':func1,'func2':func2} @classmethod def func3(cls,arg1): # following is a dict(created by reading a config file) that # contains func names as keys and boolean as values that tells # the program whether or not to run that function global funcList for func in funcList: if funcList[func]==True: cls.func_map[func](arg1) #TROUBLING PART!!! if _name__='main' SomeClass.func3('Argumentus-Primus') Exception TypeError: ""'classmethod' object is not callable""",Various errors in code that tries to call classmethods
various errors in code that tries to call classmethods," I have this code: When I run this I keep getting the error: I am unable to figure out what is wrong with this and would appreciate your help. <code>  class SomeClass: @classmethod def func1(cls,arg1): #---Do Something--- @classmethod def func2(cls,arg1): #---Do Something--- # A 'function map' that has function name as its keys and the above function # objects as values func_map={'func1':func1,'func2':func2} @classmethod def func3(cls,arg1): # following is a dict(created by reading a config file) that # contains func names as keys and boolean as values that tells # the program whether or not to run that function global funcList for func in funcList: if funcList[func]==True: cls.func_map[func](arg1) #TROUBLING PART!!! if _name__='main' SomeClass.func3('Argumentus-Primus') Exception TypeError: ""'classmethod' object is not callable""",Various errors in code that tries to call classmethods
UnicodeEncodeError when fetching an url," I have this issue trying to get all the text nodes in an HTML document using lxml but I get an UnicodeEncodeError: 'ascii' codec can't encode character u'\xe9' in position 8995: ordinal not in range(128). However, when I try to find out the type of encoding of this page (encoding = chardet.detect(response)['encoding']), it says it's utf-8. It seems weird that a single page has utf-8 and ascii. Actually, this: solves the problem.Here it's my code: Output: What can I do to solve this issue?. Keep in mind that I want to do this with a few other pages, so I don't want to encode on an individual basis.UPDATE:Maybe there is something else going on here. When I run this script on the terminal, I get a correct output but when a run it inside SublimeText, I get UnicodeEncodeError... ?UPDATE2:It's also happening when I create a file with this output. .encode('ascii', 'replace') is working but I'd like to have a more general solution.Regards <code>  fromstring(response).text_content().encode('ascii', 'replace') from lxml.html import fromstringimport urllib2import chardetrequest = urllib2.Request(my_url)request.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0)') request.add_header(""Accept-Language"", ""en-us"")response = urllib2.urlopen(request).read()print encodingprint fromstring(response).text_content() utf-8UnicodeEncodeError: 'ascii' codec can't encode character u'\xe9' in position 8995: ordinal not in range(128)",UnicodeEncodeError when fetching url
Check if file is symlink in python," In Python, is there a function to check if a given file/directory is a symlink? For example, for the below files, my wrapper function should return True. <code>  # ls -ltotal 0lrwxrwxrwx 1 root root 8 2012-06-16 18:58 dir -> ../temp/lrwxrwxrwx 1 root root 6 2012-06-16 18:55 link -> ../log",How to check if file is a symlink in Python?
Use Git Commands from python code," I have been asked to write a script that pulls the latest code from Git, makes a build, and performs some automated unit tests.I found that there are two built-in Python modules for interacting with Git that are readily available: GitPython and libgit2.What approach/module should I use? <code> ",Use Git commands within Python code
Why can't I repeat the 'for' loop for csv.Reader? (Python)," I am a beginner of Python. I am trying now figuring out why the second 'for' loop doesn't work in the following script. I mean that I could only get the result of the first 'for' loop, but nothing from the second one. I copied and pasted my script and the data csv in the below. It will be helpful if you tell me why it goes in this way and how to make the second 'for' loop work as well. My SCRIPT: ""data.csv"": <code>  import csvfile = ""data.csv""fh = open(file, 'rb')read = csv.DictReader(fh)for e in read: print(e['a'])for e in read: print(e['b']) a,b,ctree,bough,trunkanimal,leg,trunkfish,fin,body",Why can't I repeat the 'for' loop for csv.Reader?
Best idom to get and set a value in a python dict," I use a dict as a short-term cache. I want to get a value from the dictionary, and if the dictionary didn't already have that key, set it, e.g.: In the case where 'the-key' was already in cache, the second line is not necessary. Is there a better, shorter, more expressive idiom for this? <code>  val = cache.get('the-key', calculate_value('the-key'))cache['the-key'] = val",Best idiom to get and set a value in a python dict
why I can't end a raw string with a \," I am confused here, even though raw strings convert every \ to \\ but when this \ appears in the end it raises error. Update:This is now covered in Python FAQs as well: Why cant raw strings (r-strings) end with a backslash? <code>  >>> r'so\m\e \te\xt''so\\m\\e \\te\\xt'>>> r'so\m\e \te\xt\'SyntaxError: EOL while scanning string literal",Why can't I end a raw string with a backslash?
why can't I end a raw string with a \," I am confused here, even though raw strings convert every \ to \\ but when this \ appears in the end it raises error. Update:This is now covered in Python FAQs as well: Why cant raw strings (r-strings) end with a backslash? <code>  >>> r'so\m\e \te\xt''so\\m\\e \\te\\xt'>>> r'so\m\e \te\xt\'SyntaxError: EOL while scanning string literal",Why can't I end a raw string with a backslash?
how tell python script to use particular version," How do I, in the main.py module (presumably), tell Python which interpreter to use? What I mean is: if I want a particular script to use version 3 of Python to interpret the entire program, how do I do that?Bonus: How would this affect a virtualenv? Am I right in thinking that if I create a virtualenv for my program and then tell it to use a different version of Python, then I may encounter some conflicts? <code> ",How do I tell a Python script to use a particular version
Python Regular Expression example," I want to write a simple regular expression in Python that extracts a number from HTML. The HTML sample is as follows: Now, how can I extract ""123"", i.e. the contents of the first bold text after the string ""Your number is""? <code>  Your number is <b>123</b>",How to use regex to parse a number from HTML?
Python: What's the difference between __builtin__ and __builtins__?," I was coding today and noticed something. If I open a new interpreter session (IDLE) and check what's defined with the dir function I get this: Please note the last line.So, my question is:Is any an alias of the other one?Are the Python guys planning to get rid of one of those?What should I use for my own programs?What about Python 3?Any information is valuable!Important:I'm using Python 2.7.2+ on Ubuntu. <code>  $ python>>> dir()['__builtins__', '__doc__', '__name__', '__package__']>>> dir(__builtins__)['ArithmeticError', 'AssertionError', 'AttributeError', 'BaseException', 'BufferError', 'BytesWarning', 'DeprecationWarning', 'EOFError', 'Ellipsis', 'EnvironmentError', 'Exception', 'False', 'FloatingPointError', 'FutureWarning', 'GeneratorExit', 'IOError', 'ImportError', 'ImportWarning', 'IndentationError', 'IndexError', 'KeyError', 'KeyboardInterrupt', 'LookupError', 'MemoryError', 'NameError', 'None', 'NotImplemented', 'NotImplementedError', 'OSError', 'OverflowError', 'PendingDeprecationWarning', 'ReferenceError', 'RuntimeError', 'RuntimeWarning', 'StandardError', 'StopIteration', 'SyntaxError', 'SyntaxWarning', 'SystemError', 'SystemExit', 'TabError', 'True', 'TypeError', 'UnboundLocalError', 'UnicodeDecodeError', 'UnicodeEncodeError', 'UnicodeError', 'UnicodeTranslateError', 'UnicodeWarning', 'UserWarning', 'ValueError', 'Warning', 'ZeroDivisionError', '_', '__debug__', '__doc__', '__import__', '__name__', '__package__', 'abs', 'all', 'any', 'apply', 'basestring', 'bin', 'bool', 'buffer', 'bytearray', 'bytes', 'callable', 'chr', 'classmethod', 'cmp', 'coerce', 'compile', 'complex', 'copyright', 'credits', 'delattr', 'dict', 'dir', 'divmod', 'enumerate', 'eval', 'execfile', 'exit', 'file', 'filter', 'float', 'format', 'frozenset', 'getattr', 'globals', 'hasattr', 'hash', 'help', 'hex', 'id', 'input', 'int', 'intern', 'isinstance', 'issubclass', 'iter', 'len', 'license', 'list', 'locals', 'long', 'map', 'max', 'memoryview', 'min', 'next', 'object', 'oct', 'open', 'ord', 'pow', 'print', 'property', 'quit', 'range', 'raw_input', 'reduce', 'reload', 'repr', 'reversed', 'round', 'set', 'setattr', 'slice', 'sorted', 'staticmethod', 'str', 'sum', 'super', 'tuple', 'type', 'unichr', 'unicode', 'vars', 'xrange', 'zip']>>> import __builtin__['ArithmeticError', 'AssertionError', 'AttributeError', 'BaseException', 'BufferError', 'BytesWarning', 'DeprecationWarning', 'EOFError', 'Ellipsis', 'EnvironmentError', 'Exception', 'False', 'FloatingPointError', 'FutureWarning', 'GeneratorExit', 'IOError', 'ImportError', 'ImportWarning', 'IndentationError', 'IndexError', 'KeyError', 'KeyboardInterrupt', 'LookupError', 'MemoryError', 'NameError', 'None', 'NotImplemented', 'NotImplementedError', 'OSError', 'OverflowError', 'PendingDeprecationWarning', 'ReferenceError', 'RuntimeError', 'RuntimeWarning', 'StandardError', 'StopIteration', 'SyntaxError', 'SyntaxWarning', 'SystemError', 'SystemExit', 'TabError', 'True', 'TypeError', 'UnboundLocalError', 'UnicodeDecodeError', 'UnicodeEncodeError', 'UnicodeError', 'UnicodeTranslateError', 'UnicodeWarning', 'UserWarning', 'ValueError', 'Warning', 'ZeroDivisionError', '_', '__debug__', '__doc__', '__import__', '__name__', '__package__', 'abs', 'all', 'any', 'apply', 'basestring', 'bin', 'bool', 'buffer', 'bytearray', 'bytes', 'callable', 'chr', 'classmethod', 'cmp', 'coerce', 'compile', 'complex', 'copyright', 'credits', 'delattr', 'dict', 'dir', 'divmod', 'enumerate', 'eval', 'execfile', 'exit', 'file', 'filter', 'float', 'format', 'frozenset', 'getattr', 'globals', 'hasattr', 'hash', 'help', 'hex', 'id', 'input', 'int', 'intern', 'isinstance', 'issubclass', 'iter', 'len', 'license', 'list', 'locals', 'long', 'map', 'max', 'memoryview', 'min', 'next', 'object', 'oct', 'open', 'ord', 'pow', 'print', 'property', 'quit', 'range', 'raw_input', 'reduce', 'reload', 'repr', 'reversed', 'round', 'set', 'setattr', 'slice', 'sorted', 'staticmethod', 'str', 'sum', 'super', 'tuple', 'type', 'unichr', 'unicode', 'vars', 'xrange', 'zip']>>> dir(__builtin__) == dir(__builtins__) # They seem to have the same thingsTrue",What's the difference between __builtin__ and __builtins__?
Kindly help me with this error in python," it shows a runtime error : I am just a beginner at python, and I am unable to rectify this even after searching on the net. <code>  import mathimport osclass collection: col = [[0 for col in range(5)] for row in range(6)] dist = [[0 for col in range(6)] for row in range(6)] filename = """" result = """" def __init__(self,arg1): self.filename = arg1 def coll(self): for i in range(6): try: if(i==0): f = open(self.filename,'r') elif(i==1): f = open(""chap1.txt"",'r') elif(i==2): f = open(""chap2.txt"",'r') elif(i==3): f = open(""chap3.txt"",'r') elif(i==4): f = open(""chap4.txt"",'r') elif(i==5): f = open(""chap5.txt"",'r') for j in range(5): self.result = f.readline() self.col[i][j] = self.result finally: print ""file handling error"" def distance(self): for i in range[6]: for j in range[6]: dis = 0 for k in range[5]: dis += math.fabs((self.col[i][k]-self.col[j][k])*(j-i)) self.dist[i][j] = dis self.dist[i][i] = sys.maxdouble return self.distclass profile: dist = [[0 for col in range(6)]for row in range(6)] filename = """" pque = [[0 for col in range(6)]for row in range(6)] d = [[0 for col in range(6)]for row in range(6)] par = [[0 for col in range(6)]for row in range(6)] st = 0 def __init__(self,arg1): self.filename = arg1 def begin(self): ob = collection(self.filename) ob.coll() dist = ob.distance() def sssp(self): for i in range(6): pque[i] = sys.maxdouble d[i] = sys.maxdouble d[0] = 0 pque[0] = 0 while isempty()==0: u = extract_min() for i in range(6): if d[i]>d[u]+dist[u][i]: d[i] = d[u]+dist[u][i] pque_deckey(i,d[i]) par[i]=u if u!=0: print u print ""\n"" for i in range(6): print par[i] def extract_min(): ret = 0 shift = 0 minimum = pque[0] for i in range(6): if pque[i]<minimum: minimum = pque[i] ret = i pque[ret] = sys.maxdouble return ret def isempty(self): count = 0 for i in range(6): if pque[i] == sys.maxdouble: count=count+1 if count==6: return 1 else : return 0 def pque_deckey(self,im,di): pque[im]=diclass main: filename = raw_input(""enter name of student:\n"") filename = filename + "".txt"" if(os.path.exists(filename)==1): f = file(filename,""r"") else: f = file(filename,""w+"") att1 = raw_input(""att1 score:\n"") att2 = raw_input(""att2 score:\n"") att3 = raw_input(""att3 score:\n"") att4 = raw_input(""att4 score:\n"") att5 = raw_input(""att5 score:\n"") f.write(att1) f.write(""\n"") f.write(att2) f.write(""\n"") f.write(att3) f.write(""\n"") f.write(att4) f.write(""\n"") f.write(att5) f.write(""\n"") stud = profile(filename) stud.begin() stud.sssp() File ""C:\Python27\winculum.py"", line 33, in coll self.col[i][j] = self.resultTypeError: 'int' object has no attribute '__getitem__'",'int' object has no attribute '__getitem__'
has_header from csv.Sniffer gives different results for identical looking files," I have the following snippet of code: Where first_lines are the first 2048 bytes of the file. The function works well most of the time and returns True for a file that begins like this: However, the function returns False for the following file that has the same overall structure: Is this a bug in has_header or some edge-case where the heuristics of has_header fail?Some differences I have noted:the variable header on line 394 in csv.py is ['SPEC#: 1, SIZE: 18473, TIME: 0.000000'] (list of length 1) for the first file where the header is correctly determined and ['SPEC#: 1, SIZE: 184', '4, TIME: 0.000000'] (list of length 2) for the second file.after creating a dictionary of types of data for each column, the variable columnTypes is {0: None} for the first file and {} for the second file. <code>  import csvdef has_header(first_lines): sniffer = csv.Sniffer() return sniffer.has_header(first_lines) SPEC#: 1, SIZE: 18473, TIME: 0.0000001998.304312 2.156861998.773585 3.137251999.242914 3.137251999.712298 2.74512000.181736 2.941182000.651230 2.941182001.120780 2.156862001.590384 2.352942002.060043 2.941182002.529758 3.137252002.999527 2.549022003.469352 3.137252003.939232 1.960782004.409167 1.764712004.879158 2.941182005.349203 3.725492005.819304 3.333332006.289459 2.352942006.759670 1.764712007.229936 3.137252007.700258 3.529412008.170634 3.921572008.641065 3.921572009.111552 3.529412009.582094 4.705882010.052691 3.529412010.523343 3.333332010.994050 1.372552011.464812 2.352942011.935630 2.156862012.406502 3.529412012.877430 3.137252013.348413 2.156862013.819451 1.960782014.290544 1.568632014.761693 3.137252015.232896 1.764712015.704155 1.960782016.175469 3.333332016.646838 4.901962017.118262 3.529412017.589741 2.941182018.061275 1.960782018.532865 1.764712019.004510 4.117652019.476210 3.725492019.947965 2.352942020.419775 1.568632020.891640 2.156862021.363560 0.7843142021.835536 1.372552022.307567 2.941182022.779653 2.156862023.251794 4.117652023.723990 4.50982024.196241 2.74512024.668548 2.549022025.140909 1.568632025.613326 2.941182026.085798 2.352942026.558325 2.941182027.030907 3.333332027.503545 3.529412027.976237 3.725492028.448985 5.098042028.921788 4.117652029.394645 3.921572029.867559 3.137252030.340527 2.156862030.813550 2.352942031.286629 5.098042031.759762 3.333332032.232951 3.529412032.706195 3.137252033.179494 4.705882033.652849 4.313732034.126258 3.921572034.599723 3.333332035.073242 3.725492035.546817 2.549022036.020447 2.352942036.494132 2.156862036.967873 2.941182037.441668 2.74512037.915519 2.156862038.389425 2.549022038.863385 2.549022039.337401 2.352942039.811473 3.529412040.285599 3.529412040.759781 4.117652041.234017 3.529412041.708309 3.725492042.182656 3.333332042.657058 2.549022043.131515 4.117652043.606028 3.529412044.080595 4.117652044.555218 2.352942045.029896 1.96078 SPEC#: 1, SIZE: 18474, TIME: 0.0000001998.228113 36.86271998.697368 30.19611999.166679 35.88241999.636044 41.37262000.105465 38.62752000.574941 39.01962001.044473 41.96082001.514059 37.84312001.983701 35.0982002.453397 37.25492002.923149 36.47062003.392956 40.58822003.862818 39.01962004.332735 36.66672004.802708 33.33332005.272735 37.84312005.742818 35.0982006.212955 33.33332006.683148 39.01962007.153397 41.37262007.623700 41.76472008.094058 39.41182008.564472 40.78432009.034940 44.11762009.505464 42.15692009.976043 402010.446677 38.23532010.917366 39.21572011.388111 39.21572011.858910 36.27452012.329765 38.03922012.800675 42.35292013.271640 44.70592013.742660 38.03922014.213735 402014.684866 39.80392015.156051 44.9022015.627292 41.76472016.098588 44.31372016.569939 43.92162017.041345 50.19612017.512806 51.56862017.984323 46.47062018.455894 44.70592018.927521 41.96082019.399203 46.66672019.870940 41.37262020.342732 45.88242020.814579 45.29412021.286482 45.29412021.758439 49.21572022.230452 42.15692022.702520 45.29412023.174643 45.49022023.646821 40.58822024.119054 47.05882024.591343 42.15692025.063686 43.72552025.536085 46.07842026.008539 45.0982026.481048 44.9022026.953612 502027.426231 48.43142027.898906 45.0982028.371636 49.21572028.844420 47.84312029.317260 51.76472029.790155 49.41182030.263105 45.88242030.736111 51.17652031.209171 47.64712031.682287 52.5492032.155458 50.78432032.628684 45.68632033.101965 48.62752033.575301 49.41182034.048692 48.23532034.522139 49.01962034.995641 51.96082035.469197 51.37262035.942809 50.19612036.416476 54.31372036.890199 502037.363976 48.82352037.837809 49.41182038.311696 51.96082038.785639 55.0982039.259637 56.27452039.733690 50.78432040.207798 55.29412040.681962 58.23532041.156180 56.86272041.630454 602042.104783 61.17652042.579167 64.31372043.053606 60.58822043.528100 64.31372044.002650 62.15692044.477254 60.19612044.951914 68.23532045.426629 62.15692045.901399 62.7451",has_header from csv.Sniffer gives different results for files with same layout
Python: regex query," I'm trying to make a test for checking whether a sys.argv input matches the RegEx for an IP address...As a simple test, I have the following... However when I pass random values into it, it returns ""Acceptable IP address"" in most cases, except when I have an ""address"" that is basically equivalent to \d+. <code>  import repat = re.compile(""\d{1,3}.\d{1,3}.\d{1,3}.\d{1,3}"")test = pat.match(hostIP)if test: print ""Acceptable ip address""else: print ""Unacceptable ip address""",Using a RegEx to match IP addresses
Using a regex to match IP addresses in Python," I'm trying to make a test for checking whether a sys.argv input matches the RegEx for an IP address...As a simple test, I have the following... However when I pass random values into it, it returns ""Acceptable IP address"" in most cases, except when I have an ""address"" that is basically equivalent to \d+. <code>  import repat = re.compile(""\d{1,3}.\d{1,3}.\d{1,3}.\d{1,3}"")test = pat.match(hostIP)if test: print ""Acceptable ip address""else: print ""Unacceptable ip address""",Using a RegEx to match IP addresses
Using a RegEx to match IP addresses in Python," I'm trying to make a test for checking whether a sys.argv input matches the RegEx for an IP address...As a simple test, I have the following... However when I pass random values into it, it returns ""Acceptable IP address"" in most cases, except when I have an ""address"" that is basically equivalent to \d+. <code>  import repat = re.compile(""\d{1,3}.\d{1,3}.\d{1,3}.\d{1,3}"")test = pat.match(hostIP)if test: print ""Acceptable ip address""else: print ""Unacceptable ip address""",Using a RegEx to match IP addresses
Selecting columns," I have data in different columns, but I don't know how to extract it to save it in another variable. How do I select 'a', 'b' and save it in to df1?I tried None seem to work. <code>  index a b c1 2 3 42 3 4 5 df1 = df['a':'b']df1 = df.ix[:, 'a':'b']",Selecting multiple columns in a Pandas dataframe
Selecting columns in a pandas dataframe," I have data in different columns, but I don't know how to extract it to save it in another variable. How do I select 'a', 'b' and save it in to df1?I tried None seem to work. <code>  index a b c1 2 3 42 3 4 5 df1 = df['a':'b']df1 = df.ix[:, 'a':'b']",Selecting multiple columns in a Pandas dataframe
Selecting multiple columns in a pandas dataframe," I have data in different columns, but I don't know how to extract it to save it in another variable. How do I select 'a', 'b' and save it in to df1?I tried None seem to work. <code>  index a b c1 2 3 42 3 4 5 df1 = df['a':'b']df1 = df.ix[:, 'a':'b']",Selecting multiple columns in a Pandas dataframe
Read two textfile line by line simultaneously -python," I have two text files in two different languages and they are aligned line by line. I.e. the first line in textfile1 corresponds to the first line in textfile2, and so on and so forth.Is there a way to read both file line-by-line simultaneously? Below is a sample of how the files should look like, imagine the number of lines per file is around 1,000,000. textfile1: textfile2: desired output There is a Java version of this Read two textfile line by line simultaneously -java, but Python doesn't use bufferedreader that reads line by line. So how would it be done? <code>  This is a the first line in EnglishThis is a the 2nd line in EnglishThis is a the third line in English C'est la premire ligne en FranaisC'est la deuxime ligne en FranaisC'est la troisime ligne en Franais This is a the first line in English\tC'est la premire ligne en FranaisThis is a the 2nd line in English\tC'est la deuxime ligne en FranaisThis is a the third line in English\tC'est la troisime ligne en Franais",Reading two text files line by line simultaneously
python - how to get the item currently pointed at by iterator without incrementing?, Is there a way to get the item pointed at by an iterator in python without incrementing the iterator itself? For example how would I implement the following with iterators: <code>  looking_for = iter(when_to_change_the_mode)for l in listA: do_something(looking_for.current()) if l == looking_for.current(): next(looking_for),How to get the item currently pointed at by iterator without incrementing?
"Wtforms, add a class to a form dinamicly"," is there a way i could send a form's (css) class from python?For example: This renders a simple text field, but i want that text field to have the css class of .companyName, is that possible directly from python?I know that i can put a id=""companyName"" directly from python, but not class.Help.Update: I tried class_=""companyName"" and it did not work, i got: <code>  class Company(Form): companyName = TextField('Company Name', [validators.Length(min=3, max = 60)]) __init__() got an unexpected keyword argument '_class'","Wtforms, add a class to a form dynamically"
how to know if object gets deleted in python," I have an object in the heap and a reference to it. There are certain circumstances in which the object gets deleted but the reference that points to its location doesn't know that. How can I check if there is real data in the heap?For example: Then the a object gets deleted but print(a) returns a valid address. However if you try a.value() - runtime error occurs (C++ object already deleted).a is None returns False. <code>  from PySide import *a = QProgressBar()b = QProgressBar()self.setIndexWidget(index,a)self.setIndexWidget(index,b)",How to know if object gets deleted in Python
How to wrap every method of a class in python?," I'd like to wrap every method of a particular class in python, and I'd like to do so by editing the code of the class minimally. How should I go about this? <code> ",How to wrap every method of a class?
python: TypeError: object of type 'NoneType' has no len()," I am getting an error from this Python code: Error: The assert statement is throwing this error, what am I doing wrong? <code>  with open('names') as f: names = f.read() names = names.split('\n') names.pop(len(names) - 1) names = shuffle(names) f.close()assert len(names) > 100 Python: TypeError: object of type 'NoneType' has no len()",Python: TypeError: object of type 'NoneType' has no len()
Tabs in ttk.Notebook for python: Is there a way to set tabs below one another?," So far when using the ttk.Notebook widget, but I am unable to set tabs below one another, they keep piling up eastward. Is there a way to set them to stack somehow? <code> ",Is there a way to set tabs of a Notebook below one another?
Django celery reterive task status using Ajax," I'm using celery 2.5.3 and django celery - 2.5.5. And I'm using mysql as the broker.Here is the scenario, when the user request i queue a job in the worker which is getting data from another site. And this may take few minutes depending upon the size of the data. Once the job is started we have to show a loader image. And when worker finishes downloading data (which will be in html format) i have to replace the loader image with data retrieved.The reason we are using celery is that sometimes script takes more than 30 second to finish and timesout.Currently I'm planning to use an ajax call to check the status of job and this function will be used at fixed intervals.I have gone through few question, and this is what I came up with To start the worker, I'm using this code This will return the task_id to the client side and using ajax i will send request to the server to check if the job has finished or not I'm not sure if this is the right method or how will it behave in a real time scenario. Please advice.EDIT : using djcelery view to check the status Ok i have modified my code as bruno suggested and now its looks like And its seems to be working. And still using an ajax call with the task_id to retrieve the status. <code>  def testCelery(request): result=testadd.apply_async() return HttpResponse(str(result.task_id)) def getStat(request,task_id): res = AsyncResult(task_id) s=res.ready() if s==True: return HttpResponse(str(res.get())) else: return HttpResponse(str(s)) from djcelery import views as celery_viewsdef getStat(request,task_id): return celery_views.is_task_successful(request, task_id)",Django celery retrieve task status using Ajax
New line with ConfigParser - PYTHON," I have a config file using configParser: My program works fine reading and processing these values. However some of the sections are going to be quite large. I need a config file that will allow the values to be on a new line, like this: In my code I have a simple function that takes a delimiter (or separator) of the values using string.split() obviously now set to comma. I have tried the escape string of \n which does not work. Does anyone know if this is possible with python's config parser?http://docs.python.org/library/configparser.html I would use for this: <code>  <br>[ section one ]<br>one = Y,Z,X <br><br>[EG 2]<br>ias = X,Y,Z<br> [EG SECTION]<br>EG=<br>item 1 <br>item 2 <br>item 3<br>etc... # We need to extract data from the config def getFromConfig(currentTeam, section, value, delimeter): cp = ConfigParser.ConfigParser() fileName = getFileName(currentTeam) cp.read(fileName) try: returnedString = cp.get(section, value) except: # The config file could be corrupted print( ""Error reading "" + fileName + "" configuration file."" ) sys.exit(1) #Stop us from crashing later if delimeter != """": # We may not need to split returnedList = returnedString.split(delimeter) return returnedList taskStrings = list(getFromConfig(teamName, ""Y"",""Z"","",""))",New lines with ConfigParser?
New lines with Python's ConfigParser?," I have a config file using configParser: My program works fine reading and processing these values. However some of the sections are going to be quite large. I need a config file that will allow the values to be on a new line, like this: In my code I have a simple function that takes a delimiter (or separator) of the values using string.split() obviously now set to comma. I have tried the escape string of \n which does not work. Does anyone know if this is possible with python's config parser?http://docs.python.org/library/configparser.html I would use for this: <code>  <br>[ section one ]<br>one = Y,Z,X <br><br>[EG 2]<br>ias = X,Y,Z<br> [EG SECTION]<br>EG=<br>item 1 <br>item 2 <br>item 3<br>etc... # We need to extract data from the config def getFromConfig(currentTeam, section, value, delimeter): cp = ConfigParser.ConfigParser() fileName = getFileName(currentTeam) cp.read(fileName) try: returnedString = cp.get(section, value) except: # The config file could be corrupted print( ""Error reading "" + fileName + "" configuration file."" ) sys.exit(1) #Stop us from crashing later if delimeter != """": # We may not need to split returnedList = returnedString.split(delimeter) return returnedList taskStrings = list(getFromConfig(teamName, ""Y"",""Z"","",""))",New lines with ConfigParser?
"python print result like '7\xe6\x9c\x8810\xe6\x97\xa5', not i want '710'"," I fetched a web page, which contains Japanese, but when I print it to the console I didn't get the output as 710. Instead, it prints: 7\xe6\x9c\x8810\xe6\x97\xa5What should I do? <code> ","Python prints result as '7\xe6\x9c\x8810\xe6\x97\xa5', but I want '710'"
Python prints results like '7\xe6\x9c\x8810\xe6\x97\xa5' when I wanted '710'," I fetched a web page, which contains Japanese, but when I print it to the console I didn't get the output as 710. Instead, it prints: 7\xe6\x9c\x8810\xe6\x97\xa5What should I do? <code> ","Python prints result as '7\xe6\x9c\x8810\xe6\x97\xa5', but I want '710'"
"python print result like '7\xe6\x9c\x8810\xe6\x97\xa5', not i want '710'"," I fetched a web page, which contains Japanese, but when I print it to the console I didn't get the output as 710. Instead, it prints: 7\xe6\x9c\x8810\xe6\x97\xa5What should I do? <code> ","Python prints result as '7\xe6\x9c\x8810\xe6\x97\xa5', but I want '710'"
pycharm find where Python function is called," Is there a way for PyCharm to show where a given Python function is called from?I currently rely on simply searching for the function name across the project and this often works fine, but if a function name is vague there are a lot of incorrect hits. I'm wondering if I'm missing a feature somewhere, e.g. perhaps the search results could be further narrowed down to only show where modules import the module I'm searching from? <code> ",Find where Python function is called in PyCharm
HTTP Error 405: Method Not Allowed," I am using urllib2 and urllib libraries in python suppose i had the following code when i run the above code i am getting the following error can anyone let me know whats happening here and why its not workingThanks in advance............ <code>  import urllib2import urlliburl = 'http://ah.example.com'half_url = u'/servlet/av/jd?ai=782&ji=2624743&sn=I'req = urllib2.Request(url, half_url.encode('utf-8'))response = urllib2.urlopen(req)print response Traceback (most recent call last): File ""example.py"", line 39, in <module> response = urllib2.urlopen(req) File ""/usr/lib64/python2.7/urllib2.py"", line 126, in urlopen return _opener.open(url, data, timeout) File ""/usr/lib64/python2.7/urllib2.py"", line 398, in open response = meth(req, response) File ""/usr/lib64/python2.7/urllib2.py"", line 511, in http_response 'http', request, response, code, msg, hdrs) File ""/usr/lib64/python2.7/urllib2.py"", line 436, in error return self._call_chain(*args) File ""/usr/lib64/python2.7/urllib2.py"", line 370, in _call_chain result = func(*args) File ""/usr/lib64/python2.7/urllib2.py"", line 519, in http_error_default raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)urllib2.HTTPError: HTTP Error 405: Method Not Allowed",Why am I getting HTTP Error 405: Method Not Allowed when requesting a URL using urllib2?
Django Model: field value is calculation of other fileds," How will I solve total = qty * cost above. I know it will cause an error, but have no idea of how to deal with this. <code>  class PO(models.Model) qty = models.IntegerField(null=True) cost = models.IntegerField(null=True) total = qty * cost",Create a field whose value is a calculation of other fields' values
Create a field which value is a calculation of other fileds' values," How will I solve total = qty * cost above. I know it will cause an error, but have no idea of how to deal with this. <code>  class PO(models.Model) qty = models.IntegerField(null=True) cost = models.IntegerField(null=True) total = qty * cost",Create a field whose value is a calculation of other fields' values
Create a field which value is a calculation of other fields' values," How will I solve total = qty * cost above. I know it will cause an error, but have no idea of how to deal with this. <code>  class PO(models.Model) qty = models.IntegerField(null=True) cost = models.IntegerField(null=True) total = qty * cost",Create a field whose value is a calculation of other fields' values
difference pylab and pyplot," What is the difference between matplotlib.pyplot and matplotlib.pylab?Which is preferred for what usage?I am a little confused, because it seems like independent from which I import, I can do the same things. What am I missing? <code> ",What is the difference between pylab and pyplot?
How to change metadata with ffmpeg/avconv?," I am writing a python script for producing audio and video podcasts. There are a bunch of recorded media files (audio and video) and text files containing the meta information. Now I want to program a function which shall add the information from the meta data text files to all media files (the original and the converted ones). Because I have to handle many different file formats (wav, flac, mp3, mp4, ogg, ogv...) it would be great to have a tool which add meta data to arbitrary formats.My Question:How can I change the metadata of a file with ffmpeg/avconv without changing the audio or video of it and without creating a new file? Is there another commandline/python tool which would do the job for me?What I tried so far:I thought ffmpeg/avconv could be such a tool, because it can handle nearly all media formats. I hoped, that if I set -i input_file and the output_file to the same file, ffmpeg/avconv will be smart enough to leave the file unchanged. Then I could set -metadata key=value and just the metadata will be changed.But I noticed, that if I type avconv -i test.mp3 -metadata title='Test title' test.mp3 the audio test.mp3 will be reconverted in another bitrate.So I thought to use -c copy to copy all video and audio information. Unfortunately also this does not work: You see, that I cannot use -c copy if input_file and output_file are the same. Of course I could produce a temporarily file: But this solution would create (temporarily) a new file on the filesystem and is therefore not preferable. <code>  :~$ du -h test.wav # test.wav is 303 MB big303M test.wav:~$ avconv -i test.wav -c copy -metadata title='Test title' test.wavavconv version 0.8.3-4:0.8.3-0ubuntu0.12.04.1, Copyright (c) 2000-2012 theLibav developersbuilt on Jun 12 2012 16:37:58 with gcc 4.6.3[wav @ 0x846b260] max_analyze_duration reachedInput #0, wav, from 'test.wav':Duration: 00:29:58.74, bitrate: 1411 kb/s Stream #0.0: Audio: pcm_s16le, 44100 Hz, 2 channels, s16, 1411 kb/sFile 'test.wav' already exists. Overwrite ? [y/N] yOutput #0, wav, to 'test.wav':Metadata: title : Test title encoder : Lavf53.21.0 Stream #0.0: Audio: pcm_s16le, 44100 Hz, 2 channels, 1411 kb/sStream mapping:Stream #0:0 -> #0:0 (copy)Press ctrl-c to stop encodingsize= 896kB time=5.20 bitrate=1411.3kbits/s video:0kB audio:896kB global headers:0kB muxing overhead 0.005014%:~$ du -h test.wav # file size of test.wav changed dramatically900K test.wav :-$ avconv -i test.wav -c copy -metadata title='Test title' test_temp.mp3:-$ mv test_tmp.mp3 test.mp3",How to change metadata with ffmpeg/avconv without creating a new file?
Execute function in thread every n seconds, I have a threaded class whose loop needs to execute 4 times every second. I know that I can do something like but the problem is that is doesn't account for the time it takes to do_stuff(). Effectively this needs to be a real-time thread. Is there a way to accomplish this? Ideally the thread would still be put to sleep when not executing code. <code>  do_stuff()time.sleep(0.25),"Periodically execute function in thread in real time, every N seconds"
Create List of Dictionary python," I want to get all the iframe from a webpage. Code: result of print(info): result of pprint(content): Why is the value of the content not right? It's suppose to be the same as the value when I print(info). <code>  site = ""http://"" + urlf = urllib2.urlopen(site)web_content = f.read()soup = BeautifulSoup(web_content)info = {}content = []for iframe in soup.find_all('iframe'): info['src'] = iframe.get('src') info['height'] = iframe.get('height') info['width'] = iframe.get('width') content.append(info) print(info) pprint(content) {'src': u'abc.com', 'width': u'0', 'height': u'0'}{'src': u'xyz.com', 'width': u'0', 'height': u'0'}{'src': u'http://www.detik.com', 'width': u'1000', 'height': u'600'} [{'height': u'600', 'src': u'http://www.detik.com', 'width': u'1000'},{'height': u'600', 'src': u'http://www.detik.com', 'width': u'1000'},{'height': u'600', 'src': u'http://www.detik.com', 'width': u'1000'}]",Creating a list of dictionaries results in a list of copies of the same dictionary
Create List of Dictionary Python," I want to get all the iframe from a webpage. Code: result of print(info): result of pprint(content): Why is the value of the content not right? It's suppose to be the same as the value when I print(info). <code>  site = ""http://"" + urlf = urllib2.urlopen(site)web_content = f.read()soup = BeautifulSoup(web_content)info = {}content = []for iframe in soup.find_all('iframe'): info['src'] = iframe.get('src') info['height'] = iframe.get('height') info['width'] = iframe.get('width') content.append(info) print(info) pprint(content) {'src': u'abc.com', 'width': u'0', 'height': u'0'}{'src': u'xyz.com', 'width': u'0', 'height': u'0'}{'src': u'http://www.detik.com', 'width': u'1000', 'height': u'600'} [{'height': u'600', 'src': u'http://www.detik.com', 'width': u'1000'},{'height': u'600', 'src': u'http://www.detik.com', 'width': u'1000'},{'height': u'600', 'src': u'http://www.detik.com', 'width': u'1000'}]",Creating a list of dictionaries results in a list of copies of the same dictionary
python website laguage detection," i am writing a Bot that can just check thousands of website either they are in English or not.i am using Scrapy (python 2.7 framework) for crawling each website first page ,can some one suggest me which is the best way to check website language ,any help would be appreciated. <code> ",python website language detection
How to configure VirtualEnv and Django on OpenShift DIY app," As RedHat openshift doesn't support Python 2.7, I choose to use Do-It-Yourself (DIY) application.I could able to install python 2.7, setuptools, PIP successfully using an amazing tutorial. Now I want to setup a VirtualEnv and install Django and other libs in it.So, is there any similar (as mentioned above: step by step process) tutorial to configure? <code> ",How to setup a Django App on OpenShift DIY with virtualenv
"why python has ""else"" in ""for-else"" and ""while-else"""," I am a Python beginner. I find that the else in for-else and while-else is completely unnecessary. Because for and while will finally run to else, and we can use the usual lines instead.For example: and are the same.So why does Python have else in for-else and while-else? <code>  for i in range(1, 5): print ielse: print 'over' for i in range(1, 5): print iprint 'over'","Why is the purpose of the ""else"" clause following a ""for"" or ""while"" loop?"
"Why does Python have ""else"" in ""for-else"" and ""while-else""?"," I am a Python beginner. I find that the else in for-else and while-else is completely unnecessary. Because for and while will finally run to else, and we can use the usual lines instead.For example: and are the same.So why does Python have else in for-else and while-else? <code>  for i in range(1, 5): print ielse: print 'over' for i in range(1, 5): print iprint 'over'","Why is the purpose of the ""else"" clause following a ""for"" or ""while"" loop?"
"Algorithm, achieving fastest runtime"," For an algorithm competition training (not homework) we were given this question from a past year. Posted it to this site because the other site required a login.This is the problem:http://pastehtml.com/view/c5nhqhdcw.htmlImage didn't work so posted it here: It has to run in less than one second and I can only think about the slowest way to do it, this is what I tried: What I'm doing at the moment is going through each location and then going through each inhabited house for that location to find the max income location. Pseudocode: This is too slow since it's O(LN) and won't run in under a second for the largest test case. Can someone please simply tell me how to do it in the shortest run time (code isn't required unless you want to) since this has been bugging me for ages.EDIT: There must be a way of doing this in less than O(L) right? <code>  with open('islandin.txt') as fin: num_houses, length = map(int, fin.readline().split()) tot_length = length * 4 # side length of square houses = [map(int, line.split()) for line in fin] # inhabited houses read into list from text filedef cost(house_no): money = 0 for h, p in houses: if h == house_no: # Skip this house since you don't count the one you build on continue d = abs(h - house_no) shortest_dist = min(d, tot_length - d) money += shortest_dist * p return moneydef paths(): for house_no in xrange(1, length * 4 + 1): yield house_no, cost(house_no) print house_no, cost(house_no) # for testingprint max(paths(), key=lambda (h, m): m) # Gets max path based on the money it makes max_money = 0max_location = 0for every location in 1 to length * 4 + 1 money = 0 for house in inhabited_houses: money = money + shortest_dist * num_people_in_this_house if money > max_money max_money = money max_location = location",Algorithm (prob. solving) achieving fastest runtime
Calling a restApi from django view, Is there any way to make a RESTful api call from django view?I am trying to pass header and parameters along a url from the django views. I am googling from half an hour but could not find anything interesting.Any help would be appreciated  <code> ,Calling a REST API from django view
"Python: Efficient workaround for mulitprocessing a function that is a data member of a class, from within that class"," I'm aware of various discussions of limitations of the multiprocessing module when dealing with functions that are data members of a class (due to Pickling problems).But is there another module, or any sort of work-around in multiprocessing, that allows something specifically like the following (specifically without forcing the definition of the function to be applied in parallel to exist outside of the class)? Note: I can easily do this by moving my_single_function outside of the class, and passing something like foo.my_args to the map or map_async commands. But this pushes the parallelized execution of the function outside of instances of MyClass.For my application (parallelizing a large data query that retrieves, joins, and cleans monthly cross-sections of data, and then appends them into a long time-series of such cross-sections), it is very important to have this functionality inside the class since different users of my program will instantiate different instances of the class with different time intervals, different time increments, different sub-sets of data to gather, and so on, that should all be associated with that instance. Thus, I want the work of parallelizing to also be done by the instance, since it owns all the data relevant to the parallelized query, and it would just be silly to try write some hacky wrapper function that binds to some arguments and lives outside of the class (Especially since such a function would be non-general. It would need all kinds of specifics from inside the class.)  <code>  class MyClass(): def __init__(self): self.my_args = [1,2,3,4] self.output = {} def my_single_function(self, arg): return arg**2 def my_parallelized_function(self): # Use map or map_async to map my_single_function onto the # list of self.my_args, and append the return values into # self.output, using each arg in my_args as the key. # The result should make self.output become # {1:1, 2:4, 3:9, 4:16}foo = MyClass()foo.my_parallelized_function()print foo.output","Python: Efficient workaround for multiprocessing a function that is a data member of a class, from within that class"
Python class method decorator w/ self arguments?," How do I pass a class field to a decorator on a class method as an argument? What I want to do is something like: It complains that self does not exist for passing self.url to the decorator. Is there a way around this? <code>  class Client(object): def __init__(self, url): self.url = url @check_authorization(""some_attr"", self.url) def get(self): do_work()",Class method decorator with self arguments?
Python class method decorator with self arguments?," How do I pass a class field to a decorator on a class method as an argument? What I want to do is something like: It complains that self does not exist for passing self.url to the decorator. Is there a way around this? <code>  class Client(object): def __init__(self, url): self.url = url @check_authorization(""some_attr"", self.url) def get(self): do_work()",Class method decorator with self arguments?
Python: why will my function iterate with *print* but not with *return*? How to fix?," Python newbie here, running 2.7.I am trying to create a program that uses a function to generate text, and then outputs the function-generated text to a file.When just printing the function in powershell (like this: http://codepad.org/KftHaO6x), it iterates, as I want it to: When trying to output the function into a file (like this: http://codepad.org/8GJpp9QY), it only gives 1 value, i.e. does not iterate: Why is this, and how can I make the output function iterate (like it does with print)? <code>  def writecode (q, a, b, c): while b < q: b = b + 1 print ""v%d_%d_%d = pairwise (caps[%d],sals[%d],poss[%d],poss[%d],poss[%d],pos_range)"" %(a,b,c,a,a,a,b,c) print ""votes%d_%d.append(v%d_%d_%d)"" % (b,c,a,b,c,) print ""v%d_%d_%d = pairwise (caps[%d],sals[%d],poss[%d],poss[%d],poss[%d],pos_range)"" %(a,c,b,a,a,a,c,b) print ""votes%d_%d.append(v%d_%d_%d)"" % (c,b,a,c,b)writecode (5,1,0,4) def writecode (q, a, b, c): while b < q: b = b + 1 data_to_write = ""v%d_%d_%d = pairwise (caps[%d],sals[%d],poss[%d],poss[%d],poss[%d],pos_range)"" %(a,b,c,a,a,a,b,c) data_to_write_two = ""votes%d_%d.append(v%d_%d_%d)"" % (b,c,a,b,c,) data_to_write_three = ""v%d_%d_%d = pairwise (caps[%d],sals[%d],poss[%d],poss[%d],poss[%d],pos_range)"" %(a,c,b,a,a,a,c,b) data_to_write_four = ""votes%d_%d.append(v%d_%d_%d)"" % (c,b,a,c,b) return data_to_write return data_to_write_two return data_to_write_three return data_to_write_fourx = writecode (5,1,0,4)out_file = open(""code.txt"", ""a"")out_file.write(x)out_file.close()",Function iterates with print but not with return
Overriding class name, To get a string representation of a class name we can use obj.__class__.__name__ is it possible to overload these methods so that I can return my string instead of the actual class name? <code> ,Is it possible to override `__name__` derived from object?
Python sendmail blank message with certain characters," My current script allows me to send emails fine, but there are just some characters it doesn't like, particularly ':' in this sample. If I write a sample message such as, let's say ""Hi there\n how are you?"" it works fine, but let's say I try to send a url http://www.neopets.com, the email is sent blank. I believe the ':' causes this issue, so I tried escaping it, but nothing.  <code>  import smtplib, sysmensaje = sys.argv[1]def mailto(toaddrs, msg): fromaddr = 'myemailblabla' username = 'thisismyemail' password = '122344' server = smtplib.SMTP('smtp.gmail.com:587') server.starttls() server.login(username, password) server.sendmail(fromaddr, toaddrs, msg) server.quit()mailto('test@gmail.com', mensaje)",smtplib sends blank message if the message contain certain characters
Python sendmail sends blank message if the message contain certain characters," My current script allows me to send emails fine, but there are just some characters it doesn't like, particularly ':' in this sample. If I write a sample message such as, let's say ""Hi there\n how are you?"" it works fine, but let's say I try to send a url http://www.neopets.com, the email is sent blank. I believe the ':' causes this issue, so I tried escaping it, but nothing.  <code>  import smtplib, sysmensaje = sys.argv[1]def mailto(toaddrs, msg): fromaddr = 'myemailblabla' username = 'thisismyemail' password = '122344' server = smtplib.SMTP('smtp.gmail.com:587') server.starttls() server.login(username, password) server.sendmail(fromaddr, toaddrs, msg) server.quit()mailto('test@gmail.com', mensaje)",smtplib sends blank message if the message contain certain characters
Python loop over files," I have a folder with ten files in it which I want to loop through. When I print out the name of the file my code works fine: Which prints: But if I try to open the file in the loop I get an IO error: Do I need to pass the full path of the file even inside the loop to open() them? <code>  import osindir = '/home/des/test'for root, dirs, filenames in os.walk(indir): for f in filenames: print(f) 12345678910 import osindir = '/home/des/test'for root, dirs, filenames in os.walk(indir): for f in filenames: log = open(f, 'r')Traceback (most recent call last):File ""/home/des/my_python_progs/loop_over_dir.py"", line 6, in <module>log = open(f, 'r')IOError: [Errno 2] No such file or directory: '1'>>> ",Do I need to pass the full path of a file in another directory to open()?
How can I loop over files with Python?," I have a folder with ten files in it which I want to loop through. When I print out the name of the file my code works fine: Which prints: But if I try to open the file in the loop I get an IO error: Do I need to pass the full path of the file even inside the loop to open() them? <code>  import osindir = '/home/des/test'for root, dirs, filenames in os.walk(indir): for f in filenames: print(f) 12345678910 import osindir = '/home/des/test'for root, dirs, filenames in os.walk(indir): for f in filenames: log = open(f, 'r')Traceback (most recent call last):File ""/home/des/my_python_progs/loop_over_dir.py"", line 6, in <module>log = open(f, 'r')IOError: [Errno 2] No such file or directory: '1'>>> ",Do I need to pass the full path of a file in another directory to open()?
How to loop over files with Python?," I have a folder with ten files in it which I want to loop through. When I print out the name of the file my code works fine: Which prints: But if I try to open the file in the loop I get an IO error: Do I need to pass the full path of the file even inside the loop to open() them? <code>  import osindir = '/home/des/test'for root, dirs, filenames in os.walk(indir): for f in filenames: print(f) 12345678910 import osindir = '/home/des/test'for root, dirs, filenames in os.walk(indir): for f in filenames: log = open(f, 'r')Traceback (most recent call last):File ""/home/des/my_python_progs/loop_over_dir.py"", line 6, in <module>log = open(f, 'r')IOError: [Errno 2] No such file or directory: '1'>>> ",Do I need to pass the full path of a file in another directory to open()?
django NameError: name 'os' is not defined," When I try to follow this tutorial to install Google-auth2 on my Django 1.4 I get this error: This line is: <code>  Traceback (most recent call last): File ""./manage.py"", line 11, in <module> import settings File ""/home/ubuntu/xx/settings.py"", line 140, in <module> GOOGLE_OAUTH2_CLIENT_ID = os.environ['GOOGLE_OAUTH2_CLIENT_ID']NameError: name 'os' is not defined 139- LOGIN_REDIRECT_URL = '/'**140- GOOGLE_OAUTH2_CLIENT_ID = os.environ['GOOGLE_OAUTH2_CLIENT_ID']**141- GOOGLE_OAUTH2_CLIENT_SECRET = os.environ['GOOGLE_OAUTH2_CLIENT_SECRET']142- GOOGLE_WHITE_LISTED_DOMAINS = ['mydomain.org']SOCIAL_AUTH_USER_MODEL = 'auth.User'",Django NameError: name 'os' is not defined
I don't understand Bitwise Not in Python," I have been learning about Bitwise operations today and I learned that Not (~) inverses all bits, e.g.: which means ~10 should be -5 but instead I have seen that it is -11 (per the python command line) which is only two of the bits have been inverted. Can anybody explain why it isn't 10101?EDIT: After looking on my calculator I understand it a little better, But my own code for determining binary and ints is still being confused. Entering in (in byte mode) 11110101 gives me -11 but the same entered in my code gives -117: can one of you explain that? <code>  01010to10101 01010to11011 def binaryToInt(biNum, bUnsigned = False): iNum = 0 bSign = int(biNum[0]) if not (bUnsigned or biNum[-1] == ""u"") else 0 biNum = biNum[(1 if not (bUnsigned or biNum[-1] == ""u"") else 0):(len(biNum) if biNum[-1] != ""u"" else -1)] for i in xrange(len(biNum)): iNum += int(biNum[i]) * 2**(len(biNum) - 1 - i) return (iNum if not bSign else -iNum)def intToBinary(iNum, bUnsigned = False): bSign = ""1"" if iNum < 0 else ""0"" iLoopNum = int((iNum ** 2) ** 0.5) #make positive! biNum = """" while iLoopNum: biNum += str(iLoopNum%2) iLoopNum /= 2 return bSign + biNum[::-1] if not bUnsigned else biNum[::-1] + ""u""",How do bitwise operations work in Python?
How to generate a list from a pandas Data Frame with the column name and column values?," I have a pandas dataframe object that looks like this: I'd like to generate a list of lists objects where the first item is the column label and the remaining list values are the column data values: How can I do this? Thanks for the help. <code>  one two three four five0 1 2 3 4 51 1 1 1 1 1 nested_list = [['one', 1, 1] ['two', 2, 1] ['three', 3, 1] ['four', 4, 1] ['five', 5, 1]]",How to generate a list from a pandas DataFrame with the column name and column values?
What does `ns` and `us` stand for in `timeit` result?," I was trying to compare performance of two statements with timeit, and the results are something like: But I don't know what these ns and us stands for, so I don't know which one is faster. <code>  100 loops, best of 3: 100 ns per loop 100 loops, best of 3: 1.96 us per loop",What do `ns` and `us` stand for in `timeit` result?
Python: Elegant way to store dictionary permanently?," Currently expensively parsing a file, which generates a dictionary of ~400 key, value pairs, which is seldomly updated. Previously had a function which parsed the file, wrote it to a text file in dictionary syntax (ie. dict = {'Adam': 'Room 430', 'Bob': 'Room 404'}) etc, and copied and pasted it into another function whose sole purpose was to return that parsed dictionary. Hence, in every file where I would use that dictionary, I would import that function, and assign it to a variable, which is now that dictionary. Wondering if there's a more elegant way to do this, which does not involve explicitly copying and pasting code around? Using a database kind of seems unnecessary, and the text file gave me the benefit of seeing whether the parsing was done correctly before adding it to the function. But I'm open to suggestions. <code> ",Elegant way to store dictionary permanently with Python?
Python: differences between functools partial and a similar lamba?," In Python, suppose I have a function f that I want to pass around with some secondary arguments (assume for simplicity that it's just the first argument that remains variable).What are the differences between doing it these two ways (if any)? In the doc page for partial, for example, there is this quote: partial objects defined in classes behave like static methods and do not transform into bound methods during instance attribute look-up.Will the lambda-method suffer from this if used to make a class method from arguments supplied to the class (either in the constructor or through a function later on)? <code>  # Assume secondary_args and secondary_kwargs have been definedimport functoolsg1 = functools.partial(f, *secondary_args, **secondary_kwargs)g2 = lambda x: f(x, *secondary_args, **secondary_kwargs)",Differences between functools.partial and a similar lambda?
Differences between functools.partial and a similar lamba?," In Python, suppose I have a function f that I want to pass around with some secondary arguments (assume for simplicity that it's just the first argument that remains variable).What are the differences between doing it these two ways (if any)? In the doc page for partial, for example, there is this quote: partial objects defined in classes behave like static methods and do not transform into bound methods during instance attribute look-up.Will the lambda-method suffer from this if used to make a class method from arguments supplied to the class (either in the constructor or through a function later on)? <code>  # Assume secondary_args and secondary_kwargs have been definedimport functoolsg1 = functools.partial(f, *secondary_args, **secondary_kwargs)g2 = lambda x: f(x, *secondary_args, **secondary_kwargs)",Differences between functools.partial and a similar lambda?
Handling directories with spaces Python os.call()," I'm trying to create a program that scans a text file and passes arguments to subprocess. Everything works fine until I get directories with spaces in the path.My split method, which breaks down the arguments trips up over the spaces: Do, either I need function to parse the correct arguments, or I pass the whole string to the subprocess without breaking it down first.I'm a little lost though. <code>  s = ""svn move folder/hello\ world anotherfolder/hello\ world""task = s.split("" "")process = subprocess.check_call(task, shell = False)",Handling directories with spaces Python subprocess.call()
making python scirpts work with xargs," What would be the process of making my Python scripts work well with 'xargs'? For instance, I would like the following command to work through each line of text file, and execute an arbitrary command: Essentially would like each line to be passed to the hardware.py script. <code>  cat servers.txt | ./hardware.py -m ",Making Python scripts work with xargs
making python scripts work with xargs," What would be the process of making my Python scripts work well with 'xargs'? For instance, I would like the following command to work through each line of text file, and execute an arbitrary command: Essentially would like each line to be passed to the hardware.py script. <code>  cat servers.txt | ./hardware.py -m ",Making Python scripts work with xargs
How to make sys.argv arguments optional? (Python)," sys.argv takes arguments at the shell command line when running a program. How do I make these arguments optional?I know I can use try - except. But this forces you to insert either no extra arguments or all extra arguments, unless you nest more try - except which makes the code look much less readable.EditSuppose I would want the following functionality, how do I implement this? This add argument (and not --add) is optional such that just runs the program normally. <code>  $ python program.py add Peter 'Peter' was added to the list of names. $ python program.py",How to make sys.argv arguments optional?
python : scipy install on ubuntu," I'm currently following the tutorial Installing the SciPy Stack to install SciPy on Ubuntu 12.04 (Precise Pangolin) (I can't use apt-get install because I need a recent version). However, I get errors when I do the following commands: sudo python setup.py install --prefix=/usr/local # installs to /usr/local Moreover, how do I test if this module was installed correctly? <code>  python setup.py buildsudo python setup.py install --prefix=/usr/local # Installs to /usr/localpython setup.py buildmichael@michael-laptop-ubuntu:~/Downloads/scipy-0.11.0rc1$ python setup.py buildRunning from scipy source directory.blas_opt_info:blas_mkl_info: libraries mkl,vml,guide not found in /usr/local/lib libraries mkl,vml,guide not found in /usr/lib libraries mkl,vml,guide not found in /usr/lib/i386-linux-gnu NOT AVAILABLEatlas_blas_threads_info:Setting PTATLAS=ATLAS libraries ptf77blas,ptcblas,atlas not found in /usr/local/lib libraries ptf77blas,ptcblas,atlas not found in /usr/lib/sse2 libraries ptf77blas,ptcblas,atlas not found in /usr/lib libraries ptf77blas,ptcblas,atlas not found in /usr/lib/i386-linux-gnu/sse2 libraries ptf77blas,ptcblas,atlas not found in /usr/lib/i386-linux-gnu NOT AVAILABLEatlas_blas_info: libraries f77blas,cblas,atlas not found in /usr/local/lib libraries f77blas,cblas,atlas not found in /usr/lib/sse2 libraries f77blas,cblas,atlas not found in /usr/lib libraries f77blas,cblas,atlas not found in /usr/lib/i386-linux-gnu/sse2 libraries f77blas,cblas,atlas not found in /usr/lib/i386-linux-gnu NOT AVAILABLE/usr/lib/python2.7/dist-packages/numpy/distutils/system_info.py:1423: UserWarning: Atlas (http://math-atlas.sourceforge.net/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [atlas]) or by setting the ATLAS environment variable. warnings.warn(AtlasNotFoundError.__doc__)blas_info: libraries blas not found in /usr/local/lib libraries blas not found in /usr/lib libraries blas not found in /usr/lib/i386-linux-gnu NOT AVAILABLE/usr/lib/python2.7/dist-packages/numpy/distutils/system_info.py:1432: UserWarning: Blas (http://www.netlib.org/blas/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [blas]) or by setting the BLAS environment variable. warnings.warn(BlasNotFoundError.__doc__)blas_src_info: NOT AVAILABLE/usr/lib/python2.7/dist-packages/numpy/distutils/system_info.py:1435: UserWarning: Blas (http://www.netlib.org/blas/) sources not found. Directories to search for the sources can be specified in the numpy/distutils/site.cfg file (section [blas_src]) or by setting the BLAS_SRC environment variable. warnings.warn(BlasSrcNotFoundError.__doc__)Traceback (most recent call last): File ""setup.py"", line 208, in <module> setup_package() File ""setup.py"", line 199, in setup_package configuration=configuration ) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/core.py"", line 152, in setup config = configuration() File ""setup.py"", line 136, in configuration config.add_subpackage('scipy') File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 1002, in add_subpackage caller_level = 2) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 971, in get_subpackage caller_level = caller_level + 1) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 908, in _get_configuration_from_setup_py config = setup_module.configuration(*args) File ""scipy/setup.py"", line 8, in configuration config.add_subpackage('integrate') File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 1002, in add_subpackage caller_level = 2) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 971, in get_subpackage caller_level = caller_level + 1) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 908, in _get_configuration_from_setup_py config = setup_module.configuration(*args) File ""scipy/integrate/setup.py"", line 10, in configuration blas_opt = get_info('blas_opt',notfound_action=2) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/system_info.py"", line 320, in get_info return cl().get_info(notfound_action) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/system_info.py"", line 471, in get_info raise self.notfounderror(self.notfounderror.__doc__)numpy.distutils.system_info.BlasNotFoundError: Blas (http://www.netlib.org/blas/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [blas]) or by setting the BLAS environment variable.Error in sys.excepthook:Traceback (most recent call last): File ""/usr/lib/python2.7/dist-packages/apport_python_hook.py"", line 64, in apport_excepthook from apport.fileutils import likely_packaged, get_recent_crashes File ""/usr/lib/python2.7/dist-packages/apport/__init__.py"", line 1, in <module> from apport.report import Report File ""/usr/lib/python2.7/dist-packages/apport/report.py"", line 18, in <module> import problem_report File ""/usr/lib/python2.7/dist-packages/problem_report.py"", line 14, in <module> import zlib, base64, time, sys, gzip, struct, os File ""/usr/lib/python2.7/gzip.py"", line 10, in <module> import io File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/io/__init__.py"", line 83, in <module> from matlab import loadmat, savemat, byteordercodes File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/io/matlab/__init__.py"", line 11, in <module> from mio import loadmat, savemat File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/io/matlab/mio.py"", line 15, in <module> from mio4 import MatFile4Reader, MatFile4Writer File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/io/matlab/mio4.py"", line 9, in <module> import scipy.sparse File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/sparse/__init__.py"", line 180, in <module> from csr import * File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/sparse/csr.py"", line 12, in <module> from sparsetools import csr_tocsc, csr_tobsr, csr_count_blocks, \ File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/sparse/sparsetools/__init__.py"", line 4, in <module> from csr import * File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/sparse/sparsetools/csr.py"", line 25, in <module> _csr = swig_import_helper() File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/sparse/sparsetools/csr.py"", line 17, in swig_import_helper import _csrImportError: No module named _csrOriginal exception was:Traceback (most recent call last): File ""setup.py"", line 208, in <module> setup_package() File ""setup.py"", line 199, in setup_package configuration=configuration ) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/core.py"", line 152, in setup config = configuration() File ""setup.py"", line 136, in configuration config.add_subpackage('scipy') File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 1002, in add_subpackage caller_level = 2) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 971, in get_subpackage caller_level = caller_level + 1) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 908, in _get_configuration_from_setup_py config = setup_module.configuration(*args) File ""scipy/setup.py"", line 8, in configuration config.add_subpackage('integrate') File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 1002, in add_subpackage caller_level = 2) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 971, in get_subpackage caller_level = caller_level + 1) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 908, in _get_configuration_from_setup_py config = setup_module.configuration(*args) File ""scipy/integrate/setup.py"", line 10, in configuration blas_opt = get_info('blas_opt',notfound_action=2) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/system_info.py"", line 320, in get_info return cl().get_info(notfound_action) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/system_info.py"", line 471, in get_info raise self.notfounderror(self.notfounderror.__doc__)numpy.distutils.system_info.BlasNotFoundError: Blas (http://www.netlib.org/blas/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [blas]) or by setting the BLAS environment variable. michael@michael-laptop-ubuntu:~/Downloads/scipy-0.11.0rc1$ sudo python setup.py install --prefix=/usr/local[sudo] password for michael: Running from scipy source directory.blas_opt_info:blas_mkl_info: libraries mkl,vml,guide not found in /usr/local/lib libraries mkl,vml,guide not found in /usr/lib libraries mkl,vml,guide not found in /usr/lib/i386-linux-gnu NOT AVAILABLEatlas_blas_threads_info:Setting PTATLAS=ATLAS libraries ptf77blas,ptcblas,atlas not found in /usr/local/lib libraries ptf77blas,ptcblas,atlas not found in /usr/lib/sse2 libraries ptf77blas,ptcblas,atlas not found in /usr/lib libraries ptf77blas,ptcblas,atlas not found in /usr/lib/i386-linux-gnu/sse2 libraries ptf77blas,ptcblas,atlas not found in /usr/lib/i386-linux-gnu NOT AVAILABLEatlas_blas_info: libraries f77blas,cblas,atlas not found in /usr/local/lib libraries f77blas,cblas,atlas not found in /usr/lib/sse2 libraries f77blas,cblas,atlas not found in /usr/lib libraries f77blas,cblas,atlas not found in /usr/lib/i386-linux-gnu/sse2 libraries f77blas,cblas,atlas not found in /usr/lib/i386-linux-gnu NOT AVAILABLE/usr/lib/python2.7/dist-packages/numpy/distutils/system_info.py:1423: UserWarning: Atlas (http://math-atlas.sourceforge.net/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [atlas]) or by setting the ATLAS environment variable. warnings.warn(AtlasNotFoundError.__doc__)blas_info: libraries blas not found in /usr/local/lib libraries blas not found in /usr/lib libraries blas not found in /usr/lib/i386-linux-gnu NOT AVAILABLE/usr/lib/python2.7/dist-packages/numpy/distutils/system_info.py:1432: UserWarning: Blas (http://www.netlib.org/blas/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [blas]) or by setting the BLAS environment variable. warnings.warn(BlasNotFoundError.__doc__)blas_src_info: NOT AVAILABLE/usr/lib/python2.7/dist-packages/numpy/distutils/system_info.py:1435: UserWarning: Blas (http://www.netlib.org/blas/) sources not found. Directories to search for the sources can be specified in the numpy/distutils/site.cfg file (section [blas_src]) or by setting the BLAS_SRC environment variable. warnings.warn(BlasSrcNotFoundError.__doc__)Traceback (most recent call last): File ""setup.py"", line 208, in <module> setup_package() File ""setup.py"", line 199, in setup_package configuration=configuration ) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/core.py"", line 152, in setup config = configuration() File ""setup.py"", line 136, in configuration config.add_subpackage('scipy') File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 1002, in add_subpackage caller_level = 2) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 971, in get_subpackage caller_level = caller_level + 1) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 908, in _get_configuration_from_setup_py config = setup_module.configuration(*args) File ""scipy/setup.py"", line 8, in configuration config.add_subpackage('integrate') File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 1002, in add_subpackage caller_level = 2) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 971, in get_subpackage caller_level = caller_level + 1) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 908, in _get_configuration_from_setup_py config = setup_module.configuration(*args) File ""scipy/integrate/setup.py"", line 10, in configuration blas_opt = get_info('blas_opt',notfound_action=2) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/system_info.py"", line 320, in get_info return cl().get_info(notfound_action) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/system_info.py"", line 471, in get_info raise self.notfounderror(self.notfounderror.__doc__)numpy.distutils.system_info.BlasNotFoundError: Blas (http://www.netlib.org/blas/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [blas]) or by setting the BLAS environment variable.Error in sys.excepthook:Traceback (most recent call last): File ""/usr/lib/python2.7/dist-packages/apport_python_hook.py"", line 64, in apport_excepthook from apport.fileutils import likely_packaged, get_recent_crashes File ""/usr/lib/python2.7/dist-packages/apport/__init__.py"", line 1, in <module> from apport.report import Report File ""/usr/lib/python2.7/dist-packages/apport/report.py"", line 18, in <module> import problem_report File ""/usr/lib/python2.7/dist-packages/problem_report.py"", line 14, in <module> import zlib, base64, time, sys, gzip, struct, os File ""/usr/lib/python2.7/gzip.py"", line 10, in <module> import io File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/io/__init__.py"", line 83, in <module> from matlab import loadmat, savemat, byteordercodes File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/io/matlab/__init__.py"", line 11, in <module> from mio import loadmat, savemat File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/io/matlab/mio.py"", line 15, in <module> from mio4 import MatFile4Reader, MatFile4Writer File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/io/matlab/mio4.py"", line 9, in <module> import scipy.sparse File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/sparse/__init__.py"", line 180, in <module> from csr import * File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/sparse/csr.py"", line 12, in <module> from sparsetools import csr_tocsc, csr_tobsr, csr_count_blocks, \ File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/sparse/sparsetools/__init__.py"", line 4, in <module> from csr import * File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/sparse/sparsetools/csr.py"", line 25, in <module> _csr = swig_import_helper() File ""/home/michael/Downloads/scipy-0.11.0rc1/scipy/sparse/sparsetools/csr.py"", line 17, in swig_import_helper import _csrImportError: No module named _csrOriginal exception was:Traceback (most recent call last): File ""setup.py"", line 208, in <module> setup_package() File ""setup.py"", line 199, in setup_package configuration=configuration ) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/core.py"", line 152, in setup config = configuration() File ""setup.py"", line 136, in configuration config.add_subpackage('scipy') File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 1002, in add_subpackage caller_level = 2) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 971, in get_subpackage caller_level = caller_level + 1) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 908, in _get_configuration_from_setup_py config = setup_module.configuration(*args) File ""scipy/setup.py"", line 8, in configuration config.add_subpackage('integrate') File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 1002, in add_subpackage caller_level = 2) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 971, in get_subpackage caller_level = caller_level + 1) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/misc_util.py"", line 908, in _get_configuration_from_setup_py config = setup_module.configuration(*args) File ""scipy/integrate/setup.py"", line 10, in configuration blas_opt = get_info('blas_opt',notfound_action=2) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/system_info.py"", line 320, in get_info return cl().get_info(notfound_action) File ""/usr/lib/python2.7/dist-packages/numpy/distutils/system_info.py"", line 471, in get_info raise self.notfounderror(self.notfounderror.__doc__)numpy.distutils.system_info.BlasNotFoundError: Blas (http://www.netlib.org/blas/) libraries not found. Directories to search for the libraries can be specified in the numpy/distutils/site.cfg file (section [blas]) or by setting the BLAS environment variable.",SciPy/Python install on Ubuntu
Numpy transpose not giving expected result," I am trying a very basic example in Python scipy module for transpose() method but it's not giving expected result. I am using Ipython with pylab mode. If I print the contents of arrays ""a"" and ""b"", they are similar.Expectation is: (which will be result in Matlab on transpose) <code>  a = array([1,2,3]print a.shape>> (3,)b = a.transpose()print b.shape>> (3,) [1, 2, 3]",Numpy transpose of 1D array not giving expected result
parsing utf-8/unicode strings with lxml HTML," I have been trying to parse with etree.HTML() a text encoded as UTF-8 without success. So far so good. The response text is good and it is a unicode string. Now if I'm trying to get the list of CSS URIs. No issue either. Now let's change from unicode to UTF-8 and request again the list of CSS URIs. I get an empty list. Indeed, the second parsing stopped right away after the first Japanese character in the title. I'm still trying to understand what I have done wrong. <code>  pythonPython 2.7.1 (r271:86832, Jun 16 2011, 16:59:05) [GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2335.15.00)] on darwinType ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> from lxml import etree>>> import requests>>> headers = {'User-Agent': ""Opera/9.80 (Macintosh; Intel Mac OS X 10.8.0) Presto/2.12.363 Version/12.50""}>>> r = requests.get(""http://www.rakuten.co.jp/"", headers=headers)>>> r.status_code200>>> r.headers{'x-cache': 'MISS from www.rakuten.co.jp', 'transfer-encoding': 'chunked', 'set-cookie': 'wPzd=lng%3DNA%3Acnt%3DCA; expires=Tue, 13-Aug-2013 16:51:38 GMT; path=/; domain=www.rakuten.co.jp', 'server': 'Apache', 'pragma': 'no-cache', 'cache-control': 'private', 'date': 'Mon, 13 Aug 2012 16:51:38 GMT', 'content-type': 'text/html; charset=EUC-JP'}>>> responsetext = r.text >>> tree = etree.HTML(responsetext)>>> csspathlist = tree.xpath('//link[@rel=""stylesheet""]/@href')>>> csspathlist['http://a.ichiba.jp.rakuten-static.com/com/inc/home/20080930/opt/css/normal/common.css?v=1207111500', 'http://a.ichiba.jp.rakuten-static.com/com/inc/home/20080930/opt/css/normal/layout.css?v=1207111500', 'http://a.ichiba.jp.rakuten-static.com/com/inc/home/20080930/opt/css/normal/sidecolumn.css?v=1207111500', 'http://a.ichiba.jp.rakuten-static.com/com/inc/home/20080930/beta/css/liquid/api.css?v=1207111500', '/com/inc/home/20080930/beta/css/liquid/myrakuten_dpgs.css', 'http://a.ichiba.jp.rakuten-static.com/com/inc/home/20080930/opt/css/normal/leftcolumn.css?v=1207111500', 'http://a.ichiba.jp.rakuten-static.com/com/inc/home/20080930/opt/css/normal/header.css?v=1207111500', '/com/inc/home/20080930/opt/css/normal/footer.css', 'http://a.ichiba.jp.rakuten-static.com/com/inc/home/20080930/beta/css/liquid/ipad.css', 'http://a.ichiba.jp.rakuten-static.com/com/inc/home/20080930/opt/css/normal/genre.css?v=1207111500', 'http://a.ichiba.jp.rakuten-static.com/com/inc/home/20080930/opt/css/normal/supersale.css?v=1207111500', '/com/inc/home/20080930/beta/css/liquid/rakuten_membership.css', 'http://a.ichiba.jp.rakuten-static.com/com/inc/home/20080930/beta/css/noscript/set.css?v=1207111500', 'http://a.ichiba.jp.rakuten-static.com/com/inc/home/20080930/beta/css/liquid/suggest-2.0.1.css?v=1204231500', 'http://a.ichiba.jp.rakuten-static.com/com/inc/home/20080930/beta/css/liquid/liquid_banner.css?v=1203011138', 'http://a.ichiba.jp.rakuten-static.com/com/inc/home/20080930/beta/css/liquid/area_announce.css?v=1203011138'] >>> htmltext = responsetext.encode('utf-8')>>> tree2 = etree.HTML(htmltext)>>> csspathlist2 = tree2.xpath('//link[@rel=""stylesheet""]/@href')>>> csspathlist2[] >>> etree.tostring(tree2)'<html lang=""ja"" xmlns=""http://www.w3.org/1999/xhtml"" xmlns:og=""http://ogp.me/ns#"" xmlns:fb=""http://www.facebook.com/2008/fbml""><head><meta http-equiv=""Content-Type"" content=""text/html; charset=EUC-JP""/><meta http-equiv=""Content-Style-Type"" content=""text/css""/><meta http-equiv=""Content-Script-Type"" content=""text/javascript""/><title/></head></html>' <meta http-equiv=""Content-Script-Type"" content=""text/javascript""/><title> Shopping is Entertainment! </title>",Parsing UTF-8/unicode strings with lxml HTML
python gtk entry ghosttext," i have a login-window with two gtk.Entry objects, one for username, one for password. How can i add some Ghosttext to the Entry, so there is written ""Username"" in the Entry but if you click inside the text dissapears. <code> ",Python Gtk.Entry placeholder text
Add a subdirectory to a python namespae," I would like to be able to import a python module which is actually located in a subdirectory of another module.I am developing a framework with plug-ins.Since I'm expecting to have a few thousands (there's currently >250 already) and I don't want one big directory containing >1000 files I have them ordered in directories like this, where they are grouped by the first letter of their name: Since I would not like to impose a burden on developers of other plugins, or people not needing as many as I have, I would like to put each plugin in the 'framework.plugins' namespace.This way someone adding a bunch of private plugins can just do so by adding them in the folder framework.plugins and there provide a __init__.py file containing: however, currently this setup is forcing them to also use the a-z subdirectories.Sometimes a plugin is extending another plugin, so now I have a and I would like to have Is there any way to declare a namespace where the full name space name actually doesn't map to a folder structure?I am aware of the pkg_resources package, but this is only available via setuptools, and I'd rather not have an extra dependency. The solution should work in python 2.4-2.7.3update:Combining the provided answers I tried to get a list of all plugins imported in the __init__.py from plugins. However, this fails due to dependencies. Since a plugin in the 'c' folder tries to import a plugin starting with 't', and this one has not been added yet. I'm not sure If I'm on the right track here, or just overcomplicating things and better write my own PEP302 importer. However, I can't seem to find any decent examples of how these should work.Update:I tried to follow the suggesting of wrapping the __getattr__ function in my __init__.py, but this seems to no avail. <code>  framework\ __init__.py framework.py tools.py plugins\ __init__.py a\ __init__.py atlas.py ... b\ __init__.py binary.py ... c\ __init__.py cmake.py ... from pkgutil import extend_path__path__ = extend_path(__path__, __name__) from framework.plugins.a import atlas from framework.pugins import atlas import pkg_resourcespkg_resources.declare_namespace(__name__) plugins = [ x[0].find_module(x[1]).load_module(x[1]) for x in pkgutil.walk_packages([ os.path.join(framework.plugins.__path__[0], chr(y)) for y in xrange(ord('a'), ord('z') + 1) ],'framework.plugins.' ) ] import pkgutilimport osimport sysplugins = [x[1] for x in pkgutil.walk_packages([ os.path.join(__path__[0], chr(y)) for y in xrange(ord('a'), ord('z') + 1) ] )]import typesclass MyWrapper(types.ModuleType): def __init__(self, wrapped): self.wrapped = wrapped def __getattr__(self, name): if name in plugins: askedattr = name[0] + '.' + name else: askedattr = name attr = getattr(self.wrapped, askedattr) return attrsys.modules[__name__] = MyWrapper(sys.modules[__name__])",Add a subdirectory to a python namespace
the yield statement," Possible Duplicate: The Python yield keyword explained Can someone explain to me what the yield statement actually does in this bit of code here: for number in fibonacci(): # Use the generator as an iterator; print numberWhat I understand so far is, we are defining a function finonacci(), with no parameters?inside the function we are defining a and b equal to 0 and 1, next, while this is true, we are yielding a. What is this actually doing? Furthermore, while yielding a? a is now equal to b, while b is now equal to a + b.Next question, for number in fibonacci(), does this mean for every number in the function or what? I'm equally stumped on what yield and 'for number' are actually doing. Obviously I am aware that it means for every number in fibonacci() print number. Am I actually defining number without knowing it? Thanks, sorry if I'm not clear. BTW, it's for project Euler, if I knew how to program well this would be a breeze but I'm trying to learn this on the fly. <code>  def fibonacci(): a, b = 0, 1 while True: yield a a, b = b, a+b","What is a ""yield"" statement in a function?"
"What is the ""yield"" statement in a Python function?"," Possible Duplicate: The Python yield keyword explained Can someone explain to me what the yield statement actually does in this bit of code here: for number in fibonacci(): # Use the generator as an iterator; print numberWhat I understand so far is, we are defining a function finonacci(), with no parameters?inside the function we are defining a and b equal to 0 and 1, next, while this is true, we are yielding a. What is this actually doing? Furthermore, while yielding a? a is now equal to b, while b is now equal to a + b.Next question, for number in fibonacci(), does this mean for every number in the function or what? I'm equally stumped on what yield and 'for number' are actually doing. Obviously I am aware that it means for every number in fibonacci() print number. Am I actually defining number without knowing it? Thanks, sorry if I'm not clear. BTW, it's for project Euler, if I knew how to program well this would be a breeze but I'm trying to learn this on the fly. <code>  def fibonacci(): a, b = 0, 1 while True: yield a a, b = b, a+b","What is a ""yield"" statement in a function?"
python closure with initialized list virable," I've got this piece of code: if I run it, it says: but if I do this: it runs.Is there something with list? or my code organizing is wrong? <code>  #!/usr/bin/env pythondef get_match(): cache=[] def match(v): if cache: return cache cache=[v] return cache return matchm = get_match()m(1) UnboundLocalError: local variable 'cache' referenced before assignment #!/usr/bin/env pythondef get(): y = 1 def m(v): return y + v return ma=get()a(1)",python closure with assigning outer variable inside inner function
python closure with initialized list of variables," I've got this piece of code: if I run it, it says: but if I do this: it runs.Is there something with list? or my code organizing is wrong? <code>  #!/usr/bin/env pythondef get_match(): cache=[] def match(v): if cache: return cache cache=[v] return cache return matchm = get_match()m(1) UnboundLocalError: local variable 'cache' referenced before assignment #!/usr/bin/env pythondef get(): y = 1 def m(v): return y + v return ma=get()a(1)",python closure with assigning outer variable inside inner function
Where does os.delete go?," I've been using Python for a long time and have numerous scripts running all over my office. I use a few in particular scripts to back up then delete data. In these script I use os.remove function.My question is: Where does the os.remove function delete items to?Does it delete them right off the HD?I know they don't go to the recycle binDoes it simply remove the item's link, but keep it on the HD somehow? <code> ",Where does os.remove go?
How to set style display of an html element?," I get an element like So I want to set its display as inline in python itself.is it possible with python to set the display.I tried which is showing me error.One way which is getting into my mind is to use Jquery to change the display and then execute it using driver.execute but unfortunately I am not getting the right syntax to do this. Let me know how to do this (The syntax.) Thanks. <code>  cv_upload = driver.find_element_by_id('id_cv_upload') cv_upload.style.display = ""inline""",How to set style display of an html element in a selenium test?
factory_boy instance within TestCase causes unique contraint violation," I'm just getting started with the factory_boy django library for test factories, and having an issue with a duplicate key constraint violation.test_member_programme.py factories.py When running the first test passes successfully, but the second fails with the following error: My understanding is that every TestCase should run within a transaction, yet the creation of the foreign key does not appear to have rolled back before the second test runs. Clearly I'm doing something fundamentally wrong, but I'm a bit stumped! Thanks!I've tracked down the problem, but unfortunately don't know how to resolve it. The issue is that ROLLBACKs are occurring, but only on one database (this app has 2 databases). For legacy reasons, we have a separate database for django auth, flatpages etc and another db for our app. Someone with a similar problem here. <code>  from datetime import date, timedeltafrom django.test import TestCasefrom app.test.factories import MemberFactory, ProgrammeFactoryfrom app.models.member_programme import MemberProgrammeclass MemberProgrammeTestCase(TestCase): def member_programme(self): yesterday = date.today() - timedelta(days=1) return MemberProgramme.objects.create( mem=MemberFactory(), prg=ProgrammeFactory(), date_registered=yesterday) def date_registered_should_be_defined_test(self): # This test passes memprg = self.member_programme() assert hasattr(memprg, 'date_registered') def date_registered_should_be_in_past_test(self): # This test fails memprg = self.member_programme() assert memprg.date_registered < date.today() class CountryOfOriginFactory(factory.Factory): """""" Factory class for app.models.CountryOfOrigin """""" FACTORY_FOR = CountryOfOrigin code = 'UK' the_country = 'United Kingdom'class MemberFactory(factory.Factory): """""" Factory class for app.models.Member """""" FACTORY_FOR = Member first_name = 'Test' surname = 'User' sex = 'M' date_of_birth = datetime.date(1990, 1, 1) origin = factory.LazyAttribute(lambda a: CountryOfOriginFactory()) IntegrityError: duplicate key value violates unique constraint ""country_of_origin_code_key"" dba test_app 127.0.0.1 2012-09-04 21:51:50.806 UTC LOG: duration: 0.038 ms statement: BEGIN; SET TRANSACTION ISOLATION LEVEL READ COMMITTEDdba test_app 127.0.0.1 2012-09-04 21:51:50.808 UTC LOG: duration: 0.903 ms statement: INSERT INTO ""member_programme"" (""mem_id"", ""prgm_id"", ""date_registered"", ""date_completed"", ""ordinality"") VALUES (1, 1, E'2012-09-04', NULL, 1)dba test_app 127.0.0.1 2012-09-04 21:51:50.808 UTC LOG: duration: 0.150 ms statement: SELECT CURRVAL(pg_get_serial_sequence('""member_programme""','id'))dba test_app 127.0.0.1 2012-09-04 21:51:50.810 UTC LOG: duration: 1.796 ms statement: COMMITdba test_app_django 127.0.0.1 2012-09-04 21:51:50.811 UTC LOG: duration: 0.056 ms statement: ROLLBACK <---- ROLLBACK ON DJANGO DB ONLYdba test_app_django 127.0.0.1 2012-09-04 21:51:50.814 UTC LOG: disconnection: session time: 0:00:21.005 user=dba database=test_app_django host=127.0.0.1 port=60355dba test_app 127.0.0.1 2012-09-04 21:51:50.818 UTC LOG: disconnection: session time: 0:00:04.751 user=dba database=test_app host=127.0.0.1 port=60357dba test_app 127.0.0.1 2012-09-04 21:54:00.796 UTC LOG: connection authorized: user=dba database=test_appdba test_app 127.0.0.1 2012-09-04 21:54:00.802 UTC LOG: duration: 0.243 ms statement: SET DATESTYLE TO 'ISO'dba test_app 127.0.0.1 2012-09-04 21:54:00.802 UTC LOG: duration: 0.156 ms statement: SHOW client_encodingdba test_app 127.0.0.1 2012-09-04 21:54:00.803 UTC LOG: duration: 0.047 ms statement: SHOW default_transaction_isolationdba test_app 127.0.0.1 2012-09-04 21:54:00.803 UTC LOG: duration: 0.068 ms statement: BEGIN; SET TRANSACTION ISOLATION LEVEL READ COMMITTEDdba test_app 127.0.0.1 2012-09-04 21:54:00.804 UTC LOG: duration: 0.410 ms statement: SET TIME ZONE E'Pacific/Auckland'dba test_app 127.0.0.1 2012-09-04 21:54:00.805 UTC ERROR: duplicate key value violates unique constraint ""country_of_origin_code_key""",Lack of ROLLBACK within TestCase causes unique contraint violation in multi-db django app
seperating a sparse matrix," Question: How can I split 1 sparse matrix into 2, based on the values in a list?That is, I have a sparse matrix X: that I visualize in my head as a list of lists, to look like this: And I have a list y that looks like this: How can I separate X into two sparse matrices, depending on whether the corresponding value in y is positive or negative? For example, how can I get: The result (X_pos and X_neg) should also be sparse matrices obviously as it's just splitting a sparse matrix to begin with.Thanks! <code>  >>print type(X)<class 'scipy.sparse.csr.csr_matrix'> >>print X.todense()[[1,3,4] [3,2,2] [4,8,1]] y = [-1, 3, -4] >>print X_pos.todense() [[3,2,2]] >>print X_neg.todense() [[1,3,4] [4,8,1]]",splitting a sparse matrix into two
Python / Django csv.writer - is it possible to write to a variable?," Is it possible to use csv.writer to write data to a variable rather than a file?I was hoping I could do something like this: When I execute the code I get the following error: <code>  data = ''csv.writer(data)# ...... (I have removed the csv processing code for brevity)message = EmailMessage('Invoice for 2012', 'h', 'noreply@test.co.uk', ['test@test.co.uk'])message.attach('invoice.csv', data, 'text/csv')message.send() argument 1 must have a ""write"" method",Python csv.writer - is it possible to write to a variable?
How to list all the folders and files in the directory after connecting through sftp in python," I am using Python and trying to connect to SFTP and want to retrieve an XML file from there and need to place it in my local system. Below is the code: Here connection is success full. And now I want to see all the folders and all the files and need to enter in to required folder for retrieving the XML file from there.Finally my intention is to view all the folders and files after connecting to SFTP server.In the above code I had used ftp.listdir() through which I got output as some thing like below I want to know whether these are the only files present?And the command I used above is right to view the folders too?What is the command to view all the folders and files? <code>  import paramikosftpURL = 'sftp.somewebsite.com'sftpUser = 'user_name'sftpPass = 'password'ssh = paramiko.SSHClient()# automatically add keys without requiring human interventionssh.set_missing_host_key_policy( paramiko.AutoAddPolicy() )ssh.connect(sftpURL, username=sftpUser, password=sftpPass)ftp = ssh.open_sftp()files = ftp.listdir()print files ['.bash_logout', '.bash_profile', '.bashrc', '.mozilla', 'testfile_248.xml']",How to list all the folders and files in the directory after connecting through SFTP in Python
Fastest way to t if object doesn't exist in SQLAlchemy," So I'm quite new to SQLAlchemy. I have a model Showing which has about 10,000 rows in the table. Here is the class: Could anyone give me some guidance on the fastest way to insert a new showing if it doesn't exist already. I think it is slightly more complicated because a showing is only unique if the time, cinmea, and film are unique on a showing. I currently have this code: which works, but seems to be very slow. Any help is much appreciated. <code>  class Showing(Base): __tablename__ = ""showings"" id = Column(Integer, primary_key=True) time = Column(DateTime) link = Column(String) film_id = Column(Integer, ForeignKey('films.id')) cinema_id = Column(Integer, ForeignKey('cinemas.id')) def __eq__(self, other): if self.time == other.time and self.cinema == other.cinema and self.film == other.film: return True else: return False def AddShowings(self, showing_times, cinema, film): all_showings = self.session.query(Showing).options(joinedload(Showing.cinema), joinedload(Showing.film)).all() for showing_time in showing_times: tmp_showing = Showing(time=showing_time[0], film=film, cinema=cinema, link=showing_time[1]) if tmp_showing not in all_showings: self.session.add(tmp_showing) self.session.commit() all_showings.append(tmp_showing)",Fastest way to insert object if it doesn't exist with SQLAlchemy
Fastest way to insert object it doesn't exist with SQLAlchemy," So I'm quite new to SQLAlchemy. I have a model Showing which has about 10,000 rows in the table. Here is the class: Could anyone give me some guidance on the fastest way to insert a new showing if it doesn't exist already. I think it is slightly more complicated because a showing is only unique if the time, cinmea, and film are unique on a showing. I currently have this code: which works, but seems to be very slow. Any help is much appreciated. <code>  class Showing(Base): __tablename__ = ""showings"" id = Column(Integer, primary_key=True) time = Column(DateTime) link = Column(String) film_id = Column(Integer, ForeignKey('films.id')) cinema_id = Column(Integer, ForeignKey('cinemas.id')) def __eq__(self, other): if self.time == other.time and self.cinema == other.cinema and self.film == other.film: return True else: return False def AddShowings(self, showing_times, cinema, film): all_showings = self.session.query(Showing).options(joinedload(Showing.cinema), joinedload(Showing.film)).all() for showing_time in showing_times: tmp_showing = Showing(time=showing_time[0], film=film, cinema=cinema, link=showing_time[1]) if tmp_showing not in all_showings: self.session.add(tmp_showing) self.session.commit() all_showings.append(tmp_showing)",Fastest way to insert object if it doesn't exist with SQLAlchemy
Tkinter - active combo boxes," Is it possible to create a combobox that updates with the closest item in its list as you type into it? For example: Then you type ""Chr"" into the combobox, I want it to automatically fill in ""Chris"". <code>  A = ttk.Combobox()A['values'] = ['Chris', 'Jane', 'Ben', 'Megan']",Tkinter - How to create a combo box with autocompletion
Tkinter - How to create a combo box with autocompletion ," Is it possible to create a combobox that updates with the closest item in its list as you type into it? For example: Then you type ""Chr"" into the combobox, I want it to automatically fill in ""Chris"". <code>  A = ttk.Combobox()A['values'] = ['Chris', 'Jane', 'Ben', 'Megan']",Tkinter - How to create a combo box with autocompletion
Remote_api for AppEngine with PyCharm," I'd like to know if anyone has pointers about how to configure AppEngine remote_api, to so that I can debug my code locally but use the remote_api to fetch some data from my server. That way, I can test against real information.Thanks! <code> ",How do I get live data from my production App Engine app to my local dev app?
How to create numpy structured array with multiple fields," I'm new to working with numpy arrays and I'm having trouble creating a structured array. I'd like to create something similar to a Matlab structure where the fields can be arrays of different shapes. I'd like data['a'] to return matrix a, data['b'] to return matrix b, etc. When reading in a Matlab structure, the data is saved in this format so I know it must be possible. <code>  a=numpy.array([1, 2, 3, 4, 5, 6,]);b=numpy.array([7,8,9]);c=numpy.array([10,11,12,13,14,15,16,17,18,19,20]);##Doesn't do what I wantdata=numpy.array([a, b, c],dtype=[('a','f8'),('b','f8'),('c','f8')]); ",How to create numpy structured array with multiple fields of different shape?
How to create numpy structured array with multiple fields?," I'm new to working with numpy arrays and I'm having trouble creating a structured array. I'd like to create something similar to a Matlab structure where the fields can be arrays of different shapes. I'd like data['a'] to return matrix a, data['b'] to return matrix b, etc. When reading in a Matlab structure, the data is saved in this format so I know it must be possible. <code>  a=numpy.array([1, 2, 3, 4, 5, 6,]);b=numpy.array([7,8,9]);c=numpy.array([10,11,12,13,14,15,16,17,18,19,20]);##Doesn't do what I wantdata=numpy.array([a, b, c],dtype=[('a','f8'),('b','f8'),('c','f8')]); ",How to create numpy structured array with multiple fields of different shape?
best way to design a class in python," So, this is more like a philosophical question for someone who is trying to understand classes.Most of time, how i use class is actually a very bad way to use it. I think of a lot of functions and after a time just indent the code and makes it a class and replacing few stuff with self.variable if a variable is repeated a lot. (I know its bad practise)But anyways... What i am asking is: Now there are many ways to do this: Can you suggest which one is bad and which one is worse?or any other way to do this.This is just a toy example (offcourse). I mean, there is no need to have a class here if there is one function.. but lets say in __execute something() calls a whole set of other methods.. ??Thanks <code>  class FooBar: def __init__(self,foo,bar): self._foo = foo self._bar = bar self.ans = self.__execute() def __execute(self): return something(self._foo, self._bar) class FooBar: def __init__(self,foo): self._foo = foo def execute(self,bar): return something(self._foo, bar)",Best way to design a class in python
Is it possible to pipe data to a script that itself is being piped to the Python interpreter?," I need to implement an SVN pre-commit hook which executes a script that itself is stored in SVN.I can use the svn cat command to pipe that script to the Python interpreter, as follows: However, my_script.py itself requires data to be piped on STDIN.That data is not stored in a file; it is stored on the network. I would prefer not to have to download the data to a temporary file, as normally I could pipe it to a Python program: I'm not sure how to combine both of these pipes. <code>  svn cat file://$REPO/trunk/my_script.py | python - --argument1 --argument2 curl http://example.com/huge_file.txt | python my_script.py",Pipe STDIN to a script that is itself being piped to the Python interpreter?
Python: Coverting Epoch time into the datetime," I am getting a response from the rest is an Epoch time format like I want to convert that epoch seconds in MySQL format time so that I could store the differences in my MySQL database.I tried: The above result is not what I am expecting. I want it be like Please suggest how can I achieve this?Also,Why I am getting TypeError: a float is required for <code>  start_time = 1234566end_time = 1234578 >>> import time>>> time.gmtime(123456)time.struct_time(tm_year=1970, tm_mon=1, tm_mday=2, tm_hour=10, tm_min=17, tm_sec=36, tm_wday=4, tm_yday=2, tm_isdst=0) 2012-09-12 21:00:00 >>> getbbb_class.end_time = 1347516459425>>> mend = time.gmtime(getbbb_class.end_time).tm_hourTraceback (most recent call last): ...TypeError: a float is required",Converting Epoch time into the datetime
Python: Converting Epoch time into the datetime," I am getting a response from the rest is an Epoch time format like I want to convert that epoch seconds in MySQL format time so that I could store the differences in my MySQL database.I tried: The above result is not what I am expecting. I want it be like Please suggest how can I achieve this?Also,Why I am getting TypeError: a float is required for <code>  start_time = 1234566end_time = 1234578 >>> import time>>> time.gmtime(123456)time.struct_time(tm_year=1970, tm_mon=1, tm_mday=2, tm_hour=10, tm_min=17, tm_sec=36, tm_wday=4, tm_yday=2, tm_isdst=0) 2012-09-12 21:00:00 >>> getbbb_class.end_time = 1347516459425>>> mend = time.gmtime(getbbb_class.end_time).tm_hourTraceback (most recent call last): ...TypeError: a float is required",Converting Epoch time into the datetime
"in Python, How can I convert a string into a date object and get year, month and day separately?"," If I have lets say this string ""2008-12-12 19:21:10"" how can I convert it into a date and get the year, month and day from that created object separately? <code> ","How can I convert a string into a date object and get year, month and day separately?"
Modifying a virtualenv," Possible Duplicate: Revert the `--no-site-packages` option with virtualenv I've created a virtual environment using the virtualenvwrapper documentation as follows: It works fine for the most part, but I've run into a few Django issues that require me to install some global packages outside of my virtual environment. Once I've installed these packages, how to I update my virtual environment to pull in these new packages? Or do I need to recreate the environment from scratch? <code>  $ pip install virtualenvwrapper$ export WORKON_HOME=~/Envs$ mkdir -p $WORKON_HOME$ source /usr/local/bin/virtualenvwrapper.sh$ mkvirtualenv env1",Modifying a virtualenv so that packages installed in global site-packages are available
Create a list which is a substring of another list python," So I want to create a list which is a sublist of some existing list.For example,L = [1, 2, 3, 4, 5, 6, 7], I want to create a sublist li such that li contains all the elements in L at odd positions.While I can do it by But I want to know if there is another way to do the same efficiently and in fewer number of steps.  <code>  L = [1, 2, 3, 4, 5, 6, 7]li = []count = 0for i in L: if count % 2 == 1: li.append(i) count += 1",Extract elements of list at odd positions
SciPy and scikit-learn - ValueError: Dimension mismatch," I use SciPy and scikit-learn to train and apply a Multinomial Naive Bayes Classifier for binary text classification. Precisely, I use the module sklearn.feature_extraction.text.CountVectorizer for creating sparse matrices that hold word feature counts from text and the module sklearn.naive_bayes.MultinomialNB as the classifier implementation for training the classifier on training data and applying it on test data.The input to the CountVectorizer is a list of text documents represented as unicode strings. The training data is much larger than the test data. My code looks like this (simplified): Problem: As soon as MultinomialNB.predict_log_proba() is called, I get ValueError: dimension mismatch. According to the IPython stacktrace below, the error occurs in SciPy: I have no idea why this error occurs. Can anybody please explain it to me and provide a solution for this problem? Thanks a lot in advance! <code>  vectorizer = CountVectorizer(**kwargs)# sparse matrix with training dataX_train = vectorizer.fit_transform(list_of_documents_for_training)# vector holding target values (=classes, either -1 or 1) for training documents# this vector has the same number of elements as the list of documentsy_train = numpy.array([1, 1, 1, -1, -1, 1, -1, -1, 1, 1, -1, -1, -1, ...])# sparse matrix with test dataX_test = vectorizer.fit_transform(list_of_documents_for_testing)# Training stage of NB classifierclassifier = MultinomialNB()classifier.fit(X=X_train, y=y_train)# Prediction of log probabilities on test dataX_log_proba = classifier.predict_log_proba(X_test) /path/to/my/code.pyc--> 177 X_log_proba = classifier.predict_log_proba(X_test)/.../sklearn/naive_bayes.pyc in predict_log_proba(self, X) 76 in the model, where classes are ordered arithmetically. 77 """"""--> 78 jll = self._joint_log_likelihood(X) 79 # normalize by P(x) = P(f_1, ..., f_n) 80 log_prob_x = logsumexp(jll, axis=1)/.../sklearn/naive_bayes.pyc in _joint_log_likelihood(self, X) 345 """"""Calculate the posterior log probability of the samples X"""""" 346 X = atleast2d_or_csr(X)--> 347 return (safe_sparse_dot(X, self.feature_log_prob_.T) 348 + self.class_log_prior_) 349 /.../sklearn/utils/extmath.pyc in safe_sparse_dot(a, b, dense_output) 71 from scipy import sparse 72 if sparse.issparse(a) or sparse.issparse(b):--> 73 ret = a * b 74 if dense_output and hasattr(ret, ""toarray""): 75 ret = ret.toarray()/.../scipy/sparse/base.pyc in __mul__(self, other) 276 277 if other.shape[0] != self.shape[1]:--> 278 raise ValueError('dimension mismatch') 279 280 result = self._mul_multivector(np.asarray(other))",ValueError: Dimension mismatch
Python sum of ascii values of all characters in string," I am searching a more efficient way to sum-up the ASCII values of all characters in a given string, using only standard python (2.7 is preferable).Currently I have: I want to emphasize that my main focus and aspect of this question is what I wrote above.The following is somewhat less important aspect of this question and should be treated as such:So why I am asking it?! I have compared this approach vs embedding a simple C-code function which does the same here using PyInline, and it seems that a simple C embedded function is 17 times faster.If there is no Python approach faster than what I have suggested (using only standard Python), it seems strange that the Python developers haven't added such an implementation in the core.Current results for suggested answers.On my Windows 7, i-7, Python 2.7: <code>  print sum(ord(ch) for ch in text) text = ""aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa"" sum(ord(ch) for ch in text) >> 0.00521324663262 sum(array.array(""B"", text)) >> 0.0010040770317 sum(map(ord, text )) >> 0.00427160369234 sum(bytearray(text)) >> 0.000864669402933 C-code embedded: >> 0.000272828426841",Python sum of ASCII values of all characters in a string
"python pandas: Remove duplicates by columns A, keeping the row with the highest value in column B"," I have a dataframe with repeat values in column A. I want to drop duplicates, keeping the row with the highest value in column B.So this: Should turn into this: I'm guessing there's probably an easy way to do thismaybe as easy as sorting the DataFrame before dropping duplicatesbut I don't know groupby's internal logic well enough to figure it out. Any suggestions? <code>  A B1 101 202 302 403 10 A B1 202 403 10","Remove duplicates by columns A, keeping the row with the highest value in column B"
Python: Recommended way to walk complex dictionary structures imported from JSON?," Importing from JSON can get very complex and nested structures.For example: What is the recommended way to walk complex structures like the above?Apart of a few list there are mostly dictionaries, the structure can become even more imbricated so I need a general solution. <code>  {u'body': [{u'declarations': [{u'id': {u'name': u'i', u'type': u'Identifier'}, u'init': {u'type': u'Literal', u'value': 2}, u'type': u'VariableDeclarator'}], u'kind': u'var', u'type': u'VariableDeclaration'}, {u'declarations': [{u'id': {u'name': u'j', u'type': u'Identifier'}, u'init': {u'type': u'Literal', u'value': 4}, u'type': u'VariableDeclarator'}], u'kind': u'var', u'type': u'VariableDeclaration'}, {u'declarations': [{u'id': {u'name': u'answer', u'type': u'Identifier'}, u'init': {u'left': {u'name': u'i', u'type': u'Identifier'}, u'operator': u'*', u'right': {u'name': u'j', u'type': u'Identifier'}, u'type': u'BinaryExpression'}, u'type': u'VariableDeclarator'}], u'kind': u'var', u'type': u'VariableDeclaration'}], u'type': u'Program'}",How to completely traverse a complex dictionary of unknown depth?
Python: How to completely traverse a complex dictionary of unknown depth?," Importing from JSON can get very complex and nested structures.For example: What is the recommended way to walk complex structures like the above?Apart of a few list there are mostly dictionaries, the structure can become even more imbricated so I need a general solution. <code>  {u'body': [{u'declarations': [{u'id': {u'name': u'i', u'type': u'Identifier'}, u'init': {u'type': u'Literal', u'value': 2}, u'type': u'VariableDeclarator'}], u'kind': u'var', u'type': u'VariableDeclaration'}, {u'declarations': [{u'id': {u'name': u'j', u'type': u'Identifier'}, u'init': {u'type': u'Literal', u'value': 4}, u'type': u'VariableDeclarator'}], u'kind': u'var', u'type': u'VariableDeclaration'}, {u'declarations': [{u'id': {u'name': u'answer', u'type': u'Identifier'}, u'init': {u'left': {u'name': u'i', u'type': u'Identifier'}, u'operator': u'*', u'right': {u'name': u'j', u'type': u'Identifier'}, u'type': u'BinaryExpression'}, u'type': u'VariableDeclarator'}], u'kind': u'var', u'type': u'VariableDeclaration'}], u'type': u'Program'}",How to completely traverse a complex dictionary of unknown depth?
Return type of object initialization functions?," The key to this question is aiding unit-testing. If I have a busy __init__ (i.e. __init__ that does complex initialization), I cannot simply instantiate an object of a class, but I need to mock/stub out all methods invoked on dependencies within the __init__.To illustrate this problem, here is example: To test the fun* functions, every test must perform the complex construction. I find this redundant and would like to know how this problems can be elegantly handled in python, if not by moving the work from the constructor.SOLUTION:As @ecatmur suggested, to test some specific function, this code should do the trick: With this approach all the methods will be mocked out. If fun1 calls some other method you want executed (e.g. fun2) you can do it like this: SomeClass.fun2.__get__(mobject) will produce instancemethod which will provide the correct binding.Viva el Python!ORIGINAL QUESTION:Original question was centered around moving the work done in __init__ to the separate init method and different problems revolving that approach. My usual approach is to make this become this General sentiment was that moving work from __init__ is not a common practice and would be meaningless to seasoned python developers. <code>  class SomeClass(object): def __init__(self, dep1, dep2, some_string): self._dep1 = dep1 self._dep2 = dep2 self._some_string = some_string # I would need to mock everything here (imagine some even more # complicated example) for dep2element in self._dep2: dep2element.set_dep(dep1) self._dep1.set_some_string(some_string) def fun1(self): ... def fun2(self): ... def fun3(self): ... class TestSomeClass(TestCase): def create_SomeClass(self, some_string): dep1 = Mock() # mock everything required by SomeClass' constructor dep2 = Mock() # mock everything required by SomeClass' constructor return SomeClass(dep1, dep2, some_string) def test_fun1(self): sc = self.create_SomeClass('some string') ... def test_fun2(self): sc = self.create_SomeClass('some other string') ... def test_fun3(self): sc = self.create_SomeClass('yet another string') ... def test_some_method(): mobject = Mock(SomeClass) SomeClass.fun1(mobject) def test_some_method(): mobject = Mock(SomeClass) mobject.fun2 = SomeClass.fun2.__get__(mobject) SomeClass.fun1(mobject) class SomeClass(object): def __init__(self, dep1, dep2, some_string) self._dep1 = dep1 self._dep2 = dep2 # lots of mumbo-jumbo here... class SomeClass(object): def __init__(self, dep1, dep2) self._dep1 = dep1 self._dep2 = dep2 def initiate(self, some-string) # lots of mumto-jumbo here...",Removing the work from __init__ to aid unit testing
How to I get the current Ipython Notebook name, I am trying to obtain the current NoteBook name when running the IPython notebook. I know I can see it at the top of the notebook. What I am after something like I need to get the name in a variable. <code>  currentNotebook = IPython.foo.bar.notebookname(),How do I get the current IPython / Jupyter Notebook name
How to I get the current IPython Notebook name, I am trying to obtain the current NoteBook name when running the IPython notebook. I know I can see it at the top of the notebook. What I am after something like I need to get the name in a variable. <code>  currentNotebook = IPython.foo.bar.notebookname(),How do I get the current IPython / Jupyter Notebook name
How do I get the current IPython Notebook name, I am trying to obtain the current NoteBook name when running the IPython notebook. I know I can see it at the top of the notebook. What I am after something like I need to get the name in a variable. <code>  currentNotebook = IPython.foo.bar.notebookname(),How do I get the current IPython / Jupyter Notebook name
Create JSON Reponse in Django with Model," I am having some issue here. I am trying to return a JSON response made of a message and a model instance: But I keep getting: Why is that? I have seen before: and that works... what is the difference?!Thanks! <code>  class MachineModel(models.Model): name = models.CharField(max_length=64, blank=False) description = models.CharField(max_length=64, blank=False) manufacturer = models.ForeignKey(Manufacturer) added_by = models.ForeignKey(User, related_name='%(app_label)s_%(class)s_added_by') creation_date = models.DateTimeField(auto_now_add=True) last_modified = models.DateTimeField(auto_now=True) machine_model_model = form.save(commit=False) r_user = request.user.userprofile machine_model_model.manufacturer_id = manuf_id machine_model_model.added_by_id = request.user.id machine_model_model.save() alert_message = "" The'%s' model "" % machine_model_model.name alert_message += (""for '%s' "" % machine_model_model.manufacturer) alert_message += ""was was successfully created!"" test = simplejson.dumps(list(machine_model_model)) data = [{'message': alert_message, 'model': test}] response = JSONResponse(data, {}, 'application/json')class JSONResponse(HttpResponse):""""""JSON response class."""""" def __init__(self, obj='', json_opts={}, mimetype=""application/json"", *args, **kwargs): content = simplejson.dumps(obj, **json_opts) super(JSONResponse,self).__init__(content, mimetype, *args, **kwargs) File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.py"", line 178, in defaultraise TypeError(repr(o) + "" is not JSON serializable"")TypeError: <MachineModel: ""Test12""> is not JSON serializable models = Model.objects.filter(manufacturer_id=m_id)json = simplejson.dumps(models)",Create JSON Response in Django with Model
Can Go really be that much faster than python?," I think I may have implemented this incorrectly because the results do not make sense. I have a Go program that counts to 1000000000: It finishes in less than a second. On the other hand I have a Python script: It finishes in a few minutes.Why is the Go version so much faster? Are they both counting up to 1000000000 or am I missing something? <code>  package mainimport ( ""fmt"")func main() { for i := 0; i < 1000000000; i++ {} fmt.Println(""Done"") } x = 0while x < 1000000000: x+=1print 'Done'",Can Go really be that much faster than Python?
file name vs stream as a function argument," If a function takes as an input the name of a text file, I can refactor it to instead take a file object (I call it ""stream""; is there a better word?). The advantages are obvious - a function that takes a stream as an argument is:much easier to write a unit test for, since I don't need to create a temporary file just for the testmore flexible, since I can use it in situations where I somehow already have the contents of the file in a variableAre there any disadvantages to streams? Or should I always refactor a function from a file name argument to a stream argument (assuming, of course, the file is text-only)? <code> ",file name vs file object as a function argument
Separation of business logic and data access in django?," I am writing a project in Django and I see that 80% of the code is in the file models.py. This code is confusing and, after a certain time, I cease to understand what is really happening.Here is what bothers me:I find it ugly that my model level (which was supposed to beresponsible only for the work with data from a database) is alsosending email, walking on API to other services, etc.Also, I find it unacceptable to place business logic in the view, becausethis way it becomes difficult to control. For example, in myapplication there are at least three ways to create newinstances of User, but technically it should create them uniformly.I do not always notice when the methods andproperties of my models become non-deterministic and when they developside effects.Here is a simple example. At first, the User model was like this: Over time, it turned into this: What I want is to separate entities in my code:Entities of my database, persistence level: What data does my application keep?Entities of my application, business logic level: What does my application do?What are the good practices to implement such an approach that can be applied in Django? <code>  class User(db.Models): def get_present_name(self): return self.name or 'Anonymous' def activate(self): self.status = 'activated' self.save() class User(db.Models): def get_present_name(self): # property became non-deterministic in terms of database # data is taken from another service by api return remote_api.request_user_name(self.uid) or 'Anonymous' def activate(self): # method now has a side effect (send message to user) self.status = 'activated' self.save() send_mail('Your account is activated!', '', [self.email])",Separation of business logic and data access in django
Test gathering/identification failure.," Using py.test, two tests called the same in different directory causes py.test to fail. Why is that? How can I change this without renaming all the tests?To duplicate do: <code>  ; cd /var/tmp/my_test_module; mkdir -p ook/test ; mkdir -p eek/test; touch ook/test/test_proxy.py; touch eek/test/test_proxy.py; py.test============================= test session starts ==============================platform linux2 -- Python 2.7.3 -- pytest-2.2.4collected 0 items / 1 errors ==================================== ERRORS ====================================___________________ ERROR collecting ook/test/test_proxy.py ____________________import file mismatch:imported module 'test_proxy' has this __file__ attribute: /home/ygolanski/code/junk/python/mymodule/eek/test/test_proxy.pywhich is not the same as the test file we want to collect: /home/ygolanski/code/junk/python/mymodule/ook/test/test_proxy.pyHINT: remove __pycache__ / .pyc files and/or use a unique basename for your test file modules=========================== 1 error in 0.01 seconds ============================",Test discovery failure when tests in different directories are called the same
py.test - test discovery failure when tests in different directories are called the same," Using py.test, two tests called the same in different directory causes py.test to fail. Why is that? How can I change this without renaming all the tests?To duplicate do: <code>  ; cd /var/tmp/my_test_module; mkdir -p ook/test ; mkdir -p eek/test; touch ook/test/test_proxy.py; touch eek/test/test_proxy.py; py.test============================= test session starts ==============================platform linux2 -- Python 2.7.3 -- pytest-2.2.4collected 0 items / 1 errors ==================================== ERRORS ====================================___________________ ERROR collecting ook/test/test_proxy.py ____________________import file mismatch:imported module 'test_proxy' has this __file__ attribute: /home/ygolanski/code/junk/python/mymodule/eek/test/test_proxy.pywhich is not the same as the test file we want to collect: /home/ygolanski/code/junk/python/mymodule/ook/test/test_proxy.pyHINT: remove __pycache__ / .pyc files and/or use a unique basename for your test file modules=========================== 1 error in 0.01 seconds ============================",Test discovery failure when tests in different directories are called the same
Ho can I hide a django label in a custom django form?," I have a custom form that creates a hidden input of a field: I have to do this little trick to ""hide"" the label, but what I want to do is remove it from the form. I create the form like this: <code>  class MPForm( forms.ModelForm ): def __init__( self, *args, **kwargs ): super(MPForm, self).__init__( *args, **kwargs ) self.fields['mp_e'].label = """" #the trick :)class Meta: model = MeasurementPoint widgets = { 'mp_e': forms.HiddenInput() } exclude = ('mp_order') forms.MPForm()",How can I hide a django label in a custom django form?
"Chaning the ""tick frequency"" on x or y axis in python?"," I am trying to fix how python plots my data.Say and Then I would do: and the x axis' ticks are plotted in intervals of 5. Is there a way to make it show intervals of 1? <code>  x = [0,5,9,10,15] y = [0,1,2,3,4] matplotlib.pyplot.plot(x,y)matplotlib.pyplot.show()","Changing the ""tick frequency"" on x or y axis in matplotlib"
"Changing the ""tick frequency"" on x or y axis in matplotlib?"," I am trying to fix how python plots my data.Say and Then I would do: and the x axis' ticks are plotted in intervals of 5. Is there a way to make it show intervals of 1? <code>  x = [0,5,9,10,15] y = [0,1,2,3,4] matplotlib.pyplot.plot(x,y)matplotlib.pyplot.show()","Changing the ""tick frequency"" on x or y axis in matplotlib"
Python Image Library convert from Jpeg to PDF," I'm attempting to convert a JPEG file with, 200 dpi, to a PDF file, however, when I save the file as a PDF I think it's changing the dpi to 72, and thus making the image larger. I had a similar problem when initially trying to scale my JPEG image to a smaller size, and was able to solve that by specifying the dpi when I save the image. Now when I try to save this JPEG as a PDF, specifying the dpi doesn't seem to make any difference, and I get an image that is larger than my original that looks like it has a lower dpi. Is there a way to mantain a consistent resolution when converting from JPEG to PDF using PIL? Or is there a better way for me to go about doing this?This is what I have for converting a directory of files from JPEG to PDF: <code>  im = Image.open(""Image.jpg"")dpi=im.info['dpi']if im.size == (2592, 1728): out = im.resize((1188,792), Image.ANTIALIAS)elif im.size == (1728,2592): out = im.resize((792,1188), Image.ANTIALIAS)out.save(project, dpi=dpi) for infile in listing: outfile = destpath + os.path.splitext(infile)[0] + "".pdf"" current = path + infile if infile != outfile: im = Image.open(current) dpi=im.info['dpi'] im.save(outfile, ""PDF"", Quality = 100)",Image Library convert from JPEG to
Testing functions returning iterable in pyhton," I'm having difficulties testing python functions thatreturn an iterable, like functions that areyielding or functions that simply return an iterable, like return imap(f, some_iter) or return permutations([1,2,3]).So with the permutations example, I expect the output of the function to be [(1, 2, 3), (1, 3, 2), ...]. So, I start testing my code. This will not work, since perm3() is an iterable, not alist. So we can fix this particular example. And this works fine. But what if I have nested iterables? That isiterables yielding iterables? Like say the expressionsproduct(permutations([1, 2]), permutations([3, 4])). Now this isprobably not useful but it's clear that it will be (once unrolling theiterators) something like [((1, 2), (3, 4)), ((1, 2), (4, 3)), ...].However, we can not just wrap list around our result, as that will onlyturn iterable<blah> to [iterable<blah>, iterable<blah>, ...]. Wellof course I can do map(list, product(...)), but this only works for anesting level of 2.So, does the python testing community have any solution for theproblems when testing iterables? Naturally some iterables can'tbe tested in this way, like if you want an infinite generator, butstill this issue should be common enough for somebody to have thoughtabout this. <code>  def perm3(): return permutations([1,2,3])# Lets ignore test framework and such detailsdef test_perm3(): assertEqual(perm3(), [(1, 2, 3), (1, 3, 2), ...]) def test_perm3(): assertEqual(list(perm3()), [(1, 2, 3), (1, 3, 2), ...])",Testing functions returning iterable in python
"Search API Qury contain ','(Comma) or = or (), It return error"," Using app engine's search API, I tried to search the queries which contain ,, =, (, etc.It returns the following error: Why? Does the search API support these characters? <code>  Failed to parse query ""engines (Modular)""Traceback (most recent call last): File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1505, in __call__ rv = self.router.dispatch(request, response)File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1253, in default_dispatcher return route.handler_adapter(request, response)File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1077, in __call__ return handler.dispatch()File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 547, in dispatch return self.handle_exception(e, self.app.debug)File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 545, in dispatch return method(*args, **kwargs)File ""/base/data/home/apps/s~generatestock/12.362076640167792770/search.py"", line 1641, in get result = find_search_document(search_item)File ""/base/data/home/apps/s~generatestock/12.362076640167792770/search.py"", line 177, in find_search_document return search.Index(name=_INDEX_NAME).search(query)File ""/base/python27_runtime/python27_lib/versions/1/google/appengine/api/search/search.py"", line 2715, in search query = Query(query_string=query)File ""/base/python27_runtime/python27_lib/versions/1/google/appengine/api/search/search.py"", line 2286, in __init___CheckQuery(self._query_string)File ""/base/python27_runtime/python27_lib/versions/1/google/appengine/api/search/search.py"", line 1964, in _CheckQuery raise QueryError('Failed to parse query ""%s""' % query)QueryError: Failed to parse query ""Engines (Modular)""","Search API returns QueryError when Query contains ','(Comma) or = or ()"
"Search API retrun QueryError when Query which contains ','(Comma) or = or ()"," Using app engine's search API, I tried to search the queries which contain ,, =, (, etc.It returns the following error: Why? Does the search API support these characters? <code>  Failed to parse query ""engines (Modular)""Traceback (most recent call last): File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1505, in __call__ rv = self.router.dispatch(request, response)File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1253, in default_dispatcher return route.handler_adapter(request, response)File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1077, in __call__ return handler.dispatch()File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 547, in dispatch return self.handle_exception(e, self.app.debug)File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 545, in dispatch return method(*args, **kwargs)File ""/base/data/home/apps/s~generatestock/12.362076640167792770/search.py"", line 1641, in get result = find_search_document(search_item)File ""/base/data/home/apps/s~generatestock/12.362076640167792770/search.py"", line 177, in find_search_document return search.Index(name=_INDEX_NAME).search(query)File ""/base/python27_runtime/python27_lib/versions/1/google/appengine/api/search/search.py"", line 2715, in search query = Query(query_string=query)File ""/base/python27_runtime/python27_lib/versions/1/google/appengine/api/search/search.py"", line 2286, in __init___CheckQuery(self._query_string)File ""/base/python27_runtime/python27_lib/versions/1/google/appengine/api/search/search.py"", line 1964, in _CheckQuery raise QueryError('Failed to parse query ""%s""' % query)QueryError: Failed to parse query ""Engines (Modular)""","Search API returns QueryError when Query contains ','(Comma) or = or ()"
"Search API return QueryError when Query which contains ','(Comma) or = or ()"," Using app engine's search API, I tried to search the queries which contain ,, =, (, etc.It returns the following error: Why? Does the search API support these characters? <code>  Failed to parse query ""engines (Modular)""Traceback (most recent call last): File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1505, in __call__ rv = self.router.dispatch(request, response)File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1253, in default_dispatcher return route.handler_adapter(request, response)File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1077, in __call__ return handler.dispatch()File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 547, in dispatch return self.handle_exception(e, self.app.debug)File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 545, in dispatch return method(*args, **kwargs)File ""/base/data/home/apps/s~generatestock/12.362076640167792770/search.py"", line 1641, in get result = find_search_document(search_item)File ""/base/data/home/apps/s~generatestock/12.362076640167792770/search.py"", line 177, in find_search_document return search.Index(name=_INDEX_NAME).search(query)File ""/base/python27_runtime/python27_lib/versions/1/google/appengine/api/search/search.py"", line 2715, in search query = Query(query_string=query)File ""/base/python27_runtime/python27_lib/versions/1/google/appengine/api/search/search.py"", line 2286, in __init___CheckQuery(self._query_string)File ""/base/python27_runtime/python27_lib/versions/1/google/appengine/api/search/search.py"", line 1964, in _CheckQuery raise QueryError('Failed to parse query ""%s""' % query)QueryError: Failed to parse query ""Engines (Modular)""","Search API returns QueryError when Query contains ','(Comma) or = or ()"
Python: Function Not Changing Global Variable," my code is as follow: For some reason when my code enters the if statement, it doesn't exit the while loop after it's done with function().BUT, if I code it like this: ...it exits the while loop. What's going on here?I made sure that my code enters the if-statement. I haven't run the debugger yet because my code has a lot of loops (pretty big 2D array) and I gave up on debugging due to it being so tedious. How come ""done"" isn't being changed when it's in a function? <code>  done = Falsedef function(): for loop: code if not comply: done = True #let's say that the code enters this if-statementwhile done == False: function() done = Falsewhile done == False: for loop: code if not comply: done = True #let's say that the code enters this if-statement",Function not changing global variable
Function Not Changing Global Variable," my code is as follow: For some reason when my code enters the if statement, it doesn't exit the while loop after it's done with function().BUT, if I code it like this: ...it exits the while loop. What's going on here?I made sure that my code enters the if-statement. I haven't run the debugger yet because my code has a lot of loops (pretty big 2D array) and I gave up on debugging due to it being so tedious. How come ""done"" isn't being changed when it's in a function? <code>  done = Falsedef function(): for loop: code if not comply: done = True #let's say that the code enters this if-statementwhile done == False: function() done = Falsewhile done == False: for loop: code if not comply: done = True #let's say that the code enters this if-statement",Function not changing global variable
Understanding dis," I would like to understand how to use dis (the dissembler of Python bytecode). Specifically, how should one interpret the output of dis.dis (or dis.disassemble)?.Here is a very specific example (in Python 2.7.3): I see that JUMP_IF_TRUE_OR_POP etc. are bytecode instructions (although interestingly, BUILD_SET does not appear in this list, though I expect it works as BUILD_TUPLE). I think the numbers on the right-hand-side are memory allocations, and the numbers on the left are goto numbers... I notice they almost increment by 3 each time (but not quite).If I wrap dis.dis(""heapq.nsmallest(d,3)"") inside a function: <code>  dis.dis(""heapq.nsmallest(d,3)"") 0 BUILD_SET 24933 3 JUMP_IF_TRUE_OR_POP 11889 6 JUMP_FORWARD 28019 (to 28028) 9 STORE_GLOBAL 27756 (27756) 12 LOAD_NAME 29811 (29811) 15 STORE_SLICE+0 16 LOAD_CONST 13100 (13100) 19 STORE_SLICE+1 def f_heapq_nsmallest(d,n): return heapq.nsmallest(d,n)dis.dis(""f_heapq(d,3)"") 0 BUILD_TUPLE 26719 3 LOAD_NAME 28769 (28769) 6 JUMP_ABSOLUTE 25640 9 <44> # what is <44> ? 10 DELETE_SLICE+1 11 STORE_SLICE+1 ",How should I understand the output of dis.dis?
How should I be using dis.dis?," I would like to understand how to use dis (the dissembler of Python bytecode). Specifically, how should one interpret the output of dis.dis (or dis.disassemble)?.Here is a very specific example (in Python 2.7.3): I see that JUMP_IF_TRUE_OR_POP etc. are bytecode instructions (although interestingly, BUILD_SET does not appear in this list, though I expect it works as BUILD_TUPLE). I think the numbers on the right-hand-side are memory allocations, and the numbers on the left are goto numbers... I notice they almost increment by 3 each time (but not quite).If I wrap dis.dis(""heapq.nsmallest(d,3)"") inside a function: <code>  dis.dis(""heapq.nsmallest(d,3)"") 0 BUILD_SET 24933 3 JUMP_IF_TRUE_OR_POP 11889 6 JUMP_FORWARD 28019 (to 28028) 9 STORE_GLOBAL 27756 (27756) 12 LOAD_NAME 29811 (29811) 15 STORE_SLICE+0 16 LOAD_CONST 13100 (13100) 19 STORE_SLICE+1 def f_heapq_nsmallest(d,n): return heapq.nsmallest(d,n)dis.dis(""f_heapq(d,3)"") 0 BUILD_TUPLE 26719 3 LOAD_NAME 28769 (28769) 6 JUMP_ABSOLUTE 25640 9 <44> # what is <44> ? 10 DELETE_SLICE+1 11 STORE_SLICE+1 ",How should I understand the output of dis.dis?
How should understand the output of dis.dis?," I would like to understand how to use dis (the dissembler of Python bytecode). Specifically, how should one interpret the output of dis.dis (or dis.disassemble)?.Here is a very specific example (in Python 2.7.3): I see that JUMP_IF_TRUE_OR_POP etc. are bytecode instructions (although interestingly, BUILD_SET does not appear in this list, though I expect it works as BUILD_TUPLE). I think the numbers on the right-hand-side are memory allocations, and the numbers on the left are goto numbers... I notice they almost increment by 3 each time (but not quite).If I wrap dis.dis(""heapq.nsmallest(d,3)"") inside a function: <code>  dis.dis(""heapq.nsmallest(d,3)"") 0 BUILD_SET 24933 3 JUMP_IF_TRUE_OR_POP 11889 6 JUMP_FORWARD 28019 (to 28028) 9 STORE_GLOBAL 27756 (27756) 12 LOAD_NAME 29811 (29811) 15 STORE_SLICE+0 16 LOAD_CONST 13100 (13100) 19 STORE_SLICE+1 def f_heapq_nsmallest(d,n): return heapq.nsmallest(d,n)dis.dis(""f_heapq(d,3)"") 0 BUILD_TUPLE 26719 3 LOAD_NAME 28769 (28769) 6 JUMP_ABSOLUTE 25640 9 <44> # what is <44> ? 10 DELETE_SLICE+1 11 STORE_SLICE+1 ",How should I understand the output of dis.dis?
How to make scrapy CrawlSpider crawl the start_url," I've managed to code a very simple crawler with Scrapy, with these given constraints:Store all link info (e.g.: anchor text, page title), hence the 2 callbacksUse CrawlSpider to take advantage of rules, hence no BaseSpiderIt runs well, except it doesn't implement rules if I add a callback to the first request!Here is my code: (works but not properly, with a live example) I've tried solving this issue in 3 ways:1: To return a Request with the start url - rules are not executed2: Same as above, but with a callback to parse_links - Same issue3: Call parse_links after scraping the start url, by implementing parse_start_url, function does not get calledHere are the logs: VersionsPython 2.7.2Scrapy 0.14.4 <code>  from scrapy.contrib.spiders import CrawlSpider,Rulefrom scrapy.selector import HtmlXPathSelectorfrom scrapy.http import Requestfrom scrapySpider.items import SPagefrom scrapy.contrib.linkextractors.sgml import SgmlLinkExtractorclass TestSpider4(CrawlSpider): name = ""spiderSO"" allowed_domains = [""cumulodata.com""] start_urls = [""http://www.cumulodata.com""] extractor = SgmlLinkExtractor() def parse_start_url(self, response): #3 print('----------manual call of',response) self.parse_links(response) print('----------manual call done') # 1 return Request(self.start_urls[0]) # does not call parse_links(example.com) # 2 return Request(self.start_urls[0],callback = self.parse_links) # does not call parse_links(example.com) rules = ( Rule(extractor,callback='parse_links',follow=True), ) def parse_links(self, response): hxs = HtmlXPathSelector(response) print('----------- manual parsing links of',response.url) links = hxs.select('//a') for link in links: title = link.select('@title') url = link.select('@href').extract()[0] meta={'title':title,} yield Request(url, callback = self.parse_page,meta=meta) def parse_page(self, response): print('----------- parsing page: ',response.url) hxs = HtmlXPathSelector(response) item=SPage() item['url'] = str(response.request.url) item['title']=response.meta['title'] item['h1']=hxs.select('//h1/text()').extract() yield item ----------manual call of <200 http://www.cumulodata.com>)----------manual call done#No '----------- manual parsing links', so `parse_links` is never called!",Why don't my Scrapy CrawlSpider rules work?
"Why isn't admin.autodiscover() called automatically in Django when using the admin, why was it designed to be called explicitely?", Without putting admin.autodiscover() in urls.py the admin page shows You don't have permission to edit anything (See SO thread). Why is this so? If you always need to add admin.autodiscover() to edit information using the admin even though you have a superuser name and password for security why didn't the Django developers trigger admin.autodiscover() automatically?. <code> ,"Why isn't admin.autodiscover() called automatically in Django when using the admin, why was it designed to be called explicitly?"
PYTZ 'America/Edmon' offset wrong," Possible Duplicate: Weird timezone issue with pytz This seems wrong: 'America/Edmonton' and 'US/Eastern' should be the same time zone (17:00:00 STD). Not to mention 16:26:00 doesn't make any sense.-- Update --The above makes sense in context of Jon Skeet's answer. However, things get strange when I do this: I created a naive date. Since 'America/Edmonton' is my timezone, I try to set that manually: This should not have change anything because that is the correct TZ. However: This should give me an offset of 2 hours (difference between 'US/Eastern' and 'America/Edmonton') but it gives me 3 hours 26 minutes (which is 2 hours plus one hour 26 minutes :D ) inserting timezone('US/Mountain') produces the correct result in astimezone(). Creating an aware datetime with 'America/Edmonton' will also work correctly. <code>  >>> import pytz>>> z1 = timezone('America/Edmonton')>>> z2 = timezone('US/Mountain')>>> z1<DstTzInfo 'America/Edmonton' LMT-1 day, 16:26:00 STD>>>> z2<DstTzInfo 'US/Mountain' MST-1 day, 17:00:00 STD>>>> pytz.VERSION'2012f'>>> >>> d = datetime.now()>>> ddatetime.datetime(2012, 10, 9, 15, 21, 41, 644706) >>> d2 = d.replace(tzinfo=timezone('America/Edmonton'))>>> d2datetime.datetime(2012, 10, 9, 15, 21, 41, 644706, tzinfo=<DstTzInfo 'America/Edmonton' LMT-1 day, 16:26:00 STD>) >>> d2.astimezone(timezone('US/Eastern'))datetime.datetime(2012, 10, 9, 18, 55, 41, 644706, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)",PYTZ 'America/Edmonton' offset wrong
how to combine two data frames in python pandas," I'm using Pandas data frames. I have a initial data frame, say D. I extract two data frames from it like this: I want to combine A and B so I can have them as one DataFrame, something like a union operation. The order of the data is not important. However, when we sample A and B from D, they retain their indexes from D. <code>  A = D[D.label == k]B = D[D.label != k]",How do I combine two dataframes?
How do I combine two data frames into one data frame?," I'm using Pandas data frames. I have a initial data frame, say D. I extract two data frames from it like this: I want to combine A and B so I can have them as one DataFrame, something like a union operation. The order of the data is not important. However, when we sample A and B from D, they retain their indexes from D. <code>  A = D[D.label == k]B = D[D.label != k]",How do I combine two dataframes?
How do I combine two data frames?," I'm using Pandas data frames. I have a initial data frame, say D. I extract two data frames from it like this: I want to combine A and B so I can have them as one DataFrame, something like a union operation. The order of the data is not important. However, when we sample A and B from D, they retain their indexes from D. <code>  A = D[D.label == k]B = D[D.label != k]",How do I combine two dataframes?
Python: Pip is not recognized as classpath," Here is a screenshot I took.When I try to use pip in command prompt I get the following error message:pip is not recognized as an internal or external command, operable program or batch file.I already checked this thread: How do I install pip on Windows? All I could find there was I had to add ""C:\PythonX\Scripts"" to my classpath, where X stands for python version. As you can see on my screenshot I already have this path. I already tried restarting the computer but that didn't work, o.O. The screenshot also shows my C:\Python27\Scripts folder.Does anyone know what I am doing wrong? <code> ",Python: Pip command is not recognized
Python: Pip is command is not recognized," Here is a screenshot I took.When I try to use pip in command prompt I get the following error message:pip is not recognized as an internal or external command, operable program or batch file.I already checked this thread: How do I install pip on Windows? All I could find there was I had to add ""C:\PythonX\Scripts"" to my classpath, where X stands for python version. As you can see on my screenshot I already have this path. I already tried restarting the computer but that didn't work, o.O. The screenshot also shows my C:\Python27\Scripts folder.Does anyone know what I am doing wrong? <code> ",Python: Pip command is not recognized
why always add self as first argument to class methods," Possible Duplicate: Why do you need explicitly have the self argument into a Python method? I understand why self is always the first argument for class methods, this makes total sense, but if it's always the case, then why go through the hassle of typing if for every method definition? Why not make it something thats automatically done behind the scenes?Is it for clarity or is there a situation where you may not want to pass self as the first argument? <code> ",Why always add self as first argument to class methods?
Remove first word in python string?, What's the quickest/cleanest way to remove the first word of a string? I know I can use split and then iterate on the array to get my string. But I'm pretty sure it's not the nicest way to do it.Ps: I'm quite new to python and I don't know every trick. Thanks in advance for your help. <code> ,Remove the first word in a Python string?
Python: list all installed packages and their versions?," Is there a way in Python to list all installed packages and their versions?I know I can go inside python/Lib/site-packages and see what files and directories exist, but I find this very awkward. What I'm looking for something that is similar to npm list i.e. npm-ls. <code> ",How to list all installed packages and their versions in Python?
list all installed packages and their versions?," Is there a way in Python to list all installed packages and their versions?I know I can go inside python/Lib/site-packages and see what files and directories exist, but I find this very awkward. What I'm looking for something that is similar to npm list i.e. npm-ls. <code> ",How to list all installed packages and their versions in Python?
List all installed packages and their versions?," Is there a way in Python to list all installed packages and their versions?I know I can go inside python/Lib/site-packages and see what files and directories exist, but I find this very awkward. What I'm looking for something that is similar to npm list i.e. npm-ls. <code> ",How to list all installed packages and their versions in Python?
Is de morgans law pythonic?, Which of the following if statements is more Pythonic? OR Its not predicate logic so I should use the Python key words because its more readable right?In the later solution more optimal than the other? (I don't believe so.)Is there any PEP-8 guides on this?Byte code of the two approaches(if it matters): <code>  if not a and not b: do_something if not ( a or b ): do something In [43]: def func1(): if not a and not b: return ....: ....: In [46]: def func2(): if not(a or b): return ....: ....: In [49]: dis.dis(func1) 2 0 LOAD_GLOBAL 0 (a) 3 UNARY_NOT 4 JUMP_IF_FALSE 13 (to 20) 7 POP_TOP 8 LOAD_GLOBAL 1 (b) 11 UNARY_NOT 12 JUMP_IF_FALSE 5 (to 20) 15 POP_TOP 3 16 LOAD_CONST 0 (None) 19 RETURN_VALUE >> 20 POP_TOP 21 LOAD_CONST 0 (None) 24 RETURN_VALUE In [50]: dis.dis(func2) 2 0 LOAD_GLOBAL 0 (a) 3 JUMP_IF_TRUE 4 (to 10) 6 POP_TOP 7 LOAD_GLOBAL 1 (b) >> 10 JUMP_IF_TRUE 5 (to 18) 13 POP_TOP 3 14 LOAD_CONST 0 (None) 17 RETURN_VALUE >> 18 POP_TOP 19 LOAD_CONST 0 (None) 22 RETURN_VALUE ,Is De Morgan's Law Pythonic?
Python CSV - list index out of range," I get this error reading CSV file (no headers, 3 columns, 2nd and 3rd strings): Here is part of the code below. It's stupidly simple stuff to be stuck on, but I'm just blank about how it isn't working. I'm new to coding, but have dealt with csv module before and never had problems with this part, and just made some test csv file in notepad to see if it will be read from the same code, and it does. I don't know.  <code>  Traceback (most recent call last):File ""C:\Python32\fantasy.py"", line 72, in module>some=row[1]IndexError: list index out of range* import csv##############some other code, working good and I believe not relevant to this problemfile=open(r""C:\Users\me\Desktop\file-2.csv"",""r"")reader=csv.reader(file, delimiter=',', quotechar='""')for row in reader: some=row[1]",CSV - list index out of range
CSV - list index out of range," I get this error reading CSV file (no headers, 3 columns, 2nd and 3rd strings): Here is part of the code below. It's stupidly simple stuff to be stuck on, but I'm just blank about how it isn't working. I'm new to coding, but have dealt with csv module before and never had problems with this part, and just made some test csv file in notepad to see if it will be read from the same code, and it does. I don't know.  <code>  Traceback (most recent call last):File ""C:\Python32\fantasy.py"", line 72, in module>some=row[1]IndexError: list index out of range* import csv##############some other code, working good and I believe not relevant to this problemfile=open(r""C:\Users\me\Desktop\file-2.csv"",""r"")reader=csv.reader(file, delimiter=',', quotechar='""')for row in reader: some=row[1]",CSV - list index out of range
"Java: Efficient implementation for: ""Python For Else Loop"""," In Python there is an efficient for else loop implementation described hereExample code: In Java I need to write more code to achieve the same behavior: Is there any better implementation similar to Python for else loop in Java? <code>  for x in range(2, n): if n % x == 0: print n, 'equals', x, '*', n/x breakelse: # loop fell through without finding a factor print n, 'is a prime number' finishedForLoop = true;for (int x : rangeListOfIntegers){ if (n % x == 0) { //syso: Some printing here finishedForLoop = false break; }}if (finishedForLoop == true){ //syso: Some printing here}","Efficient implementation for: ""Python For Else Loop"" in Java"
"Efficient implementation for: ""Python For Else Loop"""," In Python there is an efficient for else loop implementation described hereExample code: In Java I need to write more code to achieve the same behavior: Is there any better implementation similar to Python for else loop in Java? <code>  for x in range(2, n): if n % x == 0: print n, 'equals', x, '*', n/x breakelse: # loop fell through without finding a factor print n, 'is a prime number' finishedForLoop = true;for (int x : rangeListOfIntegers){ if (n % x == 0) { //syso: Some printing here finishedForLoop = false break; }}if (finishedForLoop == true){ //syso: Some printing here}","Efficient implementation for: ""Python For Else Loop"" in Java"
dynamically adding methods to a class in Python," I'm trying to add methods to a class based on a list. The problem is that when I call _Roles.dev(), _Roles.stage(), or _Roles.prod(), I always get printed the last step that is prod instead of getting dev for dev() and so on. what's the reason for this? <code>  class _Roles(object): """""" set the roles for dev, staging and production """""" def __init__(self): from types import MethodType steps = ['dev','stage','prod'] for step in steps: def env_setter(self): print step method = MethodType(env_setter,self,self.__class__) setattr(self,step,method)",Dynamically adding methods to a class
Dynamically adding methods to a class in Python," I'm trying to add methods to a class based on a list. The problem is that when I call _Roles.dev(), _Roles.stage(), or _Roles.prod(), I always get printed the last step that is prod instead of getting dev for dev() and so on. what's the reason for this? <code>  class _Roles(object): """""" set the roles for dev, staging and production """""" def __init__(self): from types import MethodType steps = ['dev','stage','prod'] for step in steps: def env_setter(self): print step method = MethodType(env_setter,self,self.__class__) setattr(self,step,method)",Dynamically adding methods to a class
"how to convert ['_', '_', '_', '_'] into _ _ _ _ in Python"," I am using underscores to represent the length of a unknown word. How can I print just the underscores without the brackets that represent the list?Basically, if I have a list of the form ['_', '_', '_', '_'], I want to print the underscores without printing them in list syntax as ""_ _ _ _"" <code> ",Convert list of strings to space-separated string
"Expandable/Collapsible frame in python, tkinter"," Does anyone know if there is already a widget/class to handle expanding/contracting a frame based on a toggled button (checkbutton) in tkinter/ttk?This question stems from my attempt to clean up a cluttered gui that has lots of options categorized by specific actions. I would like something along the lines of:example found on googleHowever instead of just text, allow for buttons, entries, any of tkinter's widgets. If this doesn't already exist, would it be possible/useful to create a class that inherits the tkinter Frame: Note: this code is untested, just presenting concept <code>  import tkinter as tkimport ttkclass toggledFrame(tk.Frame): def __init__(self): self.show=tk.IntVar() self.show.set(0) self.toggleButton=tk.Checkbutton(self, command=self.toggle, variable=self.show) self.toggleButton.pack() self.subFrame=tk.Frame(self) def toggle(self): if bool(self.show.get()): self.subFrame.pack() else: self.subFrame.forget()",Expandable and contracting frame in Tkinter
How do you join all items in a list in python," I have a list and it adds each letter of a word one by one to this list, I don't know what will be in the list until the program is run. How do I join each letter in the list into one word? e.g. turn ['p', 'y', 't', 'h', 'o', 'n'] into ['python']. <code> ",How do you join all items in a list?
How to kill a Python while loop with a keystroke?," I am reading serial data and writing to a csv file using a while loop. I want the user to be able to kill the while loop once they feel they have collected enough data. I have done something like this using opencv, but it doesn't seem to be working in this application (and i really don't want to import opencv just for this function anyway)... So. How can I let the user break out of the loop?Also, I don't want to use keyboard interrupt, because the script needs to continue to run after the while loop is terminated. <code>  while True: #do a bunch of serial stuff #if the user presses the 'esc' or 'return' key: break # Listen for ESC or ENTER key c = cv.WaitKey(7) % 0x100 if c == 27 or c == 10: break",How to kill a while loop with a keystroke?
"Python: Method in an if statement that could return an error, but still somehow works?"," Below is a simple function to remove duplicates in a list while preserving order. I've tried it and it actually works, so the problem here is my understanding. It seems to me that the second time you run uniq.remove(item) for a given item, it will return an error (KeyError or ValueError I think?) because that item has already been removed from the unique set. Is this not the case?  <code>  def unique(seq): uniq = set(seq) return [item for item in seq if item in uniq and not uniq.remove(item)]","I think this should raise an error, but it doesn't"
PermissionError: [Errno 13] in python," Just starting to learn some Python and I'm having an issue as stated below: Seems to be a file permission error, if any one can shine some light it would be greatly appreciated.NOTE: not sure how Python and Windows files work but I'm logged in to Windows as Admin and the folder has admin permissions.I have tried changing .exe properties to run as Admin. <code>  a_file = open('E:\Python Win7-64-AMD 3.3\Test', encoding='utf-8')Traceback (most recent call last): File ""<pyshell#9>"", line 1, in <module> a_file = open('E:\Python Win7-64-AMD 3.3\Test', encoding='utf-8')PermissionError: [Errno 13] Permission denied: 'E:\\Python Win7-64-AMD 3.3\\Test\",PermissionError: [Errno 13] in Python
(SOLVED) PermissionError: [Errno 13] in python," Just starting to learn some Python and I'm having an issue as stated below: Seems to be a file permission error, if any one can shine some light it would be greatly appreciated.NOTE: not sure how Python and Windows files work but I'm logged in to Windows as Admin and the folder has admin permissions.I have tried changing .exe properties to run as Admin. <code>  a_file = open('E:\Python Win7-64-AMD 3.3\Test', encoding='utf-8')Traceback (most recent call last): File ""<pyshell#9>"", line 1, in <module> a_file = open('E:\Python Win7-64-AMD 3.3\Test', encoding='utf-8')PermissionError: [Errno 13] Permission denied: 'E:\\Python Win7-64-AMD 3.3\\Test\",PermissionError: [Errno 13] in Python
Sublime Text Python Color," I have a problem with sublime text which should be generally all editors. When I have a regular expression like this. All the text after the regular expression will be incorrectly highlighted, because of the [[], specifically the [ without a close bracket. While the intention of this regular expression is correct, the editor doesn't know this.This is just an annoyance I don't know how to deal with. Anyone know how to fix this? <code>  listRegex = re.findall(r'[*][[][[].*', testString)",Sublime Text's syntax highlighting of regexes in python leaks into surrounding code
"Python IOError: [Errno 13] Permission denied when trying to open hidden file in ""w"" mode"," I want to replace the contents of a hidden file, so I attempted to open it in w mode so it would be erased/truncated: But this resulted in a traceback: However, I was able to achieve the intended result with r+ mode: Q. What is the difference between the w and r+ modes, such that one has ""permission denied"" but the other works fine?UPDATE: I am on win7 x64 using Python 2.6.6, and the target file has its hidden attribute set. When I tried turning off the hidden attribute, w mode succeeds. But when I turn it back on, it fails again.Q. Why does w mode fail on hidden files? Is this known behaviour? <code>  >>> import os>>> ini_path = '.picasa.ini'>>> os.path.exists(ini_path)True>>> os.access(ini_path, os.W_OK)True>>> ini_handle = open(ini_path, 'w') IOError: [Errno 13] Permission denied: '.picasa.ini' >>> ini_handle = open(ini_path, 'r+')>>> ini_handle.truncate()>>> ini_handle.write(ini_new)>>> ini_handle.close()","IOError: [Errno 13] Permission denied when trying to open hidden file in ""w"" mode"
Pythonic way to Implement Data Types," The majority of my programming experience has been with C++. Inspired by Bjarne Stroustrup's talk here, one of my favorite programming techniques is ""type-rich"" programming; the development of new robust data-types that will not only reduce the amount of code I have to write by wrapping functionality into the type (for example vector addition, instead of newVec.x = vec1.x + vec2.x; newVec.y = ... etc, we can just use newVec = vec1 + vec2) but will also reveal problems in your code at compile time through the strong type system.A recent project I have undertaken in Python 2.7 requires integer values that have upper and lower bounds. My first instinct is to create a new data type (class) that will have all the same behavior as a normal number in python, but will always be within its (dynamic) boundary values. This is a good start, but it requires accessing the meat of these BoundInt types like so We can add a large number of python's ""magic method"" definitions to the class to add some more functionality: Now the code is rapidly exploding in size, and there is a ton of repetition to what is being written, for very little return, this doesn't seem very pythonic at all.Things get even more complicated when I start to want to construct BoundInt objects from normal python numbers (integers?), and other BoundInt objects Which, as far as I'm aware requires the use of rather large/ugly if/else type checking statements within the BoundInt() constructor, as python does not support (c style) overloading.All of this feels terribly like trying to write c++ code in python, a cardinal sin if one of my favorite books, Code Complete 2, is taken seriously. I feel like I am swimming against the dynamic typing current, instead of letting it carry me forward.I very much want to learn to code python 'pythonic-ally', what is the best way to approach this sort of problem domain? What are good resources to learn proper pythonic style? <code>  class BoundInt: def __init__(self, target = 0, low = 0, high = 1): self.lowerLimit = low self.upperLimit = high self._value = target self._balance() def _balance(self): if (self._value > self.upperLimit): self._value = self.upperLimit elif (self._value < self.lowerLimit): self._value = self.lowerLimit self._value = int(round(self._value)) def value(self): self._balance() return self._value def set(self, target): self._value = target self._balance() def __str__(self): return str(self._value) x = BoundInt()y = 4x.set(y) #it would be nicer to do something like x = yprint y #prints ""4""print x #prints ""1""z = 2 + x.value() #again, it would be nicer to do z = 2 + xprint z #prints ""3"" def __add__(self, other): return self._value + otherdef __sub__(self, other): return self._value - otherdef __mul__(self, other): return self._value * otherdef __div__(self, other): return self._value / otherdef __pow__(self, power): return self._value**powerdef __radd__(self, other): return self._value + other#etc etc x = BoundInt()y = BoundInt(x)z = BoundInt(4)",Pythonic way to Implement Data Types (Python 2.7)
Python QT ProgressBar," When using the following code my application stalls after a couple of seconds.And by stalls I mean hangs. I get a window from Windows saying wait or force close.I might add that this only happens when I click either inside the progress bar window or when I click outside of it so it loses focus. If I start the example and do not touch anything it works like it should. Using this like so: Thanks for any help. <code>  from PyQt4 import QtCorefrom PyQt4 import QtGuiclass ProgressBar(QtGui.QWidget): def __init__(self, parent=None, total=20): super(ProgressBar, self).__init__(parent) self.name_line = QtGui.QLineEdit() self.progressbar = QtGui.QProgressBar() self.progressbar.setMinimum(1) self.progressbar.setMaximum(total) main_layout = QtGui.QGridLayout() main_layout.addWidget(self.progressbar, 0, 0) self.setLayout(main_layout) self.setWindowTitle(""Progress"") def update_progressbar(self, val): self.progressbar.setValue(val) app = QtGui.QApplication(sys.argv)bar = ProgressBar(total=101)bar.show()for i in range(2,100): bar.update_progressbar(i) time.sleep(1)",PyQt ProgressBar
How to install manually a pypi module without pip/easy_install?," I want to use the gntp module to display toaster-like notifications for C/C++ software. I want to package all the dependencies for the software to be self-executable on another computer.The gntp module is only available through the pip installer, which cannot be used (the computer running the software does not have an internet connection)How can I install it from source?I would prefer not to force the user to install easy_install/pip and manually add the pip path to the %PATH.PS: I'm using Python 2.7 on a Windows machine. <code> ",How to manually install a pypi module without pip/easy_install?
How to load the foreign keys elements in tasty pie djnago python," In my Django model, I have 10 fields and there are 3 fields which are foreign keys.In my JSON data which is received from a GET request, I am getting all the fields but not the foreign keys.I have also done this, but I am still not getting those fields in the JSON data: For example, I have the field in model like city, but that field is not available in the JSON I get from it.Is there any way that in JSON I can get city:city__name automatically?If I do this, then I get the city, but can I do that without defining: <code>  DataFields = MyData._meta.get_all_field_names()class MyResource(ModelResource): class Meta: queryset = MyData.objects.all() resource_name = 'Myres' serializer = Serializer(formats=['json']) filtering = dict(zip(DataFields, [ALL_WITH_RELATIONS for f in DataFields])) def dehydrate(self, bundle): bundle.data[""city_name""] = bundle.obj.city__name return bundle",How to load the foreign keys elements in Tastypie
How to load the foreign keys elements in tasty pie django python," In my Django model, I have 10 fields and there are 3 fields which are foreign keys.In my JSON data which is received from a GET request, I am getting all the fields but not the foreign keys.I have also done this, but I am still not getting those fields in the JSON data: For example, I have the field in model like city, but that field is not available in the JSON I get from it.Is there any way that in JSON I can get city:city__name automatically?If I do this, then I get the city, but can I do that without defining: <code>  DataFields = MyData._meta.get_all_field_names()class MyResource(ModelResource): class Meta: queryset = MyData.objects.all() resource_name = 'Myres' serializer = Serializer(formats=['json']) filtering = dict(zip(DataFields, [ALL_WITH_RELATIONS for f in DataFields])) def dehydrate(self, bundle): bundle.data[""city_name""] = bundle.obj.city__name return bundle",How to load the foreign keys elements in Tastypie
python matplotlib get data from plot," I'm using matplotlib in python to build a scatter plot.suppose I have the following 2 data lists.X=[1,2,3,4,5]Y=[6,7,8,9,10]then I use X as the X-axis value and Y as the Y-axis value to make a scatter plot. So I will have a picture with 5 scattering points on it, right?Now the question: is it possible to build connection for these 5 points with the actual data. For example, when I click on one of these 5 points, it can tell me what original data I have used to make this point?thanks in advance <code> ",Get data from plot with matplotlib
Python: How to add psycopg2 to my virtualenv and test it with tox?," I need psycopg2 and lxml for my tests, but when I try to install it in a virtualenv through tox it fails due to the missing pg_conf or other dependencies.I found this explanation of bootstrap scripts: http://www.virtualenv.org/en/latest/index.html#bootstrap-exampleHow can I add a bootstrap script to tox's virtualenv? Do you know any good examples for my concerns (lxml and psycopg2)? <code> ",How to add a bootstrap script to tox's virtualenv?
Python: How to add a bootstrap script to tox's virtualenv?," I need psycopg2 and lxml for my tests, but when I try to install it in a virtualenv through tox it fails due to the missing pg_conf or other dependencies.I found this explanation of bootstrap scripts: http://www.virtualenv.org/en/latest/index.html#bootstrap-exampleHow can I add a bootstrap script to tox's virtualenv? Do you know any good examples for my concerns (lxml and psycopg2)? <code> ",How to add a bootstrap script to tox's virtualenv?
get a list of all routes defined in the app, I have a complex Flask-based web app. There are lots of separate files with view functions. Their URLs are defined with the @app.route('/...') decorator. Is there a way to get a list of all the routes that have been declared throughout my app? Perhaps there is some method I can call on the app object? <code> ,Get list of all routes defined in the Flask app
I dont understand Python's main block. What is that thing? Total Noob in Python. Java guy, Possible Duplicate: What does <if __name__==__main__:> do? So I start up pyscripter and I get a file with this in it: What is that? Why does my program work without it as well? Whats the purpose for this anyway?Where would my code go? Lets say a a function that prints hello world. Where would that go? where would I call it? <code>  def main(): passif __name__ == '__main__': main(),I don't understand Python's main block. What is that thing?
Python: Share singleton across modules," Given two modules, main and x with the following contents:main: and x, respectively: I would now expect that executing main would yield the same IDs and the value twenty in both instances, but what I get is this: Is there any way I can ensure that uvw is the same object at both places? <code>  class Singleton(object): _instance = None def __new__(cls, *args, **kwargs): if not cls._instance: cls._instance = super(Singleton, cls).__new__(cls, *args, **kwargs) cls._instance.x = 10 return cls._instanceuvw = Singleton()if __name__ == ""__main__"": print(id(uvw)) uvw.x += 10 print(uvw.x) import x import mainprint(id(main.uvw))print(main.uvw.x) $ python main.py1405928617771682014059286120750410",Share a singleton across modules
Dont understand this Python block of code. Multiplying boolean with float?," I don't understand the line q.append(p[i] * (hit * pHit + (1-hit) * pMiss)), because the variable hit is a boolean value. That boolean value comes from hit = (Z == world[i]) What's going on there? I only have a basic understanding of Python...  <code>  p = [0.2, 0.2, 0.2, 0.2, 0.2]world = ['green', 'red', 'red', 'green', 'green']Z = 'red'pHit = 0.6pMiss = 0.2def sense(p, Z): q=[] for i in range(len(p)): hit = (Z == world[i]) q.append(p[i] * (hit * pHit + (1-hit) * pMiss)) s = sum(q) for i in range(len(p)): q[i]=q[i]/s return qprint sense(p,Z)",Multiplying boolean with float?
Don't understand this Python block of code. Multiplying boolean with float?," I don't understand the line q.append(p[i] * (hit * pHit + (1-hit) * pMiss)), because the variable hit is a boolean value. That boolean value comes from hit = (Z == world[i]) What's going on there? I only have a basic understanding of Python...  <code>  p = [0.2, 0.2, 0.2, 0.2, 0.2]world = ['green', 'red', 'red', 'green', 'green']Z = 'red'pHit = 0.6pMiss = 0.2def sense(p, Z): q=[] for i in range(len(p)): hit = (Z == world[i]) q.append(p[i] * (hit * pHit + (1-hit) * pMiss)) s = sum(q) for i in range(len(p)): q[i]=q[i]/s return qprint sense(p,Z)",Multiplying boolean with float?
Code128 Barcode as HTML Image Tag with Data URI Scheme in Python," I need to create an Code128 Barcodes with Python/Django which have to be embeded in HTML document. I don't want to make any temporary (or cache) files on the disk. That's why I want to embed them as Data URI Scheme. The result have to be something like this: Can you recommend me an easy way to do this?Now I use ReportLab to create such a barcodes and embed them in PDF files, but I don't know how to export them as Data URI Scheme. If this is the recommended way to do this. <code>  <img src=""data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAHElEQVQI12P4//8/w38GIAXDIBKE0DHxgljNBAAO9TXL0Y4OHwAAAABJRU5ErkJggg=="" alt=""Red dot"">",Reportlab's Code128 Barcode as HTML Image Tag with Data URI Scheme in Python
SqlAlchemy filter: match all instead of any," I want to query a junction table for the value of column aID that matches all values of a list of ids ids=[3,5] in column bID.This is my junction table (JT): I have this query: session.query(JT.aID).filter(JT.bID.in_(ids)).all()This query returns the aID values 1, 2 and 3 because they all have rows with either 3 or 5 in the bID column. What I want the query to return is 2 because that is the only aID value that has all values of the ids list in its bID column.Don't know how to explain the problem better, but how can I get to the result? <code>  aID bID 1 1 1 2 2 5 2 3 1 3 3 5",SqlAlchemy: filter to match all instead of any values in list?
Python split a list into subsets based on pattern," I'm doing this but it feels this can be achieved with much less code. It is Python after all. Starting with a list, I split that list into subsets based on a string prefix. <code>  # Splitting a list into subsets# expected outcome:# [['sub_0_a', 'sub_0_b'], ['sub_1_a', 'sub_1_b']]mylist = ['sub_0_a', 'sub_0_b', 'sub_1_a', 'sub_1_b']def func(l, newlist=[], index=0): newlist.append([i for i in l if i.startswith('sub_%s' % index)]) # create a new list without the items in newlist l = [i for i in l if i not in newlist[index]] if len(l): index += 1 func(l, newlist, index)func(mylist)",How to split a list into subsets based on a pattern?
"Why does this not throw an error, and what is Python testing for?"," Without specifying the equality comparison properties of objects, Python is still doing something when using > and <. What is Python actually comparing these objects by if you don't specify __gt__ or __lt__? I would expect an unsupported operand error here, as you get when trying to add two objects together without defing __add__.  <code>  In [1]: class MyObject(object): ...: pass ...: In [2]: class YourObject(object): ...: pass ...: In [3]: me = MyObject()In [4]: you = YourObject()In [5]: me > youOut[5]: FalseIn [6]: you > meOut[6]: True","Custom class ordering: no error thrown, what is Python testing for?"
probabily density function from histogram in python to fit another histrogram," I have a question concerning fitting and getting random numbers.Situation is as such:Firstly I have a histogram from data points. I would like to interpret this histogram as probability density function (with e.g. 2 free parameters) so that I can use it to produce random numbers AND also I would like to use that function to fit another histogram.Thanks for your help <code>  import numpy as np""""""create random data points """"""mu = 10sigma = 5n = 1000datapoints = np.random.normal(mu,sigma,n)"""""" create normalized histrogram of the data """"""bins = np.linspace(0,20,21)H, bins = np.histogram(data,bins,density=True)",probability density function from histogram in python to fit another histrogram
Python - empty_string in some_string - always true?," Possible Duplicate: Why empty string is on every string? I wonder why Python returns True whenever I check if the empty string is in a string, and why its index is zero.For instance:'' in '' => true''.index('') => 0'' in 'notEmpty' => true'notEmpty'.index('') => 0I noticed it when I was writing a ROT13 function, and testing it I found that when I call it on an empty string, it returns 'n' ('n' is index13 in the alphabet). <code> ",empty_string in some_string - always true?
Python Force python to keep leading zeros of int variables," Below is a section of code which is part of a functional decryption and encryption program. As you can see the two variables i and j are first treated as string to add a 0 in front of any single digit numbers (as string). These variables then are combined to make a four digit number (still as a string). Later in the program the number created are used in a pow() function, as ints remove any leading zeros.My question: Is it possible to force python to keep the leading zero for ints? I have and continued to search online.EditTo help people I have included the encryption part of the program. This is where the problem lies. The variables created in the above code are passed through a pow() function. As this can't handle strings I have to convert the variables to ints where the leading zero is lost. <code>  while checkvar < maxvar: # is set to < as maxvar is 1 to high for the index of var #output.append(""%02d"" % number) i =ord(var[checkvar]) - 64 # Gets postional value of i i = (""%02d"" % i) if (checkvar + 1) < maxvar: j =ord(var[(checkvar + 1)]) - 64 # Gets postional value of i j = (""%02d"" % j) i = str(i) + str(j) #'Adds' the string i and j to create a new i li.append(int(i)) checkvar = checkvar + 2print li #a = li[]b=int(17)#pulic = ec=int(2773)#=nlenli=int(len(li))enchecker = int(0)#encrpted listenlist = []while enchecker < lenli: en = pow(li[enchecker],b,c)#encrpyt the message can only handle int enlist.append(int(en)) enchecker = enchecker + 1print enlist",How to retain leading zeros of int variables?
Python not working in command prompt?," When I type python into the command line, the command prompt says python is not recognized as an internal or external command, operable program, or batch file. What should I do?Note: I have Python 2.7 and Python 3.2 installed on my computer. <code> ",Python command not working in command prompt
Is there any way to add a new global variable inside a python function?," How do I declare a global variable in a function in Python? That is, so that it doesn't have to be declared before but can be used outside of the function. <code> ",Is it possible to define global variables in a function in Python
How to Declare Global Variables in Python," How do I declare a global variable in a function in Python? That is, so that it doesn't have to be declared before but can be used outside of the function. <code> ",Is it possible to define global variables in a function in Python
Declare global variables in Python," How do I declare a global variable in a function in Python? That is, so that it doesn't have to be declared before but can be used outside of the function. <code> ",Is it possible to define global variables in a function in Python
How to ignore task result in celery chord or chain?," I'm using celery, I have several tasks which needed to be executed in order.For example I have this task: And I want to do something like this: Then I get TypeError: tprint() takes exactly 1 argument (2 given).The same with chord, in this situation which I need a task to be executed after a group of tasks: So how to deal with this situation? I don't care the result of each task, but they need to be executed in order.Add a second parameter won't work: This will print out 'a' and 'None'. <code>  @celery.taskdef tprint(word): print word >>> chain(tprint.s('a') | tprint.s('b'))() >>> chord([tprint.s('a'), tprint.s('b')])(tprint.s('c')) @celery.taskdef tprint(word, ignore=None): print word>>> chain(tprint.s('a', 0) | tprint.s('b'))()",Celery: How to ignore task result in chord or chain?
Which way should I construct a console menu with classes?," I've created a console menu in both C++ and Python, but I suppose the language isn't in too big role here, since I'm asking about the structure of classes.So what I'm trying to achieve, is a menu similar to MS-DOS, where I can have parent menus (folders) and action menus (files). Here's what it could look like in the console, once printed: So as you see, I got two types of menus here;Directory (MS-DOS folder) menus, which hold other menus inside them. When activated, they open/close. For example: Root directory is now open and you can see all the menus inside it. If it gets closed, the [-] turns into [+] and you can't see any of the other menus.Action (MS-DOS file) menus, which are linked to a function. When activated, they call the function they're linked to. For example: Open snakegame could be linked to a function startSnakeGame(), which would close the menu and start the snake game.I've already coded two working implementations to get the wanted result, and I was just wondering, which one should I use? The first way is, where I only have one class called Menu, and it has all the member variables and methods coded into one class. The other way is, where I got these two different types of menus separated into two classes, with a common base class.Here are some member variables, I'll split them into three sections (base class, directory class and action class) for now, but they can be combined into one class. Base Menu:parent = A menu (directory one) that holds this/self inside a list/vector as a child (see below).label = Obviously the label which gets displayed when the menu is printed.selected = Boolean value which tells wether the menu is currently selected (fe. pointed by mouse).Directory Menu:subMenus = A list or a vector (in C++) that holds other menus inside it.open = A boolean value which tells wether the menu is open or closed.Action Menu:action = Pointer to the function which gets called when this menu gets activated.As you can see, there are just few variables which differ from the other class, and it could be set so that if action == 0 (no action) then the menu automatically changes open to false/true depending on it's current value. This way action menu would be terminated, and only down side would be that action menus would hold subMenus and closed without use.This might be all about one's opinion, but I've been thinking about this for a while, and couldn't find one way superior to other, they both have their advantages and downsides, and both work well. So I'm asking about your opinion, and I would love to hear if anyone has any reasons why they'd choose one over the other. Basically I'm asking for the reasons, I don't care about your opinion.There will be no other menu types than folder and file, so the base class could not be used for anything else.Edit: A simple Python and C++ example on how the menus are being used:Python with only one class: Python with multiple classes: C++ with one class: C++ with multiple classes: 2nd Edit: I had only implemented both ways in Python, and only the one-class way in C++. So I started to code the multi-class way in C++ too, just for fun and practice, and I ran into a problem; having one base class, I cannot add this to parent's subMenus-vector, since base class doesn't own subMenus, and base class cannot know DirectoryMenu.So I will have to hack my way through this, which is a BIGminus. Unless someone can think of a good way to implement it? <code>  [-] Root directory Open snakegame [-] First sub directory Print out stupid messages [+] Closed directory in open directory Shutdown computer [+] These directories are closed [+] You can't see the content inside them Quit menu # Using default param. here to set ""action = None"" or ""action = toggleOpen()""root = Menu(None, ""Root directory"")snake = Menu(root, ""Open snakegame"", startSnakeGame)sub1 = Menu(root, ""First sub directory"")printMsg = Menu(sub1, ""Print out stupid messages"")... # With multiple classes, action parameter no longer existsroot = DirectoryMenu(None, ""Root directory"")snake = ActionMenu(root, ""Open snakegame"", startSnakeGame)... Menu* root = new Menu(0, ""Root directory"");Menu* snake = new Menu(&root, ""Open snakegame"", &startSnakeGame);... DirectoryMenu* root = new DirectoryMenu(0, ""Root directory"");ActionMenu* snake = new ActionMenu(&root, ""Open snakegame"", &startSnakeGame);... BaseMenu::BaseMenu(BaseMenu* parent): m_parent(parent) // works{ m_parent->addSubMenuk(this); // BaseMenu doesn't have Directory's addSubMenu()} ",Best way to construct a console menu class hierarchy?
Best way to construct a console menu class hierachy?," I've created a console menu in both C++ and Python, but I suppose the language isn't in too big role here, since I'm asking about the structure of classes.So what I'm trying to achieve, is a menu similar to MS-DOS, where I can have parent menus (folders) and action menus (files). Here's what it could look like in the console, once printed: So as you see, I got two types of menus here;Directory (MS-DOS folder) menus, which hold other menus inside them. When activated, they open/close. For example: Root directory is now open and you can see all the menus inside it. If it gets closed, the [-] turns into [+] and you can't see any of the other menus.Action (MS-DOS file) menus, which are linked to a function. When activated, they call the function they're linked to. For example: Open snakegame could be linked to a function startSnakeGame(), which would close the menu and start the snake game.I've already coded two working implementations to get the wanted result, and I was just wondering, which one should I use? The first way is, where I only have one class called Menu, and it has all the member variables and methods coded into one class. The other way is, where I got these two different types of menus separated into two classes, with a common base class.Here are some member variables, I'll split them into three sections (base class, directory class and action class) for now, but they can be combined into one class. Base Menu:parent = A menu (directory one) that holds this/self inside a list/vector as a child (see below).label = Obviously the label which gets displayed when the menu is printed.selected = Boolean value which tells wether the menu is currently selected (fe. pointed by mouse).Directory Menu:subMenus = A list or a vector (in C++) that holds other menus inside it.open = A boolean value which tells wether the menu is open or closed.Action Menu:action = Pointer to the function which gets called when this menu gets activated.As you can see, there are just few variables which differ from the other class, and it could be set so that if action == 0 (no action) then the menu automatically changes open to false/true depending on it's current value. This way action menu would be terminated, and only down side would be that action menus would hold subMenus and closed without use.This might be all about one's opinion, but I've been thinking about this for a while, and couldn't find one way superior to other, they both have their advantages and downsides, and both work well. So I'm asking about your opinion, and I would love to hear if anyone has any reasons why they'd choose one over the other. Basically I'm asking for the reasons, I don't care about your opinion.There will be no other menu types than folder and file, so the base class could not be used for anything else.Edit: A simple Python and C++ example on how the menus are being used:Python with only one class: Python with multiple classes: C++ with one class: C++ with multiple classes: 2nd Edit: I had only implemented both ways in Python, and only the one-class way in C++. So I started to code the multi-class way in C++ too, just for fun and practice, and I ran into a problem; having one base class, I cannot add this to parent's subMenus-vector, since base class doesn't own subMenus, and base class cannot know DirectoryMenu.So I will have to hack my way through this, which is a BIGminus. Unless someone can think of a good way to implement it? <code>  [-] Root directory Open snakegame [-] First sub directory Print out stupid messages [+] Closed directory in open directory Shutdown computer [+] These directories are closed [+] You can't see the content inside them Quit menu # Using default param. here to set ""action = None"" or ""action = toggleOpen()""root = Menu(None, ""Root directory"")snake = Menu(root, ""Open snakegame"", startSnakeGame)sub1 = Menu(root, ""First sub directory"")printMsg = Menu(sub1, ""Print out stupid messages"")... # With multiple classes, action parameter no longer existsroot = DirectoryMenu(None, ""Root directory"")snake = ActionMenu(root, ""Open snakegame"", startSnakeGame)... Menu* root = new Menu(0, ""Root directory"");Menu* snake = new Menu(&root, ""Open snakegame"", &startSnakeGame);... DirectoryMenu* root = new DirectoryMenu(0, ""Root directory"");ActionMenu* snake = new ActionMenu(&root, ""Open snakegame"", &startSnakeGame);... BaseMenu::BaseMenu(BaseMenu* parent): m_parent(parent) // works{ m_parent->addSubMenuk(this); // BaseMenu doesn't have Directory's addSubMenu()} ",Best way to construct a console menu class hierarchy?
Python: TypeError: unhashable type: 'list'," I'm trying to take a file that looks like this: And use a dictionary to so that the output looks like this This is what I've tried I keep getting a TypeError: unhashable type: 'list'. I know that keys in a dictionary can't be lists but I'm trying to make my value into a list not the key. I'm wondering if I made a mistake somewhere. <code>  AAA x 111AAB x 111AAA x 112AAC x 123... {AAA: ['111', '112'], AAB: ['111'], AAC: [123], ...} file = open(""filename.txt"", ""r"") readline = file.readline().rstrip()while readline!= """": list = [] list = readline.split("" "") j = list.index(""x"") k = list[0:j] v = list[j + 1:] d = {} if k not in d == False: d[k] = [] d[k].append(v) readline = file.readline().rstrip()",How to overcome TypeError: unhashable type: 'list'
Pandas DataFrame: remove unwanted parts from strings in a column," I am looking for an efficient way to remove unwanted parts from strings in a DataFrame column.Data looks like: I need to trim these data to: I tried .str.lstrip('+-') and .str.rstrip('aAbBcC'), but got an error: Any pointers would be greatly appreciated! <code>  time result1 09:00 +52A2 10:00 +62B3 11:00 +44a4 12:00 +30b5 13:00 -110a time result1 09:00 522 10:00 623 11:00 444 12:00 305 13:00 110 TypeError: wrapper() takes exactly 1 argument (2 given)",Remove unwanted parts from strings in a column
How do i hide a sub Menu in QMenu," I have an application where I generate menu items, and I want to set the visibility of a particular sub-menu.I tried using setVisibility(False), but this did not work.setVisibility() works for menu items, but not for sub-menus in QMenus.Have a look at the code snippet below: In the above example, I can hide the menu item named ""One"", but not the sub-menu named ""submenu 2""Can anyone give me an idea... <code>  import sysfrom PyQt4 import QtGuiclass Window(QtGui.QWidget): def __init__(self, parent=None): super(Window, self).__init__(parent) self.menu = QtGui.QMenu() self.actio1 = QtGui.QAction('One', self) self.actio2 = QtGui.QAction('Two', self) self.menu.addAction(self.actio1) self.menu.addAction(self.actio2) self.actio1.setVisible(False) self.submenu = QtGui.QMenu('submenu', self) self.submenu.addAction('sub one') self.submenu.addAction('sub two') self.menu.addMenu(self.submenu) self.submenu2 = QtGui.QMenu('submenu 2', self) self.submenu2.addAction('sub 2 one') self.submenu2.addAction('sub 2 two') self.menu.addMenu(self.submenu2) self.submenu2.setVisible(False) layout = QtGui.QHBoxLayout() layout.addWidget(self.menu) self.setLayout(layout)if __name__ == '__main__': app = QtGui.QApplication(sys.argv) w = Window() w.show() sys.exit(app.exec_())",How do I hide a sub-menu in QMenu
keyboard input between select()," I write some codes to get the input from keyboard and also check something is alive or not: I found that when the keyboard input ends outside of the waiting in select() (usually it ends during the 5 secs of is_alive()), the if rlist: will get false.I can understand why but I don't know how to solve it.And there is still another question related to the situation mentioned above, sometimes readline() will return the last line of my input when some inputs are located across different select() waiting.That means, if I enter 'abc\n' and unfortunately the '\n' located outside of wating in select() (that means, when I press Enter, the program are executing other parts, such as is_alive()), and then if I enter 'def\n' and this time the Enter pressed successfully located within select(), I'll see the s from readline() becomes 'def\n' and the first line is disappeared.Is there any good solution to solve two issues above? I'm using FreeBSD 9.0. <code>  import sysfrom select import selecttimeout = 10while is_alive(): # is_alive is a method to check some stuffs, might take 5 secs rlist, _, _ = select([sys.stdin], [], [], timeout) if rlist: s = sys.stdin.readline() print repr(s) handle(s) # handle is a method to handle and react according to input s",Keyboard input between select() in Python
"Is ""*_"" an acceptable way to ignore arguments in python"," If I have a function/method that is an implementation of a callback for some framework, and do not care for any further arguments, it seems to be syntactically correct, and to not have pylint/IDE complaints to use *_ to express no interest in any further arguments. The point I think is to express intent to both the tools, and other developers that these arguments are not currently relevant.To clarify what I mean: I've not seen this idiom used in the wild - is it common, are there examples and are there known problems with this?For those not familiar: _ expresses the intent that I am not interested in that symbol - it is the Python ""dummy"" that is recognized by IDE's and linters. <code>  def my_callbacK_handler(a, b, *_): ...","Is ""*_"" an acceptable way to ignore arguments in Python?"
why is this string comparison returning False?," Possible Duplicate: String comparison in Python: is vs. == I'm running it from the command line with the argument first, so why does that code output: <code>  algorithm = str(sys.argv[1])print(algorithm)print(algorithm is ""first"") firstFalse",Why is this string comparison returning False?
numpy recfunctions append_field gives shape mismatch for field with," I have a structured numpy array, I want to use the recfunctions libraryhttp://pyopengl.sourceforge.net/pydoc/numpy.lib.recfunctions.htmlfunction append_fields() or rec_append_fields() to append a field with someshape to it. However, I get an error: ValueError: operands could not be broadcast together with shapes (10) (10,3) where 10 is the length of my existing array, and (3,) is the shape of the field I want to append.For example: ValueError: operands could not be broadcast together with shapes (4) (4,2) Any ideas? I tried making my_new_field a list of tuples and putting a dtypeargument with the proper shape into the append_fields(): but that seems to end up the same once it gets converted to a numpy array.None of this seems to change when I use rec_append_fields() instead of simplyappend_fields()EDIT:In light of the fact that my new field doesn't have the same shape as my array, I suppose that my desired append is impossible, suggested by @radicalbiscuit. But, I included one of the original fields in the array with shape different from the original array to make my point, which is that a field does not have to have the same shape as the structured array. How can I append a field like this? I should note that for my application, I can append an empty field as long as it's possible to somehow change the shape later. Thanks! <code>  import numpy as npfrom numpy.lib.recfunctions import append_fieldsmy_structured_array = np.array( zip([0,1,2,3],[[4.3,3.2],[1.4,5.6],[6.,2.5],[4.5,5.4]]), dtype=[('id','int8'),('pos','2float16')] )my_new_field = np.ones( len(my_structured_array), dtype='2int8' )my_appended_array = append_fields( my_structured_array, 'new', data=my_new_field ) my_new_field = len(my_structured_array)*[(1,1)]my_appended_array = append_fields( my_structured_array, 'new', data=my_new_field, dtype='2int8' ) In : my_new_field.shapeOut: (4, 2)In : my_structured_array.shapeOut: (4,) In : my_structured_array['pos'].shapeOut: (4, 2)In : my_new_field.shapeOut: (4, 2)",numpy append_field gives shape error for new field with 2d shape
Python [pygame] Documentation on pygame is crap. Just looking for some pointers," So I've gotten to the point in my program where I need to create a group for some sprites that the player can collide with without dying (like some other sprites I may have on screen).I've scoured Google but it appears that the official pygame documentation is useless and/or hard to comprehend. I'm looking for just a wee bit of help from anyone who knows a bit about this.First, I need to find out how to create a group. Does it go in the initial game setup?Then adding a sprite to a group upon its creation. The pygame site has this to say on the subject: So... how does one use this? Let's say I have an sprite named gem. I need to add gem to the gems group. Is it: I doubt it, but without any examples to go off of on the site, I am at a loss.Furthermore, I would love to be able to edit attributes for a certain group. Is this done by defining a group like I would a class? Or is it something I define within the definition for the existing sprite, but with an 'if sprite in group'? <code>  Sprite.add(*groups) gem = Sprite.add(gems)",How to use sprite groups in pygame
Python - Pytz - List of Timezones?, I would like to know what are all the possible values for the timezone argument in the Python library pytz. How to do it? <code> ,Is there a list of Pytz Timezones?
How To Use The Pass Statement In Python, I am in the process of learning Python and I have reached the section about the pass statement. The guide I'm using defines it as being a null statement that is commonly used as a placeholder.I still don't fully understand what that means though. What would be a simple/basic situation where the pass statement would be used and why would it be needed? <code> ,How to use the pass statement
How to use the pass statement In Python, I am in the process of learning Python and I have reached the section about the pass statement. The guide I'm using defines it as being a null statement that is commonly used as a placeholder.I still don't fully understand what that means though. What would be a simple/basic situation where the pass statement would be used and why would it be needed? <code> ,How to use the pass statement
How to use the pass statement in Python, I am in the process of learning Python and I have reached the section about the pass statement. The guide I'm using defines it as being a null statement that is commonly used as a placeholder.I still don't fully understand what that means though. What would be a simple/basic situation where the pass statement would be used and why would it be needed? <code> ,How to use the pass statement
How to use the pass statement?, I am in the process of learning Python and I have reached the section about the pass statement. The guide I'm using defines it as being a null statement that is commonly used as a placeholder.I still don't fully understand what that means though. What would be a simple/basic situation where the pass statement would be used and why would it be needed? <code> ,How to use the pass statement
cimport interactive interpreter error," Running cimport cython or cimport numpy in the Python interpreter results in the following error: Is it environment variables path problem? Or is it not supposed to be run in the interpreter? Please, help. I spent several days trying to get rid of the error.(By the way, I do not get an error when compiling .pyx files that use cimport numpy...)Thank you!Oleg <code>  cimport cython File ""<interactive input>"", line 1 cimport cython ^SyntaxError: invalid syntax",`cimport` causes error in interactive Python interpreter
"PHP equivalent to python's """""""," Possible Duplicate: php string escaping like pythons ? The triple-quotes in python escapes all quotes and newlines contained within. For example, Does anybody know if PHP has an equivalent to python's EDIT:For those looking at this question in the future, I have answered it, here is an example:$heading = ""Heading Gettizburgz""; print <<< END <p><h1>$heading</h1> ""in quotes"" 'in single' Four score and seven years ago<br/> our fathers set onto this continent<br/> (and so on ...)<br/> </p>END;prints:Heading Gettizburgz""in quotes"" 'in single' Four score and seven years agoour fathers set onto this continent(and so on ...)Note one important thing, you must make sure that the very last END is to the far left (fist column) of your code without ANY spaces before it.source: http://alvinalexander.com/blog/post/php/php-here-document-heredoc-syntax-examples <code>  """""" thisis alljust one string. I can even tell you ""I like pi"".Notice that the single quotes are escaped - they don't end the string; and the newlines become part of the string"""""" """""" <form><h1>PUT HTML HERE</h1> </form> """"""enter code here",PHP equivalent to python's triple-quotes - How to print bulk / lots of HTML within PHP without escaping
PHP equivalent to python's triple-quotes," Possible Duplicate: php string escaping like pythons ? The triple-quotes in python escapes all quotes and newlines contained within. For example, Does anybody know if PHP has an equivalent to python's EDIT:For those looking at this question in the future, I have answered it, here is an example:$heading = ""Heading Gettizburgz""; print <<< END <p><h1>$heading</h1> ""in quotes"" 'in single' Four score and seven years ago<br/> our fathers set onto this continent<br/> (and so on ...)<br/> </p>END;prints:Heading Gettizburgz""in quotes"" 'in single' Four score and seven years agoour fathers set onto this continent(and so on ...)Note one important thing, you must make sure that the very last END is to the far left (fist column) of your code without ANY spaces before it.source: http://alvinalexander.com/blog/post/php/php-here-document-heredoc-syntax-examples <code>  """""" thisis alljust one string. I can even tell you ""I like pi"".Notice that the single quotes are escaped - they don't end the string; and the newlines become part of the string"""""" """""" <form><h1>PUT HTML HERE</h1> </form> """"""enter code here",PHP equivalent to python's triple-quotes - How to print bulk / lots of HTML within PHP without escaping
Find basename of path," I have a path: and I would like to determine what the base is. In this example it should return ""foo"". There are a few ways I have tried: Is there a more 'Pythonic' way to access the root directory?i.e. <code>  path = foo/bar/baz root = re.search('(.+?)/(.+)', path).group(1)paths = path.split('/')[0]root = paths[0] if paths[0] or len(paths) <= 1 else '/'.join(paths[0:2])def rootname(path): head,tail = os.path.split(path) if head != '': return rootname(head) else: return pathroot = rootname(path) root = os.path.''rootname''(path)",Find root of path
"When to use Tornado, when to use Twisted / Cyclone / GEvent"," Which of these frameworks / libraries would be the best choise for building modern multiuser web application? I would love to have an asynchronous webserver which will allow me to scale easly.What solution will give the best performance / scalability / most useful framework (in terms of easy of use and easy of developing)?It would be great if it will provide good functionality (websockets, rpc, streaming, etc).What are the pros and cons of each solution? <code> ","When to use Tornado, when to use Twisted / Cyclone / GEvent / other"
How can i make the text box as choice field in django," I want to display the choice field in forms.This is my model but I am not able to see the select box, it displays as textarea. Do I need to put something in my form as well?In my previous project I did the same thing. I defined everything in my model and saw the select box but not here. <code>  SOURCE_CHOICES = Choices( ('var1', '1'), ('var2', '2'))source = models.TextField(choices=SOURCE_CHOICES, null=True, blank=True)",How can I make a text box a choice field?
update cookies in session using python-requests module," I'm using python-requests module to handle oAuth request and response.I want to set received access_token (response content as dict) in requests.session.cookies object.How can I update existing cookies of session with received response from server?[EDIT] I want to do something like: Here, requests.utils.dict_from_cookiejar(self.session.cookies) returns dict with one session key. Now, I want to update received response content in self.session.cookies. <code>  self.session = requests.session(auth=self.auth_params)resp = self.session.post(url, data=data, headers=self.headers)content = resp.content requests.utils.dict_from_cookiejar(self.session.cookies).update(content)",Update Cookies in Session Using python-requests Module
Python Requests with multiple connections," I use the Python Requests library to download a big file, e.g.: The big file downloads at +- 30 Kb per second, which is a bit slow. Every connection to the bigfile server is throttled, so I would like to make multiple connections.Is there a way to make multiple connections at the same time to download one file? <code>  r = requests.get(""http://bigfile.com/bigfile.bin"")content = r.content",Requests with multiple connections
Merge/join lists of tuples based on a common value in Python," I have two lists of dictionaries (returned as Django querysets). Each dictionary has an ID value. I'd like to merge the two into a single list of dictionaries, based on the ID value.For example: and I want a function to yield: Additional points to note:The IDs in the lists may not be in the same order (as with the example above).The lists will probably have the same number of elements, but I want to account for the option if they're not but keeping all the values from list_a (essentially list_a OUTER JOIN list_b USING user__id).I've tried doing this in SQL but it's not possible since some of the values are aggregates based on some exclusions.It's safe to assume there will only be at most one dictionary with the same user__id in each list due to the database queries used.Many thanks for your time. <code>  list_a = [{'user__name': u'Joe', 'user__id': 1}, {'user__name': u'Bob', 'user__id': 3}]list_b = [{'hours_worked': 25, 'user__id': 3}, {'hours_worked': 40, 'user__id': 1}] list_c = [{'user__name': u'Joe', 'user__id': 1, 'hours_worked': 40}, {'user__name': u'Bob', 'user__id': 3, 'hours_worked': 25}]",Merge/join lists of dictionaries based on a common value in Python
Python - find integer index of rows with NaN in pandas," I have a pandas DataFrame like this: Is there an efficient way to find the ""integer"" index of rows with NaNs? In this case the desired output should be [3, 6]. <code>  a b2011-01-01 00:00:00 1.883381 -0.4166292011-01-01 01:00:00 0.149948 -1.7821702011-01-01 02:00:00 -0.407604 0.3141682011-01-01 03:00:00 1.452354 NaN2011-01-01 04:00:00 -1.224869 -0.9474572011-01-01 05:00:00 0.498326 0.0704162011-01-01 06:00:00 0.401665 NaN2011-01-01 07:00:00 -0.019766 0.5336412011-01-01 08:00:00 -1.101303 -1.4085612011-01-01 09:00:00 1.671795 -0.764629",Find integer index of rows with NaN in pandas dataframe
Python putting an if-elif-else statement on one line," I have read the links below, but it doesn't address my question.Does Python have a ternary conditional operator? (the question is about condensing if-else statement to one line)Is there an easier way of writing an if-elif-else statement so it fits on one line?For example, Or a real-world example: I just feel if the example above could be written the following way, it could look like more concise. <code>  if expression1: statement1elif expression2: statement2else: statement3 if i > 100: x = 2elif i < 100: x = 1else: x = 0 x=2 if i>100 elif i<100 1 else 0 [WRONG]",Putting an if-elif-else statement on one line?
Flask Optional URL parameters," Is it possible to directly declare a flask URL optional parameter?Currently I'm proceeding the following way: How can I directly say that username is optional? <code>  @user.route('/<userId>')@user.route('/<userId>/<username>')def show(userId, username=None): pass",Can Flask have optional URL parameters?
Python - Saving a File being editted in GNU Nano 2.2.4, I'm very new to programming and playing around with a Raspberry Pi and following tutorials on Youtube.I have opened a file in GNU Nano 2.2.6 e.g: nano my_File.py and changed some of the data.I'm struggling on how to overwrite the file (or save it) because when i run it in a new window it uses the original data...Thanks. <code> ,Python - Saving a File being edited in GNU Nano 2.2.4
python3 imaplib.fetch TypeError: can't concat bytes to int," I have some code which fetches IMAP emails and works completely well in Python 2. In Python3 I get the following error: Traceback (most recent call last): File ""./mail.py"", line 295, in item=return_message(x) File ""./mail.py"", line 122, in return_message result, data = mail.fetch(message_id, ""(RFC822)"") File ""/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/imaplib.py"", line 460, in fetch typ, dat = self._simple_command(name, message_set, message_parts) File ""/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/imaplib.py"", line 1113, in _simple_command return self._command_complete(name, self._command(name, *args)) File ""/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/imaplib.py"", line 883, in _command data = data + b' ' + arg TypeError: can't concat bytes to intThe code from the return_message function: Runtime Information: 3.3.0 (v3.3.0:bd8afb90ebf2, Sep 29 2012, 01:25:11) [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] <code>  result, data = mail.fetch(message_id, ""(RFC822)"")raw_email = data[0][1]email_message = email.message_from_string(raw_email)",The quick brown fox jumped over the lazy dog
Python 3 imaplib.fetch TypeError: can't concat bytes to int," I have some code which fetches IMAP emails and works completely well in Python 2. In Python3 I get the following error: Traceback (most recent call last): File ""./mail.py"", line 295, in item=return_message(x) File ""./mail.py"", line 122, in return_message result, data = mail.fetch(message_id, ""(RFC822)"") File ""/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/imaplib.py"", line 460, in fetch typ, dat = self._simple_command(name, message_set, message_parts) File ""/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/imaplib.py"", line 1113, in _simple_command return self._command_complete(name, self._command(name, *args)) File ""/Library/Frameworks/Python.framework/Versions/3.3/lib/python3.3/imaplib.py"", line 883, in _command data = data + b' ' + arg TypeError: can't concat bytes to intThe code from the return_message function: Runtime Information: 3.3.0 (v3.3.0:bd8afb90ebf2, Sep 29 2012, 01:25:11) [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] <code>  result, data = mail.fetch(message_id, ""(RFC822)"")raw_email = data[0][1]email_message = email.message_from_string(raw_email)",The quick brown fox jumped over the lazy dog
Python: Displaying a grayscale Image," My aim:Read an image into the PIL format. Convert it to grayscale. Plot the image using pylab.Here is the code i'm using: My Problem:The image get's displayed like the colours are inverted.But I know that the image is getting converted to grayscale, because when I write it to the disk it is appearing as a grayscale image.(Just as I expect).I feel that the problem is somewhere in the numpy conversion.I've just started programming in Python for Image Processing.And Tips and Guideline will also be appreciated. <code>  from PIL import Imagefrom pylab import *import numpy as npinputImage='C:\Test\Test1.jpg'##outputImage='C:\Test\Output\Test1.jpg'pilImage=Image.open(inputImage)pilImage.draft('L',(500,500))imageArray= np.asarray(pilImage)imshow(imageArray)##pilImage.save(outputImage)axis('off')show()",Displaying a grayscale Image
factory method to create or update model?," I want to create a model object, like Person, if person's id doesn't not exist, or I will get that person object. The code to create a new person as following: But I don't know where to check and get the existing person object. <code>  class Person(models.Model): identifier = models.CharField(max_length = 10) name = models.CharField(max_length = 20) objects = PersonManager()class PersonManager(models.Manager): def create_person(self, identifier): person = self.create(identifier = identifier) return person",Create Django model or update if exists
Threads in Tkinter with decorators," I'm trying to implement threading(with using decorators) to my application, but can't understand some things about locks and managing threads. As I understand, method1 and method2 are not synchronized, but synchronizing of that stuff implementing with help of locks. How I can add locks to my decorator-function?How can I realize method for stopping long threads using decorators?  <code>  import threadingdef run_in_thread(fn): def run(*k, **kw): t = threading.Thread(target=fn, args=k, kwargs=kw) t.start() return runclass A: @run_in_thread def method1(self): for x in range(10000): print x @run_in_thread def method2(self): for y in list('wlkefjwfejwiefwhfwfkjshkjadgfjhkewgfjwjefjwe'): print y def stop_thread(self): passc = A()c.method1()c.method2()",Threads with decorators
cxfreeze missing distutils module," When running a cxfreeze binary from a python3.2 project I am getting the following runtime error: Correspondingly there are several distutils entries in the missing modules section of the cxfreeze output: I've tried forcing distutils to be included as a module, by both importing it in my main python file and by adding it to a cxfreeze setup.py as: Neither approach worked. It seems likely that I've somehow broken the virtualenv [as distutils seems fundamental and the warning regarding the location of distutils], repeating with a clean virtualenv replicated the problem.It may be worth noting that I installed cx-freeze by running $VIRTUAL_ENV/build/cx-freeze/setup.py install as it doesn't install cleanly in pip. <code>  /project/dist/project/distutils/__init__.py:13: UserWarning: The virtualenv distutils package at %s appears to be in the same location as the system distutils?Traceback (most recent call last): File ""/home/chrish/.virtualenvs/project/lib/python3.2/distutils/__init__.py"", line 19, in <module> import distImportError: No module named dist ? dist imported from distutils? distutils.ccompiler imported from numpy.distutils.ccompiler? distutils.cmd imported from setuptools.dist? distutils.command.build_ext imported from distutils? distutils.core imported from numpy.distutils.core... options = {""build_exe"": {""packages"" : [""distutils""]} },",cxfreeze missing distutils module inside virtualenv
"Change string encoding to the ""real"" unicode"," I am trying to read email with imaplib. I get this mail body: That is Quoted-printable encoding.I need to get utf-8 from this. It should be !I googled it, but it is too messy with Python's versions. It is already unicode in Python 3, I cann't use .encode('utf-8') here. How can I change this to utf-8?  <code>  =C4=EE=E1=F0=FB=E9 =E4=E5=ED=FC! ","Change ""Quoted-printable"" encoding to ""utf-8"""
How to access variable by id in Python?," Possible Duplicate: Get object by id()? Is there any way to use address of var in memory, provided by id(), for accessing value of var?UPD:I also want to say, that if this cannot be done in standard Python, it also would be interesting, if this somehow could be implemented via hacking C++ internals of Python.UPD2:Also would be interesting to know, how to change value of var. <code>  >>> var = 'I need to be accessed by id!'>>> address = id(var)>>> print(address)33003240",How to access variable by id?
loop through a certain type of file in a folder (python)," I'm trying to loop through only the csv files in a folder that contains many kinds of files and many folders, I just want it to list all of the .csv files in this folder.Here's what I mean: I know there is no wildcard variable in python, but is there a way of doing this? <code>  import os, syspath = ""path/to/dir""dirs = os.listdir(path)for file in dirs: if file == '*.csv': print file",Loop through all CSV files in a folder
sublime text detect overwrite mode," I am trying to write a plugin to support a vim-like replace mode in Sublime.To accomplish this, there are a couple things that I need to be able to do:enable/disable overwrite mode. (not toggle_overwrite).detect if overwrite mode is enabled.Item 2 is more important because I could always detect and toggle if necessary.How can I detect if overwrite mode is enabled? <code> ",Sublime Text detect overwrite mode
Python3 global dictionaries don't need keyword global to modify them?," I wonder why I can change global dictionary without global keyword? Why it's mandatory for other types? Is there any logic behind this?E.g. code: Gives following results: where I would expect: <code>  #!/usr/bin/env python3stringvar = ""mod""dictvar = {'key1': 1, 'key2': 2}def foo(): dictvar['key1'] += 1def bar(): stringvar = ""bar"" print(stringvar)print(dictvar)foo()print(dictvar)print(stringvar)bar()print(stringvar) me@pc:~/$ ./globalDict.py {'key2': 2, 'key1': 1}{'key2': 2, 'key1': 2} # Dictionary value has been changedmodbarmod me@pc:~/$ ./globalDict.py {'key2': 2, 'key1': 1}{'key2': 2, 'key1': 1} # I didn't use global, so dictionary remains the samemodbarmod",Global dictionaries don't need keyword global to modify them?
Creating a game board with Python and Tkinter," I am trying to build a simple game of Connect Four with Python(2.7)I have created a board, that consists of a simple multidimensional Python list.My Board list looks like this: Were X is Player1 and O is Player2 (or Computer).Now, I have created some basic code for the GUI, like this: Question: How can i create a visual representation of the board, so that for every list, there is a rectangle? Also, is there a way to detect, when a rectangle is clicked and replace the corresponding list value? <code>  board = [ [_,_,_,_,_,_,_,_,_,_], [_,_,_,_,_,_,_,_,_,_], [_,_,_,_,_,_,_,_,_,_], [_,_,_,_,_,_,_,_,_,_], [_,_,_,_,_,_,_,_,_,_], [_,_,_,_,_,_,_,_,_,_], [_,_,_,_,O,_,_,_,_,_], [_,_,_,_,X,_,_,_,_,_], [_,_,_,_,X,O,_,_,_,_], [_,_,_,_,X,O,_,_,_,_],] # Connect 4 Gameimport Tkinterscreen = Tkinter.Tk()screen.title(""My First Game"")#Create a boardboard = Tkinter.Canvas(screen,width=500,height=500)board.pack()screen.mainloop()",Creating a game board with Tkinter
python/ pygame adding scrolling," Ok so I included the code for my project below, I'm just doing some experimenting with pygame on making a platformer. I'm trying to figure out how to do some very simple scrolling that follows the player, so the player is the center of the camera and it bounces/follows him. Can anyone help me? <code>  import pygamefrom pygame import *WIN_WIDTH = 800WIN_HEIGHT = 640HALF_WIDTH = int(WIN_WIDTH / 2)HALF_HEIGHT = int(WIN_HEIGHT / 2)DISPLAY = (WIN_WIDTH, WIN_HEIGHT)DEPTH = 32FLAGS = 0CAMERA_SLACK = 30def main(): global cameraX, cameraY pygame.init() screen = pygame.display.set_mode(DISPLAY, FLAGS, DEPTH) pygame.display.set_caption(""Use arrows to move!"") timer = pygame.time.Clock() up = down = left = right = running = False bg = Surface((32,32)) bg.convert() bg.fill(Color(""#000000"")) entities = pygame.sprite.Group() player = Player(32, 32) platforms = [] x = y = 0 level = [ ""PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP"", ""P P"", ""P P"", ""P P"", ""P P"", ""P P"", ""P P"", ""P P"", ""P PPPPPPPPPPP P"", ""P P"", ""P P"", ""P P"", ""P P"", ""PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP"",] # build the level for row in level: for col in row: if col == ""P"": p = Platform(x, y) platforms.append(p) entities.add(p) if col == ""E"": e = ExitBlock(x, y) platforms.append(e) entities.add(e) x += 32 y += 32 x = 0 entities.add(player) while 1: timer.tick(60) for e in pygame.event.get(): if e.type == QUIT: raise SystemExit, ""QUIT"" if e.type == KEYDOWN and e.key == K_ESCAPE: raise SystemExit, ""ESCAPE"" if e.type == KEYDOWN and e.key == K_UP: up = True if e.type == KEYDOWN and e.key == K_DOWN: down = True if e.type == KEYDOWN and e.key == K_LEFT: left = True if e.type == KEYDOWN and e.key == K_RIGHT: right = True if e.type == KEYDOWN and e.key == K_SPACE: running = True if e.type == KEYUP and e.key == K_UP: up = False if e.type == KEYUP and e.key == K_DOWN: down = False if e.type == KEYUP and e.key == K_RIGHT: right = False if e.type == KEYUP and e.key == K_LEFT: left = False if e.type == KEYUP and e.key == K_RIGHT: right = False # draw background for y in range(32): for x in range(32): screen.blit(bg, (x * 32, y * 32)) # update player, draw everything else player.update(up, down, left, right, running, platforms) entities.draw(screen) pygame.display.update()class Entity(pygame.sprite.Sprite): def __init__(self): pygame.sprite.Sprite.__init__(self)class Player(Entity): def __init__(self, x, y): Entity.__init__(self) self.xvel = 0 self.yvel = 0 self.onGround = False self.image = Surface((32,32)) self.image.fill(Color(""#0000FF"")) self.image.convert() self.rect = Rect(x, y, 32, 32) def update(self, up, down, left, right, running, platforms): if up: # only jump if on the ground if self.onGround: self.yvel -= 10 if down: pass if running: self.xvel = 12 if left: self.xvel = -8 if right: self.xvel = 8 if not self.onGround: # only accelerate with gravity if in the air self.yvel += 0.3 # max falling speed if self.yvel > 100: self.yvel = 100 if not(left or right): self.xvel = 0 # increment in x direction self.rect.left += self.xvel # do x-axis collisions self.collide(self.xvel, 0, platforms) # increment in y direction self.rect.top += self.yvel # assuming we're in the air self.onGround = False; # do y-axis collisions self.collide(0, self.yvel, platforms) def collide(self, xvel, yvel, platforms): for p in platforms: if pygame.sprite.collide_rect(self, p): if isinstance(p, ExitBlock): pygame.event.post(pygame.event.Event(QUIT)) if xvel > 0: self.rect.right = p.rect.left print ""collide right"" if xvel < 0: self.rect.left = p.rect.right print ""collide left"" if yvel > 0: self.rect.bottom = p.rect.top self.onGround = True self.yvel = 0 if yvel < 0: self.rect.top = p.rect.bottomclass Platform(Entity): def __init__(self, x, y): Entity.__init__(self) self.image = Surface((32, 32)) self.image.convert() self.image.fill(Color(""#DDDDDD"")) self.rect = Rect(x, y, 32, 32) def update(self): passclass ExitBlock(Platform): def __init__(self, x, y): Platform.__init__(self, x, y) self.image.fill(Color(""#0033FF""))if __name__ == ""__main__"": main()",Add scrolling to a platformer in pygame
How to add scrolling to a platformer in pygame?," Ok so I included the code for my project below, I'm just doing some experimenting with pygame on making a platformer. I'm trying to figure out how to do some very simple scrolling that follows the player, so the player is the center of the camera and it bounces/follows him. Can anyone help me? <code>  import pygamefrom pygame import *WIN_WIDTH = 800WIN_HEIGHT = 640HALF_WIDTH = int(WIN_WIDTH / 2)HALF_HEIGHT = int(WIN_HEIGHT / 2)DISPLAY = (WIN_WIDTH, WIN_HEIGHT)DEPTH = 32FLAGS = 0CAMERA_SLACK = 30def main(): global cameraX, cameraY pygame.init() screen = pygame.display.set_mode(DISPLAY, FLAGS, DEPTH) pygame.display.set_caption(""Use arrows to move!"") timer = pygame.time.Clock() up = down = left = right = running = False bg = Surface((32,32)) bg.convert() bg.fill(Color(""#000000"")) entities = pygame.sprite.Group() player = Player(32, 32) platforms = [] x = y = 0 level = [ ""PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP"", ""P P"", ""P P"", ""P P"", ""P P"", ""P P"", ""P P"", ""P P"", ""P PPPPPPPPPPP P"", ""P P"", ""P P"", ""P P"", ""P P"", ""PPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPP"",] # build the level for row in level: for col in row: if col == ""P"": p = Platform(x, y) platforms.append(p) entities.add(p) if col == ""E"": e = ExitBlock(x, y) platforms.append(e) entities.add(e) x += 32 y += 32 x = 0 entities.add(player) while 1: timer.tick(60) for e in pygame.event.get(): if e.type == QUIT: raise SystemExit, ""QUIT"" if e.type == KEYDOWN and e.key == K_ESCAPE: raise SystemExit, ""ESCAPE"" if e.type == KEYDOWN and e.key == K_UP: up = True if e.type == KEYDOWN and e.key == K_DOWN: down = True if e.type == KEYDOWN and e.key == K_LEFT: left = True if e.type == KEYDOWN and e.key == K_RIGHT: right = True if e.type == KEYDOWN and e.key == K_SPACE: running = True if e.type == KEYUP and e.key == K_UP: up = False if e.type == KEYUP and e.key == K_DOWN: down = False if e.type == KEYUP and e.key == K_RIGHT: right = False if e.type == KEYUP and e.key == K_LEFT: left = False if e.type == KEYUP and e.key == K_RIGHT: right = False # draw background for y in range(32): for x in range(32): screen.blit(bg, (x * 32, y * 32)) # update player, draw everything else player.update(up, down, left, right, running, platforms) entities.draw(screen) pygame.display.update()class Entity(pygame.sprite.Sprite): def __init__(self): pygame.sprite.Sprite.__init__(self)class Player(Entity): def __init__(self, x, y): Entity.__init__(self) self.xvel = 0 self.yvel = 0 self.onGround = False self.image = Surface((32,32)) self.image.fill(Color(""#0000FF"")) self.image.convert() self.rect = Rect(x, y, 32, 32) def update(self, up, down, left, right, running, platforms): if up: # only jump if on the ground if self.onGround: self.yvel -= 10 if down: pass if running: self.xvel = 12 if left: self.xvel = -8 if right: self.xvel = 8 if not self.onGround: # only accelerate with gravity if in the air self.yvel += 0.3 # max falling speed if self.yvel > 100: self.yvel = 100 if not(left or right): self.xvel = 0 # increment in x direction self.rect.left += self.xvel # do x-axis collisions self.collide(self.xvel, 0, platforms) # increment in y direction self.rect.top += self.yvel # assuming we're in the air self.onGround = False; # do y-axis collisions self.collide(0, self.yvel, platforms) def collide(self, xvel, yvel, platforms): for p in platforms: if pygame.sprite.collide_rect(self, p): if isinstance(p, ExitBlock): pygame.event.post(pygame.event.Event(QUIT)) if xvel > 0: self.rect.right = p.rect.left print ""collide right"" if xvel < 0: self.rect.left = p.rect.right print ""collide left"" if yvel > 0: self.rect.bottom = p.rect.top self.onGround = True self.yvel = 0 if yvel < 0: self.rect.top = p.rect.bottomclass Platform(Entity): def __init__(self, x, y): Entity.__init__(self) self.image = Surface((32, 32)) self.image.convert() self.image.fill(Color(""#DDDDDD"")) self.rect = Rect(x, y, 32, 32) def update(self): passclass ExitBlock(Platform): def __init__(self, x, y): Platform.__init__(self, x, y) self.image.fill(Color(""#0033FF""))if __name__ == ""__main__"": main()",Add scrolling to a platformer in pygame
File attributes in python on ubuntu," I need to give attributes (meta data) to a file in python code on Linux, specifically Ubuntu.Specifically I need to set the author, title, album, etc. on MP4 files. <code> ",Edit MP4 metadata using Python
Hashing a tuple in python WHERE ORDER MATTERS?," I have: However, both tuple 1 and tuple2 are getting the same counts. What is a way to hash a group of 2 things such that order matters? <code>  tuple1 = token1, token2tuple2 = token2, token1for tuple in [tuple1, tuple2]: if tuple in dict: dict[tuple] += 1 else: dict[tuple] = 1",Hashing a tuple in Python where order matters?
python - self scanning code to prevent print statments," I have a python project I'm working on whereby instead of print statements I call a function say() so I can print information while in development and log information during production. However, I often forget this and put print statements in the code by mistake. Is there anyway to have the python program read its own source, and exit() if it finds any print statements outside of the function say()? <code> ",Self scanning code to prevent print statments
How to drop duplicate index of Pandas Series?," I have a Series s with duplicate index : And I just want to keep the unique rows and only one copy of the duplicate rows by: Pandas 0.10.1.dev-f7f7e13 give the below error msg So how to drop extra duplicate rows of series, keep the unique rows and only one copy of the duplicate rows in an efficient way ? (better in one line) <code>  >>> sSTK_ID RPT_Date600809 20061231 demo_str 20070331 demo_str 20070630 demo_str 20070930 demo_str 20071231 demo_str 20060331 demo_str 20060630 demo_str 20060930 demo_str 20061231 demo_str 20070331 demo_str 20070630 demo_strName: STK_Name, Length: 11 s[s.index.unique()] >>> s[s.index.unique()]Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""d:\Python27\lib\site-packages\pandas\core\series.py"", line 515, in __getitem__ return self._get_with(key) File ""d:\Python27\lib\site-packages\pandas\core\series.py"", line 558, in _get_with return self.reindex(key) File ""d:\Python27\lib\site-packages\pandas\core\series.py"", line 2361, in reindex level=level, limit=limit) File ""d:\Python27\lib\site-packages\pandas\core\index.py"", line 2063, in reindex limit=limit) File ""d:\Python27\lib\site-packages\pandas\core\index.py"", line 2021, in get_indexer raise Exception('Reindexing only valid with uniquely valued Index 'Exception: Reindexing only valid with uniquely valued Index objects>>> ",How to drop extra copy of duplicate index of Pandas Series?
How can I reference requirements.txt for the install_requires kwarg in setuptools.setup?," I have a requirements.txt file that I'm using with Travis-CI. It seems silly to duplicate the requirements in both requirements.txt and setup.py, so I was hoping to pass a file handle to the install_requires kwarg in setuptools.setup.Is this possible? If so, how should I go about doing it?Here is my requirements.txt file: <code>  guessit>=0.5.2tvdb_api>=1.8.2hachoir-metadata>=1.3.3hachoir-core>=1.3.3hachoir-parser>=1.3.4",Reference requirements.txt for the install_requires kwarg in setuptools setup.py file
How can I reference requirements.txt for the install_requires kwarg in setuptools' setup.py file?," I have a requirements.txt file that I'm using with Travis-CI. It seems silly to duplicate the requirements in both requirements.txt and setup.py, so I was hoping to pass a file handle to the install_requires kwarg in setuptools.setup.Is this possible? If so, how should I go about doing it?Here is my requirements.txt file: <code>  guessit>=0.5.2tvdb_api>=1.8.2hachoir-metadata>=1.3.3hachoir-core>=1.3.3hachoir-parser>=1.3.4",Reference requirements.txt for the install_requires kwarg in setuptools setup.py file
Reference requirements.txt for the install_requires kwarg in setuptools' setup.py file?," I have a requirements.txt file that I'm using with Travis-CI. It seems silly to duplicate the requirements in both requirements.txt and setup.py, so I was hoping to pass a file handle to the install_requires kwarg in setuptools.setup.Is this possible? If so, how should I go about doing it?Here is my requirements.txt file: <code>  guessit>=0.5.2tvdb_api>=1.8.2hachoir-metadata>=1.3.3hachoir-core>=1.3.3hachoir-parser>=1.3.4",Reference requirements.txt for the install_requires kwarg in setuptools setup.py file
Reference requirements.txt for the install_requires kwarg in setuptools setup.py file?," I have a requirements.txt file that I'm using with Travis-CI. It seems silly to duplicate the requirements in both requirements.txt and setup.py, so I was hoping to pass a file handle to the install_requires kwarg in setuptools.setup.Is this possible? If so, how should I go about doing it?Here is my requirements.txt file: <code>  guessit>=0.5.2tvdb_api>=1.8.2hachoir-metadata>=1.3.3hachoir-core>=1.3.3hachoir-parser>=1.3.4",Reference requirements.txt for the install_requires kwarg in setuptools setup.py file
can we load panda dataframe in .net ironpython?, Can we load a pandas DataFrame in .NET space using iron python? If not I am thinking of converting pandas df into a csv file and then reading in .net space. <code> ,Can we load pandas DataFrame in .NET ironpython?
matplotlib scatter plot with different text at each data point," I am trying to make a scatter plot and annotate data points with different numbers from a list.So, for example, I want to plot y vs x and annotate with corresponding numbers from n. Any ideas? <code>  y = [2.56422, 3.77284, 3.52623, 3.51468, 3.02199]z = [0.15, 0.3, 0.45, 0.6, 0.75]n = [58, 651, 393, 203, 123]ax = fig.add_subplot(111)ax1.scatter(z, y, fmt='o')",Matplotlib scatter plot with different text at each data point
"PyQt, Qtable clears column when resize mode is ""fixed"""," I have a bug in next function. This function refreshes table with, adjust column sizes and sorts table. But in some cases for some rows text in column 3 and 4 is empty. I found out, that it happens in ""problem code"" section.What do i wrong in this section? I just want to change size for specific columns. Please help to find out the problem or show me my mistakes. P.s. it looks not clearly, but it's because i try to catch the bug <code>  def print_day_worklog(self): logging.info('RefReshing day worklog table') selected_day = self.ui.dateDayWorklogEdit.date().toPyDate() day_work = db.get_day_worklog(self.creds[3], selected_day) self.ui.tableDayWorklog.setRowCount(len(day_work)) for row, entry in enumerate(day_work): e0 = QtGui.QTableWidgetItem(entry[0]) e1 = QtGui.QTableWidgetItem(entry[1]) e2 = QtGui.QTableWidgetItem(entry[2].strftime('%H:%M')) e3 = QtGui.QTableWidgetItem(entry[3].strftime('%H:%M')) e4 = QtGui.QTableWidgetItem(utils.get_time_spent_string(entry[2], entry[3])) # debug print e0.text(), e1.text(), e2.text(), e3.text(), e4.text() self.ui.tableDayWorklog.setItem(row, 0, e0) self.ui.tableDayWorklog.setItem(row, 1, e1) self.ui.tableDayWorklog.setItem(row, 2, e2) self.ui.tableDayWorklog.setItem(row, 3, e3) self.ui.tableDayWorklog.setItem(row, 4, e4) # problem code starts self.ui.tableDayWorklog.horizontalHeader().setResizeMode(1, QtGui.QHeaderView.Stretch) for column in (0,2,3,4): self.ui.tableDayWorklog.resizeColumnToContents(column) self.ui.tableDayWorklog.horizontalHeader().setResizeMode(column, QtGui.QHeaderView.Fixed) self.ui.tableDayWorklog.horizontalHeader().setResizeMode(column, QtGui.QHeaderView.Fixed) self.ui.tableDayWorklog.sortByColumn(2,0) # problem code ends","PyQt, Qtable clears column when sorting enabled"
How to efficiently handle pandas read_csv with datetime index," I have enormous files that look like this:05/31/2012,15:30:00.029,1306.25,1,E,0,,1306.2505/31/2012,15:30:00.029,1306.25,8,E,0,,1306.25I can easily read them using the following: Is there any way to efficiently parse dates like this into pandas timestamps? If not, is there any guide for writing a cython function that can passed to date_parser= ?I tried writing my own parser function and it still takes too long for the project I am working on.  <code>  pd.read_csv(gzip.open(""myfile.gz""), header=None,names= [""date"",""time"",""price"",""size"",""type"",""zero"",""empty"",""last""], parse_dates=[[0,1]])",Speed-improvement on large pandas read_csv with datetime index
Chained get requests in python requests," I'm interrogating a nested dictionary using the dict.get('keyword') method. Currently my syntax is... However, sometimes one of the ""parent"" or ""child"" tags doesn't exist, and my script fails. I know using get() I can include a default in the case the key doesn't exist of the form... But if I include any Null, '', or empty I can think of, the chained .get(""child"") fails when called on ''.get(""child"") since """" has no method .get().The way I'm solving this now is by using a bunch of sequential try-except around each .get("""") call, but that seems foolish and unpython---is there a way to default return ""skip"" or ""pass"" or something that would still support chaining and fail intelligently, rather than deep-dive into keys that don't exist?Ideally, I'd like this to be a list comprehension of the form: but this is currently impossible when an absent parent causes the .get(""child"") call to terminate my program. <code>  M = cursor_object_results_of_db_queryfor m in M: X = m.get(""gparents"").get(""parent"").get(""child"") for x in X: y = x.get(""key"") get(""parent"", '') orget(""parent"", 'orphan') [m.get(""gparents"").get(""parent"").get(""child"") for m in M]","Chained, nested dict() get calls in python"
Python unittest - asserting dictionary with sets," While writing some tests for my class, I encountered interesting simple problem. I would like to assertDictEqual two dictionaries containing some list. But this lists may not be sorted in a same way -> which results in failed testExample: This fail from time to time, depending on order of elements in list Any ideas how to assert this in a simple way?I was thinking about using set instead of list or sorting lists before comparison. <code>  def test_myobject_export_into_dictionary(self): obj = MyObject() resulting_dictionary = { 'state': 2347, 'neighbours': [1,2,3] } self.assertDictEqual(resulting_dictionary, obj.exportToDict()) FAIL: test_myobject_export_into_dictionary------------------------------------- 'neighbours': [1,2,3],+ 'neighbours': [1,3,2],",Python unittest - asserting dictionary with lists
byte reverse AB CD to CD AB with poython," I have a .bin file, and I want to simply byte reverse the hex data. Say for instance @ 0x10 it reads AD DE DE C0, want it to read DE AD C0 DE.I know there is a simple way to do this, but I am am beginner and just learning python and am trying to make a few simple programs to help me through my daily tasks. I would like to convert the whole file this way, not just 0x10.I will be converting at start offset 0x000000 and blocksize/length is 1000000.here is my code, maybe you can tell me what to do. i am sure i am just not getting it, and i am new to programming and python. if you could help me i would very much appreciate it. and you can see the module for reversing, i have tried many different suggestions and it will either pass the file through untouched, or it will throw errors. i know module reverse is empty now, but i have tried all kinds of things. i just need module reverse to convert AB CD to CD AB.thanks for any inputEDIT: the file is 16 MB and i want to reverse the byte order of the whole file. <code>  def main(): infile = open(""file.bin"", ""rb"") new_pos = int(""0x000000"", 16) chunk = int(""1000000"", 16) data = infile.read(chunk) reverse(data)def reverse(data): output(data)def output(data): with open(""reversed"", ""wb"") as outfile: outfile.write(data)main()",byte reverse AB CD to CD AB with python
manipulate pixel of an image with python," I have searched on Google but I couldn't find anything. I want to create a Python script that can import an image, change the order of pixels, and save an output image.I have worked with Python a lot, but only with the built-in libraries. So if I have to use new commands, please describe it as much as you can. <code> ",How to reorder pixels
python inserting variable string as file name," I'm trying to create a file with a unique file name for every time my script runs. I am only intending to do this to every week or month. so I chose to use the date for the file name. is where I'm getting this error. it works if I use a static filename, is there an issue with the open function, that means you can't pass a string like this?name is a string and has values such as : Many thanks for any help. <code>  f = open('%s.csv', 'wb') %name Traceback (most recent call last):File ""C:\Users\User\workspace\new3\stjohnsinvoices\BabblevoiceInvoiceswpath.py"", line 143, in <module>f = open('%s.csv', 'ab') %nameTypeError: unsupported operand type(s) for %: 'file' and 'str' 31/1/2013BVI",Python inserting variable string as file name
Which Python tenary operation is better and why?," I have been reading all over the place, including Does Python have a ternary conditional operator?. It is supposed that is better code than But no one ever explains why. Will someone please elaborate?If it's mere readability, then it's really just a matter of preference: some people will like the one and some people will like the other. So my question is: is there some really technical advantage for going one way versus the other. <code>  result = a if a > b else b result = a > b and a or b",Which Python ternary operation is better and why?
Pandas DatetimeIndex slicing error," I have a pandas dataframe df: And I would like to slice the dataframe to return a four hour window of data from 2012-12-20 05:00:00 to 2012-12-20 09:00:00When I try: The following error occurs: I have also tried (from Pandas DataFrame slicing by day/hour/minute): which returns the exact same error. <code>  Out[16]:<class 'pandas.core.frame.DataFrame'>DatetimeIndex: 269850 entries, 2012-12-19 16:15:36 to 2012-12-20 14:36:55Data columns:X1 269850 non-null valuesX2 269848 non-null valuesX3 269848 non-null valuesdtypes: float64(2), object(1) Slicedf = df.truncate(before='12/20/2012 05:00:00',after='12/20/2012 09:00:00') KeyError: datetime.datetime(2012, 12, 20, 5, 0) from datetime import datetimex=datetime(2012,12,20,5,0,0)y=datetime(2012,12,20,9,0,0)Slicedf = df.ix[x:y]",Pandas DatetimeIndex truncate error
writing hex with python," i am trying to patch a hex file.i have two patch files (hex) named ""patch 1"" and ""patch 2""the file to be patched is a 16 MB file named ""file.bin"".i have tried many different way for the past 6 or 7 hours to figure out how to do it. I can write a string to a file all day long, but i am trying to do the following:open patch1.bin with read bytesopen patch2.bin with read bytesopen file.bin with write bytesi want to seek to positions 0xc0010, and 0x7c0010, and apply patch1.binthen i want to seek to 0x040000 and apply patch2.binso all in all i will have 3 patches applied, then close the ""file.bin""if someone cold give me an example i would very much appreciate it :)i tried this first: but was informed i was was trying to write a string to a file, when indeed its not what i wanted, lolthen i tried this: but it did not work either, as no matter what i tried, i could not get it to write the data at he correct offset. I did manage a few times to write the patch1 to file.bin, but it did not patch at the right offset, as a matter of fact it deleted the file.bin and just copied patch1 in its place. which ofcourse is wrong.i must remind you i am new to python and programming, but i am really trying to dig my feet into it and learn, so any good examples will be examined and hopefully will be a good learning lesson for me :)thanks guys and gals for helping me figure out what i was doing wrong :) <code>  patch1 = open(""patch1"", ""r"");patch2 = open(""patch2"", ""r"");main = open(""file.bin"", ""w"");main.seek(0xC0010);main.write(patch1);main.seek(0x7C0010);main.write(patch1);main.seek(0x40000);main.write(patch2);main.close(); infile1 = open(""patch1.bin"", ""rb"") new_pos1 = int(""0x00"", 16)infile1.seek(new_pos1, 0)infile2 = open('file.bin', 'wb')new_pos2 = int('0xc0010', 16)infile2.seek(new_pos2, 0xc0010)chunk1 = int(""6FFFE0"", 16) #this is how long patch1 file isdata1 = infile1.read(chunk1)with open(""file.bin"", ""a"") as outfile: outfile.write(data1)",Modifying binary file with Python
How to get the last n row of pandas dataframe?," I have pandas dataframe df1 and df2 (df1 is vanila dataframe, df2 is indexed by 'STK_ID' & 'RPT_Date') : I can get the last 3 rows of df2 by: while df1.ix[-3:] give all the rows: Why ? How to get the last 3 rows of df1 (dataframe without index) ?Pandas 0.10.1 <code>  >>> df1 STK_ID RPT_Date TClose sales discount0 000568 20060331 3.69 5.975 NaN1 000568 20060630 9.14 10.143 NaN2 000568 20060930 9.49 13.854 NaN3 000568 20061231 15.84 19.262 NaN4 000568 20070331 17.00 6.803 NaN5 000568 20070630 26.31 12.940 NaN6 000568 20070930 39.12 19.977 NaN7 000568 20071231 45.94 29.269 NaN8 000568 20080331 38.75 12.668 NaN9 000568 20080630 30.09 21.102 NaN10 000568 20080930 26.00 30.769 NaN>>> df2 TClose sales discount net_sales cogsSTK_ID RPT_Date 000568 20060331 3.69 5.975 NaN 5.975 2.591 20060630 9.14 10.143 NaN 10.143 4.363 20060930 9.49 13.854 NaN 13.854 5.901 20061231 15.84 19.262 NaN 19.262 8.407 20070331 17.00 6.803 NaN 6.803 2.815 20070630 26.31 12.940 NaN 12.940 5.418 20070930 39.12 19.977 NaN 19.977 8.452 20071231 45.94 29.269 NaN 29.269 12.606 20080331 38.75 12.668 NaN 12.668 3.958 20080630 30.09 21.102 NaN 21.102 7.431 >>> df2.ix[-3:] TClose sales discount net_sales cogsSTK_ID RPT_Date 000568 20071231 45.94 29.269 NaN 29.269 12.606 20080331 38.75 12.668 NaN 12.668 3.958 20080630 30.09 21.102 NaN 21.102 7.431 >>> df1.ix[-3:] STK_ID RPT_Date TClose sales discount0 000568 20060331 3.69 5.975 NaN1 000568 20060630 9.14 10.143 NaN2 000568 20060930 9.49 13.854 NaN3 000568 20061231 15.84 19.262 NaN4 000568 20070331 17.00 6.803 NaN5 000568 20070630 26.31 12.940 NaN6 000568 20070930 39.12 19.977 NaN7 000568 20071231 45.94 29.269 NaN8 000568 20080331 38.75 12.668 NaN9 000568 20080630 30.09 21.102 NaN10 000568 20080930 26.00 30.769 NaN",How to get the last N rows of a pandas DataFrame?
"Pandas DataFrame, how do i split a column into two"," I have a data frame with one (string) column and I'd like to split it into two (string) columns, with one column header as 'fips' and the other 'row'My dataframe df looks like this: I do not know how to use df.row.str[:] to achieve my goal of splitting the row cell. I can use df['fips'] = hello to add a new column and populate it with hello. Any ideas? <code>  row0 00000 UNITED STATES1 01000 ALABAMA2 01001 Autauga County, AL3 01003 Baldwin County, AL4 01005 Barbour County, AL fips row0 00000 UNITED STATES1 01000 ALABAMA 2 01001 Autauga County, AL3 01003 Baldwin County, AL4 01005 Barbour County, AL",How to split a dataframe string column into two columns?
How to split a column into two columns?," I have a data frame with one (string) column and I'd like to split it into two (string) columns, with one column header as 'fips' and the other 'row'My dataframe df looks like this: I do not know how to use df.row.str[:] to achieve my goal of splitting the row cell. I can use df['fips'] = hello to add a new column and populate it with hello. Any ideas? <code>  row0 00000 UNITED STATES1 01000 ALABAMA2 01001 Autauga County, AL3 01003 Baldwin County, AL4 01005 Barbour County, AL fips row0 00000 UNITED STATES1 01000 ALABAMA 2 01001 Autauga County, AL3 01003 Baldwin County, AL4 01005 Barbour County, AL",How to split a dataframe string column into two columns?
improve nested loop performance python numpy," I am doing a molecular dynamics simulation of an Argon liquid in Python. I have a stable version running, however it runs slowly for more than 100 atoms. I identified the bottleneck to be the following nested for loop. It's a force calculation put inside a function that is called from my main.py script: the variables in CAPITAL letters are constant, defined in a config.py file. ""currentPositions"" is a 3 by number-of-particles matrix.I have already implemented a version of the nested for loop with scipy.weave, which was inspired from this website: http://www.scipy.org/PerformancePython.However, I don't like the loss of flexibility. I'm interested in ""vectorizing"" this for loop. I just don't really get how that works. Can anybody give me a clue or a good tutorial that teaches this? <code>  def computeForce(currentPositions): potentialEnergy = 0 force = zeros((NUMBER_PARTICLES,3)) for iParticle in range(0,NUMBER_PARTICLES-1): for jParticle in range(iParticle + 1, NUMBER_PARTICLES): distance = currentPositions[iParticle] - currentPositions[jParticle] distance = distance - BOX_LENGTH * (distance/BOX_LENGTH).round() #note: this is so much faster than scipy.dot() distanceSquared = distance[0]*distance[0] + distance[1]*distance[1] + distance[2]*distance[2] if distanceSquared < CUT_OFF_RADIUS_SQUARED: r2i = 1. / distanceSquared r6i = r2i*r2i*r2i lennardJones = 48. * r2i * r6i * (r6i - 0.5) force[iParticle] += lennardJones*distance force[jParticle] -= lennardJones*distance potentialEnergy += 4.* r6i * (r6i - 1.) - CUT_OFF_ENERGYreturn(force,potentialEnergy)",improve nested loop performance
Python: the array contains in an array (return index)?," I use Python with numpy.I have a numpy array may_a: I have a numpy array may_b: I need to find array may_b in array may_a.In the output I need to get indexes of occurrences. Can someone please suggest, how do I get out_index? <code>  may_a = numpy.array([False, True, False, True, True, False, True, False, True, True, False]) may_b = numpy.array([False,True,True,False]) out_index=[2,7]",Return the indexes of a sub-array in an array
Django remove unicode in values_list," I performed this operation to retrieve a queryset: And it returns these results: The results are all in unicode (u'). How do I remove them all so that I get the result: <code>  Name.objects.values_list('name', flat=True) [u'accelerate', u'acute', u'bear', u'big'] ['accelerate', 'acute', 'bear', 'big']",Python 2 How to change list of unicode returned by values_list operation to list of strings
how to make array better?," I created an dictionary of the 26 alphabet letters like this: I'm trying make my code better and my question is,is there any shorter way to do this without typing all these numbers out? <code>  aDict={ ""a"": 1, ""b"": 2, ""c"": 3, ""d"": 4, etc...}",How to create a dict with letters as keys in a concise way?
how to make dictionary better?," I created an dictionary of the 26 alphabet letters like this: I'm trying make my code better and my question is,is there any shorter way to do this without typing all these numbers out? <code>  aDict={ ""a"": 1, ""b"": 2, ""c"": 3, ""d"": 4, etc...}",How to create a dict with letters as keys in a concise way?
matplotlib: aspect ratio in subplots with various y-axes," I would like the following code to produce 4 subplots of the same size with a common aspect ratio between the size of x-axis and y-axis set by me. Referring to the below example, I would like all of the subplots look exactly like the first one (upper left). What is wrong right now is that the size of the y-axis is correlated with its largest value. That is the behaviour I want to avoid. Surprisingly, matplotlib produces the right thing by default (picture below): I just want to add to this an ability to control the aspect ratio between lengths of x and y-axes. <code>  import matplotlib.pyplot as pltimport numpy as npdef main(): fig = plt.figure(1, [5.5, 3]) for i in range(1,5): fig.add_subplot(221+i-1, adjustable='box', aspect=1) plt.plot(np.arange(0,(i)*4,i)) plt.show()if __name__ == ""__main__"": main() import matplotlib.pyplot as plt import numpy as np def main(): fig = plt.figure(1, [5.5, 3]) for i in range(1,5): fig.add_subplot(221+i-1) plt.plot(np.arange(0,(i)*4,i)) plt.show() ",Aspect ratio in subplots with various y-axes
How to return a matplotlib.figure.Figure object from Pandas plot function?," I have a dt: And use pandas function plot to create a barplot object But I want a <matplotlib.figure.Figure object> instead to pass to other function, just like the object type below: So how can I transform <matplotlib.axes.AxesSubplot object> to <matplotlib.figure.Figure object> or directly return ""Figure object"" from Pandas plot ? <code>  >>> dt sales mg ID 600519 600809 600519 600809RPT_Date 20060331 13.5301 5.8951 9.4971 3.040820060630 6.6048 2.4081 4.3088 1.404020060930 12.3889 3.6053 9.1455 2.075420061231 16.5100 3.3659 12.4682 1.881020070331 15.8754 5.9129 11.5833 3.673620070630 10.4155 3.5759 7.8966 2.281220070930 18.2929 3.5280 14.3552 2.158420071231 27.7905 5.4510 23.7820 3.2568 >>> fig = dt.plot(kind='bar', use_index=True)>>> fig<matplotlib.axes.AxesSubplot object at 0x0B387150> >>> plt.figure()<matplotlib.figure.Figure object at 0x123A6910>",How to return a matplotlib.figure.Figure object from Pandas plot function
Selecting/Excluding sets of columns in Panda," I would like to create views or dataframes from an existing dataframe based on column selections.For example, I would like to create a dataframe df2 from a dataframe df1 that holds all columns from it except two of them. I tried doing the following, but it didn't work: What am I doing wrong? Perhaps more generally, what mechanisms does pandas have to support the picking and exclusions of arbitrary sets of columns from a dataframe? <code>  import numpy as npimport pandas as pd# Create a dataframe with columns A,B,C and Ddf = pd.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))# Try to create a second dataframe df2 from df with all columns except 'B' and Dmy_cols = set(df.columns)my_cols.remove('B').remove('D')# This returns an error (""unhashable type: set"")df2 = df[my_cols]",Selecting/excluding sets of columns in pandas
Selecting/Excluding sets of columns in Pandas," I would like to create views or dataframes from an existing dataframe based on column selections.For example, I would like to create a dataframe df2 from a dataframe df1 that holds all columns from it except two of them. I tried doing the following, but it didn't work: What am I doing wrong? Perhaps more generally, what mechanisms does pandas have to support the picking and exclusions of arbitrary sets of columns from a dataframe? <code>  import numpy as npimport pandas as pd# Create a dataframe with columns A,B,C and Ddf = pd.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))# Try to create a second dataframe df2 from df with all columns except 'B' and Dmy_cols = set(df.columns)my_cols.remove('B').remove('D')# This returns an error (""unhashable type: set"")df2 = df[my_cols]",Selecting/excluding sets of columns in pandas
What is difference between getuid() and geteuid()?, The documentation for os.getuid() says: Return the current processs user id.And of os.geteuid() says: Return the current processs effective user id.So what is the difference between user id and effective user id?For me both works same (on both 2.x and 3.x). I am using it to check if script is being run as root. <code> ,What is difference between os.getuid() and os.geteuid()?
Installing Settuptolls with root - Getting a PythonPath error," I already did the virtual python enviroment. When I'm trying to install setuptools I get the following: Error: When I check the system path, I received the following output: How can I fix this issue?UPDATEI fix this issue by edit the .bashrc and I add the following line: <code>  python setup.py install --prefix=/home/dgomez/ TEST FAILED: /home/dgomez//lib/python2.7/site-packages/ does NOT support .pth fileserror: bad install directory or PYTHONPATHYou are attempting to install a package to a directory that is noton PYTHONPATH and which Python does not read "".pth"" files from. Theinstallation directory you specified (via --install-dir, --prefix, orthe distutils default setting) was: /home/dgomez//lib/python2.7/site-packages/and your PYTHONPATH environment variable currently contains: '' >>> import sys>>> import sys/usr/lib/python27.zip/usr/lib64/python2.7/usr/lib64/python2.7/plat-linux2/usr/lib64/python2.7/lib-tk/usr/lib64/python2.7/lib-old/usr/lib64/python2.7/lib-dynload/usr/lib64/python2.7/site-packages/usr/local/lib64/python2.7/site-packages/usr/local/lib/python2.7/site-packages/usr/lib64/python2.7/site-packages/gst-0.10/usr/lib64/python2.7/site-packages/gtk-2.0/usr/lib/python2.7/site-packages PYTHONPATH=""${PYTHONPATH}:/home/dgomez/lib/python2.7/site-packages/""export PYTHONPATH ",Installing Setuptools with root - Getting a PythonPath error
Numpy vectorize as a decorator," I tried to vectorize (agreed, not the most efficient way to do it, but my question is rather on the decorator use) the following function EDIT: After restarting IPython, the output was OK.Can anyone explain why the result of diff_if_bigger got tansformed into an array of np.int even if the first argument x is here an aray of np.float, contrarily to what's in the doc????Now, I want to force a float output, so I did this By the way, even this doesn't work!!! So what's going on??EDIT: In fact, the latter worked after I restarted IPython.So after my previous two edits, my question is now twofold:1- How can I use np.vectorize as a decorator with arguments?2- How can I clean IPython state? <code>  @np.vectorize def diff_if_bigger(x, y): return y - x if y > x else 0 x = np.array([5.6, 7.0]) y = 8 diff_if_bigger(x, y) # outputs array([2, 1]) which is not what I want @np.vectorize('np.float') def diff_if_bigger(x, y): return y - x if y > x else 0 # Error !! # TypeError: Object is not callable. @np.vectorize(otypes='np.float') def diff_if_bigger(x, y): return y - x if y > x else 0 # Again error !! # TypeError: __init__() takes at least 2 arguments (2 given) @np.vectorize(otypes=[np.float]) def diff_if_bigger(x, y): return y - x if y > x else 0 # Still an error !! # TypeError: __init__() takes at least 2 arguments (2 given) vec_diff = np.vectorize(diff_if_bigger, otypes=[np.float])",Numpy vectorize as a decorator with arguments
How to preview a part of a large pandas DataFrame?," I am just getting started with pandas in the IPython Notebook and encountering the following problem: When a DataFrame read from a CSV file is small, the IPython Notebook displays it in a nice table view. When the DataFrame is large, something like this is ouput: I would like to see a small portion of the data frame as a table just to make sure it is in the right format. What options do I have? <code>  In [27]:evaluation = readCSV(""evaluation_MO_without_VNS_quality.csv"").filter([""solver"", ""instance"", ""runtime"", ""objective""])In [37]:evaluationOut[37]:<class 'pandas.core.frame.DataFrame'>Int64Index: 333 entries, 0 to 332Data columns:solver 333 non-null valuesinstance 333 non-null valuesruntime 333 non-null valuesobjective 333 non-null valuesdtypes: int64(1), object(3)","How to preview a part of a large pandas DataFrame, in iPython notebook?"
Python logical expression," Design a logical expression equivalent to the following statement: x is a list of three or five elements, the second element of which is the string 'Hip' and the first of which is not a number or Boolean.What I have: My question: How do you check for whether or not it is a Boolean or a number? <code>  x = ['Head', 'Hip', 10]print x[1] is 'Hip'",Check if object is a number or boolean
Python check if object is a number or boolean," Design a logical expression equivalent to the following statement: x is a list of three or five elements, the second element of which is the string 'Hip' and the first of which is not a number or Boolean.What I have: My question: How do you check for whether or not it is a Boolean or a number? <code>  x = ['Head', 'Hip', 10]print x[1] is 'Hip'",Check if object is a number or boolean
Python sending output to both file and terminal," I want to use Python to send output to both a file log.txt and STDOUT on the terminal. Here is what I have: This program sends output to the file and stdout. My question is: How did the write function to the file get called?  <code>  import sysclass Logger(object): def __init__(self, filename=""Default.log""): self.terminal = sys.stdout self.log = open(filename, ""a"") def write(self, message): self.terminal.write(message) self.log.write(message)sys.stdout = Logger(""log.txt"")print ""Hello world !"" #This line is saved in log.txt and STDOUT","Python, how to send output to both file and terminal"
Python 3.3 TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'," I am unsure why I am getting this error <code>  count=int(input (""How many donuts do you have?""))if count <= 10: print (""number of donuts: "" ) +str(count)else: print (""Number of donuts: many"")","How to fix ""TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'""?"
Django n00b questions: combining queryset into single string?," Say I have an extremely simple model that is just a list of words: After a user submits a form, I want to... Retrieve four random wordsCombine them into a single stringEnsure a duplicate string has not been previously generated and if so, run it againSave that to the database when it's goodReturn the result to the user. I know how to get four random words: I know how to make this a context and return it to a template, at which point I can do whatever with it, but I'm stumped on how I do this behind the scenes so I can do the rest of my stuff before returning it to the user. The final string should look like this: Furthermore, where in my app do I do this? I come from PHP and there, I would have a functions.php file or something to perform backend stuff and keep it out of the presentation. I've found a few other posts from people stating they use a functions.py, but I'm not sure how to include external pages that aren't in the same folder as the existing views.py. If I do: It only works if functions.py is in the folder as wherever I am importing it from. <code>  class WordList(models.Model): word = models.CharField(max_length=60) WordList.objects.order_by('?')[:4] these-are-my-words from functions import myfunc",Combining queryset into single string with Django
Django base View class," I'm diving into Django's generic views, figuring out how they return a simple HttpResponse object, like a simple view function would.I have written a simple project for testing, and I added some logging commands to the basic View classed defined in the file django/views/generic/base.py, so that I can track what is happening under the hood.I have some questions that came up during my research.I've been trying to keep this post short, however, for the a complete understanding I felt it essential to include the code snippets and the logs.I will be really thankful to anyone who takes the time to give some helpful comment, possibly answering some of my questions.urls.py views.py django/views/generic/base.py Logs from some test requests After all, my questions1.According to the logs, as_view is called before View.init.Does it mean that it calls a View method even before creating a View instance?2.Why isn't as_view() called after it is executed for the first call?I'm not yet an expert at Python's imports, compilation and memory use,but I have this feeling that they play some role here.3.In the definition of view(), what does the following snippet do? According to the logs, it triggers View.init.Is it that it creates a new View instance with initkwargs and assigns it to the instance in use (self)?If so, why is it needed?4.How can we make use of initkwargs (arguments of as_view)? <code>  from django.conf.urls import patterns, urlfrom views import WelcomeViewurlpatterns = patterns('', url(r'^welcome/(?P<name>\w+)/$', WelcomeView.as_view()),) from django.http import HttpResponsefrom django.views.generic import Viewclass WelcomeView(View): def get(self, request, name): return HttpResponse('What is up, {0}?'.format(name)) class View(object): """""" Intentionally simple parent class for all views. Only implements dispatch-by-method and simple sanity checking. """""" http_method_names = ['get', 'post', 'put', 'delete', 'head', 'options', 'trace'] def __init__(self, **kwargs): #####logging logging.error('*** View.__init__ is started with kwargs: {0} ***'.format( repr(kwargs))) """""" Constructor. Called in the URLconf; can contain helpful extra keyword arguments, and other things. """""" # Go through keyword arguments, and either save their values to our # instance, or raise an error. for key, value in kwargs.iteritems(): setattr(self, key, value) #####logging logging.error('*** View.__init__ reached its end. No return value. ***') @classonlymethod def as_view(cls, **initkwargs): #####logging logging.error('*** View.as_view is started with initkwargs: {0} ***'.format( repr(initkwargs))) """""" Main entry point for a request-response process. """""" # sanitize keyword arguments for key in initkwargs: if key in cls.http_method_names: raise TypeError(u""You tried to pass in the %s method name as a "" u""keyword argument to %s(). Don't do that."" % (key, cls.__name__)) if not hasattr(cls, key): raise TypeError(u""%s() received an invalid keyword %r"" % ( cls.__name__, key)) def view(request, *args, **kwargs): #####logging logging.error('*** View.as_view.view is called with args: {0};\ and kwargs: {1} ***'.format( repr(args), repr(kwargs))) self = cls(**initkwargs) if hasattr(self, 'get') and not hasattr(self, 'head'): self.head = self.get #####logging logging.error('*** View.as_view.view reached its end.\ Now calls dispatch() and returns the return value.') return self.dispatch(request, *args, **kwargs) # take name and docstring from class update_wrapper(view, cls, updated=()) # and possible attributes set by decorators # like csrf_exempt from dispatch update_wrapper(view, cls.dispatch, assigned=()) #####logging logging.error('*** View.as_view reached its end. Now returns view. ***') return view def dispatch(self, request, *args, **kwargs): # Try to dispatch to the right method; if a method doesn't exist, # defer to the error handler. Also defer to the error handler if the # request method isn't on the approved list. #####logging logging.error('*** View.dispatch called, with args: {0};\ and kwargs: {1} ***'.format( repr(args), repr(kwargs))) if request.method.lower() in self.http_method_names: handler = getattr(self, request.method.lower(), self.http_method_not_allowed) else: handler = self.http_method_not_allowed self.request = request self.args = args self.kwargs = kwargs #####logging logging.error('*** View.dispatch reached its end.\ Now calls handler and returns the return value. ***') return handler(request, *args, **kwargs) def http_method_not_allowed(self, request, *args, **kwargs): allowed_methods = [m for m in self.http_method_names if hasattr(self, m)] logger.warning('Method Not Allowed (%s): %s', request.method, request.path, extra={ 'status_code': 405, 'request': self.request } ) return http.HttpResponseNotAllowed(allowed_methods) Django version 1.4.5, using settings 'try1.settings'Development server is running at http://127.0.0.1:8000/Quit the server with CONTROL-C.ERROR:root:*** View.as_view is started with initkwargs: {} ***ERROR:root:*** View.as_view reached its end. Now returns view. ***ERROR:root:*** View.as_view.view is called with args: (); and kwargs: {'name': u'Dude'} ***ERROR:root:*** View.__init__ is started with kwargs: {} ***ERROR:root:*** View.__init__ reached its end. No return value. ***ERROR:root:*** View.as_view.view reached its end. Now calls dispatch() and returns the return value.ERROR:root:*** View.dispatch called, with args: (); and kwargs: {'name': u'Dude'} ***ERROR:root:*** View.dispatch reached its end. Now calls handler and returns the return value. ***[24/Feb/2013 12:43:19] ""GET /welcome/Dude/ HTTP/1.1"" 200 17ERROR:root:*** View.as_view.view is called with args: (); and kwargs: {'name': u'Dude'} ***ERROR:root:*** View.__init__ is started with kwargs: {} ***ERROR:root:*** View.__init__ reached its end. No return value. ***ERROR:root:*** View.as_view.view reached its end. Now calls dispatch() and returns the return value.ERROR:root:*** View.dispatch called, with args: (); and kwargs: {'name': u'Dude'} ***ERROR:root:*** View.dispatch reached its end. Now calls handler and returns the return value. ***[24/Feb/2013 12:43:32] ""GET /welcome/Dude/ HTTP/1.1"" 200 17[24/Feb/2013 12:44:43] ""GET /welcome/ HTTP/1.1"" 404 1939ERROR:root:*** View.as_view.view is called with args: (); and kwargs: {'name': u'Bro'} ***ERROR:root:*** View.__init__ is started with kwargs: {} ***ERROR:root:*** View.__init__ reached its end. No return value. ***ERROR:root:*** View.as_view.view reached its end. Now calls dispatch() and returns the return value.ERROR:root:*** View.dispatch called, with args: (); and kwargs: {'name': u'Bro'} ***ERROR:root:*** View.dispatch reached its end. Now calls handler and returns the return value. ***[24/Feb/2013 12:44:59] ""GET /welcome/Bro/ HTTP/1.1"" 200 16 self = cls(**initkwargs)",How does django's View class work
How to save and load cookies using python selenium webdriver," How can I save all cookies in Python's Selenium WebDriver to a .txt file, and then load them later?The documentation doesn't say much of anything about the getCookies function. <code> ",How to save and load cookies using Python + Selenium WebDriver
what is the use of join() in python threading," I was studying the python threading and came across join().The author told that if thread is in daemon mode then i need to use join() so that thread can finish itself before main thread terminates.but I have also seen him using t.join() even though t was not daemonexample code is this i don't know what is use of t.join() as it is not daemon and i can see no change even if i remove it <code>  import threadingimport timeimport logginglogging.basicConfig(level=logging.DEBUG, format='(%(threadName)-10s) %(message)s', )def daemon(): logging.debug('Starting') time.sleep(2) logging.debug('Exiting')d = threading.Thread(name='daemon', target=daemon)d.setDaemon(True)def non_daemon(): logging.debug('Starting') logging.debug('Exiting')t = threading.Thread(name='non-daemon', target=non_daemon)d.start()t.start()d.join()t.join()",What is the use of join() in Python threading?
Behaviour of exec function in Python 2 and Python 3," Following code gives different output in Python2 and in Python3: Python2 prints: Python3 prints: Why does Python2 bind the variable b inside the execute function to the values in the string of the exec function, while Python3 doesn't do this? How can I achieve the behavior of Python2 in Python3? I already tried to pass dictionaries for globals and locals to exec function in Python3, but nothing worked so far.--- EDIT ---After reading Martijns answer I further analyzed this with Python3. In following example I give the locals() dictionay as d to exec, but d['b'] prints something else than just printing b. The comparison of the ids of d and locals() shows that they are the same object. But under these conditions b should be the same as d['b']. What is wrong in my example? <code>  from sys import versionprint(version)def execute(a, st): b = 42 exec(""b = {}\nprint('b:', b)"".format(st)) print(b)a = 1.execute(a, ""1.E6*a"") 2.7.2 (default, Jun 12 2011, 15:08:59) [MSC v.1500 32 bit (Intel)]('b:', 1000000.0)1000000.0 3.2.3 (default, Apr 11 2012, 07:15:24) [MSC v.1500 32 bit (Intel)]b: 1000000.042 from sys import versionprint(version)def execute(a, st): b = 42 d = locals() exec(""b = {}\nprint('b:', b)"".format(st), globals(), d) print(b) # This prints 42 print(d['b']) # This prints 1000000.0 print(id(d) == id(locals())) # This prints Truea = 1.execute(a, ""1.E6*a"")3.2.3 (default, Apr 11 2012, 07:15:24) [MSC v.1500 32 bit (Intel)]b: 1000000.0421000000.0True",Behavior of exec function in Python 2 and Python 3
How to calculate cosine similarity given 2 sentence strings? - Python," From Python: tf-idf-cosine: to find document similarity , it is possible to calculate document similarity using tf-idf cosine. Without importing external libraries, are that any ways to calculate cosine similarity between 2 strings? <code>  s1 = ""This is a foo bar sentence .""s2 = ""This sentence is similar to a foo bar sentence .""s3 = ""What is this string ? Totally not related to the other two lines .""cosine_sim(s1, s2) # Should give high cosine similaritycosine_sim(s1, s3) # Shouldn't give high cosine similarity valuecosine_sim(s2, s3) # Shouldn't give high cosine similarity value",Calculate cosine similarity given 2 sentence strings
How can I do multiple substitutions using regex in python?," I can use this code below to create a new file with the substitution of a with aa using regular expressions. I was wondering do I have to use this line, new_text = re.sub(""a"", ""aa"", text.read()), multiple times but substitute the string for others letters that I want to change in order to change more than one letter in my text? That is, so a-->aa,b--> bb and c--> cc. So I have to write that line for all the letters I want to change or is there an easier way. Perhaps to create a ""dictionary"" of translations. Should I put those letters into an array? I'm not sure how to call on them if I do.  <code>  import rewith open(""notes.txt"") as text: new_text = re.sub(""a"", ""aa"", text.read()) with open(""notes2.txt"", ""w"") as result: result.write(new_text)",How can I do multiple substitutions using regex?
How do I update pip itself from inside my virutal environment?," I'm able to update pip-managed packages, but how do I update pip itself? According to pip --version, I currently have pip 1.1 installed in my virtualenv and I want to update to the latest version. What's the command for that? Do I need to use distribute or is there a native pip or virtualenv command? I've already tried pip update and pip update pip with no success. <code> ",How do I update/upgrade pip itself from inside my virtual environment?
How do I update pip itself from inside my virtual environment?," I'm able to update pip-managed packages, but how do I update pip itself? According to pip --version, I currently have pip 1.1 installed in my virtualenv and I want to update to the latest version. What's the command for that? Do I need to use distribute or is there a native pip or virtualenv command? I've already tried pip update and pip update pip with no success. <code> ",How do I update/upgrade pip itself from inside my virtual environment?
How do I update\upgrade pip itself from inside my virtual environment?," I'm able to update pip-managed packages, but how do I update pip itself? According to pip --version, I currently have pip 1.1 installed in my virtualenv and I want to update to the latest version. What's the command for that? Do I need to use distribute or is there a native pip or virtualenv command? I've already tried pip update and pip update pip with no success. <code> ",How do I update/upgrade pip itself from inside my virtual environment?
Python : get filename from filepointer," If I have a file pointer is it possible to get the filename? Is it possible to get ""hello.txt"" using fp? <code>  fp = open(""C:\hello.txt"")",Get filename from file pointer
Does anyone know a xlib function to trap a keypress event without lose the original focus?," Does anyone know an xlib function to trap a keypress event without losing the original focus? How to get rid of it?(or ""to use XGrabKey() without generating Grab-style focusout""?)(or ""How to get rid of NotifyGrab and NotifyUngrab focus events at system level?)The XGrabKey will lose focus on key pressed and restore focus on key released.And I want to trap the keypress without leak it to the original window (just as XGrabKey can do it).References:...XGrabKey will steal focus...https://bugs.launchpad.net/gtkhotkey/+bug/390552/comments/8...The program receives control to do something in response to the key combination. Meanwhile, the program has been temporarily focused...During XGrabKey(board), discover which window had been focused...The XGrabKeyboard function actively grabs control of the keyboard and generates FocusIn and FocusOut events...http://www.x.org/archive/X11R6.8.0/doc/XGrabKeyboard.3.html#toc3...I can't see a way to provide metacity's current desktop changin behavior (changing and showing the popup dialog at the same time) without causing a Grab-type focus out on the window...https://mail.gnome.org/archives/wm-spec-list/2007-May/msg00000.html...Fullscreen mode should not exit on FocusOut events with NotifyGrab...https://bugzilla.mozilla.org/show_bug.cgi?id=578265grabbing keyboard doesnt allow changing focus ...grabbing keyboard doesnt allow changing focusFocus Events Generated by Grabs (both the active grab of XGrabKeyboard and the passive grab of XGrabKey)http://www.x.org/releases/X11R7.6/doc/libX11/specs/libX11/libX11.html#Focus_Events_Generated_by_Grabsthe XGrabKey source code: http://cgit.freedesktop.org/xorg/lib/libX11/tree/src/GrKey.c maybe we could modify this to get rid of focus-out events?there is ""DoFocusEvents(keybd, oldWin, grab->window, NotifyGrab);"" in ActivateKeyboardGrab(): http://cgit.freedesktop.org/xorg/xserver/tree/dix/events.cI'm writting a one-keystroke to keys-combination(and mouse movement) mapping software:https://code.google.com/p/diyism-myboard/I have realized it in Windows with RegisterHotKey() and UnRegisterHotKey(): https://code.google.com/p/diyism-myboard/downloads/detail?name=MyBoard.pasAnd i want to migrate it into Linux with XGrabKey() and XUngrabKey(): https://code.google.com/p/diyism-myboard/downloads/detail?name=myboard.pyI have created $10 bounty to resolve this problem. We need more backers to place bounties.https://www.bountysource.com/issues/1072081-right-button-menu-flashes-while-jkli-keys-move-the-mouse-pointer <code> ",Why XGrabKey generates extra focus-out and focus-in events?
Does anyone know an xlib function to trap a keypress event without losing the original focus?," Does anyone know an xlib function to trap a keypress event without losing the original focus? How to get rid of it?(or ""to use XGrabKey() without generating Grab-style focusout""?)(or ""How to get rid of NotifyGrab and NotifyUngrab focus events at system level?)The XGrabKey will lose focus on key pressed and restore focus on key released.And I want to trap the keypress without leak it to the original window (just as XGrabKey can do it).References:...XGrabKey will steal focus...https://bugs.launchpad.net/gtkhotkey/+bug/390552/comments/8...The program receives control to do something in response to the key combination. Meanwhile, the program has been temporarily focused...During XGrabKey(board), discover which window had been focused...The XGrabKeyboard function actively grabs control of the keyboard and generates FocusIn and FocusOut events...http://www.x.org/archive/X11R6.8.0/doc/XGrabKeyboard.3.html#toc3...I can't see a way to provide metacity's current desktop changin behavior (changing and showing the popup dialog at the same time) without causing a Grab-type focus out on the window...https://mail.gnome.org/archives/wm-spec-list/2007-May/msg00000.html...Fullscreen mode should not exit on FocusOut events with NotifyGrab...https://bugzilla.mozilla.org/show_bug.cgi?id=578265grabbing keyboard doesnt allow changing focus ...grabbing keyboard doesnt allow changing focusFocus Events Generated by Grabs (both the active grab of XGrabKeyboard and the passive grab of XGrabKey)http://www.x.org/releases/X11R7.6/doc/libX11/specs/libX11/libX11.html#Focus_Events_Generated_by_Grabsthe XGrabKey source code: http://cgit.freedesktop.org/xorg/lib/libX11/tree/src/GrKey.c maybe we could modify this to get rid of focus-out events?there is ""DoFocusEvents(keybd, oldWin, grab->window, NotifyGrab);"" in ActivateKeyboardGrab(): http://cgit.freedesktop.org/xorg/xserver/tree/dix/events.cI'm writting a one-keystroke to keys-combination(and mouse movement) mapping software:https://code.google.com/p/diyism-myboard/I have realized it in Windows with RegisterHotKey() and UnRegisterHotKey(): https://code.google.com/p/diyism-myboard/downloads/detail?name=MyBoard.pasAnd i want to migrate it into Linux with XGrabKey() and XUngrabKey(): https://code.google.com/p/diyism-myboard/downloads/detail?name=myboard.pyI have created $10 bounty to resolve this problem. We need more backers to place bounties.https://www.bountysource.com/issues/1072081-right-button-menu-flashes-while-jkli-keys-move-the-mouse-pointer <code> ",Why XGrabKey generates extra focus-out and focus-in events?
Conflicting Adjacent Regular Expression Substitutions," I have the following regular expression substitution: I use the regular expression on the string ""3 a 5 b"".I get back ""3*a 5*b"".I am thinking I should get back ""3*a*5*b"".So somehow my regular expression substitutions are interfering with each other.What can I do to get the result I want, other than iterative runs of the regular expression? <code>  input=re.sub( r""([a-zA-Z0-9])\s+([a-zA-Z0-9])"" , r""\1*\2"" , input )",Use regular expressions to replace overlapping subpatterns
Python 2.x - default arguments with *args and **kwargs," In Python 2.x (I use 2.7), which is the proper way to use default arguments with *args and **kwargs?I've found a question on SO related to this topic, but that is for Python 3:Calling a Python function with *args,**kwargs and optional / default argumentsThere, they say this method works: In 2.7, it results in a SyntaxError. Is there any recommended way to define such a function?I got it working this way, but I'd guess there is a nicer solution. <code>  def func(arg1, arg2, *args, opt_arg='def_val', **kwargs): #... def func(arg1, arg2, *args, **kwargs): opt_arg ='def_val' if kwargs.__contains__('opt_arg'): opt_arg = kwargs['opt_arg'] #...",Default arguments with *args and **kwargs
Choose a file starting with a given string in Python," In a directory I have a lot of files, named more or less like this: In Python, I have to write a code that selects from the directory a file starting with a certain string. For example, if the string is 001_MN_DX, Python selects the first file, and so on.How can I do it? <code>  001_MN_DX_1_M_32001_MN_SX_1_M_33012_BC_2_F_23......",Choose a file starting with a given string
Removing unicode \u2026 like characters in a string in python2.7," I have a string in python2.7 like this, How do i convert it to this, <code>  This is some \u03c0 text that has to be cleaned\u2026! it\u0027s annoying! This is some text that has to be cleaned! its annoying!",Removing unicode \u2026 like characters in a string
mathmatical limits in python?," I am trying to do mathematical limits in python.I have defined a function for smoke this draws one side of the smoke, the other side yet to be done.now the problem arises when i try to draw about 4 smoke circles(y=4) that the smoke starts turning the wrong way. to fix this, i considered doing a mathematical limit. I would make a variable and then do a limit on this variable: how may i do this? or is there another way not involving limits? btw this is in turtle (python language but turtle imported)thanks <code>  import turtlet = turtle.Pen()def drawsmoke(y): i = 0 while i < ((2 * y) - 1): t.seth(i * 5) t.circle((10 + i), 160) i = i + 2 smkang=(i*5) lim smkang->20 ",mathematical limits in python?
When is i += different from i= i + in python?, I was told that += can have different effects than the standard notation of i = i +. Is there a case in which i += 1 would be different from i = i + 1? <code> ,"When is ""i += x"" different from ""i = i + x"" in Python?"
"When is ""i += x"" different from ""i = i + x"" in python?", I was told that += can have different effects than the standard notation of i = i +. Is there a case in which i += 1 would be different from i = i + 1? <code> ,"When is ""i += x"" different from ""i = i + x"" in Python?"
Django: How can i create a multiple select form?," I'm beginner in Django/Python and I need to create a multiple select form. I know it's easy but I can't find any example. I know how to create a CharField with a widget but I get confused of all the options inside fields.py.For example I don't know which one of the followings is best for a multiple select form. And here is the form I need to create. EDIT:One more small question. If I want to add to each option one more attribute like data: How can I do it?Thanks for any help! <code>  'ChoiceField', 'MultipleChoiceField','ComboField', 'MultiValueField','TypedChoiceField', 'TypedMultipleChoiceField' <form action="""" method=""post"" accept-charset=""utf-8""> <select name=""countries"" id=""countries"" class=""multiselect"" multiple=""multiple""> <option value=""AUT"" selected=""selected"">Austria</option> <option value=""DEU"" selected=""selected"">Germany</option> <option value=""NLD"" selected=""selected"">Netherlands</option> <option value=""USA"">United States</option> </select> <p><input type=""submit"" value=""Continue &rarr;""></p> </form> <option value=""AUT"" selected=""selected"" data-index=1>Austria</option>",Django: How can I create a multiple select form?
Django template loop over dictionary.items with items as key," I have a dictionary in my template that I want to loop through in the usual way But in dictionary I have a key called 'items', so my loop return the value of dictionary['items'] and tries to unpack the result as key, value.How can I tell Django to use the function items instead of accessing the key? <code>  {% for key, value in dictionary.items %}","Django template loop over dictionary.items with ""items"" as key"
Project Euler #2 in Python," BackgroundI am stuck on this problem:Each new term in the Fibonacci sequence is generated by adding the previous two terms. By starting with 1 and 2, the first 10 terms will be:1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ...By considering the terms in the Fibonacci sequence whose values do not exceed four million, find the sum of the even-valued terms.I tried to discover if the issue was my Fibonacci number generator, the code which gets the even numbers, or even the way that I add the numbers to no avail.CodeI decided to store the numbers in lists. Here, I create them. Then, I created my generator. This is a potential area of issues. Then, I created some code to check if a number is even, and then add it to the even_fibs list. This is another weakpoint in the code. Lastly, I display the information. QuestionI know that this is a terrible way to ask a question, but what is wrong? Please don't give me the answer - just point out the problematic section. <code>  list_of_numbers = [] #Holds all the fibseven_fibs = [] #Holds only even fibs x,y = 0,1 #sets x to 0, y to 1while x+y <= 4000000: #Gets numbers till 4 million list_of_numbers.append(y) x, y = y, x+y #updates the fib sequence coord = 0for number in range(len(list_of_numbers)): test_number = list_of_numbers [coord] if (test_number % 2) == 0: even_fibs.append(test_number) coord+=1 print ""Normal: "", list_of_numbers #outputs full sequenceprint ""\nEven Numbers: "", even_fibs #outputs even numbersprint ""\nSum of Even Numbers: "", sum(even_fibs) #outputs the sum of even numbers",Project Euler #2 in Python
Even Fibonacci numbers in Python," BackgroundI am stuck on this problem:Each new term in the Fibonacci sequence is generated by adding the previous two terms. By starting with 1 and 2, the first 10 terms will be:1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ...By considering the terms in the Fibonacci sequence whose values do not exceed four million, find the sum of the even-valued terms.I tried to discover if the issue was my Fibonacci number generator, the code which gets the even numbers, or even the way that I add the numbers to no avail.CodeI decided to store the numbers in lists. Here, I create them. Then, I created my generator. This is a potential area of issues. Then, I created some code to check if a number is even, and then add it to the even_fibs list. This is another weakpoint in the code. Lastly, I display the information. QuestionI know that this is a terrible way to ask a question, but what is wrong? Please don't give me the answer - just point out the problematic section. <code>  list_of_numbers = [] #Holds all the fibseven_fibs = [] #Holds only even fibs x,y = 0,1 #sets x to 0, y to 1while x+y <= 4000000: #Gets numbers till 4 million list_of_numbers.append(y) x, y = y, x+y #updates the fib sequence coord = 0for number in range(len(list_of_numbers)): test_number = list_of_numbers [coord] if (test_number % 2) == 0: even_fibs.append(test_number) coord+=1 print ""Normal: "", list_of_numbers #outputs full sequenceprint ""\nEven Numbers: "", even_fibs #outputs even numbersprint ""\nSum of Even Numbers: "", sum(even_fibs) #outputs the sum of even numbers",Project Euler #2 in Python
how to read a long multiline string line by line in python," I have a wallop of a string with many lines. How do I read the lines one by one with a for clause? Here is what I am trying to do and I get an error on the textData var referenced in the for line in textData line. The textData variable does exist, I print it before going down, but I think that the pre-compiler is kicking up the error. <code>  for line in textData print line lineResult = libLAPFF.parseLine(line)",How to read a long multiline string line by line in python
how to render django template from code instead of file," I am writing a Google App Engine webapp that renders some html to a Django template. I want to either render the template using either a file or just some json thats very similar to that in file. Is it possible to use Django to render this to a file that is read in and stored in database? The oldAPI.HTML is just an old version of api.html but with some small changes. Rendering Django to the api-html file works fine.I understand that you can't store files on GAE, how can i dynamically use Django to render to HTML stored in memory? <code>  path = """"oldAPI = APIVersion().get_by_key_name(version)if oldAPI is None: path = os.path.join(os.path.dirname(__file__), ""api.html"")template_values = { 'responseDict': responseDict, } if path: self.response.out.write(template.render(path, template_values)) else: self.response.out.write(template.render(oldAPI.html,template_values))",how to render django template from code instead of file on Google App Engine
Virtualenv / pip installation log files?," Do pip or virtualenv keep a log of installations?E.g. Say I create a virtualenv and install a package in it. Is there a place where a log of the package installation is kept? If not, what would be a good way to log package installations ? (other than manually copying the output of pip install to a text file) <code>  > source [my virtual env]/bin/activate> pip install matplotlib",Virtualenv / pip installation log?
"Python, virtualenv and configuration files"," I have a Python software which includes a configuration file and a manpage. To install these, I have the following line in my setup.py (as described on http://docs.python.org/2/distutils/setupscript.html#installing-additional-files): This works just fine when I want to install the software as root with python setup.py install, but of course fails in a virtualenv, as the user is not permitted to write to /etc and /usr/share/man.What is the best practice to fix that? Check for VIRTUAL_ENV in the current environment and just not install those files at all? The software will look for foo.conf in the local directory, so that should be no problem. The user would miss the manpage, but there is no sane way to install it anyways, as man won't look for it anywhere near the virtualenv. <code>  data_files = [('/etc/foo', ['foo.conf']), ('/usr/share/man/man1', ['foo.1'])]",How to test if Python is running from a virtualenv during package installation
Python 3 Timed Input," What I would like to be able to do is ask a user a question using input. For example: and then if the time elapses print something like Any help pointing me in the right direction would be greatly appreciated. <code>  print('some scenario')prompt = input(""You have 10 seconds to choose the correct answer...\n"") print('Sorry, times up.')",Time-Limited Input?
Easy emulation of macros in Python," I would like to invoke the following code in-situ wherever I refer to MY_MACRO in my code below. Here is some code that would use MY_MACRO: In case it helps:One of the reasons why I would like to have this ability is because I would like to avoid repeating the code of MY_MACRO wherever I need it. Having something short and easy would be very helpful. Another reason is because I want to embed an IPython shell wihthin the macro and I would like to have access to all variables in locals().items() (see this other question)Is this at all possible in Python? What would be the easiest way to get this to work?Please NOTE that the macro assumes access to the entire namespace of the scope in which it's called (i.e. merely placing the code MY_MACRO in a function would not work). Note also that if I place MY_MACRO in a function, lineno would output the wrong line number. <code>  # MY_MACROframeinfo = getframeinfo(currentframe())msg = 'We are on file ' + frameinfo.filename + ' and line ' + str(frameinfo.lineno)# Assumes access to namespace and the variables in which `MY_MACRO` is called. current_state = locals().items() def some_function: MY_MACROdef some_other_function: some_function() MY_MACROclass some_class: def some_method: MY_MACRO",Adding Macros to Python
"Python print isn't using __repr__, __unicode__ or __str__ for uncode subclass?"," Python print isn't using __repr__, __unicode__ or __str__ for my unicode subclass when printing. Any clues as to what I am doing wrong?Here is my code:Using Python 2.5.2 (r252:60911, Oct 13 2009, 14:11:59) I'm not sure if this is an accurate approximation of the above, but just for comparison: [EDITED...]It sounds like the best way to get a string object that isinstance(instance, basestring) and offers control over unicode return values, and with a unicode repr is... The _str_ and _repr_ above add nothing to this example but the idea is to show a pattern explicitly, to be extended as needed.Just to prove that this pattern grants control: Thoughts? <code>  >>> class MyUni(unicode):... def __repr__(self):... return ""__repr__""... def __unicode__(self):... return unicode(""__unicode__"")... def __str__(self):... return str(""__str__"")... >>> s = MyUni(""HI"")>>> s'__repr__'>>> print s'HI' >>> class MyUni(object):... def __new__(cls, s):... return super(MyUni, cls).__new__(cls)... def __repr__(self):... return ""__repr__""... def __unicode__(self):... return unicode(""__unicode__"")... def __str__(self):... return str(""__str__"")...>>> s = MyUni(""HI"")>>> s'__repr__'>>> print s'__str__' >>> class UserUnicode(str):... def __repr__(self):... return ""u'%s'"" % super(UserUnicode, self).__str__()... def __str__(self):... return super(UserUnicode, self).__str__()... def __unicode__(self):... return unicode(super(UserUnicode, self).__str__())...>>> s = UserUnicode(""HI"")>>> su'HI'>>> print s'HI'>>> len(s)2 >>> class UserUnicode(str):... def __repr__(self):... return ""u'%s'"" % ""__repr__""... def __str__(self):... return ""__str__""... def __unicode__(self):... return unicode(""__unicode__"")... >>> s = UserUnicode(""HI"")>>> su'__repr__'>>> print s'__str__'","Python print isn't using __repr__, __unicode__ or __str__ for unicode subclass?"
"python ""import datetime"" v.s. ""from datetime import datetime"""," I have a script that needs to execute the following at different lines in the script: In my import statements I have the following: I get the following error: If I change the order of the import statements to: I get the following error: If I again change the import statement to: I get the following error: What is going on here and how do I get both to work? <code>  today_date = datetime.date.today()date_time = datetime.strp(date_time_string, '%Y-%m-%d %H:%M') from datetime import datetimeimport datetime AttributeError: 'module' object has no attribute 'strp' import datetimefrom datetime import datetime AttributeError: 'method_descriptor' object has no attribute 'today' import datetime AttributeError: 'module' object has no attribute 'strp'","""import datetime"" v.s. ""from datetime import datetime"""
sliding window in numpy," I have a Numpy array of shape (6,2): I need a sliding window with step size 1 and window size 3 like this: I'm looking for a Numpy solution. If your solution could parametrise the shape of the original array as well as the window size and step size, that'd be great.I found this related answer Using strides for an efficient moving average filter but I don't see how to specify the stepsize there and how to collapse the window from the 3d to a continuous 2d array. Also this Rolling or sliding window iterator? but that's in Python and I'm not sure how efficient that is. Also, it supports elements but does not join them together in the end if each element has multiple features. <code>  [[ 0, 1], [10,11], [20,21], [30,31], [40,41], [50,51]] [[ 0, 1,10,11,20,21], [10,11,20,21,30,31], [20,21,30,31,40,41], [30,31,40,41,50,51]]",Sliding window of M-by-N shape numpy.ndarray
sliding window of M-by-N shpae numpy.ndarray," I have a Numpy array of shape (6,2): I need a sliding window with step size 1 and window size 3 like this: I'm looking for a Numpy solution. If your solution could parametrise the shape of the original array as well as the window size and step size, that'd be great.I found this related answer Using strides for an efficient moving average filter but I don't see how to specify the stepsize there and how to collapse the window from the 3d to a continuous 2d array. Also this Rolling or sliding window iterator? but that's in Python and I'm not sure how efficient that is. Also, it supports elements but does not join them together in the end if each element has multiple features. <code>  [[ 0, 1], [10,11], [20,21], [30,31], [40,41], [50,51]] [[ 0, 1,10,11,20,21], [10,11,20,21,30,31], [20,21,30,31,40,41], [30,31,40,41,50,51]]",Sliding window of M-by-N shape numpy.ndarray
sliding window of M-by-N shape numpy.ndarray," I have a Numpy array of shape (6,2): I need a sliding window with step size 1 and window size 3 like this: I'm looking for a Numpy solution. If your solution could parametrise the shape of the original array as well as the window size and step size, that'd be great.I found this related answer Using strides for an efficient moving average filter but I don't see how to specify the stepsize there and how to collapse the window from the 3d to a continuous 2d array. Also this Rolling or sliding window iterator? but that's in Python and I'm not sure how efficient that is. Also, it supports elements but does not join them together in the end if each element has multiple features. <code>  [[ 0, 1], [10,11], [20,21], [30,31], [40,41], [50,51]] [[ 0, 1,10,11,20,21], [10,11,20,21,30,31], [20,21,30,31,40,41], [30,31,40,41,50,51]]",Sliding window of M-by-N shape numpy.ndarray
Python Getting Time Zone from Lat Long Coordinates," I am trying to get the time zones for latitude and longitude coordinates but am having a few problemsThe mistakes are probably very basicI have a table in a database with around 600 rows. Each row contains a lat long coordinate for somewhere in the worldI want to feed these co-ordinates into a function and then retrieve the time zone. The aim being to convert events which have a local time stamp within each of these 600 places into UTC timeI found a blog post which uses a piece of code to derive timezones from geographical coordinates.When I try to run the code, I get the error geonames is not defined. I have applied for an account with geonames.I think I have just saved the function file in the wrong directory or something simple. Can anyone help <code>  #-------------------------------------------------------------------------------# Converts latitude longitude into a time zone# REF: https://gist.github.com/pamelafox/2288222# REF: http://blog.pamelafox.org/2012/04/converting-addresses-to-timezones-in.html#-------------------------------------------------------------------------------geonames_client = geonames.GeonamesClient('Username_alpha')geonames_result = geonames_client.find_timezone({'lat': 48.871236, 'lng': 2.77928})user.timezone = geonames_result['timezoneId']",Getting Time Zone from Lat Long Coordinates?
How to get rid of extenstions from file basename using python," I have got the complete path of files in a list like this: what I want is to get just the file NAMES without their extensions, like: What I have tried is: But it results in: <code>  a = ['home/robert/Documents/Workspace/datafile.xlsx', 'home/robert/Documents/Workspace/datafile2.xls', 'home/robert/Documents/Workspace/datafile3.xlsx'] b = ['datafile', 'datafile2', 'datafile3'] xfn = re.compile(r'(\.xls)+')for name in a: fp, fb = os.path.split(fp) ofn = xfn.sub('', name) b.append(ofn) b = ['datafilex', 'datafile2', 'datafile3x']",How to get rid of extensions from file basename using python
how to define global variable in cherrypy," I need to access a global variable that keeps its state over diffferent server requsts.In this example the global variable is r and it is incremented at each request.How can I make r global in cherrypy? <code>  import cherrypyimport urllibclass Root(object): @cherrypy.expose def index(self, **params): jsondict = [('foo', '1'), ('fo', '2')] p = urllib.urlencode(jsondict) if r!=1 r=r+1 raise cherrypy.HTTPRedirect(""/index?"" + p) return ""hi""cherrypy.config.update({ 'server.socketPort': 8080 })cherrypy.quickstart(Root())if __name__ == '__main__': r=1",How do I use a global variable in cherrypy?
Python run generator using multiple cores for optimization," After many attempts trying optimize code, it seems that one last resource would be to attempt to run the code below using multiple cores. I don't know exactly how to convert/re-structure my code so that it can run much faster using multiple cores. I will appreciate if I could get guidance to achieve the end goal. The end goal is to be able to run this code as fast as possible for arrays A and B where each array holds about 700,000 elements. Here is the code using small arrays. The 700k element arrays are commented out. What I am trying to do is to mimic a MATLAB function called ismember[2] (The one that is formatted as: [Lia,Locb] = ismember(A,B). I am just trying to get the Locb part only. From Matlab: Locb, contain the lowest index in B for each value in A that is a member of B. The output array, Locb, contains 0 wherever A is not a member of BOne of the main problems is that I need to be able to perform this operation as efficient as possible. For testing I have two arrays of 700k elements. Creating a generator and going through the values of the generator doesn't seem to get the job done fast.  <code>  import numpy as npdef ismember(a,b): for i in a: index = np.where(b==i)[0] if index.size == 0: yield 0 else: yield indexdef f(A, gen_obj): my_array = np.arange(len(A)) for i in my_array: my_array[i] = gen_obj.next() return my_array#A = np.arange(700000)#B = np.arange(700000)A = np.array([3,4,4,3,6])B = np.array([2,5,2,6,3])gen_obj = ismember(A,B)f(A, gen_obj)print 'done'# if we print f(A, gen_obj) the output will be: [4 0 0 4 3]# notice that the output array needs to be kept the same size as array A.","Python equivalent of MATLAB's ""ismember"" function"
Install bitbucket dependencies via pip," The situation I'm trying to resolve is installing a package from a private repository on bitbucket which has it's own dependency on another private repository in bitbucket.I use this to kick off the install: which then attempts to download it's dependencies from setup.py that look like: This fails, the pip log looks like: The curious thing about this setup is, if I take a clone of project-one and run from there, project-two is fetched from bitbucket and installed into my virtualenv. My understanding was that pip was using setup tools under the hood, so my assumption was the success of that test validated my approach.Any suggestions appreciated.FOLLOW UP:So the accepted answer is quite right - but my problem had the additional complexity of being a private repo (https + http auth-basic). Using the syntax still caused a 401. Running up a shell and using pip.download.py to run urlopen demonstrates the underlying problem (ie pip needs additional setup in urllib2 to get this working).The problem is mentioned here but I couldn't get that working. <code>  pip install -e git+https://bitbucket.org/myuser/project-one.git/master#egg=django_one install_requires = ['project-two',],dependency_links = ['git+https://bitbucket.org/myuser/project-two.git/master#egg=project_two'], Downloading/unpacking project-two (from project-one) Getting page https://pypi.python.org/simple/project-two/ Could not fetch URL https://pypi.python.org/simple/project-two/: HTTP Error 404: Not Found (project-two does not have any releases) Will skip URL https://pypi.python.org/simple/project-two/ when looking for download links for project-two (from project-one) Getting page https://pypi.python.org/simple/ URLs to search for versions for project-two (from project-one): * https://pypi.python.org/simple/project-two/ * git+https://bitbucket.org/myuser/project-two.git/master#egg=project-two Getting page https://pypi.python.org/simple/project-two/ Cannot look at git URL git+https://bitbucket.org/myuser/project-two.git/master#egg=project-two Could not fetch URL https://pypi.python.org/simple/project-two/: HTTP Error 404: Not Found (project-two does not have any releases) Will skip URL https://pypi.python.org/simple/project-two/ when looking for download links for project-two (from project-one) Skipping link git+https://bitbucket.org/myuser/project-two.git/master#egg=project-two; wrong project name (not project-two) Could not find any downloads that satisfy the requirement project-two (from project-one) python setup install dependency_links=[""http://user:password@bitbucket.org/myuser/...""]",Install transitive bitbucket dependencies via pip
Install transient bitbucket dependencies via pip," The situation I'm trying to resolve is installing a package from a private repository on bitbucket which has it's own dependency on another private repository in bitbucket.I use this to kick off the install: which then attempts to download it's dependencies from setup.py that look like: This fails, the pip log looks like: The curious thing about this setup is, if I take a clone of project-one and run from there, project-two is fetched from bitbucket and installed into my virtualenv. My understanding was that pip was using setup tools under the hood, so my assumption was the success of that test validated my approach.Any suggestions appreciated.FOLLOW UP:So the accepted answer is quite right - but my problem had the additional complexity of being a private repo (https + http auth-basic). Using the syntax still caused a 401. Running up a shell and using pip.download.py to run urlopen demonstrates the underlying problem (ie pip needs additional setup in urllib2 to get this working).The problem is mentioned here but I couldn't get that working. <code>  pip install -e git+https://bitbucket.org/myuser/project-one.git/master#egg=django_one install_requires = ['project-two',],dependency_links = ['git+https://bitbucket.org/myuser/project-two.git/master#egg=project_two'], Downloading/unpacking project-two (from project-one) Getting page https://pypi.python.org/simple/project-two/ Could not fetch URL https://pypi.python.org/simple/project-two/: HTTP Error 404: Not Found (project-two does not have any releases) Will skip URL https://pypi.python.org/simple/project-two/ when looking for download links for project-two (from project-one) Getting page https://pypi.python.org/simple/ URLs to search for versions for project-two (from project-one): * https://pypi.python.org/simple/project-two/ * git+https://bitbucket.org/myuser/project-two.git/master#egg=project-two Getting page https://pypi.python.org/simple/project-two/ Cannot look at git URL git+https://bitbucket.org/myuser/project-two.git/master#egg=project-two Could not fetch URL https://pypi.python.org/simple/project-two/: HTTP Error 404: Not Found (project-two does not have any releases) Will skip URL https://pypi.python.org/simple/project-two/ when looking for download links for project-two (from project-one) Skipping link git+https://bitbucket.org/myuser/project-two.git/master#egg=project-two; wrong project name (not project-two) Could not find any downloads that satisfy the requirement project-two (from project-one) python setup install dependency_links=[""http://user:password@bitbucket.org/myuser/...""]",Install transitive bitbucket dependencies via pip
python: elegant way of finding the GPS coodinates of a circle around a certain GPS location," I have a set of GPS coordinates in decimal notation, and I'm looking for a way to find the coordinates in a circle with variable radius around each location. Here is an example of what I need. It is a circle with 1km radius around the coordinate 47,11. What I need is the algorithm for finding the coordinates of the circle, so I can use it in my kml file using a polygon. Ideally for python.  <code> ",python: elegant way of finding the GPS coordinates of a circle around a certain GPS location
How to return an array of at lest 4D: efficient method to simulate numpy.atleast_4d," numpy provides three handy routines to turn an array into at least a 1D, 2D, or 3D array, e.g. through numpy.atleast_3dI need the equivalent for one more dimension: atleast_4d. I can think of various ways using nested if statements but I was wondering whether there is a more efficient and faster method of returning the array in question. In you answer, I would be interested to see an estimate (O(n)) of the speed of execution if you can. <code> ",How to return an array of at least 4D: efficient method to simulate numpy.atleast_4d
Python tkinter - how to delete all children elements?," I am writing a GUI-based program using Python's tkinter library. I am facing a problem: I need to delete all children elements (without deleting a parent element, which in my case is colorsFrame). My code: How do I achieve this? <code>  infoFrame = Frame(toolsFrame, height = 50, bd = 5, bg = 'white')colorsFrame = Frame(toolsFrame)# adding some elementsinfoFrame.pack(side = 'top', fill = 'both')colorsFrame.pack(side = 'top', fill = 'both')# set the clear buttonButton(buttonsFrame, text = ""Clear area"", command = self.clearArea).place(x = 280, y = 10, height = 30)",How to delete all children elements?
Django adaptors CSV taking hours to import review code?," I'm using Django adaptors to upload a simple CSV. It seems to work perfectly when I'm importing 100 or 200 contacts. But when I try to upload a 165kb file with 5000 contacts, it never completes. I let let it keep trying, and when I came back after 1 hour it was still trying.What's wrong with this? There is no way a 165kb file should take over an hour to import with Django adaptors. Is there something wrong with the code? CsvModel <code>  def process(self): self.date_start_processing = timezone.now() try: # Try and import CSV ContactCSVModel.import_data(data=self.filepath, extra_fields=[ {'value': self.group_id, 'position': 5}, {'value': self.uploaded_by.id, 'position': 6}]) self._mark_processed(self.num_records) except Exception as e: self._mark_failed(unicode(e)) class ContactCSVModel(CsvModel): first_name = CharField() last_name = CharField() company = CharField() mobile = CharField() group = DjangoModelField(Group) contact_owner = DjangoModelField(User) class Meta: delimiter = ""^"" dbModel = Contact update = {'keys': [""mobile"", ""group""]}",Django adaptors CSV taking hours to import
Counting the amount of occurences in a list of tuples," I am fairly new to python, but I haven't been able to find a solution to my problem anywhere.I want to count the occurrences of a string inside a list of tuples.Here is the list of tuples: I've tried this but it just prints 0 As the same ID occurs twice in the list, this should return: I also tried to increment a counter for each occurrence of the same ID but couldn't quite grasp how to write it.*EDIT:Using Eumiro's awesome answer. I just realized that I didn't explain the whole problem.I actually need the total amount of entries which has a value more than 1. But if I try doing: I get this error: <code>  list1 = [ ('12392', 'some string', 'some other string'), ('12392', 'some new string', 'some other string'), ('7862', None, 'some other string') ] for entry in list1: print list1.count(entry[0]) 21 for name, value in list1: if value > 1: print value ValueError: Too many values to unpack",Counting the amount of occurrences in a list of tuples
'Module' Object Has no Attribute 'models'," I'm working on a simple blog engine. Here is my initial code for the models: When I try to run python manage.py syncdb blog, I get the error I'm using sqlite3. I haven't set up any views or tests yet. In settings.py, I have included the following apps: Any ideas what could be going wrong here?  <code>  from django.db import modelsfrom django.contrib.auth.models import Userclass Entry(models.Model): title = models.CharField(max_length=80) author = models.models.models.ForeignKey(User) pubdate = models.DateTimeField() text = models.TextField() tags = models.ManyToManyField(Tag)class Tag(models.Model): name = models.CharField(max_length=25)class Comment(models.Model): author = models.ForeignKey(User) pubdate = models.DateTimeField() text = models.TextField() 'Module' Object Has no Attribute 'models' 'django.contrib.auth','django.contrib.contenttypes','django.contrib.sessions','django.contrib.sites','django.contrib.messages','django.contrib.staticfiles','blogApp','south',",'Module' Object Has no Attribute 'models' error in django
Change tkinter button state," I need to change the state of a Button from DISABLED to NORMAL when some event occurs.The button is currently created in the DISABLED state using the following code: How can I change the state to NORMAL? <code>  self.x = Button(self.dialog, text=""Download"", state=DISABLED, command=self.download).pack(side=LEFT)",How to change Tkinter Button state from disabled to normal?
Alphabet range python," Instead of making a list of alphabet characters like this: is there any way that we can group it to a range or something? For example, for numbers it can be grouped using range(): <code>  alpha = ['a', 'b', 'c', 'd'.........'z'] range(1, 10)",Alphabet range in Python
Alphabet Range on Python," Instead of making a list of alphabet characters like this: is there any way that we can group it to a range or something? For example, for numbers it can be grouped using range(): <code>  alpha = ['a', 'b', 'c', 'd'.........'z'] range(1, 10)",Alphabet range in Python
Is it possible to create encodeb64 from Image object?," I am looking to create base64 inline encoded data of images for display in a table using canvases. Python generates and creates the web page dynamically. As it stands python uses the Image module to create thumbnails. After all of the thumbnails are created Python then generates base64 data of each thumbnail and puts the b64 data into hidden spans on the user's webpage. A user then clicks check marks by each thumbnail relative to their interest. They then create a pdf file containing their selected images by clicking a generate pdf button. The JavaScript using jsPDF generates the hidden span b64 data to create the image files in the pdf file and then ultimately the pdf file.I am looking to hopefully shave down Python script execution time and minimize some disk I/O operations by generating the base64 thumbnail data in memory while the script executes.Here is an example of what I would like to accomplish. This doesn't work sadly, get a TypeErorr - Any thoughts on how to accomplish this? <code>  import os, sysimport Imagesize = 128, 128 im = Image.open(""/original/image/1.jpeg"") im.thumbnail(size) thumb = base64.b64encode(im) TypeError: must be string or buffer, not instance",Is it possible to create encoded base64 URL from Image object?
Python validation mobile number," I'm trying to validate a mobile number, the following is what I have done so far, but it does not appear to work.I need it to raise a validation error when the value passed does not look like a mobile number. Mobile numbers can be 10 to 14 digits long, start with 0 or 7, and could have 44 or +44 added to them. <code>  def validate_mobile(value): """""" Raise a ValidationError if the value looks like a mobile telephone number. """""" rule = re.compile(r'/^[0-9]{10,14}$/') if not rule.search(value): msg = u""Invalid mobile number."" raise ValidationError(msg)",How do I validate a mobile number using Python?
"Faulty display of plot after set_xlim,set_ylim,set_zlim commands with tkinter and matplotlib"," I'm building a GUI with Tkinter and ttk and using matplotlib in order to creat interactive plots - again, like millions other people do.Even though most problems I encountered so far are well documented, this one seems rare:When plotting in 3d and adjusting the axis scale with set_lim() commands afterwards, the plotted line exceeds the coordinate-system which looks not good. Also, I'm not happy with the frame that seems to be a little to small. Here is an example: Just run the code, and hit the ""SET"" Button. There is my problem.Edit: Added Screenshot: <code>  # Missmatch.py""""""Graphical User Interface for plotting the resultscalculated in the script in Octave""""""# importing librariesimport matplotlib, ttk, threadingmatplotlib.use('TkAgg')import numpy as nmimport scipy as scimport pylab as plimport decimal as dcfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg, NavigationToolbar2TkAggfrom matplotlib.figure import Figurefrom mpl_toolkits.mplot3d import Axes3Dfrom oct2py import octave as ocimport Tkinter as tkiclass CS: """"""CS - Controlset. This part creates the GUI with all important Elements. Major changes and calculations will be executed in the Calculation-Class in a seperate thread. This prevents the GUI from hanging"""""" def __init__(self,parent): """"""Building the main GUI"""""" self.ThisParent=parent ### Entire Window # Mainframe that contains everything. self.main=tki.Frame(parent) # Pack manager to expand the mainframe as the windowsize changes. self.main.pack(fill=tki.BOTH, expand=tki.YES) # Configure the grid of the mainframe so that only the top left # cell grows if the users expands the window. self.main.grid_rowconfigure(0, weight=1) self.main.grid_rowconfigure(1, weight=1) ### Canvas for drawings # Creating a figure of desired size self.f = Figure(figsize=(6,6), dpi=100) # Creating a canvas that lives inside the figure self.Paper=FigureCanvasTkAgg(self.f, master=self.main) # Making the canvas's drawings visible (updating) self.Paper.show() # positioning the canvas self.Paper.get_tk_widget().grid(row=0,rowspan=3, column=0, sticky='NSWE') # creating a toolbarframe for options regarding the plots self.toolbarframe=tki.Frame(self.main) self.toolbarframe.grid(row=3, column=0, sticky='NWE') # Creating a toolbar for saving, zooming etc. (matplotlib standard) self.toolbar = NavigationToolbar2TkAgg(self.Paper, self.toolbarframe) self.toolbar.grid(row=0,column=0, sticky='NWE') # setting the standard option on zoom self.toolbar.zoom() ### Axis configuration toolbar # A frame containing the axis config-menu self.axisscaleframe=tki.Frame(self.main) self.axisscaleframe.grid(row=5, column=0, sticky='SNEW') # In that Frame, some Entry-boxes to specify scale self.xaxisscalef=ttk.Entry(self.axisscaleframe, width=10) self.xaxisscalef.insert(0,0) self.xaxisscalet=ttk.Entry(self.axisscaleframe, width=10) self.xaxisscalet.insert(0,15) self.yaxisscalef=ttk.Entry(self.axisscaleframe, width=10) self.yaxisscalef.insert(0,0) self.yaxisscalet=ttk.Entry(self.axisscaleframe, width=10) self.yaxisscalet.insert(0,15) self.zaxisscalef=ttk.Entry(self.axisscaleframe, width=10) self.zaxisscalef.insert(0,0) self.zaxisscalet=ttk.Entry(self.axisscaleframe, width=10) self.zaxisscalet.insert(0,15) # And some Labels so we know what the boxes are for self.xaxlab=ttk.Label(self.axisscaleframe, text='X-Axis', width=10) self.yaxlab=ttk.Label(self.axisscaleframe, text='Y-Axis', width=10) self.zaxlab=ttk.Label(self.axisscaleframe, text='Z-Axis', width=10) self.axinfolab=ttk.Label(self.axisscaleframe, text='Adjust axis scale:') # And a Button to validate the desired configuration self.scaleset=ttk.Button(self.axisscaleframe, text='Set', command=self.SetAxis2) self.scaleset.bind('<Return>', self.SetAxis) # Let's organize all this in the axisscaleframe-grid self.axinfolab.grid(row=0, column=0, sticky='W') self.xaxlab.grid(row=1, column=0, sticky='W') self.yaxlab.grid(row=2, column=0, sticky='W') self.zaxlab.grid(row=3, column=0, sticky='W') self.xaxisscalef.grid(row=1,column=1, sticky='W') self.yaxisscalef.grid(row=2,column=1, sticky='W') self.xaxisscalet.grid(row=1,column=2, sticky='W') self.yaxisscalet.grid(row=2,column=2, sticky='W') self.zaxisscalef.grid(row=3,column=1,sticky='W') self.zaxisscalet.grid(row=3,column=2,sticky='W') self.scaleset.grid(row=3,column=3,sticky='E') def SetAxis(self,event): self.SetAxis2() def SetAxis2(self): self.x1=float(self.xaxisscalef.get()) self.x2=float(self.xaxisscalet.get()) self.y1=float(self.yaxisscalef.get()) self.y2=float(self.yaxisscalet.get()) self.z1=float(self.zaxisscalef.get()) self.z2=float(self.zaxisscalet.get()) self.a.set_xlim(self.x1, self.x2) self.a.set_ylim(self.y1, self.y2) self.a.set_zlim(self.z1, self.z2) self.Paper.show() print ""Set axis""class Calculate3D(threading.Thread): def __init__(self): threading.Thread.__init__(self) def run(self): self.x=range(100) self.y=range(100) self.z=range(100) print 'Done!' controlset.a = controlset.f.add_subplot(111, projection='3d') controlset.a.clear() controlset.a.plot(self.x,self.y,self.z) controlset.a.mouse_init() controlset.a.set_xlabel('X') controlset.a.set_ylabel('Y') controlset.a.set_zlabel('Z') controlset.a.set_title('Title') controlset.Paper.show() returnmainw=tki.Tk()mainw.title(""Example"")mainw.geometry('+10+10')controlset=CS(mainw) #for this example code, we run our Calculate3D class automaticallyCL=Calculate3D()CL.run()mainw.mainloop()","set_xlim,set_ylim,set_zlim commands in matplotlib fail to clip displayed data"
how python count list length," I want to know how len() works.Does it count from beginning to end of a list every time I call len(), or, since list is also an class, does len() just return a variable in the list object which record the list's length?Also, I hope someone can tell me where I can find the source code of those built-in functions like 'len()', 'map()', etc. <code> ",How to Python count list length
Is it possible to set different .pylintrc file per project in eclipse?, I saw I can change it per Eclipse instance using this solution.I would like to set it per project. Is it possible? <code> ,"For Pylint, is it possible to have a different pylintrc file for each Eclipse project?"
Pyodbc error - probably wrong unixodbc configuration," I have trouble getting pyodbc work. I have unixodbc , unixodbc-dev, odbc-postgresql, pyodbc packages installed on my Linux Mint 14.I am losing hope to find solution on my own, any help appreciated. See details below:Running: Gives me: # odbcinst -j gives: Which makes me think there is a unixodbc configuration problem.Here are my unixodbc config file contents:File /etc/odbcinst.ini: File /etc/odbc.ini : File ~/.odbc.ini: <code>  >>> import pyodbc>>> conn = pyodbc.connect(""DRIVER={PostgreSQL};SERVER=localhost;DATABASE=test;USER=openerp;OPTION=3;"") >>> pyodbc.Error: ('IM002', '[IM002] [unixODBC][Driver Manager]Data source name not found, and no default driver specified (0) (SQLDriverConnect)') unixODBC 2.2.14DRIVERS............: /etc/odbcinst.iniSYSTEM DATA SOURCES: /etc/odbc.iniFILE DATA SOURCES..: /etc/ODBCDataSourcesUSER DATA SOURCES..: /home/atman/.odbc.iniSQLULEN Size.......: 4SQLLEN Size........: 4SQLSETPOSIROW Size.: 2 [PostgreSQL ANSI]Description = PostgreSQL ODBC driver (ANSI version)Driver = psqlodbca.soSetup = libodbcpsqlS.soDebug = 0CommLog = 1UsageCount = 2[PostgreSQL Unicode]Description = PostgreSQL ODBC driver (Unicode version)Driver = psqlodbcw.soSetup = libodbcpsqlS.soDebug = 0CommLog = 1UsageCount = 2 [PostgreSQL test]Description = PostgreSQL Driver = PostgreSQL ANSITrace = NoTraceFile = /tmp/psqlodbc.logDatabase = template1Servername = localhostUserName =Password =Port =ReadOnly = YesRowVersioning = NoShowSystemTables = NoShowOidColumn = NoFakeOidIndex = NoConnSettings = [DEFAULT]Driver = PostgreSQL[PostgreSQL]Description = Test to PostgresDriver = PostgreSQLTrace = YesTraceFile = sql.logDatabase = nickServername = localhostUserName =Password =Port = 5432Protocol = 6.4ReadOnly = NoRowVersioning = NoShowSystemTables = NoShowOidColumn = NoFakeOidIndex = NoConnSettings =","Pyodbc - ""Data source name not found, and no default driver specified"""
python How to not print comma in last element in a for loop," My code iterates over a set and print actor's names: The result looks like: But I want it to detect the last element so that it won't print the last comma. The result should be instead: How can I do that? <code>  for actor in actorsByMovies(): print actor+"","", Brad Pitt, George Clooney, Brad Pitt, George Clooney",Python: How to not print comma in last element in a for loop?
removing gridlines from excel using python," I'm trying to remove gridlines from excel worksheet which I created using openpyxl, and it's not working. I'm doing this: The that code prints the 'False', yet the saved file shows gridlines. <code>  wb = Workbook() ws = wb.get_active_sheet()ws.show_gridlines = Falseprint ws.show_gridlineswb.save('file.xlsx')",Removing gridlines from excel using python (openpyxl)
"Overriding nested encoding of inherited default supported objects like dict, list"," I've set up some classes of my own that are subclassed from a dictionary to act like them. Yet when I want to encode them to JSON (using Python) I want them to be serialized in a way that I can decode them back to the original objects instead of to a dict.So I want to support nested objects of my own classes (that are inherited from dict).I had tried stuff like: And: It works in general, yet not when they are nested or the first object getting dumped is not of those types. Thus this only works when the input object is of that type. Yet not when it's nested.I'm not sure how to encode this JSON recursively so all nested/contained instances are encoded according to the same rules.I thought it would be easier to the use the JSONEncoder's default method (as that gets called whenever an object is of an unsupported type.) Yet since my objects are inherited from dict they get parsed to dictionaries instead of being process by the 'default' method. <code>  class ShadingInfoEncoder(json.JSONEncoder): def encode(self, o): if type(o).__name__ == ""NodeInfo"": return '{""_NodeInfo"": ' + super(ShadingInfoEncoder, self).encode(o) + '}' elif type(o).__name__ == ""ShapeInfo"": return '{""_ShapeInfo"": ' + super(ShadingInfoEncoder, self).encode(o) + '}' elif type(o).__name__ == ""ShaderInfo"": return '{""_ShaderInfo"": ' + super(ShadingInfoEncoder, self).encode(o) + '}' return super(ShadingInfoEncoder, self).encode(o) class ShadingInfoEncoder(json.JSONEncoder): def encode(self, o): if isinstance(o, NodeInfo): return '{""_NodeInfo"": ' + super(ShadingInfoEncoder, self).encode(o) + '}' elif isinstance(o, ShapeInfo): return '{""_ShapeInfo"": ' + super(ShadingInfoEncoder, self).encode(o) + '}' elif isinstance(o, ShaderInfo): return '{""_ShaderInfo"": ' + super(ShadingInfoEncoder, self).encode(o) + '}' return super(ShadingInfoEncoder, self).encode(o)","Overriding nested JSON encoding of inherited default supported objects like dict, list"
Parallelism in Julia. Features and Limitations," In their arXiv paper, the original authors of Julia mention the following: 2.14 Parallelism. Parallel execution is provided by a message-based multi-processing system implemented in Julia in the standard library. The language design supports the implementation of such libraries by providing symmetric coroutines, which can also be thought of as cooperatively scheduled threads. This feature allows asynchronous communication to be hidden inside libraries, rather than requiring the user to set up callbacks. Julia does not currently support native threads, which is a limitation, but has the advantage of avoiding the complexities of synchronized use of shared memory.What do they mean by saying that Julia does not support native threads? What is a native thread?Do other interpreted languages such as Python or R support this type of parallelism? Is Julia alone in this? <code> ",Parallelism in Julia: Native Threading Support
How can i create the empty json object in python," I have this code But I get this error I just want that if don't have mydata in POST, then I don't get that error. <code>  json.loads(request.POST.get('mydata',dict())) No JSON object could be decoded",How can I create the empty json object?
virtualenv required to pip install virtualenv," I am trying to instal virtualenv and/or virtualenvwrapper on a mac osx 10.8.3I have been fighting with python for the last two days. Finally I was able to install python 2.7.4 using brew. Before I had virtualenv installed using easy_install. Then I tried to uninstall it, trying to get my computer in the same situation as the one of my colleagues. Maybe I uninstalled it with success, maybe not. I don't know how to test it. Now I am supposed to install virtualenv using - But it gives me - pip install virtualenvwrapper gives exactly the same output.Also the variable: PIP_RESPECT_VIRTUALENV is null: How can I solve this issue?Thanks <code>  pip install virtualenv Could not find an activated virtualenv (required). echo $PIP_RESPECT_VIRTUALENV",pip: Could not find an activated virtualenv (required)
Convert Unicode data to int in pythom," I am getting values passed from url as : Then using it as It prints : <type 'unicode'>but i keep getting the type of limit as unicode, which raises an error from query!! I am converting unicode to int but why is it not converting??Please help!!! <code>  user_data = {}if (request.args.get('title')) : user_data['title'] =request.args.get('title')if(request.args.get('limit')) : user_data['limit'] = request.args.get('limit') if 'limit' in user_data : limit = user_data['limit']conditions['id'] = {'id':1}int(limit)print type(limit)data = db.entry.find(conditions).limit(limit)",Convert Unicode data to int in python
How to load in Python-RSA a public RSA key from a file generated with openssl?," I generated a private and a public key using OpenSSL with the following commands: I then tried to load them with a python script using Python-RSA: My python script fails when it tries to load the public key: <code>  openssl genrsa -out private_key.pem 512openssl rsa -in private_key.pem -pubout -out public_key.pem import osimport rsawith open('private_key.pem') as privatefile: keydata = privatefile.read()privkey = rsa.PrivateKey.load_pkcs1(keydata,'PEM')with open('public_key.pem') as publicfile: pkeydata = publicfile.read()pubkey = rsa.PublicKey.load_pkcs1(pkeydata)random_text = os.urandom(8)#Generate signaturesignature = rsa.sign(random_text, privkey, 'MD5')print signature#Verify tokentry: rsa.verify(random_text, signature, pubkey)except: print ""Verification failed"" ValueError: No PEM start marker ""-----BEGIN RSA PUBLIC KEY-----"" found",How to load a public RSA key into Python-RSA from a file?
Is it possible to use argsort in ascending order," Consider the following code: This gives me indices of the n smallest elements. Is it possible to use this same argsort in descending order to get the indices of n highest elements? <code>  avgDists = np.array([1, 8, 6, 9, 4])ids = avgDists.argsort()[:n]",Is it possible to use argsort in descending order?
Is it possible to use argsort in descending order," Consider the following code: This gives me indices of the n smallest elements. Is it possible to use this same argsort in descending order to get the indices of n highest elements? <code>  avgDists = np.array([1, 8, 6, 9, 4])ids = avgDists.argsort()[:n]",Is it possible to use argsort in descending order?
Check if element is already in a Queue [Python]," I am using the Queue library in python and I want to keep queue entries unique.As such I want to check 'something' isn't already in the queue before adding to it, essentially a function like this which works on the Queue library: Or, should I be using a different library/method to achieve this? <code>  queue = Queue.Queue()def in_queue(u): return u in queue",Check if element is already in a Queue
Python pandas plot is a no-show," When I run this code I get no plot. This is from Wes McKinney's book on using Python for data analysis.Can anyone point me in the right direction? <code>  import pandas as pdimport numpy as npdef add_prop(group): births = group.births.astype(float) group['prop'] = births/births.sum() return grouppieces = []columns = ['name', 'sex', 'births']for year in range(1880, 2012): path = 'yob%d.txt' % year frame = pd.read_csv(path, names = columns) frame['year'] = year pieces.append(frame) names = pd.concat(pieces, ignore_index = True)total_births = names.pivot_table('births', rows = 'year', cols = 'sex', aggfunc = sum)total_births.plot(title = 'Total Births by sex and year')",Matplotlib plot is a no-show
pyqt - displaying widget on top of widget," I'm making an application that shows a map of an area and I'm trying to draw nodes on top of it which can represent information.I made it all work, but did so simply by making one custom widget which I showed and printing everything again and again everytime information changed. Also I couldn't 'connect' the nodes to listeners, because they were just images in the original widget.This made me want to reform my GUI and now I'm trying to make every class a custom widget! But there's a problem, my MapNodes aren't showing up anymore.I searched stackoverflow and found this helpful thread:How to set absolute position of the widgets in qtSo I have to give my mapnodes a parent, and parent = the widget that is being shown (?)Anyway, here is my throw at it, pasting the relevant code here. Hint at where stuff might go horrible wrong: all the inits EDIT - I forgot to add the code that adds the nodes. So after an event comes in the node needs to be created, this method fires creating the node: <code>  app = QtGui.QApplication(list())mutexbranch = Lock()mutexnode = Lock()def exec(): return app.exec_()#Singleton Pattern: wanneer en object aan iets moet kunnen# waar het inherent door de structuur niet aankon# wordt dit via dit singleton opgelostclass GuiInternalCommunication: realmap = 0class MapView(QtGui.QWidget, listener.Listener): def __init__(self, mapimagepath): QtGui.QMainWindow.__init__(self) listener.Listener.__init__(self) self.map = Map(self, mapimagepath) #self.setCentralWidget(self.map) self.initUI() def initUI(self): self.setWindowTitle('Population mapping') hbox = QtGui.QHBoxLayout() hbox.addWidget(self.map) self.setLayout(hbox) resolution = QtGui.QDesktopWidget().screenGeometry() self.setGeometry(20,20,550,800) self.show()######################################################################class Map(QtGui.QWidget): def __init__(self, parent, mapimagepath): QtGui.QWidget.__init__(self, parent) #self.timer = QtCore.QBasicTimer() #coordinaten hoeken NE en SW voor kaart in map graphics van SKO self.realmap = RealMap( mapimagepath, (51.0442, 3.7268), (51.0405, 3.7242), 550, 800) GuiInternalCommunication.realmap = self.realmap self.needsupdate = True self.timelabel = 0 parent.setGeometry(0,0,self.realmap.width, self.realmap.height) self.mapNodes = {} self.mapBranches = {} def paintEvent(self, event): painter = QtGui.QPainter() painter.begin(self) rect = self.contentsRect() #teken achtergrond self.realmap.drawRealMap(painter) #teken branches mutexbranch.acquire() try: for branch, mapBranch in self.mapBranches.items(): mapBranch.drawMapBranch(painter) finally: mutexbranch.release() ######################################################################class RealMap(QtGui.QWidget): def __init__(self, path, coordRightTop, coordLeftBot, width, height, pixpermet = 2.6): super(RealMap, self).__init__() self.path = path self.mapimage = QtGui.QImage(self.path) self.coordLeftBot = coordLeftBot self.coordRightTop = coordRightTop self.width = width self.height = height self.realdim = self.calcRealDim() self.pixpermet = pixpermet def paintEvent(self, e): painter = QtGui.QPainter() painter.begin(self) self.drawRealMap(self, painter) painter.end() def drawRealMap(self, painter): painter.drawImage(0,0,self.mapimage)######################################################################class MapNode(QtGui.QWidget): dangertocolor = {""normal"":""graphics//gradients//green.png"", ""elevated"":""graphics//gradients//orange.png"", ""danger"":""graphics//gradients//red.png""} gradimage = {""normal"":QtGui.QImage(dangertocolor[""normal""]), ""elevated"":QtGui.QImage(dangertocolor[""elevated""]), ""danger"":QtGui.QImage(dangertocolor[""danger""])} btimage = QtGui.QImage(""graphics//BT-icon.png"") def __init__(self, scanner, x, y, danger = 0, parent = None): # MapNode erft over van QWidget super(MapNode, self).__init__() QtGui.QWidget.__init__(self, parent) self.scanner = scanner self.x = x self.y = y self.danger = 'normal' self.calcDanger(danger) self.grads = {} self.grad = QtGui.QImage(MapNode.dangertocolor[self.danger]) def paintEvent(self, e): painter = QtGui.QPainter() painter.begin(self) self.drawMapNode(painter) painter.end() def drawMapNode(self, painter): realmap = GuiInternalCommunication.realmap radiusm = self.scanner.range radiusp = radiusm*realmap.pixpermet factor = radiusp/200 # basis grootte gradinten is 200 pixels. grad = MapNode.gradimage[self.danger] grad = grad.scaled(grad.size().width()*factor, grad.size().height()*factor) painter.drawImage(self.x-100*factor,self.y-100*factor, grad) painter.drawImage(self.x-10, self.y-10,MapNode.btimage) painter.drawText(self.x-15, self.y+20, str(self.scanner.sensorid) + '-' + str(self.scanner.name))######################################################################class MapBranch: branchpens = {""normal"": QtGui.QPen(QtCore.Qt.green, 3, QtCore.Qt.DashLine), ""elevated"": QtGui.QPen(QtGui.QColor(255, 51, 0), 3, QtCore.Qt.DashLine), #mandarine orange hex is 255-165-0 ""danger"": QtGui.QPen(QtCore.Qt.red, 3, QtCore.Qt.DashLine)} def __init__(self, branch, mapnode1, mapnode2, danger = 0): self.mapnode1 = mapnode1 self.mapnode2 = mapnode2 self.branch = branch self.danger = danger self.calcDanger(danger) def drawMapBranch(self, painter): painter.setPen(MapBranch.branchpens[self.danger]) painter.drawLine(self.mapnode1.x, self.mapnode1.y, self.mapnode2.x, self.mapnode2.y) def addNode(self, scanner): mutexnode.acquire() try: coord = self.realmap.convertLatLon2Pix((scanner.latitude, scanner.longitude)) self.mapNodes[scanner.sensorid] = MapNode(scanner, coord[0], coord[1], parent = self) self.mapNodes[scanner.sensorid].move(coord[0],coord[1]) #self.mapNodes[scanner.sensorid].show() finally: mutexnode.release()",PyQt - displaying widget on top of widget
"Python, how to check if a result set is empty?"," I have a sql statement that returns no hits. For example, 'select * from TAB where 1 = 2'.I want to check how many rows are returned, Here I get already exception: ""(0, 'No result set')"" How can I prevend this exception, check whether the result set is empty? <code>  cursor.execute(query_sql)rs = cursor.fetchall()",How to check if a result set is empty?
how to login to a website with python and mechanize," i'm trying to log in to the website http://www.magickartenmarkt.de and do some analyzing in the member-area (https://www.magickartenmarkt.de/?mainPage=showWants). I saw other examples for this, but i don't get why my approaches didn't work. I identified the right forms for the first approach, but it's not clear if it worked.In the second approach the returing webpage shows me that i don't have access to the member area. I would by glad for any help. <code>  import urllib2import cookielibimport urllibimport requestsimport mechanizefrom mechanize._opener import urlopenfrom mechanize._form import ParseResponseUSERNAME = 'Test'PASSWORD = 'bla123'URL = ""http://www.magickartenmarkt.de""# first approachrequest = mechanize.Request(URL)response = mechanize.urlopen(request)forms = mechanize.ParseResponse(response, backwards_compat=False)# I don't want to close?!#response.close()# Username and Password are stored in this formform = forms[1]form[""username""] = USERNAMEform[""userPassword""] = PASSWORD#proof entering data has workeduser = form[""username""] # a string, NOT a Control instanceprint userpw = form[""userPassword""] # a string, NOT a Control instanceprint pw#is this the page where I will redirected after login?print urlopen(form.click()).read () #second approachcj = cookielib.CookieJar()opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))login_data = urllib.urlencode({'username' : USERNAME, 'userPassword': PASSWORD})#loginresponse_web = opener.open(URL, login_data)#did it work? for me not....resp = opener.open('https://www.magickartenmarkt.de/?mainPage=showWants')print resp.read()",How to login to a website with python and mechanize
nltp.download() hangs on MAC OS 10.8.3," nltk.download() is hanging for me on OS X. Here is what happens: After that, it completely freezes.I installed everything according to the ntlk install page. I'm on OS X 10.8.3. On my Linux box, it just works with no problems.Any ideas? <code>  $python>>> Python 2.7.2 (default, Oct 11 2012, 20:14:37) >>> [GCC 4.2.1 Compatible Apple Clang 4.0 (tags/Apple/clang-418.0.60)] on darwin>>> import nltk>>> nltk.download()showing info http://nltk.github.com/nltk_data/",nltk.download() hangs on OS X
nltk.download() hangs on Mac OS X 10.8.3," nltk.download() is hanging for me on OS X. Here is what happens: After that, it completely freezes.I installed everything according to the ntlk install page. I'm on OS X 10.8.3. On my Linux box, it just works with no problems.Any ideas? <code>  $python>>> Python 2.7.2 (default, Oct 11 2012, 20:14:37) >>> [GCC 4.2.1 Compatible Apple Clang 4.0 (tags/Apple/clang-418.0.60)] on darwin>>> import nltk>>> nltk.download()showing info http://nltk.github.com/nltk_data/",nltk.download() hangs on OS X
"nltk.download() hangs on Mac OS X 10.8.3, 10.8.2"," nltk.download() is hanging for me on OS X. Here is what happens: After that, it completely freezes.I installed everything according to the ntlk install page. I'm on OS X 10.8.3. On my Linux box, it just works with no problems.Any ideas? <code>  $python>>> Python 2.7.2 (default, Oct 11 2012, 20:14:37) >>> [GCC 4.2.1 Compatible Apple Clang 4.0 (tags/Apple/clang-418.0.60)] on darwin>>> import nltk>>> nltk.download()showing info http://nltk.github.com/nltk_data/",nltk.download() hangs on OS X
calling python method from C++ callback," I am trying to call methods in a python class from C++. The C++ method from which this is called is a C++ callback.Within this method when I am trying to call python method, it was giving segmentation fault.I have saved an instance of python function in a global variable like where PlxMsgWrapper is a python method, which will be used in the callback.In the callback, the arguments are created as When creating the In this line its giving segmentation fault. After this the actual python method is called as <code>  // (pFunc is global variable of type PyObject*)pFunc = PyDict_GetItemString(pDict, ""PlxMsgWrapper""); PyObject* args = PyTuple_Pack(2, PyString_FromString(header.c_str()), PyString_FromString(payload.c_str())); PyObject * pInstance = PyObject_CallObject(pFunc, args); PyObject* recv_msg_func = PyObject_GetAttrString(module, (char *)""recvCallback"");args = PyTuple_Pack(1, pInstance);PyObject_CallObject(recv_msg_func, args);",Calling python method from C++ (or C) callback
How active Legend using PathCollections in Matplot?," I'm plotting groups of circles using collections and I am not able to generate the legend of the three categories. I want:Cat 1: red circlesCat 2: blue circlesCat 3: yellow circles PathCollections are not iterable objects, so trying to generate the legend the following way; does not work.How can the caption to appear?My system run on Python 2.7 and Matplotlib 1.2.0_1Note that the command print p.get_label() shows that the object has an associated label, but matplotlib is unable to mount the legend. <code>  import matplotlibimport matplotlib.pyplot as pltfrom matplotlib.collections import PatchCollectionfrom matplotlib.patches import Circleimport numpy as np# (modified from one of the matplotlib gallery examples)resolution = 50 # the number of verticesN = 50Na = 25Nb = 10x = np.random.random(N)y = np.random.random(N)radii = 0.1*np.random.random(30)xa = np.random.random(Na)ya = np.random.random(Na)radiia = 0.1*np.random.random(50)xb = np.random.random(Nb)yb = np.random.random(Nb)radiib = 0.1*np.random.random(60)patches = []patchesa = []patchesb = []for x1,y1,r in zip(x, y, radii): circle = Circle((x1,y1), r) patches.append(circle)for x1,y1,r in zip(xa, ya, radiia): circle = Circle((x1,y1), r) patchesa.append(circle)for x1,y1,r in zip(xb, yb, radiib): circle = Circle((x1,y1), r) patchesb.append(circle)fig = plt.figure()ax = fig.add_subplot(111)colors = 100*np.random.random(N)p = PatchCollection(patches, cmap=matplotlib.cm.jet, alpha=0.4, label= ""Cat 1"", facecolor=""red"")pa = PatchCollection(patchesa, cmap=matplotlib.cm.jet, alpha=0.3, label= ""Cat 2"", facecolor=""blue"")pb = PatchCollection(patchesb, cmap=matplotlib.cm.jet, alpha=0.4, label= ""Cat 3"", facecolor=""yellow"")#p.set_array(colors)ax.add_collection(p)ax.add_collection(pa)ax.add_collection(pb)ax.legend(loc = 2)plt.colorbar(p)print p.get_label()plt.show() legend([p, pa, pb], [""cat 1"", ""2 cat"", ""cat 3""])",Legend using PathCollections in matplotlib
"HDF5 and SQLite. Concurrency, compression & I/O performance"," I have the following questions about HDF5 performance and concurrency:Does HDF5 support concurrent write access? Concurrency considerations aside, how is HDF5 performance in terms of I/O performance (does compression rates affect the performance)?Since I use HDF5 with Python, how does its performance compare to Sqlite?References:http://www.sqlite.org/faq.html#q5Locking sqlite file on NFS filesystem possible?http://pandas.pydata.org/ <code> ","HDF5 - concurrency, compression & I/O performance"
How to determine thelearning rate and the variance in a gradient descent algorithm," I started to learn the machine learning last week. when I want to make a gradient descent script to estimate the model parameters, I came across a problem: How to choose a appropriate learning rate and varianceI found thatdifferent (learning ratevariance) pairs may lead to different results, some times you even can't convergence. Also, if change to another training data set, a well-chose learning ratevariancepair probably will not work. For example(script below)when I set the learning rate to 0.001 and variance to 0.00001, for 'data1', I can get the suitable theta0_guess and theta1_guess. But for data2, they can't make the algorithem convergence, even when I tried dozens of learning ratevariancepairs still can't reach to convergence. So if anybody could tell me that are there some criteria or methods to determine the learning ratevariancepair. <code>  import sysdata1 = [(0.000000,95.364693) , (1.000000,97.217205) , (2.000000,75.195834), (3.000000,60.105519) , (4.000000,49.342380), (5.000000,37.400286), (6.000000,51.057128), (7.000000,25.500619), (8.000000,5.259608), (9.000000,0.639151), (10.000000,-9.409936), (11.000000, -4.383926), (12.000000,-22.858197), (13.000000,-37.758333), (14.000000,-45.606221)]data2 = [(2104.,400.), (1600.,330.), (2400.,369.), (1416.,232.), (3000.,540.)]def create_hypothesis(theta1, theta0): return lambda x: theta1*x + theta0def linear_regression(data, learning_rate=0.001, variance=0.00001): theta0_guess = 1. theta1_guess = 1. theta0_last = 100. theta1_last = 100. m = len(data) while (abs(theta1_guess-theta1_last) > variance or abs(theta0_guess - theta0_last) > variance): theta1_last = theta1_guess theta0_last = theta0_guess hypothesis = create_hypothesis(theta1_guess, theta0_guess) theta0_guess = theta0_guess - learning_rate * (1./m) * sum([hypothesis(point[0]) - point[1] for point in data]) theta1_guess = theta1_guess - learning_rate * (1./m) * sum([ (hypothesis(point[0]) - point[1]) * point[0] for point in data]) return ( theta0_guess,theta1_guess )points = [(float(x),float(y)) for (x,y) in data1]res = linear_regression(points)print res",How to determine the learning rate and the variance in a gradient descent algorithm
Flask Template - for loop," didn't find another post which has the similar problem, I'm trying to generate some checkboxes with flask and wtforms, at the moment I've got this piece of code: This works so far, but now I try to do this with a simple for-loop like: I tried with (), {} and {{}} ... is this even possible? <code>  <div class=""control-group""> <p><strong>Check the enabled BRI Ports</strong></p> <label class=""checkbox inline""> {{ form.bri1(value=1) }} {{ form.bri1.label }} </label> <label class=""checkbox inline""> {{ form.bri2(value=1) }} {{ form.bri2.label }} </label> <label class=""checkbox inline""> {{ form.bri3(value=1) }} {{ form.bri3.label }} </label> <label class=""checkbox inline""> {{ form.bri4(value=1) }} {{ form.bri4.label }} </label></div> <div class=""control-group""> <p><strong>Check the enabled BRI Ports</strong></p> {% for n in range(1,6) %} <label class=""checkbox inline""> {{ form.brin.label }} {% endfor %}</div>",Jinja2 Template - for loop
Python - is there a different between raising an exception and exception()," Defining a parameterless exception: When raised, is there any difference between: and I couldn't find any; is it simply an overloaded syntax? <code>  class MyException(Exception): pass raise MyException raise MyException()","Is there a difference between ""raise exception()"" and ""raise exception"" without parenthesis?"
Is there a difference between raising an exception and exception()," Defining a parameterless exception: When raised, is there any difference between: and I couldn't find any; is it simply an overloaded syntax? <code>  class MyException(Exception): pass raise MyException raise MyException()","Is there a difference between ""raise exception()"" and ""raise exception"" without parenthesis?"
Integrating a scripting layer for my App (like SL4A)," I need to add scripting layer to my android App. So I can remotely prepare a script that my app download form a web service and execute on the user device.I found a interesting project called Scripting Layer for Android (SL4A) here:http://code.google.com/p/android-scripting/I'm not sure I can execute Python script without installing the PythonForAndroid_r4.apk first. I can't force my customer to install that application!So my question is, can the SL4A layer be integrated in my app without the need to install other apk?I need to execute actions like update data in the DB, create/read/delete a file on the sd card... Not so complex but I see SL4A can do a lot of things like these.Other scripting libraries?EDIT:Found also MVEL: http://mvel.codehaus.org/ but I think it needs to be integrated to execute complex operations like accessing a DB... <code> ",How to integrate Python scripting in my Android App (like SL4A)
Integrating a scripting layer in my App (like SL4A)," I need to add scripting layer to my android App. So I can remotely prepare a script that my app download form a web service and execute on the user device.I found a interesting project called Scripting Layer for Android (SL4A) here:http://code.google.com/p/android-scripting/I'm not sure I can execute Python script without installing the PythonForAndroid_r4.apk first. I can't force my customer to install that application!So my question is, can the SL4A layer be integrated in my app without the need to install other apk?I need to execute actions like update data in the DB, create/read/delete a file on the sd card... Not so complex but I see SL4A can do a lot of things like these.Other scripting libraries?EDIT:Found also MVEL: http://mvel.codehaus.org/ but I think it needs to be integrated to execute complex operations like accessing a DB... <code> ",How to integrate Python scripting in my Android App (like SL4A)
How to integrate scripting capabilities in my App (like SL4A)," I need to add scripting layer to my android App. So I can remotely prepare a script that my app download form a web service and execute on the user device.I found a interesting project called Scripting Layer for Android (SL4A) here:http://code.google.com/p/android-scripting/I'm not sure I can execute Python script without installing the PythonForAndroid_r4.apk first. I can't force my customer to install that application!So my question is, can the SL4A layer be integrated in my app without the need to install other apk?I need to execute actions like update data in the DB, create/read/delete a file on the sd card... Not so complex but I see SL4A can do a lot of things like these.Other scripting libraries?EDIT:Found also MVEL: http://mvel.codehaus.org/ but I think it needs to be integrated to execute complex operations like accessing a DB... <code> ",How to integrate Python scripting in my Android App (like SL4A)
How to integrate Python scripting capabilities in my App (like SL4A)," I need to add scripting layer to my android App. So I can remotely prepare a script that my app download form a web service and execute on the user device.I found a interesting project called Scripting Layer for Android (SL4A) here:http://code.google.com/p/android-scripting/I'm not sure I can execute Python script without installing the PythonForAndroid_r4.apk first. I can't force my customer to install that application!So my question is, can the SL4A layer be integrated in my app without the need to install other apk?I need to execute actions like update data in the DB, create/read/delete a file on the sd card... Not so complex but I see SL4A can do a lot of things like these.Other scripting libraries?EDIT:Found also MVEL: http://mvel.codehaus.org/ but I think it needs to be integrated to execute complex operations like accessing a DB... <code> ",How to integrate Python scripting in my Android App (like SL4A)
How to integrate Python scripting in my App (like SL4A)," I need to add scripting layer to my android App. So I can remotely prepare a script that my app download form a web service and execute on the user device.I found a interesting project called Scripting Layer for Android (SL4A) here:http://code.google.com/p/android-scripting/I'm not sure I can execute Python script without installing the PythonForAndroid_r4.apk first. I can't force my customer to install that application!So my question is, can the SL4A layer be integrated in my app without the need to install other apk?I need to execute actions like update data in the DB, create/read/delete a file on the sd card... Not so complex but I see SL4A can do a lot of things like these.Other scripting libraries?EDIT:Found also MVEL: http://mvel.codehaus.org/ but I think it needs to be integrated to execute complex operations like accessing a DB... <code> ",How to integrate Python scripting in my Android App (like SL4A)
How to debug a Python seg fault?," How can I debug a Python segmentation fault?We are trying to run our python code on SuSE 12.3. We get reproducible segmentation faults. The python code has been working on other platforms without segmentation faults, for years.We only code Python, no C extension ....What is the best way to debug this? I know a bit ansi c, but that was ten years ago ....Python 2.7.5UpdateThe segmentation fault happens on interpreter shutdown.I can run the script several times: But the segmentation faults happen, if I leave the pdb with ctrl-d.Update 2I now try to debug it with gdb: Update 3I installed gdbinit from http://hg.python.org/cpython/file/default/Misc/gdbinitand the debugging symbols from http://download.opensuse.org/debug/distribution/12.3/repo/oss/suse/x86_64/ What now?Update 4We installed the a new RPM (python-2.7.5-3.1.x86_64). We get less segfaults, but they still happen.Here is the link to repository:http://download.opensuse.org/repositories/devel:/languages:/python:/Factory/openSUSE_12.3/x86_64/ Update 5Solved my initial problem:It was http://bugs.python.org/issue1856 (shutdown (exit) can hang or segfault with daemon threads running)Related: Detect Interpreter shut down in daemon thread <code>  python -m pdb myscript.py arg1 arg1continueruncontinuerun gdb > file python> run myscript.py arg1 arg2Program received signal SIGSEGV, Segmentation fault.[Switching to Thread 0x7fffefbe2700 (LWP 15483)]0x00007ffff7aef93c in PyEval_EvalFrameEx () from /usr/lib64/libpython2.7.so.1.0(gdb) bt#0 0x00007ffff7aef93c in PyEval_EvalFrameEx () from /usr/lib64/libpython2.7.so.1.0#1 0x00007ffff7af5303 in PyEval_EvalCodeEx () from /usr/lib64/libpython2.7.so.1.0#2 0x00007ffff7adc858 in ?? () from /usr/lib64/libpython2.7.so.1.0#3 0x00007ffff7ad840d in PyObject_Call () from /usr/lib64/libpython2.7.so.1.0#4 0x00007ffff7af1082 in PyEval_EvalFrameEx () from /usr/lib64/libpython2.7.so.1.0#5 0x00007ffff7af233d in PyEval_EvalFrameEx () from /usr/lib64/libpython2.7.so.1.0#6 0x00007ffff7af233d in PyEval_EvalFrameEx () from /usr/lib64/libpython2.7.so.1.0#7 0x00007ffff7af5303 in PyEval_EvalCodeEx () from /usr/lib64/libpython2.7.so.1.0#8 0x00007ffff7adc5b6 in ?? () from /usr/lib64/libpython2.7.so.1.0#9 0x00007ffff7ad840d in PyObject_Call () from /usr/lib64/libpython2.7.so.1.0#10 0x00007ffff7ad9171 in ?? () from /usr/lib64/libpython2.7.so.1.0#11 0x00007ffff7ad840d in PyObject_Call () from /usr/lib64/libpython2.7.so.1.0#12 0x00007ffff7aeeb62 in PyEval_CallObjectWithKeywords () from /usr/lib64/libpython2.7.so.1.0#13 0x00007ffff7acc757 in ?? () from /usr/lib64/libpython2.7.so.1.0#14 0x00007ffff7828e0f in start_thread () from /lib64/libpthread.so.0#15 0x00007ffff755c7dd in clone () from /lib64/libc.so.6 (gdb) pystackNo symbol ""_PyUnicode_AsString"" in current context.",How to debug a Python segmentation fault?
what is the right way to treat Python argparse.Namespace() as a dictionary?," If I want to use the results of argparse.ArgumentParser(), which is a Namespace object, with a method that expects a dictionary or mapping-like object (see collections.Mapping), what is the right way to do it? Is it proper to ""reach into"" an object and use its __dict__ property?I would think the answer is no: __dict__ smells like a convention for implementation, but not for an interface, the way __getattribute__ or __setattr__ or __contains__ seem to be. <code>  C:\>pythonPython 2.7.3 (default, Apr 10 2012, 23:31:26) [MSC v.1500 32 bit (Intel)] on win32Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import argparse>>> args = argparse.Namespace()>>> args.foo = 1>>> args.bar = [1,2,3]>>> args.baz = 'yippee'>>> args['baz']Traceback (most recent call last): File ""<stdin>"", line 1, in <module>TypeError: 'Namespace' object has no attribute '__getitem__'>>> dir(args)['__class__', '__contains__', '__delattr__', '__dict__', '__doc__', '__eq__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__ne__','__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_get_args', '_get_kwargs', 'bar', 'baz', 'foo']",What is the right way to treat Python argparse.Namespace() as a dictionary?
What is the right way to treat argparse.Namespace() as a dictionary?," If I want to use the results of argparse.ArgumentParser(), which is a Namespace object, with a method that expects a dictionary or mapping-like object (see collections.Mapping), what is the right way to do it? Is it proper to ""reach into"" an object and use its __dict__ property?I would think the answer is no: __dict__ smells like a convention for implementation, but not for an interface, the way __getattribute__ or __setattr__ or __contains__ seem to be. <code>  C:\>pythonPython 2.7.3 (default, Apr 10 2012, 23:31:26) [MSC v.1500 32 bit (Intel)] on win32Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import argparse>>> args = argparse.Namespace()>>> args.foo = 1>>> args.bar = [1,2,3]>>> args.baz = 'yippee'>>> args['baz']Traceback (most recent call last): File ""<stdin>"", line 1, in <module>TypeError: 'Namespace' object has no attribute '__getitem__'>>> dir(args)['__class__', '__contains__', '__delattr__', '__dict__', '__doc__', '__eq__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__ne__','__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_get_args', '_get_kwargs', 'bar', 'baz', 'foo']",What is the right way to treat Python argparse.Namespace() as a dictionary?
Custom arrow style for pyplot.annotate," I am using matplotlib.pyplot.annotate to draw an arrow on my plot, like so: I want to use an arrow style that has a flat line at one end and an arrow at the other, so combining the styles ""|-|"" and ""->"" to make something we might call ""|->"", but I can't figure out how to define my own style.I thought I might try something like (which should just be the same as ""->"" for now; I can tweak the style later) but then how do I tell plt.annotate to use myarrow as the style? There is no arrowstyle property for plt.annotate, and arrowprops=dict(arrowstyle=myarrow) doesn't work either.I've also tried defining it in the arrowprops dictionary, such as but that gives me errors about no attribute 'set_head_width'.So, how can I define my own style for pyplot.annotate to use? <code>  import matplotlib.pyplot as pltplt.annotate("""",(x,ybottom),(x,ytop),arrowprops=dict(arrowstyle=""->"")) import matplotlib.patches as patches myarrow = patches.ArrowStyle(""Fancy"", head_length=0.4,head_width=0.2) plt.annotate("""",(x,ybottom),(x,ytop),arrowprops=dict(head_length=0.4,head_width=0.2))","Custom arrow style for matplotlib, pyplot.annotate"
unittest.mock: asserting partial match for method argument," Rubyist writing Python here. I've got some code that looks kinda like this: database.Query is mocked out, and I want to test that the ID gets injected in correctly without hardcoding the entire SQL statement into my test. In Ruby/RR, I would have done this: But I can't see a way to set up a 'selective mock' like that in unittest.mock, at least without some hairy side_effect logic. So I tried using the regexp in the assertion instead: But that doesn't work either. This approach does work, but it's ugly: Better ideas? <code>  result = database.Query('complicated sql with an id: %s' % id) mock(database).query(/#{id}/) with patch(database) as MockDatabase: instance = MockDatabase.return_value ... instance.Query.assert_called_once_with(re.compile(""%s"" % id)) with patch(database) as MockDatabase: instance = MockDatabase.return_value ... self.assertIn(id, instance.Query.call_args[0][0])",asserting partial match for method argument
Missing data in pd.crosstabs," I'm making some crosstabs with pandas: But what I actually want is the following: I found workaround by adding new column and set levels as new MultiIndex, but it seems to be difficult...Is there any way to pass MultiIndex to crosstabs function to predefine output columns? <code>  a = np.array(['foo', 'foo', 'foo', 'bar', 'bar', 'foo', 'foo'], dtype=object)b = np.array(['one', 'one', 'two', 'one', 'two', 'two', 'two'], dtype=object)c = np.array(['dull', 'dull', 'dull', 'dull', 'dull', 'shiny', 'shiny'], dtype=object)pd.crosstab(a, [b, c], rownames=['a'], colnames=['b', 'c'])b one two c dull dull shinya bar 1 1 0foo 2 1 2 b one two c dull shiny dull shinya bar 1 0 1 0foo 2 0 1 2",Missing data in pandas.crosstab
"Os.open vs open, what to use"," I'm new to python and, looking at the docs, saw that there are at least two ways of opening a file for access os.open and open. What is the difference between os.open and open?When should I use os.open?When should I use open?  <code> ","os.open vs open, what to use"
Can xlwt write actual date cells?," I'm using xlwt to make a .xls spreadsheet, and I need to create date cells.I've got writing out numbers, and setting the number format string to make them look like dates, but critically they aren't actually getting written as dates - if you do format cell in Excel, it's a ""custom"" Category rather than a ""date"" one, and this matters.Can I make xlwt actually write ""date"" cells? <code> ",Writing xlwt dates with Excel 'date' format
How to test mocked __builtin__.open in Python?," I have a class which I instantiate by giving a file name like parser = ParserClass('/path/to/file'), then I call parser.parse() method which opens and reads the file.Now I want to unit test that if something bad happening inside: the correct Exception will be raised, so I want to mock the __builtin__.open like this: but this gives me an AttributeError: StringIO instance has no attribute '__exit__'.I tought StringIO behaves exactly like a file object, but it seems, this is not the case.How could I test this method with a given content (test_lines) with mock objects? What should I use instead? <code>  with open(filename, 'rb') as fp: // do something from mock import MagicMock, patchfrom StringIO import StringIOtest_lines = StringIO(""""""some test lines, emulating a real file content"""""")mock_open = MagicMock(return_value=test_lines)with patch('__builtin__.open', mock_open): self.mock.parse()",How to unit test with a mocked file object in Python?
python shell in emacs freezes when using matplotlib," I previously thought that was the issue with IPython, but today I tested again, here is what I did:Run emacs -Q in cmd windowOpen a .py fileM-x, then run python-shell-switch-to-shell, RET, and RET. Then I have the Python shell readyI in put the following code then: Actually after this, no figure shows up, and the shell is frozen, e.g., when I input: nothing happened...I haven't tested other plotting tools but matplotlib. I don't know if it is a bug. I've searched for a while, here and though Google, but no luck. My system is: Emacs 24.3 32 bit for Windows, under Windows 7. If others can duplicate same issue as here, I will report this as a bug.I used IPython as the Python shell by: Then, I input figure(); plot([1,2,3]), as expected, the figure popup and freezes. Then I did: C-c C-d which runs comint-send-eof, and the figure actually get updated! But my IPython shell session is also terminated with the following message: Any helpful clue here?! <code>  Python 2.7.3 (default, Apr 10 2012, 23:31:26) [MSC v.1500 32 bit (Intel)] on win32Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import matplotlib.pyplot as plt>>> plt.ion()>>> plt.plot([1,2,3])[<matplotlib.lines.Line2D object at 0x03068610>]>>> >>> print(""hello"") C:/Python27/python.exe -i C:/Python27/Scripts/ipython-script.py --pylab In [6]:Do you really want to exit ([y]/n)?Traceback (most recent call last):File ""C:/Python27/Scripts/ipython-script.py"", line 9, in <module>load_entry_point('ipython==0.13.1', 'console_scripts', 'ipython')()SystemExitIf you suspect this is an IPython bug, please report it at:https://github.com/ipython/ipython/issuesor send an email to the mailing list at ipython-dev@scipy.orgYou can print a more detailed traceback right now with ""%tb"", or use ""%debug""to interactively debug it.Extra-detailed tracebacks for bug-reporting purposes can be enabled via:%config Application.verbose_crash=True",Python shell in Emacs freezes when using matplotlib
"using map(int,raw_input().split()) in python"," Though I like python very much, When I need to get multiple integer inputs in the same line, I prefer C/C++. If I use python, I use: Is this the only way or is there any pythonic way to do it? And does this cost much as far as time is considered?  <code>  a = map(int, raw_input().split())","using map(int, raw_input().split())"
Abstracting data from text Python," Suppose I have something like this: What is the best (most efficient) way to extract the text in between the tags? Should I use regex for this? My current technique relies on splitting the string on li tags and using a for loop, just wondering if there was a faster way to do this. <code>  var = '<li> <a href=""/...html"">Energy</a> <ul> <li> <a href=""/...html"">Coal</a> </li> <li> <a href=""/...html"">Oil </a> </li> <li> <a href=""/...html"">Carbon</a> </li> <li> <a href=""/...html"">Oxygen</a> </li'",Efficient way to extract text from between tags
Extracting data from text Python," Suppose I have something like this: What is the best (most efficient) way to extract the text in between the tags? Should I use regex for this? My current technique relies on splitting the string on li tags and using a for loop, just wondering if there was a faster way to do this. <code>  var = '<li> <a href=""/...html"">Energy</a> <ul> <li> <a href=""/...html"">Coal</a> </li> <li> <a href=""/...html"">Oil </a> </li> <li> <a href=""/...html"">Carbon</a> </li> <li> <a href=""/...html"">Oxygen</a> </li'",Efficient way to extract text from between tags
Trying to get another person's game to work," I am getting an error when running a python program: The game is from here.What causes this error? <code>  Traceback (most recent call last): File ""C:\Program Files (x86)\Wing IDE 101 4.1\src\debug\tserver\_sandbox.py"", line 110, in <module> File ""C:\Program Files (x86)\Wing IDE 101 4.1\src\debug\tserver\_sandbox.py"", line 27, in __init__ File ""C:\Program Files (x86)\Wing IDE 101 4.1\src\debug\tserver\class\inventory.py"", line 17, in __init__builtins.NameError: global name 'xrange' is not defined",NameError: global name 'xrange' is not defined in Python 3
NameError: global name 'xrange' is not defined," I am getting an error when running a python program: The game is from here.What causes this error? <code>  Traceback (most recent call last): File ""C:\Program Files (x86)\Wing IDE 101 4.1\src\debug\tserver\_sandbox.py"", line 110, in <module> File ""C:\Program Files (x86)\Wing IDE 101 4.1\src\debug\tserver\_sandbox.py"", line 27, in __init__ File ""C:\Program Files (x86)\Wing IDE 101 4.1\src\debug\tserver\class\inventory.py"", line 17, in __init__builtins.NameError: global name 'xrange' is not defined",NameError: global name 'xrange' is not defined in Python 3
Does not python contain library to convert latitude and longitude in DMS format into decimal format and vice versa?, Does python contain any library to convert latitude and longitude in DMS format into decimal format and vice versa? <code> ,How to convert latitude and longitude in DMS format into decimal format (and vice versa)?
Pass a tuple or list as an argument without unpacking? (Python 2.7)," Question: What are the pros and cons of writing an __init__ that takes a collection directly as an argument, rather than unpacking its contents?Context: I'm writing a class to process data from several fields in a database table. I iterate through some large (~100 million rows) query result, passing one row at a time to a class that performs the processing. Each row is retrieved from the database as a tuple (or optionally, as a dictionary).Discussion: Assume I'm interested in exactly three fields, but what gets passed into my class depends on the query, and the query is written by the user. The most basic approach might be one of the following: Here are some examples of rows that might be passed to a new instance: When faced with the above, Direct always runs (at least to this point) but is very likely to be buggy (GIGO). It takes one argument and assigns it exactly as given, so this could be a tuple or list of any size, a Null value, a function reference, etc. This is the most quick-and-dirty way I can think of to initialize the object, but I feel like the class should complain immediately when I give it data it's clearly not designed to handle.Simple handles bad1 correctly, is buggy when given bad2, and throws an error when given bad3. It's convenient to be able to effectively truncate the inputs from bad1 but not worth the bugs that would come from bad2. This one feels naive and inconsistent.Unpack seems like the safest approach, because it throws an error in all three ""bad"" cases. The last thing we want to do is silently fill our database with bad information, right? It takes the tuple directly, but allows me to identify its contents as distinct attributes instead of forcing me to keep referring to indices, and complains if the tuple is the wrong size. On the other hand, why pass a collection at all? Since I know I always want three fields, I can define __init__ to explicitly accept three arguments, and unpack the collection using the *-operator as I pass it to the new object: The only differences I see are that the __init__ definition is a bit more verbose and we raise TypeError instead of ValueError when the tuple is the wrong size. Philosophically, it seems to make sense that if we are taking some group of data (a row of a query) and examining its parts (three fields), we should pass a group of data (the tuple) but store its parts (the three attributes). So Unpack would be better.If I wanted to accept an indeterminate number of fields, rather than always three, I still have the choice to pass the tuple directly or use arbitrary argument lists (*args, **kwargs) and *-operator unpacking. So I'm left wondering, is this a completely neutral style decision? <code>  class Direct: def __init__(self, names): self.names = namesclass Simple: def __init__(self, names): self.name1 = names[0] self.name2 = names[1] self.name3 = names[2]class Unpack: def __init__(self, names): self.name1, self.name2, self.name3 = names good = ('Simon', 'Marie', 'Kent') # Exactly what we wantbad1 = ('Simon', 'Marie', 'Kent', '10 Main St') # Extra field(s) behindbad2 = ('15', 'Simon', 'Marie', 'Kent') # Extra field(s) in frontbad3 = ('Simon', 'Marie') # Forgot a field class Explicit: def __init__(self, name1, name2, name3): self.name1 = name1 self.name2 = name2 self.name3 = name3names = ('Guy', 'Rose', 'Deb')e = Explicit(*names)",Passing a collection argument without unpacking its contents
Pass a tuple or list as an argument without unpacking?," Question: What are the pros and cons of writing an __init__ that takes a collection directly as an argument, rather than unpacking its contents?Context: I'm writing a class to process data from several fields in a database table. I iterate through some large (~100 million rows) query result, passing one row at a time to a class that performs the processing. Each row is retrieved from the database as a tuple (or optionally, as a dictionary).Discussion: Assume I'm interested in exactly three fields, but what gets passed into my class depends on the query, and the query is written by the user. The most basic approach might be one of the following: Here are some examples of rows that might be passed to a new instance: When faced with the above, Direct always runs (at least to this point) but is very likely to be buggy (GIGO). It takes one argument and assigns it exactly as given, so this could be a tuple or list of any size, a Null value, a function reference, etc. This is the most quick-and-dirty way I can think of to initialize the object, but I feel like the class should complain immediately when I give it data it's clearly not designed to handle.Simple handles bad1 correctly, is buggy when given bad2, and throws an error when given bad3. It's convenient to be able to effectively truncate the inputs from bad1 but not worth the bugs that would come from bad2. This one feels naive and inconsistent.Unpack seems like the safest approach, because it throws an error in all three ""bad"" cases. The last thing we want to do is silently fill our database with bad information, right? It takes the tuple directly, but allows me to identify its contents as distinct attributes instead of forcing me to keep referring to indices, and complains if the tuple is the wrong size. On the other hand, why pass a collection at all? Since I know I always want three fields, I can define __init__ to explicitly accept three arguments, and unpack the collection using the *-operator as I pass it to the new object: The only differences I see are that the __init__ definition is a bit more verbose and we raise TypeError instead of ValueError when the tuple is the wrong size. Philosophically, it seems to make sense that if we are taking some group of data (a row of a query) and examining its parts (three fields), we should pass a group of data (the tuple) but store its parts (the three attributes). So Unpack would be better.If I wanted to accept an indeterminate number of fields, rather than always three, I still have the choice to pass the tuple directly or use arbitrary argument lists (*args, **kwargs) and *-operator unpacking. So I'm left wondering, is this a completely neutral style decision? <code>  class Direct: def __init__(self, names): self.names = namesclass Simple: def __init__(self, names): self.name1 = names[0] self.name2 = names[1] self.name3 = names[2]class Unpack: def __init__(self, names): self.name1, self.name2, self.name3 = names good = ('Simon', 'Marie', 'Kent') # Exactly what we wantbad1 = ('Simon', 'Marie', 'Kent', '10 Main St') # Extra field(s) behindbad2 = ('15', 'Simon', 'Marie', 'Kent') # Extra field(s) in frontbad3 = ('Simon', 'Marie') # Forgot a field class Explicit: def __init__(self, name1, name2, name3): self.name1 = name1 self.name2 = name2 self.name3 = name3names = ('Guy', 'Rose', 'Deb')e = Explicit(*names)",Passing a collection argument without unpacking its contents
"In this case, why does `x += y` produce a different result than `x = x + y`?"," In this case, why does x += y produce a different result than x = x + y? <code>  import numpy as npx = np.repeat([1], 10)y = np.random.random(len(x))x += yprint x# Output: [1 1 1 1 1 1 1 1 1 1]x = x + yprint x# Output: [ 1.50859536 1.31434732 1.15147365 1.76979431 1.64727364# 1.02372535 1.39335253 1.71878847 1.48823703 1.99458116]","Numpy, why does `x += y` produce a different result than `x = x + y`?"
"What exactly is the difference between shallow copy, deepcopy and normal assignment operation?"," I get the following results: If I perform deepcopy: results are the same: If I work on assignment operations: then results are: Can somebody explain what exactly makes a difference between the copies? Is it something related to mutable & immutable objects? If so, can you please explain it to me? <code>  import copya = ""deepak""b = 1, 2, 3, 4c = [1, 2, 3, 4]d = {1: 10, 2: 20, 3: 30}a1 = copy.copy(a)b1 = copy.copy(b)c1 = copy.copy(c)d1 = copy.copy(d)print(""immutable - id(a)==id(a1)"", id(a) == id(a1))print(""immutable - id(b)==id(b1)"", id(b) == id(b1))print(""mutable - id(c)==id(c1)"", id(c) == id(c1))print(""mutable - id(d)==id(d1)"", id(d) == id(d1)) immutable - id(a)==id(a1) Trueimmutable - id(b)==id(b1) Truemutable - id(c)==id(c1) Falsemutable - id(d)==id(d1) False a1 = copy.deepcopy(a)b1 = copy.deepcopy(b)c1 = copy.deepcopy(c)d1 = copy.deepcopy(d) immutable - id(a)==id(a1) Trueimmutable - id(b)==id(b1) Truemutable - id(c)==id(c1) Falsemutable - id(d)==id(d1) False a1 = ab1 = bc1 = cd1 = d immutable - id(a)==id(a1) Trueimmutable - id(b)==id(b1) Truemutable - id(c)==id(c1) Truemutable - id(d)==id(d1) True","What is the difference between shallow copy, deepcopy and normal assignment operation?"
Retreiving a Foreign Key value with django-rest-framework serializers," I'm using the django rest framework to create an API. I have the following models: To create a serializer for the categories I'd do: ... and this would provide me with: How would I go about getting the reverse from an Item serializer, ie: I've read through the docs on reverse relationships for the rest framework but that appears to be the same result as the non-reverse fields. Am I missing something obvious? <code>  class Category(models.Model): name = models.CharField(max_length=100) def __unicode__(self): return self.nameclass Item(models.Model): name = models.CharField(max_length=100) category = models.ForeignKey(Category, related_name='items') def __unicode__(self): return self.name class CategorySerializer(serializers.ModelSerializer): items = serializers.RelatedField(many=True) class Meta: model = Category [{'items': [u'Item 1', u'Item 2', u'Item 3'], u'id': 1, 'name': u'Cat 1'}, {'items': [u'Item 4', u'Item 5', u'Item 6'], u'id': 2, 'name': u'Cat 2'}, {'items': [u'Item 7', u'Item 8', u'Item 9'], u'id': 3, 'name': u'Cat 3'}] [{u'id': 1, 'name': 'Item 1', 'category_name': u'Cat 1'},{u'id': 2, 'name': 'Item 2', 'category_name': u'Cat 1'},{u'id': 3, 'name': 'Item 3', 'category_name': u'Cat 1'},{u'id': 4, 'name': 'Item 4', 'category_name': u'Cat 2'},{u'id': 5, 'name': 'Item 5', 'category_name': u'Cat 2'},{u'id': 6, 'name': 'Item 6', 'category_name': u'Cat 2'},{u'id': 7, 'name': 'Item 7', 'category_name': u'Cat 3'},{u'id': 8, 'name': 'Item 8', 'category_name': u'Cat 3'},{u'id': 9, 'name': 'Item 9', 'category_name': u'Cat 3'}]",Retrieving a Foreign Key value with django-rest-framework serializers
Opening a wave file in python: unknown format: 49. What's going wrong?," I try to open a wave file with the wave module, but I keep getting the same error whatever I try.The line with the error is the following: This is the error message: String f is a path to a .WAV file and it works when played in any of my media players.I have of course imported the wave module.I tried f both as a relative and an absolute path.I tried replacing ""WAV"" by ""wav"".What is the error caused by? <code>  wav = wave.open(f) Traceback (most recent call last): File ""annotate.py"", line 47, in <module> play(file) File ""annotate.py"", line 33, in play wav = wave.open(f) File ""C:\Program Files (x86)\Python\lib\wave.py"", line 498, in open return Wave_read(f) File ""C:\Program Files (x86)\Python\lib\wave.py"", line 163, in __init__ self.initfp(f) File ""C:\Program Files (x86)\Python\lib\wave.py"", line 143, in initfp self._read_fmt_chunk(chunk) File ""C:\Program Files (x86)\Python\lib\wave.py"", line 269, in _read_fmt_chunk raise Error('unknown format: %r' % (wFormatTag,))wave.Error: unknown format: 49",Opening a wave file in Python: unknown format: 49. What's going wrong?
Is there a scala equivalent to python's list comprehension," I'm translating some of my Python code to Scala, and I was wondering if there's an equivalent to Python's list-comprehension: Essentially I'm trying to remove certain elements from the list if it matches. <code>  [x for x in list if x!=somevalue]",Is there a Scala equivalent to Python's list comprehension?
Python tkinter binding mousewheel to scrollbar," I have this scroll-able frame (frame inside canvas actually). I would like to bind mouse wheel to the scrollbar so that user can scroll down the frame without having to use arrow buttons on the scrollbar. After looking around, i added a binding to my canvas1 like this This is the function: But the scroll bar won't move when i use mousewheel. Can anyone help me with this? All i want is when the user use mousewheel (inside the frame area/on the scrollbar), the canvas should automatically scroll up or down. <code>  import Tkinter as tkclass Scrollbarframe(): def __init__(self, parent,xsize,ysize,xcod,ycod): def ScrollAll(event): canvas1.configure(scrollregion=canvas1.bbox(""all""),width=xsize,height=ysize,bg='white') self.parent=parent self.frame1=tk.Frame(parent,bg='white') self.frame1.place(x=xcod,y=ycod) canvas1=tk.Canvas(self.frame1) self.frame2=tk.Frame(canvas1,bg='white',relief='groove',bd=1,width=1230,height=430) scrollbar1=tk.Scrollbar(self.frame1,orient=""vertical"",command=canvas1.yview) canvas1.configure(yscrollcommand=scrollbar1.set) scrollbar1.pack(side=""right"",fill=""y"") canvas1.pack(side=""left"") canvas1.create_window((0,0),window=self.frame2,anchor='nw') self.frame2.bind(""<Configure>"",ScrollAll) self.frame1.bind(""<MouseWheel>"", self.OnMouseWheel) def OnMouseWheel(self,event): self.scrollbar1.yview(""scroll"",event.delta,""units"") return ""break"" ",tkinter: binding mousewheel to scrollbar
python pandas/numpy True/False to 1/0 mapping," I have a column in python pandas DataFrame that has boolean True/False values, but for further calculations I need 1/0 representation. Is there a quick pandas/numpy way to do that? <code> ",How can I map True/False to 1/0 in a Pandas DataFrame?
Sorting list of values in a dictionary," I have values like this: I want to sort the values in each set in increasing order. I don't want to sort between the sets, but the values in each set. <code>  set(['0.000000000', '0.009518000', '10.277200999', '0.030810999', '0.018384000', '4.918560000'])set(['4.918859000', '0.060758000', '4.917336999', '0.003949999', '0.013945000', '10.281522000', '0.025082999']) ",Sorting a set of values
SQLAlchemy Relationship Error," I used sqlautocode to generate my model and all the relationships. I'm trying to do a simple query like For some reason I keep getting this error message: If I comment out the relationship definitions, then the query above works. The relationship definitions generated by sqlautocode look right to me but I'm new to SqlAlchemy. I'm not sure how to fix this. I tried changing from relation() to relationship() but I still get the same error.Using sqlalchemy 0.8.2 and sqlautocode 0.6.Note there's a many-to-one relation between Event and Event_Type and a many-to-one between Event and Venue.model.py Error log <code>  obj = session.query(Venue).filter(Venue.symbol==""CARNEGIE_HALL"").one() File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/relationships.py"", line 331, in _annotate_present_fks secondarycols = util.column_set(self.secondary.c) AttributeError: 'Event' object has no attribute 'c' DeclarativeBase = declarative_base()metadata = DeclarativeBase.metadatametadata.bind = engineEvent = Table(u'Event', metadata, Column(u'id', INTEGER(), primary_key=True, nullable=False), Column(u'venue_id', INTEGER(), ForeignKey('Venue.id'), nullable=False), Column(u'event_type_id', INTEGER(), ForeignKey('Event_Type.id'), nullable=False),)Venue = Table(u'Venue', metadata, Column(u'id', INTEGER(), ForeignKey('Obj.id'), primary_key=True, nullable=False), Column(u'venue_type_id', INTEGER(), ForeignKey('Venue_Type.id'), nullable=False), Column(u'name', VARCHAR(length=100), nullable=False), Column(u'symbol', VARCHAR(length=50), nullable=False),)class Event(DeclarativeBase): __table__ = Event #relation definitions Event_Type = relation('EventType', primaryjoin='Event.event_type_id==EventType.id') Venue = relation('Venue', primaryjoin='Event.venue_id==Venue.id')class EventType(DeclarativeBase): __tablename__ = 'Event_Type' __table_args__ = {} #column definitions code = Column(u'code', VARCHAR(length=50), nullable=False) description = Column(u'description', VARCHAR(length=250)) id = Column(u'id', INTEGER(), primary_key=True, nullable=False) name = Column(u'name', VARCHAR(length=100), nullable=False) #relation definitions Venues = relation('Venue', primaryjoin='EventType.id==Event.event_type_id', secondary=Event, secondaryjoin='Event.venue_id==Venue.id')class Venue(DeclarativeBase): __table__ = Venue #relation definitions Event_Types = relation('EventType', primaryjoin='Venue.id==Event.venue_id', secondary=Event, secondaryjoin='Event.event_type_id==EventType.id') mod_wsgi (pid=10861): Exception occurred processing WSGI script '/home/uname/web/html/foo/app/main.py'. Traceback (most recent call last): File ""/home/uname/web/html/foo/app/main.py"", line 208, in application return callback(environ, start_response) File ""/home/uname/web/html/foo/app/main.py"", line 68, in monitor obj = session.query(Venue).filter(Venue.symbol==""CARNEGIE_HALL"").one() File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 1106, in query return self._query_cls(entities, self, **kwargs) File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/query.py"", line 115, in __init__ self._set_entities(entities) File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/query.py"", line 124, in _set_entities self._set_entity_selectables(self._entities) File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/query.py"", line 157, in _set_entity_selectables ent.setup_entity(*d[entity]) File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/query.py"", line 2728, in setup_entity self._with_polymorphic = ext_info.with_polymorphic_mappers File ""/usr/lib64/python2.6/site-packages/sqlalchemy/util/langhelpers.py"", line 614, in __get__ obj.__dict__[self.__name__] = result = self.fget(obj) File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/mapper.py"", line 1426, in _with_polymorphic_mappers configure_mappers() File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/mapper.py"", line 2121, in configure_mappers mapper._post_configure_properties() File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/mapper.py"", line 1243, in _post_configure_properties prop.init() File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/interfaces.py"", line 231, in init self.do_init() File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/properties.py"", line 1028, in do_init self._setup_join_conditions() File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/properties.py"", line 1102, in _setup_join_conditions can_be_synced_fn=self._columns_are_mapped File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/relationships.py"", line 115, in __init__ self._annotate_fks() File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/relationships.py"", line 311, in _annotate_fks self._annotate_present_fks() File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/relationships.py"", line 331, in _annotate_present_fks secondarycols = util.column_set(self.secondary.c) AttributeError: 'Event' object has no attribute 'c'",SQLAlchemy Relationship Error: object has no attribute 'c'
Celery passing messages to task," To my knowledge, Celery acts as both the producer and consumer of messages. This is not what I want to achieve. I want Celery to act as the consumer only, to fire certain tasks based on messages that I send to my AMQP broker of choice. Is this possible?Or do I need to make soup by adding carrot to my stack? <code> ",Celery and custom consumers
Python: convert boolean array to int array," I use Scilab, and want to convert an array of booleans into an array of integers: In Scilab I can use: or even just multiply it by 1: Is there a simple command for this in Python, or would I have to use a loop? <code>  >>> x = np.array([4, 3, 2, 1])>>> y = 2 >= x>>> yarray([False, False, True, True], dtype=bool) >>> bool2s(y)0. 0. 1. 1. >>> 1*y0. 0. 1. 1. ",How to convert a boolean array to an int array
"python: Given an _io.BytesIO image buffer, generate image in html"," Is it possible to generate a functional image tag in html from a BytesIO buffer? I'd like to do something along these lines: May be necessary to re-format the value of the buffer in some way, or to change the content of the 'src' data. Thank you. <code>  import matplotlibmatplotlib.use('Agg') import pylabimport Imageimport iotemp_data = {'x':[1,2,3],'y':[2,4,5]}pylab.plot(temp_data['x'], temp_data['y'])img_buffer = io.BytesIO()pylab.savefig(img_buffer, format = 'png')img_buffer.seek(0)img_tag = ""<img src='data:image/png;base64,'"" + img_buffer.getvalue() + ""</img>""","python: Given a BytesIO buffer, generate img tag in html?"
"python: Given a BytesIO image buffer, generate image tag in html"," Is it possible to generate a functional image tag in html from a BytesIO buffer? I'd like to do something along these lines: May be necessary to re-format the value of the buffer in some way, or to change the content of the 'src' data. Thank you. <code>  import matplotlibmatplotlib.use('Agg') import pylabimport Imageimport iotemp_data = {'x':[1,2,3],'y':[2,4,5]}pylab.plot(temp_data['x'], temp_data['y'])img_buffer = io.BytesIO()pylab.savefig(img_buffer, format = 'png')img_buffer.seek(0)img_tag = ""<img src='data:image/png;base64,'"" + img_buffer.getvalue() + ""</img>""","python: Given a BytesIO buffer, generate img tag in html?"
MATLAB twice as fast as NUMPY," I am an engineering grad student currently making the transition from MATLAB to Python for the purposes of numerical simulation. I was under the impression that for basic array manipulation, Numpy would be as fast as MATLAB. However, it appears for two different programs I write that MATLAB is a little under twice as fast as Numpy. The test code I am using for Numpy (Python 3.3) is: Whereas for MATLAB 2012a I am using: The algorithm I am using is the one used on a NASA website comparing Numpy and MATLAB. The website shows that Numpy surpasses MATLAB in terms of speed for this algorithm. Yet my results show a 0.49 s simulation time for Numpy and a 0.29 s simulation time for MATLAB. I also have run a Gauss-Seidel solver on both Numpy and Matlab and I get similar results (16.5 s vs. 9.5 s)I am brand new to Python and am not extremely literate in terms of programming. I am using the WinPython 64 bit Python distribution but have also tried Pythonxy to no avail.One thing I have read which should improve performance is building Numpy using MKL. Unfortunately I have no idea how to do this on Windows. Do I even need to do this?Any suggestions? <code>  import numpy as npimport timea = np.random.rand(5000,5000,3)tic = time.time()a[:,:,0] = a[:,:,1]a[:,:,2] = a[:,:,0]a[:,:,1] = a[:,:,2]toc = time.time() - ticprint(toc) a = rand(5000,5000,3);tic;a(:,:,1) = a(:,:,2);a(:,:,3) = a(:,:,1);a(:,:,2) = a(:,:,3);toc",MATLAB twice as fast as Numpy
Storing and loading numpy arrays as string," In my program I'm working with various numpy arrays of varying sizes. I need to store them into XML files for later use. I did not write them to binary files so I have all my data in one place (the XML file) and not scattered through 200 files.So I tried to use numpy's array_str() method to transform an array into a String.The resulting XML looks like this: The Weights are the values I want to store. Now the problem is that numpy's fromstring() method can't reload these apparently...I get ""ValueError: string size must be a multiple of element size""I wrote them with ""np.array_str(w1)"" and try to read them with ""np.fromstring(w_str1)"".Apparently the result is only a 1D array even if it works, so I have to restore the shape manually. Ugh, that is a pain already since I'll also have to store it somehow too.What is the best way to do this properly? Preferably one that also saves my array's shape and datatype without manual housekeeping for every little thing. <code>  -<Test date=""2013-07-10-17:19""> <Neurons>5</Neurons> <Errors>[7.7642140551985428e-06, 7.7639131137987232e-06]</Errors> <Iterations>5000</Iterations> <Weights1>[[ 0.99845902 -0.70780512 0.26981375 -0.6077122 0.09639695] [ 0.61856711 -0.74684913 0.20099992 0.99725171 -0.41826754] [ 0.79964397 0.56620812 -0.64055346 -0.50572793 -0.50100635]]</Weights1> <Weights2>[[-0.1851452 -0.22036027] [ 0.19293429 -0.1374252 ] [-0.27638478 -0.38660974] [ 0.30441414 -0.01531598] [-0.02478953 0.01823584]]</Weights2></Test>",Storing and loading numpy arrays as files
Python TkMessageBox not closing after click OK," I have created a script in Python which notifies me at a given event.I am using the following function to produce the warning window: The window draws fine, but when I click ok, the window stays in place with the button depressed. Im using xfce.Is there anyway to get the window to close after ok is clicked?A comment indicated this may be to do with surrounding code, so for completeness: <code>  def window_warn(): ''' This function will throw up a window with some text ''' #These two lines get rid of tk root window root = Tkinter.Tk() root.withdraw() #tkMessageBox.deiconify() TkMessageBox.showwarning(""New Case"", ""You have a new case\n Please restart pycheck"") return print ""Just started newcase check""while True: if ""Uncommitted"" in webpage: print ""oh look, 'Uncommitted' is in the url returned from the last function"" #If this hits we call a notification window window_warn() print ""sleeping"" time.sleep(10) webpage = scrape_page() else: print ""nothing"" time.sleep(20) webpage = scrape_page()",Tkinter TkMessageBox not closing after click OK
"converting ""yield from"" statement to python 2.7 code"," I had a code below in Python 3.2 and I wanted to run it in Python 2.7. I did convert it (have put the code of missing_elements in both versions) but I am not sure if that is the most efficient way to do it. Basically what happens if there are two yield from calls like below in upper half and lower half in missing_element function? Are the entries from the two halves (upper and lower) appended to each other in one list so that the parent recursion function with the yield from call and use both the halves together? <code>  def missing_elements(L, start, end): # Python 3.2 if end - start <= 1: if L[end] - L[start] > 1: yield from range(L[start] + 1, L[end]) returnindex = start + (end - start) // 2# is the lower half consecutive?consecutive_low = L[index] == L[start] + (index - start)if not consecutive_low: yield from missing_elements(L, start, index)# is the upper part consecutive?consecutive_high = L[index] == L[end] - (end - index)if not consecutive_high: yield from missing_elements(L, index, end)def main(): L = [10, 11, 13, 14, 15, 16, 17, 18, 20] print(list(missing_elements(L, 0, len(L)-1))) L = range(10, 21) print(list(missing_elements(L, 0, len(L)-1)))def missing_elements(L, start, end): # Python 2.7 return_list = [] if end - start <= 1: if L[end] - L[start] > 1: return range(L[start] + 1, L[end]) index = start + (end - start) // 2 # is the lower half consecutive? consecutive_low = L[index] == L[start] + (index - start) if not consecutive_low: return_list.append(missing_elements(L, start, index)) # is the upper part consecutive? consecutive_high = L[index] == L[end] - (end - index) if not consecutive_high: return_list.append(missing_elements(L, index, end)) return return_list","Converting ""yield from"" statement to Python 2.7 code"
Can Django do multi-thread works?," The sequence I would like to accomplish:A user clicks a button on a web pageSome functions in model.py start to run. For example, gathering some data by crawling the internetWhen the functions are finished, the results are returned to the user.Should I open a new thread inside of model.py to execute my functions? If so, how do I do this? <code> ",Can you perform multi-threaded tasks within Django?
"in python, mixin is like composition? then why just use composition?"," I understand mixin as what looks like inheritance but what is more like composition.(edit: I tend to think giving additional functionality/attributes by mixin rather than giving another is-a relationship.)Mentally, I'm saying something like this when I use mixin: I'm giving you this mixin you are missing, rather than you are actually this mixin-type as well.(is-a) And I read few times, you should prefer composition over inheritance. We could just use straight compositions instead of mixins, what is mixin for? If I have to guess, it's because my_instance.foo() is easier than my_instance.another_instance.foo()?(You can use my_instance.foo() if mixin has foo(), you need my_instance.another_instance.foo() when you composite another_instance as an attribute of my_instance) Are there any other reason?Edit: So even though I feel it's has-a, mixin is still is-a relationship. and benefit you get when you use is-a here is, cleaner interface. That' how I interpret delnan's answer.  <code> ","In Python, are mixins equivalent to composition? If so, then why not just use composition?"
"in python, mixin is like composition? then why not just use composition?"," I understand mixin as what looks like inheritance but what is more like composition.(edit: I tend to think giving additional functionality/attributes by mixin rather than giving another is-a relationship.)Mentally, I'm saying something like this when I use mixin: I'm giving you this mixin you are missing, rather than you are actually this mixin-type as well.(is-a) And I read few times, you should prefer composition over inheritance. We could just use straight compositions instead of mixins, what is mixin for? If I have to guess, it's because my_instance.foo() is easier than my_instance.another_instance.foo()?(You can use my_instance.foo() if mixin has foo(), you need my_instance.another_instance.foo() when you composite another_instance as an attribute of my_instance) Are there any other reason?Edit: So even though I feel it's has-a, mixin is still is-a relationship. and benefit you get when you use is-a here is, cleaner interface. That' how I interpret delnan's answer.  <code> ","In Python, are mixins equivalent to composition? If so, then why not just use composition?"
Python: # Comments after backslash, This doesn't work: Neither does this: Neither does this: Is there a way to make comments in the code broken into multiple lines? <code>  something = \ line_of_code * \ # Comment another_line_of_code * \ # Comment and_another_one * \ # Comment etc something = \ # Comment \ line_of_code * \ # Comment \ another_line_of_code * ... something = \ ''' Comment ''' \ line_of_code * \ ''' Comment ''' \ another_line_of_code * ...,Is there a way to put comments in multiline code?
Python: '#' Comments after backslash, This doesn't work: Neither does this: Neither does this: Is there a way to make comments in the code broken into multiple lines? <code>  something = \ line_of_code * \ # Comment another_line_of_code * \ # Comment and_another_one * \ # Comment etc something = \ # Comment \ line_of_code * \ # Comment \ another_line_of_code * ... something = \ ''' Comment ''' \ line_of_code * \ ''' Comment ''' \ another_line_of_code * ...,Is there a way to put comments in multiline code?
"Why does python allow an empty function body without a ""pass"" statement?"," In a recent review of some code similar to the above, a colleague asked:How come method_one is successfully parsed and accepted by python? Doesn't an empty function need a body consisting of just pass? i.e. shouldn't it look like this? My response at the time was something like:Although the docstring is usually not considered to be part of the function body, because it is not ""executed"", it is parsed as such, so the pass can be omitted.In the spirit of sharing knowledge Q&A style, I thought I'd post the more rigorous answer here. <code>  class SomeThing(object): """"""Represents something"""""" def method_one(self): """"""This is the first method, will do something useful one day"""""" def method_two(self, a, b): """"""Returns the sum of a and b"""""" return a + b def method_one(self): """"""This is the first method, will do something useful one day"""""" pass","Why does python allow an empty function (with doc-string) body without a ""pass"" statement?"
Django display loading time on every page," In Django, how can I return the time it took to load a page (not the date) in every page of the site, without having to write in every views.py a code similar to the following one? If using a TEMPLATE_CONTEXT_PROCESSOR is the best option.How would I get the whole page loading time from there, instead of just getting the template loading time?UPDATE:As the initial question doesn't seem to be clear enough, here is an approach of what would be the Python version of what I want to do. <code>  start = time.time()#model operationsloadingpagetime = time.time() - start #!/usr/bin/env pythonimport cgitb; cgitb.enable() import timeprint 'Content-type: text/html\n\n'start = time.time()print '<html>'print '<head>'print '</head>'print '<body>'print '<div>HEADER</div>'print '<div>'print '<p>Welcome to my Django Webpage!</p>'print '<p>Welcome to my Django Webpage!</p>'print '<p>Welcome to my Django Webpage!</p>'print '</div>'time.sleep(3)loadingtime = time.time() - startprint '<div>It took ',loadingtime,' seconds to load the page</div>'print '</body>'print '</html>'",Django: display time it took to load a page on every page
How to plot two columns of a pandas data frame using points?," I have a pandas dataframe and would like to plot values from one column versus the values from another column. Fortunately, there is plot method associated with the data-frames that seems to do what I need: Unfortunately, it looks like among the plot styles (listed here after the kind parameter) there are not points. I can use lines or bars or even density but not points. Is there a work around that can help to solve this problem. <code>  df.plot(x='col_name_1', y='col_name_2')",How to plot two columns of a pandas data frame using points
Python Key input," (In 2013) I don't know why Python is that weird, you can't find this by searching in google very easily, but it's quite simple.How can I detect 'SPACE' or actually any key?How can I do this: This should be included in python core, so please do not link modules not related for core python. <code>  print('You pressed %s' % key)",Detect key input in Python
construct pandas DataFrame from values in variables," This may be a simple question, but I can not figure out how to do this. Lets say that I have two variables as follows. I want to construct a DataFrame from this: This generates an error: ValueError: If using all scalar values, you must pass an indexI tried this also: This gives the same error message. <code>  a = 2b = 3 df2 = pd.DataFrame({'A':a,'B':b}) df2 = (pd.DataFrame({'a':a,'b':b})).reset_index()","Constructing pandas DataFrame from values in variables gives ""ValueError: If using all scalar values, you must pass an index"""
Convert string to image in python," I started to learn python a week ago and want to write a small program that converts a email to a image (.png) so that it can be shared on forums without risking to get lots of spam mails.It seems like the python standard library doesn't contain a module that can do that but I've found out that there's a PIL module for it (PIL.ImageDraw).My problem is that I can't seem to get it working.So basically my questions are:How to draw a text onto a image.How to create a blank (white) imageIs there a way to do this without actually creating a file so that I can show it in a GUI before saving it?Current Code: <code>  import Imageimport ImageDrawimport ImageFontdef getSize(txt, font): testImg = Image.new('RGB', (1, 1)) testDraw = ImageDraw.Draw(testImg) return testDraw.textsize(txt, font)if __name__ == '__main__': fontname = ""Arial.ttf"" fontsize = 11 text = ""example@gmail.com"" colorText = ""black"" colorOutline = ""red"" colorBackground = ""white"" font = ImageFont.truetype(fontname, fontsize) width, height = getSize(text, font) img = Image.new('RGB', (width+4, height+4), colorBackground) d = ImageDraw.Draw(img) d.text((2, height/2), text, fill=colorText, font=font) d.rectangle((0, 0, width+3, height+3), outline=colorOutline) img.save(""D:/image.png"")",How to convert a string to an image?
How to check if object is pickleable in python?," I have a list of objects of various types that I want to pickle. I would like to pickle only those which are pickleable. Is there a standard way to check if an object is of pickleable type, other than trying to pickle it?The documentation says that if a pickling exception occurs it may be already after some of the bytes have been written to the file, so trying to pickle the objects as a test doesn't seem like a good solution.I saw this post but it doesn't answer my question. <code> ",How to check if an object is pickleable
Annotating points from a Pandas Dataframe Matplotlib plot," Given a DataFrame like: Using the code: I generate a plot like:I cannot figure out how to get the axis to attach an annotation?This example Annotate Time Series plot in Matplotlib is very close but I don't know how to specify the x and y axis from the DataFrame?So it should be close to: But what do I use instead of x[1] and y[1] to get the axis? I tried ['MORLD'][1] and ['SOLD_PRICE'][1] and got index out of range... <code>  LIST_PRICE SOLD_PRICEMOYRLD 1999-03-31 317062.500000 3148001999-06-30 320900.000000 3071001999-09-30 400616.666667 3661601999-12-31 359900.000000 NaN2000-03-31 359785.714286 330750 import matplotlib.dates as mdatesax3=df5.plot()ax3.set_ylim(100000,600000)ax3.set_title('Heatherwood-Quarterly') ax3.annotate('Test', (mdates.date2num(x[1]), y[1]), xytext=(15, 15), textcoords='offset points', arrowprops=dict(arrowstyle='-|>'))fig.autofmt_xdate()plt.show()",Annotating points from a Pandas Dataframe in Matplotlib plot
How can I dynamically create static methods for a class in python," If I define a little python program as I receive the traceback error What I am trying to figure out is, how can I dynamically set a class method to a class without instantiating an object?Edit:The answer for this problem is returns the following output <code>  class a(): def _func(self): return ""asdf"" # Not sure what to resplace __init__ with so that a.func will return asdf def __init__(self, *args, **kwargs): setattr(self, 'func', classmethod(self._func))if __name__ == ""__main__"": a.func Traceback (most recent call last): File ""setattr_static.py"", line 9, in <module> a.funcAttributeError: class a has no attribute 'func' class a(): passdef func(cls, some_other_argument): return some_other_argumentsetattr(a, 'func', classmethod(func))if __name__ == ""__main__"": print(a.func) print(a.func(""asdf"")) <bound method type.func of <class '__main__.a'>>asdf",How can I dynamically create class methods for a class in python
How to equalize the scales of x-axis and y-axis in Python matplotlib?," I wish to draw lines on a square graph.The scales of x-axis and y-axis should be the same.e.g. x ranges from 0 to 10 and it is 10cm on the screen. y has to also range from 0 to 10 and has to be also 10 cm.The square shape has to be maintained, even if I mess around with the window size.Currently, my graph scales together with the window size.How may I achieve this?UPDATE:I tried the following, but it did not work. <code>  plt.xlim(-3, 3)plt.ylim(-3, 3)plt.axis('equal')",How to equalize the scales of x-axis and y-axis in matplotlib
How to equalize the scales of x-axis and y-axis in matplotlib?," I wish to draw lines on a square graph.The scales of x-axis and y-axis should be the same.e.g. x ranges from 0 to 10 and it is 10cm on the screen. y has to also range from 0 to 10 and has to be also 10 cm.The square shape has to be maintained, even if I mess around with the window size.Currently, my graph scales together with the window size.How may I achieve this?UPDATE:I tried the following, but it did not work. <code>  plt.xlim(-3, 3)plt.ylim(-3, 3)plt.axis('equal')",How to equalize the scales of x-axis and y-axis in matplotlib
Any suggestions for a simple server for file transfer in Mac?," I share files in a folder to other devices by invoking a server using python -m SimpleHTTPServer. I just tried to stream videos/audio (standard mp4 & mp3, both under 20MB) to another computer using this & it WORKS (but by throwing the errors (listed down) in the terminal). Somehow, the video/audio fails (except very small mp3 files) to play with Safari in iPhone/iPad. It is definitely not related to the media files, as I streamed them successfully using Apache in iPhone. Any idea why it happens?  <code>  Exception happened during processing of request from ('192.168.1.2', 51775)Traceback (most recent call last): File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/SocketServer.py"", line 284, in _handle_request_noblock self.process_request(request, client_address) File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/SocketServer.py"", line 310, in process_request self.finish_request(request, client_address) File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/SocketServer.py"", line 323, in finish_request self.RequestHandlerClass(request, client_address, self) File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/SocketServer.py"", line 641, in __init__ self.finish() File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/SocketServer.py"", line 694, in finish self.wfile.flush() File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/socket.py"", line 303, in flush self._sock.sendall(view[write_offset:write_offset+buffer_size])error: [Errno 32] Broken pipe",Audio/Video streaming fails using SimpleHTTPServer
Can the python interpreter welcome message be suppressed?," Instead of: I'd like, for example: <code>  $ pythonPython 2.7.2 (default, Oct 11 2012, 20:14:37)[GCC 4.2.1 Compatible Apple Clang 4.0 (tags/Apple/clang-418.0.60)] on darwinType ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> $ python --quiet>>>",Can the Python interpreter welcome message be suppressed?
Combining to series into a dataframe in pandas, I have two Series s1 and s2 with the same (non-consecutive) indices. How do I combine s1 and s2 to being two columns in a DataFrame and keep one of the indices as a third column? <code> ,Combining two Series into a DataFrame in pandas
Combining two series into a dataframe in pandas, I have two Series s1 and s2 with the same (non-consecutive) indices. How do I combine s1 and s2 to being two columns in a DataFrame and keep one of the indices as a third column? <code> ,Combining two Series into a DataFrame in pandas
Combining two series's in Padas along the common id, I have two series in pandas.series 1: and series 2: How do I combine the tables along the ids to form the below? <code>  id count_11 33 194 155 56 2 id count_21 33 14 15 26 1 id count_1 count_21 3 33 19 14 15 15 5 26 2 1,Combining two series in pandas along their index
Dispay pydoc's description as part of argparse '--help'," I am using argparse.ArgumentParser() in my script, I would like to display the pydoc description of my script as part of the '--help' option of the argparse.One possibly solution can be to use the formatter_class or the description attribute of ArgumentParser to configure the displaying of help. But in this case, we need to use the 'pydoc' command internally to fetch the description.Do we have some other ways (possibly elegant) to do it? <code> ",Display pydoc's description as part of argparse '--help'
Comapre `float` and `float64` in python," I have to compare two numbers. One of them comes from regulat python code and comes other from numpy. Debugger shows they have same value '29.0', but type of first is float and type of second is float64, so a == b and a - b == 0 is False. How can I deal with it? Is there any way to force a regular python variable to be float64 or numpy use float by default?Update: In the end of all these values comes from the same file where 29.0 is written, so I don't think there are differences in numeric values. <code> ",Compare `float` and `float64` in python
Why if True is slower than if 1?," Why is if True slower than if 1 in Python? Shouldn't if True be faster than if 1?I was trying to learn the timeit module. Starting with the basics, I tried these: I am confused by these things:According to the response from Mr. Sylvain Defresne in this question, everything is implicitly converted to a bool first and then checked. So why is if True slower than if 1?Why is test3 slower than test1 even though only the return values are different?Like Question 2, but why is test4 a little faster than test2?NOTE: I ran timeit three times and took the average of the results, then posted the times here along with the code.This question does not relate to how to do micro benchmarking(which I did in this example but I also understand that it is too basic) but why checking a 'True' variable is slower than a constant. <code>  >>> def test1():... if True:... return 1... else:... return 0>>> print timeit(""test1()"", setup = ""from __main__ import test1"")0.193144083023>>> def test2():... if 1:... return 1... else:... return 0>>> print timeit(""test2()"", setup = ""from __main__ import test2"")0.162086009979>>> def test3():... if True:... return True... else:... return False>>> print timeit(""test3()"", setup = ""from __main__ import test3"")0.214574098587>>> def test4():... if 1:... return True... else:... return False>>> print timeit(""test4()"", setup = ""from __main__ import test4"")0.160849094391",Why is if True slower than if 1?
Receve replys from Gmail with smtplib - Python," Ok, I am working on a type of system so that I can start operations on my computer with sms messages. I can get it to send the initial message: I just need to know how to wait for the reply or pull the reply from Gmail itself, then store it in a variable for later functions. <code>  import smtplib fromAdd = 'GmailFrom' toAdd = 'SMSTo' msg = 'Options \nH - Help \nT - Terminal' username = 'GMail' password = 'Pass' server = smtplib.SMTP('smtp.gmail.com:587') server.starttls() server.login(username , password) server.sendmail(fromAdd , toAdd , msg) server.quit()",Receive replies from Gmail with smtplib - Python
Function to guarantee minimum number of dimensions (ndim) for a numpy.ndarray," There are many situations where slicing operations in 2D arrays produce a 1D array as output, example: There are workarounds like: Is there any numpy built in function that transforms an input array to a given number of dimensions? Like: <code>  a = np.random.random((3,3))# array([[ 0.4986962 , 0.65777899, 0.16798398],# [ 0.02767355, 0.49157946, 0.03178513],# [ 0.60765513, 0.65030948, 0.14786596]])a[0,:]# array([ 0.4986962 , 0.65777899, 0.16798398]) a[0:1,:]# ora[0,:][np.newaxis,:]# array([[ 0.4986962 , 0.65777899, 0.16798398]]) np.minndim(a, ndim=2)",Function that guarantees a minimum number of dimensions (ndim) for a numpy.ndarray
using a loop to remove items (some of whom might be identical) from a list Python," Im writing a simple function to take out any odd numbers from a list and return a list of only the even ones. when applied aboveit returns: [4, 5, 4]why doesnt the second 5 get removed as it alsojustifys the if?Im looking less for a different method to the problem and more to understand why this happens. thanks and sorry if this is stupid q..Joe <code>  def purify(numbers):for i in numbers: if i%2!=0: numbers.remove(i)return numbers print purify([4,5,5,4])",How to filter a list
NetworkX - sort and iterate over edges," I have a directed graph in NetworkX. The edges are weighted from 0 to 1, representing probabilities that they occurred. The network connectivity is quite high, so I want to prune the edges such for every node, only the highest probability node remains.I'm not sure how to iterate over every node and keep only the highest weighted in_edges in the graph. Is there a networkx function that allows us to do this?Here is an example of what I'd like to be able to do. If there are two edges into a node, and they are both of the highest weight, I'd like to keep them both. <code>  Nodes:A, B, C, DEdges:A->B, weight=1.0A->C, weight=1.0A->D, weight=0.5B->C, weight=0.9B->D, weight=0.8C->D, weight=0.9Final Result Wanted:A->B, weight=1.0A->C, weight=1.0C->D, weight=0.9",Find highest weight edge(s) for a given node
NetworkX - find highest weight edge(s) for a given node," I have a directed graph in NetworkX. The edges are weighted from 0 to 1, representing probabilities that they occurred. The network connectivity is quite high, so I want to prune the edges such for every node, only the highest probability node remains.I'm not sure how to iterate over every node and keep only the highest weighted in_edges in the graph. Is there a networkx function that allows us to do this?Here is an example of what I'd like to be able to do. If there are two edges into a node, and they are both of the highest weight, I'd like to keep them both. <code>  Nodes:A, B, C, DEdges:A->B, weight=1.0A->C, weight=1.0A->D, weight=0.5B->C, weight=0.9B->D, weight=0.8C->D, weight=0.9Final Result Wanted:A->B, weight=1.0A->C, weight=1.0C->D, weight=0.9",Find highest weight edge(s) for a given node
python profiling using line_profiler - clever way to remove @profile statements on-the-fly?," I want to use the excellent line_profiler, but only some of the time. To make it work I add before every function call, e.g. and execute But I don't want to have to put the @profile decorators in by hand each time, because most of the time I want to execute the code without them, and I get an exception if I try to include them, e.g. Is there a happy medium where I can dynamically have the decorators removed based on some condition switch/argument, without having to do things manually and/or modify each function too much? <code>  @profile @profiledef myFunc(args): blah return kernprof.py -l -v mycode.py args mycode.py args",Python profiling using line_profiler - clever way to remove @profile statements on-the-fly?
Quick sort with Python," I am totally new to python and I am trying to implement quicksort in it.Could someone please help me complete my code?I do not know how to concatenate the three arrays and printing them.  <code>  def sort(array=[12,4,5,6,7,3,1,15]): less = [] equal = [] greater = [] if len(array) > 1: pivot = array[0] for x in array: if x < pivot: less.append(x) if x == pivot: equal.append(x) if x > pivot: greater.append(x) sort(less) sort(pivot) sort(greater)",Quicksort with Python
how to setup custom middleware in django," I am trying to create middleware to optionally pass a kwarg to every view that meets a condition.The problem is that I cannot find an example of how to set up the middleware. I have seen classes that override the method I want to, process_view: But where do I put this class? Do I create a middleware app and put this class inside of it and then reference it in settings.middleware? <code>  Class CheckConditionMiddleware(object): def process_view(self, request): return None ",How to set up custom middleware in Django
how to use variable with the String in Python," I want to include file name 'main.txt' in the subject for that I am passing file name from command line. but getting error in doing so <code>  python sample.py main.txt #running python with argument msg['Subject'] = ""Auto Hella Restart Report ""sys.argv[1] #line where i am using that passed argument",how to use concatenate a fixed string and a variable in Python
"Python, split tuple items to single stuff"," I have tuple in Python that looks like this: and I wanna split it out so I could get every item from tuple independent so I could do something like this: or something similar to that, My need is to have every item separated. I tried with .split("","") on tuple but I've gotten error which says that tuple doesn't have split option. <code>  tuple = ('sparkbrowser.com', 0, 'http://facebook.com/sparkbrowser', 'Facebook') domain = ""sparkbrowser.com""level = 0url = ""http://facebook.com/sparkbrowser""text = ""Facebook""",Split tuple items to separate variables
How to make Mac OS use the python installed by Hombrew," I have searched online for a while for this question, and what I have done so far is installed python32 in homebrewchanged my .bash_profile and added the following line to it: export PATH=/usr/local/bin:/usr/local/sbin:~/bin:$PATHbut when I close the terminal and start again, I type 'which python', it still prints: /usr/bin/pythonand type 'python --version' still got: Python 2.7.2I also tried the following instruction: brew link --overwrite pythonor try to remove python installed by homebrew by running this instruction: brew remove pythonbut both of the above two instructions lead to this error: Error: No such keg: /usr/local/Cellar/pythoncan anybody help, thanks <code> ",How to make Mac OS use the python installed by Homebrew
Mac set default Python version to 3.3, I'm running Mountain Lion and the basic default Python version is 2.7. I downloaded Python 3.3 and want to set it as default.Currently: How do I set it so that every time I run $ python it opens 3.3? <code>  $ python version 2.7.5$ python3.3 version 3.3,How to set Python's default version to 3.x on OS X?
How to set Python's default version to 3.3 in Mac?, I'm running Mountain Lion and the basic default Python version is 2.7. I downloaded Python 3.3 and want to set it as default.Currently: How do I set it so that every time I run $ python it opens 3.3? <code>  $ python version 2.7.5$ python3.3 version 3.3,How to set Python's default version to 3.x on OS X?
How can I tell OS X to use Python 3.3 by default?, I'm running Mountain Lion and the basic default Python version is 2.7. I downloaded Python 3.3 and want to set it as default.Currently: How do I set it so that every time I run $ python it opens 3.3? <code>  $ python version 2.7.5$ python3.3 version 3.3,How to set Python's default version to 3.x on OS X?
How to set Python's default version to 3.3 on OS X?, I'm running Mountain Lion and the basic default Python version is 2.7. I downloaded Python 3.3 and want to set it as default.Currently: How do I set it so that every time I run $ python it opens 3.3? <code>  $ python version 2.7.5$ python3.3 version 3.3,How to set Python's default version to 3.x on OS X?
How can I print accented characteres in Python?," I'm new to python. I'm trying to print accented characters, like this: But when I execute this code, I get: I'm using 64-bit Windows 7 & Python 2.7.5, I have the code in file.py and execute it with <code>  # -*- coding: utf-8 -*- print '' >> python file.py",Printing accented characters in Python 2.7
"How to call tempfile.mkstemp() with ""with""?"," To me the most idiomatic way of calling tempfile.mkstemp() would be as: however, this obviously(?) raises AttributeError: __exit__Calling os.close(fd) explicitly using try-finally is an easy way to solve this, but feels like violation of There should be one-- and preferably only one --obvious way to do it.Is there a way to ""fix"" this in tempfile or is there a rationale why this has been implemented this way? <code>  with tempfile.mkstemp() as fd, filename: pass","How to call tempfile.mkstemp() with ""with""? - or why doesn't it return an fd with __exit__()?"
access dict_keys element by index python3," I'm trying to access a dict_key's element by its index: I want to get foo.same with: How can I do this? <code>  test = {'foo': 'bar', 'hello': 'world'}keys = test.keys() # dict_keys objectkeys.index(0)AttributeError: 'dict_keys' object has no attribute 'index' keys[0]TypeError: 'dict_keys' object does not support indexing",Accessing dict_keys element by index in Python3
How to access request in Flask MIddleware," I want to access request.url in middleware.Flask app - test.py middleware.py: I understand request can be accessed in Flask application context. We normally use But in middleware, I don't have access to Flask app object.How do I proceed?Thanks for any help.. <code>  from flask import Flaskfrom middleware import TestMiddlewareapp = Flask(__name__)app.wsgi_app = TestMiddleware(app.wsgi_app)@app.route('/')def hello_world(): return 'Hello World!'if __name__ == '__main__': app.run() from flask import requestclass TestMiddleware(object): def __init__(self, app): self.app = app def __call__(self, environ, start_response): # How do I access request object here. print ""I'm in middleware"" return self.app(environ, start_response) with app.test_request_context()",How to access request in Flask Middleware
how to do this in numpy?," I have an array and I would like to produce a smaller array by scanning a 2x2 non-overlappingly windows and getting the maximum. Here is an example: So a matrix like this: Should become this: How can I do this more efficiently? <code>  import numpy as npnp.random.seed(123)np.set_printoptions(linewidth=1000,precision=3)arr = np.random.uniform(-1,1,(4,4))res = np.zeros((2,2))for i in xrange(res.shape[0]): for j in xrange(res.shape[1]): ii = i*2 jj = j*2 res[i][j] = max(arr[ii][jj],arr[ii+1][jj],arr[ii][jj+1],arr[ii+1][jj+1])print arrprint res [[ 0.393 -0.428 -0.546 0.103] [ 0.439 -0.154 0.962 0.37 ] [-0.038 -0.216 -0.314 0.458] [-0.123 -0.881 -0.204 0.476]] [[ 0.439 0.962] [-0.038 0.476]] ",Windowed maximum in numpy
Tkinter: resizing the window swallows the widgets, I have a gui window which I can resize. I want to set min resizing values (and max as well) so that my widgets would not be swallowed when making the window too small or app would not look ugly when resized too much.How do I set min and max resize sizes for my main window? <code> ,How do I set min and max resize sizes for my main window?
the first arguement to execute must be a string or unicode query," I am trying to upload a blob data to ms-sql db, using pyodbc. And I get ""the first argument to execute must be a string or unicode query"" error.The code is The first argument, ObjectID, is sent as a string. I don't see any problem but am I missing something? <code>  file = pyodbc.Binary(open(""some_pdf_file.pdf"", ""r"").read())cur.execute(""INSERT INTO BlobDataForPDF(ObjectID, FileData, Extension) VALUES ('1', "" + file + "", '.PDF')"")cur.commit()",The first argument to execute must be a string or unicode query
the first argument to execute must be a string or unicode query," I am trying to upload a blob data to ms-sql db, using pyodbc. And I get ""the first argument to execute must be a string or unicode query"" error.The code is The first argument, ObjectID, is sent as a string. I don't see any problem but am I missing something? <code>  file = pyodbc.Binary(open(""some_pdf_file.pdf"", ""r"").read())cur.execute(""INSERT INTO BlobDataForPDF(ObjectID, FileData, Extension) VALUES ('1', "" + file + "", '.PDF')"")cur.commit()",The first argument to execute must be a string or unicode query
Python: How to set UTC offset for datetime?," My Python-based web server needs to perform some date manipulation using the client's timezone, represented by its UTC offset. How do I construct a datetime object with the specified UTC offset as timezone? <code> ",How to set UTC offset for datetime?
What is the difference between py[cod] vs .pyc files," What is the difference (if any) between "".pyc"" and "".py[cod]"" notation in ignoring files. I am noticing I have both on my git ignore file. Thanks <code> ","What is the difference between ""py[cod]"" and ""pyc"" in .gitignore notation?"
"Python - Replacing element in list without list comprehension, slicing or using [ ]s"," I'm taking this online Python course and they do not like the students using one-line solutions. The course will not accept brackets for this solution.I already solved the problem using list comprehension, but the course rejected my answer.The problem reads: Using index and other list methods, write a function replace(list, X, Y) which replaces all occurrences of X in list with Y. For example, if L = [3, 1, 4, 1, 5, 9] then replace(L, 1, 7) would change the contents of L to [3, 7, 4, 7, 5, 9]. To make this exercise a challenge, you are not allowed to use []. Note: you don't need to use return.This is what I have so far, but it breaks because of TypeError: 'int' object is not iterable. This was my original answer, but it was rejected. Any ideas on how to fix my longer solution? <code>  list = [3, 1, 4, 1, 5, 9]def replace(list, X, Y): while X in list: for i,v in range(len(list)): if v==1: list.remove(1) list.insert(i, 7)replace(list, 1, 7) list = [3, 1, 4, 1, 5, 9]def replace(list, X, Y): print([Y if v == X else v for v in list])replace(list, 1, 7)","Replacing element in list without list comprehension, slicing or using [ ]s"
Parse the date from the GitHub API in Python, The current value that is returned from a GitHub API request looks like this:2013-09-12T22:42:02ZHow do I parse this value and make it look nicer? <code> ,Parse and format the date from the GitHub API in Python
String concatenation without '+' operator in python, I was playing with python and I realized we don't need to use '+' operator to concatenate static strings. But it fails if I assign it to a variable.For example: Now I have two questions:Why statement 3 does not work while statement 1 does?Is there any technical difference such as calculation speed etc. between statement 1 and 2? <code>  string1 = 'Hello' 'World' #1 works finestring2 = 'Hello' + 'World' #2 also works finestring3 = 'Hello'string4 = 'World'string5 = string3 string4 #3 causes syntax errorstring6 = string3 + string4 #4 works fine,String concatenation without '+' operator
Changing Strings to Floats in a .csv with Python," Quick question for an issue I haven't managed to solve quickly:I'm working with a .csv file and can't seem to find a simple way to convert strings to floats. Here's my code, As you can see, it will currently print the type of every y element in x set of lists in the variable row; this produces a long list of ""<type 'float'>"". But this doesn't actually change each element to a float, nor does setting the for loop to execute float(y) (a type test returns 'string' for each element) work either.I also tried literal_eval, but that failed as well. The only way to change the list elements to floats is to create a new list, either with list comprehension or manually, but that loses the original formatting of each list (as lists of a set amount of elements within one larger list).I suppose the overall question is really just ""What's the easiest way to read, organize, and synthesize data in .csv or excel format using Python?""Thanks in advance to those courteous/knowledgeable enough to help. <code>  import csvdef readLines(): with open('testdata.csv', 'rU') as data: reader = csv.reader(data) row = list(reader) for x in row: for y in x: print type(float(y)),readLines()",Changing strings to floats in an imported .csv
Changing strings to Floats in an imported .csv," Quick question for an issue I haven't managed to solve quickly:I'm working with a .csv file and can't seem to find a simple way to convert strings to floats. Here's my code, As you can see, it will currently print the type of every y element in x set of lists in the variable row; this produces a long list of ""<type 'float'>"". But this doesn't actually change each element to a float, nor does setting the for loop to execute float(y) (a type test returns 'string' for each element) work either.I also tried literal_eval, but that failed as well. The only way to change the list elements to floats is to create a new list, either with list comprehension or manually, but that loses the original formatting of each list (as lists of a set amount of elements within one larger list).I suppose the overall question is really just ""What's the easiest way to read, organize, and synthesize data in .csv or excel format using Python?""Thanks in advance to those courteous/knowledgeable enough to help. <code>  import csvdef readLines(): with open('testdata.csv', 'rU') as data: reader = csv.reader(data) row = list(reader) for x in row: for y in x: print type(float(y)),readLines()",Changing strings to floats in an imported .csv
python - pandas: create dummies from column with multiple values," I am looking for for a pythonic way to handle the following problem.The pandas.get_dummies() method is great to create dummies from a categorical column of a dataframe. For example, if the column has values in ['A', 'B'], get_dummies() creates 2 dummy variables and assigns 0 or 1 accordingly.Now, I need to handle this situation. A single column, let's call it 'label', has values like ['A', 'B', 'C', 'D', 'A*C', 'C*D'] . get_dummies() creates 6 dummies, but I only want 4 of them, so that a row could have multiple 1s. Is there a way to handle this in a pythonic way? I could only think of some step-by-step algorithm to get it, but that would not include get_dummies(). ThanksEdited, hope it is more clear! <code> ",Create dummies from column with multiple values in pandas
Naming convention in Collections, Why the mixture of lowercase and UpperCamelCase? Why collections instead of Collections?I sometimes do this for example: by mistake. What rule of thumb can I use to avoid such mistakes in the future? <code>  namedtupledeque Counter OrderedDictdefaultdict from collections import default_dict,Naming convention in Collections: why are some lowercase and others CapWords?
Python - Finding nullbyte in string," I'm having an issue parsing data after reading a file. What I'm doing is reading a binary file in and need to create a list of attributes from the read file all of the data in the file is terminated with a null byte. What I'm trying to do is find every instance of a null byte terminated attribute.Essentially taking a string like and storing it in a list.The real issue is I need to keep the null bytes in tact, I just need to be able to find each instance of a null byte and store the data that precedes it.  <code>  Health\x00experience\x00charactername\x00",How to find null byte in a string in Python?
Django compressor using gzip," I am trying to serve gzip files from amazon s3.This is my settings.py: When I do this django creates *.gz files for every *.js and *.css compressed but strangely only the *.css files are served as gzip. I can see on the aws s3 that the .css files have the Content-Encoding: gzip and the *.js don't. What is going on here? <code>  AWS_IS_GZIPPED = TrueAWS_PRELOAD_METADATA = True DEFAULT_FILE_STORAGE = 'storages.backends.s3boto.S3BotoStorage'STATICFILES_STORAGE = 'storages.backends.s3boto.S3BotoStorage'AWS_STORAGE_BUCKET_NAME = 'elasticbeanstalk-eu-west-1-2051565523'STATIC_URL = 'https://%s.s3.amazonaws.com/' % AWS_STORAGE_BUCKET_NAMECOMPRESS_OFFLINE = TrueCOMPRESS_ENABLED = TrueCOMPRESS_URL = STATIC_URLCOMPRESS_CSS_FILTERS = [ 'compressor.filters.css_default.CssAbsoluteFilter', 'compressor.filters.cssmin.CSSMinFilter']COMPRESS_JS_FILTERS = [ 'compressor.filters.jsmin.JSMinFilter',]COMPRESS_STORAGE = 'compressor.storage.GzipCompressorFileStorage' ",Django compressor using gzip to serve javascript
Import abitrary python source file. (Python 3.3+)," How can I import an arbitrary python source file (whose filename could contain any characters, and does not always ends with .py) in Python 3.3+?I used imp.load_module as follows: It still works in Python 3.3, but according to imp.load_module documentation, it is deprecated: Deprecated since version 3.3: Unneeded as loaders should be used to load modules and find_module() is deprecated.and imp module documentation recommends to use importlib: Note New programs should use importlib rather than this module.What is the proper way to load an arbitrary python source file in Python 3.3+ without using the deprecated imp.load_module function? <code>  >>> import imp>>> path = '/tmp/a-b.txt'>>> with open(path, 'U') as f:... mod = imp.load_module('a_b', f, path, ('.py', 'U', imp.PY_SOURCE))...>>> mod<module 'a_b' from '/tmp/a-b.txt'>",Import arbitrary python source file. (Python 3.3+)
Using BeautifulSoup to select div blocks withing HTML," I am trying to parse several div blocks using Beautiful Soup using some html from a website. However, I cannot work out which function should be used to select these div blocks. I have tried the following: I want to be able to select everything between <div class=""crBlock ""> and its correct end </div>. (Obviously there are other div tags but I want to select the block all the way down to the one that represents the end of this section of html.) <code>  import urllib2from bs4 import BeautifulSoupdef getData(): html = urllib2.urlopen(""http://www.racingpost.com/horses2/results/home.sd?r_date=2013-09-22"", timeout=10).read().decode('UTF-8') soup = BeautifulSoup(html) print(soup.title) print(soup.find_all('<div class=""crBlock "">'))getData()",Using BeautifulSoup to select div blocks within HTML
"Scrapy, scrapping data inside a javascript"," I am using scrapy to screen scrape data from a website. However, the data I wanted wasn't inside the html itself, instead, it is from a javascript. So, my question is:How to get the values (text values) of such cases? This, is the site I'm trying to screen scrape:https://www.mcdonalds.com.sg/locate-us/Attributes I'm trying to get:Address, Contact, Operating hours.If you do a ""right click"", ""view source"" inside a chrome browser you will see that such values aren't available itself in the HTML.EditSry paul, i did what you told me to, found the admin-ajax.php and saw the body but, I'm really stuck now.How do I retrieve the values from the json object and store it into a variable field of my own? It would be good, if you could share how to do just one attribute for the public and to those who just started scrapy as well.Here's my code so farItems.py McDonalds.py Sry for long edit, so in short, how do i store the json value into my attribute? for eg***item['address'] = * how to retrieve ****P.S, not sure if this helps but, i run these scripts on the cmd line usingscrapy crawl mcdonalds -o McDonalds.json -t json ( to save all my data into a json file )I cannot stress enough on how thankful i feel. I know it's kind of unreasonable to ask this of u, will totally be okay even if you dont have time for this. <code>  class McDonaldsItem(Item):name = Field()address = Field()postal = Field()hours = Field() from scrapy.spider import BaseSpiderfrom scrapy.selector import HtmlXPathSelectorimport refrom fastfood.items import McDonaldsItemclass McDonaldSpider(BaseSpider):name = ""mcdonalds""allowed_domains = [""mcdonalds.com.sg""]start_urls = [""https://www.mcdonalds.com.sg/locate-us/""]def parse_json(self, response): js = json.loads(response.body) pprint.pprint(js)","Scrapy, scraping data inside a Javascript"
"Scrapy, scrapping data inside a Javascript"," I am using scrapy to screen scrape data from a website. However, the data I wanted wasn't inside the html itself, instead, it is from a javascript. So, my question is:How to get the values (text values) of such cases? This, is the site I'm trying to screen scrape:https://www.mcdonalds.com.sg/locate-us/Attributes I'm trying to get:Address, Contact, Operating hours.If you do a ""right click"", ""view source"" inside a chrome browser you will see that such values aren't available itself in the HTML.EditSry paul, i did what you told me to, found the admin-ajax.php and saw the body but, I'm really stuck now.How do I retrieve the values from the json object and store it into a variable field of my own? It would be good, if you could share how to do just one attribute for the public and to those who just started scrapy as well.Here's my code so farItems.py McDonalds.py Sry for long edit, so in short, how do i store the json value into my attribute? for eg***item['address'] = * how to retrieve ****P.S, not sure if this helps but, i run these scripts on the cmd line usingscrapy crawl mcdonalds -o McDonalds.json -t json ( to save all my data into a json file )I cannot stress enough on how thankful i feel. I know it's kind of unreasonable to ask this of u, will totally be okay even if you dont have time for this. <code>  class McDonaldsItem(Item):name = Field()address = Field()postal = Field()hours = Field() from scrapy.spider import BaseSpiderfrom scrapy.selector import HtmlXPathSelectorimport refrom fastfood.items import McDonaldsItemclass McDonaldSpider(BaseSpider):name = ""mcdonalds""allowed_domains = [""mcdonalds.com.sg""]start_urls = [""https://www.mcdonalds.com.sg/locate-us/""]def parse_json(self, response): js = json.loads(response.body) pprint.pprint(js)","Scrapy, scraping data inside a Javascript"
why does the child class does not inherit the method from parent class in python in this example?," I have the following code. I get this error: why is the foo method not inherited in Bar. EDIT: It works fine, if you call super which is commented out. <code>  class Foo(object): def __init__(self): self.__baz = 40 def foo(self): print self.__bazclass Bar(Foo): def __init__(self): #super(Bar, self).__init__() self.__baz = 21 def bar(self): print self.__bazx = Bar()x.foo()x.bar() Traceback (most recent call last): File ""classes.py"", line 15, in <module> x.foo() File ""classes.py"", line 5, in foo print self.__bazAttributeError: 'Bar' object has no attribute '_Foo__baz'",Why does the child class does not inherit the method from parent class in python in this example?
To convert string to variable name in python," I have any string. like 'buffalo', I want to convert this string to some variable name like, not only this example, I want to convert any input string to some variable name. How should I do that (in python)? <code>  x='buffalo' buffalo=4 ",Convert string to variable name in python
To convert string to variable name," I have any string. like 'buffalo', I want to convert this string to some variable name like, not only this example, I want to convert any input string to some variable name. How should I do that (in python)? <code>  x='buffalo' buffalo=4 ",Convert string to variable name in python
Read CSV - Python," I am trying to read a simple CSV file like below, and put its contents in a 2D array: To do this, I use this: But I always got this message: I thought the problem was with the first column of each row. So, I tried to read it without the first column, but I couldn't find out how.So, how could I ignore the first column? Is there a way to read this file with the first column? <code>  """",""x"",""y"",""sim1"",""sim2"",""sim3"",""sim4"",""sim5"",""sim6"",""sim7"",""sim8"",""sim9"",""sim10"",""sim11"",""sim12""""1"",181180,333740,5.56588745117188,6.29487752914429,7.4835410118103,5.75873327255249,6.62183284759521,5.81478500366211,4.85671949386597,5.90418815612793,6.32611751556396,6.99649047851562,6.52076387405396,5.68944215774536""2"",181140,333700,6.36264753341675,6.5217604637146,6.16843748092651,5.55328798294067,7.00429201126099,6.43625402450562,6.17744159698486,6.72836923599243,6.38574266433716,6.81451606750488,6.68060827255249,6.14339065551758""3"",181180,333700,6.16541910171509,6.44704437255859,7.51744651794434,5.46270132064819,6.8890323638916,6.46842670440674,6.07698059082031,6.2140531539917,6.43774271011353,6.21923875808716,6.43355655670166,5.90692138671875 data = np.loadtxt(""Data/sim.csv"", delimiter=',', skiprows=1) ""ValueError: could not convert string to float: ""1""",How to read a CSV without the first column
Removing JSON property in array of objects with Python," I have a JSON array that I'm cleaning up in Python. I want to remove the imageData property:data.json I am setting up a list comprehension to remove the property, but I'm not sure how to create the variable that focuses on imageData: When I print the list comprehension, it returns an empty array. What do I have to correct to get this working properly? <code>  [{""title"": ""foo"", ""imageData"": ""xyz123""},{""title"": ""bar"", ""imageData"": ""abc123""},{""title"": ""baz"", ""imageData"": ""def456""}] import jsonwith open('data.json') as json_data: data = json.load(json_data) clean_data = [ item for item in data if not item['imageData'] ] # Write `clean_data` to new json file",Removing JSON property in array of objects
Python 2.7: Byte Array to Hex String," I have data stored in a byte array. How can I convert this data into a hex string?Example of my byte array: <code>  array_alpha = [ 133, 53, 234, 241 ]",Byte Array to Hex String
python using any() and all() to check if a list contains one set of values or another," My code is for a Tic Tac Toe game and checking for a draw state but I think this question could be more useful in a general sense.I have a list that represents the board, it looks like this: When a player makes a move the int they moved on is replaced with their marker ('x' or 'o'), I already have checks in place to look for a winning state, what I can't do is check for a draw state, where none of the list values are ints but a winning state has not been set.The code I have so far: The if statement works, the elif does not, I think the problem is my 'or' operator, what I want to check for is: if the every item on the board is either playerOne marker or playerTwo marker, if I where to make the code: I would be checking to see if every place on the board was playerOne or every place on the board is playerTwo, which it won't be.So how do I check if the board is taken up by a combination of playerOne markers and playerTwo markers? <code>  board = [1,2,3,4,5,6,7,8,9] if any(board) != playerOne or any(board) != playerTwo: print 'continue'elif all(board) == playerOne or playerTwo: print 'Draw' elif all(board) == playerOne or all(board) == playerTwo:",Using any() and all() to check if a list contains one set of values or another
How string are stored in python memory model," I am from c background and a beginner in python. I want to know how strings are actually stored in memory in case of python.I did something like I did not understand how each character is getting stored in memory and and why id of s is not equal to id of s[0] (like it use to be in c) and why id of s1 and s2 are same? <code>  s=""foo""id(s)=140542718184424id(s[0])= 140542719027040id(s[1])= 140542718832152id(s[2])= 140542718832152",How strings are stored in python memory model
python check that key is defined in dictionary, How to check that the key is defined in dictionary in python? <code>  a={}...if 'a contains key b': a[b] = a[b]+1else a[b]=1,Python check that key is defined in dictionary
python - read whole file at once, I need to read whole source data from file something.zip (not uncompress it)I tried but it returns only few bytes and not whole source data. Any idea how to achieve it? Thanks  <code>  f = open('file.zip')s = f.read()f.close()return s,read whole file at once
Python Sort dict list with itemgetter," Here is my dictionary: I want to sort it by the field ""age"".With lambda I use: How to do this with itemgetter? <code>  a = [{""id"":1,""data"":{""age"":16,""name"":a}}, {""id"":3,""data"":{""age"":35,""name"":b}}, {""id"":2,""data"":{""age"":9,""name"":c}}] sorted(a, key=lambda k: k['data']['age'])",Sort dict list with itemgetter
How to get correct line number where exception was thrown using concurrent.futures (python 2.7)," Example of using concurrent.futures (backport for 2.7): Output: String ""...\_base.py"", line 356, in __get_result"" is not endpoint I expected to see. Is it possible to get real line where exception was thrown? Something like: Python3 seems to show correct line number in this case. Why can't python2.7? And is there any workaround? <code>  import concurrent.futures # line 01def f(x): # line 02 return x * x # line 03data = [1, 2, 3, None, 5] # line 04with concurrent.futures.ThreadPoolExecutor(len(data)) as executor: # line 05 futures = [executor.submit(f, n) for n in data] # line 06 for future in futures: # line 07 print(future.result()) # line 08 149Traceback (most recent call last): File ""C:\test.py"", line 8, in <module> print future.result() # line 08 File ""C:\dev\Python27\lib\site-packages\futures-2.1.4-py2.7.egg\concurrent\futures\_base.py"", line 397, in result return self.__get_result() File ""C:\dev\Python27\lib\site-packages\futures-2.1.4-py2.7.egg\concurrent\futures\_base.py"", line 356, in __get_result raise self._exceptionTypeError: unsupported operand type(s) for *: 'NoneType' and 'NoneType' File ""C:\test.py"", line 3, in f return x * x # line 03",Getting original line number for exception in concurrent.futures
log a variable name and value," I am looking for a way to quickly print a variable name and value while rapidly developing/debugging a small Python script on a Unix command line/ssh session.It seems like a very common requirement and it seems wasteful (on keystrokes and time/energy) to duplicate the variable_names on every line which prints or logs its value. i.e. rather than I want to be able to do the following for str, int, list, dict: and get the following output: Ideally the output would also log the function name.I have seen some solutions attempting to use locals, globals, frames etc., But I have not yet seen something that works for ints, strings, lists, and works inside functions too.Thanks! <code>  print 'my_variable_name:', my_variable_name log(my_variable_name)log(i)log(my_string)log(my_list) my_variable_name:some stringi:10my_string:a string of wordsmy_list:[1, 2, 3]",How to log a variable's name and value?
Difference between sort and sort_index, Python Pandas provides two methods for sorting DataFrame :sort_values (or DEPRECATED sort)sort_indexWhat are differences between these two methods ? <code> ,What is the difference between sort_values and sort_index?
Difference between sort_values and sort_index, Python Pandas provides two methods for sorting DataFrame :sort_values (or DEPRECATED sort)sort_indexWhat are differences between these two methods ? <code> ,What is the difference between sort_values and sort_index?
Python getting name of subclass from superclass," I have a base class bc and a number of subclasses based on bc. The idea is that when somemethod() is invoked on an instance of a subclass of bc, it will be able to use the name of the most derived subclass of that instance without needing to know in advance what potential subclasses may exist.I have put together a test case for this: When this code is run it produces base_class rather than sub_class. <code>  class bc(Object): def get_subclass_name(self): # Retrieve the name of the most derived subclass of this instance pass def somemethod(self): x = self.get_subclass_name()class sc1(bc): passclass sc2(bc) pass class base_class(object): @classmethod def get_subclass_name(cls): return cls.__name__ def somemethod(self): print(base_class.get_subclass_name())class sub_class(base_class): passsub_class().somemethod()",Getting name of subclass from superclass?
"When is semicolon use in Python considered ""good""?"," Python is a ""whitespace delimited"" language. However, the use of semicolons are allowed. For example, the following works but is frowned upon: I've been using python for several years now, and the only time I have ever used semicolons is in generating one-time command-line scripts with python: or adding code in comments on SO (i.e. ""you should try import os; print os.path.join(a,b)"")I also noticed in this answer to a similar question that the semicolon can also be used to make one line if blocks, as in which is convenient for the two usage examples I gave (command-line scripts and comments).The above examples are for communicating code in paragraph form or making short snippets, but not something I would expect in a production codebase. Here is my question: in python, is there ever a reason to use the semicolon in a production code? I imagine that they were added to the language solely for the reasons I have cited, but its always possible that Guido had a grander scheme in mind. No opinions please; I'm looking either for examples from existing code where the semicolon was useful, or some kind of statement from the python docs or from Guido about the use of the semicolon. <code>  print(""Hello!"");print(""This is valid""); python -c ""import inspect, mymodule; print(inspect.getfile(mymodule))"" if x < y < z: print(x); print(y); print(z) ","When is semicolon use in Python considered ""good"" or ""acceptable""?"
"Python PIL, preserve dpi when resizing"," I have a large image, which I resize in PIL so that it is 250 pixels wide. This is the width that it will be shown on my website. However, the resolution is really bad. I see that it has changed the dpi from 180 to 96. If I resize the image in a program like Windows Paint then it maintains the 180 dpi. This Paint-resized image looks a lot better on my website. (The paint-resized image is 40kb while the PIL resized image is 16kb)How do I maintain the dpi (or set it to some max that looks good on websites)I resize this using PIL: I have tried: but it makes no difference. The dpi is 180, but the resolution is still bad. I'm guessing that the dpi needs to be set during resizing?EDIT:The issue seems to be the saving, not the resizing. Even if I start with the Paint-resized image (and therefore do not resize the image in PIL), it still saves it as the crappy quality 96 dpi (16kb) intead of keeping it as it is. <code>  image = image.resize((new_width, new_height), Image.ANTIALIAS)image.save(filepath) dpi = image.info['dpi'] # (180, 180)image.save(filepath, dpi = dpi) ","Python PIL, preserve quality when resizing and saving"
"Python PIL, preserve quality when resizing"," I have a large image, which I resize in PIL so that it is 250 pixels wide. This is the width that it will be shown on my website. However, the resolution is really bad. I see that it has changed the dpi from 180 to 96. If I resize the image in a program like Windows Paint then it maintains the 180 dpi. This Paint-resized image looks a lot better on my website. (The paint-resized image is 40kb while the PIL resized image is 16kb)How do I maintain the dpi (or set it to some max that looks good on websites)I resize this using PIL: I have tried: but it makes no difference. The dpi is 180, but the resolution is still bad. I'm guessing that the dpi needs to be set during resizing?EDIT:The issue seems to be the saving, not the resizing. Even if I start with the Paint-resized image (and therefore do not resize the image in PIL), it still saves it as the crappy quality 96 dpi (16kb) intead of keeping it as it is. <code>  image = image.resize((new_width, new_height), Image.ANTIALIAS)image.save(filepath) dpi = image.info['dpi'] # (180, 180)image.save(filepath, dpi = dpi) ","Python PIL, preserve quality when resizing and saving"
how to render downloadable zip file in django," Im going over the django documentation and I found this piece of code that allows you to render a file as attachment The foo.zip file was created using pythons zipfile.ZipFile().writestr method But when I tried the code above to render the file, Im getting this error 'utf8' codec can't decode byte 0x89 in position 10: invalid start byteAny suggestions on how to do this right? <code>  dl = loader.get_template('files/foo.zip')context = RequestContext(request)response = HttpResponse(dl.render(context), content_type = 'application/force-download')response['Content-Disposition'] = 'attachment; filename=""%s""' % 'foo.zip'return response zip = zipfile.ZipFile('foo.zip', 'a', zipfile.ZIP_DEFLATED)zipinfo = zipfile.ZipInfo('helloworld.txt', date_time=time.localtime(time.time()))zipinfo.create_system = 1zip.writestr(zipinfo, StringIO.StringIO('helloworld').getvalue())zip.close()",how to serve downloadable zip file in django
"Converting JSON String to Dictionary, Not List (Python)"," I am trying to pass in a JSON file and convert the data into a dictionary.So far, this is what I have done: I'm expecting json1_data to be a dict type but it actually comes out as a list type when I check it with type(json1_data). What am I missing? I need this to be a dictionary so I can access one of the keys.  <code>  import jsonjson1_file = open('json1')json1_str = json1_file.read()json1_data = json.loads(json1_str)",Converting JSON String to Dictionary Not List
how to set values to rows of booelean filtered dataframe column," I'm trying to set the values of ""FreeSec"" column to True for the filtered rows of my pandas dataframe. Here is the code: However, when I check the values they are still set to False. What am I missing here? <code>  data[data[""Brand""].isin(group_clients)].FreeSec = True >>> data[data[""Brand""].isin(group_clients)].FreeSec12 False163 False164 False165 False166 False167 False168 False169 False",how to set values to rows of boolean filtered dataframe column
Flask Mega Tutorial - jinja2.exceptions.UndefinedError: 'form' is undefined," I am working through Miguel Grinberg's Flask Mega Tutorial and I cannot figure out why the index page now fails to load. Here is the traceback: Here is the code for the index page in question: And here is the code in the view: I've read through the tutorial multiple times, and compared my code to the source at the end of each lesson, and I have no idea why it is not working. I'm not sure why it's having trouble passing form in this view when it does not return an error passing forms in another view.Can anyone point me in the right direction? <code>  File ""/home/asdoylejr/microblog/flask/lib/python2.7/site-packages/flask/app.py"", line 1836, in __call__ return self.wsgi_app(environ, start_response) File ""/home/asdoylejr/microblog/flask/lib/python2.7/site-packages/flask/app.py"", line 1820, in wsgi_app response = self.make_response(self.handle_exception(e)) File ""/home/asdoylejr/microblog/flask/lib/python2.7/site-packages/flask/app.py"", line 1403, in handle_exception reraise(exc_type, exc_value, tb) File ""/home/asdoylejr/microblog/flask/lib/python2.7/site-packages/flask/app.py"", line 1817, in wsgi_app response = self.full_dispatch_request() File ""/home/asdoylejr/microblog/flask/lib/python2.7/site-packages/flask/app.py"", line 1477, in full_dispatch_request rv = self.handle_user_exception(e) File ""/home/asdoylejr/microblog/flask/lib/python2.7/site-packages/flask/app.py"", line 1381, in handle_user_exception reraise(exc_type, exc_value, tb) File ""/home/asdoylejr/microblog/flask/lib/python2.7/site-packages/flask/app.py"", line 1475, in full_dispatch_request rv = self.dispatch_request() File ""/home/asdoylejr/microblog/flask/lib/python2.7/site-packages/flask/app.py"", line 1461, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File ""/home/asdoylejr/microblog/flask/lib/python2.7/site-packages/flask_login.py"", line 658, in decorated_view return func(*args, **kwargs) File ""/home/asdoylejr/microblog/app/views.py"", line 44, in index posts = posts) File ""/home/asdoylejr/microblog/flask/lib/python2.7/site-packages/flask/templating.py"", line 128, in render_template context, ctx.app) File ""/home/asdoylejr/microblog/flask/lib/python2.7/site-packages/flask/templating.py"", line 110, in _render rv = template.render(context) File ""/home/asdoylejr/microblog/flask/lib/python2.7/site-packages/jinja2/environment.py"", line 969, in render return self.environment.handle_exception(exc_info, True) File ""/home/asdoylejr/microblog/flask/lib/python2.7/site-packages/jinja2/environment.py"", line 742, in handle_exception reraise(exc_type, exc_value, tb) File ""/home/asdoylejr/microblog/app/templates/index.html"", line 2, in top-level template code {% extends ""base.html"" %} File ""/home/asdoylejr/microblog/app/templates/base.html"", line 30, in top-level template code {% block content %}{% endblock %} File ""/home/asdoylejr/microblog/app/templates/index.html"", line 7, in block ""content"" {{form.hidden_tag()}} File ""/home/asdoylejr/microblog/flask/lib/python2.7/site-packages/jinja2/environment.py"", line 397, in getattr return getattr(obj, attribute)UndefinedError: 'form' is undefined <!-- extend base layout -->{% extends ""base.html"" %}{% block content %}<h1>Hi, {{g.user.nickname}}!</h1><form action="""" method=""post"" name=""post""> {{form.hidden_tag()}}<table> <tr> <td>Say something:</td> <td>{{form.post(size = 30, maxlength = 140)}}</td> <td> {% for error in form.errors.post %} <span style=""color: red;"">[{{error}}]</span><br> {% endfor %} </td> </tr> <tr> <td></td> <td><input type=""submit"" value=""Post!""></td> <td></td> </tr></table></form>{% for post in posts %}<p> {{post.author.nickname}} says: <b>{{post.body}}</b></p>{% endfor %}{% endblock %} @app.route('/', methods = ['GET', 'POST'])@app.route('/index', methods = ['GET', 'POST'])@login_requireddef index(): form = PostForm() if form.validate_on_submit(): post = Post(body = form.post.data, timestamp = datetime.utcnow(), author = g.user) db.session.add(post) db.session.commit() flash('Your post is now live!') return redirect(url_for('index')) posts = g.user.followed_posts().all() return render_template(""index.html"", title = 'Home', user = user, posts = posts)","Rendering template gives ""jinja2.exceptions.UndefinedError: 'form' is undefined"""
Choose adapter depending on libraries installed," I am designing a library that has adapters that supports a wide-range of libraries. I want the library to dynamically choose which ever adapter that has the library it uses installed on the machine when importing specific classes.The goal is to be able to change the library that the program depends on without having to make modifications to the code. This particular feature is for handling RabbitMQ connections, as we have had a lot of problems with pika, we want to be able to change to a different library e.g. pyAMPQ or rabbitpy without having to change the underlying code. I was thinking of implementing something like this in the __init__.py file of servicelibrary.simple. Then when the user imports the library The underlying layer looks something like thisalternative.py synchronous.py This would automatically pick the second one when the first one is not installed.Is there a better way of implementing something like this? If anyone could link a library/adapter with a similar implementation that would be helpful as well.[Edit] What would be the cleanest way to implement something like this? In the future I would also like to be able to change the default preference. Ultimately I may just settle for using the library installed, as I can control that, but it would be a nice feature to have. Alexanders suggestion is interesting, but I would like to know if there is a cleaner way.[Edit2]The original example was simplified. Each module may contain multiple types of imports, e.g. Consumer and Publisher. <code>  try: #import pika # Is pika installed? from servicelibrary.simple.synchronous import Publisher from servicelibrary.simple.synchronous import Consumerexcept ImportError: #import ampq # Is ampq installed? from servicelibrary.simple.alternative import Publisher from servicelibrary.simple.alternative import Consumer from servicelibrary.simple import Publisher import amqpclass Publisher(object): ......class Consumer(object): ...... import pikaclass Publisher(object): ......class Consumer(object): ...... ",Choose adapter dynamically depending on librarie(s) installed
Split a multidim numpy array using a condition," I have a multidimensional numpy array.The first array indicates the quality of the data. 0 is good, 1 is not so good.For a first check I only want to use good data.How do I split the array into two new ones?My own idea does not work: Here is a small example indicating my problem: The print statement gives me [1., 1., 1.].But I am looking for [[1., 1., 1.], [300., 222., 333.], [900., 900., 900.]]. <code>  good_data = [x for x in data[0,:] if x = 1.0]bad_data = [x for x in data[0,:] if x = 0.0] import numpy as npflag = np.array([0., 0., 0., 1., 1., 1.])temp = np.array([300., 310., 320., 300., 222., 333.])pressure = np.array([1013., 1013., 1013., 900., 900., 900.])data = np.array([flag, temp, pressure])good_data = data[0,:][data[0,:] == 1.0]bad_data = data[0,:][data[0,:] == 0.0]print good_data",Split a multidimensional numpy array using a condition
"Python set ""hide"" attribute on folders in windows OS"," Trying to hide folder without success. I've found this : but it did not work for me. What am I doing wrong? <code>  import ctypesctypes.windll.kernel32.SetFileAttributesW('G:\Dir\folder1', 2)","Set ""hide"" attribute on folders in windows OS?"
Python: list with tuples sort by multipe conditions," I am currently trying to sort the following list: These are the steps I want to take in order to sort it:Sort the list by the value of the first element of the tuplesNext, sort the list by the length of the second element of the tuples (not the value, the length!) AFTER step 1 finishes.Next, sort the list by the value of the second element of the tuples AFTER step 1 and step 2 finishes.My attempt: However, I received a syntax error concerning the x after key= len.What is the right variable I should be using in this case?The correct, sorted list should be: Thank you for help. <code>  list_ = [(1, '0101'), (1, '1010'), (1, '101'), (2, '01'), (2, '010'), (2, '10')] sorted_by_length = sorted(list_, key=len x:x[1]) sorted_by_length = [(1, '101'), (1, '0101'), (1, '1010'), (2, '01'), (2, '10'), (2, '010')]",Sorting a list of tuples with multiple conditions
Sort list with tuples with multiple conditions in Python," I am currently trying to sort the following list: These are the steps I want to take in order to sort it:Sort the list by the value of the first element of the tuplesNext, sort the list by the length of the second element of the tuples (not the value, the length!) AFTER step 1 finishes.Next, sort the list by the value of the second element of the tuples AFTER step 1 and step 2 finishes.My attempt: However, I received a syntax error concerning the x after key= len.What is the right variable I should be using in this case?The correct, sorted list should be: Thank you for help. <code>  list_ = [(1, '0101'), (1, '1010'), (1, '101'), (2, '01'), (2, '010'), (2, '10')] sorted_by_length = sorted(list_, key=len x:x[1]) sorted_by_length = [(1, '101'), (1, '0101'), (1, '1010'), (2, '01'), (2, '10'), (2, '010')]",Sorting a list of tuples with multiple conditions
Python UTC datetime object's ISO format dont include Z (Zulu or Zero offset), Why python 2.7 doesn't include Z character (Zulu or zero offset) at the end of UTC datetime object's isoformat string unlike JavaScript? Whereas in javascript <code>  >>> datetime.datetime.utcnow().isoformat()'2013-10-29T09:14:03.895210' >>> console.log(new Date().toISOString()); 2013-10-29T09:38:41.341Z,Python UTC datetime object's ISO format doesn't include Z (Zulu or Zero offset)
"Is python's list ""in"" operator thread-safe?"," Is obj in a_list thread-safe while a_list might be modified in a different thread?Here's a comprehensive yet non-exhaustive list of examples of list operations and whether or not they are thread safe, however I couldn't find any reference for the in language construct.In terms of python implementation, I use CPython, but answers re other implementations would be helpful too for the community. <code> ","Is python's ""in"" language construct thread-safe for lists?"
IPyhton: Manipulate-like command," In Wolfram Mathematica, I can interactively modify the value of a parameter by using the Manipulate[] command.For example, Manipulate[n, {n, 1, 20}]shows a slider through which is possible to vary the value of n.Is there any simple way (i.e. something like a magic or a decorator, like in SAGE) to achieve the same result in the IPython notebook? <code> ",iPython: Manipulate-like command
what's the maximum number of repetitions allowed in a Python regular expression?," In Python 2.7 and 3, the following works: but this gives an error: It seems like there is an upper limit on the number of repetitions allowed. Is this part of the regular expression specification, or a Python-specific limitation? If Python-specific, is the actual number documented somewhere, and does it vary between implementations? <code>  >>> re.search(r""a{1,9999}"", 'aaa')<_sre.SRE_Match object at 0x1f5d100> >>> re.search(r""a{1,99999}"", 'aaa') Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/usr/lib/python2.7/re.py"", line 142, in search return _compile(pattern, flags).search(string) File ""/usr/lib/python2.7/re.py"", line 240, in _compile p = sre_compile.compile(pattern, flags) File ""/usr/lib/python2.7/sre_compile.py"", line 523, in compile groupindex, indexgroupRuntimeError: invalid SRE code",What's the maximum number of repetitions allowed in a Python regex?
Save part of a Regex pattern for latter," Consider this (very simplified) example string: As you can see, it is two digit/letter/letter/digit values separated by a comma.Now, I could match this with the following: The problem is though, I have to write \d\w\w\d twice. With small patterns, this isn't so bad but, with more complex Regexes, writing the exact same thing twice makes the end pattern enormous and cumbersome to work with. It also seems redundant.I tried using a named capture group: But it didn't work because it was looking for two occurrences of 1aw2, not digit/letter/letter/digit.Is there any way to save part of a pattern, such as \d\w\w\d, so it can be used latter on in the same pattern? In other words, can I reuse a sub-pattern in a pattern? <code>  1aw2,5cx7 >>> from re import match>>> match(""\d\w\w\d,\d\w\w\d"", ""1aw2,5cx7"")<_sre.SRE_Match object at 0x01749D40>>>> >>> from re import match>>> match(""(?P<id>\d\w\w\d),(?P=id)"", ""1aw2,5cx7"")>>>",Reuse part of a Regex pattern
How to merge 2 dictionaries with same key names," I am new to Python and am trying to write a function that will merge two dictionary objects in python.For instance I need to produce a new merged dictionary Function should also take a parameter conflict (set to True or False). When conflict is set to False, above is fine. When conflict is set to True, code will merge the dictionary like this instead: I am trying to append the 2 dictionaries, but not sure how to do it the right way. <code>  dict1 = {'a':[1], 'b':[2]}dict2 = {'b':[3], 'c':[4]} dict3 = {'a':[1], 'b':[2,3], 'c':[4]} dict3 = {'a':[1], 'b_1':[2], 'b_2':[3], 'c':[4]} for key in dict1.keys(): if dict2.has_key(key): dict2[key].append(dict1[key])",How to merge two dictionaries with same key names
Print string in middle of plotted line (mimic contour plot labels)," The contour plot demo shows how you can plot the curves with the level value plotted over them, see below.Is there a way to do this same thing for a simple line plot like the one obtained with the code below? <code>  import matplotlib.pyplot as plt x = [1.81,1.715,1.78,1.613,1.629,1.714,1.62,1.738,1.495,1.669,1.57,1.877,1.385]y = [0.924,0.915,0.914,0.91,0.909,0.905,0.905,0.893,0.886,0.881,0.873,0.873,0.844]# This is the string that should show somewhere over the plotted line.line_string = 'name of line'# plottingplt.plot(x,y)plt.show()",Print string over plotted line (mimic contour plot labels)
Efficient way to shift 2D-matrixes in both directions," Given a two dimensional matrix, e.g. What is the most efficient way of implementing a shift operation on columns and rows?E.g. but I'm using collections.deque on both depths because of this answer but while a 'up' or 'down' only requires 1 shift, a 'left' or 'right' requires N shifts (my implementation is using a for cycle for each row).In C I think this can be improved using pointer arithmetic (see e.g. this answer).Is there a better pythonic way?EDIT:By efficient I mean if there is a way of avoiding the N shifts.We can assume the matrix is squared.The shift can be in place.Thanks to martineau for pointing out these important points of the question.I'm sorry I didn't pointed them out before. <code>  l = [[1,1,1], [2,5,2], [3,3,3]]) shift('up', l) [[2, 5, 2], [3, 3, 3], [1, 1, 1]] shift('left', l) [[1, 1, 1], [5, 2, 2], [3, 3, 3]]",Efficient way to shift 2D-matrices in both directions?
Efficient way to shift 2D-matrixes in python in both directions," Given a two dimensional matrix, e.g. What is the most efficient way of implementing a shift operation on columns and rows?E.g. but I'm using collections.deque on both depths because of this answer but while a 'up' or 'down' only requires 1 shift, a 'left' or 'right' requires N shifts (my implementation is using a for cycle for each row).In C I think this can be improved using pointer arithmetic (see e.g. this answer).Is there a better pythonic way?EDIT:By efficient I mean if there is a way of avoiding the N shifts.We can assume the matrix is squared.The shift can be in place.Thanks to martineau for pointing out these important points of the question.I'm sorry I didn't pointed them out before. <code>  l = [[1,1,1], [2,5,2], [3,3,3]]) shift('up', l) [[2, 5, 2], [3, 3, 3], [1, 1, 1]] shift('left', l) [[1, 1, 1], [5, 2, 2], [3, 3, 3]]",Efficient way to shift 2D-matrices in both directions?
Pandas: A clean way to initialize data frame with a list of namedtuple," I'm new to pandas, therefore perhaps I'm asking a very stupid question. Normally initialization of data frame in pandas would be column-wise, where I put in dict with key of column names and values of list-like object with same length.But I would love to initialize row-wise without dynamically concat-ing rows. Say I have a list of namedtuple, is there a optimized operation that will give me a pandas data frame directly from it? <code> ",Pandas: create dataframe from list of namedtuple
Read file data without saving it," I am writing my first flask application. I am dealing with file uploads, and basically what I want is to read the data/content of the uploaded file without saving it and then print it on the resulting page. Yes, I am assuming that the user uploads a text file always.Here is the simple upload function i am using: Right now, I am saving the file, but what I need is that 'a' variable to contain the content/data of the file .. any ideas? <code>  @app.route('/upload/', methods=['GET', 'POST'])def upload(): if request.method == 'POST': file = request.files['file'] if file: filename = secure_filename(file.filename) file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename)) a = 'file uploaded' return render_template('upload.html', data = a)",Read file data without saving it in Flask
Use `as` in `if` statement," Is it possible to use as in if statement like with that we use, for example: This is my code: Can I use if in this type: In first if I called my_list four times. I can use a variable but I want to know is there any way to use as? <code>  with open(""/tmp/foo"", ""r"") as ofile: # do_something_with_ofile def my_list(rtrn_lst=True): if rtrn_lst: return [12, 14, 15] return []if my_list(): print(my_list()[2] * mylist()[0] / mylist()[1]) if my_list() as lst: print(lst[2] * lst[0] / lst[1])","Can I use the ""as"" mechanism in an if statement"
How to export data in pythn with exel format?," views.py urls.py Last, in your page create a button or link that will point in exporting.page.html Nothing getting file option for download and not giving any error but i can see all result in log its working fine. <code>  def export_to_excel(request): lists = MyModel.objects.all() # your excel html format template_name = ""sample_excel_format.html"" response = render_to_response(template_name, {'lists': lists}) # this is the output file filename = ""model.csv"" response['Content-Disposition'] = 'attachment; filename='+filename response['Content-Type'] = 'application/vnd.ms-excel; charset=utf-16' return response from django.conf.urls.defaults import *urlpatterns = patterns('app_name.views', url(r'^export/$', 'export_to_excel', name='export_to_excel'), ) <a href=""{% url app_name:export_to_excel %}"">Export</a>",How to export data in python with excel format?
IPython/matplotlib: Return subplot from function," Using Matplotlib in an IPython Notebook, I would like to create a figure with subplots which are returned from a function: I know that I can directly add a plot to an axis: But how can I return a plot from a function and use it as a subplot? <code>  import matplotlib.pyplot as plt%matplotlib inlinedef create_subplot(data): more_data = do_something_on_data() bp = plt.boxplot(more_data) # return boxplot? return bp# make figure with subplotsf, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10,5))ax1 -> how can I get the plot from create_subplot() and put it on ax1?ax1 -> how can I get the plot from create_subplot() and put it on ax2? ax1.boxplot(data)",Return a subplot from a function
What is the time complexity of sum in Python?, What is the time complexity of the sum() function? <code> ,What is the time complexity of sum() in Python?
Get current celery task id anywhere in the thred," I'd like to get the task id inside a running task,without knowing which task I'm in.(That's why I can't use https://stackoverflow.com/a/8096086/245024)I'd like it to be something like this: This pattern returns in many different tasks, and I don't want to carry the task context to every inner method call.One option could be to use the thread local storage, but then I will need to initialize it before the task starts, and clean it after it finished.Is there something simpler? <code>  @taskdef my_task(): foo()def foo(): logger.log(current_task_id)",Get current celery task id anywhere in the thread
python count all possible combinations for a table," I have a table that looks like below: From here, I have to find all the possible combinations of ""+"",""-"" and ""?"" for all combinations of (PotA and PotB), (PotA and PotC), and so on, (PotA, PotB and PotC), and finally to (PotA, PotB, PotC, PotD and PotE). Actually the ""Pot"" row keeps going on, but here I only show till PotE for simplification.To do this, first, I read the file like as follows, and then, generate all possible possibilities for a combination of two for count each possibility. So, the end result would be like this: Is there any good python method to get the proper logic for this problem?Do I have to read the data with the headers as keys and columns as value of a list?I cannot get a proper logic. Please give me some help. <code>  PotA PotB PotC PotD PotEA + + + + +B - ? + + ?C + + + + +D + - + - +E + + + + + def readDatafile(): filename = (""data.txt"") infile = open(filename,'r') for line in infile.readlines(): line = line.strip() print (line) # just to check the data, need to carry on from here.""""""Generate all possible permutations for later count""""""def doPermutations(items, n): if n == 0: yield '' else: for i in range(len(items)): for base in doPermutations(items, n - 1): yield str(items[i]) + str(base)def makeAllPossibleList(): nlength = 2 # This should go outside of the function and will be same as the number of Pots lpossibility = ['+', '-', '?'] litems = [] for i in doPermutations(lpossibility, int(nlength)): litems.append(i) for x in items: print (x) # This generate all the possible items for combination of two Combination: Possibility CountPotA, PotB: ++ 3PotA, PotB: +- 1PotA, PotB: +? 0PotA, PotB: -+ 0PotA, PotB: -- 0PotA, PotB: -? 1PotA, PotB: ?+ 0PotA, PotB: ?- 0PotA, PotB: ?? 0PotA, PotC: ...PotA, PotC: ..........PotA, PotB, PotC, PotD, PotE: +++++ 3PotA, PotB, PotC, PotD, PotE: ++++- 0PotA, PotB, PotC, PotD, PotE: ++++? 0.......",Python count all possible combinations for a table
Cherrypy: 500 ValueError: Page handlers MUST return bytes," I'm getting the following error from my cherrypy script that is being generated by the submit module. ValueError: Page handlers MUST return bytes. Use tools.encode if you wish to return unicodeI turned tool.encode on in my config but I'm still getting this error. I'm allowing users to upload content via the jQuery Form Plugin. Anythoughts as to why I'm getting this error?Here is my cherrypy file: HTML: jQuery (frontend.js): <code>  class Root(object): @cherrypy.exposedef index(self) return open('/home/joestox/webapps/freelinreg_static/index.html')@cherrypy.exposedef submit(self, myfile): cherrypy.session['myfile'] = myfile data_name = myfile.filename #Send back to JQuery with Ajax #Put in JSON form data_name= json.dumps(dict(title = data_name)) cherrypy.response.headers['Content-Type'] = 'application/json' return data_namecherrypy.config.update({ 'tools.staticdir.debug': True, 'log.screen': True, 'server.socket_host': '127.0.0.1', 'server.socket_port': *****, 'tools.sessions.on': True, 'tools.encode.on': True, 'tools.encode.encoding': 'utf-8',})config = {}cherrypy.tree.mount(Root(), '/', config=config)cherrypy.engine.start() <!DOCTYPE html> <html> <head> <script type='text/javascript' src='freelinreg_static/google.js'></script> <script type='text/javascript' src='freelinreg_static/frontend.js'></script> <script type='text/javascript' src='freelinreg_static/malsup.js'></script> </head>",Python Cherrypy: 500 ValueError: Page handlers MUST return bytes
Using sklearn.RFECV in python," I was trying to narrow down the number of features really relevant for my classifier using rfecv. This is the code I have written At call of ""rfecv.fit(X,Y)"" my code throws an error from the metrices.py file ""ValueError: unknown is not supported""The error sprouts in sklearn.metrics.metrics: This is a classification problem, target values only 0 or 1.The data set can be found at Kaggle Competition DataIf anyone can point out where I am going wrong, I would appreciate it. <code>  import sklearnimport pandas as pimport numpy as npimport scipy as spimport pylab as plfrom sklearn import linear_model, cross_validation, metricsfrom sklearn.svm import SVCfrom sklearn.feature_selection import RFECVfrom sklearn.metrics import zero_one_lossfrom sklearn import preprocessing#from sklearn.feature_extraction.text import CountVectorizer#from sklearn.feature_selection import SelectKBest, chi2modelType = ""notext""# ----------------------------------------------------------# Prepare the Data# ----------------------------------------------------------training_data = np.array(p.read_table('F:/NYC/NYU/SM/3/SNLP/Project/Data/train.tsv'))print (""Read Data\n"")# get the target variable and set it as Y so we can predict itY = training_data[:,-1]print(Y)# not all data is numerical, so we'll have to convert those fields# fix ""is_news"":training_data[:,17] = [0 if x == ""?"" else 1 for x in training_data[:,17]]# fix -1 entries in hasDomainLinktraining_data[:,14] = [0 if x ==""-1"" else x for x in training_data[:,10]]# fix ""news_front_page"":training_data[:,20] = [999 if x == ""?"" else x for x in training_data[:,20]]training_data[:,20] = [1 if x == ""1"" else x for x in training_data[:,20]]training_data[:,20] = [0 if x == ""0"" else x for x in training_data[:,20]]# fix ""alchemy category"":training_data[:,3] = [0 if x==""arts_entertainment"" else x for x in training_data[:,3]]training_data[:,3] = [1 if x==""business"" else x for x in training_data[:,3]]training_data[:,3] = [2 if x==""computer_internet"" else x for x in training_data[:,3]]training_data[:,3] = [3 if x==""culture_politics"" else x for x in training_data[:,3]]training_data[:,3] = [4 if x==""gaming"" else x for x in training_data[:,3]]training_data[:,3] = [5 if x==""health"" else x for x in training_data[:,3]]training_data[:,3] = [6 if x==""law_crime"" else x for x in training_data[:,3]]training_data[:,3] = [7 if x==""recreation"" else x for x in training_data[:,3]]training_data[:,3] = [8 if x==""religion"" else x for x in training_data[:,3]]training_data[:,3] = [9 if x==""science_technology"" else x for x in training_data[:,3]]training_data[:,3] = [10 if x==""sports"" else x for x in training_data[:,3]]training_data[:,3] = [11 if x==""unknown"" else x for x in training_data[:,3]]training_data[:,3] = [12 if x==""weather"" else x for x in training_data[:,3]]training_data[:,3] = [999 if x==""?"" else x for x in training_data[:,3]]print (""Corrected outliers data\n"")# ----------------------------------------------------------# Models# ----------------------------------------------------------if modelType == ""notext"": print (""no text model\n"") #ignore features which are useless X = training_data[:,list([3, 5, 6, 7, 8, 9, 10, 14, 15, 16, 17, 19, 20, 22, 25])] scaler = preprocessing.StandardScaler() print(""initialized scaler \n"") scaler.fit(X,Y) print(""fitted train data and labels\n"") X = scaler.transform(X) print(""Transformed train data\n"") svc = SVC(kernel = ""linear"") print(""Initialized SVM\n"") rfecv = RFECV(estimator = svc, cv = 5, loss_func = zero_one_loss, verbose = 1) print(""Initialized RFECV\n"") rfecv.fit(X,Y) print(""Fitted train data and label\n"") rfecv.support_ print (""Optimal Number of features : %d"" % rfecv.n_features_) savetxt('rfecv.csv', rfecv.ranking_, delimiter=',', fmt='%f') # No metrics support ""multiclass-multioutput"" format if (y_type not in [""binary"", ""multiclass"", ""multilabel-indicator"", ""multilabel-sequences""]): raise ValueError(""{0} is not supported"".format(y_type))",ValueError: unknown is not supported in sklearn.RFECV
Error using sklearn.RFECV in python," I was trying to narrow down the number of features really relevant for my classifier using rfecv. This is the code I have written At call of ""rfecv.fit(X,Y)"" my code throws an error from the metrices.py file ""ValueError: unknown is not supported""The error sprouts in sklearn.metrics.metrics: This is a classification problem, target values only 0 or 1.The data set can be found at Kaggle Competition DataIf anyone can point out where I am going wrong, I would appreciate it. <code>  import sklearnimport pandas as pimport numpy as npimport scipy as spimport pylab as plfrom sklearn import linear_model, cross_validation, metricsfrom sklearn.svm import SVCfrom sklearn.feature_selection import RFECVfrom sklearn.metrics import zero_one_lossfrom sklearn import preprocessing#from sklearn.feature_extraction.text import CountVectorizer#from sklearn.feature_selection import SelectKBest, chi2modelType = ""notext""# ----------------------------------------------------------# Prepare the Data# ----------------------------------------------------------training_data = np.array(p.read_table('F:/NYC/NYU/SM/3/SNLP/Project/Data/train.tsv'))print (""Read Data\n"")# get the target variable and set it as Y so we can predict itY = training_data[:,-1]print(Y)# not all data is numerical, so we'll have to convert those fields# fix ""is_news"":training_data[:,17] = [0 if x == ""?"" else 1 for x in training_data[:,17]]# fix -1 entries in hasDomainLinktraining_data[:,14] = [0 if x ==""-1"" else x for x in training_data[:,10]]# fix ""news_front_page"":training_data[:,20] = [999 if x == ""?"" else x for x in training_data[:,20]]training_data[:,20] = [1 if x == ""1"" else x for x in training_data[:,20]]training_data[:,20] = [0 if x == ""0"" else x for x in training_data[:,20]]# fix ""alchemy category"":training_data[:,3] = [0 if x==""arts_entertainment"" else x for x in training_data[:,3]]training_data[:,3] = [1 if x==""business"" else x for x in training_data[:,3]]training_data[:,3] = [2 if x==""computer_internet"" else x for x in training_data[:,3]]training_data[:,3] = [3 if x==""culture_politics"" else x for x in training_data[:,3]]training_data[:,3] = [4 if x==""gaming"" else x for x in training_data[:,3]]training_data[:,3] = [5 if x==""health"" else x for x in training_data[:,3]]training_data[:,3] = [6 if x==""law_crime"" else x for x in training_data[:,3]]training_data[:,3] = [7 if x==""recreation"" else x for x in training_data[:,3]]training_data[:,3] = [8 if x==""religion"" else x for x in training_data[:,3]]training_data[:,3] = [9 if x==""science_technology"" else x for x in training_data[:,3]]training_data[:,3] = [10 if x==""sports"" else x for x in training_data[:,3]]training_data[:,3] = [11 if x==""unknown"" else x for x in training_data[:,3]]training_data[:,3] = [12 if x==""weather"" else x for x in training_data[:,3]]training_data[:,3] = [999 if x==""?"" else x for x in training_data[:,3]]print (""Corrected outliers data\n"")# ----------------------------------------------------------# Models# ----------------------------------------------------------if modelType == ""notext"": print (""no text model\n"") #ignore features which are useless X = training_data[:,list([3, 5, 6, 7, 8, 9, 10, 14, 15, 16, 17, 19, 20, 22, 25])] scaler = preprocessing.StandardScaler() print(""initialized scaler \n"") scaler.fit(X,Y) print(""fitted train data and labels\n"") X = scaler.transform(X) print(""Transformed train data\n"") svc = SVC(kernel = ""linear"") print(""Initialized SVM\n"") rfecv = RFECV(estimator = svc, cv = 5, loss_func = zero_one_loss, verbose = 1) print(""Initialized RFECV\n"") rfecv.fit(X,Y) print(""Fitted train data and label\n"") rfecv.support_ print (""Optimal Number of features : %d"" % rfecv.n_features_) savetxt('rfecv.csv', rfecv.ranking_, delimiter=',', fmt='%f') # No metrics support ""multiclass-multioutput"" format if (y_type not in [""binary"", ""multiclass"", ""multilabel-indicator"", ""multilabel-sequences""]): raise ValueError(""{0} is not supported"".format(y_type))",ValueError: unknown is not supported in sklearn.RFECV
Printing a tree datastructure in Python," I was looking for a possible implementation of tree printing, which prints the tree in a user-friendly way, and not as an instance of object.I came across this solution on the net:source: http://cbio.ufs.ac.za/live_docs/nbn_tut/trees.html This code prints the tree in the following way: Is it possible to have the same result but without changing the __repr__ method, because I am using it for another purpose.EDIT:Solution without modifying __repr__ and __str__ <code>  class node(object): def __init__(self, value, children = []): self.value = value self.children = children def __repr__(self, level=0): ret = ""\t""*level+repr(self.value)+""\n"" for child in self.children: ret += child.__repr__(level+1) return ret 'grandmother' 'daughter' 'granddaughter' 'grandson' 'son' 'granddaughter' 'grandson' def other_name(self, level=0): print '\t' * level + repr(self.value) for child in self.children: child.other_name(level+1)",Printing a Tree data structure in Python
Doubt regarding description of ppoints function in R," The R ppoints function is described as: I've been trying to replicate this function in python and I have a couple of doubts.1- The first m in (1:m - a)/(m + (1-a)-a) is always an integer: int(n) (ie: the integer of n) if length(n)==1 and length(n) otherwise.2- The second m in the same equation is NOT an integer if length(n)==1 (it assumes the real value of n) and it IS an integer (length(n)) otherwise.3- The n in a = ifelse(n <= 10, 3/8, 1/2) is the real number n if length(n)==1 and the integer length(n) otherwise.This points are not made clear at all in the description and I'd very much appreciate if someone could confirm that this is the case.AddWell this was initially posted at https://stats.stackexchange.com/ because I was hoping to get the input of staticians who work with the ppoints function. Since it has been migrated here, I'll paste below the function I wrote to replicate ppoints in python. I've tested it and both seem to give back the same results, but I'd be great if someone could clarify the points made above because they are not made at all clear by the function's description. <code>  Ordinates for Probability PlottingDescription: Generates the sequence of probability points (1:m - a)/(m + (1-a)-a) where m is either n, if length(n)==1, or length(n).Usage: ppoints(n, a = ifelse(n <= 10, 3/8, 1/2))... def ppoints(vector): ''' Mimics R's function 'ppoints'. ''' m_range = int(vector[0]) if len(vector)==1 else len(vector) n = vector[0] if len(vector)==1 else len(vector) a = 3./8. if n <= 10 else 1./2 m_value = n if len(vector)==1 else m_range pp_list = [((m+1)-a)/(m_value+(1-a)-a) for m in range(m_range)] return pp_list",Imitating 'ppoints' R function in python
Unicode error with lxml in python," I am using lxml to make an xml file and my sample program is : And I am getting the following error output : I thought it of a Unicode Error,so tried changing encoding of 'dt' with codes likestr(dt)unicode(dt).encode('unicode_escape')dt.encocde('ascii','ignore')dt.encode('ascii','decode')and some others also, but none worked and same error msg generated. <code>  from lxml import etreeimport datetimedt=datetime.datetime(2013,11,30,4,5,6)dt=dt.strftime('%Y-%m-%d')page=etree.Element('html')doc=etree.ElementTree(page)dateElm=etree.SubElement(page,dt)outfile=open('somefile.xml','w')doc.write(outfile) dateElm=etree.SubElement(page,dt) File ""lxml.etree.pyx"", line 2899, in lxml.etree.SubElement (src/lxml/lxml.etree.c:62284) File ""apihelpers.pxi"", line 171, in lxml.etree._makeSubElement (src/lxml/lxml.etree.c:14296) File ""apihelpers.pxi"", line 1523, in lxml.etree._tagValidOrRaise (src/lxml/lxml.etree.c:26852)ValueError: Invalid tag name u'2013-11-30'","""Invalid tag name"" error when creating element with lxml in python"
Dihedral/Torsion Angle From Four Points Cartesian Coordinates in Python," What suggestions do people have for quickly calculating dihedral angles in Python?In the diagrams, phi is the dihedral angle:What's your best for calculating angles in the range 0 to pi? What about 0 to 2pi?""Best"" here means some mix of fast and numerically stable. Methods that return values over the full range 0 to 2pi are preferred but if you have an incredibly fast way of calculating the dihedral over 0 to pi share that too.Here are my 3 best efforts. Only the 2nd one returns angles between 0 and 2pi. It's also the slowest.General comments about my approaches:arccos() in Numpy seems plenty stable but since people raise this issue I may just not fully understand it.The use of einsum came from here. Why is numpy's einsum faster than numpy's built in functions?The diagrams and some inspiration came from here. How do I calculate a dihedral angle given Cartesian coordinates?The 3 approaches with comments: Benchmarking: Benchmarking output: As you can see in the benchmarking, the last one tends to be the fastest while the second one is the only one that returns angles from the full range of 0 to 2pi since it uses arctan2. <code>  import numpy as npfrom time import time# This approach tries to minimize magnitude and sqrt calculationsdef dihedral1(p): # Calculate vectors between points, b1, b2, and b3 in the diagram b = p[:-1] - p[1:] # ""Flip"" the first vector so that eclipsing vectors have dihedral=0 b[0] *= -1 # Use dot product to find the components of b1 and b3 that are not # perpendicular to b2. Subtract those components. The resulting vectors # lie in parallel planes. v = np.array( [ v - (v.dot(b[1])/b[1].dot(b[1])) * b[1] for v in [b[0], b[2]] ] ) # Use the relationship between cos and dot product to find the desired angle. return np.degrees(np.arccos( v[0].dot(v[1])/(np.linalg.norm(v[0]) * np.linalg.norm(v[1]))))# This is the straightforward approach as outlined in the answers to# ""How do I calculate a dihedral angle given Cartesian coordinates?""def dihedral2(p): b = p[:-1] - p[1:] b[0] *= -1 v = np.array( [ v - (v.dot(b[1])/b[1].dot(b[1])) * b[1] for v in [b[0], b[2]] ] ) # Normalize vectors v /= np.sqrt(np.einsum('...i,...i', v, v)).reshape(-1,1) b1 = b[1] / np.linalg.norm(b[1]) x = np.dot(v[0], v[1]) m = np.cross(v[0], b1) y = np.dot(m, v[1]) return np.degrees(np.arctan2( y, x ))# This one starts with two cross products to get a vector perpendicular to# b2 and b1 and another perpendicular to b2 and b3. The angle between those vectors# is the dihedral angle.def dihedral3(p): b = p[:-1] - p[1:] b[0] *= -1 v = np.array( [np.cross(v,b[1]) for v in [b[0], b[2]] ] ) # Normalize vectors v /= np.sqrt(np.einsum('...i,...i', v, v)).reshape(-1,1) return np.degrees(np.arccos( v[0].dot(v[1]) ))dihedrals = [ (""dihedral1"", dihedral1), (""dihedral2"", dihedral2), (""dihedral3"", dihedral3) ] # Testing arccos near 0# Answer is 0.000057p1 = np.array([ [ 1, 0, 0 ], [ 0, 0, 0 ], [ 0, 0, 1 ], [ 0.999999, 0.000001, 1 ] ])# +x,+yp2 = np.array([ [ 1, 0, 0 ], [ 0, 0, 0 ], [ 0, 0, 1 ], [ 0.1, 0.6, 1 ] ])# -x,+yp3 = np.array([ [ 1, 0, 0 ], [ 0, 0, 0 ], [ 0, 0, 1 ], [-0.3, 0.6, 1 ] ])# -x,-yp4 = np.array([ [ 1, 0, 0 ], [ 0, 0, 0 ], [ 0, 0, 1 ], [-0.3, -0.6, 1 ] ])# +x,-yp5 = np.array([ [ 1, 0, 0 ], [ 0, 0, 0 ], [ 0, 0, 1 ], [ 0.6, -0.6, 1 ] ])for d in dihedrals: name = d[0] f = d[1] print ""%s: %12.6f %12.6f %12.6f %12.6f %12.6f"" \ % (name, f(p1), f(p2), f(p3), f(p4), f(p5))printdef profileDihedrals(f): t0 = time() for i in range(20000): p = np.random.random( (4,3) ) f(p) p = np.random.randn( 4,3 ) f(p) return(time() - t0)print ""dihedral1: "", profileDihedrals(dihedral1)print ""dihedral2: "", profileDihedrals(dihedral2)print ""dihedral3: "", profileDihedrals(dihedral3) dihedral1: 0.000057 80.537678 116.565051 116.565051 45.000000dihedral2: 0.000057 80.537678 116.565051 -116.565051 -45.000000dihedral3: 0.000057 80.537678 116.565051 116.565051 45.000000dihedral1: 2.79781794548dihedral2: 3.74271392822dihedral3: 2.49604296684",Dihedral/Torsion Angle From Four Points in Cartesian Coordinates in Python
How to properly handle a cyclic module dependency in Python?," Trying to find a good and proper pattern to handle a circular module dependency in Python. Usually, the solution is to remove it (through refactoring); however, in this particular case we would really like to have the functionality that requires the circular import.EDIT: According to answers below, the usual angle of attack for this kind of issue would be a refactor. However, for the sake of this question, assume that is not an option (for whatever reason).The problem:The logging module requires the configuration module for some of its configuration data. However, for some of the configuration functions I would really like to use the custom logging functions that are defined in the logging module. Obviously, importing the logging module in configuration raises an error.The possible solutions we can think of:Don't do it. As I said before, this is not a good option, unless all other possibilities are ugly and bad.Monkey-patch the module. This doesn't sound too bad: load the logging module dynamically into configuration after the initial import, and before any of its functions are actually used. This implies defining global, per-module variables, though.Dependency injection. I've read and run into dependency injection alternatives (particularly in the Java Enterprise space) and they remove some of this headache; however, they may be too complicated to use and manage, which is something we'd like to avoid. I'm not aware of how the panorama is about this in Python, though.What is a good way to enable this functionality?Thanks very much! <code> ",How to properly handle a circular module dependency in Python?
Display message when going over something with mouse cursor in Python," I have a GUI made with TKinter in Python. I would like to be able to display a message when my mouse cursor goes, for example, on top of a label or button. The purpose of this is to explain to the user what the button/label does or represents.Is there a way to display text when hovering over a tkinter object in Python? <code> ",Display message when hovering over something with mouse cursor in Python
Various Urllib2 errors when running selenium webdriver on a VPS," I'm using Selenium with Python bindings to scrape AJAX content from a web page with headless Firefox. It works perfectly when run on my local machine. When I run the exact same script on my VPS, errors get thrown on seemingly random (yet consistent) lines. My local and remote systems have the same exact OS/architecture, so I'm guessing the difference is VPS-related.For each of these tracebacks, the line is run 4 times before an error is thrown.I most often get this URLError when executing JavaScript to scroll an element into view. Occasionally I'll get this BadStatusLine when reading text from an element. A couple times I've gotten a socket error: I'm scraping from Google without a proxy, so my first thought was that my IP address is recognized as a VPS and put under a 5-time page-manipulation limitation or something. But my initial research indicates that these errors would not arise from being blocked.Any insight into what these errors mean collectively, or on the necessary considerations when making HTTP requests from a VPS would be much appreciated.UpdateAfter a little thinking and looking into what a webdriver really is -- automated browser input -- I should have been confused about why remote_connection.py is making urllib2 requests at all. It would seem that the text method of the WebElement class is an ""extra"" feature of the python bindings that isn't part of the Selenium core. That doesn't explain the above errors, but it may indicate that the text method shouldn't be used for scraping.Update 2I realized that, for my purposes, Selenium's only function is getting the ajax content to load. So after the page loads, I'm parsing the source with lxml rather than getting elements with Selenium, i.e.: However, page_source is yet another method that results in a call to urllib2, and I consistently get the BadStatusLine error the second time I use it. Minimizing urllib2 requests is definitely a step in the right direction.Update 3Eliminating urllib2 requests by grabbing the source with javascript is better yet: ConclusionThese errors can be avoided by doing a time.sleep(10) between every few requests. The best explanation I've come up with is that Google's firewall recognizes my IP as a VPS and therefore puts it under a stricter set of blocking rules.This was my initial thought, but I still find it hard to believe because my web searches return no indication that the above errors could be caused by a firewall.If this is the case though, I would think the stricter rules could be circumvented with a proxy, though that proxy might have to be a local system or tor to avoid the same restrictions. <code>  File ""google_scrape.py"", line 18, in _get_data driver.execute_script(""arguments[0].scrollIntoView(true);"", e) File ""/home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 396, in execute_script {'script': script, 'args':converted_args})['value'] File ""/home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 162, in execute response = self.command_executor.execute(driver_command, params) File ""/home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/remote_connection.py"", line 355, in execute return self._request(url, method=command_info[0], data=data) File ""/home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/remote_connection.py"", line 402, in _request response = opener.open(request) File ""/usr/lib64/python2.7/urllib2.py"", line 404, in open response = self._open(req, data) File ""/usr/lib64/python2.7/urllib2.py"", line 422, in _open '_open', req) File ""/usr/lib64/python2.7/urllib2.py"", line 382, in _call_chain result = func(*args) File ""/usr/lib64/python2.7/urllib2.py"", line 1214, in http_open return self.do_open(httplib.HTTPConnection, req) File ""/usr/lib64/python2.7/urllib2.py"", line 1184, in do_open raise URLError(err)urllib2.URLError: <urlopen error [Errno 111] Connection refused> File ""google_scrape.py"", line 19, in _get_data if e.text.strip(): File ""/home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/webelement.py"", line 55, in text return self._execute(Command.GET_ELEMENT_TEXT)['value'] File ""/home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/webelement.py"", line 233, in _execute return self._parent.execute(command, params) File ""/home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 162, in execute response = self.command_executor.execute(driver_command, params) File ""/home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/remote_connection.py"", line 355, in execute return self._request(url, method=command_info[0], data=data) File ""/home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/remote_connection.py"", line 402, in _request response = opener.open(request) File ""/usr/lib64/python2.7/urllib2.py"", line 404, in open response = self._open(req, data) File ""/usr/lib64/python2.7/urllib2.py"", line 422, in _open '_open', req) File ""/usr/lib64/python2.7/urllib2.py"", line 382, in _call_chain result = func(*args) File ""/usr/lib64/python2.7/urllib2.py"", line 1214, in http_open return self.do_open(httplib.HTTPConnection, req) File ""/usr/lib64/python2.7/urllib2.py"", line 1187, in do_open r = h.getresponse(buffering=True) File ""/usr/lib64/python2.7/httplib.py"", line 1045, in getresponse response.begin() File ""/usr/lib64/python2.7/httplib.py"", line 409, in begin version, status, reason = self._read_status() File ""/usr/lib64/python2.7/httplib.py"", line 373, in _read_status raise BadStatusLine(line)httplib.BadStatusLine: '' File ""google_scrape.py"", line 19, in _get_data if e.text.strip(): File ""/home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/webelement.py"", line 55, in text return self._execute(Command.GET_ELEMENT_TEXT)['value'] File ""/home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/webelement.py"", line 233, in _execute return self._parent.execute(command, params) File ""/home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 162, in execute response = self.command_executor.execute(driver_command, params) File ""/home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/remote_connection.py"", line 355, in execute return self._request(url, method=command_info[0], data=data) File ""/home/ryne/.virtualenvs/DEV/lib/python2.7/site-packages/selenium/webdriver/remote/remote_connection.py"", line 402, in _request response = opener.open(request) File ""/usr/lib64/python2.7/urllib2.py"", line 404, in open response = self._open(req, data) File ""/usr/lib64/python2.7/urllib2.py"", line 422, in _open '_open', req) File ""/usr/lib64/python2.7/urllib2.py"", line 382, in _call_chain result = func(*args) File ""/usr/lib64/python2.7/urllib2.py"", line 1214, in http_open return self.do_open(httplib.HTTPConnection, req) File ""/usr/lib64/python2.7/urllib2.py"", line 1187, in do_open r = h.getresponse(buffering=True) File ""/usr/lib64/python2.7/httplib.py"", line 1045, in getresponse response.begin() File ""/usr/lib64/python2.7/httplib.py"", line 409, in begin version, status, reason = self._read_status() File ""/usr/lib64/python2.7/httplib.py"", line 365, in _read_status line = self.fp.readline(_MAXLINE + 1) File ""/usr/lib64/python2.7/socket.py"", line 476, in readline data = self._sock.recv(self._rbufsize)socket.error: [Errno 104] Connection reset by peer html = lxml.html.fromstring(driver.page_source) html = lxml.html.fromstring(driver.execute_script(""return window.document.documentElement.outerHTML""))",Various Urllib2 errors when running Selenium webdriver on a VPS
Can python abstract base classes inherit from classes other than object?," It seems as if that when I have an abstract base class that inherits from gevent.Greenlet (which inherits from the C extension module greenlet: https://github.com/python-greenlet/greenlet) then classes that implement it do not raise any of the abc errors about unimplemented methods. If I inherit from object it fails as expected: What is the right way to implement this functionality? <code>  class ActorBase(gevent.Greenlet): __metaclass__ = abc.ABCMeta @abc.abstractmethod def foo(self): print ""foo""class ActorBaseTest(ActorBase): def bar(self): print ""bar""abt = ActorBaseTest() # no errors! class ActorBase(object): __metaclass__ = abc.ABCMeta @abc.abstractmethod def foo(self): print ""foo""class ActorBaseTest(ActorBase): def bar(self): print ""bar"">>> abt = ActorBaseTest()Traceback (most recent call last): File ""/home/dw/.virtualenvs/prj/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2827, in run_codeexec code_obj in self.user_global_ns, self.user_ns File ""<ipython-input-6-d67a142e7297>"", line 1, in <module> abt = ActorBaseTest()TypeError: Can't instantiate abstract class ActorBaseTest with abstract methods foo",Can python abstract base classes inherit from C extensions?
Strange behavior when comparing uncode objects with string objects," when comparing two strings in python, it works fine and when comparing a string object with a unicode object it fails as expected however when comparing a string object with a converted unicode (unicode --> str) object it failsA Demo:Works as expected: Pretty much yeah: Not expected: Why doesn't the third example work as expected when both the type's are of the same class? <code>  >>> if 's' is 's': print ""Hurrah!""... Hurrah! >>> if 's' is u's': print ""Hurrah!""... >>> if 's' is str(u's'): print ""Hurrah!""... >>> type('s')<type 'str'>>>> type(str(u's'))<type 'str'>",Strange behavior when comparing unicode objects with string objects
How to use a different version of python duing NPM install?," I have terminal access to a VPS running centos 5.9 and default python 2.4.3 installed. I also installed python 2.7.3 via these commands: (I used make altinstall instead of make install) then I installed node.js from source via these commands: The problem is, when I use npm install and try to install a node.js package which requires python > 2.4.3 I get this error: how should I ""pass the --python switch to point to Python >= v2.5.0""? <code>  wget http://www.python.org/ftp/python/2.7.3/Python-2.7.3.tgztar -xf Python-2.7.3.tgzcd Python-2.7.3./configuremakemake altinstall python2.7 ./configuremakemake install gyp ERR! configure errorgyp ERR! stack Error: Python executable ""python"" is v2.4.3, which is not supported by gyp.gyp ERR! stack You can pass the --python switch to point to Python >= v2.5.0 & < 3.0.0.gyp ERR! stack at failPythonVersion (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/configure.js:125:14)gyp ERR! stack at /usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/configure.js:114:9",How to use a different version of python during NPM install?
Extract points/coordinates from Python shapely polygon," How do you get/extract the points that define a shapely polygon?Thanks!Example of a shapely polygon <code>  from shapely.geometry import Polygon# Create polygon from lists of pointsx = [list of x vals]y = [list of y vals]polygon = Polygon(x,y)",Extract points/coordinates from a polygon in Shapely
Scrapy : How to pass list of arguments through spider?," Creating a scraper for fantasy team. Looking for a way to pass a list of the players names as arguments, and then for each player_name in player_list run the parsing code.I currently have something like this I'm assuming entering a list of arguments is the same as just one argument through the command line so I enter something like this: Problem 2!Solved the first issue by inputting a comma delimited list of arguments like so I now want to go through each ""name"" (i.e. 'abc def') to find the first initial of their last name (in this case 'd').I use the code And I end up with the result [[""'"",'a','b','c',... etc]] Why does python not assign player_name to each 'name' (e.g. 'abc def' and 'ghi jkl')? can someone explain this logic to me, and I will probably understand the right way to do it afterwards! <code>  class statsspider(BaseSpider):name = 'statsspider'def __init__ (self, domain=None, player_list=""""): self.allowed_domains = ['sports.yahoo.com'] self.start_urls = [ 'http://sports.yahoo.com/nba/players', ] self.player_list= ""%s"" % player_listdef parse(self, response): example code yield request scrapy crawl statsspider -a player_list=['xyz','abc'] scrapy crawl statsspider -a player_list=""abc def,ghi jkl"" array = []for player_name in self.player_list: array.append(player_name)print array",Scrapy : How to pass list of arguments through command prompt to spider?
Where Django is thread safe and where its not?," Recently I've read this article:http://blog.roseman.org.uk/2010/02/01/middleware-post-processing-django-gotcha/I don't understand, why does the solution described there work?Why does instantiating separate objects make data chunk thread-safe?I have two guesses:Django explicitly holds middleware objects in shared memory, and do not do this for other objects, so other objects are thread-safe.In second example, in article, lifetime of thread-safety-critical data is much less that in first example, so probably, thread-unsafe operations just have no time to occur.There is also issues with thread-safety in Django templates.My question is - how to guess when Django thread-safe and where its not? is there any logic in it or conventions? Another question - I know that request object is thread safe - it is clear, that it wouldn't be safe, web-sites built with Django would be not able to operate, but what exactly makes it thread-safe? <code> ",Is middleware in Django thread-safe?
"How to understand Python's phylosophy ""never is often better than *right* now""", I don't quite understand the second sentence here from The Zen of Python: Now is better than never. Although never is often better than right now.Can anyone explain it or give an example? <code> ,"What is the meaning of Python's philosophy ""never is often better than *right* now"""
"How to understand Python's philosophy ""never is often better than *right* now""", I don't quite understand the second sentence here from The Zen of Python: Now is better than never. Although never is often better than right now.Can anyone explain it or give an example? <code> ,"What is the meaning of Python's philosophy ""never is often better than *right* now"""
Constructing a Co-occurance matrix in python pandas," I know how to do this in R. But, is there any function in pandas that transforms a dataframe to an nxn co-occurrence matrix containing the counts of two aspects co-occurring. For example a matrix df: would yield: Since the matrix is mirrored on the diagonal I guess there would be a way to optimize code. <code>  import pandas as pddf = pd.DataFrame({'TFD' : ['AA', 'SL', 'BB', 'D0', 'Dk', 'FF'], 'Snack' : ['1', '0', '1', '1', '0', '0'], 'Trans' : ['1', '1', '1', '0', '0', '1'], 'Dop' : ['1', '0', '1', '0', '1', '1']}).set_index('TFD')print df>>> Dop Snack TransTFD AA 1 1 1SL 0 0 1BB 1 1 1D0 0 1 0Dk 1 0 0FF 1 0 1[6 rows x 3 columns] Dop Snack TransDop 0 2 3Snack 2 0 2Trans 3 2 0",Constructing a co-occurrence matrix in python pandas
Python import module from subdirectory," I know this question has been asked many times here and I'v probably read most of the answers (including this and that) as well as the python documentation but still can not find an answer to my very simple import problem. It's so simple that I must miss something stupid but I don't see it yet.I have setup the following structure: I just want to load mymodule.py from myscript.py (or the commandline python interpreter which should be the same).myscript.py contains: __init.py__ contains: mymodule.py contains My goal is to call myfunction from myscript.py but if I try to call the module I get What I already tried:I tried everything under OSX and Ubuntu Linux to reduce the possibility of a faulty python installation.I set the PYTHONPATH environment variable to the myproject directory as well as to . and to both.I left __init.py__ blankI tried the import statements also from the python interpreter started from the myproject directoryI tried the following import statements: all without success (same error message).If I put mymodule.py in the project directory without using a package, import works fine. But I don't see why the import from the subpackages is not working.Any idea how I can get that to work?Thanks for help! <code>  myproject myscript.py MyPackage __init.py__ mymodule.py #!/usr/bin/pythonimport MyPackage from . import mymodule #!/usr/bin/pythondef myfunction(): print ""mymessage"" $python myscript.py Traceback (most recent call last): File ""myscript.py"", line 2, in <module> import MyPackageImportError: No module named MyPackage from MyPackage import mymoduleimport MyPackage.mymoduleimport MyPackage.mymodule as module",Python cannot import module from subdirectory even with a file named __init.py__ in the directory
Debugging with GDB+Python+OpenCV: Why the firstly opened OpenCV window is not shown active," I have found a very strange problem about using the python scripts under Python Windows command line prompt, to reproduce this issue, you can simply do those steps:start a Python command line prompt(this is usually to hit the Start Menu->Python 2.7->Python(command line). type the following text, and hit Enter key. type the following text, and hit Enter key. You will see a message box opened, but this message box window is not activated. Use the mouse to click on the icon of the message box in task bar to activate the message box Close the message box type the text again in the Python prompt shell Now, the message box is showed activated (the expected behavior) So, my question is, why the first message box(window) is not shown active? I originally find this issue when I run a Python pretty printer under GDB command line, because I want to use some python pretty printer to visual the data, like this GDB cv::Mat python object issue when debugging a c++ program, I need to show the OpenCV Image window immediately after I type the plot command.But later I found that this is an issue related to Python itself. <code>  import ctypes ctypes.windll.user32.MessageBoxA(0, ""Your text"", ""Your title"", 1) ctypes.windll.user32.MessageBoxA(0, ""Your text"", ""Your title"", 1)",Windows+Python: Why is the first opened window not shown active?
Debugging with GDB+Python+OpenCV: Why is the first opened OpenCV window not shown active?," I have found a very strange problem about using the python scripts under Python Windows command line prompt, to reproduce this issue, you can simply do those steps:start a Python command line prompt(this is usually to hit the Start Menu->Python 2.7->Python(command line). type the following text, and hit Enter key. type the following text, and hit Enter key. You will see a message box opened, but this message box window is not activated. Use the mouse to click on the icon of the message box in task bar to activate the message box Close the message box type the text again in the Python prompt shell Now, the message box is showed activated (the expected behavior) So, my question is, why the first message box(window) is not shown active? I originally find this issue when I run a Python pretty printer under GDB command line, because I want to use some python pretty printer to visual the data, like this GDB cv::Mat python object issue when debugging a c++ program, I need to show the OpenCV Image window immediately after I type the plot command.But later I found that this is an issue related to Python itself. <code>  import ctypes ctypes.windll.user32.MessageBoxA(0, ""Your text"", ""Your title"", 1) ctypes.windll.user32.MessageBoxA(0, ""Your text"", ""Your title"", 1)",Windows+Python: Why is the first opened window not shown active?
Debugging with GDB+Python: Why is the first opened window not shown active?," I have found a very strange problem about using the python scripts under Python Windows command line prompt, to reproduce this issue, you can simply do those steps:start a Python command line prompt(this is usually to hit the Start Menu->Python 2.7->Python(command line). type the following text, and hit Enter key. type the following text, and hit Enter key. You will see a message box opened, but this message box window is not activated. Use the mouse to click on the icon of the message box in task bar to activate the message box Close the message box type the text again in the Python prompt shell Now, the message box is showed activated (the expected behavior) So, my question is, why the first message box(window) is not shown active? I originally find this issue when I run a Python pretty printer under GDB command line, because I want to use some python pretty printer to visual the data, like this GDB cv::Mat python object issue when debugging a c++ program, I need to show the OpenCV Image window immediately after I type the plot command.But later I found that this is an issue related to Python itself. <code>  import ctypes ctypes.windll.user32.MessageBoxA(0, ""Your text"", ""Your title"", 1) ctypes.windll.user32.MessageBoxA(0, ""Your text"", ""Your title"", 1)",Windows+Python: Why is the first opened window not shown active?
Comparing dictionaries based on a combination of fields," I have a list ""records"" like this I want to remove records that are ""duplicates"", where equality is defined by a list of logical conditions. Each element in the list is an OR condition, and all elements are ANDed together. For example: means that two records are considered equal if their name AND (their price OR url) are equal. For the above example: So the resulting list must contain items 1, 2, 3, 7 and 8.Please take into account thatthere might be more AND conditions: ['name'], ['price', 'url'], ['weight'], ['size'], ...the OR groups in the conditions list can be longer than 2 items, e.g. ['name'], ['price', 'url', 'weight']...the source list is very long, an O(n^2) alogirthm is out of the question <code>  data = [ {'id':1, 'name': 'A', 'price': 10, 'url': 'foo'}, {'id':2, 'name': 'A', 'price': 20, 'url': 'bar'}, {'id':3, 'name': 'A', 'price': 30, 'url': 'baz'}, {'id':4, 'name': 'A', 'price': 10, 'url': 'baz'}, {'id':5, 'name': 'A', 'price': 20, 'url': 'bar'}, {'id':6, 'name': 'A', 'price': 30, 'url': 'foo'}, {'id':7, 'name': 'A', 'price': 99, 'url': 'quu'}, {'id':8, 'name': 'B', 'price': 10, 'url': 'foo'},] filters = [ ['name'], ['price', 'url'] ] For item 1 the duplicates are 4 (by name and price) and 6 (name+url)For item 2 - 5 (name+price, name+url)For item 3 - 4 (name+url) and 6 (name+price)For item 7 there are no duplicates (neither price nor url match)For item 8 there are no duplicates (name doesn't match)",Comparing dictionaries based on a combination of keys
Create pandas dataframe from objects and list," I finally have output of data I need from a file with many json objects but I need some help with converting the below output into a single dataframe as it loops through the data. Here is the code to produce the output including a sample of what the output looks like:original data: Sample output I get when I run the above which I would like to store in a pandas dataframe as 3 columns. So the below code seems a lot closer in that it gives me a funky df if I pass the in the list and Transpose the df. Any idea on how I can get this reshaped properly? Dataframe: Here is my final code and output. How do I capture each dataframe it creates through the loop and concatenate them on the fly as one dataframe object?  <code>  {""zipcode"":""08989"",""current""{""canwc"":null,""cig"":4900,""class"":""observation"",""clds"":""OVC"",""day_ind"":""D"",""dewpt"":19,""expireTimeGMT"":1385486700,""feels_like"":34,""gust"":null,""hi"":37,""humidex"":null,""icon_code"":26,""icon_extd"":2600,""max_temp"":37,""wxMan"":""wx1111""},""triggers"":[53,31,9,21,48,7,40,178,55,179,176,26,103,175,33,51,20,57,112,30,50,113]}{""zipcode"":""08990"",""current"":{""canwc"":null,""cig"":4900,""class"":""observation"",""clds"":""OVC"",""day_ind"":""D"",""dewpt"":19,""expireTimeGMT"":1385486700,""feels_like"":34,""gust"":null,""hi"":37,""humidex"":null,""icon_code"":26,""icon_extd"":2600,""max_temp"":37, ""wxMan"":""wx1111""},""triggers"":[53,31,9,21,48,7,40,178,55,179,176,26,103,175,33,51,20,57,112,30,50,113]}def lines_per_n(f, n): for line in f: yield ''.join(chain([line], itertools.islice(f, n - 1)))for fin in glob.glob('*.txt'): with open(fin) as f: for chunk in lines_per_n(f, 5): try: jfile = json.loads(chunk) zipcode = jfile['zipcode'] datetime = jfile['current']['proc_time'] triggers = jfile['triggers'] print pd.Series(jfile['zipcode']), pd.Series(jfile['current']['proc_time']),\ jfile['triggers'] except ValueError, e: pass else: pass 08988 20131126102946 []08989 20131126102946 [53, 31, 9, 21, 48, 7, 40, 178, 55, 179]08988 20131126102946 []08989 20131126102946 [53, 31, 9, 21, 48, 7, 40, 178, 55, 179]00544 20131126102946 [178, 30, 176, 103, 179, 112, 21, 20, 48] def series_chunk(chunk): jfile = json.loads(chunk) zipcode = jfile['zipcode'] datetime = jfile['current']['proc_time'] triggers = jfile['triggers'] return jfile['zipcode'],\ jfile['current']['proc_time'],\ jfile['triggers']for fin in glob.glob('*.txt'): with open(fin) as f: for chunk in lines_per_n(f, 7): df1 = pd.DataFrame(list(series_chunk(chunk))) print df1.T[u'08988', u'20131126102946', []][u'08989', u'20131126102946', [53, 31, 9, 21, 48, 7, 40, 178, 55, 179]][u'08988', u'20131126102946', []][u'08989', u'20131126102946', [53, 31, 9, 21, 48, 7, 40, 178, 55, 179]] 0 1 20 08988 20131126102946 [] 0 1 20 08989 20131126102946 [53, 31, 9, 21, 48, 7, 40, 178, 55, 179, 176, ... 0 1 20 08988 20131126102946 [] 0 1 20 08989 20131126102946 [53, 31, 9, 21, 48, 7, 40, 178, 55, 179, 176, ... for fin in glob.glob('*.txt'): with open(fin) as f: print pd.concat([series_chunk(chunk) for chunk in lines_per_n(f, 7)], axis=1).T 0 1 20 08988 20131126102946 []1 08989 20131126102946 [53, 31, 9, 21, 48, 7, 40, 178, 55, 179, 176, ... 0 1 20 08988 20131126102946 []1 08989 20131126102946 [53, 31, 9, 21, 48, 7, 40, 178, 55, 179, 176, ...",Create pandas dataframe from json objects
Python: why keyword is doesn't work here," I know that is is used to compare if two objects are the same but == is for equality. From my experience is always worked for numbers because Python reuse numbers. for example: And I'm used to using is whenever I compare something to a number. But is didn't work for this program below: When I used is inside the query function like this [link for link in links if link.submitter_id is 62443] I'll get an empty list. But if I use ==, it worked fine.For the most part, the code was directly taken from the udacity site but I also tried it on my local machine. The same result. So I think the numbers are now different objects in this case but why? Is there a need for this?EDIT: Yes. I admit this question is duplicate and should be closed. But it's duplicate with the first post not the second. I didn't know that question before posting this. My problem was that I thought number objects would always be reused.Thanks to everyone, I got rid of a bad habit. <code>  >>>a = 3>>>a is 3True from collections import namedtuple# Code taken directly from [Udacity site][1].# make a basic Link classLink = namedtuple('Link', ['id', 'submitter_id', 'submitted_time', 'votes', 'title', 'url'])# list of Links to work withlinks = [ Link(0, 60398, 1334014208.0, 109, ""C overtakes Java as the No. 1 programming language in the TIOBE index."", ""http://pixelstech.net/article/index.php?id=1333969280""), Link(1, 60254, 1333962645.0, 891, ""This explains why technical books are all ridiculously thick and overpriced"", ""http://prog21.dadgum.com/65.html""), Link(23, 62945, 1333894106.0, 351, ""Learn Haskell Fast and Hard"", ""http://yannesposito.com/Scratch/en/blog/Haskell-the-Hard-Way/""), Link(2, 6084, 1333996166.0, 81, ""Announcing Yesod 1.0- a robust, developer friendly, high performance web framework for Haskell"", ""http://www.yesodweb.com/blog/2012/04/announcing-yesod-1-0""), Link(3, 30305, 1333968061.0, 270, ""TIL about the Lisp Curse"", ""http://www.winestockwebdesign.com/Essays/Lisp_Curse.html""), Link(4, 59008, 1334016506.0, 19, ""The Downfall of Imperative Programming. Functional Programming and the Multicore Revolution"", ""http://fpcomplete.com/the-downfall-of-imperative-programming/""), Link(5, 8712, 1333993676.0, 26, ""Open Source - Twitter Stock Market Game - "", ""http://www.twitstreet.com/""), Link(6, 48626, 1333975127.0, 63, ""First look: Qt 5 makes JavaScript a first-class citizen for app development"", ""http://arstechnica.com/business/news/2012/04/an-in-depth-look-at-qt-5-making-javascript-a-first-class-citizen-for-native-cross-platform-developme.ars""), Link(7, 30172, 1334017294.0, 5, ""Benchmark of Dictionary Structures"", ""http://lh3lh3.users.sourceforge.net/udb.shtml""), Link(8, 678, 1334014446.0, 7, ""If It's Not on Prod, It Doesn't Count: The Value of Frequent Releases"", ""http://bits.shutterstock.com/?p=165""), Link(9, 29168, 1334006443.0, 18, ""Language proposal: dave"", ""http://davelang.github.com/""), Link(17, 48626, 1334020271.0, 1, ""LispNYC and EmacsNYC meetup Tuesday Night: Large Scale Development with Elisp "", ""http://www.meetup.com/LispNYC/events/47373722/""), Link(101, 62443, 1334018620.0, 4, ""research!rsc: Zip Files All The Way Down"", ""http://research.swtch.com/zip""), Link(12, 10262, 1334018169.0, 5, ""The Tyranny of the Diff"", ""http://michaelfeathers.typepad.com/michael_feathers_blog/2012/04/the-tyranny-of-the-diff.html""), Link(13, 20831, 1333996529.0, 14, ""Understanding NIO.2 File Channels in Java 7"", ""http://java.dzone.com/articles/understanding-nio2-file""), Link(15, 62443, 1333900877.0, 1244, ""Why vector icons don't work"", ""http://www.pushing-pixels.org/2011/11/04/about-those-vector-icons.html""), Link(14, 30650, 1334013659.0, 3, ""Python - Getting Data Into Graphite - Code Examples"", ""http://coreygoldberg.blogspot.com/2012/04/python-getting-data-into-graphite-code.html""), Link(16, 15330, 1333985877.0, 9, ""Mozilla: The Web as the Platform and The Kilimanjaro Event"", ""https://groups.google.com/forum/?fromgroups#!topic/mozilla.dev.planning/Y9v46wFeejA""), Link(18, 62443, 1333939389.0, 104, ""github is making me feel stupid(er)"", ""http://www.serpentine.com/blog/2012/04/08/github-is-making-me-feel-stupider/""), Link(19, 6937, 1333949857.0, 39, ""BitC Retrospective: The Issues with Type Classes"", ""http://www.bitc-lang.org/pipermail/bitc-dev/2012-April/003315.html""), Link(20, 51067, 1333974585.0, 14, ""Object Oriented C: Class-like Structures"", ""http://cecilsunkure.blogspot.com/2012/04/object-oriented-c-class-like-structures.html""), Link(10, 23944, 1333943632.0, 188, ""The LOVE game framework version 0.8.0 has been released - with GLSL shader support!"", ""https://love2d.org/forums/viewtopic.php?f=3&amp;t=8750""), Link(22, 39191, 1334005674.0, 11, ""An open letter to language designers: Please kill your sacred cows. (megarant)"", ""http://joshondesign.com/2012/03/09/open-letter-language-designers""), Link(21, 3777, 1333996565.0, 2, ""Developers guide to Garage48 hackatron"", ""http://martingryner.com/developers-guide-to-garage48-hackatron/""), Link(24, 48626, 1333934004.0, 17, ""An R programmer looks at Julia"", ""http://www.r-bloggers.com/an-r-programmer-looks-at-julia/"")]# links is a list of Link objects. Links have a handful of properties. For# example, a Link's number of votes can be accessed by link.votes if ""link"" is a# Link.# make the function query() return a list of Links submitted by user 62443, by# submission time ascendingdef query(): print ""hello"" print [link for link in links if link.submitter_id == 62443] # is does not work return sorted([link for link in links if link.submitter_id == 62443],key = lambda x: x[2])query()","Why doesn't ""is"" keyword work here?"
"Python: why ""is"" keyword doesn't work here"," I know that is is used to compare if two objects are the same but == is for equality. From my experience is always worked for numbers because Python reuse numbers. for example: And I'm used to using is whenever I compare something to a number. But is didn't work for this program below: When I used is inside the query function like this [link for link in links if link.submitter_id is 62443] I'll get an empty list. But if I use ==, it worked fine.For the most part, the code was directly taken from the udacity site but I also tried it on my local machine. The same result. So I think the numbers are now different objects in this case but why? Is there a need for this?EDIT: Yes. I admit this question is duplicate and should be closed. But it's duplicate with the first post not the second. I didn't know that question before posting this. My problem was that I thought number objects would always be reused.Thanks to everyone, I got rid of a bad habit. <code>  >>>a = 3>>>a is 3True from collections import namedtuple# Code taken directly from [Udacity site][1].# make a basic Link classLink = namedtuple('Link', ['id', 'submitter_id', 'submitted_time', 'votes', 'title', 'url'])# list of Links to work withlinks = [ Link(0, 60398, 1334014208.0, 109, ""C overtakes Java as the No. 1 programming language in the TIOBE index."", ""http://pixelstech.net/article/index.php?id=1333969280""), Link(1, 60254, 1333962645.0, 891, ""This explains why technical books are all ridiculously thick and overpriced"", ""http://prog21.dadgum.com/65.html""), Link(23, 62945, 1333894106.0, 351, ""Learn Haskell Fast and Hard"", ""http://yannesposito.com/Scratch/en/blog/Haskell-the-Hard-Way/""), Link(2, 6084, 1333996166.0, 81, ""Announcing Yesod 1.0- a robust, developer friendly, high performance web framework for Haskell"", ""http://www.yesodweb.com/blog/2012/04/announcing-yesod-1-0""), Link(3, 30305, 1333968061.0, 270, ""TIL about the Lisp Curse"", ""http://www.winestockwebdesign.com/Essays/Lisp_Curse.html""), Link(4, 59008, 1334016506.0, 19, ""The Downfall of Imperative Programming. Functional Programming and the Multicore Revolution"", ""http://fpcomplete.com/the-downfall-of-imperative-programming/""), Link(5, 8712, 1333993676.0, 26, ""Open Source - Twitter Stock Market Game - "", ""http://www.twitstreet.com/""), Link(6, 48626, 1333975127.0, 63, ""First look: Qt 5 makes JavaScript a first-class citizen for app development"", ""http://arstechnica.com/business/news/2012/04/an-in-depth-look-at-qt-5-making-javascript-a-first-class-citizen-for-native-cross-platform-developme.ars""), Link(7, 30172, 1334017294.0, 5, ""Benchmark of Dictionary Structures"", ""http://lh3lh3.users.sourceforge.net/udb.shtml""), Link(8, 678, 1334014446.0, 7, ""If It's Not on Prod, It Doesn't Count: The Value of Frequent Releases"", ""http://bits.shutterstock.com/?p=165""), Link(9, 29168, 1334006443.0, 18, ""Language proposal: dave"", ""http://davelang.github.com/""), Link(17, 48626, 1334020271.0, 1, ""LispNYC and EmacsNYC meetup Tuesday Night: Large Scale Development with Elisp "", ""http://www.meetup.com/LispNYC/events/47373722/""), Link(101, 62443, 1334018620.0, 4, ""research!rsc: Zip Files All The Way Down"", ""http://research.swtch.com/zip""), Link(12, 10262, 1334018169.0, 5, ""The Tyranny of the Diff"", ""http://michaelfeathers.typepad.com/michael_feathers_blog/2012/04/the-tyranny-of-the-diff.html""), Link(13, 20831, 1333996529.0, 14, ""Understanding NIO.2 File Channels in Java 7"", ""http://java.dzone.com/articles/understanding-nio2-file""), Link(15, 62443, 1333900877.0, 1244, ""Why vector icons don't work"", ""http://www.pushing-pixels.org/2011/11/04/about-those-vector-icons.html""), Link(14, 30650, 1334013659.0, 3, ""Python - Getting Data Into Graphite - Code Examples"", ""http://coreygoldberg.blogspot.com/2012/04/python-getting-data-into-graphite-code.html""), Link(16, 15330, 1333985877.0, 9, ""Mozilla: The Web as the Platform and The Kilimanjaro Event"", ""https://groups.google.com/forum/?fromgroups#!topic/mozilla.dev.planning/Y9v46wFeejA""), Link(18, 62443, 1333939389.0, 104, ""github is making me feel stupid(er)"", ""http://www.serpentine.com/blog/2012/04/08/github-is-making-me-feel-stupider/""), Link(19, 6937, 1333949857.0, 39, ""BitC Retrospective: The Issues with Type Classes"", ""http://www.bitc-lang.org/pipermail/bitc-dev/2012-April/003315.html""), Link(20, 51067, 1333974585.0, 14, ""Object Oriented C: Class-like Structures"", ""http://cecilsunkure.blogspot.com/2012/04/object-oriented-c-class-like-structures.html""), Link(10, 23944, 1333943632.0, 188, ""The LOVE game framework version 0.8.0 has been released - with GLSL shader support!"", ""https://love2d.org/forums/viewtopic.php?f=3&amp;t=8750""), Link(22, 39191, 1334005674.0, 11, ""An open letter to language designers: Please kill your sacred cows. (megarant)"", ""http://joshondesign.com/2012/03/09/open-letter-language-designers""), Link(21, 3777, 1333996565.0, 2, ""Developers guide to Garage48 hackatron"", ""http://martingryner.com/developers-guide-to-garage48-hackatron/""), Link(24, 48626, 1333934004.0, 17, ""An R programmer looks at Julia"", ""http://www.r-bloggers.com/an-r-programmer-looks-at-julia/"")]# links is a list of Link objects. Links have a handful of properties. For# example, a Link's number of votes can be accessed by link.votes if ""link"" is a# Link.# make the function query() return a list of Links submitted by user 62443, by# submission time ascendingdef query(): print ""hello"" print [link for link in links if link.submitter_id == 62443] # is does not work return sorted([link for link in links if link.submitter_id == 62443],key = lambda x: x[2])query()","Why doesn't ""is"" keyword work here?"
Pyplot pcolormesh confuzed when alpha not 1," I am having some difficulty with pyplot's awesome drawing abilities. I have selected my very own colormap This is basically just the Dark2 colormap, discretized to n (in my case 6) values with the zero value mapping to pure white. The main difference, however, is that the alpha values for my custom colormap are set to 0.2, not 1 as is default.The problem is that when I plot something using this, like the result is something like this:This looks nice enough, but you can clearly see that around each box, there is a very thin border of the same color as the box but with alpha set to 1.EDIT: As suggested in the comments, the cause of these borders is probably overlap between the boxes.Is there a way to clean this up? <code>  n = 6map = matplotlib.cm.get_cmap('Dark2')cmap = colors.ListedColormap([(0,0,0,0)] + [[map(i * 1.0 / n)[j] for j in range(3)] + [0.2] for i in range(1, n + 1)]) plt.pcolormesh(np.random.rand(10,10), cmap = cmapInv)",Pyplot pcolormesh confused when alpha not 1
Lasso on sklearn strange behaviour," When I run something like I get the error: The interesting thing is that A is never rank defficient. (I think) <code>  import numpyfrom sklearn import linear_modelA= #somethingb= #somethingclf=linear_model.Lasso(alpha=0.015, fit_intercept=False, tol=0.00000000000001, max_iter=10000000000000, positive=True)clf.fit(A,b) usr/local/lib/python2.7/dist-packages/scikit_learn-0.14.1-py2.7-linux-x86_64.egg/sklearn/linear_model/coordinate_descent.py:418: UserWarning: Objective did notconverge. You might want to increase the number of iterations' to increase the number of iterations')",Lasso on sklearn does not converge
What's the fastest way to locate a list element within a list(python)?," The list is similar to this: Now I need to locate an item (get index) only by its first value (e.g. ""332""). Is there any better way to do this, apart from iterating from the first one to compare with each value?Code: <code>  [[""12"", ""stuA"", ""stuB""], [""51"", ""stuC"", ""stuD""], ..., [""3234"", ""moreStuff"", ""andMore""]] index = 0for item in thelist: if item[0] == ""332"": print index index = index + 1",What's the fastest way to locate a list element within a list in python?
PRAW: How do get a reddit comment object with just the comment ID?," I'm working on a bot where I only have the comment IDs, e.g., t1_asdasd. I don't have access to the parent thread or anything. Can I pull the corresponding comment object with just the comment ID? <code> ",PRAW: How to get a reddit comment object with just the comment ID?
How to play an mp3 file in python on windows 7 in py2exe?," I am trying to find a way to create text to speech in python (I am on windows 7). I am using pyinstaller to compile this program. I have tried a large number of approaches, including using Google's unofficial text to speech program accessed through the urllib2 module. This ends up creating an mp3 file. For details on the code, much of this code is from http://glowingpython.blogspot.com/2012/11/text-to-speech-with-correct-intonation.html. I have then needed to play the mp3 file that this generates. I have used mplayer, ffmpeg, mp3play, audiere, pydub, and pygame all with the same results: no sound was played, yet no exceptions were raised. I have even used the same pygame code on a raspberry pi and successfully played an mp3 file. I have also tried converting it to a wav file, which has worked fine, only when I try to play it with pygame or winsound, the same thing happens. No sound, no exceptions. My current code uses winsound, playing a wav file that I can successfully play in the windows media player (I can even open it in windows media player from python, using os.startfile()). Here it is: I am also trying to use pygame mixer an music modules. For example: I have even played sounds from python successfully with the winsound and win32api Beep() functions. However, this obviously cannot play an mp3 or wav file. I have also tried a completely different text to speech engine, which plays the sound without an mp3 file in the mix, using pyttsx: This has also failed to create sound, or raise an exception. Because of this pattern, I have a feeling that this has something to do with the system, but it doesn't seem like it is something obvious.Because this almost definitely has something to do with the hardware (pygame.mixer has worked this way on different hardware, and I am sure it usually works on windows) it may be important to know I am using a Toshiba laptop. Also, I am using python 2.7.Ideally, I would like to do this with pygame, because I have the most experience using it and there are some sound editing features I would like to access in pygame if at all possible.I also tried using 64 bit python (I was using 32 bit python on 64 bit windows 7). It still failed to work.I also tried playing an mp3 file inside a Ubuntu virtual box environment, but on the same device. It still didn't work. This isn't particularly surprising because virtualbox uses a lot of the resources (like screen and wifi) from the host operating system, hence it wouldn't necessarily play sounds any differently. Any way around this would be helpful. Some sounds play fine, just not specifically mp3 or wav files in python, so there is probably a solution. <code>  winsound.PlaySound(""file.wav"", winsound.SND_FILENAME) #the wav file is in the same directory as the program init() #this is pygame.init(), I only imported init and the mixer modulepygame.mixer.init() #initializes pygame.mixerpygame.mixer.music.load(filename) #loads it in musicpygame.mixer.music.play() #plays it in musictime.sleep(20) import pyttsxengine = pyttsx.init()def tts(mytext): engine.say(mytext) engine.runAndWait()",How to do text to speech with python on a Toshiba laptop and Windows 7?
How to do text to speech in python on windows 7 in py2exe?," I am trying to find a way to create text to speech in python (I am on windows 7). I am using pyinstaller to compile this program. I have tried a large number of approaches, including using Google's unofficial text to speech program accessed through the urllib2 module. This ends up creating an mp3 file. For details on the code, much of this code is from http://glowingpython.blogspot.com/2012/11/text-to-speech-with-correct-intonation.html. I have then needed to play the mp3 file that this generates. I have used mplayer, ffmpeg, mp3play, audiere, pydub, and pygame all with the same results: no sound was played, yet no exceptions were raised. I have even used the same pygame code on a raspberry pi and successfully played an mp3 file. I have also tried converting it to a wav file, which has worked fine, only when I try to play it with pygame or winsound, the same thing happens. No sound, no exceptions. My current code uses winsound, playing a wav file that I can successfully play in the windows media player (I can even open it in windows media player from python, using os.startfile()). Here it is: I am also trying to use pygame mixer an music modules. For example: I have even played sounds from python successfully with the winsound and win32api Beep() functions. However, this obviously cannot play an mp3 or wav file. I have also tried a completely different text to speech engine, which plays the sound without an mp3 file in the mix, using pyttsx: This has also failed to create sound, or raise an exception. Because of this pattern, I have a feeling that this has something to do with the system, but it doesn't seem like it is something obvious.Because this almost definitely has something to do with the hardware (pygame.mixer has worked this way on different hardware, and I am sure it usually works on windows) it may be important to know I am using a Toshiba laptop. Also, I am using python 2.7.Ideally, I would like to do this with pygame, because I have the most experience using it and there are some sound editing features I would like to access in pygame if at all possible.I also tried using 64 bit python (I was using 32 bit python on 64 bit windows 7). It still failed to work.I also tried playing an mp3 file inside a Ubuntu virtual box environment, but on the same device. It still didn't work. This isn't particularly surprising because virtualbox uses a lot of the resources (like screen and wifi) from the host operating system, hence it wouldn't necessarily play sounds any differently. Any way around this would be helpful. Some sounds play fine, just not specifically mp3 or wav files in python, so there is probably a solution. <code>  winsound.PlaySound(""file.wav"", winsound.SND_FILENAME) #the wav file is in the same directory as the program init() #this is pygame.init(), I only imported init and the mixer modulepygame.mixer.init() #initializes pygame.mixerpygame.mixer.music.load(filename) #loads it in musicpygame.mixer.music.play() #plays it in musictime.sleep(20) import pyttsxengine = pyttsx.init()def tts(mytext): engine.say(mytext) engine.runAndWait()",How to do text to speech with python on a Toshiba laptop and Windows 7?
Python List to postgresql dict," I have a list: I have to insert it into a postgresql array:(ALTER TABLE ""aTable"" ADD COLUMN ""Test"" text[];)The syntax for adding data to the postgresql is: How can I convert the list to the correct format? <code>  [u'ABC', u'DEF', u'GHI'] update ""aTable"" SET ""Test"" = '{""ABC"", ""DEF"", ""GHI""}'",Python List to PostgreSQL Array
Python class confusion," I'm doing some things in Python (3.3.3), and I came across something that is confusing me since to my understanding classes get a new id each time they are called.Lets say you have this in some .py file: The above returns the same id which is confusing me since I'm calling on it so it shouldn't be the same, right? Is this how Python works when the same class is called twice in a row or not? It gives a different id when I wait a few seconds but if I do it at the same like the example above it doesn't seem to work that way, which is confusing me. It returns the same thing, but why? I also notice it with ranges for example Is there any particular reason for Python doing this when the class is called quickly? I didn't even know Python did this, or is it possibly a bug? If it is not a bug can someone explain to me how to fix it or a method so it generates a different id each time the method/class is called? I'm pretty puzzled on how that is doing it because if I wait, it does change but not if I try to call the same class two or more times. <code>  class someClass: passprint(someClass())print(someClass()) >>> print(someClass());print(someClass())<__main__.someClass object at 0x0000000002D96F98><__main__.someClass object at 0x0000000002D96F98> for i in range(10): print(someClass())",Why is the id of a Python class not unique when called quickly?
How to force python float operation on float32 rather than float64," I want to do some math operations (+, -, *, /) on float32 rather than on float64 type. I need do these operations on number or numpy.array, and also some numpy math functions, such as sqrt mean. How do I do this? <code> ",How to force python float operation on float32 rather than float64?
Check if a chain is in a list," I have these two arrays: And: Is there a way to check if B is a sublist in A with the same exact order of items? <code>  A = [1,2,3,4,5,6,7,8,9,0] B = [4,5,6,7]",Check if all elements of one array is in another array
How to remove project PyCharm?," If I'm closing a project and then just delete the project folder, after PyCharm restarts an empty project folder is created again. <code> ",How to remove project in PyCharm?
Python: how to normalize a confusion matrix?," I calculated a confusion matrix for my classifier using confusion_matrix() from scikit-learn. The diagonal elements of the confusion matrix represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier.I would like to normalize my confusion matrix so that it contains only numbers between 0 and 1. I would like to read the percentage of correctly classified samples from the matrix.I found several methods how to normalize a matrix (row and column normalization) but I don't know much about maths and am not sure if this is the correct approach. <code> ",How to normalize a confusion matrix?
Setting style in PyQt4 using QStyleFactory() from a list of styles using QComboBox()," I've been implementing an application using PyQt4.In this application I want to set the style according to the user's choice, and I want to set the style without restarting the dialog again.Here's my piece of code which is affecting the styling area: But I keep getting only the ""Plastique"" style. <code>  import sysfrom PyQt4.QtCore import *from PyQt4.QtGui import *from PyQt4 import QtGuistyles = [""Plastique"",""Cleanlooks"",""CDE"",""Motif"",""GTK+""]class AppWidget(QWidget): def __init__(self,parent=None): super(AppWidget,self).__init__(parent) global styles # declaring global # I've skipped the useless codes horizontalLayout = QHBoxLayout() self.styleLabel =QLabel(""Set Style:"") self.styleComboBox = QComboBox() self.styleComboBox.addItems(styles) # adding the styles list horizontalLayout.addWidget(self.styleLabel) horizontalLayout.addWidget(self.styleComboBox) # skip more code self.setLayout(layout) def getStyle(self): return self.styleComboBox.currentIndex() # get the current index from combobox # another way i also implement is : # return self.styleComboBox.currentText() # after that i remove the global and directly access using this method # which is of no successif __name__ == ""__main__"": global styles # declaring global app = QApplication(sys.argv) widgetApp = AppWidget() i = widgetApp.getStyle() # assign the index here QtGui.QApplication.setStyle(QtGui.QStyleFactory.create(styles[i])) # setting the style widgetApp.show() app.exec_() print i",Setting style in using QStyleFactory from a list of styles in a QComboBox
How to download datasets for sklearn? - pyhton," In NLTK there is a nltk.download() function to download the datasets that are comes with the NLP suite.In sklearn, it talks about loading data sets (http://scikit-learn.org/stable/datasets/) and fetching datas from http://mldata.org/ but for the rest of the datasets, the instructions were to download from the source.Where should I save the data that I've downloaded from the source? Are there any other steps after I save the data into the correct directory before I can call from my python code?Is there an example of how to download e.g. the 20newsgroups dataset?I've pip installed sklearn and tried this but I got an IOError. Most probably because I haven't downloaded the dataset from the source. <code>  >>> from sklearn.datasets import fetch_20newsgroups>>> fetch_20newsgroups(subset='train')Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/usr/local/lib/python2.7/dist-packages/sklearn/datasets/twenty_newsgroups.py"", line 207, in fetch_20newsgroups cache_path=cache_path) File ""/usr/local/lib/python2.7/dist-packages/sklearn/datasets/twenty_newsgroups.py"", line 89, in download_20newsgroups tarfile.open(archive_path, ""r:gz"").extractall(path=target_dir) File ""/usr/lib/python2.7/tarfile.py"", line 1678, in open return func(name, filemode, fileobj, **kwargs) File ""/usr/lib/python2.7/tarfile.py"", line 1727, in gzopen **kwargs) File ""/usr/lib/python2.7/tarfile.py"", line 1705, in taropen return cls(name, mode, fileobj, **kwargs) File ""/usr/lib/python2.7/tarfile.py"", line 1574, in __init__ self.firstmember = self.next() File ""/usr/lib/python2.7/tarfile.py"", line 2334, in next raise ReadError(""empty file"")tarfile.ReadError: empty file",How to download datasets for sklearn? - python
in python: iterate over each string in a list," Im new to python and i need some help with this.TASK : given a list --> words = ['aba', 'xyz', 'xgx', 'dssd', 'sdjh']i need to compare the first and the last element of each string in the list, if the first and the last element in the string is the same , then increment the count.Given list is : If i try manually, i can iterate over each element of the strings in the list. But, When i try to iterate over all the elements of all the strings in the list , using a FOR loop.i get an error. ERROR: please let me know, how would i achieve comparing the first and the last element of a no. strings in the list.Thanks in advance. <code>  words = ['aba', 'xyz', 'xgx', 'dssd', 'sdjh'] words = ['aba', 'xyz', 'xgx', 'dssd', 'sdjh']w1 = words[0]print w1abafor i in w1: print iabaif w1[0] == w1[len(w1) - 1]: c += 1 print c1 words = ['aba', 'xyz', 'xgx', 'dssd', 'sdjh']c = 0for i in words: w1 = words[i] if w1[0] == w1[len(w1) - 1]: c += 1 print c Traceback (most recent call last): File ""<stdin>"", line 2, in <module>TypeError: list indices must be integers, not str",How to iterate over each string in a list of strings and operate on it's elements
Simple evaluation of a 2D KDE with sklearn," I'm attempting to compare the performance of sklearn.neighbors.KernelDensity versus scipy.stats.gaussian_kde for a two dimensional array.From this article I see that the bandwidths (bw) are treated differently in each function. The article gives a recipe for setting the correct bw in scipy so it will be equivalent to the one used in sklearn . Basically it divides the bw by the sample standard deviation. The result is this: where x is the sample array I'm using to obtain the KDE. This works just fine in 1D, but I can't make it work in 2D.Here's a MWE of what I got: ( iso2 is presented as an exponential since sklearn returns the log values)The results I get for iso1 and iso2 are different and I'm lost as to how should I affect the bandwidth (in either function) to make them equal (as they should).AddI was advised over at sklearn chat (by ep) that I should scale the values in (x,y) before calculating the kernel with scipy in order to obtain comparable results with sklearn.So this is what I did: ie: I scaled both dimensions before getting the kernel with scipy while leaving the line that obtains the kernel in sklearn untouched.This gave better results but there's still differences in the kernels obtained:where the red dot is the (x1,y1) point in the code. So as can be seen, there are still differences in the shapes of the density estimates, albeit very small ones. Perhaps this is the best that can be achieved? <code>  # For sklearnbw = 0.15# For scipybw = 0.15/x.std(ddof=1) import numpy as npfrom scipy import statsfrom sklearn.neighbors import KernelDensity# Generate random data.n = 1000m1, m2 = np.random.normal(0.2, 0.2, size=n), np.random.normal(0.2, 0.2, size=n)# Define limits.xmin, xmax = min(m1), max(m1)ymin, ymax = min(m2), max(m2)# Format data.x, y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]positions = np.vstack([x.ravel(), y.ravel()])values = np.vstack([m1, m2])# Define some point to evaluate the KDEs.x1, y1 = 0.5, 0.5# -------------------------------------------------------# Perform a kernel density estimate on the data using scipy.kernel = stats.gaussian_kde(values, bw_method=0.15/np.asarray(values).std(ddof=1))# Get KDE value for the point.iso1 = kernel((x1,y1))print 'iso1 = ', iso[0]# -------------------------------------------------------# Perform a kernel density estimate on the data using sklearn.kernel_sk = KernelDensity(kernel='gaussian', bandwidth=0.15).fit(zip(*values))# Get KDE value for the point.iso2 = kernel_sk.score_samples([[x1, y1]])print 'iso2 = ', np.exp(iso2[0]) # Scale values.x_val_sca = np.asarray(values[0])/np.asarray(values).std(axis=1)[0]y_val_sca = np.asarray(values[1])/np.asarray(values).std(axis=1)[1]values = [x_val_sca, y_val_sca]kernel = stats.gaussian_kde(values, bw_method=bw_value)",Relation between 2D KDE bandwidth in sklearn vs bandwidth in scipy
How to create equal-width columns in Python 2.7 with Tkinter," How can I force the columns in a Tkinter application window to be of equal width?The tkdocs website states as follows:The width of each column (or height of each row) depends on the width or height of the widgets contained within the column or row. This means when sketching out your user interface, and dividing it into rows and columns, you don't need to worry about each column or row being equal width [or height, presumably].TkDocs tutorial: The Grid Geometry ManagerBut I want the columns to be equal width, preferably by making the width of all columns depend upon the widest widget in any column. Is there a way of achieving this cleanly (i.e. not by playing around with cell padding until I get them all the same by trial and error or by arbitrarily assigning an apparently adequate minimum width to every column)? Also, can it be selectively done for some, but not all columns in a grid (e.g. so that columns X and Y are are sized according to the widest widget in column X or Y, but column Z is sized according to the widest widget in column Z)? <code> ",How to create equal-width grid columns with Tkinter?
Using Flask-Mail to send Email - socket.gaierr," I am building a simple contact page using Flask and Flask-Mail. I built the app following this tutorial - Add a contact page - and now when I try to send the message I receive the eror gaierror: [Errno -2] Name or service not known. I have been googling the error for a while and can't find any similar examples on line. I can't even figure out what name or service it can't find. The traceback page will let me expand a row and execute some Python code. It provides a dump() function that will show me all the variables and can be called on objects to see their info if that will help.routes.py : the form: the traceback: <code>  from forms import ContactFormfrom flask.ext.mail import Message, Mailmail = Mail()app = Flask(__name__)app.secret_key = 'development key'app.config['MAIL_SERVER'] = 'smtp.google.com'app.config['MAIL_PORT'] = 465app.config['MAIL_USE_SSL'] = Trueapp.config['MAIL_USERNAME'] = 'email'app.config['MAIL_PASSWORD'] = 'password'mail.init_app(app)@app.route('/contact', methods=['GET', 'POST'])def contact(): form = ContactForm() if request.method == 'POST': if not form.validate(): from flask.ext.wtf import Form, validatorsfrom wtforms.fields import TextField, TextAreaField, SubmitFieldimport wtformsclass ContactForm(Form): name = TextField(""Name"", [wtforms.validators.Required('Please enter your name')]) email = TextField(""Email"", [wtforms.validators.Required('Please enter your email'), wtforms.validators.Email()]) subject = TextField(""Subject"", [wtforms.validators.Required('Please enter a subject')]) message = TextAreaField(""Message"", [wtforms.validators.Required('Please enter a message')]) submit = SubmitField(""Send"") flash('All fields are required.') return render_template('contact.html', form=form) else: msg = Message(form.subject.data, sender='imauld@gmail.com', recipients=['imauld@gmail.com']) msg.body = """"""From: %s &lt;%s&gt; %s"""""" % (form.name.data, form.email.data, form.message.data) mail.send(msg) return render_template('contact.html', success=True) elif request.method == 'GET': return render_template('contact.html', form=form) File ""/home/ian/PycharmProjects/flaskapp/lib/python2.7/site-packages/flask/app.py"", line 1836, in __call__return self.wsgi_app(environ, start_response)File ""/home/ian/PycharmProjects/flaskapp/lib/python2.7/site-packages/flask/app.py"", line 1820, in wsgi_appresponse = self.make_response(self.handle_exception(e))File ""/home/ian/PycharmProjects/flaskapp/lib/python2.7/site-packages/flask/app.py"", line 1403, in handle_exceptionreraise(exc_type, exc_value, tb)File ""/home/ian/PycharmProjects/flaskapp/lib/python2.7/site-packages/flask/app.py"", line 1817, in wsgi_appresponse = self.full_dispatch_request()File ""/home/ian/PycharmProjects/flaskapp/lib/python2.7/site-packages/flask/app.py"", line 1477, in full_dispatch_requestrv = self.handle_user_exception(e)File ""/home/ian/PycharmProjects/flaskapp/lib/python2.7/site-packages/flask/app.py"", line 1381, in handle_user_exceptionreraise(exc_type, exc_value, tb)File ""/home/ian/PycharmProjects/flaskapp/lib/python2.7/site-packages/flask/app.py"", line 1475, in full_dispatch_requestrv = self.dispatch_request()File ""/home/ian/PycharmProjects/flaskapp/lib/python2.7/site-packages/flask/app.py"", line 1461, in dispatch_requestreturn self.view_functions[rule.endpoint](**req.view_args)File ""/home/ian/PycharmProjects/flaskapp/app/routes.py"", line 39, in contactmail.send(msg)File ""/home/ian/PycharmProjects/flaskapp/lib/python2.7/site-packages/flask_mail.py"", line 415, in sendwith self.connect() as connection:File ""/home/ian/PycharmProjects/flaskapp/lib/python2.7/site-packages/flask_mail.py"", line 123, in __enter__self.host = self.configure_host()File ""/home/ian/PycharmProjects/flaskapp/lib/python2.7/site-packages/flask_mail.py"", line 135, in configure_hosthost = smtplib.SMTP_SSL(self.mail.server, self.mail.port)File ""/usr/lib/python2.7/smtplib.py"", line 776, in __init__SMTP.__init__(self, host, port, local_hostname, timeout)File ""/usr/lib/python2.7/smtplib.py"", line 249, in __init__(code, msg) = self.connect(host, port)File ""/usr/lib/python2.7/smtplib.py"", line 309, in connectself.sock = self._get_socket(host, port, self.timeout)File ""/usr/lib/python2.7/smtplib.py"", line 781, in _get_socketnew_socket = socket.create_connection((host, port), timeout)File ""/usr/lib/python2.7/socket.py"", line 553, in create_connectionfor res in getaddrinfo(host, port, 0, SOCK_STREAM):gaierror: [Errno -2] Name or service not known",Using Flask-Mail to send Email through Gmail- socket.gaierr
"Python and ""arbitrary precision integers"""," Python is supposed to have ""arbitrary precision integers,"" according to the answer in Python integer ranges. But this result is plainly not arbitrary precision: According to PEP 237, bignum is arbitrarily large (not just the size of C's long type). And Wikipedia says Python's bignum is arbitrary precision.So why the incorrect result from the above line of code? <code>  $ python -c 'print(""%d"" % (999999999999999999999999/3))'333333333333333327740928","Is Python incorrectly handling this ""arbitrary precision integer""?"
Calculate bandwidth usage per IP with scapy," I'm using scapy to sniff a mirror port and generate a list of the top 10 ""talkers"", i.e. a list of hosts using the most bandwidth on my network. I'm aware of tools already available such as iftop and ntop, but I need more control over the output.The following script samples traffic for 30 seconds and then prints a list of the top 10 talkers in the format ""source host -> destination host: bytes"". That's great, but how can I calculate average bytes per second?I got the sense that changing sample_interval down to 1 second doesn't allow for a good sampling of traffic, so it seems I need to average it out. So I tried this at the end of the script: bytes per second = (total bytes / sample_interval)but the resulting Bytes/s seems much lower. For example, I generated an rsync between two hosts at a throttled rate of 1.5 MB/s, but using the above average calculation, my script kept calculating the rate between these hosts as around 200 KB/s...much lower than 1.5 MB/s as I'd expect. I can confirm with iftop that 1.5 MB/s is in fact the rate between these two hosts.Am I totaling up packet lengths incorrectly with scapy (see traffic_monitor_callbak function)? Or is this a poor solution altogether :)? I'm not sure if this is a programming (scapy/python) question or more of a general networking question, so I'm calling it a network programming question. <code>  from scapy.all import *from collections import defaultdictimport socketfrom pprint import pprintfrom operator import itemgettersample_interval = 30 # how long to capture traffic, in seconds# initialize traffic dicttraffic = defaultdict(list)# return human readable units given bytesdef human(num): for x in ['bytes','KB','MB','GB','TB']: if num < 1024.0: return ""%3.1f %s"" % (num, x) num /= 1024.0# callback function to process each packet# get total packets for each source->destination combodef traffic_monitor_callbak(pkt): if IP in pkt: src = pkt.sprintf(""%IP.src%"") dst = pkt.sprintf(""%IP.dst%"") size = pkt.sprintf(""%IP.len%"") # initialize if (src, dst) not in traffic: traffic[(src, dst)] = 0 else: traffic[(src, dst)] += int(size)sniff(iface=""eth1"", prn=traffic_monitor_callbak, store=0, timeout=sample_interval)# sort by total bytes, descendingtraffic_sorted = sorted(traffic.iteritems(), key=itemgetter(1), reverse=True) # print top 10 talkersfor x in range(0, 10): src = traffic_sorted[x][0][0] dst = traffic_sorted[x][0][1] host_total = traffic_sorted[x][3] # get hostname from IP try: src_hostname = socket.gethostbyaddr(src) except: src_hostname = src try: dst_hostname = socket.gethostbyaddr(dst) except: dst_hostname = dst print ""%s: %s (%s) -> %s (%s)"" % (human(host_total), src_hostname[0], src, dst_hostname[0], dst)","Calculate bandwidth usage per IP with scapy, iftop-style"
pip install pycurl - ssl backend error," I was trying to install pycurl in a virtualenv using pip and I got this error I read some documentation saying that ""To fix this, you need to tell setup.py what SSL backend is used"" (source) although I am not sure how to do this since I installed pycurl using pip.How can I specify the SSL backend when installing pycurl with pip?Thanks <code>  ImportError: pycurl: libcurl link-time ssl backend (openssl) is different from compile-time ssl backend (none/other)",SSL backend error when using OpenSSL
"""TypeError: dist must be a Distribution instance"" when resolving BeautifulSoup dependency"," My package depends on BeautifulSoup. If I install my package in a fresh virtualenv via python setup.py develop, I get the following error. If I execute python setup.py develop a second time, everything seems to work fine. I have no idea, what's happening. How to fix it to get a reproducable setup? <code>  Best match: beautifulsoup4 4.3.2Downloading https://pypi.python.org/packages/source/b/beautifulsoup4/beautifulsoup4-4.3.2.tar.gz#md5=b8d157a204d56512a4cc196e53e7d8eeProcessing beautifulsoup4-4.3.2.tar.gzWriting /tmp/easy_install-1eBfi3/beautifulsoup4-4.3.2/setup.cfgRunning beautifulsoup4-4.3.2/setup.py -q bdist_egg --dist-dir /tmp/easy_install-1eBfi3/beautifulsoup4-4.3.2/egg-dist-tmp-YmoFSqTraceback (most recent call last): File ""setup.py"", line 73, in <module> """""", File ""/usr/lib/python2.7/distutils/core.py"", line 152, in setup dist.run_commands() File ""/usr/lib/python2.7/distutils/dist.py"", line 953, in run_commands self.run_command(cmd) File ""/usr/lib/python2.7/distutils/dist.py"", line 972, in run_command cmd_obj.run() File ""/home/domma/VirtualEnvs/orcid/local/lib/python2.7/site-packages/setuptools/command/develop.py"", line 27, in run self.install_for_development() File ""/home/domma/VirtualEnvs/orcid/local/lib/python2.7/site-packages/setuptools/command/develop.py"", line 129, in install_for_development self.process_distribution(None, self.dist, not self.no_deps) File ""/home/domma/VirtualEnvs/orcid/local/lib/python2.7/site-packages/setuptools/command/easy_install.py"", line 671, in process_distribution [requirement], self.local_index, self.easy_install File ""/home/domma/VirtualEnvs/orcid/local/lib/python2.7/site-packages/pkg_resources.py"", line 564, in resolve dist = best[req.key] = env.best_match(req, self, installer) File ""/home/domma/VirtualEnvs/orcid/local/lib/python2.7/site-packages/pkg_resources.py"", line 802, in best_match return self.obtain(req, installer) # try and download/install File ""/home/domma/VirtualEnvs/orcid/local/lib/python2.7/site-packages/pkg_resources.py"", line 814, in obtain return installer(requirement) File ""/home/domma/VirtualEnvs/orcid/local/lib/python2.7/site-packages/setuptools/command/easy_install.py"", line 593, in easy_install return self.install_item(spec, dist.location, tmpdir, deps) File ""/home/domma/VirtualEnvs/orcid/local/lib/python2.7/site-packages/setuptools/command/easy_install.py"", line 623, in install_item dists = self.install_eggs(spec, download, tmpdir) File ""/home/domma/VirtualEnvs/orcid/local/lib/python2.7/site-packages/setuptools/command/easy_install.py"", line 809, in install_eggs return self.build_and_install(setup_script, setup_base) File ""/home/domma/VirtualEnvs/orcid/local/lib/python2.7/site-packages/setuptools/command/easy_install.py"", line 1015, in build_and_install self.run_setup(setup_script, setup_base, args) File ""/home/domma/VirtualEnvs/orcid/local/lib/python2.7/site-packages/setuptools/command/easy_install.py"", line 1000, in run_setup run_setup(setup_script, args) File ""/home/domma/VirtualEnvs/orcid/local/lib/python2.7/site-packages/setuptools/sandbox.py"", line 50, in run_setup lambda: execfile( File ""/home/domma/VirtualEnvs/orcid/local/lib/python2.7/site-packages/setuptools/sandbox.py"", line 100, in run return func() File ""/home/domma/VirtualEnvs/orcid/local/lib/python2.7/site-packages/setuptools/sandbox.py"", line 52, in <lambda> {'__file__':setup_script, '__name__':'__main__'} File ""setup.py"", line 27, in <module> 'raven', File ""/usr/lib/python2.7/distutils/core.py"", line 152, in setup dist.run_commands() File ""/usr/lib/python2.7/distutils/dist.py"", line 953, in run_commands self.run_command(cmd) File ""/usr/lib/python2.7/distutils/dist.py"", line 970, in run_command cmd_obj = self.get_command_obj(command) File ""/usr/lib/python2.7/distutils/dist.py"", line 846, in get_command_obj cmd_obj = self.command_obj[command] = klass(self) File ""/home/domma/VirtualEnvs/orcid/local/lib/python2.7/site-packages/setuptools/__init__.py"", line 69, in __init__ _Command.__init__(self,dist) File ""/usr/lib/python2.7/distutils/cmd.py"", line 59, in __init__ raise TypeError, ""dist must be a Distribution instance""TypeError: dist must be a Distribution instance",TypeError: dist must be a Distribution instance
How to add ipython syntax highlighting to pygmentize," I need syntax highlighting in a latex project, using the minted package I can use the pygments lexers, there is a lexer for ipython here.How can I add this to pygments ?? (windows or OSX) <code> ",How to add ipython syntax highlighting to pygmentize?
How to redirect the raw_input to stderr not stdout, I want to redirect the stdout to a file. But This will affect the raw_input. I need to redirect the output of raw_input to stderr instead of stdout. How can I do that? <code> ,How to redirect the raw_input to stderr and not stdout?
Trouble with SqlAlchemy in Python," I have a database that I don't have metadata or orm classes for (the database already exists).I managed to get the select stuff working by: So far so good.The only way I can get an update/insert working is by executing a raw query like: I would like to make it so I can make a class out of that like: Tried (this is just one of the approaches I tried): I can't get that to execute on: or Any suggestion how to construct an insert or update without writing ORM models? <code>  from sqlalchemy.sql.expression import ColumnClausefrom sqlalchemy.sql import table, column, select, update, insertfrom sqlalchemy.ext.declarative import *from sqlalchemy.orm import sessionmakerfrom sqlalchemy import create_engineimport pyodbcdb = create_engine('mssql+pyodbc://pytest')Session = sessionmaker(bind=db)session = Session()list = []list.append (column(""field1""))list.append (column(""field2""))list.append (column(""field3""))s = select(list)s.append_from('table')s.append_whereclause(""field1 = 'abc'"")s = s.limit(10)result = session.execute(s)out = result.fetchall()print(out) session.execute(<Some sql>) u = Update(""table"")u.Set(""file1"",""some value"")u.Where(<some conditon>)seasion.execute(u) i = insert(""table"")v = i.values([{""name"":""name1""}, {""name"":""name2""}])u = update(""table"")u = u.values({""name"": ""test1""}) session.execute(i) session.execute(u)",Insert and update with core SQLAlchemy
python alternative of below C code," I have some C code that works well: data inside EMPLOYEE.DAT: I'm having trouble translating this code to Python: Is there any way to implement that in Python? Furthermore, what are Pythonic alternatives of exit() & EOF? <code>  #include<stdio.h>#include<stdlib.h>int main(){ FILE *fp; struct emp { char name[40]; int age; float bs; }; struct emp e; fp=fopen(""EMPLOYEE.DAT"",""r""); if(fp==NULL) { puts(""Cannot open file""; exit(1); } while(fscanf(f,""%s %d %f"",&e.name,&e.age,&e.bs)!=EOF) printf(""%s %d %f\n"",e.name,e.age,e.bs); fclose(fp); return 0;} Sunil 34 1250.50Sameer 21 1300.50rahul 34 1400.50 while(fscanf(f,""%s %d %f"",&e.name,&e.age,&e.bs)!=EOF) printf(""%s %d %f\n"",e.name,e.age,e.bs);",Python alternative to fscanf C code
Python alternative of below C code," I have some C code that works well: data inside EMPLOYEE.DAT: I'm having trouble translating this code to Python: Is there any way to implement that in Python? Furthermore, what are Pythonic alternatives of exit() & EOF? <code>  #include<stdio.h>#include<stdlib.h>int main(){ FILE *fp; struct emp { char name[40]; int age; float bs; }; struct emp e; fp=fopen(""EMPLOYEE.DAT"",""r""); if(fp==NULL) { puts(""Cannot open file""; exit(1); } while(fscanf(f,""%s %d %f"",&e.name,&e.age,&e.bs)!=EOF) printf(""%s %d %f\n"",e.name,e.age,e.bs); fclose(fp); return 0;} Sunil 34 1250.50Sameer 21 1300.50rahul 34 1400.50 while(fscanf(f,""%s %d %f"",&e.name,&e.age,&e.bs)!=EOF) printf(""%s %d %f\n"",e.name,e.age,e.bs);",Python alternative to fscanf C code
vertical bar in Python, There is a code and in class' method there is a line: I can't understand what it means. I didn't find (|=) in the list of basic Python operators. <code>  object.attribute |= variable,Vertical bar in Python bitwise assignment operator
Djnago + Boto + Python 3," How can I store my Django uploaded files on S3 using Python 3 on EC2 Amazon Linux? If I can't, how can I share uploaded files between 2 EC2 instances if I'm using ELB?I tried to use the django-storages-py3 + boto#py3kport but it doesn't work, when I'm trying to upload files I get an exception: string expected bytes givenUPDATE:This is how I'm using the django-storages-py3 + boto#py3kport fd - S3BotoStorageFile: uploadstg/a6d2532d-34c9-4793-9d43-e9a3e475fc6f.pngfile - InMemoryUploadedFile: 1.png (image/png)Traceback Upload using Django admin: <code>  from django.core.files.storage import default_storagefd = default_storage.open('%s/%s' % ('uploadstg', str(filename)), 'wb')for chunk in file.chunks(): fd.write(chunk)fd.close() Environment:Request Method: POSTRequest URL: http://---/items/Tpp/create/Django Version: 1.6.1Python Version: 3.3.3Installed Applications:('django.contrib.admin', 'haystack', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'django.contrib.sites', 'modeltranslation', 'south', 'core', 'storages', 'appl')Installed Middleware:('django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', 'tpp.SiteUrlMiddleWare.SiteUrlMiddleWare')Traceback:File ""/usr/local/bin/test/lib/python3.3/site-packages/django/core/handlers/base.py"" in get_response 114. response = wrapped_callback(request, *callback_args, **callback_kwargs)File ""/var/www/html/tpp/tppcenter/views.py"" in get_item_form 95. com = form.save(request.user)File ""/var/www/html/tpp/tppcenter/forms.py"" in save 212. self._save_file(self.fields[title].initial, title, path_to_images)File ""/var/www/html/tpp/tppcenter/forms.py"" in _save_file 239. fd.write(chunk)File ""/usr/local/bin/test/lib/python3.3/site-packages/django_storages-1.1.8-py3.3.egg/storages/backends/s3boto.py"" in write 161. return super(S3BotoStorageFile, self).write(*args, **kwargs)Exception Type: TypeError at /items/Tpp/create/Exception Value: string argument expected, got 'bytes' Environment:Request Method: POSTRequest URL: http://----/admin/core/user/1/Django Version: 1.6.1Python Version: 3.3.3Installed Applications:('django.contrib.admin', 'haystack', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'django.contrib.sites', 'modeltranslation', 'south', 'core', 'storages', 'appl')Installed Middleware:('django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware', 'tpp.SiteUrlMiddleWare.SiteUrlMiddleWare')Traceback:File ""/usr/local/bin/test/lib/python3.3/site-packages/django/core/handlers/base.py"" in get_response 114. response = wrapped_callback(request, *callback_args, **callback_kwargs)File ""/usr/local/bin/test/lib/python3.3/site-packages/django/contrib/admin/options.py"" in wrapper 432. return self.admin_site.admin_view(view)(*args, **kwargs)File ""/usr/local/bin/test/lib/python3.3/site-packages/django/utils/decorators.py"" in _wrapped_view 99. response = view_func(request, *args, **kwargs)File ""/usr/local/bin/test/lib/python3.3/site-packages/django/views/decorators/cache.py"" in _wrapped_view_func 52. response = view_func(request, *args, **kwargs)File ""/usr/local/bin/test/lib/python3.3/site-packages/django/contrib/admin/sites.py"" in inner 198. return view(request, *args, **kwargs)File ""/usr/local/bin/test/lib/python3.3/site-packages/django/utils/decorators.py"" in _wrapper 29. return bound_func(*args, **kwargs)File ""/usr/local/bin/test/lib/python3.3/site-packages/django/utils/decorators.py"" in _wrapped_view 99. response = view_func(request, *args, **kwargs)File ""/usr/local/bin/test/lib/python3.3/site-packages/django/utils/decorators.py"" in bound_func 25. return func(self, *args2, **kwargs2)File ""/usr/local/bin/test/lib/python3.3/site-packages/django/db/transaction.py"" in inner 339. return func(*args, **kwargs)File ""/usr/local/bin/test/lib/python3.3/site-packages/django/contrib/admin/options.py"" in change_view 1230. self.save_model(request, new_object, form, True)File ""/usr/local/bin/test/lib/python3.3/site-packages/django/contrib/admin/options.py"" in save_model 860. obj.save()File ""/usr/local/bin/test/lib/python3.3/site-packages/django/db/models/base.py"" in save 545. force_update=force_update, update_fields=update_fields)File ""/usr/local/bin/test/lib/python3.3/site-packages/django/db/models/base.py"" in save_base 573. updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)File ""/usr/local/bin/test/lib/python3.3/site-packages/django/db/models/base.py"" in _save_table 632. for f in non_pks]File ""/usr/local/bin/test/lib/python3.3/site-packages/django/db/models/base.py"" in <listcomp> 632. for f in non_pks]File ""/usr/local/bin/test/lib/python3.3/site-packages/django/db/models/fields/files.py"" in pre_save 252. file.save(file.name, file, save=False)File ""/usr/local/bin/test/lib/python3.3/site-packages/django/db/models/fields/files.py"" in save 86. self.name = self.storage.save(name, content)File ""/usr/local/bin/test/lib/python3.3/site-packages/django/core/files/storage.py"" in save 49. name = self._save(name, content)File ""/usr/local/bin/test/lib/python3.3/site-packages/django_storages-1.1.8-py3.3.egg/storages/backends/s3boto.py"" in _save 392. self._save_content(key, content, headers=headers)File ""/usr/local/bin/test/lib/python3.3/site-packages/django_storages-1.1.8-py3.3.egg/storages/backends/s3boto.py"" in _save_content 403. rewind=True, **kwargs)File ""/usr/local/bin/test/src/boto/build/lib/boto/s3/key.py"" in set_contents_from_file 1241. chunked_transfer=chunked_transfer, size=size)File ""/usr/local/bin/test/src/boto/build/lib/boto/s3/key.py"" in send_file 726. chunked_transfer=chunked_transfer, size=size)File ""/usr/local/bin/test/src/boto/build/lib/boto/s3/key.py"" in _send_file_internal 893. if self.base64md5:File ""/usr/local/bin/test/src/boto/build/lib/boto/s3/key.py"" in _get_base64md5 177. return binascii.b2a_base64(self.local_hashes['md5']).rstrip('\n')Exception Type: TypeError at /admin/core/user/1/Exception Value: Type str doesn't support the buffer API",Django + Boto + Python 3
Pandas: find column whose name contains a specific string," I have a dataframe with column names, and I want to find the one that contains a certain string, but does not exactly match it. I'm searching for 'spike' in column names like 'spike-2', 'hey spike', 'spiked-in' (the 'spike' part is always continuous). I want the column name to be returned as a string or a variable, so I access the column later with df['name'] or df[name] as normal. I've tried to find ways to do this, to no avail. Any tips? <code> ",Find column whose name contains a specific string
Start ipython qtconsole als interactive interpreter after script execution," I have ipython with qtconsole installed and can start it via ipython qtconsole. I can also run a script via ipython -i my_script.py to stay in the interactive interpreter after the script finishes or if an exception is thrown. But I could not figure out how to combine them: I would like to do ipython -i qtconsole my_script.py but whatever I try, it complains about invalid flags. Any hint how to do that? <code> ",Start ipython qtconsole as interactive interpreter after script execution
"Python: What ""TypeError 'xxx' object is not callable"" means?"," As a starting developer in Python I've seen this error message many times appearing in my console but I don't fully understand what does it means.Could anyone tell me, in a general way, what kind of action produces this error? <code> ","What does ""TypeError 'xxx' object is not callable"" means?"
Python TkInter refresh canvas," Hello I have a tuple in python with colours that are related to squares that are drawn in the canvas by the following dictionary: To be more specific for example a node at the tuple is: This means that 4 squares should be drawn this way: and then their colours should be changed accordingly to the next node in my tupleTo do this I iterate the tuple and for each element I draw a new rectangle at the canvas and then I call the time.sleep() function in order to give time to the user to see the differences to the previous state.My problem is that only the last node is rendered correctly while all the others aren't shown. Can you help me?Here is my code so far: All in all, I try to make an animation described above. Is there anything I don't think correctly or something to refresh the canvas at each loop? <code>  colour_mapping = {0: ""red"", 1: ""green"", 2: ""blue"" , 3:""purple""} ((2, 3), (3, 3)) blue square purple squarepurple square purple square self.parent.title(""AlienTiles"")self.style = Style()self.style.theme_use(""default"")self.frame = Frame(self, relief=RAISED, borderwidth=1)self.frame.pack(fill=BOTH, expand=1)self.canvas = Canvas(self.frame)self.canvas.pack(fill=BOTH, expand=1)self.pack(fill=BOTH, expand=1)for i in range(len(path)) : #the tuple is path state = path[i].state print state time.sleep(1) y_offset=10 for x in state: start_x=40 start_y=10 i=1 x_offset=0 for y in x: x0=(start_x*i)+x_offset y0=(start_y*i)+y_offset x1=x0+size y1=y0+size colour=colour_mapping[y] print colour self.canvas.create_rectangle(x0, y0, x1, y1, fill=colour) x_offset=x_offset+size+10 y_offset=y_offset+size+10",Python Tkinter refresh canvas
Flask app-object suddenly an Boolean," I am trying to debug a view in my Flask app that is return a 500 status with the error TypeError: 'bool' object is not callable in the traceback. The view calls login_user from Flask-Login then returns True to indicate that the login was successful.I have debugged until app_iter = app(environ, start_response) and the app is now a boolean with the value True rather than the Flask app object.  <code>  Traceback (most recent call last): File ""D:\Python27\lib\site-packages\flask\app.py"", line 1836, in __call__ return self.wsgi_app(environ, start_response) File ""D:\Python27\lib\site-packages\flask\app.py"", line 1820, in wsgi_app response = self.make_response(self.handle_exception(e)) File ""D:\Python27\lib\site-packages\flask\app.py"", line 1403, in handle_exception reraise(exc_type, exc_value, tb) File ""D:\Python27\lib\site-packages\flask\app.py"", line 1817, in wsgi_app response = self.full_dispatch_request() File ""D:\Python27\lib\site-packages\flask\app.py"", line 1478, in full_dispatch_request response = self.make_response(rv) File ""D:\Python27\lib\site-packages\flask\app.py"", line 1577, in make_response rv = self.response_class.force_type(rv, request.environ) File ""D:\Python27\lib\site-packages\werkzeug\wrappers.py"", line 824, in force_type response = BaseResponse(*_run_wsgi_app(response, environ)) File ""D:\Python27\lib\site-packages\werkzeug\wrappers.py"", line 57, in _run_wsgi_app return _run_wsgi_app(*args) File ""D:\Python27\lib\site-packages\werkzeug\test.py"", line 854, in run_wsgi_app app_iter = app(environ, start_response)TypeError: 'bool' object is not callable @app.route('/login', methods=['POST'])def login(): username = request.form['username'] user = User.query.filter_by(username=username).first() if user: login_user(user) return True return False","Flask view raises TypeError: object is not callable, or says that it did not return a response"
Flask view raises TypeError: 'bool' object is not callable," I am trying to debug a view in my Flask app that is return a 500 status with the error TypeError: 'bool' object is not callable in the traceback. The view calls login_user from Flask-Login then returns True to indicate that the login was successful.I have debugged until app_iter = app(environ, start_response) and the app is now a boolean with the value True rather than the Flask app object.  <code>  Traceback (most recent call last): File ""D:\Python27\lib\site-packages\flask\app.py"", line 1836, in __call__ return self.wsgi_app(environ, start_response) File ""D:\Python27\lib\site-packages\flask\app.py"", line 1820, in wsgi_app response = self.make_response(self.handle_exception(e)) File ""D:\Python27\lib\site-packages\flask\app.py"", line 1403, in handle_exception reraise(exc_type, exc_value, tb) File ""D:\Python27\lib\site-packages\flask\app.py"", line 1817, in wsgi_app response = self.full_dispatch_request() File ""D:\Python27\lib\site-packages\flask\app.py"", line 1478, in full_dispatch_request response = self.make_response(rv) File ""D:\Python27\lib\site-packages\flask\app.py"", line 1577, in make_response rv = self.response_class.force_type(rv, request.environ) File ""D:\Python27\lib\site-packages\werkzeug\wrappers.py"", line 824, in force_type response = BaseResponse(*_run_wsgi_app(response, environ)) File ""D:\Python27\lib\site-packages\werkzeug\wrappers.py"", line 57, in _run_wsgi_app return _run_wsgi_app(*args) File ""D:\Python27\lib\site-packages\werkzeug\test.py"", line 854, in run_wsgi_app app_iter = app(environ, start_response)TypeError: 'bool' object is not callable @app.route('/login', methods=['POST'])def login(): username = request.form['username'] user = User.query.filter_by(username=username).first() if user: login_user(user) return True return False","Flask view raises TypeError: object is not callable, or says that it did not return a response"
Effeciently enumerating all simple paths in networkx," I am attempting to do some graph analysis on the Dewey Decimal Classification, so that I can make a distance between two books. The DDC has several relations: ""hierarchy"", ""see-also"", ""class-elsewhere"", here I represent them with different colours. Since these relations are not symmetric you will notice we have a Directed Graph. Below is a picture of the graph of all vertices maximum 4 edges away from 394.1.The distance metric between classifications A and B, should be the shortest path between A and B. However the colours have no inherent weighted value or preference. But the user will give provide one. So given a dictionary of weights, example: I would like to return the weighted shortest path. I thought that would not be a problem if I could preprocess all simple paths between the two nodes, and then find which was the shortest given the weights dict. However since the graph contains >50,000 nodes. Computing nx.all_simple_paths(G, a, b) has not returned after 24 hours of computation. Are there any suggestions for parallelizing finding all_simple_paths. Or a technique to compute the shortest path given the weights_dict that does not involve computing all_simple_paths? <code>  weights_dict_test = {'notational_hiearchy':1, 'see_reference':0.5, 'class_elsewhere':2}",Efficiently enumerating all simple paths of DiGraph in networkx
Effeciently enumerating all simple paths of DiGraph in networkx," I am attempting to do some graph analysis on the Dewey Decimal Classification, so that I can make a distance between two books. The DDC has several relations: ""hierarchy"", ""see-also"", ""class-elsewhere"", here I represent them with different colours. Since these relations are not symmetric you will notice we have a Directed Graph. Below is a picture of the graph of all vertices maximum 4 edges away from 394.1.The distance metric between classifications A and B, should be the shortest path between A and B. However the colours have no inherent weighted value or preference. But the user will give provide one. So given a dictionary of weights, example: I would like to return the weighted shortest path. I thought that would not be a problem if I could preprocess all simple paths between the two nodes, and then find which was the shortest given the weights dict. However since the graph contains >50,000 nodes. Computing nx.all_simple_paths(G, a, b) has not returned after 24 hours of computation. Are there any suggestions for parallelizing finding all_simple_paths. Or a technique to compute the shortest path given the weights_dict that does not involve computing all_simple_paths? <code>  weights_dict_test = {'notational_hiearchy':1, 'see_reference':0.5, 'class_elsewhere':2}",Efficiently enumerating all simple paths of DiGraph in networkx
How do I change the range of the x-axis with datetimes in MatPlotLib?," I'm trying to plot a graph of dates on the x-axis and values on the y-axis. It works fine, except that I can't get the range of the x-axis to be appropriate. The x-axis range is always Jan 2012 to Jan 2016, despite my dates being from today. I am even specifying that xlim should be the first and last date. I'm writing this for python-django, if that's relevant. And here is the output: <code>  import datetime import matplotlib.pyplot as plt x = [datetime.date(2014, 1, 29), datetime.date(2014, 1, 29), datetime.date(2014, 1, 29)] y = [2, 4, 1] fig, ax = plt.subplots() ax.plot_date(x, y) ax.set_xlim([x[0], x[-1]]) canvas = FigureCanvas(plt.figure(1)) response = HttpResponse(content_type='image/png') canvas.print_png(response) return response",How do I change the range of the x-axis with datetimes in matplotlib?
python requests too many redirects," I'm trying to fetch a page using python requests with this code in views.py: I get the error Exceeded 30 redirects. My app is a Google App Engine Django app. I've gotten this code to work in the python console and a django server on pythonanywhere.com, but for some reason it isn't working on google app engine. What could be causing this? ThanksEdit: It seems like there is another issue with the Requests module in my app. I have this code to add an email to a mailchimp list: but if fails with the error HTTPS/SSL is required <code>  s = requests.Session()r = s.get(""https://www.23andme.com/"") m = mailchimp.Mailchimp(MAILCHIMP_API_KEY)list_response = m.lists.list()",python requests not working with google app engine
python requests too many redirects when using django," I'm trying to fetch a page using python requests with this code in views.py: I get the error Exceeded 30 redirects. My app is a Google App Engine Django app. I've gotten this code to work in the python console and a django server on pythonanywhere.com, but for some reason it isn't working on google app engine. What could be causing this? ThanksEdit: It seems like there is another issue with the Requests module in my app. I have this code to add an email to a mailchimp list: but if fails with the error HTTPS/SSL is required <code>  s = requests.Session()r = s.get(""https://www.23andme.com/"") m = mailchimp.Mailchimp(MAILCHIMP_API_KEY)list_response = m.lists.list()",python requests not working with google app engine
python requests too many redirects when using django on google app engine," I'm trying to fetch a page using python requests with this code in views.py: I get the error Exceeded 30 redirects. My app is a Google App Engine Django app. I've gotten this code to work in the python console and a django server on pythonanywhere.com, but for some reason it isn't working on google app engine. What could be causing this? ThanksEdit: It seems like there is another issue with the Requests module in my app. I have this code to add an email to a mailchimp list: but if fails with the error HTTPS/SSL is required <code>  s = requests.Session()r = s.get(""https://www.23andme.com/"") m = mailchimp.Mailchimp(MAILCHIMP_API_KEY)list_response = m.lists.list()",python requests not working with google app engine
"Generate random array of 0 and 1 with specific ratio (python , numpy)"," I want to generate a random array of size N which only contains 0 and 1, I want my array to have some ratio between 0 and 1. For example, 90% of the array be 1 and the remaining 10% be 0 (I want this 90% to be random along with the whole array).right now I have: But I can't control the ratio between 0 and 1. <code>  randomLabel = np.random.randint(2, size=numbers)",Generate random array of 0 and 1 with a specific ratio
Generate random array of 0 and 1 with specific ratio," I want to generate a random array of size N which only contains 0 and 1, I want my array to have some ratio between 0 and 1. For example, 90% of the array be 1 and the remaining 10% be 0 (I want this 90% to be random along with the whole array).right now I have: But I can't control the ratio between 0 and 1. <code>  randomLabel = np.random.randint(2, size=numbers)",Generate random array of 0 and 1 with a specific ratio
How to spit a pandas dataframe or series by day," I have a long time series, eg. Now I want to extract all sub-DataFrames for each day, to get the following output: What is the most effective way to do this avoiding to check if the index.date==give_date which is very slow. Also, the user does not know a priory the range of days in the frame.Any hint do do this with an iterator? My current solution is this, but it is not so elegant and has two issues defined below: This approach has the following problems:a=np.unique(df.index.date) can take a lot of timedf[day_now:day_next] includes the day_next, but I need to exclude it in the range <code>  import pandas as pdindex=pd.date_range(start='2012-11-05', end='2012-11-10', freq='1S').tz_localize('Europe/Berlin')df=pd.DataFrame(range(len(index)), index=index, columns=['Number']) df_2012-11-05: data frame with all data referring to day 2012-11-05df_2012-11-06: etc.df_2012-11-07df_2012-11-08df_2012-11-09df_2012-11-10 time_zone='Europe/Berlin'# find all daysa=np.unique(df.index.date) # this can take a lot of timea.sort()results=[]for i in range(len(a)-1): day_now=pd.Timestamp(a[i]).tz_localize(time_zone) day_next=pd.Timestamp(a[i+1]).tz_localize(time_zone) results.append(df[day_now:day_next]) # how to select if I do not want day_next included?# last dayresults.append(df[day_next:])",How to split a pandas dataframe or series by day (possibly using an iterator)
how to split a string with brackets using regular expression in python?," Suppose I have a string like str = ""[Hi all], [this is] [an example] "". I want to split it into several pieces, each of which consists content inside a pair bracket. In another word, i want to grab the phrases inside each pair of bracket. The result should be like: How can I achieve this goal using a regular expression in Python? <code>  ['Hi all', 'this is', 'an example']",Splitting a string with brackets using regular expression in python
Python multiprocessing and orphaned children," From the python terminal, I run some command like the following, to spawn a long-running child process: This command returns, and I can do other things in the python terminal, but anything printed by the child is still printed to my python terminal session.When I exit the terminal (either with exit or CTRL+D), the exit command it hangs. If I hit CTRL+C during this hang, the child process is terminated.If I kill the python terminal process manually (via the posix kill command), the child process is instead orphaned, and continues running with its output presumably discarded.If I run this code with python -c, it waits for the child to terminate, and CTRL+C kills both parent and child.Which run configurations of python kill children when the parents are terminated? In particular, if a python-mod_wsgi-apache webserver spawns child processes and then is restarted, are the children killed?[ As an aside, what is the proper way of detaching child processes spawned from the terminal? Is there a way more elegant than the following: Deliberately make an orphan process in python ]Update: python subprocesses spawned with multiprocessing.Process by a web server running under apache are not killed when apache is restarted. <code>  from multiprocessing.process import ProcessProcess(target=LONG_RUNNING_FUNCTION).start()",Python multiprocessing and independence of children processes
easiest way to include a stop perameter in enumerate python?," Ss there a simple way to iterate over an iterable object that allows the specification of an end point, say -1, as well as the start point in enumerate. e.g. So if my object has a length of 10, what is the simplest way to to get it to start iterating at index 2 and stop iterating at index 9?Alternatively, is there something from itertools that is more suitable for this. I am specifically interested in high performance methods.In addition, when the start option was introduced in 2.6, is there any reason why a stop option was not? Cheers <code>  for i, row in enumerate(myiterable, start=2): # will start indexing at 2",Easiest way to include a stop parameter in enumerate python?
How to use pycharm to debug scrapy projects," I am working on Scrapy 0.20 with Python 2.7. I found PyCharm has a good Python debugger. I want to test my Scrapy spiders using it. Anyone knows how to do that please? What I have triedActually I tried to run the spider as a script. As a result, I built that script. Then, I tried to add my Scrapy project to PyCharm as a model like this: But I don't know what else I have to do <code>  File->Setting->Project structure->Add content root.",How to use PyCharm to debug Scrapy projects
Why does pylint object to single character variable names?," I'm still getting used to Python conventions and using Pylint to make my code more Pythonic, but I'm puzzled by the fact that Pylint doesn't like single character variable names. I have a few loops like this: and when I run pylint, I'm getting Invalid name ""x"" for type variable (should match [a-z_][a-z0-9_]{2,30} -- that suggests that a valid variable name must be between 3 and 31 characters long, but I've looked through the PEP8 naming conventions and I don't see anything explicit regarding single lower case letters, and I do see a lot of examples that use them.Is there something I'm missing in PEP8 or is this a standard that is unique to Pylint? <code>  for x in x_values: my_list.append(x)",Why does Pylint object to single-character variable names?
Display text on the screen without a window using Python," The Problem:I need to write text directly to the screen without a window. The text needs to appear above all other windows and full-screen applications and should not be clickable or interactable in any way.Example:The text doesn't need to have a transparent background as seen in the example. I can use either Python 2 or 3 on Windows 7.My Attempt at a Solution:I tried making a standalone label using Tkinter:Edit: Improved with the help of Christian Rapp What works:The text is shown without a windowThe text remains above all other windowsThe background can be transparentWhat doesn't:The text does not show above fullscreen applicationsThe text blocks click events that occur over itBackground transparency isn't alpha, so there's hard edges <code>  import Tkinterlabel = Tkinter.Label(text='Text on the screen', font=('Times','30'), fg='black', bg='white')label.master.overrideredirect(True)label.master.geometry(""+250+250"")label.master.lift()label.master.wm_attributes(""-topmost"", True)label.master.wm_attributes(""-disabled"", True)label.master.wm_attributes(""-transparentcolor"", ""white"")label.pack()label.mainloop()",How to display text on the screen without a window using Python
Python make sure my AJAX requests are originating from the same server," I have already asked a question about IP Authentication here: TastyPie Authentication from the same serverHowever, I need something more! An IP address could be very easily spoofed. Scenario: My API (TastyPie) and Client App (in javascript) are on the same server/site/domain. My users don't login. I want to consume my API in my javascript client side.Question: How can I make sure (authentication) that my AJAX requests are originating from the same server? I'm using Tatypie. I need to authentication that the requests from the client are being made on the same server/domain etc. I cannot use 'logged in sessions' as my users don't login. I have looked at private keys and generating a signature but they can viewed in the javascript making that method insecure. If I do it in a way to request a signature form the server (hiding the private key in some python code) anyone can make the same http request to get_signature that my javascript makes, thus defeating the point.I also tried to have the Django view put the signature in the view eliminating the need to make the get_signature call. This is safe, but means that I have to now refresh the page every time to get a new signature. From a users point of view only the first call to the API would work, after which they need to refresh, again pointless. I cannot believe I'm the only person with this requirement. This is a common scenario I'm sure. Please help :) An example using custom authentication in Tastypie would be welcome too. ThanksAdded: <code> ",How to make sure that my AJAX requests are originating from the same server in Python
Mocking HTTP server in Python, I'm writing a REST client and I need to mock a HTTP server in my tests. What would be the most appropriate library to do that? It would be great if I could create expected HTTP requests and compare them to actual. <code> ,Mocking a HTTP server in Python
numpy concatenate two arrays vertically," I tried the following: However, I'd expect at least that one result looks like this Why is it not concatenated vertically? <code>  >>> a = np.array([1,2,3])>>> b = np.array([4,5,6])>>> np.concatenate((a,b), axis=0)array([1, 2, 3, 4, 5, 6])>>> np.concatenate((a,b), axis=1)array([1, 2, 3, 4, 5, 6]) array([[1, 2, 3], [4, 5, 6]])",Concatenate two NumPy arrays vertically
Setting the limits on a colorbar in matplotlib," I have seen so many examples that just don't apply to my case. What I would like to do is set a simple minimum and maximum value for a colorbar. Setting a range for an image cmap is easy but this does not apply the same range to the minimum and maximum values of the colorbar.The code below may explain: The colorbar is still fixed to the limits of the data z, although the cmap range is now fixed between 0 and 1. <code>  triang = Triangulation(x,y)plt.tricontourf(triang, z, vmax=1., vmin=0.)plt.colorbar()",Setting the limits on a colorbar of a contour plot
Can isAlive() be false immediately after starting a thread?, In the python documentation at http://docs.python.org/2/library/threading.html#thread-objects it says that [isAlive()] returns True just before the run() method starts until just after the run() method terminatesBut then the start() method says that: [start()] arranges for the objects run() method to be invoked in a separate thread of control.Does this mean if I call t.start() and then immediately check t.isAlive() it's possible I could get False because the thread hasn't started yet? <code> ,Can isAlive() be False immediately after calling start() because the thread hasn't yet started?
Instances of a python unittest," I have this test how I can run instances for the test ? run the same test for a list of inputs and outputs ([""Bob"", ""Alice"", ...]) , maybe like <code>  import unittestclass TestName(unittest.TestCase): def setUp(self): self.name = ""Bob"" self.expected_name = ""Bob"" def test_name(self): # ... some operation over self.name print self.name self.assertEquals(self.name, self.expected_name)if __name__ == '__main__': unittest.main(verbosity=2) TestName(name=""Bob"", expected_name=""Bob"")TestName(name=""Alice"", expected_name=""Alice"")",unittest - run the same test for a list of inputs and outputs
Argument with optional value," I am creating a python script where I want to have an argument that manipulates how many search results you get as output. I've currently named the argument --head. This is the functionality I'd like it to have:When --head is not passed at the command line I'd like it to default to one value. In this case, a rather big one, like 80When --head is passed without any value, I'd like it to default to another value. In this case, something limited, like 10 When --head is passed with a value, I'd like it to store the value it was passed.Here is some code describing the problem: I know I probably can write a custom action for this, but I first want to see if there is any default behaviour that does this. <code>  >>> import argparse>>> parser = argparse.ArgumentParser()>>> parser.add_argument('-h', '--head', dest='size', const=80, default=10, action=""I don't know"", help='Only print the head of the output')>>> # OFC, that last line will fail because the action is uknown,... # but here is how I'd like it to work... parser.parse_args(''.split())Namespace(size=80)>>> parser.parse_args('--head'.split())Namespace(size=10)>>> parser.parse_args('--head 15'.split())Namespace(size=15)",How to make an optional value for argument using argparse?
Python - Argparse argument with optional value," I am creating a python script where I want to have an argument that manipulates how many search results you get as output. I've currently named the argument --head. This is the functionality I'd like it to have:When --head is not passed at the command line I'd like it to default to one value. In this case, a rather big one, like 80When --head is passed without any value, I'd like it to default to another value. In this case, something limited, like 10 When --head is passed with a value, I'd like it to store the value it was passed.Here is some code describing the problem: I know I probably can write a custom action for this, but I first want to see if there is any default behaviour that does this. <code>  >>> import argparse>>> parser = argparse.ArgumentParser()>>> parser.add_argument('-h', '--head', dest='size', const=80, default=10, action=""I don't know"", help='Only print the head of the output')>>> # OFC, that last line will fail because the action is uknown,... # but here is how I'd like it to work... parser.parse_args(''.split())Namespace(size=80)>>> parser.parse_args('--head'.split())Namespace(size=10)>>> parser.parse_args('--head 15'.split())Namespace(size=15)",How to make an optional value for argument using argparse?
Finding all possible substring inside a string. Python Regex," I want to find all possible substrings inside a string with the following requirement: The substring starts with N, the next letter is anything but P, and the next letter is S or TWith the test string ""NNSTL"", I would like to get as results ""NNS"" and ""NST""Is this possible with Regex? <code> ",Finding all possible substrings within a string. Python Regex
Finding all possible substrings inside a string. Python Regex," I want to find all possible substrings inside a string with the following requirement: The substring starts with N, the next letter is anything but P, and the next letter is S or TWith the test string ""NNSTL"", I would like to get as results ""NNS"" and ""NST""Is this possible with Regex? <code> ",Finding all possible substrings within a string. Python Regex
Psycopg2 acces PostgreSQL databese on romote host without manually opening tunnel," My standard procedure for accessing a PostgreSQL database on a remote server is to first create an ssh tunnel as: and then run my query in python from another shell as: This piece of python code works nicely once the tunnel is created. However, I would like psycopg2 to already open the SSH tunnel or reach ""somehow"" the remote database without need to redirect it on my localhost.Is it possible to do this with psycopg2?Is otherwise possible open the ssh tunnel in my python code?if I use: The shell will be redirected to the remote host blocking the execution of main thread. <code>  ssh username1@remote.somewhere.com -L 5432:localhost:5432 -p 222 conn = psycopg2.connect(""host=localhost"" + "" dbname="" + conf.dbname + "" user="" + conf.user + "" password="" + conf.password)cur = conn.cursor()cur.execute(query) os.system(""ssh username1@remote.somewhere.com -L 5432:localhost:5432 -p 222"")",Psycopg2 access PostgreSQL database on remote host without manually opening ssh tunnel
Psycopg2 access PostgreSQL databese on romote host without manually opening ssh tunnel," My standard procedure for accessing a PostgreSQL database on a remote server is to first create an ssh tunnel as: and then run my query in python from another shell as: This piece of python code works nicely once the tunnel is created. However, I would like psycopg2 to already open the SSH tunnel or reach ""somehow"" the remote database without need to redirect it on my localhost.Is it possible to do this with psycopg2?Is otherwise possible open the ssh tunnel in my python code?if I use: The shell will be redirected to the remote host blocking the execution of main thread. <code>  ssh username1@remote.somewhere.com -L 5432:localhost:5432 -p 222 conn = psycopg2.connect(""host=localhost"" + "" dbname="" + conf.dbname + "" user="" + conf.user + "" password="" + conf.password)cur = conn.cursor()cur.execute(query) os.system(""ssh username1@remote.somewhere.com -L 5432:localhost:5432 -p 222"")",Psycopg2 access PostgreSQL database on remote host without manually opening ssh tunnel
Psycopg2 access PostgreSQL databese on remote host without manually opening ssh tunnel," My standard procedure for accessing a PostgreSQL database on a remote server is to first create an ssh tunnel as: and then run my query in python from another shell as: This piece of python code works nicely once the tunnel is created. However, I would like psycopg2 to already open the SSH tunnel or reach ""somehow"" the remote database without need to redirect it on my localhost.Is it possible to do this with psycopg2?Is otherwise possible open the ssh tunnel in my python code?if I use: The shell will be redirected to the remote host blocking the execution of main thread. <code>  ssh username1@remote.somewhere.com -L 5432:localhost:5432 -p 222 conn = psycopg2.connect(""host=localhost"" + "" dbname="" + conf.dbname + "" user="" + conf.user + "" password="" + conf.password)cur = conn.cursor()cur.execute(query) os.system(""ssh username1@remote.somewhere.com -L 5432:localhost:5432 -p 222"")",Psycopg2 access PostgreSQL database on remote host without manually opening ssh tunnel
what is the correct workflow in using sphinx for python project documentation," I want to use Sphinx to document a large project that is currently not well-documented. In particular I want to use sphinx-apidoc to produce the documentation from the code as I document it. However I also want to have some pages of tutorial on how to use the project etc., but it seems when I call sphinx-apidoc it generates the whole document at once.So my question is: What is the correct workflow here so I could write the tutorial pages that are to be written manually and at the same time update the documentation in the code? Note that if I update the manually written tutorial pages (e.g. included in the index.txt) and run sphinx-apidoc I will lose them as the whole document is generated at once.In general are there any guidelines as how to proceed in building the documentation?Note: The source of inconvenience is that the basic procedure assumes you have all the code documentation already in place and will not make any updates as you proceed in producing the documentation. At least this is what I need to resolve.  <code> ",What is the correct workflow in using Sphinx for Python project documentation?
pandas Subtract Dataframe with a row from another dataframe," I would like to subtract all rows in a dataframe with one row from another dataframe.(Difference from one row)Is there an easy way to do this? Like df-df2)? Here is an output that works for the first row, however I want the remaining rows to be detracted as well... <code>  df = pd.DataFrame(abs(np.floor(np.random.rand(3, 5)*10)),... columns=['a', 'b', 'c', 'd', 'e'])dfOut[18]: a b c d e0 8 9 8 6 41 3 0 6 4 82 2 5 7 5 6df2 = pd.DataFrame(abs(np.floor(np.random.rand(1, 5)*10)),... columns=['a', 'b', 'c', 'd', 'e'])df2 a b c d e0 8 1 3 7 5 df-df2 a b c d e0 0 8 5 -1 -11 NaN NaN NaN NaN NaN2 NaN NaN NaN NaN NaN",How to subtract all rows in a dataframe with a row from another dataframe?
Iterated function in python," Given a function f( ), a number x and an integer N, I want to compute the List: An obvious way to do this in Python is the following Python code: But I want to know if there is a better or faster, way to do this. <code>  y = [x, f(x), f(f(x)), ..., f(f... M times...f(f(x)) ] y = [x]for i in range(N-1): y.append(f(y[-1]))",Iterated function in Python
Get total physical memory from python," How can I get the total physical memory within Python in a distribution agnostic fashion? I don't need used memory, just the total physical memory. <code> ",Get total physical memory in Python
Can't use chrome driver for selenium," I'm having trouble using the Chrome driver for Selenium. I have the chromedriver downloaded and saved to C:\Chrome: Using that gives me the following error: Any help would be appreciated.  <code>  driver = webdriver.Chrome(executable_path=""C:/Chrome/"") Traceback (most recent call last): File ""C:\Python33\lib\subprocess.py"", line 1105, in _execute_child startupinfo)PermissionError: [WinError 5] Access is deniedDuring handling of the above exception, another exception occurred:Traceback (most recent call last): File ""C:\Python33\lib\site-packages\selenium\webdriver\chrome\service.py"", line 63, in start self.service_args, env=env, stdout=PIPE, stderr=PIPE) File ""C:\Python33\lib\subprocess.py"", line 817, in __init__ restore_signals, start_new_session) File ""C:\Python33\lib\subprocess.py"", line 1111, in _execute_child raise WindowsError(*e.args)PermissionError: [WinError 5] Access is deniedDuring handling of the above exception, another exception occurred:Traceback (most recent call last): File ""C:/Users/Wilson/Dropbox/xxx.py"", line 71, in <module> driver = webdriver.Chrome(executable_path=""C:/Chrome/"") File ""C:\Python33\lib\site-packages\selenium\webdriver\chrome\webdriver.py"", line 59, in __init__ self.service.start() File ""C:\Python33\lib\site-packages\selenium\webdriver\chrome\service.py"", line 68, in start and read up at http://code.google.com/p/selenium/wiki/ChromeDriver"")selenium.common.exceptions.WebDriverException: Message: 'ChromeDriver executable needs to be available in the path. Please download from http://chromedriver.storage.googleapis.com/index.html ",Can't use chrome driver for Selenium
Dango admin: i can't list all of my fields in list_editable without causing errors," I am trying to use list_editable to make all my fields editable on the same page. But unless I also have something in list_display_links I get errors. The problems I don't have any unused fields to put there. I am probably misunderstanding a concept somewhere.What I have done is create a 'dummy' field in the model: dummy = None. This is not only clunky and probably wrong - but it also causes the dummy field to appear in my admin. What am I doing wrong? I tried reading the docs but I can't find the solution to my problem. I would like to go about this the ""right way"", whatever that may be.Here is my code:models.py admin.py <code>  ...class Slider(models.Model): slider_title = models.CharField(max_length=20) slider_text = models.TextField(max_length=200) slider_order = models.PositiveSmallIntegerField( default=1, blank=True, null=True, choices=[(1, 'first'), (2, 'middle'), (3, 'last')]) dummy = None def clean(self): validate_only_three_instances(self) def __str__(self): return self.slider_title... ...class SliderAdmin(admin.ModelAdmin): # remove ""add"" button def has_add_permission(self, request): return False fieldsets = [ (None, {'fields': ['slider_title']}), (None, {'fields': ['slider_text']}), (None, {'fields': ['slider_order']}), ] list_display = ( 'slider_title', 'slider_text', 'slider_order', 'dummy',) list_display_links = ('dummy',) list_editable = ('slider_title', 'slider_text', 'slider_order',)...",Cannot list all of my fields in list_editable without causing errors
How to check if a module/library/package is native python?," I have installed sooo many libraries/modules/packages with pip and now I cannot differentiate which is native to the python standard library and which is not. This causes problem when my code works on my machine but it doesn't work anywhere else.How can I check if a module/library/package that I import in my code is from the python stdlib? Assume that the checking is done on the machine with all the external libraries/modules/packages, otherwise I could simply do a try-except import on the other machine that doesn't have them.For example, I am sure these imports work on my machine, but when it's on a machine with only a plain Python install, it breaks: <code>  from bs4 import BeautifulSoupimport nltkimport PILimport gensim",How to check if a module/library/package is part of the python standard library?
Python strip() vs. strip(string.whitespace)," I have a Unicode string with some non-breaking spaces at the beginning and end. I get different results when using strip() vs. strip(string.whitespace). The documentation for strip() says, ""If omitted or None, the chars argument defaults to removing whitespace."" The documentation for string.whitespace says, ""A string containing all characters that are considered whitespace.""So if string.whitespace contains all characters that are considered whitespace, then why are the results different? Does it have something to do with Unicode?I am using Python 2.7.6 <code>  >>> import string>>> s5 = u'\xa0\xa0hello\xa0\xa0'>>> print s5.strip()hello>>> print s5.strip(string.whitespace)hello",strip() and strip(string.whitespace) give different results despite documentation suggesting they should be the same
converting each element of list to tuple," to convert each element of list to tuple like following : convert to tuple list: Actually I have dict with keys like this so for searching purpose I need to have these. <code>  l = ['abc','xyz','test'] newl = [('abc',),('xyz',),('test',)]",Converting each element of a list to tuple
Python: What is the difference of os.path.basename() and os.path.dirname() functions?," What is the difference between os.path.basename() and os.path.dirname()? I already searched for answers and read some links, but didn't understand. Can anyone give a simple explanation? <code> ",What is the difference between os.path.basename() and os.path.dirname()?
logging formatter to have lower-case level name," I have a simple logger here. I don't want the levelname to be all caps but lower-case (e.g., info instead of the default INFO). How can I fix it?  <code>  import logginglog = logging.getLogger(__name__)ch = logging.StreamHandler()ch.setLevel(logging.ERROR)chformatter = logging.Formatter('%(levelname)s %(message)s')ch.setFormatter(chformatter)log.addHandler(ch)log.error(""blah"")",How can I make the level name be lower-case?
Extreact metadata from a Video," I am getting an MJPEG stream from an IP Camera which I am viewing and saving on my computer. The code of how I am doing it can be found here. The answer explains how to extract the images from the stream and save them.For extracting the images I am using the method listed in the answer and for saving it I am simply putting the images in an avi container using OpenCV. The code is given below. Here bitmap is the image that I am displaying and putting in the avi container.Since the image is from an IP Camera it must have some metyadata like a time stamp which the camera inserts.Question: How do I extract the metadata?I have thought of 2 ways to do this:Extract the frames from the video and then access them to access the time stamp.Extract the time stamp from the video itself.How do I proceed? Which method do I use? I am using Python and Opencv and am working on Windows 7.I also read this like related to what I am trying to do. It did not solve my problem. <code>  writer=cv.CreateVideoWriter(""video1.avi"", cv.CV_FOURCC('X', '2', '6', '4'), fps, (320,240))cv_image = cv2.imdecode(np.fromstring(jpg, dtype=np.uint8),cv2.CV_LOAD_IMAGE_COLOR)bitmap=cv.CreateImageHeader((cv_image.shape[1], cv_image.shape[0]), cv.IPL_DEPTH_8U, 3)cv.SetData(bitmap, cv_image.tostring(), cv_image.dtype.itemsize * 3 * cv_image.shape[1])cv.WriteFrame(writer, bitmap)",Extract metadata from a Video/Image
how to minize a function using Deap?," I need to minimize a function using genetic algorithm and PSO. Different posts suggest to use DEAP (I am using python) but I do not even understand how to start. We can consider for example f on the interval i How can I minimize this function using DEAP? <code>  i=arange(-10,10,0.1)def f(x): return x*sin(x)",how to minimize a function using Deap?
How do __enter__ and __exit__ work in Python decorator classes?," I'm trying to create a decorator class that counts how many times a function is called, but I'm getting an error message that says: and I really don't know how I'm giving it four arguments. My code looks like this: Are there some other parameters I should (or shouldn't) be passing into the def exit function? Any tips or ideas would be appreciated.As an aside, my line of code that says ""print 'f count =',f.count"" appears to be outputting the memory address rather than the value, but that's a whole different problem. <code>  ""TypeError: __exit__() takes exactly 1 argument (4 given)"" class fcount2(object): __instances = {} def __init__(self, f): self.__f = f self.__numcalls = 0 fcount2.__instances[f] = self def __call__(self, *args, **kwargs): self.__numcalls += 1 return self.__f(*args, **kwargs) def __enter__(self): return self def __exit__(self): return self @staticmethod def count(f): return fcount2.__instances[self.__f].__numcalls@fcount2def f(n): return n+2for n in range(5): print f(n) print 'f count =',f.countdef foo(n): return n*nwith fcount2(foo) as g: print g(1) print g(2)print 'g count =',g.countprint 'f count =',f.countwith fcount2(f) as g: print g(1) print g(2)print 'g count =',g.countprint 'f count =',f.countwith f: print f(1) print g(2)print 'g count =',g.countprint 'f count =',f.count",NULL NULL NULL NULL
Shuffle a numpy array," I have a 2-d numpy array that I would like to shuffle. Is the best way to reshape it to 1-d, shuffle and reshape again to 2-d or is it possible to shuffle without reshaping?just using the random.shuffle doesn't yield expected results and numpy.random.shuffle shuffles only rows: <code>  import randomimport numpy as npa=np.arange(9).reshape((3,3))random.shuffle(a)print a[[0 1 2] [3 4 5] [3 4 5]]a=np.arange(9).reshape((3,3))np.random.shuffle(a)print a[[6 7 8] [3 4 5] [0 1 2]]","Shuffle a numpy array in 1d, then shuffle and reshape back to 2d"
Unifying bin locations across histograms," Say I have several histograms, each with counts at different bin locations (on a real axis). e.g. How can I normalize these histograms so that I get PDFs where the integral of each PDF adds up to one within a given range (e.g. 0 and 100)? We can assume that the histogram counts events on pre-defined bin size (e.g. 10)Most implementations I have seen are based, for example, on Gaussian Kernels (see scipy and scikit-learn) that start from the data. In my case, I need to do this from the histograms, since I don't have access to the original data.Update:Note that all current answers assume that we are looking at a random variable that lives in (-Inf, +Inf). That's fine as a rough approximation, but this may not be the case depending on the application, where the variable may be defined within some other range [a,b] (e.g. 0 and 100 in the above case) <code>  def generate_random_histogram(): # Random bin locations between 0 and 100 bin_locations = np.random.rand(10,) * 100 bin_locations.sort() # Random counts between 0 and 50 on those locations bin_counts = np.random.randint(50, size=len(bin_locations)) return {'loc': bin_locations, 'count':bin_counts}# We can assume that the bin size is either pre-defined or that # the bin edges are on the middle-point between consecutive counts.hists = [generate_random_histogram() for x in xrange(3)]",Making Probability Distribution Functions (PDFs) from histograms
Making consistent PDFs from histograms," Say I have several histograms, each with counts at different bin locations (on a real axis). e.g. How can I normalize these histograms so that I get PDFs where the integral of each PDF adds up to one within a given range (e.g. 0 and 100)? We can assume that the histogram counts events on pre-defined bin size (e.g. 10)Most implementations I have seen are based, for example, on Gaussian Kernels (see scipy and scikit-learn) that start from the data. In my case, I need to do this from the histograms, since I don't have access to the original data.Update:Note that all current answers assume that we are looking at a random variable that lives in (-Inf, +Inf). That's fine as a rough approximation, but this may not be the case depending on the application, where the variable may be defined within some other range [a,b] (e.g. 0 and 100 in the above case) <code>  def generate_random_histogram(): # Random bin locations between 0 and 100 bin_locations = np.random.rand(10,) * 100 bin_locations.sort() # Random counts between 0 and 50 on those locations bin_counts = np.random.randint(50, size=len(bin_locations)) return {'loc': bin_locations, 'count':bin_counts}# We can assume that the bin size is either pre-defined or that # the bin edges are on the middle-point between consecutive counts.hists = [generate_random_histogram() for x in xrange(3)]",Making Probability Distribution Functions (PDFs) from histograms
Making consistent Probability Distribution Functions (PDFs) from histograms," Say I have several histograms, each with counts at different bin locations (on a real axis). e.g. How can I normalize these histograms so that I get PDFs where the integral of each PDF adds up to one within a given range (e.g. 0 and 100)? We can assume that the histogram counts events on pre-defined bin size (e.g. 10)Most implementations I have seen are based, for example, on Gaussian Kernels (see scipy and scikit-learn) that start from the data. In my case, I need to do this from the histograms, since I don't have access to the original data.Update:Note that all current answers assume that we are looking at a random variable that lives in (-Inf, +Inf). That's fine as a rough approximation, but this may not be the case depending on the application, where the variable may be defined within some other range [a,b] (e.g. 0 and 100 in the above case) <code>  def generate_random_histogram(): # Random bin locations between 0 and 100 bin_locations = np.random.rand(10,) * 100 bin_locations.sort() # Random counts between 0 and 50 on those locations bin_counts = np.random.randint(50, size=len(bin_locations)) return {'loc': bin_locations, 'count':bin_counts}# We can assume that the bin size is either pre-defined or that # the bin edges are on the middle-point between consecutive counts.hists = [generate_random_histogram() for x in xrange(3)]",Making Probability Distribution Functions (PDFs) from histograms
Python return None when attribute not exist,"  <code>  class test(object):def __init__(self, a = 0): test.a = at = test()print test.a ## obviously we get 0''' ====== Question ====== '''print test.somethingelse ## I want if attributes not exist, return None. How to do that?",Return None when attribute does not exist
Creating pivot table in Excel using python," I adapted the following code found here to create a pivot table in my existing excel sheet: This line of code, wb.ActiveSheet.PivotTables(tname).AddDataField(wb.ActiveSheet.PivotTables(tname).PivotFields(sumvalue),sumvalue,win32c.xlSum) returns the following error: pywintypes.com_error: (-2147352567, 'Exception occurred.', (0, u'Microsoft Excel', u'PivotFields method of PivotTable class failed', u'xlmain11.chm', 0, -2146827284), None).When I remove that line, the pivot table is generated without any data fields. Is there something I'm doing wrong? <code>  import win32com.client as win32win32c = win32.constantsimport sysimport itertoolstablecount = itertools.count(1)def addpivot(wb,sourcedata,title,filters=(),columns=(), rows=(),sumvalue=(),sortfield=""""): newsheet = wb.Sheets.Add() newsheet.Cells(1,1).Value = title newsheet.Cells(1,1).Font.Size = 16 tname = ""PivotTable%d""%tablecount.next() pc = wb.PivotCaches().Add(SourceType=win32c.xlDatabase, SourceData=sourcedata) pt = pc.CreatePivotTable(TableDestination=""%s!R4C1""%newsheet.Name, TableName=tname, DefaultVersion=win32c.xlPivotTableVersion10) for fieldlist,fieldc in ((filters,win32c.xlPageField), (columns,win32c.xlColumnField), (rows,win32c.xlRowField)): for i,val in enumerate(fieldlist): wb.ActiveSheet.PivotTables(tname).PivotFields(val).Orientation = fieldc wb.ActiveSheet.PivotTables(tname).PivotFields(val).Position = i+1 wb.ActiveSheet.PivotTables(tname).AddDataField(wb.ActiveSheet.PivotTables(tname). PivotFields(sumvalue),sumvalue,win32c.xlSum)def runexcel(): excel = win32.gencache.EnsureDispatch('Excel.Application') #excel.Visible = True try: wb = excel.Workbooks.Open('18.03.14.xls') except: print ""Failed to open spreadsheet 18.03.14.xls"" sys.exit(1) ws = wb.Sheets('defaulters') xldata = ws.UsedRange.Value newdata = [] for row in xldata: if len(row) == 4 and row[-1] is not None: newdata.append(list(row)) rowcnt = len(newdata) colcnt = len(newdata[0]) wsnew = wb.Sheets.Add() wsnew.Range(wsnew.Cells(1,1),wsnew.Cells(rowcnt,colcnt)).Value = newdata wsnew.Columns.AutoFit() src = ""%s!R1C1:R%dC%d""%(wsnew.Name,rowcnt,colcnt) addpivot(wb,src, title=""Employees by leads"", filters=(""Leads"",), columns=(), rows=(""Name"",), sumvalue=""Actual hours"", sortfield=()) if int(float(excel.Version)) >= 12: wb.SaveAs('new18.03.14.xlsx',win32c.xlOpenXMLWorkbook) else: wb.SaveAs('new18.03.14.xls') excel.Application.Quit()if __name__ == ""__main__"": runexcel()",Creating pivot table in Excel using python causes pywintypes.com_error
Tuple in list ---> list," Say I have a list with one or more tuples in it: What's the best way to get rid of the tuples so that it's just an int list? <code>  [0, 2, (1, 2), 5, 2, (3, 5)] [0, 2, 1, 2, 5, 2, 3, 5]","How to flatten a list with various data types (int, tuple)"
"Python enum, when and where to use?"," Python 3.4.0 introduced enum, I've read the doc but still don't know the usage of it. From my perspective, enum.Enum is an extended namedtuple type, which may not be true. So these are what I want to know about Enum:When and where to use Enum?Why do we need Enum? What are the advantages?What exactly is an Enum? <code> ","Python Enum, when and where to use?"
Difference between 'and' (boolean) vs. '&' (bitwise) in python. Why difference in behavior with lists vs numpy arrays?," What explains the difference in behavior of boolean and bitwise operations on lists vs NumPy arrays? I'm confused about the appropriate use of & vs and in Python, illustrated in the following examples. This answer and this answer helped me understand that and is a boolean operation but & is a bitwise operation. I read about bitwise operations to better understand the concept, but I am struggling to use that information to make sense of my above 4 examples.Example 4 led me to my desired output, so that is fine, but I am still confused about when/how/why I should use and vs &. Why do lists and NumPy arrays behave differently with these operators? Can anyone help me understand the difference between boolean and bitwise operations to explain why they handle lists and NumPy arrays differently?  <code>  mylist1 = [True, True, True, False, True]mylist2 = [False, True, False, True, False]>>> len(mylist1) == len(mylist2)True# ---- Example 1 ---->>> mylist1 and mylist2[False, True, False, True, False]# I would have expected [False, True, False, False, False]# ---- Example 2 ---->>> mylist1 & mylist2TypeError: unsupported operand type(s) for &: 'list' and 'list'# Why not just like example 1?>>> import numpy as np# ---- Example 3 ---->>> np.array(mylist1) and np.array(mylist2)ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()# Why not just like Example 4?# ---- Example 4 ---->>> np.array(mylist1) & np.array(mylist2)array([False, True, False, False, False], dtype=bool)# This is the output I was expecting!",'and' (boolean) vs '&' (bitwise) - Why difference in behavior with lists vs numpy arrays?
"Python: Split sting into a list, with items of equal length"," I have a string (without spaces) which I need to split into a list with items of equal length. I'm aware of the split() method, but as far as I'm aware this only splits via spaces and not via length.What I want to do is something like this: I have thought about looping through the list but I was wondering if there was a simpler solution? <code>  string = ""abcdefghijklmnopqrstuvwx""string = string.Split(0 - 3)print(string)>>> [""abcd"", ""efgh"", ""ijkl"", ""mnop"", ""qrst"", ""uvwx""]","Split string into a list, with items of equal length"
"Python: Split string into a list, with items of equal length"," I have a string (without spaces) which I need to split into a list with items of equal length. I'm aware of the split() method, but as far as I'm aware this only splits via spaces and not via length.What I want to do is something like this: I have thought about looping through the list but I was wondering if there was a simpler solution? <code>  string = ""abcdefghijklmnopqrstuvwx""string = string.Split(0 - 3)print(string)>>> [""abcd"", ""efgh"", ""ijkl"", ""mnop"", ""qrst"", ""uvwx""]","Split string into a list, with items of equal length"
"Split string into a list, with items of equal length"," I have a string (without spaces) which I need to split into a list with items of equal length. I'm aware of the split() method, but as far as I'm aware this only splits via spaces and not via length.What I want to do is something like this: I have thought about looping through the list but I was wondering if there was a simpler solution? <code>  string = ""abcdefghijklmnopqrstuvwx""string = string.Split(0 - 3)print(string)>>> [""abcd"", ""efgh"", ""ijkl"", ""mnop"", ""qrst"", ""uvwx""]","Split string into a list, with items of equal length"
"matplotlib: get text bounding box, independent of backend"," I would like to get the bounding box (dimensions) around some text in a matplotlib figure. The post here, helped me realize that I can use the method text.get_window_extent(renderer) to get the bounding box, but I have to supply the correct renderer. Some backends do not have the method figure.canvas.get_renderer(), so I tried matplotlib.backend_bases.RendererBase() to get the renderer and it did not produce satisfactory results. Here is a simple example This produces the following plot:Clearly the red box is too small. I think a Paul's answer here found the same issue. The black box looks great, but I cannot use the MacOSX backend, or any others that do not have the method figure.canvas.get_renderer().In case it matters, I am on Mac OS X 10.8.5, Matplotlib 1.3.0, and Python 2.7.5 <code>  import matplotlib as mplimport matplotlib.pyplot as pltfrom matplotlib.patches import Rectanglefig = plt.figure()ax = plt.subplot()txt = fig.text(0.15,0.5,'afdjsklhvvhwd', fontsize = 36)renderer1 = fig.canvas.get_renderer()renderer2 = mpl.backend_bases.RendererBase()bbox1 = txt.get_window_extent(renderer1)bbox2 = txt.get_window_extent(renderer2)rect1 = Rectangle([bbox1.x0, bbox1.y0], bbox1.width, bbox1.height, \ color = [0,0,0], fill = False)rect2 = Rectangle([bbox2.x0, bbox2.y0], bbox2.width, bbox2.height, \ color = [1,0,0], fill = False)fig.patches.append(rect1)fig.patches.append(rect2)plt.draw()","Get text bounding box, independent of backend"
Python - extract a string between double quotes," I'm reading a response from a source which is an journal or an essay and I have the html response as a string like: According to some, dreams express ""profound aspects of personality"" (Foulkes 184), though others disagree.My goal is just to extract all of the quotes out of the given string and save each of them into a list. My approach was: Somehow it didn't work for me. Any helps on my regex here? Thanks a lot. <code>  [match.start() for m in re.Matches(inputString, ""\""([^\""]*)\""""))]",Extract a string between double quotes
How to input matrix (2D list) in python 3.4?," I tried to create this code to input an m by n matrix. I intended to input [[1,2,3],[4,5,6]] but the code yields [[4,5,6],[4,5,6]. Same things happen when I input other m by n matrix, the code yields an m by n matrix whose rows are identical. Perhaps you can help me to find what is wrong with my code. <code>  m = int(input('number of rows, m = '))n = int(input('number of columns, n = '))matrix = []; columns = []# initialize the number of rowsfor i in range(0,m): matrix += [0]# initialize the number of columnsfor j in range (0,n): columns += [0]# initialize the matrixfor i in range (0,m): matrix[i] = columnsfor i in range (0,m): for j in range (0,n): print ('entry in row: ',i+1,' column: ',j+1) matrix[i][j] = int(input())print (matrix)",How to input matrix (2D list) in Python?
How to input matrix (2D list) in Python 3.4?," I tried to create this code to input an m by n matrix. I intended to input [[1,2,3],[4,5,6]] but the code yields [[4,5,6],[4,5,6]. Same things happen when I input other m by n matrix, the code yields an m by n matrix whose rows are identical. Perhaps you can help me to find what is wrong with my code. <code>  m = int(input('number of rows, m = '))n = int(input('number of columns, n = '))matrix = []; columns = []# initialize the number of rowsfor i in range(0,m): matrix += [0]# initialize the number of columnsfor j in range (0,n): columns += [0]# initialize the matrixfor i in range (0,m): matrix[i] = columnsfor i in range (0,m): for j in range (0,n): print ('entry in row: ',i+1,' column: ',j+1) matrix[i][j] = int(input())print (matrix)",How to input matrix (2D list) in Python?
Python get the x words in a string," I'm looking for a code that takes the 4 (or 5) first words in a script.I tried this: But I can't take more than 2 strings: I would like to have: <code>  import re my_string = ""the cat and this dog are in the garden"" a = my_string.split(' ', 1)[0]b = my_string.split(' ', 1)[1] a = theb = cat and this dog are in the garden a = theb = catc = andd = this...",Python get the x first words in a string
Pre-processing a corpus in Python," I would like to preprocess a corpus of documents using Python in the same way that I can in R. For example, given an initial corpus, corpus, I would like to end up with a preprocessed corpus that corresponds to the one produced using the following R code: Is there a simple or straightforward preferably pre-built method of doing this in Python? Is there a way to ensure exactly the same results?For example, I would like to preprocess @Apple ear pods are AMAZING! Best sound from in-ear headphones I've ever had!into ear pod amaz best sound inear headphon ive ever <code>  library(tm)library(SnowballC)corpus = tm_map(corpus, tolower)corpus = tm_map(corpus, removePunctuation)corpus = tm_map(corpus, removeWords, c(""myword"", stopwords(""english"")))corpus = tm_map(corpus, stemDocument)",Exactly replicating R text preprocessing in python
Converting between datetime and Timestamp objects," I have the following: and I read on this answer that I could use pandas.to_datetime() to convert from Timestamps to datetime objects, but it doesn't seem to work: Why? How can I convert between these two formats? <code>  > date1Timestamp('2014-01-23 00:00:00', tz=None)> date2datetime.date(2014, 3, 26) > pd.to_datetime(date1) Timestamp('2014-01-23 00:00:00', tz=None)",Converting between datetime and Pandas Timestamp objects
"importing python modules in NumPy, matplotlib packages"," When I try to use pyplot from matplotlib: It gives me AttributeError: 'module' object has no attribute 'pyplot'It can be solved with: But what I am really confused with is, gives me <module 'numpy.random' from '/Applications/Canopy.app/appdata/canopy-1.0.3.1262.macosx-x86_64/Canopy.app/Contents/lib/python2.7/site-packages/numpy/random/__init__.pyc'>What is the difference between two cases? pyplot cannot be called in the first example, but random was in the second. I think it's related with some kind of packages and modules. But I'm not such a professional to the python, thus asking for an answer. <code>  import matplotlibprint matplotlib.pyplot # just checking import matplotlib.pyplot import numpyprint numpy.random","what is the difference between importing python sub-modules from NumPy, matplotlib packages"
"importing python sub-modules from NumPy, matplotlib packages"," When I try to use pyplot from matplotlib: It gives me AttributeError: 'module' object has no attribute 'pyplot'It can be solved with: But what I am really confused with is, gives me <module 'numpy.random' from '/Applications/Canopy.app/appdata/canopy-1.0.3.1262.macosx-x86_64/Canopy.app/Contents/lib/python2.7/site-packages/numpy/random/__init__.pyc'>What is the difference between two cases? pyplot cannot be called in the first example, but random was in the second. I think it's related with some kind of packages and modules. But I'm not such a professional to the python, thus asking for an answer. <code>  import matplotlibprint matplotlib.pyplot # just checking import matplotlib.pyplot import numpyprint numpy.random","what is the difference between importing python sub-modules from NumPy, matplotlib packages"
DJANGO - Serving MEDIA/uploaded files in production," I currently have this in my project urls.py, the last line is what's important. I've been told and I've read that this is not suitable for a production environment. Why is this the case? <code>  urlpatterns = patterns('', url(r'^', include('polls.urls', namespace=""polls"")), url(r'^admin/', include(admin.site.urls)),) + static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)",Django - Serving MEDIA/uploaded files in production
What does the C in Logistic Regression represent? How should it effect my code?," I am using sklearn.linear_model.LogisticRegression in scikit learn to run a Logistic Regression. What does C mean here in simple terms please? What is regularization strength? <code>  C : float, optional (default=1.0) Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.",What is the inverse of regularization strength in Logistic Regression? How should it affect my code?
Is an else: pass statement required at the end of a Python if/elif statement?," If I write: and I just want to pass otherwise, is writing out the following required at the end?: It seems to run fine without the else: statement in the interpreter, is there a reason I'm not aware of that I should always include else: pass in these cases? <code>  if a == b: # do somethingelif a == c: # do something else else: pass",Is it best practice to include an else: pass statement at the end of a Python if or if/elif statement?
How to remove an element from a set?," I think this may be related to set being mutable.Basically, I can remove an element from a set using set.discard(element). However, set.discard(element) itself returns None. But I'd like to get a copy of the updated set. For example, if I have a list of sets, how can I get an updated copy conveniently using list comprehension operations?Sample code: will return <code>  test = [{'', 'a'}, {'b', ''}]print [x.discard('') for x in test]print test [None, None][set(['a']), set(['b'])]",How to remove specific element from sets inside a list using list comprehension
Python: native infinite range?," Does python have a native iterable of the infinite integer series?I've tried range(float('inf')) and iter(int), but neither work.I can obviously implement my own generator along the lines of but this feels like something which should already exist. <code>  def int_series(next=1): while True: next += 1 yield next",Native infinite range?
Convert contour to 2d point list in python opencv binding," The following is a contour structure returned by OpenCV. It's deeply nested, the first element of the tuple is a list of points in the contour.Any idea to convert this to a 2d point list (n x 2)? I think numpy.reshape can be used, but I couldn't find a very general way to do that. Thanks. <code>  contours = cv2.findContours(bw_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)contours => ([array([[[ 19, 20]], [[ 18, 21]], [[ 17, 21]], [[ 17, 22]], [[ 16, 23]], [[ 16, 130]], [[ 17, 131]], [[ 17, 132]], [[ 21, 132]], [[ 43, 110]], [[ 44, 110]], [[ 75, 141]], [[ 81, 141]], [[109, 113]], [[145, 149]], [[149, 149]], [[149, 21]], [[148, 21]], [[147, 20]]], dtype=int32)], array([[[-1, -1, -1, -1]]], dtype=int32))",Convert contour to 2d point list in opencv
What is the proper way to take take a directory path as user input?," Below is a snippet of code I am trying to use to take a directory path as ""raw input"" from the user. I receive the following error after the input is taken from the user: Ignoring what I have done below, is there a particular way I am supposed to take the path from user so that Python accepts it? For example, the directory and file I'm looking for is  <code>  Traceback (most recent call last): File ""C:\Users\larece.johnson\Desktop\Python Programs\Hello World 2"", line 14, in <module> f = open(str,""r+"") #I open the text file here which the user gave meIOError: [Errno 2] No such file or directory: 'C:/Users/larece.johnson/Desktop/Python Programs/BostonLog.log.2014-04-01' C:/Users/larece.johnson/Desktop/Python Programs/BostonLog.log.2014-04-01 import re #this library is used so that I can use the ""search"" functionimport os #this is needed for using directory paths and manipulating them str ="""" #initializing string variable for raw data input#print os.getcwd()#f = open(""C:/Users/larece.johnson/Desktop/BostonLog.log.2014-04-02.log"",""r+"")str = raw_input(""Enter the name of your text file - please use / backslash when typing in directory path: ""); #User will enter the name of text file for mef = open(str,""r+"")",What is the proper way to take a directory path as user input?
How to log exception from a celery task using sentry and get context vars," Second Edit: After a bit of digging, the question changed from how to log an exception with local variables to how to prevent celery from sending the 2nd log message which does not has the local vars. After the below attempt, I actually noticed that I was always receiving 2 emails, one with local vars per frame and the other without.First Edit: I've managed to sort of get the local variables, by adding a custom on_failure override (using annotations for all tasks like so: But the problem now is that the error arrives 3 times, once through the celery logger and twice through root (although I'm not propagating 'celery' logger in my logging settings)Original question:I have a django/celery project to which I recently added a sentry handler as the root logger with level 'ERROR'. This works fine for most errors and exceptions occurring in django, except the ones coming from celery workers.What happens is that sentry receives an exception with the traceback and the locals of the daemon, but does not include the f_locals (local vars) of each frame in the stack. And these do appear in normal python/django exceptions.I guess I could try to catch all exceptions and log with exc_info manually. But this is less than ideal. <code>  def include_f_locals(self, exc, task_id, args, kwargs, einfo): import logging logger = logging.getLogger('celery') logger.error(exc, exc_info=einfo)CELERY_ANNOTATIONS = {'*': {'on_failure': include_f_locals}}",How to prevent duplicate exception logging from a celery task
How to determine if a black-box is polynomial or exponential in Python," I have a problem which essentially reduces to this:You have a black-box function that accepts inputs of length n.You can measure the amount of time the function takes to return the answer, but you can't see exactly how it was calculated.You have to determine whether the time-complexity of this function is polynomial or exponential.The way I did this was by running thousands of random sample inputs of varying lengths through the function, then plotting them on a scatter plot with times on the y-axis and input length on the x-axis.I have the scatter plotted using numpy, but now how do I draw polynomial and exponential best fit lines? And what metrics can I calculate to determine which best fit lines fit best?(Asked a similar question but theoretically and no emphasis on a particular language on Computer Science: https://cs.stackexchange.com/questions/23686/how-to-determine-if-a-black-box-is-polynomial-or-exponential) <code> ",How to determine if a black-box is polynomial or exponential
scikit grid search over multiple classifiers python," Is there a better inbuilt way to do grid search and test multiple models in a single pipeline? Of course the parameters of the models would be different, which made is complicated for me to figure this out. Here is what I did: However, this approach is still giving the best model within each classifier, and not comparing between classifiers. <code>  from sklearn.pipeline import Pipelinefrom sklearn.ensemble import RandomForestClassifierfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.svm import SVCfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.grid_search import GridSearchCVdef grid_search(): pipeline1 = Pipeline(( ('clf', RandomForestClassifier()), ('vec2', TfidfTransformer()) )) pipeline2 = Pipeline(( ('clf', KNeighborsClassifier()), )) pipeline3 = Pipeline(( ('clf', SVC()), )) pipeline4 = Pipeline(( ('clf', MultinomialNB()), )) parameters1 = { 'clf__n_estimators': [10, 20, 30], 'clf__criterion': ['gini', 'entropy'], 'clf__max_features': [5, 10, 15], 'clf__max_depth': ['auto', 'log2', 'sqrt', None] } parameters2 = { 'clf__n_neighbors': [3, 7, 10], 'clf__weights': ['uniform', 'distance'] } parameters3 = { 'clf__C': [0.01, 0.1, 1.0], 'clf__kernel': ['rbf', 'poly'], 'clf__gamma': [0.01, 0.1, 1.0], } parameters4 = { 'clf__alpha': [0.01, 0.1, 1.0] } pars = [parameters1, parameters2, parameters3, parameters4] pips = [pipeline1, pipeline2, pipeline3, pipeline4] print ""starting Gridsearch"" for i in range(len(pars)): gs = GridSearchCV(pips[i], pars[i], verbose=2, refit=False, n_jobs=-1) gs = gs.fit(X_train, y_train) print ""finished Gridsearch"" print gs.best_score_",grid search over multiple classifiers
scikit grid search over multiple classifiers," Is there a better inbuilt way to do grid search and test multiple models in a single pipeline? Of course the parameters of the models would be different, which made is complicated for me to figure this out. Here is what I did: However, this approach is still giving the best model within each classifier, and not comparing between classifiers. <code>  from sklearn.pipeline import Pipelinefrom sklearn.ensemble import RandomForestClassifierfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.svm import SVCfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.grid_search import GridSearchCVdef grid_search(): pipeline1 = Pipeline(( ('clf', RandomForestClassifier()), ('vec2', TfidfTransformer()) )) pipeline2 = Pipeline(( ('clf', KNeighborsClassifier()), )) pipeline3 = Pipeline(( ('clf', SVC()), )) pipeline4 = Pipeline(( ('clf', MultinomialNB()), )) parameters1 = { 'clf__n_estimators': [10, 20, 30], 'clf__criterion': ['gini', 'entropy'], 'clf__max_features': [5, 10, 15], 'clf__max_depth': ['auto', 'log2', 'sqrt', None] } parameters2 = { 'clf__n_neighbors': [3, 7, 10], 'clf__weights': ['uniform', 'distance'] } parameters3 = { 'clf__C': [0.01, 0.1, 1.0], 'clf__kernel': ['rbf', 'poly'], 'clf__gamma': [0.01, 0.1, 1.0], } parameters4 = { 'clf__alpha': [0.01, 0.1, 1.0] } pars = [parameters1, parameters2, parameters3, parameters4] pips = [pipeline1, pipeline2, pipeline3, pipeline4] print ""starting Gridsearch"" for i in range(len(pars)): gs = GridSearchCV(pips[i], pars[i], verbose=2, refit=False, n_jobs=-1) gs = gs.fit(X_train, y_train) print ""finished Gridsearch"" print gs.best_score_",grid search over multiple classifiers
to tell fullwidth character from halfwidth in python," I want to check whether a character is fullwidth or halfwidth using Python Please help me to change this pseudocode into real python code. <code>  string=""hallo""for char in string: if( \uFF60- \u0f01 and \uFFE0-\uFFE6 ): print( char +""is fullwidth"") elif(\uFF61-\uFFDC and \uFFE8-\uFFEE):print(char+ "" is halfwidth"")",Checking a character is fullwidth or halfwidth in Python
"Make subset of array, based on values of two ofther arrays in Python"," I am using Python. How to make a subselection of a vector, based on the values of two other vectors with the same length?For example this three vectors I want to do something like this: But the first statement results in an error: In Matlab, I would use: How to do this in Python? <code>  c1 = np.array([1,9,3,5])c2 = np.array([2,2,3,2])c3 = np.array([2,3,2,3])c2==2array([ True, True, False, True], dtype=bool)c3==3array([False, True, False, True], dtype=bool) elem = (c2==2 and c3==3)c1sel = c1[elem] Traceback (most recent call last):File ""<stdin>"", line 1, in <module>ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() elem = find(c2==2 & c3==3);c1sel = c1(elem);","Make subset of array, based on values of two other arrays in Python"
How to dynamically build a JSON object with Python?," I am new to Python and I am playing with JSON data. I would like to dynamically build a JSON object by adding some key-value to an existing JSON object.I tried the following but I get TypeError: 'str' object does not support item assignment: <code>  import jsonjson_data = json.dumps({})json_data[""key""] = ""value""print 'JSON: ', json_data",How to dynamically build a JSON object?
unittest testing Flask-Security: Cannot get past login page," I'm trying to add tests to my basic app. Accessing everything requires login.Here's my test case class: I am running my tests use nosetests in Terminal like so: source my_env/bin/activate && nosetests --exeBasic tests like these fail: From the output, I see that r.data is just the HTML of the login page with no errors (e.g., wrong username or password) or alerts (""Please log in to access this page"").I am logging in during the setUp process, so test_authenticated_access should have let me either access /myroute/ or redirect me to the login page with the flashed message ""Please log in to access this page"". But it didn't.I can't figure out what's wrong. I based my test off of ones I found in Flask documentation and this app boilerplate <code>  class MyAppTestCase(FlaskTestCaseMixin): def _create_app(self): raise NotImplementedError def _create_fixtures(self): self.user = EmployeeFactory() def setUp(self): super(MyAppTestCase, self).setUp() self.app = self._create_app() self.client = self.app.test_client() self.app_context = self.app.app_context() self.app_context.push() db.create_all() self._create_fixtures() self._create_csrf_token() def tearDown(self): super(MyAppTestCase, self).tearDown() db.drop_all() self.app_context.pop() def _post(self, route, data=None, content_type=None, follow_redirects=True, headers=None): content_type = content_type or 'application/x-www-form-urlencoded' return self.client.post(route, data=data, follow_redirects=follow_redirects, content_type=content_type, headers=headers) def _login(self, email=None, password=None): email = email or self.user.email password = password or 'password' data = { 'email': email, 'password': password, 'remember': 'y' } return self._post('/login', data=data)class MyFrontendTestCase(MyAppTestCase): def _create_app(self): return create_app(settings) def setUp(self): super(MyFrontendTestCase, self).setUp() self._login() class CoolTestCase(MyFrontendTestCase): def test_logged_in(self): r = self._login() self.assertIn('MyAppName', r.data) def test_authenticated_access(self): r = self.get('/myroute/') self.assertIn('MyAppName', r.data)",(unittest) Testing Flask-Security: Cannot get past login page
"how to subtract,mathematically, two lists in pyahon?"," I know subtraction of lists is not supported in python, however there are some ways to omit the common elements between two lists. But what I want to do is subtraction of each element in one list individually with the corresponding element in another list and return the result as an output list. How can I do this? <code>  A = [3, 4, 6, 7] B = [1, 3, 6, 3] print A - B #Should print [2, 1, 0, 4]",How to mathematically subtract two lists in python?
python Matplotlib stacked bar chart," I required to come up with a bar chart of different list like so it's just a simple code that I would like to stack them up but I encountered an error and I don't know what to do about it <code>  import mathimport numpy as npimport matplotlib.pyplot as pltmonth=[""dec-09"",""jan"",""feb""]n=len(month)kitchen=[57.801,53.887,49.268]laundry=[53.490,56.568,53.590]air=[383.909,395.913,411.714]other=[519.883,483.293,409.956]ind=np.arange(n)width=0.35p1=plt.bar(ind,kitchen,width,color=""cyan"")p2=plt.bar(ind,laundry,width,color=""red"",bottom=kitchen)p3=plt.bar(ind,air,width,color=""green"",bottom=kitchen+laundry)p4=plt.bar(ind,other,width,color=""blue"",bottom=kitchen+laundry+air)plt.ylabel(""KWH"")plt.title(""winter"")plt.xticks(ind+width/2,(""dec-09"",""jan"",""feb""))plt.show() p3=plt.bar(ind,air,width,color=""green"",bottom=kitchen+laundry)File ""C:\Python33\lib\site-packages\matplotlib\pyplot.py"", line 2515, in barret = ax.bar(left, height, width=width, bottom=bottom, **kwargs)File ""C:\Python33\lib\site-packages\matplotlib\axes.py"", line 5007, in barnbars)AssertionError: incompatible sizes: argument 'bottom' must be length 3 or scalar",matplotlib stacked bar chart AssertionError: incompatible sizes: argument 'bottom' must be length 3 or scalar
Django's prefetch_related for count only," I have a situation something like this (the actual code is bound up in a template, and omitted for brevity). I've managed to considerably reduce the total number of queries using Django's awesome prefetch_related method. However I'm wondering if this situation could be further optimized. From what I understand prefetch_related retrieves all of the data associated with the related models. Seeing as I only care about the amount of related models, and not about the models themselves, it seems like this query could be optimized further so that it doesn't retrieve a bunch of unnecessary data. Is there a way to do this in Django without dropping down to raw SQL?  <code>  threads = Thread.objects.all()for thread in threads: print(thread.comments.count()) print(thread.upvotes.count()) threads = Thread.objects.prefetch_related('comments').prefetch_related('upvotes')",Get count of related model efficiently in Django
Regex: Matches anything except space and new line," I have a string, I just want to match string for any character except for space and new line. What must be regular expression for this?I know regular expressions for anything but space i.e. [^ ]+ and regular expression for anything but new line [^\n]+ (I'm on Windows). I am not able to figure how to club them together. <code> ",How to matches anything except space and new line?
is there a need for import string module in python?," I am a beginner in programming and Python is my first language. I am using a Python shell right now, but don't understand why we need to import the string module.I know that importing string imports some functions, but when I tried using functions like string.split and string.join, they all work without the import, so I assume that means that they are just Python builtins.Is there anything that works once you import the string module that wouldn't work otherwise? <code> ",Is there a reason to import the string module in Python?
Is there a reason to import the string module in python?," I am a beginner in programming and Python is my first language. I am using a Python shell right now, but don't understand why we need to import the string module.I know that importing string imports some functions, but when I tried using functions like string.split and string.join, they all work without the import, so I assume that means that they are just Python builtins.Is there anything that works once you import the string module that wouldn't work otherwise? <code> ",Is there a reason to import the string module in Python?
Why are some numpy calls not implremented as methods?," I always considered Python as a highly object-oriented programming language. Recently, I've been using numpy a lot and I'm beginning to wonder why a number of things are implemented there as functions only, and not as methods of the numpy.array (or ndarray) object.If we have a given array a for example, you can do which seems just fine but there are a lot of calls that do not work in the same way as in: I find this confusing, unintuitive and I do not see any reason behind it. There might be a good one, though; maybe someone can just enlighten me. <code>  a = np.array([1, 2, 3])np.sum(a)>>> 6a.sum()>>> 6 np.amax(a)>>> 3a.amax()>>> AttributeError: 'numpy.ndarray' object has no attribute 'amax'",Why are some numpy calls not implemented as methods?
Asking the User For Input Until He Gives a Valid Response," I am writing a program that accepts an input from the user. The program works as expected as long as the the user enters meaningful data. But it fails if the user enters invalid data: Instead of crashing, I would like the program to ask for the input again. Like this: How can I make the program ask for valid inputs instead of crashing when non-sensical data is entered?How can I reject values like -1, which is a valid int, but nonsensical in this context? <code>  #note: Python 2.7 users should use `raw_input`, the equivalent of 3.X's `input`age = int(input(""Please enter your age: ""))if age >= 18: print(""You are able to vote in the United States!"")else: print(""You are not able to vote in the United States."") C:\Python\Projects> canyouvote.pyPlease enter your age: 23You are able to vote in the United States! C:\Python\Projects> canyouvote.pyPlease enter your age: dickety sixTraceback (most recent call last): File ""canyouvote.py"", line 1, in <module> age = int(input(""Please enter your age: ""))ValueError: invalid literal for int() with base 10: 'dickety six' C:\Python\Projects> canyouvote.pyPlease enter your age: dickety sixSorry, I didn't understand that.Please enter your age: 26You are able to vote in the United States!",Asking the user for input until they give a valid response
Asking the user for input until he gives a valid response," I am writing a program that accepts an input from the user. The program works as expected as long as the the user enters meaningful data. But it fails if the user enters invalid data: Instead of crashing, I would like the program to ask for the input again. Like this: How can I make the program ask for valid inputs instead of crashing when non-sensical data is entered?How can I reject values like -1, which is a valid int, but nonsensical in this context? <code>  #note: Python 2.7 users should use `raw_input`, the equivalent of 3.X's `input`age = int(input(""Please enter your age: ""))if age >= 18: print(""You are able to vote in the United States!"")else: print(""You are not able to vote in the United States."") C:\Python\Projects> canyouvote.pyPlease enter your age: 23You are able to vote in the United States! C:\Python\Projects> canyouvote.pyPlease enter your age: dickety sixTraceback (most recent call last): File ""canyouvote.py"", line 1, in <module> age = int(input(""Please enter your age: ""))ValueError: invalid literal for int() with base 10: 'dickety six' C:\Python\Projects> canyouvote.pyPlease enter your age: dickety sixSorry, I didn't understand that.Please enter your age: 26You are able to vote in the United States!",Asking the user for input until they give a valid response
Flask-SQLAlchemy invalid transaction persisting across requests," SummaryOne of our threads in production hit an error and is now producing InvalidRequestError: This session is in 'prepared' state; no further SQL can be emitted within this transaction. errors, on every request with a query that it serves, for the rest of its life! It's been doing this for days, now! How is this possible, and how can we prevent it going forward?BackgroundWe are using a Flask app on uWSGI (4 processes, 2 threads), with Flask-SQLAlchemy providing us DB connections to SQL Server.The problem seemed to start when one of our threads in production was tearing down its request, inside this Flask-SQLAlchemy method: ...and somehow managed to call self.session.commit() when the transaction was invalid. This resulted in sqlalchemy.exc.InvalidRequestError: Can't reconnect until invalid transaction is rolled back getting output to stdout, in defiance of our logging configuration, which makes sense since it happened during the app context tearing down, which is never supposed to raise exceptions. I'm not sure how the transaction got to be invalid without response_or_exec getting set, but that's actually the lesser problem AFAIK.The bigger problem is, that's when the ""'prepared' state"" errors started, and haven't stopped since. Every time this thread serves a request that hits the DB, it 500s. Every other thread seems to be fine: as far as I can tell, even the thread that's in the same process is doing OK.Wild guessThe SQLAlchemy mailing list has an entry about the ""'prepared' state"" error saying it happens if a session started committing and hasn't finished yet, and something else tries to use it. My guess is that the session in this thread never got to the self.session.remove() step, and now it never will.I still feel like that doesn't explain how this session is persisting across requests though. We haven't modified Flask-SQLAlchemy's use of request-scoped sessions, so the session should get returned to SQLAlchemy's pool and rolled back at the end of the request, even the ones that are erroring (though admittedly, probably not the first one, since that raised during the app context tearing down). Why are the rollbacks not happening? I could understand it if we were seeing the ""invalid transaction"" errors on stdout (in uwsgi's log) every time, but we're not: I only saw it once, the first time. But I see the ""'prepared' state"" error (in our app's log) every time the 500s occur.Configuration detailsWe've turned off expire_on_commit in the session_options, and we've turned on SQLALCHEMY_COMMIT_ON_TEARDOWN. We're only reading from the database, not writing yet. We're also using Dogpile-Cache for all of our queries (using the memcached lock since we have multiple processes, and actually, 2 load-balanced servers). The cache expires every minute for our major query.Updated 2014-04-28: Resolution stepsRestarting the server seems to have fixed the problem, which isn't entirely surprising. That said, I expect to see it again until we figure out how to stop it. benselme (below) suggested writing our own teardown callback with exception handling around the commit, but I feel like the bigger problem is that the thread was messed up for the rest of its life. The fact that this didn't go away after a request or two really makes me nervous! <code>  @teardowndef shutdown_session(response_or_exc): if app.config['SQLALCHEMY_COMMIT_ON_TEARDOWN']: if response_or_exc is None: self.session.commit() self.session.remove() return response_or_exc",Invalid transaction persisting across requests
"what's the different between getattr(self, obj) and self.obj in python?"," I thought they were the same before I ran this code: It raises AttributeError: 'C' object has no attribute '__a' with the getattr statement, but why? <code>  class B(object): def show(self): self.__a = ""test"" print ""B"" def this_b(self): print ""this_b"" print self.__a print getattr(self, '__a') #exceptionclass C(B): def show(self): print ""C"" # B.show(self) super(C, self).show() def call(self): print ""call"" self.show() self.this_b() # print self.__aC().call()","What's the difference between getattr(self, '__a') and self.__a in python?"
"what's the different between getattr(self, 'obj') and self.obj in python?"," I thought they were the same before I ran this code: It raises AttributeError: 'C' object has no attribute '__a' with the getattr statement, but why? <code>  class B(object): def show(self): self.__a = ""test"" print ""B"" def this_b(self): print ""this_b"" print self.__a print getattr(self, '__a') #exceptionclass C(B): def show(self): print ""C"" # B.show(self) super(C, self).show() def call(self): print ""call"" self.show() self.this_b() # print self.__aC().call()","What's the difference between getattr(self, '__a') and self.__a in python?"
"what's the different between getattr(self, '__a') and self.__a in python?"," I thought they were the same before I ran this code: It raises AttributeError: 'C' object has no attribute '__a' with the getattr statement, but why? <code>  class B(object): def show(self): self.__a = ""test"" print ""B"" def this_b(self): print ""this_b"" print self.__a print getattr(self, '__a') #exceptionclass C(B): def show(self): print ""C"" # B.show(self) super(C, self).show() def call(self): print ""call"" self.show() self.this_b() # print self.__aC().call()","What's the difference between getattr(self, '__a') and self.__a in python?"
"Flask Microframework with Python, I keep getting TemplateNotFound error"," I am trying to render the file home.html. The file exists in my project, but I keep getting jinja2.exceptions.TemplateNotFound: home.html when I try to render it. Why can't Flask find my template?  <code>  from flask import Flask, render_templateapp = Flask(__name__)@app.route('/')def home(): return render_template('home.html') /myproject app.py home.html",Flask raises TemplateNotFound error even though template file exists
Which cut-off for collapsing this tree?," I have a Newick tree that is built by comparing similarity (euclidean distance) of Position Weight Matrices (PWMs or PSSMs) of putative DNA regulatory motifs that are 4-9 bp long DNA sequences. An interactive version of the tree is up on iTol (here), which you can freely play with - just press ""update tree"" after setting your parameters:My specific goal: to collapse the motifs (tips/terminal nodes/leaves) together if their average distances to the nearest parent clade is < X (ETE2 Python package). This is biologically interesting since some of the gene regulatory DNA motifs may be homologous (paralogues or orthologues) with one another. This collapsing can be done via the iTol GUI linked above, e.g. if you choose X = 0.001 then some motifs become collapsed into triangles (motif families). My question: Could anybody suggest an algorithm that would either output or help visualise which value of X is appropriate for ""maximizing the biological or statistical relevance"" of the collapsed motifs? Ideally there would be some obvious step change in some property of the tree when plotted against X which suggests to the algorithm a sensible X. Are there any known algorithms/scripts/packages for this? Perhaps the code will plot some statistic against the value of X? I've tried plotting X vs. mean cluster size (matplotlib) but I don't see an obvious ""step increase"" to inform me which value of X to use:My code and data: A link to my Python script is [here][8], I have heavily commented it and it will generate the tree data and plot above for you (use the arguments d_from, d_to and d_step to explore the distance cut-offs, X). You will need to install ete2 by simply executing these two bash commands if you have easy-install and Python: <code>  apt-get install python-setuptools python-numpy python-qt4 python-scipy python-mysqldb python-lxmleasy_install -U ete2",Algorithm to decide cut-off for collapsing this tree?
Program to decide cut-off for collapsing this tree?," I have a Newick tree that is built by comparing similarity (euclidean distance) of Position Weight Matrices (PWMs or PSSMs) of putative DNA regulatory motifs that are 4-9 bp long DNA sequences. An interactive version of the tree is up on iTol (here), which you can freely play with - just press ""update tree"" after setting your parameters:My specific goal: to collapse the motifs (tips/terminal nodes/leaves) together if their average distances to the nearest parent clade is < X (ETE2 Python package). This is biologically interesting since some of the gene regulatory DNA motifs may be homologous (paralogues or orthologues) with one another. This collapsing can be done via the iTol GUI linked above, e.g. if you choose X = 0.001 then some motifs become collapsed into triangles (motif families). My question: Could anybody suggest an algorithm that would either output or help visualise which value of X is appropriate for ""maximizing the biological or statistical relevance"" of the collapsed motifs? Ideally there would be some obvious step change in some property of the tree when plotted against X which suggests to the algorithm a sensible X. Are there any known algorithms/scripts/packages for this? Perhaps the code will plot some statistic against the value of X? I've tried plotting X vs. mean cluster size (matplotlib) but I don't see an obvious ""step increase"" to inform me which value of X to use:My code and data: A link to my Python script is [here][8], I have heavily commented it and it will generate the tree data and plot above for you (use the arguments d_from, d_to and d_step to explore the distance cut-offs, X). You will need to install ete2 by simply executing these two bash commands if you have easy-install and Python: <code>  apt-get install python-setuptools python-numpy python-qt4 python-scipy python-mysqldb python-lxmleasy_install -U ete2",Algorithm to decide cut-off for collapsing this tree?
can i write the output format by prettytable into a file?," prettytable.PrettyTable can represent tabular data in visually appealing formatted tables, and print these tables to the terminal. How do I save a table as a text file? <code> ",Can i write the output format created by prettytable into a file?
Pymongo find and update," I have a find query in a mongodb collection and would like this query to also update a field... something like this... I found the findandmodify method but could not find any example of how to use it... Thanks in advance for any help! <code>  db = pymongo.MongoClient(DB_HOST)[COLLECTION][Product]new_posts = db.find({'type':{'$ne':'overview'}, 'indice':0, 'thread_id':{'$nin':front_db_ids}, 'updated':{'$exists':False}},{'_id': 0}) + {{'$set': {'updated':'yes'}}, multi=True",Pymongo find and modify
Check if main script is still running from a thread," How can I check if the Main Thread is alive from another ( non-daemon, child ) thread?The child thread is a non-daemon thread and I'd like to check if the Main thread is still running or not, and stop this non-daemon thread based on the result.( Making the thread daemon is not good for my situation because my thread writes to stdout which creates problems when the thread is set as a daemon)Using python 2.7 <code> ",Check if the Main Thread is still alive from another thread
Check if a thread's main script is still alive," How can I check if the Main Thread is alive from another ( non-daemon, child ) thread?The child thread is a non-daemon thread and I'd like to check if the Main thread is still running or not, and stop this non-daemon thread based on the result.( Making the thread daemon is not good for my situation because my thread writes to stdout which creates problems when the thread is set as a daemon)Using python 2.7 <code> ",Check if the Main Thread is still alive from another thread
How to export graph in RDF file using RDFLIB (python)," I'm trying to generate RDF data using RDFLib in Python 3.4.A minimal example: This code results in the following error: If I replace the last line with: I do not get the error, but the result is a string representation of a binary stream (a single line of text starting with b'): QuestionHow do I correctly export the graph into a file? <code>  from rdflib import Namespace, URIRef, Graphfrom rdflib.namespace import RDF, FOAFdata = Namespace(""http://www.example.org#"")g = Graph()g.add( (URIRef(data.Alice), RDF.type , FOAF.person) )g.add( (URIRef(data.Bob), RDF.type , FOAF.person) )g.add( (URIRef(data.Alice), FOAF.knows, URIRef(data.Bob)) )#write attemptfile = open(""output.txt"", mode=""w"")file.write(g.serialize(format='turtle')) file.write(g.serialize(format='turtle'))TypeError : must be str, not bytes file.write(str(g.serialize(format='turtle'))) b'@prefix ns1: <http://xmlns.com/foaf/0.1/> .\n@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n@prefix xml: <http://www.w3.org/XML/1998/namespace> .\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n\n<http://www.example.org#Alice> a ns1:person ;\n ns1:knows <http://www.example.org#Bob> .\n\n<http://www.example.org#Bob> a ns1:person .\n\n'",How to export graph in RDF file using RDFLib
Return multiple columns from apply pandas," I have a pandas DataFrame, df_test. It contains a column 'size' which represents size in bytes. I've calculated KB, MB, and GB using the following code: I've run this over 120,000 rows and time it takes about 2.97 seconds per column * 3 = ~9 seconds according to %timeit.Is there anyway I can make this faster? For example, can I instead of returning one column at a time from apply and running it 3 times, can I return all three columns in one pass to insert back into the original dataframe?The other questions I've found all want to take multiple values and return a single value. I want to take a single value and return multiple columns. <code>  df_test = pd.DataFrame([ {'dir': '/Users/uname1', 'size': 994933}, {'dir': '/Users/uname2', 'size': 109338711},])df_test['size_kb'] = df_test['size'].astype(int).apply(lambda x: locale.format(""%.1f"", x / 1024.0, grouping=True) + ' KB')df_test['size_mb'] = df_test['size'].astype(int).apply(lambda x: locale.format(""%.1f"", x / 1024.0 ** 2, grouping=True) + ' MB')df_test['size_gb'] = df_test['size'].astype(int).apply(lambda x: locale.format(""%.1f"", x / 1024.0 ** 3, grouping=True) + ' GB')df_test dir size size_kb size_mb size_gb0 /Users/uname1 994933 971.6 KB 0.9 MB 0.0 GB1 /Users/uname2 109338711 106,776.1 KB 104.3 MB 0.1 GB[2 rows x 5 columns]",Return multiple columns from pandas apply()
Python: how to batch rename mixedCase to lower_case_with_underscores," I've written a fair bit of my first significant Python script. I just finished reading PEP 8, and I learned that lower_case_with_underscores is preferred for instance variable names. I've been using mixedCase for variable names throughout, and I'd like my code to be make more Pythonic by changing those to lower_case_with_underscores if that's how we do things around here.I could probably write some script that searches for mixedCase and tries to smartly replace it, but before I potentially reinvent the wheel, my question is whether a solution for that already exists, either within a Python-savvy editor or as a standalone application; or whether there's another approach that would accomplish the task of converting all mixedCase variable names to lower_case_with_underscores. I have searched a fair bit for a solution but didn't turn up anything. Any technique that specifically would yield this result would be appreciated. <code> ",Python: how to batch rename mixed case to lower case with underscores
Efficient array multiplication in Numpy (paralelization)," I have compiled numpy 1.6.2 and scipy with MKL hoping to have a better performance.Currently I have a code that relies heavily on np.einsum(), and I was told that einsum is not good with MKL, because there is almost none vectorization. =(So I was thinking to re write some of my code with np.dot() and slicing, just to be able to get some multi-core speed up.I really like the simplicity of np.einsum() and the readability is good to.Anyway, for example, I have a multi-dimensional matrix multiplication of the form: So how do I transform something like this, or others 3,4 and 5 dimensional array multiplications in np.dot() efficient MKL operations?I will ad more info:I am computing this equation:For doing this, I am using the code: That is not that fast, same thing coded in cython is 5x times faster: Is there a way to do this in pure python with the performance of cython? (I havent been able to figure out how to tensordot this equation)Haven't been able to do prange in this cython code, lots gil and nogil errors. <code>  np.einsum('mi,mnijqk->njqk',A,B) np.einsum('mn,mni,nij,nik,mi->njk',a,np.exp(b[:,:,np.newaxis]*U[np.newaxis,:,:]),P,P,X) #STACKOVERFLOW QUESTION:from __future__ import divisionimport numpy as npcimport numpy as npcimport cythoncdef extern from ""math.h"": double exp(double x)DTYPE = np.floatctypedef np.float_t DTYPE_t@cython.boundscheck(False) # turn of bounds-checking for entire functiondef cython_DX_h(np.ndarray[DTYPE_t, ndim=3] P, np.ndarray[DTYPE_t, ndim=1] a, np.ndarray[DTYPE_t, ndim=1] b, np.ndarray[DTYPE_t, ndim=2] U, np.ndarray[DTYPE_t, ndim=2] X, int I, int M): assert P.dtype == DTYPE and a.dtype == DTYPE and b.dtype == DTYPE and U.dtype == DTYPE and X.dtype == DTYPEcdef np.ndarray[DTYPE_t,ndim=3] DX_h=np.zeros((N,I,I),dtype=DTYPE)cdef unsigned int j,n,k,m,ifor n in range(N): for j in range(I): for k in range(I): aux=0 for m in range(N): for i in range(I): aux+=a[m,n]*exp(b[m,n]*U[n,i])*P[n,i,j]*P[n,i,k]*X[m,i] DX_h[n,j,k]=auxreturn DX_h",Numpy np.einsum array multiplication using multiple cores
N-D array multiplication using multiple cores," I have compiled numpy 1.6.2 and scipy with MKL hoping to have a better performance.Currently I have a code that relies heavily on np.einsum(), and I was told that einsum is not good with MKL, because there is almost none vectorization. =(So I was thinking to re write some of my code with np.dot() and slicing, just to be able to get some multi-core speed up.I really like the simplicity of np.einsum() and the readability is good to.Anyway, for example, I have a multi-dimensional matrix multiplication of the form: So how do I transform something like this, or others 3,4 and 5 dimensional array multiplications in np.dot() efficient MKL operations?I will ad more info:I am computing this equation:For doing this, I am using the code: That is not that fast, same thing coded in cython is 5x times faster: Is there a way to do this in pure python with the performance of cython? (I havent been able to figure out how to tensordot this equation)Haven't been able to do prange in this cython code, lots gil and nogil errors. <code>  np.einsum('mi,mnijqk->njqk',A,B) np.einsum('mn,mni,nij,nik,mi->njk',a,np.exp(b[:,:,np.newaxis]*U[np.newaxis,:,:]),P,P,X) #STACKOVERFLOW QUESTION:from __future__ import divisionimport numpy as npcimport numpy as npcimport cythoncdef extern from ""math.h"": double exp(double x)DTYPE = np.floatctypedef np.float_t DTYPE_t@cython.boundscheck(False) # turn of bounds-checking for entire functiondef cython_DX_h(np.ndarray[DTYPE_t, ndim=3] P, np.ndarray[DTYPE_t, ndim=1] a, np.ndarray[DTYPE_t, ndim=1] b, np.ndarray[DTYPE_t, ndim=2] U, np.ndarray[DTYPE_t, ndim=2] X, int I, int M): assert P.dtype == DTYPE and a.dtype == DTYPE and b.dtype == DTYPE and U.dtype == DTYPE and X.dtype == DTYPEcdef np.ndarray[DTYPE_t,ndim=3] DX_h=np.zeros((N,I,I),dtype=DTYPE)cdef unsigned int j,n,k,m,ifor n in range(N): for j in range(I): for k in range(I): aux=0 for m in range(N): for i in range(I): aux+=a[m,n]*exp(b[m,n]*U[n,i])*P[n,i,j]*P[n,i,k]*X[m,i] DX_h[n,j,k]=auxreturn DX_h",Numpy np.einsum array multiplication using multiple cores
how to check whether a jpeg image is color or gray scale using pyhton only, I have to write a test case in python to check whether a jpg image is in color or grayscale. Can anyone please let me know if there is any way to do it with out installing extra libraries like opencv? <code> ,How to check whether a jpeg image is color or gray scale using only Python stdlib
How to pass a value of an environment variable for debugging in PyCharm?," I am trying to learn PyCharm, need to pass an environment variable as a command line parameter to my process, e.g. execute an equivalent of myScript.py -u $myVar on Linux, or myScript.py -u %myVar% on Windows.How do I specify that in the PyCharm configuration? I don't want my script to depend on the name myVar, just on the content of that environment variable. <code> ",How to pass an environment variable as a command line parameter in Run/Debug configuration in PyCharm?
Use Python to write CSV output to STDOUT," I know I can write a CSV file with something like: How would I instead write that output to stdout? <code>  with open('some.csv', 'w', newline='') as f:",How to write CSV output to stdout?
Drop all duplicate rows in Python Pandas," The pandas drop_duplicates function is great for ""uniquifying"" a dataframe. However, one of the keyword arguments to pass is take_last=True or take_last=False, while I would like to drop all rows which are duplicates across a subset of columns. Is this possible? As an example, I would like to drop rows which match on columns A and C so this should drop rows 0 and 1. <code>  A B C0 foo 0 A1 foo 1 A2 foo 1 B3 bar 1 A",Drop all duplicate rows across multiple columns in Python Pandas
Drop all duplicate rows avross multiple columns in Python Pandas," The pandas drop_duplicates function is great for ""uniquifying"" a dataframe. However, one of the keyword arguments to pass is take_last=True or take_last=False, while I would like to drop all rows which are duplicates across a subset of columns. Is this possible? As an example, I would like to drop rows which match on columns A and C so this should drop rows 0 and 1. <code>  A B C0 foo 0 A1 foo 1 A2 foo 1 B3 bar 1 A",Drop all duplicate rows across multiple columns in Python Pandas
PyMongo complex sort," I wondering how to convert the follow mongodb query to pymongo syntax I tried this: But I got the follow error on sort: Anyone can help me?Thanks in advance <code>  db.articles.find( { $text: { $search: ""cake"" } }, { score: { $meta: ""textScore"" } }).sort( { score: { $meta: ""textScore"" } } ).limit(3) results = \ mongo.db.products.find({ '$text': { '$search': 'cake' } }, { 'score': { '$meta': 'textScore' } }) \ .sort({ 'score': { '$meta': 'textScore' } }) \ .limit(3) raise TypeError(""second item in each key pair must be 1, -1, ""TypeError: second item in each key pair must be 1, -1, '2d', 'geoHaystack', or another valid MongoDB index specifier.",PyMongo sort with metadata
"Pre-Populate a WTform in flask, with data from a sql-alchamy object"," I am fairly new to flask framework and was creating an edit profile page for a webportal. I am stuck at a point and am unable to autofill a form.Here is my form class : This is my function that evaluates the form. And my html template for form is form is : I have an object, of user class. And from that object I want to prefill this form.How can I prepopulate the values in the form. I am trying to implement the edit profile functionality here.  <code>  class EditProfile(Form): username = TextField('Username', [Required()]) email = TextField('Email', [Required()]) about = TextAreaField('About', [Required()]) website = TextField('Website', [Required()]) def editprofile(nickname = None): if g.fas_user['username'] == nickname or request.method == 'POST': form = EditProfile() form_action = url_for('profile.editprofile') if request.method == 'POST' and form.validate(): if form.username.data == nickname : query = EditProfile(form.username.data, form.email.data, form.about.data, form.website.data, ) print query #debug db.session.add(query) db.session.commit() flash('User Updated') print ""added"" return(url_for('profile.editprofile')) return render_template('profile/add.html', form=form, form_action=form_action, title=""Update Profile"") else: return ""Unauthorised"" {% extends ""base.html"" %} {% block title %} {{ title }} {% endblock %} {% block content %} {% from ""_formhelpers.html"" import render_field %} <div id=""Edit Profile""> <h2>{{ title }}</h2> <form method=""post"" action=""{{ form_action }}""> <fieldset> <legend></legend> {{ render_field(form.username) }} {{ render_field(form.email)}} {{ render_field(form.about )}} {{ render_field(form.website) }} </fieldset> <input type=""submit"" class=""button"" value=""Save""/> </form> </div> {% endblock %}","Pre-Populate a WTforms in flask, with data from a SQLAlchemy object"
How can I return python's import this as a string?," Typing will return the Zen of Python, but nowhere do I seem to be able find a solution about how to set it equal to a string variable which I can use further on in my code... <code>  import this ","How can I return python's ""import this"" as a string?"
Literal parenthesis with python regex," I have a dictionary ( e.g. English - Croatian). It may contain sentences and phrases. I'm translating a file of form ""english text"" = ""english text"" into form ""english text"" = ""croatian text"" and using python regex module to do so. The regex I'm using looks like this (given variable original which is text in English that should be translated: That way I'am able to capture exactly the english text inside the quotes on the right-hand side and substitute it with Croatian. However, the problem appears if the original text contains parenthesis inside. In example: In that case an error ""unbalanced parenthesis"" is raised. If original would be hard-coded, I could solve the problem by putting However, there is a whole file full of *original * variables.Is there any solution to this problem other than changing original variable by preceeding all parenthesis in it with a backslash? <code>  regexString = '(?<= = "")'+original+'(?="")' original = 'This is a wonderland :)' original = 'This is a wonderland :\\)'",Literal parenthesis with python regex
Literal parenthesis with python regex.," I have a dictionary ( e.g. English - Croatian). It may contain sentences and phrases. I'm translating a file of form ""english text"" = ""english text"" into form ""english text"" = ""croatian text"" and using python regex module to do so. The regex I'm using looks like this (given variable original which is text in English that should be translated: That way I'am able to capture exactly the english text inside the quotes on the right-hand side and substitute it with Croatian. However, the problem appears if the original text contains parenthesis inside. In example: In that case an error ""unbalanced parenthesis"" is raised. If original would be hard-coded, I could solve the problem by putting However, there is a whole file full of *original * variables.Is there any solution to this problem other than changing original variable by preceeding all parenthesis in it with a backslash? <code>  regexString = '(?<= = "")'+original+'(?="")' original = 'This is a wonderland :)' original = 'This is a wonderland :\\)'",Literal parenthesis with python regex
"Is a Python requests Session object shared between gevent greenlets, thread-safe?"," Can a requests library session object be used across greenlets safely in a gevented program?EDIT - ADDING MORE EXPLANATION:When a greenlet yields because it has made a socket call to send the request to the server, can the same socket (inside the shared session object) be used safely by another greenlet to send its own request?END EDITI attempted to test this with the code posted here - https://gist.github.com/donatello/0b399d0353cb29dc91b0 - however I got no errors or unexpected results. However, this does not validate thread safety.In the test, I use a shared session object to make lots of requests and try to see if the object gets the requests mixed up - it is kind of naive, but I do not get any exceptions.For convenience, I am re-pasting the code here:client.py server.py Exact library versions:requirements.txt Is there some other test that can check greenlet thread safety? The requests documentation is not too clear on this point. <code>  import geventfrom gevent.monkey import patch_allpatch_all()import requestsimport jsons = requests.Session()def make_request(s, d): r = s.post(""http://127.0.0.1:5000"", data=json.dumps({'value': d})) if r.content.strip() != str(d*2): print(""Sent %s got %s"" % (r.content, str(d*2))) if r.status_code != 200: print(r.status_code) print(r.content)gevent.joinall([ gevent.spawn(make_request, s, v) for v in range(300)]) from gevent.wsgi import WSGIServerfrom gevent.monkey import patch_allpatch_all()from flask import Flaskfrom flask import requestimport timeimport jsonapp = Flask(__name__)@app.route('/', methods=['POST', 'GET'])def hello_world(): d = json.loads(request.data) return str(d['value']*2)if __name__ == '__main__': http_server = WSGIServer(('', 5000), app) http_server.serve_forever() Flask==0.10.1Jinja2==2.7.2MarkupSafe==0.23Werkzeug==0.9.4argparse==1.2.1gevent==1.0.1greenlet==0.4.2gunicorn==18.0itsdangerous==0.24requests==2.3.0wsgiref==0.1.2","Is a Python requests Session object shared between gevent greenlets, thread-safe (between greenlets)?"
Why Virtualenv is preffered during python project development?, I am a beginner in Python. I read virtualenv is preferred during Python project development.I couldn't understand this point at all. Why is virtualenv preferred? <code> ,Why is virtualenv necessary?
Django - staticfiles_dirs vs static_root vs media_root," What are the differences of these three static url? I am not sure if I am right, I am using the MEDIA_ROOT to store my uploaded photos (via models.ImageField()) However, I created a JS script to my admin and in admin.py. I defined the media as below: and my settings.py: and I added the custom.js to STATIC_ROOT/admin/custom.js, but it is not working. Throwing 404 not found error. And then I change the STATIC_ROOT to STATICFILES_DIRS, and it works!! So, I am not understand what is going on here. In fact, I just don't understand what is the difference between STATIC_ROOT and STATICFILES_DIRS. Currently I am testing Django in my machine via virtualenv, not deployed yet, is it the reason STATIC_ROOT not working??  <code>  ....class Media: js = ('/admin/custom.js', ) .... STATIC_ROOT = ""/home/user/project/django1/top/listing/static"" ....STATICFILES_DIRS = ""/home/user/project/django1/top/listing/static""","Differences between STATICFILES_DIR, STATIC_ROOT and MEDIA_ROOT"
add column with constant value to pandas dataframe," Given a DataFrame: What is the simplest way to add a new column containing a constant value eg 0? This is my solution, but I don't know why this puts NaN into 'new' column? <code>  np.random.seed(0)df = pd.DataFrame(np.random.randn(3, 3), columns=list('ABC'), index=[1, 2, 3])df A B C1 1.764052 0.400157 0.9787382 2.240893 1.867558 -0.9772783 0.950088 -0.151357 -0.103219 A B C new1 1.764052 0.400157 0.978738 02 2.240893 1.867558 -0.977278 03 0.950088 -0.151357 -0.103219 0 df['new'] = pd.Series([0 for x in range(len(df.index))]) A B C new1 1.764052 0.400157 0.978738 0.02 2.240893 1.867558 -0.977278 0.03 0.950088 -0.151357 -0.103219 NaN",Add column with constant value to pandas dataframe
How can I evaluate a list comprehension from the Python debugger?," In debugging my code, I want to use a list comprehension. However, it seems I cannot evaluate a list comprehension from the debugger when I'm inside a function.I am using Python 3.4.Script contents: Interactive debugging: Why is x unknown to the list comprehension? How could I evaluate a list comprehension from the debugger, or achieve an equivalent behaviour? Is this a bug, or is it some sort of fundamental limitation to the debugger? <code>  $ cat test.py #!/usr/bin/pythondef foo(): x = [1, 2, 3, 3, 4] print(x)foo() $ python3 -mpdb test.py > /tmp/test.py(3)<module>()-> def foo():(Pdb) step> /tmp/test.py(8)<module>()-> foo()(Pdb) --Call--> /tmp/test.py(3)foo()-> def foo():(Pdb) > /tmp/test.py(4)foo()-> x = [1, 2, 3, 3, 4](Pdb) > /tmp/test.py(6)foo()-> print(x)(Pdb) p [x for _ in range(1)]*** NameError: name 'x' is not defined(Pdb) p x[1, 2, 3, 3, 4]",List comprehension scope error from Python debugger
"Pandas: Group by calendar-week, then plot barplot for the real datetime"," EDITI found a quite nice solution and posted it below as an answer.The result will look like this:Some example data you can generate for this problem: resulting in: I'd like to group by calendar-week and by value of col1. Like this: resulting in: Then I want a plot to be generated like this:That means: calendar-week and year (datetime) on the x-axis and for each of the grouped col1 one bar.The problem I'm facing is: I only have integers describing the calendar week (KW in the plot), but I somehow have to merge back the date on it to get the ticks labeled by year as well. Furthermore I can't only plot the grouped calendar week because I need a correct order of the items (kw 47, kw 48 (year 2013) have to be on the left side of kw 1 (because this is 2014)).EDITI figured out from here:http://pandas.pydata.org/pandas-docs/stable/visualization.html#visualization-barplot that grouped bars need to be columns instead of rows. So I thought about how to transform the data and found the method pivot which turns out to be a great function. reset_index is needed to transform the multiindex into columns. At the end I fill NaNs by zero: transforms the data into: which looks like the example data in the docs to be plotted in grouped bars: gets this:whereas I have the problem with the axis as it is now sorted (from 1-52), which is actually wrong, because calendar week 52 belongs to year 2013 in this case... Any ideas on how to merge back the real datetime for the calendar-weeks and use them as x-axis ticks? <code>  codes = list('ABCDEFGH'); dates = pd.Series(pd.date_range('2013-11-01', '2014-01-31')); dates = dates.append(dates)dates.sort()df = pd.DataFrame({'amount': np.random.randint(1, 10, dates.size), 'col1': np.random.choice(codes, dates.size), 'col2': np.random.choice(codes, dates.size), 'date': dates}) In [55]: dfOut[55]: amount col1 col2 date0 1 D E 2013-11-010 5 E B 2013-11-011 5 G A 2013-11-021 7 D H 2013-11-022 5 E G 2013-11-032 4 H G 2013-11-033 7 A F 2013-11-043 3 A A 2013-11-044 1 E G 2013-11-054 7 D C 2013-11-055 5 C A 2013-11-065 7 H F 2013-11-066 1 G B 2013-11-076 8 D A 2013-11-077 1 B H 2013-11-087 8 F H 2013-11-088 3 A E 2013-11-098 1 H D 2013-11-099 3 B D 2013-11-109 1 H G 2013-11-1010 6 E E 2013-11-1110 6 F E 2013-11-1111 2 G B 2013-11-1211 5 H H 2013-11-1212 5 F G 2013-11-1312 5 G B 2013-11-1313 8 H B 2013-11-1413 6 G F 2013-11-1414 9 F C 2013-11-1514 4 H A 2013-11-15.. ... ... ... ...77 9 A B 2014-01-1777 7 E B 2014-01-1778 4 F E 2014-01-1878 6 B E 2014-01-1879 6 A H 2014-01-1979 3 G D 2014-01-1980 7 E E 2014-01-2080 6 G C 2014-01-2081 9 H G 2014-01-2181 9 C B 2014-01-2182 2 D D 2014-01-2282 7 D A 2014-01-2283 6 G B 2014-01-2383 1 A G 2014-01-2384 9 B D 2014-01-2484 7 G D 2014-01-2485 7 A F 2014-01-2585 9 B H 2014-01-2586 9 C D 2014-01-2686 5 E B 2014-01-2687 3 C H 2014-01-2787 7 F D 2014-01-2788 3 D G 2014-01-2888 4 A D 2014-01-2889 2 F A 2014-01-2989 8 D A 2014-01-2990 1 A G 2014-01-3090 6 C A 2014-01-3091 6 H C 2014-01-3191 2 G F 2014-01-31[184 rows x 4 columns] kw = lambda x: x.isocalendar()[1]grouped = df.groupby([df['date'].map(kw), 'col1'], sort=False).agg({'amount': 'sum'}) In [58]: groupedOut[58]: amountdate col144 D 8 E 10 G 5 H 445 D 15 E 1 G 1 H 9 A 13 C 5 B 4 F 846 E 7 G 13 H 17 B 9 F 2347 G 14 H 4 A 40 C 7 B 16 F 1348 D 7 E 16 G 9 H 2 A 7 C 7 B 2... ...1 H 14 A 14 B 15 F 192 D 13 H 13 A 13 B 10 F 323 D 8 E 18 G 3 H 6 A 30 C 9 B 6 F 54 D 9 E 12 G 19 H 9 A 8 C 18 B 185 D 11 G 2 H 6 A 5 C 9 F 9[87 rows x 1 columns] A = grouped.reset_index().pivot(index='date', columns='col1', values='amount').fillna(0) col1 A B C D E F G Hdate1 4 31 0 0 0 18 13 82 0 12 13 22 1 17 0 83 3 10 4 13 12 8 7 64 17 0 10 7 0 25 7 45 7 0 7 9 8 6 0 744 0 0 2 11 7 0 0 245 9 3 2 14 0 16 21 246 0 14 7 2 17 13 11 847 5 13 0 15 19 7 5 1048 15 8 12 2 20 4 7 649 20 0 0 18 22 17 11 050 7 11 8 6 5 6 13 1051 8 26 0 0 5 5 16 952 8 13 7 5 4 10 0 11 A. plot(kind='bar')","Pandas: Group by calendar-week, then plot grouped barplots for the real datetime"
Pyhton output on both console and file," I'm writing a code to analyze PDF file. I want to display the output on the console as well as to have a copy of the output in a file, I used this code save the output in a file: but could I display the output into console as well but without using classes because I'm not good with them? <code>  import syssys.stdout = open('C:\\users\\Suleiman JK\\Desktop\\file.txt',""w"")print ""test""",Python output on both console and file
About the changing id of a Python immutable string," Something about the id of objects of type str (in python 2.7) puzzles me. The str type is immutable, so I would expect that once it is created, it will always have the same id. I believe I don't phrase myself so well, so instead I'll post an example of input and output sequence. so in the meanwhile, it changes all the time. However, after having a variable pointing at that string, things change: So it looks like it freezes the id, once a variable holds that value. Indeed, after del so and del not_so, the output of id('so') start changing again.This is not the same behaviour as with (small) integers.I know there is not real connection between immutability and having the same id; still, I am trying to figure out the source of this behaviour. I believe that someone whose familiar with python's internals would be less surprised than me, so I am trying to reach the same point...UpdateTrying the same with a different string gave different results... Now it is equal... <code>  >>> id('so')140614155123888>>> id('so')140614155123848>>> id('so')140614155123808 >>> so = 'so'>>> id('so')140614155123728>>> so = 'so'>>> id(so)140614155123728>>> not_so = 'so'>>> id(not_so)140614155123728 >>> id('hello')139978087896384>>> id('hello')139978087896384>>> id('hello')139978087896384",About the changing id of an immutable string
Python pandas.read_csv : low_memory and memory_map flags," the function signature for pandas.read_csv gives, among others, the following options: I couldn't find any documentation for either low_memoryor memory_map flags. I am confused about whether these features are implemented yet and if so how do they work. Specifically,memory_map: If implemented does it use np.memmap and if so does it store the individual columns as memmap or the rows. low_memory: Does it specify something like cache to store in memory?can we convert an existing DataFrame to a memmapped DataFrameP.S. : versions of relevant modules  <code>  read_csv(filepath_or_buffer, low_memory=True, memory_map=False, iterator=False, chunksize=None, ...) pandas==0.14.0scipy==0.14.0numpy==1.8.1",What do low_memory and memory_map flags do in pd.read_csv
What does low_memory and memory_map flags do in pd.read_csv," the function signature for pandas.read_csv gives, among others, the following options: I couldn't find any documentation for either low_memoryor memory_map flags. I am confused about whether these features are implemented yet and if so how do they work. Specifically,memory_map: If implemented does it use np.memmap and if so does it store the individual columns as memmap or the rows. low_memory: Does it specify something like cache to store in memory?can we convert an existing DataFrame to a memmapped DataFrameP.S. : versions of relevant modules  <code>  read_csv(filepath_or_buffer, low_memory=True, memory_map=False, iterator=False, chunksize=None, ...) pandas==0.14.0scipy==0.14.0numpy==1.8.1",What do low_memory and memory_map flags do in pd.read_csv
How do i find the percentage of the most common element in a list? [PYTHON]," I have been recently using Counter().most_common but the problem is that I need to turn the bit where it shows how much it came up into a percentage, for example: to: Is there any way of doing this using Counter().most_common, or any other method?Here is part of my code: <code>  [(2, 5), (10, 5)] [(2, 50%), (10, 50%)] while count < int(DR): count = count + int(1) DV.append(random.randint(1, int(DI))) if count == int(DR): print ('\n(The Number that was rolled , the amount of times it came up):') global x print (Counter(DV).most_common(int((DI))))",How do i find the percentage of the most common element in a list?
How do i find the percentage of the most common element in a list?," I have been recently using Counter().most_common but the problem is that I need to turn the bit where it shows how much it came up into a percentage, for example: to: Is there any way of doing this using Counter().most_common, or any other method?Here is part of my code: <code>  [(2, 5), (10, 5)] [(2, 50%), (10, 50%)] while count < int(DR): count = count + int(1) DV.append(random.randint(1, int(DI))) if count == int(DR): print ('\n(The Number that was rolled , the amount of times it came up):') global x print (Counter(DV).most_common(int((DI))))",How do i find the percentage of the most common element in a list?
How do i find the percentage of the most common element in a list? [SOLVED]," I have been recently using Counter().most_common but the problem is that I need to turn the bit where it shows how much it came up into a percentage, for example: to: Is there any way of doing this using Counter().most_common, or any other method?Here is part of my code: <code>  [(2, 5), (10, 5)] [(2, 50%), (10, 50%)] while count < int(DR): count = count + int(1) DV.append(random.randint(1, int(DI))) if count == int(DR): print ('\n(The Number that was rolled , the amount of times it came up):') global x print (Counter(DV).most_common(int((DI))))",How do i find the percentage of the most common element in a list?
How to put complex in to numpy's array?," According to my example code. i tried to put complex number into the numpy's array but it didn't work. May be i miss some basic thing. <code>  >>> import numpy as np>>> A = np.zeros((3,3))>>> A[0,0] = 9>>> Aarray([[ 9., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]])>>> A[0,1] = 1+2jTraceback (most recent call last): File ""<stdin>"", line 1, in <module>TypeError: can't convert complex to float>>> A[0,1] = np.complex(1, 2)Traceback (most recent call last): File ""<stdin>"", line 1, in <module>TypeError: can't convert complex to float",How to put complex into a numpy's array?
Python - How to unit test a function that uses Popen?," I am writing a program which contains a lot of file operation. Some operations are done by calling subprocess.Popen, eg, split -l 50000 ${filename}, gzip -d -f ${filename} ${filename}..Now I want to unit test the functionality of the program. But how could I unit test these functions?Any suggestions? <code> ",How to unit test a function that uses Popen?
How do I convert a list of lists to a panda dataframe?," How do I convert a list of lists to a panda dataframe?it is not in the form of coloumns but instead in the form of rows. for example: I want it to be shown as rows and not coloumns. currently it shows somethign like this I want the rows and coloumns to be switched. Moreover, How do I make it for all 5 main lists?This is how I want the output to look like with other coloumns also filled in. However. df.transpose() won't help. <code>  #!/usr/bin/env pythonfrom random import randrangeimport pandasdata = [[[randrange(0,100) for j in range(0, 12)] for y in range(0, 12)] for x in range(0, 5)]print datadf = pandas.DataFrame(data[0], columns=['B','P','F','I','FP','BP','2','M','3','1','I','L'])print df data[0][0] == [64, 73, 76, 64, 61, 32, 36, 94, 81, 49, 94, 48] B P F I FP BP 2 M 3 1 I L0 64 73 76 64 61 32 36 94 81 49 94 481 57 58 69 46 34 66 15 24 20 49 25 982 99 61 73 69 21 33 78 31 16 11 77 713 41 1 55 34 97 64 98 9 42 77 95 414 36 50 54 27 74 0 8 59 27 54 6 905 74 72 75 30 62 42 90 26 13 49 74 96 41 92 11 38 24 48 34 74 50 10 42 97 77 9 77 63 23 5 50 66 49 5 66 988 90 66 97 16 39 55 38 4 33 52 64 59 18 14 62 87 54 38 29 10 66 18 15 8610 60 89 57 28 18 68 11 29 94 34 37 5911 78 67 93 18 14 28 64 11 77 79 94 66 B P F I FP BP 2 M 3 1 I L0 64 1 73 1 76 2 64 3 61 4 32 5 36 6 94 7 81 8 49 9 94 10 48",Transpose pandas dataframe
Rpy2 error wac-a-mole," I'm running Python (x,y) 2.7 on windows 7 32 bit and R version 3.1.0. I've been trying to install Rpy2 and have been getting many errors. I finally found this site which has pre-compiled python modules for windows http://www.lfd.uci.edu/~gohlke/pythonlibs/, so I downloaded rpy22.4.2.win32py2.7.exe. When I did this and tried I had an error saying it could not find R_HOME, so I updated my path variables. This was fixed, but then I got an error saying it could not find R_USER. Once again, I updated my PYTHONPATH variables based on SO responses. This didn't work, and so I'm stuck. I've updated my PYTHONPATH both inside Spyder and also in my system variables, but still no luck. Does anyone know what could be going on? This is the error I get: This is what my PYTHONPATH includes: This is what my PATH includes: Thanks for any help you can provide! <code>  import rpy2.robjects as robjects Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""C:\Python27\lib\site-packages\rpy2\robjects\__init__.py"", line 18, in <module> from rpy2.robjects.robject import RObjectMixin, RObject File ""C:\Python27\lib\site-packages\rpy2\robjects\robject.py"", line 5, in <module> rpy2.rinterface.initr()RuntimeError: R_USER not defined. C:\Python27\Lib\site-packages\rpy2;C:\Program Files\R\R-3.1.0\bin\i386;C:\Python27\Lib\site-packages\rpy2\robjects C:\Python27\Lib\site-packages\PyQt4;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;%SYSTEMROOT%\System32\WindowsPowerShell\v1.0\;c:\Program Files\Intel\DMIX;C:\Program Files\Intel\Services\IPT\;C:\Python27;C:\Python27\DLLs;C:\Python27\Scripts;C:\Python27\Lib\site-packages\vtk;C:\Python27\gnuplot\binary;C:\Program Files\pythonxy\SciTE-3.1.0;C:\Program Files\pythonxy\console;C:\MinGW32-xy\bin;C:\Program Files\R\R-3.1.0\bin;C:\MinGW32-xy\mingw32\bin;C:\MinGW32-xy\bin",Rpy2 error wac-a-mole: R_USER not defined
"random() * random() different as pow(random(), 2)?"," Is there are difference between random() * random() and random() ** 2? random() returns a value between 0 and 1 from a uniform distribution.When testing both versions of random square numbers I noticed a little difference. I created 100000 random square numbers and counted how many numbers are in each interval of 0.01 (0.00 to 0.01, 0.01 to 0.02, ...). It seems that these versions of squared random number generation are different.Squaring a random number instead of multiplying two random numbers has you reuse a random number, but I think the distribution should remain the same. Is there really a difference? If not, why is my test showing a difference?I generate two random binned distributions for random() * random() and one for random() ** 2 like so: which gives The expected random error is the difference in the first two: But the difference between the first and third is much larger, hinting that the distributions are different: <code>  from random import randomlst = [0 for i in range(100)]lst2, lst3 = list(lst), list(lst)#create two random distributions for random() * random()for i in range(100000): lst[int(100 * random() * random())] += 1for i in range(100000): lst2[int(100 * random() * random())] += 1for i in range(100000): lst3[int(100 * random() ** 2)] += 1 >>> lst[ 5626, 4139, 3705, 3348, 3085, 2933, 2725, 2539, 2449, 2413, 2259, 2179, 2116, 2062, 1961, 1827, 1754, 1743, 1719, 1753, 1522, 1543, 1513, 1361, 1372, 1290, 1336, 1274, 1219, 1178, 1139, 1147, 1109, 1163, 1060, 1022, 1007, 952, 984, 957, 906, 900, 843, 883, 802, 801, 710, 752, 705, 729, 654, 668, 628, 633, 615, 600, 566, 551, 532, 541, 511, 493, 465, 503, 450, 394, 405, 405, 404, 332, 369, 369, 332, 316, 272, 284, 315, 257, 224, 230, 221, 175, 209, 188, 162, 156, 159, 114, 131, 124, 96, 94, 80, 73, 54, 45, 43, 23, 18, 3]>>> lst2[ 5548, 4218, 3604, 3237, 3082, 2921, 2872, 2570, 2479, 2392, 2296, 2205, 2113, 1990, 1901, 1814, 1801, 1714, 1660, 1591, 1631, 1523, 1491, 1505, 1385, 1329, 1275, 1308, 1324, 1207, 1209, 1208, 1117, 1136, 1015, 1080, 1001, 993, 958, 948, 903, 843, 843, 849, 801, 799, 748, 729, 705, 660, 701, 689, 676, 656, 632, 581, 564, 537, 517, 525, 483, 478, 473, 494, 457, 422, 412, 390, 384, 352, 350, 323, 322, 308, 304, 275, 272, 256, 246, 265, 227, 204, 171, 191, 191, 136, 145, 136, 108, 117, 93, 83, 74, 77, 55, 38, 32, 25, 21, 1]>>> lst3[ 10047, 4198, 3214, 2696, 2369, 2117, 2010, 1869, 1752, 1653, 1552, 1416, 1405, 1377, 1328, 1293, 1252, 1245, 1121, 1146, 1047, 1051, 1123, 1100, 951, 948, 967, 933, 939, 925, 940, 893, 929, 874, 824, 843, 868, 800, 844, 822, 746, 733, 808, 734, 740, 682, 713, 681, 675, 686, 689, 730, 707, 677, 645, 661, 645, 651, 649, 672, 679, 593, 585, 622, 611, 636, 543, 571, 594, 593, 629, 624, 593, 567, 584, 585, 610, 549, 553, 574, 547, 583, 582, 553, 536, 512, 498, 562, 536, 523, 553, 485, 503, 502, 518, 554, 485, 482, 470, 516] [ 78, 79, 101, 111, 3, 12, 147, 31, 30, 21, 37, 26, 3, 72, 60, 13, 47, 29, 59, 162, 109, 20, 22, 144, 13, 39, 61, 34, 105, 29, 70, 61, 8, 27, 45, 58, 6, 41, 26, 9, 3, 57, 0, 34, 1, 2, 38, 23, 0, 69, 47, 21, 48, 23, 17, 19, 2, 14, 15, 16, 28, 15, 8, 9, 7, 28, 7, 15, 20, 20, 19, 46, 10, 8, 32, 9, 43, 1, 22, 35, 6, 29, 38, 3, 29, 20, 14, 22, 23, 7, 3, 11, 6, 4, 1, 7, 11, 2, 3, 2] [ 4421, 59, 491, 652, 716, 816, 715, 670, 697, 760, 707, 763, 711, 685, 633, 534, 502, 498, 598, 607, 475, 492, 390, 261, 421, 342, 369, 341, 280, 253, 199, 254, 180, 289, 236, 179, 139, 152, 140, 135, 160, 167, 35, 149, 62, 119, 3, 71, 30, 43, 35, 62, 79, 44, 30, 61, 79, 100, 117, 131, 168, 100, 120, 119, 161, 242, 138, 166, 190, 261, 260, 255, 261, 251, 312, 301, 295, 292, 329, 344, 326, 408, 373, 365, 374, 356, 339, 448, 405, 399, 457, 391, 423, 429, 464, 509, 442, 459, 452, 513]",Why is random() * random() different to random() ** 2?
Why this code work fine in python 2 and not in python 3?," Why does the following code work fine in Python 2.x and not in Python 3.3+: Python 2.7.6 output: Python 3.1.5 output: Python 3.2.3 and 3.2.5 output: Python 3.3.5 and 3.4.1 output: <code>  class TestA(object): def __new__(cls, e): return super(TestA, cls).__new__(TestB, e)class TestB(TestA): def __init__(self, e): print(self, e)TestA(1) (<__main__.TestB object at 0x7f6303378ad0>, 1) __main__:3: DeprecationWarning: object.__new__() takes no parameters<__main__.TestB object at 0x7f2f69db8f10> 1 <__main__.TestB object at 0xcda690> 1 Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 3, in __new__TypeError: object() takes no parameters",Why does object.__new__ with arguments work fine in Python 2.x and not in Python 3.3+?
Why does this code work fine in Python 2.x and not in Python 3.3+?," Why does the following code work fine in Python 2.x and not in Python 3.3+: Python 2.7.6 output: Python 3.1.5 output: Python 3.2.3 and 3.2.5 output: Python 3.3.5 and 3.4.1 output: <code>  class TestA(object): def __new__(cls, e): return super(TestA, cls).__new__(TestB, e)class TestB(TestA): def __init__(self, e): print(self, e)TestA(1) (<__main__.TestB object at 0x7f6303378ad0>, 1) __main__:3: DeprecationWarning: object.__new__() takes no parameters<__main__.TestB object at 0x7f2f69db8f10> 1 <__main__.TestB object at 0xcda690> 1 Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 3, in __new__TypeError: object() takes no parameters",Why does object.__new__ with arguments work fine in Python 2.x and not in Python 3.3+?
PyCharm with Git. Shall I ignore the .idea folder?," I read about Git integration in PyCharm, and created a Git repository from PyCharm. I did this in PyCharm because I was hoping PyCharm would know whether the .idea folder should be ignored, and if that's the case, it would automatically create a .gitignore file with the line .idea/ in it. But it didn't, so I assumed that I shouldn't ignore the .idea foler. However, I did a quick search and found someone's example .gitignore file, here, which clearly ignores the .idea folder.So, my question is, should the .idea folder be ignored or not? <code> ",Should I ignore the .idea folder when using PyCharm with Git?
Django & Compression: Unsure in which part to do data compression," I made my Django models and after inserting a test/dummy record to my PostgreSQL database I've realized that my data is quite large for each record. The sum of the data in all fields will be around 700 KB per record. I am estimating I will have around five million records so this will get really large around the 3350 GB mark. Most of my data is big JSON dumps (around 70+ KB for each field). I am unsure if PostgreSQL will auto compress my data when handling through Django framework. I was wondering whether I should compress my data before entering it into the database. Questions:Does PostgreSQL auto compress my string fields using some x compression algorithm when using Django model field type TextField?Should I not rely on PostgreSQL and just compress my data beforehand and then enter it into the DB? If so, which compression library should I use? I already tried zlib in Python and seems great, but, I've read that there is gzip library as well and I am confused which would be the most effective (in terms of compression and decompression speed as well as the percentage of compression).EDIT: I was reading up on this Django snippet for CompressedTextField which sparked my confusion regarding which compression library to use. I saw a few people use zlib while some used gzip.EDIT 2: This stackoverflow question says that PostgreSQL does compression of string data automatically.EDIT 3: PostgreSQL uses pg_lzcompress.c for compression which is a part of the LZ compression family. Is it safe to assume that we don't need to use some other form of compression (zlib or gzip) on the TextField itself since it will be of datatype text (variable length string) in the DB itself? <code> ",Django: TextField (string) data compression on database level or code level
Where is the C implementation of Python's ability to multiply a string by an integer?, Python allows the multiplication of strings by integers: How is this implemented in CPython? I would particularly appreciate a pointer to the source code; the Mercurial repository is a labyrinth beyond my abilities to navigate. <code>  >>> 'hello' * 5'hellohellohellohellohello',How is string multiplication implemented in CPython?
Python cant find module in the same folder," My python somehow can't find any modules in the same directory.What am I doing wrong? (python2.7)So I have one directory '2014_07_13_test', with two files in it: test.pyhello.pywhere hello.py: and test.py: Still python gives me What's wrong? <code>  # !/usr/local/bin/python# -*- coding: utf-8 -*-def hello1(): print 'HelloWorld!' # !/usr/local/bin/python# -*- coding: utf-8 -*-from hello import hello1hello1() >>> Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<string>"", line 4, in <module>ImportError: No module named hello",Python can't find module in the same folder
Alternative method to create a function which checks whether string is pangram," I want to create a function which takes a string as input and check whether the string is pangram or not (pangram is a piece of text which contains every letter of the alphabet).I wrote the following code, which works, but I am looking for an alternative way to do it, hopefully a shorted way. I feel like this question might be against the rules of this website but hopefully it isn't. I am just curious and would like to see alternative ways to do this. <code>  import stringdef is_pangram (gram): gram = gram.lower() gram_list_old = sorted([c for c in gram if c != ' ']) gram_list = [] for c in gram_list_old: if c not in gram_list: gram_list.append(c) if gram_list == list(string.ascii_lowercase): return True else: return False",How to check if string is a pangram?
using TOR with python stem (basic)," I'm trying to get an .onion website's content to python, a little research showed that 'stem' and as i am running this tutorial script, or more specifically, when i'm trying to use stem.process.launch_tor_with_config, i get this error: 'tor' isn't available on your system. Maybe it's not in your PATH?I supposed to have some sort of tor process installed, I got the tor browser bundle and put the /Tor library (with the tor.exe) in it in my path, and it's not helping...Obviously I'm missing something VERY BASIC, please advise....Many thanks... <code> ",TOR with python stem (basic) - 'tor' not in PATH
Error while using histogram/Bins," Well I think matplotlib got downloaded but with my new script I get this error: And my code is: #!/usr/bin/python But the code thats getting me the error is the new addition that I added which is the last part, reproduced below: <code>  /usr/lib64/python2.6/site-packages/matplotlib/backends/backend_gtk.py:621: DeprecationWarning: Use the new widget gtk.Tooltip self.tooltips = gtk.Tooltips()Traceback (most recent call last): File ""vector_final"", line 42, in <module>plt.hist(data, num_bins) File ""/usr/lib64/python2.6/site-packages/matplotlib/pyplot.py"", line 2008, in histret = ax.hist(x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, **kwargs) File ""/usr/lib64/python2.6/site-packages/matplotlib/axes.py"", line 7098, in histw = [None]*len(x)TypeError: len() of unsized object l=[]with open(""testdata"") as f: line = f.next() f.next()# skip headers nat = int(line.split()[0]) print nat for line in f: if line.strip(): if line.strip(): l.append(map(float,line.split()[1:])) b = 0 a = 1for b in range(53): for a in range(b+1,54): import operator import matplotlib.pyplot as plt import numpy as np vector1 = (l[b][0],l[b][1],l[b][2]) vector2 = (l[a][0],l[a][1],l[a][2]) x = vector1 y = vector2 vector3 = list(np.array(x) - np.array(y)) dotProduct = reduce( operator.add, map( operator.mul, vector3, vector3)) dp = dotProduct**.5 print dp data = dp num_bins = 200 # <- number of bins for the histogram plt.hist(data, num_bins) plt.show() data = dp num_bins = 200 # <- number of bins for the histogram plt.hist(data, num_bins) plt.show()",How to make a histogram from a list of data
Get index of the python iterator object," How to obtain the index of the current item of a Python iterator in a loop? For example when using regular expression finditer function which returns an iterator, how you can access the index of the iterator in a loop. <code>  for item in re.finditer(pattern, text): # How to obtain the index of the ""item""",How to get the index of the current iterator item in a loop?
How to get the index of the iterator object?," How to obtain the index of the current item of a Python iterator in a loop? For example when using regular expression finditer function which returns an iterator, how you can access the index of the iterator in a loop. <code>  for item in re.finditer(pattern, text): # How to obtain the index of the ""item""",How to get the index of the current iterator item in a loop?
How to get the index of the the current iterator item in a loop?," How to obtain the index of the current item of a Python iterator in a loop? For example when using regular expression finditer function which returns an iterator, how you can access the index of the iterator in a loop. <code>  for item in re.finditer(pattern, text): # How to obtain the index of the ""item""",How to get the index of the current iterator item in a loop?
matplotlib: Change grid interval and specify tick labels," I am trying to plot counts in gridded plots, but I haven't been able to figure out how to go about it.I want:to have dotted grids at an interval of 5;to have major tick labels only every 20;for the ticks to be outside the plot; andto have ""counts"" inside those grids.I have checked for potential duplicates, such as here and here, but have not been able to figure it out.This is my code: This is what I get.I also have a problem with the data points being overwritten.Could anybody PLEASE help me with this problem? <code>  import matplotlib.pyplot as pltfrom matplotlib.ticker import MultipleLocator, FormatStrFormatterfor key, value in sorted(data.items()): x = value[0][2] y = value[0][3] count = value[0][4] fig = plt.figure() ax = fig.add_subplot(111) ax.annotate(count, xy = (x, y), size = 5) # overwrites and I only get the last data point plt.close() # Without this, I get a ""fail to allocate bitmap"" error.plt.suptitle('Number of counts', fontsize = 12)ax.set_xlabel('x')ax.set_ylabel('y')plt.axes().set_aspect('equal')plt.axis([0, 1000, 0, 1000])# This gives an interval of 200.majorLocator = MultipleLocator(20)majorFormatter = FormatStrFormatter('%d')minorLocator = MultipleLocator(5)# I want the minor grid to be 5 and the major grid to be 20.plt.grid()filename = 'C:\Users\Owl\Desktop\Plot.png'plt.savefig(filename, dpi = 150)plt.close()",Change grid interval and specify tick labels in Matplotlib
How to select tags by attribute value with beautiful soup," I have the following HTML fragment: I am trying to select header column only if attribute data-name=""result-name""I've tried: This gives: How can I get this working? <code>  >>> a<div class=""headercolumn""><h2><a class=""results"" data-name=""result-name"" href=""/xxy> my text</a></h2> >>> a.select('a[""data-name=""result-name""""]') ValueError: Unsupported or invalid CSS selector: ",How to select tags by attribute value with Beautiful Soup
Pythonic way of creating a defaultdict with empty numpy array," I'm wondering if there's a more clever way to create a default dict from collections.The dict should have an empty numpy ndarray as default value.My best result is so far: However, i'm wondering if there's a possibility to skip the lambda term and create the dict in a more direct way. Like:  <code>  import collectionsd = collections.defaultdict(lambda: numpy.ndarray(0)) d = collections.defaultdict(numpy.ndarray(0)) # <- Nice and short - but not callable",Creating a defaultdict with empty numpy array
PyDev: Is it possible to invoke debugging specific command from console (with breakpoints)?," Suppose I wrote a function, which I want to debug. PyDev helps debugging a lot with its advanced features, like breakpoints.After setting the breakpoints, one way of debugging is to write an invocation of the function in def main() or just in the body of the module, and to press the button. Is it possible to launch a debug session of the function together with its arguments from the console? (Just like in RStudio or VBA...)This question is a duplicate of pydev: debug in console mode (interactive)? Update:I really don't know, what is the magic combination of clicks, that make the interactive debugging possible.Here is what I do, which is not working (on Ubuntu 14.04 and Eclipse 4.4.I20140606-1215 with PyDev 3.6.0.2014062323, Python 3.4.0, IPython 1.2.1)First try:First, I create a new PyDev Project: Then I insert a new .py file with some code on, and create some breakpoints: Then I right click somewhere on the code, Debug As...->Python run.And then 2 consoles open, none of them is IPython. The active console is named [Debug console] proba.py. Typing commands into this console doesn't make the computer to execute them.There is other console available, named just proba.py. This console is fully interactive, although it is not IPython. This is indeed an interactive debugger. One can step through the code and inspect variables. Unfortunately updating the variables is not supported; if I enter a command a=10 the variable doesn't get updated. Second tryThis time, before launching the debug session let's try starting IPython. After Ctrl+Alt+Enter I choose Console for currently active editor:Then Python3 (because this is what I need):After that I have a fully working IPython console. When I execute the file via execfile, it triggers an error Failed to create input stream: Read timed out: Although it seems, that stepping through the code works, I cannot access the variables from the IPython console, although it is possible to access them from Variables view: <code> ",PyDev: How to invoke debugging specific command from console (with breakpoints)?
"Opening an excel file manually allows formulas to run, opening an excel file with VBScript doesn't"," I'm having a problem with a script not updating an excel file, and I reduced it to the following problem:If I open an excel file, I can go to the Formulas tab and click ""Calculate Now"" and it'll spend a bit of time updating all the calculations. If I run a VBScript just to open the file (see following code), if I go to the Formulas tab and click ""Calculate Now"" it'll just refresh immediately and nothing will change. I've tried all sorts of stuff like: But those seem to do the same thing as going to the tab and clicking ""Calculate"" resulting in just a quick refresh and the excel page not trying to update at all.The same is true when using python's win32com module. I can't run calculations in the opened file. The same is also true using PowerShell. So why does opening a file with these languages somehow shut off the ability to calculate the formulas? <code>  Dim objXLApp, objXLWb, objXLWsSet objXLApp = CreateObject(""Excel.Application"")objXLApp.Visible = TrueSet objXLWb = objXLApp.Workbooks.Open(file_path.xls) objXLApp.Calculation = xlAutomaticobjXLApp.CalculateobjXLApp.CalculateFullobjXLApp.CalculateFullRebuildobjXLWb.RefreshAllobjXLWs.EnableCalculation = TrueobjXLWs.Calculate import win32com.client as win32excel = win32.Dispatch('Excel.Application')excel.Visible = Trueexcel_workbook = excel.Workbooks.Open(file_path.xls) $excel = New-Object -com excel.application$excel.Visible = $True$workbook = $excel.Workbooks.Open( $file_path )","Opening an excel file manually allows formulas to run, opening an excel file with VBScript or PowerShell or Python's win32com doesn't"
"Opening an excel file manually allows formulas to run, opening an excel file with VBScript or Python's win32com doesn't"," I'm having a problem with a script not updating an excel file, and I reduced it to the following problem:If I open an excel file, I can go to the Formulas tab and click ""Calculate Now"" and it'll spend a bit of time updating all the calculations. If I run a VBScript just to open the file (see following code), if I go to the Formulas tab and click ""Calculate Now"" it'll just refresh immediately and nothing will change. I've tried all sorts of stuff like: But those seem to do the same thing as going to the tab and clicking ""Calculate"" resulting in just a quick refresh and the excel page not trying to update at all.The same is true when using python's win32com module. I can't run calculations in the opened file. The same is also true using PowerShell. So why does opening a file with these languages somehow shut off the ability to calculate the formulas? <code>  Dim objXLApp, objXLWb, objXLWsSet objXLApp = CreateObject(""Excel.Application"")objXLApp.Visible = TrueSet objXLWb = objXLApp.Workbooks.Open(file_path.xls) objXLApp.Calculation = xlAutomaticobjXLApp.CalculateobjXLApp.CalculateFullobjXLApp.CalculateFullRebuildobjXLWb.RefreshAllobjXLWs.EnableCalculation = TrueobjXLWs.Calculate import win32com.client as win32excel = win32.Dispatch('Excel.Application')excel.Visible = Trueexcel_workbook = excel.Workbooks.Open(file_path.xls) $excel = New-Object -com excel.application$excel.Visible = $True$workbook = $excel.Workbooks.Open( $file_path )","Opening an excel file manually allows formulas to run, opening an excel file with VBScript or PowerShell or Python's win32com doesn't"
Save turtle output as jpeg [python]," I have a fractal image creator. It creates a random fractal tree like thing. When done, it prompts the user to save the tree. I have it saving as a .svg right now and that works BUT I want it to save to a more convenient file type, like jpeg. Any ideas?Code: <code>  import turtleimport randomfrom sys import exitfrom time import clockimport canvasvgturtle.colormode(255)red = 125green = 70blue = 38 pen = 10def saveImg(): print(""Done."") save = input(""Would you like to save this tree? Y/N \n"") if save.upper() == ""Y"": t.hideturtle() name = input(""What would you like to name it? \n"") nameSav = name + "".svg"" ts = turtle.getscreen().getcanvas() canvasvg.saveall(nameSav, ts) elif save.upper() == ""N"": def runChk(): runAgain = input(""Would you like to run again? Y/N (N will exit)"") if runAgain.upper() == ""Y"": print(""Running"") main() elif runAgain.upper() == ""N"": print(""Exiting..."") exit() else: print(""Invalid response."") runChk() runChk() else: print(""Invalid response."") saveImg()def tree(branchLen, t, red, green, blue, pen): time = str(round(clock())) print(""Drawing... "" + time) if branchLen > 3: pen = pen*0.8 t.pensize(pen) if (red > 10 and green < 140): red = red - 15 green = green + 8 if branchLen > 5: angle = random.randrange(18, 55) angleTwo = 0.5*angle sub = random.randrange(1,16) t.color(red, green, blue) t.forward(branchLen) t.right(angleTwo) tree(branchLen-sub,t, red, green, blue, pen) t.left(angle) tree(branchLen-sub, t, red, green, blue, pen) t.right(angleTwo) t.backward(branchLen)def main(): t = turtle.Turtle() myWin = turtle.Screen() t.speed(0) t.hideturtle() t.left(90) t.up() t.backward(100) t.down() print(""Please wait while I draw..."") tree(random.randrange(60,95),t,red,green,blue, pen) saveImg()main()",Save turtle output as jpeg
Why isn't fromfile-prefix-chars working from the command line," I'm trying to use the fromfile-prefix-chars feature of argparse in Python to load all my command line arguments from a file, but it keeps complaining that I haven't specified some argument.The code: The argument file: The command line and output: You can see that I'm providing the required argument in the file, but argparse isn't seeing it. <code>  import argparsedef go(): parser = argparse.ArgumentParser(fromfile_prefix_chars='@') parser.add_argument(""--option1"") parser.add_argument(""--option2"", type=int, required=True) args = parser.parse_args()if __name__ == ""__main__"": go() --option1 foo--option2 1234 $ python testargparse.py @testargsusage: testargparse.py [-h] [--option1 OPTION1] --option2 OPTION2testargparse.py: error: argument --option2 is required",Why isn't fromfile-prefix-chars in Python argparse working?
Python: get a permutation of a list as a function of a unique given index in O(n)," I would like to have a function get_permutation that, given a list l and an index i, returns a permutation of l such that the permutations are unique for all i bigger than 0 and lower than n! (where n = len(l)).I.e. get_permutation(l,i) != get_permutation(l,j) if i!=j for all i, j s.t. 0 <= i and j < len(l)!).Moreover, this function has to run in O(n). For example, this function would comply the with the requirements, if it weren't for the exponential order: Does anyone has a solution for the above described problem?EDIT: I want the permutation from the index NOT the index from the permutation <code>  def get_permutation(l, i): return list(itertools.permutations(l))[i]",Get a permutation as a function of a unique given index in O(n)
Python: get a permutation as a function of a unique given index in O(n)," I would like to have a function get_permutation that, given a list l and an index i, returns a permutation of l such that the permutations are unique for all i bigger than 0 and lower than n! (where n = len(l)).I.e. get_permutation(l,i) != get_permutation(l,j) if i!=j for all i, j s.t. 0 <= i and j < len(l)!).Moreover, this function has to run in O(n). For example, this function would comply the with the requirements, if it weren't for the exponential order: Does anyone has a solution for the above described problem?EDIT: I want the permutation from the index NOT the index from the permutation <code>  def get_permutation(l, i): return list(itertools.permutations(l))[i]",Get a permutation as a function of a unique given index in O(n)
PANDAS PYTHON: Move column by name to front of table," Here is my df: How can I move a column by name (""Mid"") to the front of the table, index 0. This is what the result should look like: My current code moves the column by index using df.columns.tolist() but I'd like to shift it by name.  <code>  Net Upper Lower Mid ZsoreAnswer option More than once a day 0% 0.22% -0.12% 2 65 Once a day 0% 0.32% -0.19% 3 45Several times a week 2% 2.45% 1.10% 4 78Once a week 1% 1.63% -0.40% 6 65 Mid Upper Lower Net ZsoreAnswer option More than once a day 2 0.22% -0.12% 0% 65 Once a day 3 0.32% -0.19% 0% 45Several times a week 4 2.45% 1.10% 2% 78Once a week 6 1.63% -0.40% 1% 65",Move column by name to front of table in pandas
"Python Loop index of key, value for loop when using items()"," Im looping though a dictionary using And I wondered if theres some pythonic way to also access the loop index / iteration number. Access the index while still maintaining access to the key value information. its is because I need to detect the first time the loop runs. So inside I can have something like <code>  for key, value in mydict.items(): for key, value, index in mydict.items(): if index != 1:","Python loop index of key, value for-loop when using items()"
What happens when a function returns its name itself in python?," What is the meaning of the last line in the code?EDIT: <code>  def traceit(frame, event, trace_arg): global stepping if event == 'line': if stepping or frame.f_lineno in breakpoints: resume = False while not resume: print(event, frame.f_lineno, frame.f_code.co_name, frame.f_locals) command = input_command() resume = debug(command, frame.f_locals) return traceit def remove_html_markup(s): tag = False quote = False out = """" for c in s: if c == '<' and not quote: tag = True elif c == '>' and not quote: tag = False elif c == '""' or c == ""'"" and tag: quote = not quote elif not tag: out = out + c return outdef main(): print (remove_html_markup('xyz')) print (remove_html_markup('""<b>foo</b>""')) print (remove_html_markup(""'<b>foo</b>'""))# globalsbreakpoints = {9: True}stepping = Falsedef debug(command, my_locals): global stepping global breakpoints if command.find(' ') > 0: arg = command.split(' ')[1] else: arg = None if command.startswith('s'): # step stepping = True return True elif command.startswith('c'): # continue stepping = False return True elif command.startswith('q'): # quit sys.exit(0) else: print (""No such command"", repr(command)) return Falsecommands = ['s', 's', 's', 'q']def input_command(): #command = raw_input(""(my-spyder) "") global commands command = commands.pop(0) return commanddef traceit(frame, event, trace_arg): global stepping if event == 'line': if stepping or frame.f_lineno in breakpoints: resume = False while not resume: print(event, frame.f_lineno, frame.f_code.co_name, frame.f_locals) command = input_command() resume = debug(command, frame.f_locals) return traceit# Using the tracersys.settrace(traceit)main()sys.settrace(None)",What happens when a function returns its own name in python?
python confusing function referece," Can anyone explain to me why the two functions below a and b are behaving differently. Function a changes names locally and b changes the actual object.Where can I find the correct documentation for this behavior? Output: <code>  def a(names): names = ['Fred', 'George', 'Bill']def b(names): names.append('Bill')first_names = ['Fred', 'George']print ""before calling any function"",first_namesa(first_names)print ""after calling a"",first_namesb(first_names)print ""after calling b"",first_names before calling any function ['Fred', 'George']after calling a ['Fred', 'George']after calling b ['Fred', 'George', 'Bill']",Python confusing function reference
How to check if a variable is a dictionary in python," How would you check if a variable is a dictionary in Python?For example, I'd like it to loop through the values in the dictionary until it finds a dictionary. Then, loop through the one it finds: <code>  dict = {'abc': 'abc', 'def': {'ghi': 'ghi', 'jkl': 'jkl'}}for k, v in dict.iteritems(): if ###check if v is a dictionary: for k, v in v.iteritems(): print(k, ' ', v) else: print(k, ' ', v)",How to check if a variable is a dictionary in Python?
Pandas - Get first row value of a given column," This seems like a ridiculously easy question... but I'm not seeing the easy answer I was expecting.So, how do I get the value at an nth row of a given column in Pandas? (I am particularly interested in the first row, but would be interested in a more general practice as well).For example, let's say I want to pull the 1.2 value in Btime as a variable.Whats the right way to do this? <code>  >>> df_test ATime X Y Z Btime C D E0 1.2 2 15 2 1.2 12 25 121 1.4 3 12 1 1.3 13 22 112 1.5 1 10 6 1.4 11 20 163 1.6 2 9 10 1.7 12 29 124 1.9 1 1 9 1.9 11 21 195 2.0 0 0 0 2.0 8 10 116 2.4 0 0 0 2.4 10 12 15",Get first row value of a given column
scikit-learn GridDearchCV python stops working when n_jobs>1," I have previously asked here come up with following lines of code: But when I've run this there has appeared another problem that was not related to the previously asked question. Python ends up with following OS error message: When I have replaced the second line in my code by: Then everything works fine except I don't use multiple threads.My operating system is OSX 10.9.4My python version is 2.7.8 |Anaconda 2.0.1 (x86_64)| (default, Jul 2 2014, 15:36:00) [GCC 4.2.1 (Apple Inc. build 5577)]My scikit-lern version is 0.14.1My numpy version is 1.8.1And my scipy version is 0.14.0My question is if anybody has an idea how to make GridSearchCV run on more than one thread? EDIT:I have realized that actually this error happens only for some of my input data sets. Unfortunately the problematic datasets (its X) are too big so it is not possible to copy them in here. Input features data is basically tf-idf vectors and y vectors are floats > 0, particularly: The version with 1 job works for all of my input data sets, even for this one. <code>  parameters = [{'weights': ['uniform'], 'n_neighbors': [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]}]clf = GridSearchCV(neighbors.KNeighborsRegressor(), parameters, n_jobs=4)clf.fit(features, rewards) Process: Python [1327]Path: /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/PythonIdentifier: PythonVersion: 2.7.2.5 (2.7.2.5.r64662-trunk)Code Type: X86-64 (Native)Parent Process: Python [1316]Responsible: Sublime Text 2 [308]User ID: 501Date/Time: 2014-08-12 10:27:24.640 +0200OS Version: Mac OS X 10.9.4 (13E28)Report Version: 11Anonymous UUID: D10CD8B7-221F-B121-98D4-4574A1F2189FSleep/Wake UUID: 0B9C4AE0-26E6-4DE8-B751-665791968115Crashed Thread: 0 Dispatch queue: com.apple.main-threadException Type: EXC_BAD_ACCESS (SIGSEGV)Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000110VM Regions Near 0x110:--> __TEXT 0000000100000000-0000000100001000 [ 4K] r-x/rwx SM=COW /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/PythonApplication Specific Information:*** multi-threaded process forked ***crashed on child side of fork pre-execThread 0 Crashed:: Dispatch queue: com.apple.main-thread0 libdispatch.dylib 0x00007fff91534c90 dispatch_group_async_f + 1411 libBLAS.dylib 0x00007fff9413f791 APL_sgemm + 10612 libBLAS.dylib 0x00007fff9413cb3f cblas_sgemm + 12673 _dotblas.so 0x0000000102b0236e dotblas_matrixproduct + 59344 org.activestate.ActivePython27 0x00000001000c552d PyEval_EvalFrameEx + 239495 org.activestate.ActivePython27 0x00000001000c7ad6 PyEval_EvalCodeEx + 21186 org.activestate.ActivePython27 0x00000001000c5d10 PyEval_EvalFrameEx + 259687 org.activestate.ActivePython27 0x00000001000c7ad6 PyEval_EvalCodeEx + 21188 org.activestate.ActivePython27 0x000000010003d390 function_call + 1769 org.activestate.ActivePython27 0x000000010000be12 PyObject_Call + 9810 org.activestate.ActivePython27 0x00000001000c098a PyEval_EvalFrameEx + 458611 org.activestate.ActivePython27 0x00000001000c7ad6 PyEval_EvalCodeEx + 211812 org.activestate.ActivePython27 0x00000001000c5d10 PyEval_EvalFrameEx + 2596813 org.activestate.ActivePython27 0x00000001000c7ad6 PyEval_EvalCodeEx + 211814 org.activestate.ActivePython27 0x00000001000c5d10 PyEval_EvalFrameEx + 2596815 org.activestate.ActivePython27 0x00000001000c7137 PyEval_EvalFrameEx + 3112716 org.activestate.ActivePython27 0x00000001000c7137 PyEval_EvalFrameEx + 3112717 org.activestate.ActivePython27 0x00000001000c7ad6 PyEval_EvalCodeEx + 211818 org.activestate.ActivePython27 0x000000010003d390 function_call + 17619 org.activestate.ActivePython27 0x000000010000be12 PyObject_Call + 9820 org.activestate.ActivePython27 0x00000001000c098a PyEval_EvalFrameEx + 458621 org.activestate.ActivePython27 0x00000001000c7ad6 PyEval_EvalCodeEx + 211822 org.activestate.ActivePython27 0x000000010003d390 function_call + 17623 org.activestate.ActivePython27 0x000000010000be12 PyObject_Call + 9824 org.activestate.ActivePython27 0x000000010001d36d instancemethod_call + 36525 org.activestate.ActivePython27 0x000000010000be12 PyObject_Call + 9826 org.activestate.ActivePython27 0x0000000100077dfa slot_tp_call + 7427 org.activestate.ActivePython27 0x000000010000be12 PyObject_Call + 9828 org.activestate.ActivePython27 0x00000001000c098a PyEval_EvalFrameEx + 458629 org.activestate.ActivePython27 0x00000001000c7ad6 PyEval_EvalCodeEx + 211830 org.activestate.ActivePython27 0x000000010003d390 function_call + 17631 org.activestate.ActivePython27 0x000000010000be12 PyObject_Call + 9832 org.activestate.ActivePython27 0x00000001000c098a PyEval_EvalFrameEx + 458633 org.activestate.ActivePython27 0x00000001000c7137 PyEval_EvalFrameEx + 3112734 org.activestate.ActivePython27 0x00000001000c7137 PyEval_EvalFrameEx + 3112735 org.activestate.ActivePython27 0x00000001000c7ad6 PyEval_EvalCodeEx + 211836 org.activestate.ActivePython27 0x000000010003d390 function_call + 17637 org.activestate.ActivePython27 0x000000010000be12 PyObject_Call + 9838 org.activestate.ActivePython27 0x000000010001d36d instancemethod_call + 36539 org.activestate.ActivePython27 0x000000010000be12 PyObject_Call + 9840 org.activestate.ActivePython27 0x0000000100077a28 slot_tp_init + 8841 org.activestate.ActivePython27 0x0000000100074e25 type_call + 24542 org.activestate.ActivePython27 0x000000010000be12 PyObject_Call + 9843 org.activestate.ActivePython27 0x00000001000c267d PyEval_EvalFrameEx + 11997 44 org.activestate.ActivePython27 0x00000001000c7137 PyEval_EvalFrameEx + 3112745 org.activestate.ActivePython27 0x00000001000c7137 PyEval_EvalFrameEx + 3112746 org.activestate.ActivePython27 0x00000001000c7ad6 PyEval_EvalCodeEx + 211847 org.activestate.ActivePython27 0x000000010003d390 function_call + 17648 org.activestate.ActivePython27 0x000000010000be12 PyObject_Call + 9849 org.activestate.ActivePython27 0x000000010001d36d instancemethod_call + 36550 org.activestate.ActivePython27 0x000000010000be12 PyObject_Call + 9851 org.activestate.ActivePython27 0x0000000100077a28 slot_tp_init + 8852 org.activestate.ActivePython27 0x0000000100074e25 type_call + 24553 org.activestate.ActivePython27 0x000000010000be12 PyObject_Call + 9854 org.activestate.ActivePython27 0x00000001000c267d PyEval_EvalFrameEx + 1199755 org.activestate.ActivePython27 0x00000001000c7ad6 PyEval_EvalCodeEx + 211856 org.activestate.ActivePython27 0x00000001000c5d10 PyEval_EvalFrameEx + 2596857 org.activestate.ActivePython27 0x00000001000c7ad6 PyEval_EvalCodeEx + 211858 org.activestate.ActivePython27 0x000000010003d390 function_call + 17659 org.activestate.ActivePython27 0x000000010000be12 PyObject_Call + 98 60 org.activestate.ActivePython27 0x000000010001d36d instancemethod_call + 36561 org.activestate.ActivePython27 0x000000010000be12 PyObject_Call + 9862 org.activestate.ActivePython27 0x0000000100077dfa slot_tp_call + 7463 org.activestate.ActivePython27 0x000000010000be12 PyObject_Call + 9864 org.activestate.ActivePython27 0x00000001000c267d PyEval_EvalFrameEx + 1199765 org.activestate.ActivePython27 0x00000001000c7ad6 PyEval_EvalCodeEx + 211866 org.activestate.ActivePython27 0x00000001000c5d10 PyEval_EvalFrameEx + 2596867 org.activestate.ActivePython27 0x00000001000c7ad6 PyEval_EvalCodeEx + 211868 org.activestate.ActivePython27 0x00000001000c5d10 PyEval_EvalFrameEx + 2596869 org.activestate.ActivePython27 0x00000001000c7ad6 PyEval_EvalCodeEx + 211870 org.activestate.ActivePython27 0x00000001000c5d10 PyEval_EvalFrameEx + 2596871 org.activestate.ActivePython27 0x00000001000c7ad6 PyEval_EvalCodeEx + 211872 org.activestate.ActivePython27 0x00000001000c7bf6 PyEval_EvalCode + 5473 org.activestate.ActivePython27 0x00000001000ed31e PyRun_FileExFlags + 17474 org.activestate.ActivePython27 0x00000001000ed5d9 PyRun_SimpleFileExFlags + 48975 org.activestate.ActivePython27 0x00000001001041dc Py_Main + 294076 org.activestate.ActivePython27.app 0x0000000100000ed4 0x100000000 + 3796Thread 0 crashed with X86 Thread State (64-bit):rax: 0x0000000000000100 rbx: 0x00007fff7cd43640 rcx: 0x0000000000000000 rdx: 0x0000000105e00000rdi: 0x0000000000000008 rsi: 0x0000000105e01000 rbp: 0x00007fff5fbfa370 rsp: 0x00007fff5fbfa350r8: 0x0000000000000001 r9: 0x0000000105e00000 r10: 0x0000000105e01000 r11: 0x0000000000000000r12: 0x000000010ba10530 r13: 0x000000010b000000 r14: 0x00000001066d1970 r15: 0x00007fff915311afrip: 0x00007fff91534c90 rfl: 0x0000000000010206 cr2: 0x0000000000000110Logical CPU: 2Error Code: 0x00000006Trap Number: 14.........VM Region Summary:ReadOnly portion of Libraries: Total=183.7M resident=97.0M(53%) swapped_out_or_unallocated=86.7M(47%)Writable regions: Total=1.3G written=142.8M(11%) resident=503.6M(39%) swapped_out=0K(0%) unallocated=791.7M(61%) clf = GridSearchCV(neighbors.KNeighborsRegressor(), parameters, n_jobs=1) [60.0, 7.0, 12.0, 21.0, 5.5, 3.0, 0.0, 2.5, 11.0, 3.0, 16.0, 2.0, 0.0, 4.5, 2.5, 6.0, 9.5, 2.5, 15.0, 7.0, 8.0, 13.0, 14.0, 8.0, 3.5, 6.0, 22.5, 7.0, 4.0, 3.5, 4.5, 6.0, 5.5, 7.0, 2.0, 0.0, 0.0, 0.0, 14.5, 8.0, 7.5, 2.5, 11.5, 1.0, 3.0, 14.5, 10.0, 14.5, 8.0, 8.0, 7.0, 2.5, 3.5, 3.0, 13.5, 7.0, 6.5, 2.5, 9.0, 8.0, 11.0, 17.5, 12.5, 4.5, 5.5, 8.0, 2.0, 7.0, 4.0, 1.5, 3.0, 21.5, 4.5, 4.0, 7.0, 9.0, 13.5, 8.0, 10.5, 4.5, 1.5, 11.5, 7.5, 11.5, 4.5, 5.0, 7.0, 9.5, 4.0, 4.0, 6.0, 3.5, 4.5, 7.5, 3.5, 3.5, 3.5, 6.0, 5.0, 5.5, 25.0, 6.5, 5.0, 2.0, 2.0, 10.5, 0.0, 6.5, 19.0, 9.0, 1.0, 1.5, 1.0, 0.0, 1.0, 4.5, 2.5, 17.5, 39.5, 7.5, 5.5, 8.0, 1.0, 6.0, 12.0, 10.0, 5.5, 19.0, 4.5, 1.5, 25.5, 4.0, 10.0, 18.5, 9.5, 10.5, 2.5, 6.0, 1.0, 10.0, 8.5, 12.5, 13.5, 5.0, 6.5, 11.0, 4.5, 8.0, 7.5, 11.5, 14.5, 9.0, 3.0, 1.5, 3.5, 5.5, 2.5, 12.5, 6.5, 5.5, 5.0, 0.0, 8.0, 3.0, 14.5, 5.0, 14.0, 7.0, 13.5, 12.5, 4.0, 1.5, 6.5, 10.5, 9.0, 16.5, 4.0, 4.0, 15.0, 11.5, 2.5, 8.5, 3.0, 5.0, 4.0, 8.5, 6.0, 5.0, 5.0, 5.0, 5.5, 8.0, 11.0, 4.0, 0.0, 5.5, 0.0, 4.5, 1.5, 0.0, 6.5, 11.0, 2.5, 8.0, 15.5, 5.5, 4.5, 5.0, 4.0, 5.5, 10.5, 7.5, 6.5, 8.5, 2.5, 1.5, 1.5, 18.0, 15.0, 14.0, 9.5, 5.5, 7.5, 14.5, 2.5, 5.0, 60.0, 6.5, 14.5, 6.5, 4.0, 1.5, 2.0, 4.0, 27.0, 3.0, 5.0, 4.0, 2.5, 1.0, 1.5, 1.5, 9.0, 4.0, 8.5, 4.0, 4.0, 0.0, 1.5, 7.5, 1.5, 7.5, 1.0, 28.5, 15.5, 7.5, 1.0, 2.5, 2.5, 2.5, 16.0, 5.5, 8.5, 4.0, 2.5, 5.0, 2.5, 6.0, 11.0, 10.0, 4.5, 6.5, 8.0, 6.0, 4.5, 15.5, 4.0, 5.0]",scikit-learn's GridSearchCV stops working when n_jobs>1
how to get shapefile geometry type in PyQgis," I'm writing a script that is dependent on knowing the geometry type of the loaded shapefile.but I've looked in the pyqgis cookbook and API and can't figure out how to call it. infact, I have trouble interpreting the API, so any light shed on that subject would be appreciated. Thank you <code> ",How to get shapefile geometry type in PyQGIS?
How to delete delete a session variable in django?," my session variable is 'cart': if I want to delete all my cart variable I just do this in Python: but I just want to delete key '8', so I try this but it doesn't work: however if I print request.session['cart']['8'] and get a, b <code>  cart = {'8': ['a'], ['b'], '9': ['c'], ['d']} del request.session['cart'] del request.session['cart']['8']",How to delete a session variable in django?
calculating time difference between two rows," I'm trying to calculate the time difference between two rows using shift(), but I get an unexpected error. I may be missing something obvious This statement produces a ValueError: Cannot shift with no offset. What am I missing? <code>  df['Delta'] = (df.index - df.index.shift(1))",Calculating time difference between two rows
Calculating time difference between two rows in Python," I'm trying to calculate the time difference between two rows using shift(), but I get an unexpected error. I may be missing something obvious This statement produces a ValueError: Cannot shift with no offset. What am I missing? <code>  df['Delta'] = (df.index - df.index.shift(1))",Calculating time difference between two rows
"PIP install ""error: package directory X does not exist"""," I am trying to install this package via PIP. It gives me the following error: I find this weird, because the relevant setup.py does not mention any packages variable, but only py_modules.What's wrong? Can you help me out? Here is the full output of pip install -e RTbatch: <code>  error: package directory 'RTbatch' does not exist Obtaining file:///home/chymera/RTbatch Running setup.py (path:/home/chymera/RTbatch/setup.py) egg_info for package from file:///home/chymera/RTbatch /usr/lib64/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'heywords' warnings.warn(msg) error: package directory 'RTbatch' does not exist Complete output from command python setup.py egg_info: /usr/lib64/python2.7/distutils/dist.py:267: UserWarning: Unknown distribution option: 'heywords' warnings.warn(msg)running egg_infocreating RTbatch.egg-infowriting requirements to RTbatch.egg-info/requires.txtwriting RTbatch.egg-info/PKG-INFOwriting top-level names to RTbatch.egg-info/top_level.txtwriting dependency_links to RTbatch.egg-info/dependency_links.txtwriting manifest file 'RTbatch.egg-info/SOURCES.txt'warning: manifest_maker: standard file '-c' not founderror: package directory 'RTbatch' does not exist----------------------------------------Cleaning up...Command python setup.py egg_info failed with error code 1 in /home/chymera/RTbatchStoring debug log for failure in /home/chymera/.pip/pip.log","PIP install ""error: package directory 'X' does not exist"""
"sqlite3.OperationalError: near ""?"": syntax error"," I want to dynamically choose what table to use in a SQL query, but I just keep getting error however I am trying to format this. Also tried %s instead of ?. Any suggestions?  <code>  group_food = (group, food)group_food_new = (group, food, 1)with con: cur = con.cursor() tmp = cur.execute(""SELECT COUNT(Name) FROM (?) WHERE Name=?"", group_food) if tmp == 0: cur.execute(""INSERT INTO ? VALUES(?, ?)"", group_food_new) else: times_before = cur.execute(""SELECT Times FROM ? WHERE Name=?"", group_food) group_food_update = (group, (times_before +1), food) cur.execute(""UPDATE ? SET Times=? WHERE Name=?"", group_food_update)","Inserting a table name into a query gives sqlite3.OperationalError: near ""?"": syntax error"
Python wrong calculation with float, OK... obviously I did something wrong... i know that these float thing are sometimes a little bit difficult from c#... but i thought that * 1000 should work... OK wrongCan someone tell me how to do this better?thank you very much <code>  x = '16473.6'y = str(int(float(x) * 1000))print(y)>>>16473599,Why does python provide wrong calculations with float
Gmail API python - how to send message to multiple recipients?," I'm having some trouble sending a message to multiple addresses using the Gmail API. I've successfully sent a message to only one address, but get the following error when I include multiple comma-separated addresses in the 'To' field:An error occurred: <HttpError 400 when requestinghttps://www.googleapis.com/gmail/v1/users/me/messages/send?alt=jsonreturned ""Invalid to header"">I'm using the CreateMessage and SendMessage methods from this Gmail API guide:https://developers.google.com/gmail/api/guides/sendingThat guide states that the Gmail API requires messages that are RFC-2822 compliant. I again didn't have much luck using some of these addressing examples in the RFC-2822 guide:https://www.rfc-editor.org/rfc/rfc2822#appendix-AI'm under the impression that 'mary@x.test, jdoe@example.org, one@y.test' should be a valid string to pass into the 'to' parameter of CreateMessage, but the error that I received from SendMessage leads me to believe otherwise.Please let me know if you can recreate this problem, or if you have any advice on where I may be making a mistake. Thank you!Edit: Here is the actual code that yields an error... <code>  def CreateMessage(sender, to, subject, message_text): message = MIMEText(message_text) message['to'] = to message['from'] = sender message['subject'] = subject return {'raw': base64.urlsafe_b64encode(message.as_string())}def SendMessage(service, user_id, message): try: message = (service.users().messages().send(userId=user_id, body=message) .execute()) print 'Message Id: %s' % message['id'] return message except errors.HttpError, error: print 'An error occurred: %s' % errordef ComposeEmail(): # build gmail_service object using oauth credentials... to_addr = 'Mary Smith <mary@x.test>, jdoe@example.org, Who? <60one@y.test>' from_addr = 'me@address.com' message = CreateMessage(from_addr,to_addr,'subject text','message body') message = SendMessage(gmail_service,'me',message)",How to send message to multiple recipients?
Specify format for input arguments argparse python," I have a Python script that requires some command line inputs and I am using argparse for parsing them. I found the documentation a bit confusing and couldn't find a way to check for a format in the input parameters. What I mean by checking format is explained with this example script: I need to check for option -s and -e that the input by the user is in the format YYYY-MM-DD. Is there an option in argparse that I do not know of which accomplishes this? <code>  parser.add_argument('-s', ""--startdate"", help=""The Start Date - format YYYY-MM-DD "", required=True)parser.add_argument('-e', ""--enddate"", help=""The End Date format YYYY-MM-DD (Inclusive)"", required=True)parser.add_argument('-a', ""--accountid"", type=int, help='Account ID for the account for which data is required (Default: 570)')parser.add_argument('-o', ""--outputpath"", help='Directory where output needs to be stored (Default: ' + os.path.dirname(os.path.abspath(__file__)))",Specify date format for Python argparse input arguments
Specify date format for input arguments argparse python," I have a Python script that requires some command line inputs and I am using argparse for parsing them. I found the documentation a bit confusing and couldn't find a way to check for a format in the input parameters. What I mean by checking format is explained with this example script: I need to check for option -s and -e that the input by the user is in the format YYYY-MM-DD. Is there an option in argparse that I do not know of which accomplishes this? <code>  parser.add_argument('-s', ""--startdate"", help=""The Start Date - format YYYY-MM-DD "", required=True)parser.add_argument('-e', ""--enddate"", help=""The End Date format YYYY-MM-DD (Inclusive)"", required=True)parser.add_argument('-a', ""--accountid"", type=int, help='Account ID for the account for which data is required (Default: 570)')parser.add_argument('-o', ""--outputpath"", help='Directory where output needs to be stored (Default: ' + os.path.dirname(os.path.abspath(__file__)))",Specify date format for Python argparse input arguments
repeating each element of numpy array 5 times," I want to repeat each element of data 5 times and make new array as follows: How can I do it?What about repeating the whole array 5 times? <code>  import numpy as npdata = np.arange(-50,50,10)print data[-50 -40 -30 -20 -10 0 10 20 30 40] ans = [-50 -50 -50 -50 -50 -40 -40 ... 40] ans = [-50 -40 -30 -20 -10 0 10 20 30 40 -50 -40 -30 -20 -10 0 10 20 30 40 -50 -40 -30 -20 -10 0 10 20 30 40 -50 -40 -30 -20 -10 0 10 20 30 40 -50 -40 -30 -20 -10 0 10 20 30 40 .......]",Repeating each element of a numpy array 5 times
What is the python equivalent of JavaScript's Array.prototype.some?," Does python have any equivalent to JavaScript's Array.prototype.some / every?Trivial JavaScript example: Will output: The below python seems to be functionally equivalent, but I do not know if there is a more ""pythonic"" approach. <code>  var arr = [ ""a"", ""b"", ""c"" ];arr.some(function (element, index) { console.log(""index: "" + index + "", element: "" + element) if(element === ""b""){ return true; }}); index: 0, element: aindex: 1, element: b arr = [ ""a"", ""b"", ""c"" ]for index, element in enumerate(arr): print(""index: %i, element: %s"" % (index, element)) if element == ""b"": break",What is the python equivalent of JavaScript's Array.prototype.some / every?
hot to set Jenkins env variable with python script," I have a jenkins build process and i use a python script to calculate the new version: And then i want to run on a different step. but the ${NEW_POM_VERSION} stays the same and does not translate to the value i set.Am i trying to call the variable in the wrong way. i also tried using $NEW_POM_VERSION which didn't work as wellso how am i supposed to export the variable correctly to my environment.Thanks. <code>  import stringimport osprint 'Current version is ' + os.environ['POM_VERSION']versionArr = string.split(os.environ['POM_VERSION'], '.')versionArr[2] = str(int(versionArr[2]) + 1)if int(versionArr[2]) > 100: versionArr[2] = '0' versionArr[1] = str(int(versionArr[1]) + 1)if int(versionArr[1]) > 100: versionArr[0] = str(int(versionArr[0]) + 1) versionArr[1] = '0'print versionArrprint 'New version will be: ' + versionArr[0] + '.' + versionArr[1] + '.' + versionArr[2]os.environ['NEW_POM_VERSION'] = versionArr[0] + '.' + versionArr[1] + '.' + versionArr[2] versions:set -DnewVersion=${NEW_POM_VERSION} -DgenerateBackupPoms=false",how to set Jenkins env variable with python script
Github cloning a private rep with python," How to clone a private repository from Github using python?I found some good information about git and python, but I started learning python few days back. <code> ",Cloning a private Github repo using a script
PyQt: Customizing QRubberBand look while using cleanlooks style," What I try is to customize the look and feel of my custom QRubberBand a bit. I want to achieve a blue selection edge with a transparent blue selection rectangle background. Problem is that the background is always transparent when I use CleanLooks style but when I switch to for example WindowsVista style everything works fine.I'm using PyQt 10.4.3 on a Windows machine.Here is a small code sample from which you can see this strange behavior. <code>  from PyQt4 import QtGui,QtCorefrom PyQt4.QtCore import Qtclass Selector(QtGui.QRubberBand): """""" Custom QRubberBand """""" def __init__(self,*arg,**kwargs): super(Selector,self).__init__(*arg,**kwargs) def paintEvent(self, QPaintEvent): painter = QtGui.QPainter(self) # set pen painter.setPen(QtGui.QPen(Qt.blue,4)) # set brush color = QtGui.QColor(Qt.blue) painter.setBrush(QtGui.QBrush(color)) # set opacity painter.setOpacity(0.3) # draw rectangle painter.drawRect(QPaintEvent.rect())class Panel(QtGui.QWidget): def __init__(self,parent = None): super(Panel,self).__init__(parent) self.rubberBand = Selector(QtGui.QRubberBand.Rectangle,self) def mousePressEvent(self, QMouseEvent): self.clickPosition = QMouseEvent.pos() self.rubberBand.setGeometry(QtCore.QRect(self.clickPosition,QtCore.QSize())) self.rubberBand.show() def mouseMoveEvent(self, QMouseEvent): pos = QMouseEvent.pos() self.rubberBand.setGeometry(QtCore.QRect(self.clickPosition,pos).normalized()) def mouseReleaseEvent(self, QMouseEvent): self.rubberBand.hide()if __name__ == ""__main__"": import sys app = QtGui.QApplication([]) QtGui.QApplication.setStyle(""cleanlooks"") pn = Panel() pn.show() sys.exit(app.exec_())",Customizing QRubberBand look while using cleanlooks style
Does scipy logsumexp() include the maxval hack?," Does the scipy's logsumexp() implementation include the hack that prevents underflow by subtracting the maximum found value in the array from each element?The one explained here below, where m = maxval: <code> ",Does scipy logsumexp() deal with the underflow challenge?
"What are some rules of thumb for deciding between __get__, __getattr__, and __getattribute__? (Py3+)"," What are some general rules of thumb for choosing which of these to implement in a given class, in a given situation?I have read the docs, and so understand the difference between them. Rather, I am looking for guidance on how to best integrate their usage into my workflow by being better able to notice more subtle opportunities to use them, and which to use when. That kind of thing. The methods in question are (to my knowledge):  <code>  ## fallback__getattr____setattr____delattr__ ## full control__getattribute__##(no __setattribute__ ? What's the deal there?) ## (the descriptor protocol)__get____set____delete__","What are some rules of thumb for deciding between __get__, __getattr__, and __getattribute__?"
Python safe method to get value of nested dictionary, I have a nested dictionary. Is there only one way to get values out safely? Or maybe python has a method like get() for nested dictionary ? <code>  try: example_dict['key1']['key2']except KeyError: pass,Safe method to get value of nested dictionary
compare two json objects python," How can I test whether two JSON objects are equal in python, disregarding the order of lists?For example ...JSON document a: JSON document b: a and b should compare equal, even though the order of the ""errors"" lists are different. <code>  { ""errors"": [ {""error"": ""invalid"", ""field"": ""email""}, {""error"": ""required"", ""field"": ""name""} ], ""success"": false} { ""success"": false, ""errors"": [ {""error"": ""required"", ""field"": ""name""}, {""error"": ""invalid"", ""field"": ""email""} ]}",How to compare two JSON objects with the same elements in a different order equal?
What is being pickled with python multiprocessor?," I know that multiprocessing uses pickling in order to have the processes run on different CPUs, but I think I am a little confused as to what is being pickled. Lets look at this code. I assume what is being pickled is the def f(I) and the argument going in. First, is this assumption correct?Second, lets say f(I) has a function call within in it like: Does the randomfunction's definition get pickled as well, or is it only the function call?Further more, if that function call was located in another file, would the process be able to call it? <code>  from multiprocessing import Processdef f(I): print('hello world!',I)if __name__ == '__main__': for I in (range1, 3): Process(target=f,args=(I,)).start() def f(I): print('hello world!',I) randomfunction()",What is being pickled when I call multiprocessing.Process?
Python CSV read error," I created a python script which works with a test CSV dataset of 10 records. When I scaled this up to the actual datasets (a few thousand rows), I am getting the following error: _csv.Error: new-line character seen in unquoted field - do you need to open the file in universal-newline mode?The code is as follows: The full error code is: Perhaps there is a scale problem with the CSV reading method I am using? <code>  with open('./Origins.csv', 'r') as csvfile: reader = csv.DictReader(csvfile) origincoords = ['{Y},{X}'.format(**row) for row in reader] Traceback (most recent call last): File ""./Driving.py"", line 14, in <module> origincoords = ['{Y},{X}'.format(**row) for row in reader] File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/csv.py"", line 103, in next self.fieldnames File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/csv.py"", line 90, in fieldnames self._fieldnames = self.reader.next()_csv.Error: new-line character seen in unquoted field - do you need to open the file in universal-newline mode?",CSV read error: new-line character seen in unquoted field
Python tuple equality," I expected the following two tuples to compare unequal, but they don't: Why is that? <code>  >>> x = tuple(set([1, ""a"", ""b"", ""c"", ""z"", ""f""]))>>> y = tuple(set([""a"", ""b"", ""c"", ""z"", ""f"", 1])) >>> x == y>>> True",Why are tuples constructed from differently initialized sets equal?
Numpy: Rearrange array based upon index array," I'm looking for a one line solution that would help me do the following. Suppose I have I'd like to rearrange it based upon an input ordering. If there were a numpy function called arrange, it would do the following: Formally, if the array to be reordered is m x n, and the ""index"" array is 1 x n, the ordering would be determined by the array called ""index"".Does numpy have a function like this? <code>  array = np.array([10, 20, 30, 40, 50]) newarray = np.arrange(array, [1, 0, 3, 4, 2])print newarray [20, 10, 40, 50, 30]",How to rearrange array based upon index array
Counting total number of threads executed in multiprocessing during execution," I'd love to give an indication of the current talk in total that we are only. I'm farming work out and would like to know current progress. So if I sent 100 jobs to 10 processors, how can I show what the current number of jobs that have returned is. I can get the id's but but how do I count up the number of completed returned jobs from my map function.I'm calling my function as the following: And in my function I can print the current name <code>  op_list = pool.map(PPMDR_star, list(varg)) current = multiprocessing.current_process()print 'Running: ', current.name, current._identity",Counting total number of tasks executed in a multiprocessing.Pool during execution
Unable to import wxPython using Python 3," I installed wxPython 3.0.1.1, but I'm unable to import wx using Python 3.4.1. I am getting the following error: Nevertheless, I can import wx if I use Python 2.7 (the default installation in my OS X 10.9): How can I use wxPython for Python 3, and specifically for Python 3.4.1? <code>  Python 3.4.1 (v3.4.1:c0e311e010fc, May 18 2014, 00:54:21) [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwinType ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import wxTraceback (most recent call last): File ""<stdin>"", line 1, in <module>ImportError: No module named 'wx' Python 2.7.5 (default, Mar 9 2014, 22:15:05) [GCC 4.2.1 Compatible Apple LLVM 5.0 (clang-500.0.68)] on darwinType ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import wx>>>",How to use wxPython for Python 3?
Check in Pyton if URL exists," There are quite a few questions on SO about this topic, but none of which answers the following issue. Checking a normal URL with Python requests can be done easily like so: A status code of 200 means the page exists. In this particular case, it's a fan page on Facebook.Trying this with a normal user profile on Facebook can work, too: However, there are (seemingly random) user profiles that result in a 404 status code, despite a normal browser returns a 200: Using a custom header with User-Agent string or checking the URL with other methods all fails the same way: There are other examples of URLs that inexplicably fail with the above methods. One of which is this: http://www.rajivbajaj.net/It works perfectly with a 200 status code in all browsers, but results in a 403 for all Python methods described above.I'm trying to write a reliable URL validator, but I can't see why those URLs are failing these tests. Any ideas? <code>  print requests.head('https://www.facebook.com/pixabay').status_code print requests.head('https://www.facebook.com/steinberger.simon').status_code print requests.head('https://www.facebook.com/drcarl').status_code import requests, urllib, urllib2url = 'https://www.facebook.com/drcarl'print requests.head(url).status_code# using an User-Agent stringheaders = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.107 Safari/537.36' }print requests.head(url, headers=headers).status_code# using GET instead if HEAD as request methodprint requests.get(url, stream=True).status_code# using urllibprint urllib.urlopen(url).getcode()# using urllib2try: r = urllib2.urlopen(url) print r.getcode()except urllib2.HTTPError as e: print e.code",Check in Python if URL exists
Fast way to perform complex operations on large numpy arrays," I have a 2-dimensional array of integers, we'll call it ""A"". I want to create a 3-dimensional array ""B"" of all 1s and 0s such that:for any fixed (i,j) sum(B[i,j,:])==A[i.j], that is, B[i,j,:] contains A[i,j] 1s in it the 1s are randomly placed in the 3rd dimension. I know how I would do this using standard python indexing but this turns out to be very slow. I am looking for a way to do this that takes advantage of the features that can make Numpy fast.Here is how I would do it using standard indexing: Can someone please explain how I can do this in a faster way?Edit: Here is an example ""A"": in this case X=Y=5 and Z>=5 <code>  B=np.zeros((X,Y,Z))indexoptions=range(Z)for i in xrange(Y): for j in xrange(X): replacedindices=np.random.choice(indexoptions,size=A[i,j],replace=False) B[i,j,[replacedindices]]=1 A=np.array([[0,1,2,3,4],[0,1,2,3,4],[0,1,2,3,4],[0,1,2,3,4],[0,1,2,3,4]])",quickly calculate randomized 3D numpy array from 2D numpy array
Pyhton Scrapy tutorial KeyError: 'Spider not found:," I'm trying to write my first scrapy spider, Ive been following the tutorial at http://doc.scrapy.org/en/latest/intro/tutorial.html But I'm getting an error ""KeyError: 'Spider not found: ""I think I'm running the command from the correct directory (the one with the scrapy.cfg file) Here is the error I'm getting This is my virtualenv: Here is the code for my spider wth the name attribute filled in: <code>  (proscraper)#( 10/14/14@ 2:06pm )( tim@localhost ):~/Workspace/Development/hacks/prosum-scraper/scrapy tree. scrapy __init__.py items.py pipelines.py settings.py spiders __init__.py juno_spider.py scrapy.cfg2 directories, 7 files(proscraper)#( 10/14/14@ 2:13pm )( tim@localhost ):~/Workspace/Development/hacks/prosum-scraper/scrapy lsscrapy scrapy.cfg (proscraper)#( 10/14/14@ 2:13pm )( tim@localhost ):~/Workspace/Development/hacks/prosum-scraper/scrapy scrapy crawl juno/home/tim/.virtualenvs/proscraper/lib/python2.7/site-packages/twisted/internet/_sslverify.py:184: UserWarning: You do not have the service_identity module installed. Please install it from <https://pypi.python.org/pypi/service_identity>. Without the service_identity module and a recent enough pyOpenSSL tosupport it, Twisted can perform only rudimentary TLS client hostnameverification. Many valid certificate/hostname mappings may be rejected. verifyHostname, VerificationError = _selectVerifyImplementation()Traceback (most recent call last): File ""/home/tim/.virtualenvs/proscraper/bin/scrapy"", line 9, in <module> load_entry_point('Scrapy==0.24.4', 'console_scripts', 'scrapy')() File ""/home/tim/.virtualenvs/proscraper/lib/python2.7/site-packages/scrapy/cmdline.py"", line 143, in execute _run_print_help(parser, _run_command, cmd, args, opts) File ""/home/tim/.virtualenvs/proscraper/lib/python2.7/site-packages/scrapy/cmdline.py"", line 89, in _run_print_help func(*a, **kw) File ""/home/tim/.virtualenvs/proscraper/lib/python2.7/site-packages/scrapy/cmdline.py"", line 150, in _run_command cmd.run(args, opts) File ""/home/tim/.virtualenvs/proscraper/lib/python2.7/site-packages/scrapy/commands/crawl.py"", line 58, in run spider = crawler.spiders.create(spname, **opts.spargs) File ""/home/tim/.virtualenvs/proscraper/lib/python2.7/site-packages/scrapy/spidermanager.py"", line 44, in create raise KeyError(""Spider not found: %s"" % spider_name)KeyError: 'Spider not found: juno' (proscraper)#( 10/14/14@ 2:13pm )( tim@localhost ):~/Workspace/Development/hacks/prosum-scraper/scrapy pip freezeScrapy==0.24.4Twisted==14.0.2cffi==0.8.6cryptography==0.6cssselect==0.9.1ipdb==0.8ipython==2.3.0lxml==3.4.0pyOpenSSL==0.14pycparser==2.10queuelib==1.2.2six==1.8.0w3lib==1.10.0wsgiref==0.1.2zope.interface==4.1.1 (proscraper)#( 10/14/14@ 2:14pm )( tim@localhost ):~/Workspace/Development/hacks/prosum-scraper/scrapy cat scrapy/spiders/juno_spider.py import scrapyclass JunoSpider(scrapy.Spider): name = ""juno"" allowed_domains = [""http://www.juno.co.uk/""] start_urls = [ ""http://www.juno.co.uk/dj-equipment/"" ] def parse(self, response): filename = response.url.split(""/"")[-2] with open(filename, 'wb') as f: f.write(response.body)",Python Scrapy tutorial KeyError: 'Spider not found:
How to match regexes in python 3?," I was in IDLE, and decided to use regex to sort out a string. But when I typed in what the online tutorial told me to, all it would do was print: Full program: result: Could anybody tell me how to actually print the result? <code>  <_sre.SRE_Match object at 0x00000000031D7E68> import rereg = re.compile(""[a-z]+8?"")str = ""ccc8""print(reg.match(str)) <_sre.SRE_Match object at 0x00000000031D7ED0>",How to print regex match results in python 3?
Flask unittest breaks when a SQLAlchmey after_delete event triggers a task," When one of my unit tests deletes a SQLAlchemy object, the object triggers an after_delete event which triggers a Celery task to delete a file from the drive.The task is CELERY_ALWAYS_EAGER = True when testing. gist to reproduce the issue easilyThe example has two tests. One triggers the task in the event, the other outside the event. Only the one in the event closes the connection.To quickly reproduce the error you can run: The stack: <code>  git clone https://gist.github.com/5762792fc1d628843697.gitcd 5762792fc1d628843697virtualenv venv. venv/bin/activatepip install -r requirements.txtpython test.py $ python test.pyE======================================================================ERROR: test_delete_task (__main__.CeleryTestCase)----------------------------------------------------------------------Traceback (most recent call last): File ""test.py"", line 73, in test_delete_task db.session.commit() File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/scoping.py"", line 150, in do return getattr(self.registry(), name)(*args, **kwargs) File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 776, in commit self.transaction.commit() File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 377, in commit self._prepare_impl() File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 357, in _prepare_impl self.session.flush() File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 1919, in flush self._flush(objects) File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 2037, in _flush transaction.rollback(_capture_exception=True) File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 63, in __exit__ compat.reraise(type_, value, traceback) File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 2037, in _flush transaction.rollback(_capture_exception=True) File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 393, in rollback self._assert_active(prepared_ok=True, rollback_ok=True) File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 223, in _assert_active raise sa_exc.ResourceClosedError(closed_msg)ResourceClosedError: This transaction is closed----------------------------------------------------------------------Ran 1 test in 0.014sFAILED (errors=1)",Connection is closed when a SQLAlchemy event triggers a Celery task
Connection gets closed when a SQLAlchemy event triggers a celery task," When one of my unit tests deletes a SQLAlchemy object, the object triggers an after_delete event which triggers a Celery task to delete a file from the drive.The task is CELERY_ALWAYS_EAGER = True when testing. gist to reproduce the issue easilyThe example has two tests. One triggers the task in the event, the other outside the event. Only the one in the event closes the connection.To quickly reproduce the error you can run: The stack: <code>  git clone https://gist.github.com/5762792fc1d628843697.gitcd 5762792fc1d628843697virtualenv venv. venv/bin/activatepip install -r requirements.txtpython test.py $ python test.pyE======================================================================ERROR: test_delete_task (__main__.CeleryTestCase)----------------------------------------------------------------------Traceback (most recent call last): File ""test.py"", line 73, in test_delete_task db.session.commit() File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/scoping.py"", line 150, in do return getattr(self.registry(), name)(*args, **kwargs) File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 776, in commit self.transaction.commit() File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 377, in commit self._prepare_impl() File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 357, in _prepare_impl self.session.flush() File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 1919, in flush self._flush(objects) File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 2037, in _flush transaction.rollback(_capture_exception=True) File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 63, in __exit__ compat.reraise(type_, value, traceback) File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 2037, in _flush transaction.rollback(_capture_exception=True) File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 393, in rollback self._assert_active(prepared_ok=True, rollback_ok=True) File ""/home/brice/Code/5762792fc1d628843697/venv/local/lib/python2.7/site-packages/sqlalchemy/orm/session.py"", line 223, in _assert_active raise sa_exc.ResourceClosedError(closed_msg)ResourceClosedError: This transaction is closed----------------------------------------------------------------------Ran 1 test in 0.014sFAILED (errors=1)",Connection is closed when a SQLAlchemy event triggers a Celery task
"Python pandas: get elements (index,col) below diagonal in dataframe"," I have a pandas DataFrame, df. I want to extract a list of all the (col, index) in the df for which the value at (col, index) > .95. Additionally, I want to condition on the fact that they are in the lower diagonal of the df, not including the diagonal itself. (If it helps, it's a correlation df, so the diagonals are 1's which is not what I am interested in.)How can I do this? <code> ","pandas: get elements (index, col) below diagonal in DataFrame"
"pandas: get elements (index ,col) below diagonal in DataFrame"," I have a pandas DataFrame, df. I want to extract a list of all the (col, index) in the df for which the value at (col, index) > .95. Additionally, I want to condition on the fact that they are in the lower diagonal of the df, not including the diagonal itself. (If it helps, it's a correlation df, so the diagonals are 1's which is not what I am interested in.)How can I do this? <code> ","pandas: get elements (index, col) below diagonal in DataFrame"
opencv for python 3 under windows," I've been searching around the Internet for a while but I have not been able to find detailed instructions on how to install OpenCV for Python 3.x under Windows.I would really appreciate if anyone here can share his/her method if he/she had successfully installed OpenCV for Python 3.x, either from a pre-built binary or from the source code, for either version OpenCV 2.x or OpenCV 3.x. <code> ",OpenCV for Python 3.x under Windows
OpenCV for Python 3.x under windows," I've been searching around the Internet for a while but I have not been able to find detailed instructions on how to install OpenCV for Python 3.x under Windows.I would really appreciate if anyone here can share his/her method if he/she had successfully installed OpenCV for Python 3.x, either from a pre-built binary or from the source code, for either version OpenCV 2.x or OpenCV 3.x. <code> ",OpenCV for Python 3.x under Windows
Python UTF-8 to Unicode Converter, My string is Nim B Tt (Thin s Nht Hnh) and I want to decode it to Nim B Tt (Thin s Nht Hnh). I see in that site can do that http://www.enderminh.com/minh/utf8-to-unicode-converter.aspxand I start to try by Python but actually it is not correct because original string is utf-8 but the string show is not my expecting result.Note: it is Vietnamese character.How to resolve that case? Is that Windows Unicode or something? How to detect the encoding here. <code>  mystr = '09. Bt Nh Tm Kinh'mystr.decode('utf-8'),How to fix broken utf-8 encoding in Python?
Python How to fix broken utf-8 encoding?, My string is Nim B Tt (Thin s Nht Hnh) and I want to decode it to Nim B Tt (Thin s Nht Hnh). I see in that site can do that http://www.enderminh.com/minh/utf8-to-unicode-converter.aspxand I start to try by Python but actually it is not correct because original string is utf-8 but the string show is not my expecting result.Note: it is Vietnamese character.How to resolve that case? Is that Windows Unicode or something? How to detect the encoding here. <code>  mystr = '09. Bt Nh Tm Kinh'mystr.decode('utf-8'),How to fix broken utf-8 encoding in Python?
How to use sha256 hash in Python," I am trying to read in a file of passwords. Then I am trying to compute the hash for each password and compare it to a hash I already have to determine if I have discovered the password. However the error message I keep getting is ""TypeError: Unicode-objects must be encoded before hashing"". Here is my code: Can anyone please help and provide an explanation? <code>  from hashlib import sha256with open('words','r') as f: for line in f: hashedWord = sha256(line.rstrip()).hexdigest() if hashedWord == 'ca52258a43795ab5c89513f9984b8f3d3d0aa61fb7792ecefe8d90010ee39f2': print(line + ""is one of the words!"")",Python: TypeError: Unicode-objects must be encoded before hashing
A python function handles both a single number and a numpy array," As the title says, suppose I want to write a sign function (let's forget sign(0) for now), obviously we expect sign(2) = 1 and sign(array([-2,-2,2])) = array([-1,-1,1]). The following function won't work however, because it can't handle numpy arrays. The next function won't work either since x doesn't have a shape member if it's just a single number. Even if some trick like y = x*0 + 1 is used, y won't have a [] method. Even with the idea from another question(how can I make a numpy function that accepts a numpy array, an iterable, or a scalar?), the next function won't work when x is a single number because in this case x.shape and y.shape are just () and indexing y is illegal. The only solution seems to be that first decide if x is an array or a number, but I want to know if there is something better. Writing branchy code would be cumbersome if you have lots of small functions like this. <code>  def sign(x): if x>0: return 1 else: return -1 def sign(x): y = ones(x.shape) y[x<0] = -1 return y def sign(x): x = asarray(x) y = ones(x.shape) y[x<0] = -1 return y",A python function that accepts as an argument either a scalar or a numpy array
python 3 map/lamda method with 2 inputs," I have a dictionary like the following in python 3: I want to convert all he values to int using map function, and I wrote something like this: but the python complains: TypeError: () missing 1 required positional argument: 'val'My question is how can I write a lambda function with two inputs (E.g. key and val) <code>  ss = {'a':'2', 'b','3'} list(map(lambda key,val: int(val), ss.items())))",python 3 map/lambda method with 2 inputs
Sorting string according to a custom alphabet order in Python," I am looking for an efficient way to sort a list of strings according a custom alphabet.For example, I have a string alphabet which is ""bafmxpzv"" and a list of strings composed from only the characters contained in that alphabet.I would like a way to sort that list similarly to other common sorts, but using this custom alphabet. How can I do that? <code> ",Sorting string values according to a custom alphabet in Python
Optimizing db queries in Django Rest Framework," I have the following models: And those models are used in the following view and serializer: So basically, for each user returned by the GetAllUsers view, I want to print out whether the user is a friend with the requester (actually I should check both from_ and to_friend, but does not matter for the question in point)What I see is that for N users in database, there is 1 query for getting all N users, and then 1xN queries in the serializer's get_is_friend_alreadyIs there a way to avoid this in the rest-framework way? Maybe something like passing a select_related included query to the serializer that has the relevant Friendship rows? <code>  class User(models.Model): name = models.Charfield() email = models.EmailField()class Friendship(models.Model): from_friend = models.ForeignKey(User) to_friend = models.ForeignKey(User) class GetAllUsers(generics.ListAPIView): authentication_classes = (SessionAuthentication, TokenAuthentication) permission_classes = (permissions.IsAuthenticated,) serializer_class = GetAllUsersSerializer model = User def get_queryset(self): return User.objects.all()class GetAllUsersSerializer(serializers.ModelSerializer): is_friend_already = serializers.SerializerMethodField('get_is_friend_already') class Meta: model = User fields = ('id', 'name', 'email', 'is_friend_already',) def get_is_friend_already(self, obj): request = self.context.get('request', None) if request.user != obj and Friendship.objects.filter(from_friend = user): return True else: return False",Optimizing database queries in Django REST framework
Python launch multiple subprocess with a pool/queue recover output as soon as finished and launch next subprocess," I'm currently launching a subprocess and parsing stdout on the go without waiting for it to finish to parse stdout. In my script I perform this action multiple times, indeed depending on the number of input samples.Main problem here is that every subprocess is a program/tool that uses 1 CPU for 100% while it's running. And it takes sometime.. maybe 20-40 min per input.What I would like to achieve, is to set a pool, queue (I'm not sure what's the exact terminology here) of N max subprocess job process running at same time. So I could maximize performance, and not proceed sequentially.So an execution flow for example a max 4 jobs pool should be:Launch 4 subprocess.When one of jobs finishes, parse stdout and launch next.Do this until all the jobs in queue are finished.If I can achieve this I really don't know how I could identify which sample subprocess is the one that has finished. At this moment, I don't need to identify them since each subprocess runs sequentially and I parse stdout as subprocess is printing stdout.This is really important, since I need to identify the output of each subprocess and assign it to it's corresponding input/sample. <code>  for sample in all_samples: my_tool_subprocess = subprocess.Popen('mytool {}'.format(sample),shell=True, stdout=subprocess.PIPE) line = True while line: myline = my_tool_subprocess.stdout.readline() #here I parse stdout..",Python multiple subprocess with a pool/queue recover output as soon as one finishes and launch next job in queue
Python launch multiple subprocess with a pool/queue recover output as soon as one finishes and launch next job in queue," I'm currently launching a subprocess and parsing stdout on the go without waiting for it to finish to parse stdout. In my script I perform this action multiple times, indeed depending on the number of input samples.Main problem here is that every subprocess is a program/tool that uses 1 CPU for 100% while it's running. And it takes sometime.. maybe 20-40 min per input.What I would like to achieve, is to set a pool, queue (I'm not sure what's the exact terminology here) of N max subprocess job process running at same time. So I could maximize performance, and not proceed sequentially.So an execution flow for example a max 4 jobs pool should be:Launch 4 subprocess.When one of jobs finishes, parse stdout and launch next.Do this until all the jobs in queue are finished.If I can achieve this I really don't know how I could identify which sample subprocess is the one that has finished. At this moment, I don't need to identify them since each subprocess runs sequentially and I parse stdout as subprocess is printing stdout.This is really important, since I need to identify the output of each subprocess and assign it to it's corresponding input/sample. <code>  for sample in all_samples: my_tool_subprocess = subprocess.Popen('mytool {}'.format(sample),shell=True, stdout=subprocess.PIPE) line = True while line: myline = my_tool_subprocess.stdout.readline() #here I parse stdout..",Python multiple subprocess with a pool/queue recover output as soon as one finishes and launch next job in queue
Can switching in-and-out `PyFrameObject`s be a good implementation of continuations?," I'm interested in continuations, specifically in Python's C-API. From what i understand, the nature of continuations requires un-abstracting low-level calling conventions in order to manipulate the call stack as needed. I was fortunate enough to come across a few examples of these scattered here and there. In the few examples i've come across, this un-abstraction is done using either clever C (with assumptions about the environment), or custom assembly.However, what's cool about Python is that it has its own interpreter stack made up of PyFrameObjects. Assuming single-threaded applications for now, shouldn't it be enough to just switch in-and-out PyFrameObjectss to implement continuations in Python's C-API? Why do these authors even bother with the low-level stuff? <code> ",Can switching in-and-out PyFrameObjects be a good implementation of continuations?
Extract Number from String - Python," I am new to Python and I have a String, I want to extract the numbers from the string. For example: Output is ['4', '3']I want to get 3158 only, as an Integer preferably, not as List.  <code>  str1 = ""3158 reviews""print (re.findall('\d+', str1 ))",Extract Number from String in Python
"Get a ""plain old"" array back from an itertools.chain object"," Suppose I have list_of_numbers = [[1, 2], [3], []] and I want the much simpler object list object x = [1, 2, 3].Following the logic of this related solution, I do Unfortunately, chain is not exactly what I want because (for instance) running chain at the console returns <itertools.chain object at 0x7fb535e17790>. What is the function f such that if I do x = f(chain) and then type x at the console I get [1, 2, 3]?Update: Actually the result I ultimately need is array([1, 2, 3]). I'm adding a line in a comment on the selected answer to address this. <code>  list_of_numbers = [[1, 2], [3], []]import itertoolschain = itertools.chain(*list_of_numbers)",Get an array back from an itertools.chain object
Printing variables in python 3.4," So the syntax seems to have changed from what I learned in Python 2... here is what I have so far The first value being an int, the second a string, and the final an int.How can I alter my print statement so that it prints the variables correctly? <code>  for key in word: i = 1 if i < 6: print ( ""%s. %s appears %s times."") % (str(i), key, str(wordBank[key]))",Printing variables in Python 3.4
Add cylinder to 3D plot," I would like to add a transparent cylinder to my 3D scatter plot. How can I do it? This is the code I am using to make the plot: <code>  fig = plt.figure(2, figsize=(8, 6))ax = fig.add_subplot(111, projection='3d')ax.scatter(X, Y, Z, c=Z,cmap=plt.cm.Paired)ax.set_xlabel(""X"")ax.set_ylabel(""Y"")ax.set_zlabel(""Z"")plt.xticks()",Add cylinder to plot
Python - writing a CSV from Flask framework," I have no problems writing a CSV outside of the Flask framework. But when I try to write it from Flask, it writes to the CSV, but only on one line. Here is the template I'm following This writes the CSV perfectly, but when I try with my code, I get one long row. My code: My output: Thanks.EDIT: I tried just about all the answers and they worked for the most part, but I chose vectorfrog's because it fit with what I was trying to accomplish. <code>  @app.route('/download')def download(): csv = """"""""REVIEW_DATE"",""AUTHOR"",""ISBN"",""DISCOUNTED_PRICE""""1985/01/21"",""Douglas Adams"",0345391802,5.95""1990/01/12"",""Douglas Hofstadter"",0465026567,9.95""1998/07/15"",""Timothy """"The Parser"""" Campbell"",0968411304,18.99""1999/12/03"",""Richard Friedman"",0060630353,5.95""2004/10/04"",""Randel Helms"",0879755725,4.50"""""" response = make_response(csv) response.headers[""Content-Disposition""] = ""attachment; filename=books.csv"" return response @app.route('/download')def post(self): # lots of code csvList.append([all,my,data,goes,here]) csvList = str(re.sub('\[|\]','',str(csvList))) # convert to a string; remove brackets response = make_response(csvList) response.headers['Content-Disposition'] = ""attachment; filename=myCSV.csv"" return response Nashville Physician Service Ce,Treasury Specialist,Brentwood,TN,(615) 507-1646,La Petite Academy,Afternoon Teacher Aide,Goodlettsville,TN,(615) 859-2034,Nashville Physician Service Ce,Denial Resolution Specialist,Brentwood,TN,(615) 507-1646",Writing a CSV from Flask framework
Create pdf from pdf using python," When I print a PDF from any of my source PDFs, the file size drops and removes the text boxes presents in form. In short, it flattens the file.This is behavior I want to achieve.The following code to create a PDF using another PDF as a source (the one I want to flatten), it writes the text boxes form as well.Can I get a PDF without the text boxes, flatten it? Just like Adobe does when I print a PDF as a PDF.My other code looks something like this minus some things: Summing up: I have a pdf, I add a text box to it, covering up info and adding new info, and then I print a pdf from that pdf. The text box becomes not editable or moveable any longer. I wanted to automate that process but everything I tried still allowed that text box to be editable. <code>  import osimport StringIOfrom pyPdf import PdfFileWriter, PdfFileReaderfrom reportlab.pdfgen import canvasfrom reportlab.lib.pagesizes import letterdirectory = os.path.join(os.getcwd(), ""source"") # dir we are interested infif = [f for f in os.listdir(directory) if f[-3:] == 'pdf'] # get the PDFsfor i in fif: packet = StringIO.StringIO() can = canvas.Canvas(packet, pagesize=letter) can.rotate(-90) can.save() packet.seek(0) new_pdf = PdfFileReader(packet) fname = os.path.join('source', i) existing_pdf = PdfFileReader(file(fname, ""rb"")) output = PdfFileWriter() nump = existing_pdf.getNumPages() page = existing_pdf.getPage(0) for l in range(nump): output.addPage(existing_pdf.getPage(l)) page.mergePage(new_pdf.getPage(0)) outputStream = file(""out-""+i, ""wb"") output.write(outputStream) outputStream.close() print fName + "" written as"", i",Generate flattened PDF with Python
Print pdf from pdf using Python," When I print a PDF from any of my source PDFs, the file size drops and removes the text boxes presents in form. In short, it flattens the file.This is behavior I want to achieve.The following code to create a PDF using another PDF as a source (the one I want to flatten), it writes the text boxes form as well.Can I get a PDF without the text boxes, flatten it? Just like Adobe does when I print a PDF as a PDF.My other code looks something like this minus some things: Summing up: I have a pdf, I add a text box to it, covering up info and adding new info, and then I print a pdf from that pdf. The text box becomes not editable or moveable any longer. I wanted to automate that process but everything I tried still allowed that text box to be editable. <code>  import osimport StringIOfrom pyPdf import PdfFileWriter, PdfFileReaderfrom reportlab.pdfgen import canvasfrom reportlab.lib.pagesizes import letterdirectory = os.path.join(os.getcwd(), ""source"") # dir we are interested infif = [f for f in os.listdir(directory) if f[-3:] == 'pdf'] # get the PDFsfor i in fif: packet = StringIO.StringIO() can = canvas.Canvas(packet, pagesize=letter) can.rotate(-90) can.save() packet.seek(0) new_pdf = PdfFileReader(packet) fname = os.path.join('source', i) existing_pdf = PdfFileReader(file(fname, ""rb"")) output = PdfFileWriter() nump = existing_pdf.getNumPages() page = existing_pdf.getPage(0) for l in range(nump): output.addPage(existing_pdf.getPage(l)) page.mergePage(new_pdf.getPage(0)) outputStream = file(""out-""+i, ""wb"") output.write(outputStream) outputStream.close() print fName + "" written as"", i",Generate flattened PDF with Python
How to update a node attribute in networx python?," I created my graph, everything looks great so far, but I want to update color of my nodes after creation.My goal is to visualize DFS, I will first show the initial graph and then color nodes step by step as DFS solves the problem.If anyone is interested, sample code is available on Github <code> ",How to set colors for nodes in NetworkX?
How to set colors for nodes in networx python?," I created my graph, everything looks great so far, but I want to update color of my nodes after creation.My goal is to visualize DFS, I will first show the initial graph and then color nodes step by step as DFS solves the problem.If anyone is interested, sample code is available on Github <code> ",How to set colors for nodes in NetworkX?
How to set colors for nodes in networkx python?," I created my graph, everything looks great so far, but I want to update color of my nodes after creation.My goal is to visualize DFS, I will first show the initial graph and then color nodes step by step as DFS solves the problem.If anyone is interested, sample code is available on Github <code> ",How to set colors for nodes in NetworkX?
"pandas Filter function returned a Series, but epected a scalar bool"," I am attempting to use filter on a pandas dataframe to filter out all rows that match a duplicate value(need to remove ALL the rows when there are duplicates, not just the first or last).This is what I have that works in the editor : But when I run my script with this code in it I get the error: TypeError: filter function returned a Series, but expected a scalar boolI am creating the dataframe by concatenating two other frames immediately before trying to apply the filter. <code>  df = df.groupby(""student_id"").filter(lambda x: x.count() == 1)","Pandas Filter function returned a Series, but expected a scalar bool"
"what is ""request"" in Django HttpResponse"," In the Django tutorial for the first app in Django we have And then the urls.py has Now my question is what is the ""request"" parameter passed to the index function, also when the function index is called in urls.py it is not passed and variables it is just called as views.index in the line url(r'^$', views.index, name='index'), <code>  from django.http import HttpResponsedef index(request): return HttpResponse(""Hello, world. You're at the polls index."") from django.conf.urls import urlfrom polls import viewsurlpatterns = [ url(r'^$', views.index, name='index'),]","what is ""request"" in Django view"
delete elements in nested dictionary," I have a nested dictionary and I want to be able to delete an arbitrary key inside of it. The dictionary could look like this: But it could be of arbitrary size. The problem is that the keys should be taken from a ""key list"" looking, for example, like this: key_list could be of arbitrary size and have any of the dictionary's keys in it.Because of the above criteria, I can't just use: because I can't know beforehand which keys that key_list will contain.So, how would a generic code look like that based on the content of key_list, deletes the corresponding item in the dictionary D? <code>  D={'key1':{'key2':{'key3':'value3', 'key4':'value4'}, 'key5':'value5'}} key_list = ['key1', 'key2', 'key4'] del D['key1']['key2']['key4']",Dynamically delete an item from a nested dictionary
Dynamically delete elements in nested dictionary," I have a nested dictionary and I want to be able to delete an arbitrary key inside of it. The dictionary could look like this: But it could be of arbitrary size. The problem is that the keys should be taken from a ""key list"" looking, for example, like this: key_list could be of arbitrary size and have any of the dictionary's keys in it.Because of the above criteria, I can't just use: because I can't know beforehand which keys that key_list will contain.So, how would a generic code look like that based on the content of key_list, deletes the corresponding item in the dictionary D? <code>  D={'key1':{'key2':{'key3':'value3', 'key4':'value4'}, 'key5':'value5'}} key_list = ['key1', 'key2', 'key4'] del D['key1']['key2']['key4']",Dynamically delete an item from a nested dictionary
ctypes in python size with the sys.getsizeof(Var) method vs ctypes.sizeof(Var)," I have a question about variable size in python, I'm using the the Ctypes because i want a 1 byte number, but when I tried to check it's size in python (via sys.getsize) it said it was 80 byte but when i checked with the ctypes (via ctypes.sizeof) it said it was 1 byte only, can someone tell me what is the difference and why is there 2 different sizes? is it because python is using an object or wrapper? and when its sent to c it views the real size? results in <code>  import sysimport ctypesprint(""size in ctypes is : "",ctypes.sizeof(ctypes.c_byte(1)))print(""size in sys is : "",sys.getsizeof(ctypes.c_byte(1))) size in ctypes is : 1size in sys is : 80",ctypes in python size with the `sys.getsizeof(Var)` method vs `ctypes.sizeof(Var)`
Python pandas best way to select all columns starting with X," I have a DataFrame: I want to select values of 1 in columns starting with foo.. Is there a better way to do it other than: Something similar to writing something like: The answer should print out a DataFrame like this: <code>  import pandas as pdimport numpy as npdf = pd.DataFrame({'foo.aa': [1, 2.1, np.nan, 4.7, 5.6, 6.8], 'foo.fighters': [0, 1, np.nan, 0, 0, 0], 'foo.bars': [0, 0, 0, 0, 0, 1], 'bar.baz': [5, 5, 6, 5, 5.6, 6.8], 'foo.fox': [2, 4, 1, 0, 0, 5], 'nas.foo': ['NA', 0, 1, 0, 0, 0], 'foo.manchu': ['NA', 0, 0, 0, 0, 0],}) df2 = df[(df['foo.aa'] == 1)|(df['foo.fighters'] == 1)|(df['foo.bars'] == 1)|(df['foo.fox'] == 1)|(df['foo.manchu'] == 1)] df2= df[df.STARTS_WITH_FOO == 1] bar.baz foo.aa foo.bars foo.fighters foo.fox foo.manchu nas.foo0 5.0 1.0 0 0 2 NA NA1 5.0 2.1 0 1 4 0 02 6.0 NaN 0 NaN 1 0 15 6.8 6.8 1 0 5 0 0[4 rows x 7 columns]",pandas: best way to select all columns whose names start with X
pandas: best way to select all columns starting with X," I have a DataFrame: I want to select values of 1 in columns starting with foo.. Is there a better way to do it other than: Something similar to writing something like: The answer should print out a DataFrame like this: <code>  import pandas as pdimport numpy as npdf = pd.DataFrame({'foo.aa': [1, 2.1, np.nan, 4.7, 5.6, 6.8], 'foo.fighters': [0, 1, np.nan, 0, 0, 0], 'foo.bars': [0, 0, 0, 0, 0, 1], 'bar.baz': [5, 5, 6, 5, 5.6, 6.8], 'foo.fox': [2, 4, 1, 0, 0, 5], 'nas.foo': ['NA', 0, 1, 0, 0, 0], 'foo.manchu': ['NA', 0, 0, 0, 0, 0],}) df2 = df[(df['foo.aa'] == 1)|(df['foo.fighters'] == 1)|(df['foo.bars'] == 1)|(df['foo.fox'] == 1)|(df['foo.manchu'] == 1)] df2= df[df.STARTS_WITH_FOO == 1] bar.baz foo.aa foo.bars foo.fighters foo.fox foo.manchu nas.foo0 5.0 1.0 0 0 2 NA NA1 5.0 2.1 0 1 4 0 02 6.0 NaN 0 NaN 1 0 15 6.8 6.8 1 0 5 0 0[4 rows x 7 columns]",pandas: best way to select all columns whose names start with X
SQLAlchemy: How to get a single object from joining two tables?," DB - SQLAlchemy settingConsider a classic setting of two tables - user and api_key, represented by SQLAlchemy objects as: Other fields omitted for clarity.My QuerySuppose that I want to get the user name and the api key for a specific user id: I get two objects - user and api_key, from which I can fetch user.name and api_key.api_key.The problemI would like to wrap this call with a function, which would return a single objects whose fields would be the union of the user fields and the api_key fields - the same way a SQL join returns a table with the columns of both tables being joined. Is there a wayo to automatically get a object whose fields are the union of the fields of both tables?Wha have I triedI can define a mapper class for each Join operation, but I wonder if the mapping could be done automatically. <code>  class User(Base): __tablename__ = 'user' user_id = Column(String) user_name = Column(String) vioozer_api_key = Column(String, ForeignKey(""api_key.api_key""))class ApiKey(Base): __tablename__ = 'api_key' api_key = Column(String(37), primary_key=True) user, api_key = database.db_session.query(User, ApiKey)\ .join(ApiKey, User.vioozer_api_key==ApiKey.api_key)\ .filter(User.user_id=='user_00000000000000000000000000000000').first()",SQLAlchemy: Getting a single object from joining multiple tables
"Python: Exception argument and ""not inheriting from Exception"""," I have a database access module that I call with a query or a command. It figures out what to do with the database, and tries to do it. But, for instance if the query or command string is pathological, the call to the underlying PGDB module can throw an exception.Some incredibly useful information is returned by PGDB (from PostgreSQL under that), specifically calling out what errors were found in the query or command. This kind of usage of PGDB's various functions retrieves that information: Then when the class returns, having failed, the object contains the message in .error and viola, I can fix my stupidity in the query or command.And this all seems to work just fine -- (in Python 2.2.2, which might change to 2.higher someday... but not now -- and never, ever to 3.whatever)But... I have found this bit of opacity: ""exception doesn't have to be inherited from Exception. Thus plain 'except:' catches all exceptions, not only system. String exceptions are one example of an exception that doesn't inherit from Exception""So here's the question: Why do I care? If an exception is thrown, I want to know why. I don't care where it came from, really, I just want the error message, and I sure don't want Python to come to a crashing halt. That would include if the error came from the string thing or whatever. So except catching everything is good. Or it should be.Is it that the parameter of Exception means, I won't catch an error if it comes from, for instance, the String innards? And that then, the code will halt with an uncaught exception? And that then I'd need a series of catches for every type that ""doesn't inherit from exception"" in order to get the behavior I want? Something like this: ...because that really... kinda sucks.And, if that's the case, is there some other way I can catch all exceptions of all types and grab the error message for them? It seems like this would be something that is highly, highly desirable (and it also seems like a one-liner should solve it, and it should look somewhat like the former example, rather than the latter.)Please, before you answer: Yes, I know Python 2.2.2 is old. No, it's not going to be upgraded soon. This is a production system with several million lines of code; it's stable and we want it to remain that way, based upon, ""not broken, not fixing.""I just need a solid understanding of this part of the exception process beaten into me. All the explanations seem to be making assumptions about what I know that are... optimistic. :)Thanks for any insight. <code>  try: pgdb.dothing(mod.withx)except Exception, e: mod.error = 'pgdb.dothing('+str(type(mod.withx))+str(mod.withx)+') failed with: '+str(e) try: pgdb.dothing(mod.withx)except Exception, e: mod.error = 'pgdb.dothing('+str(type(mod.withx))+str(mod.withx)+') failed with: '+str(e)except: mod.error = 'pgdb.dothing('+str(type(mod.withx))+str(mod.withx)+') failed with: WTF???'",Catching exceptions that don't inherit from Exception
utility of parameter out in numpy functions, What is the utility of the out parameter in certain numpy functions such as cumsum or cumprod or other mathematical functions?If the result is huge in size does it help to use the out parameter to improve computational time or memory efficiency?This thread gives some information about how to use it. But I want to know when should I use it and what could the benefits be? <code> ,Utility of parameter 'out' in numpy functions
How can I simply calculate the rolling/moving variance of a timeseries in python?," I have a simple time series and I am struggling to estimate the variance within a moving window. More specifically, I cannot figure some issues out relating to the way of implementing a sliding window function. For example, when using NumPy and window size = 20: Perhaps I am mistaken somewhere, in this line of thought. Does anyone know a straightforward way to do this?Any help/advice would be most welcome. <code>  def rolling_window(a, window): shape = a.shape[:-1] + (a.shape[-1] - window + 1, window) strides = a.strides + (a.strides[-1],) return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides) rolling_window(data, 20)np.var(rolling_window(data, 20), -1)datavar=np.var(rolling_window(data, 20), -1)",How can I simply calculate the rolling/moving variance of a time series in python?
how to get argparse to read arguments from a file," I would like to know how to use python's argparse module to read arguments both from the command line and possibly from text files. I know of argparse's fromfile_prefix_chars but that's not exactly what I want. I want the behavior, but I don't want the syntax. I want an interface that looks like this: When argparse sees -A, it should stop reading from sys.argv or whatever I give it, and call a function I write that will read somefile.text and return a list of arguments. When the file is exhausted it should resume parsing sys.argv or whatever. It's important that the processing of the arguments in the file happen in order (ie: -foo should be processed, then the arguments in the file, then -bar, so that the arguments in the file may override --foo, and --bar might override what's in the file). Is such a thing possible? Can I write a custom function that pushes new arguments onto argparse's stack, or something to that effect? <code>  $ python myprogram.py --foo 1 -A somefile.txt --bar 2",how to get argparse to read arguments from a file with an option rather than prefix
Installing .net in Python," Really sorry guys! But for the life of me I am nearing the breaking point of going mad trying to installPython for .NET. I read the instructions meticulously below and still can't seem to get the installation to work. I am running python on an Spyder IDE, I a have windows 7. I downloaded the files pythonnet-2.0-Beta0-clr4.0_140_py27_UCS2_x86. I found my python directory by using the following commands: I dragged all the unziped files into the directory and tried to load CLR But I keep getting this error message. What am I doing wrong? Did a miss a step somewhere?Here are the instructions I have been following:How to install Python for .NET on Windowsandhttp://pythonnet.github.io/ <code>  >>> import os>>> os.getcwd()'C:\\Users\\Jessica' >>> import clr as ccllrrUnhandled Exception: System.BadImageFormatException: Could not load file or assembly 'clr.pyd' or one of its dependencies. This assembly is built by a runtime newer than the currently loaded runtime and cannot be loaded.File name: 'clr.pyd'",Python for .NET installation: Could not load file or assembly 'clr.pyd'
Private variables in Python during debugging," Well, I placed a breakpoint at a = 0.When I stop at the breakpoint, I want to evaluate __whiteList.The error is: Well, this is a mystery to me. Because I have the getter method and outside the class it works perfectly.Well, you may tell me that I could easily take no notice of it as it works outside the class. But I need it during the debugging. Could you comment on why I can't catch the value at the breakpoint? <code>  class Controller: def __init__(self): self.__whiteList = self.readFile('whiteList.txt') a = 0 # Breakpoint def getWhiteList(self): return self.__whiteList AttributeError:'Controller' object has no attribute '__whiteList'",Private variables during debugging
Changin fontsize in python subplots," I have made a phase plot of a bistable stable, with the nulclines on the main graph and have added a subplot with the trajectories overlaying it. However, no matter what I try, I cannot seem to get the x and y labels to increase in font size to 20.Any help would be greatly appreciated.Although there are similar questions, the answers to said queries don't seem to apply to this particular problem.Thanks again! <code>  import numpy as npfrom scipy.integrate import odeintimport matplotlib.pyplot as pltfrom mpl_toolkits.axes_grid.axislines import SubplotZerofrom matplotlib import pylabfrom pylab import linspacefrom numpy import meshgridfrom numpy import hypota1 = 1.0 #(Rate constant)g1 = 4.0 # Hill number for cdc2b1 = 200.0 # Rate Constantk1 = 30.0 #Michaelis Constantv =1 #coefficient that reflects the strangth of the influence of Wee1 on Cdc2a2 = 1.0# Rate Constantg2 = 4.0 #Hill number for Wee1b2 = 10.0 # Rate Constantk2 = 1.0# Michaelis constant# Function for calculating the phase plotdef Function(u,t=0,mu=.1): x1 = u[0] y1 = u[1] dv = (a2* (1.0 - y1) - (b2 * y1 * x1**g2) /(k2 + (x1**g2))) # Model of Cdc2 dx = (a1* (1.0 - x1) - (b1 * x1 * ((v * y1)**g1)) / (k1 + ((v*y1) **g1))) # Model of Wee1 return (dx,dv)t = linspace(0,1,1) #Return list from 0 to 1 in 25 intervalsu0 = np.array([1,1]) # Creates array for odeint function mu = [1,10] #call mu for 2for m in mu:#Get u (differentiation function ) u = odeint(Function,u0,t,args=(m,))# ax.plot(u[0:,0],u[0:,1])x = linspace(0,1,17) #Creates values for xy = linspace(0,1,18)#Creates values for y to plotx,y = meshgrid(x,y)# creates a grid of x by yX,Y = Function([x,y])# Applies funciton to gridM = (hypot(X,Y))# Get hypotenuse of X by YX,Y = X/M, Y/M# Calculate length(strength) of arrows#Calculate Nulclines-----------------------------------------------------------Nulclinevalues = np.arange(0, 1+0.001, 0.001)#Calulate values to set nulcinetoNulclineXX = []# set to an arrayNulclineYY = []#set to an array # Following 2 formulas show the calculation fo the nullclinesdef calcnulclineyy(xx1): oa2 = 1.0#RAte constant og2 = 4.0 #Hill number for Wee1 ob2 = 10.0#Rate constant ok2 = 1.0#Michaelis constant YY = (oa2*((xx1)**og2) + ok2) / (oa2*((xx1**og2)+ok2)+(ob2*(xx1**og2)))return YYdef calcnulclinexx(yy1): oa1 = 1.0 #Rate constant og1 = 4.0 # Hill number for cdc2 ob1 = 200.0 #Rate constant ok1 = 30.0#Michaelis constant ov = 1##coefficient that reflects the strength of the influence of Wee1 on Cdc2 og2 = 4.0 #Hill number for Wee1 XX = (oa1*(ok1+(ov*yy1)**og2)) / (oa1*(ok1+(ov*yy1)**og1)+ob1*(ov*yy1)**og1)return XXfor YY in Nulclinevalues: # print Y NulclineXX.append(calcnulclinexx(YY))for XX in Nulclinevalues: #Print X NulclineYY.append(calcnulclineyy(XX))fig = plt.figure(figsize=(6,6)) # 6x6 imageax = SubplotZero(fig,111,) #Plot arrows over figurefig.add_subplot(ax) # Plot arrows over figure# Plot both nulcines on same graphplt.axis((0,1,0,1))ax.set_title('v = 1',fontweight=""bold"", size=20) # Titleax.set_ylabel('Active Wee1', fontsize = 20.0) # Y labelax.set_xlabel('Active Cdc2-cyclin B', fontsize = 20) # X labelplt.plot (NulclineXX,Nulclinevalues, label = "" Cdc2 nulcline"",c = 'r', linewidth = '2')plt.plot (Nulclinevalues,NulclineYY, label = ""Wee1 nulcline"",c = '#FF8C00', linewidth = '2')ax.quiver(x,y,X,Y,M) # plot quiver plot on graphax.grid(True) # Show major ticks ax.legend(handletextpad=0,loc='upper right') # Plot legendplt.show() # Show plot",Changing fontsize in python subplots
scrapy: exceptions.AttributeError: 'unicode' object has no attribute 'dont_filter'," In scrapy, I am getting the error exceptions.AttributeError: 'unicode' object has no attribute 'dont_filter'. After searching around, I found this answer (which made sense as it was the only bit of code I modified before getting the error) according to which I modified my code. I changed start_request to yield values in the list instead of retruning it whole but I'm still getting it. Any ideas? I have checked the other parts of the code to affirm that everything else is fine.Traceback: <code>  def start_requests(self): connection = pymongo.Connection(settings['MONGODB_SERVER'], settings['MONGODB_PORT']) db = connection[settings['MONGODB_DB']] collection = db[settings['MONGODB_COLLECTION']] for el in [i['url'] for i in collection.find({}, {'_id':0, 'url':1})]: yield el [-] Unhandled Error Traceback (most recent call last): File ""/home/myName/scrapy-test/venv/local/lib/python2.7/site-packages/scrapy/crawler.py"", line 93, in start self.start_reactor() File ""/home/myName/scrapy-test/venv/local/lib/python2.7/site-packages/scrapy/crawler.py"", line 130, in start_reactor reactor.run(installSignalHandlers=False) # blocking call File ""/home/myName/scrapy-test/venv/local/lib/python2.7/site-packages/twisted/internet/base.py"", line 1192, in run self.mainLoop() File ""/home/myName/scrapy-test/venv/local/lib/python2.7/site-packages/twisted/internet/base.py"", line 1201, in mainLoop self.runUntilCurrent() --- <exception caught here> --- File ""/home/myName/scrapy-test/venv/local/lib/python2.7/site-packages/twisted/internet/base.py"", line 824, in runUntilCurrent call.func(*call.args, **call.kw) File ""/home/myName/scrapy-test/venv/local/lib/python2.7/site-packages/scrapy/utils/reactor.py"", line 41, in __call__ return self._func(*self._a, **self._kw) File ""/home/myName/scrapy-test/venv/local/lib/python2.7/site-packages/scrapy/core/engine.py"", line 120, in _next_request self.crawl(request, spider) File ""/home/myName/scrapy-test/venv/local/lib/python2.7/site-packages/scrapy/core/engine.py"", line 176, in crawl self.schedule(request, spider) File ""/home/myName/scrapy-test/venv/local/lib/python2.7/site-packages/scrapy/core/engine.py"", line 182, in schedule return self.slot.scheduler.enqueue_request(request) File ""/home/myName/scrapy-test/venv/local/lib/python2.7/site-packages/scrapy/core/scheduler.py"", line 48, in enqueue_request if not request.dont_filter and self.df.request_seen(request): exceptions.AttributeError: 'unicode' object has no attribute 'dont_filter'",Exceptions.AttributeError: 'unicode' object has no attribute 'dont_filter'
Need urgent help .... extracting image from video at a given time using opencv," My task is to make a utility that can take a video and time in seconds.The utility should write out jpeg images from the video with the given input.E.g. let the video name be abc.mpeg and time be supplied to the tool as 20 seconds. The utility should write out image from video @ 20th second. The above code gives all frames of the entire video, my concern is how can I pass time and get the frame at the specified time? <code>  # Import the necessary packages import argparse import cv2 vidcap = cv2.VideoCapture('Wildlife.mp4') success,image = vidcap.read() count = 0; while success: success,image = vidcap.read() cv2.imwrite(""frame%d.jpg"" % count, image) # save frame as JPEG file if cv2.waitKey(10) == 27: # exit if Escape is hit break count += 1",Extracting image from video at a given time using OpenCV
xmltodict - handle 1 to n elements," I'm using xmltodict to parse an XML config. The XML has structures where an element can occur in 1 to n instances, where both are valid: and I'm parsing this with xmltodict as follows: and it gives back a single unicode or a list (depending the items found), so I always need to add an extra check to ensure if I need to handle a list or a string: Is there a better/simpler/more elegant way to handle these? <code>  <items> <item-ref>abc</item-ref></items> <items> <item-ref>abc</item-ref> <item-ref>dca</item-ref> <item-ref>abb</item-ref></items> document['items']['item-ref'] if isinstance(document['items']['item-ref'], list): my_var = document['items']['item-ref']else: my_var = [document['items']['item-ref']] #create list manually",Handle 1 to n elements
TypeError: split() takes no keyword arguments," I am trying to separate a section of a document into its different components which are separated by ampersands. This is what I have: Error: Can someone explain the error to me and also provide an alternate method for me to make this work? <code>  name,function,range,w,h,k,frac,constraint = str.split(str=""&"", num=8) TypeError: split() takes no keyword arguments",TypeError: split() takes no keyword arguments in Python 2.x
PyCharm and debugging Python code," I use PyCharm Community Edition 3.4.I have added self.__a to Watches.This is my example: So, I start debugging and stop at the breakpoint. The self.__a watch shows {AttributeError}'Box' object has no attribute 'a'. I press Alt + F8 and evaluate self.__a = a. The result is None.Then I evaluate self.__a and the result is 1. My watch for self.__a still shows {AttributeError}'Box' object has no attribute 'a'. I delete it. Then I add another watch self.__a. It shows 1.Could you clarify what is going on here? <code>  class Box: def __init__(self, a, b, c): self.__a = a self._b = b self.c = c d = 0 #Breakpoint.a = Box(1, 2, 3)",PyCharm and debugging private attributes
how to change the location of the pointer in python?," I want to paint some special words while the program is getting them , actually in real-time .so I've wrote this piece of code which do it quite good but i still have problem with changing the location of the pointer with move keys on keyboard and start typing from where i moved it .can anyone give me a hint how to do it ?here is the CODE : <code>  from colorama import initfrom colorama import Foreimport sysimport msvcrtspecial_words = ['test' , 'foo' , 'bar', 'Ham']my_text = ''init( autoreset = True)while True: c = msvcrt.getch() if ord(c) == ord('\r'): # newline, stop break elif ord(c) == ord('\b') : sys.stdout.write('\b') sys.stdout.write(' ') my_text = my_text[:-1] #CURSOR_UP_ONE = '\x1b[1A' #ERASE_LINE = '\x1b[2K' #print ERASE_LINE, elif ord(c) == 224 : set (-1, 1) else: my_text += c sys.stdout.write(""\r"") # move to the line beginning for j, word in enumerate(my_text.split()): if word in special_words: sys.stdout.write(Fore.GREEN+ word) else: sys.stdout.write(Fore.RESET + word) if j != len(my_text.split())-1: sys.stdout.write(' ') else: for i in range(0, len(my_text) - my_text.rfind(word) - len(word)): sys.stdout.write(' ') sys.stdout.flush()",How to change the location of the pointer in python?
"pika, stop_cosuming does not work"," I'm new to rabbitmq and pika, and is having trouble with stopping consuming.channel and queue setting: Basically, consumer and producer are like this:consumer: producer: consumer task gave me output: However, ""finish"" didn't get printed, so I'm guessing it's because channel.stop_consuming(task_id) didn't stop consuming. If so, what is the correct way to do? Thank you. <code>  connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))channel = connection.channel()channel.queue_declare(queue=new_task_id, durable=True, auto_delete=True) def task(task_id): def callback(channel, method, properties, body): if body != ""quit"": print(body) else: print(body) channel.stop_consuming(task_id) channel.basic_consume(callback, queue=task_id, no_ack=True) channel.start_consuming() print(""finish"") return ""finish"" proc = Popen(['app/sample.sh'], shell=True, stdout=PIPE)while proc.returncode is None: # running line = proc.stdout.readline() if line: channel.basic_publish( exchange='', routing_key=self.request.id, body=line ) else: channel.basic_publish( exchange='', routing_key=self.request.id, body=""quit"" ) break # ... output from sample.sh, as expectedquit}q(UstatusqUSUCCESSqU tracebackqNUresultqNUtask_idqU1419350416qUchildrenq]u.","pika, stop_consuming does not work"
How to add third party java jars for use in pyspark," I have some third-party database client libraries in Java. I want to access them through E.g.: to make the client class (not a JDBC driver!) available to the Python client via the Java gateway: It is not clear where to add the third-party libraries to the JVM classpath. I tried to add to file compute-classpath.sh, but that did not seem to work. I get:Py4jError: Trying to call a packageAlso, when comparing to Hive: the hive JAR files are not loaded via file compute-classpath.sh, so that makes me suspicious. There seems to be some other mechanism happening to set up the JVM side classpath. <code>  java_gateway.py java_import(gateway.jvm, ""org.mydatabase.MyDBClient"")",How to add third-party Java JAR files for use in PySpark
pip cffi package installation falied on osx," I am installing cffi package for cryptography and Jasmin installation.I did some research before posting question, so I found following option but which is seems not working:SystemMac OSx 10.9.5python2.7Error Please guide me on following issue.ThanksCommandenv DYLD_LIBRARY_PATH=/usr/local/opt/openssl/lib/ ARCHFLAGS=""-Wno-error=unused-command-line-argument-hard-error-in-future"" LDFLAGS=""-L/usr/local/opt/openssl/lib"" CFLAGS=""-I/usr/local/opt/openssl/include"" sudo -E pip install cffiLOG <code>  c/_cffi_backend.c:13:10: fatal error: 'ffi.h' file not found#include <ffi.h> ^1 warning and 1 error generated. bhushanvaiude$ env DYLD_LIBRARY_PATH=/usr/local/opt/openssl/lib/ ARCHFLAGS=""-Wno-error=unused-command-line-argument-hard-error-in-future"" LDFLAGS=""-L/usr/local/opt/openssl/lib"" CFLAGS=""-I/usr/local/opt/openssl/include"" sudo -E pip install cffiPassword:Downloading/unpacking cffi Downloading cffi-0.8.6.tar.gz (196kB): 196kB downloaded Running setup.py egg_info for package cffi warning: unknown warning option '-Werror=unused-command-line-argument-hard-error-in-future'; did you mean '-Werror=unused-command-line-argument'? [-Wunknown-warning-option] 1 warning generated. Downloading/unpacking pycparser (from cffi) Downloading pycparser-2.10.tar.gz (206kB): 206kB downloaded Running setup.py egg_info for package pycparser Installing collected packages: cffi, pycparser Running setup.py install for cffi warning: unknown warning option '-Werror=unused-command-line-argument-hard-error-in-future'; did you mean '-Werror=unused-command-line-argument'? [-Wunknown-warning-option] 1 warning generated. building '_cffi_backend' extension cc -DNDEBUG -g -fwrapv -Os -Wall -Wstrict-prototypes -I/usr/local/opt/openssl/include -Qunused-arguments -pipe -Wno-error=unused-command-line-argument-hard-error-in-future -DUSE__THREAD -I@@HOMEBREW_CELLAR@@/libffi/3.0.13/lib/libffi-3.0.13/include -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c c/_cffi_backend.c -o build/temp.macosx-10.9-intel-2.7/c/_cffi_backend.o warning: unknown warning option '-Werror=unused-command-line-argument-hard-error-in-future'; did you mean '-Werror=unused-command-line-argument'? [-Wunknown-warning-option] c/_cffi_backend.c:13:10: fatal error: 'ffi.h' file not found #include <ffi.h> ^ 1 warning and 1 error generated. error: command 'cc' failed with exit status 1 Complete output from command /Users/****project path***/bin/python -c ""import setuptools;__file__='/Users/****project path***/build/cffi/setup.py';exec(compile(open(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /var/folders/7w/8z_mn3g120n34bv0w780gnd00000gn/T/pip-e6d6Ay-record/install-record.txt --single-version-externally-managed --install-headers /Users/****project path***/include/site/python2.7: warning: unknown warning option '-Werror=unused-command-line-argument-hard-error-in-future'; did you mean '-Werror=unused-command-line-argument'? [-Wunknown-warning-option]1 warning generated.running installrunning buildrunning build_pycreating buildcreating build/lib.macosx-10.9-intel-2.7creating build/lib.macosx-10.9-intel-2.7/cfficopying cffi/__init__.py -> build/lib.macosx-10.9-intel-2.7/cfficopying cffi/api.py -> build/lib.macosx-10.9-intel-2.7/cfficopying cffi/backend_ctypes.py -> build/lib.macosx-10.9-intel-2.7/cfficopying cffi/commontypes.py -> build/lib.macosx-10.9-intel-2.7/cfficopying cffi/cparser.py -> build/lib.macosx-10.9-intel-2.7/cfficopying cffi/ffiplatform.py -> build/lib.macosx-10.9-intel-2.7/cfficopying cffi/gc_weakref.py -> build/lib.macosx-10.9-intel-2.7/cfficopying cffi/lock.py -> build/lib.macosx-10.9-intel-2.7/cfficopying cffi/model.py -> build/lib.macosx-10.9-intel-2.7/cfficopying cffi/vengine_cpy.py -> build/lib.macosx-10.9-intel-2.7/cfficopying cffi/vengine_gen.py -> build/lib.macosx-10.9-intel-2.7/cfficopying cffi/verifier.py -> build/lib.macosx-10.9-intel-2.7/cffirunning build_extbuilding '_cffi_backend' extensioncreating build/temp.macosx-10.9-intel-2.7creating build/temp.macosx-10.9-intel-2.7/ccc -DNDEBUG -g -fwrapv -Os -Wall -Wstrict-prototypes -I/usr/local/opt/openssl/include -Qunused-arguments -pipe -Wno-error=unused-command-line-argument-hard-error-in-future -DUSE__THREAD -I@@HOMEBREW_CELLAR@@/libffi/3.0.13/lib/libffi-3.0.13/include -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c c/_cffi_backend.c -o build/temp.macosx-10.9-intel-2.7/c/_cffi_backend.owarning: unknown warning option '-Werror=unused-command-line-argument-hard-error-in-future'; did you mean '-Werror=unused-command-line-argument'? [-Wunknown-warning-option]c/_cffi_backend.c:13:10: fatal error: 'ffi.h' file not found#include <ffi.h> ^1 warning and 1 error generated.error: command 'cc' failed with exit status 1----------------------------------------Cleaning up...",pip cffi package installation failed on osx
"In python, how do I find the largest integer less than x?"," If x is 2.3, then math.floor(x) returns 2.0, the largest integer smaller than or equal to x (as a float.)How would I get i the largest integer strictly smaller than x (as a integer)?The best I came up with is: Is there a better way?Note, that if x is 2.0 then math.floor(x) returns 2.0 but I need the largest integer smaller than 2.0, which is 1. <code>  i = int(math.ceil(x)-1)",How do I find the largest integer less than x?
How to replace all Negative Numbers in Pandas DataFrame for Zero, I would like to know if there is someway of replacing all DataFrame negative numbers by zeros? <code> ,How to replace negative numbers in Pandas Data Frame by zero
Why is lil_matrix and dok_matrix so slow compared to common dict of dicts?," I want to iteratively build sparse matrices, and noticed that there are two suitable options for this according to the SciPy documentation:LiL matrix: class scipy.sparse.lil_matrix(arg1, shape=None, dtype=None, copy=False)[source] Row-based linked list sparse matrix This is an efficient structure for constructing sparse matrices incrementally.DoK matrix: class scipy.sparse.dok_matrix(arg1, shape=None, dtype=None, copy=False)[source] Dictionary Of Keys based sparse matrix. This is an efficient structure for constructing sparse matrices incrementally.But when I'm running benchmarks comparing to building a dictionary of dictionary of values (which later can be easily converted to sparse matrix), the latter turns out to be about 10-20 times faster than using any of the sparse matrix models: Results: What is it that causes such a overhead for the matrix models, and is there some way to speed it up? Are there use cases where either dok or lil are to prefer over a common dict of dicts? <code>  from scipy.sparse import dok_matrix, lil_matrixfrom timeit import timeitfrom collections import defaultdictdef common_dict(rows, cols): freqs = defaultdict(lambda: defaultdict(int)) for row, col in zip(rows, cols): freqs[row][col] += 1 return freqsdef dok(rows, cols): freqs = dok_matrix((1000,1000)) for row, col in zip(rows, cols): freqs[row,col] += 1 return freqsdef lil(rows, cols): freqs = lil_matrix((1000,1000)) for row, col in zip(rows, cols): freqs[row,col] += 1 return freqsdef benchmark(): cols = range(1000) rows = range(1000) res = timeit(""common_dict({},{})"".format(rows, cols), ""from __main__ import common_dict"", number=100) print(""common_dict: {}"".format(res)) res = timeit(""dok({},{})"".format(rows, cols), ""from __main__ import dok"", number=100) print(""dok: {}"".format(res)) res = timeit(""lil({},{})"".format(rows, cols), ""from __main__ import lil"", number=100) print(""lil: {}"".format(res)) benchmark()common_dict: 0.11778324202168733dok: 2.2927695910912007lil: 1.3541790939634666",Why are lil_matrix and dok_matrix so slow compared to common dict of dicts?
Conditional rendering of optional HTML segment using flask render_template," I am calling render_template from a couple of different places, and I'd like to control whether I render certain HTML segments, depending on where I'm calling from.For example: I intended to use the show_results bool to flag whether the optional segment should be rendered or not. However, I'm missing what wrapper I should have in the optional delimited portion of the HTML code to control whether the segment should be rendered or not. How can I accomplish this? <code>  render_template('index.html', form=form, show_results=1)",Conditional rendering of HTML segment using render_template
how to add placeholder in a text field in tkinter," I have created a login window in tkinter which has two Entry field, first one is Username and second one is Password.code I want a placeholder called ""Username"" in the Entry, but if you click inside the entry box, the text should disappear. <code>  from tkinter import *ui = Tk()e1 = Entry(ui)#i need a placeholder ""Username"" in the above entry fielde1.pack()ui.mainloop()",How to add placeholder to an Entry in tkinter?
how to solve diff. eq. using scipy.integrate.odeint?," I want to solve this differential equations with the given initial conditions: the ans should be y=2*exp(2*x)-x*exp(-x)here is my code: but what I get is different from the answer.what have I done wrong? <code>  (3x-1)y''-(3x+2)y'+(6x-8)y=0, y(0)=2, y'(0)=3 def g(y,x): y0 = y[0] y1 = y[1] y2 = (6*x-8)*y0/(3*x-1)+(3*x+2)*y1/(3*x-1) return [y1,y2]init = [2.0, 3.0]x=np.linspace(-2,2,100)sol=spi.odeint(g,init,x)plt.plot(x,sol[:,0])plt.show()",How to solve differential equation using Python builtin function odeint?
Couldn't create working venv on Python 3.4.2," I installed Python 3.4.2 and Virtualenv 12.0.5 in my Linux Mint 17.1Then I tried creating:$ virtualenv venvAnd also using --clear and/or -p /usr/bin/python3.4, always getting the messages:Using base prefix '/usr'New python executable in venv/bin/python3Also creating executable in venv/bin/pythonERROR: The executable venv/bin/python3 could not be run: [Errno 13] Permission deniedAnother try was:$ pyvenv-3.4 venvIt gave no errors on creation, but in the venv/bin file the python3.4 is a symbolic link to /usr/local/bin/python3.4. Then when I activate and install any lib using pip or pip3, then try to import it, I get the error:Traceback (most recent call last): File ""<stdin>"", line 1, in <module>ImportError: No module named 'anymoduledownloaded'I always used virtualenv in Python 2.X and never got this kind of errors. Any thoughts on what am I doing wrong?Thanks!!=======EDITED=======This is the output of my partitions (fdisk -l): And also my fstab: <code>  Device Boot Start End Blocks Id System/dev/sda1 2048 98707455 49352704 83 Linux/dev/sda2 303507456 3890644991 1793568768 5 Extended/dev/sda3 * 98707456 303507455 102400000 7 HPFS/NTFS/exFAT/dev/sda4 3890644992 3907028991 8192000 82 Linux swap / Solaris/dev/sda5 303509504 3890644991 1793567744 7 HPFS/NTFS/exFAT` <file system> <mount point> <type> <options> <dump> <pass>-> was on /dev/sda1 during installationUUID=a38f9c6d-3cd9-4486-b896-acbc6182ec61 / ext4 errors=remount-ro 0 1-> swap was on /dev/sda4 during installationUUID=efad7b53-79a8-4230-8226-9ca90c68ea9d none swap sw 0 0`",Couldn't create working virtual environment for Python 3.4
Python Terminal Overwrite Many Lines," I want to overwrite the hello. But when a \n was printed i can't come back to that line.So what do applications do which overwrite many lines like the program htop. <code>  import sysprint 'hello'print 'huhu',print '\r\r\rnooooo\r'",Terminal - How to overwrite many lines?
Easiest and quickest way to override a Django model field from a parent class," so i have seen a lot of solutions but i found most answers for older versions of Django and i was wondering if there are any changes on that.Basically i am using Django 1.7, and i wanted to override the uusername field in the default django USER class to allow more than 30 chars i.e max_length=75.Should i be using AbstractBaseUser instead or is there an easier way to put max_length=75 <code> ",Easiest and quickest way to increase Django's default username max length from 30 to 75
Removing first line of Big CSV file in Python v3," How should I remove the first line of a big CSV file in python?I looked into the previous solutions in here, one was: which gave me this error: the other solution was: Which brings memory issue! <code>  with open(""test.csv"",'r') as f: with open(""updated_test.csv"",'w') as f1: f.next() # skip header line for line in f: f1.write(line) f.next() # skip header lineAttributeError: '_io.TextIOWrapper' object has no attribute 'next' with open('file.txt', 'r') as fin: data = fin.read().splitlines(True)with open('file.txt', 'w') as fout: fout.writelines(data[1:])",Removing first line of Big CSV file?
Convert BNF grammar to regex for simple script," How can I describe a grammar using regex (or pyparsing is better?) for a script languge presented below (BackusNaur Form): Example of the script: I use python re.compile and want to divide everything in groups, something like this: Updated:I've found out that pyparsing is much better solution instead of regex.  <code>  <root> := <tree> | <leaves><tree> := <group> [* <group>] <group> := ""{"" <leaves> ""}"" | <leaf>;<leaves> := {<leaf>;} leaf<leaf> := <name> = <expression>{;}<name> := <string_without_spaces_and_tabs><expression> := <string_without_spaces_and_tabs> { stage = 3; some.param1 = [10, 20];} *{ stage = 4; param3 = [100,150,200,250,300]} * endparam = [0, 1] [ [ 'stage', '3'], [ 'some.param1', '[10, 20]'] ],[ ['stage', '4'], ['param3', '[100,150,200,250,300]'] ],[ ['endparam', '[0, 1]'] ]",Convert BNF grammar to pyparsing
Installing latest python-daemon (2.0.3) breaks subsequent pip installs," I ran into a problem when installing a package which depended on python-daemon. I ultimately traced it to the latest version of the package python-daemon (2.0.3) released yesterday. Testing in a virtual environment on an Ubuntu 14.04 machine and issuing the following commands: So the install of python-daemon seemed to work but something affected pip or setuptools because other packages (celery, flask), I try to install with pip after this gives me the same traceback: If I uninstall python-daemon with pip things again and packages that weren't installing now install fine. Has anyone else come across this or something similar with a different project? My solution was to pip install the previous version but was wondering what might be causing such an error. <code>  (venv) $ pip listargparse (1.2.1)pip (1.5.6)setuptools (3.6)wsgiref (0.1.2)(venv) $ pip install redis ... works fine ....(venv) $ pip install python-daemon ... snip ... File ""/home/pwj/.virtualenvs/venv/local/lib/python2.7/site-packages/pkg_resources.py"", line 2147, in load['__name__'])ImportError: No module named version(venv)02:15 PM tmp$ pip listargparse (1.2.1)lockfile (0.10.2)pip (1.5.6)python-daemon (2.0.3)setuptools (3.6)wsgiref (0.1.2) ... snip ... File ""/home/pwj/.virtualenvs/venv/local/lib/python2.7/site-packages/pkg_resources.py"", line 2147, in load['__name__'])ImportError: No module named version (venv) $ pip install python-daemon==2.0.2... works ...",How to declare build-time dependencies without breaking other packages?
Python- Can I use += on multiple variables on one line?," While shortening my code I was cutting down a few variable declarations onto one line- However, when I tried doing the same thing to this code- This throws the error I have read the relevant Python documentation, but I still can't find a way to shorten this particular bit of code. <code>  ##For example- going from-Var1 =15Var2 = 26Var3 = 922##To-Var1, Var2, Var3 = 15, 26, 922 User_Input += Master_Key[Input_ref]Key += Master_Key[Key_ref]Key2 += Master_Key[Key_2_Ref]##Which looks like-User_Input, Key, Key2 += Master_Key[Input_Ref], Master_Key[Key_Ref], Master_Key[Key_2_Ref] SyntaxError: illegal expression for augmented assignment",Can I use += on multiple variables on one line?
Flask unit test: set session['variable']," I am trying to find out how to run a test on a function which grabs a variable value from session['user_id']. This is the specific test method: This is the view being tested: This is the entire test file: The following error is shown when running the test: Why am I getting this error and how do I fix it? <code>  def test_myProfile_page(self): with app.test_client() as c: with c.session_transaction() as sess: sess['user_id'] = '1' rv = c.get('/myProfile') assert 'My Profile' in rv.data @app.route('/myProfile')def myProfile(): if not session.get('logged_in'): return render_template('login.html') else: profileID = session['user_id'] userList = users.query.filter_by(id=profileID).all() flash('My Profile') return render_template('myProfile.html', userList=userList) import osimport appimport unittestimport tempfileclass AppTestCase(unittest.TestCase): def setUp(self): self.db_fd, app.app.config['DATABASE'] = tempfile.mkstemp() app.app.config['TESTING'] = True self.app = app.app.test_client() def tearDown(self): os.close(self.db_fd) os.unlink(app.app.config['DATABASE']) def test_profile_page(self): rv = self.app.get('/profile1') assert 'Profile' in rv.data def login(self, username, password): return self.app.post('/login', data=dict( username=username, password=password ), follow_redirects=True) def logout(self): return self.app.get('/logout', follow_redirects=True) def test_login_logout(self): rv = self.login('Alex', 'passwordAlex') assert 'Welcome' in rv.data rv = self.logout() assert 'You have been logged out' in rv.data rv = self.login('Alex', 'noPassword') assert 'You have to Login' in rv.data rv = self.login('WrongName', 'passwordAlex') assert 'You have to Login' in rv.data def test_myProfile_page(self): with app.test_client() as c: with c.session_transaction() as sess: sess['user_id'] = '1' rv = c.get('/myProfile') assert 'My Profile' in rv.dataif __name__ == '__main__': unittest.main() ERROR: test_myProfile_page (__main__.AppTestCase)----------------------------------------------------------------------Traceback (most recent call last): File ""app_tests.py"", line 46, in test_myProfile_page with app.test_client() as c:AttributeError: 'module' object has no attribute 'test_client'----------------------------------------------------------------------Ran 3 tests in 0.165sFAILED (errors=1)",Can't create test client during unit test of Flask app
How to get the scrapy form working," I tried to use scrapy to complete the login and collect my project commit count. And here is the code. After running the code It should show me the result page of the form, which should be a failed login in the same page as the username and password is fake. However it shows me the search page. The log file is located in pastebinHow should the code be fixed? Thanks in advance. <code>  from scrapy.item import Item, Fieldfrom scrapy.http import FormRequestfrom scrapy.spider import Spiderfrom scrapy.utils.response import open_in_browserclass GitSpider(Spider): name = ""github"" allowed_domains = [""github.com""] start_urls = [""https://www.github.com/login""] def parse(self, response): formdata = {'login': 'username', 'password': 'password' } yield FormRequest.from_response(response, formdata=formdata, clickdata={'name': 'commit'}, callback=self.parse1) def parse1(self, response): open_in_browser(response) scrapy runspider github.py",How to submit a form in scrapy?
How to get the scrapy form submission working," I tried to use scrapy to complete the login and collect my project commit count. And here is the code. After running the code It should show me the result page of the form, which should be a failed login in the same page as the username and password is fake. However it shows me the search page. The log file is located in pastebinHow should the code be fixed? Thanks in advance. <code>  from scrapy.item import Item, Fieldfrom scrapy.http import FormRequestfrom scrapy.spider import Spiderfrom scrapy.utils.response import open_in_browserclass GitSpider(Spider): name = ""github"" allowed_domains = [""github.com""] start_urls = [""https://www.github.com/login""] def parse(self, response): formdata = {'login': 'username', 'password': 'password' } yield FormRequest.from_response(response, formdata=formdata, clickdata={'name': 'commit'}, callback=self.parse1) def parse1(self, response): open_in_browser(response) scrapy runspider github.py",How to submit a form in scrapy?
Random state (Pseudo-random number)in Scikit learn," I want to implement a machine learning algorithm in scikit learn, but I don't understand what this parameter random_state does? Why should I use it? I also could not understand what is a Pseudo-random number.  <code> ",Random state (Pseudo-random number) in Scikit learn
Python with scikit-image to invert colors Back and white," I read an image with ndimage, which results in a binary image like this:I would like to invert the image such that white turns into black, and vice versa.Help is appreciated. <code> ",How to invert black and white with scikit-image?
from ... import * in a function not allowed, From the documentation: The wild card form of import from module import * is only allowed at the module level. Attempting to use it in class or function definitions will raise a SyntaxError.Why? What's the sense of avoiding to use it in a function? What's the problem? <code> ,"Why is ""from ... import *"" in a function not allowed?"
Pandas + scikit-learn K-means not," I am currently trying to do some k-means clustering using my data which is stored in my pandas.dataframe (actually in one of its columns). Odd thing is that instead of treating each row as a separate example it threats all rows as one example but in very high dimension. So for example: then I just check that it looks fine: Looks ok, 891 float64 number. I do custering: And I check for cluster centers: It returns me one giant array.Furthermore: Gives me: What am I doing wrong? Why it thinks I have a one example with 891 dimensions, instead of having 891 example?Just to illustrate it better, if I try 2 clusters: Traceback (most recent call last): File """", line 1, in k_means.fit(df.AgeFill) File ""D:\Apps\Python\lib\site-packages\sklearn\cluster\k_means_.py"", line 724, in fit X = self._check_fit_data(X) File ""D:\Apps\Python\lib\site-packages\sklearn\cluster\k_means_.py"", line 693, in _check_fit_data X.shape[0], self.n_clusters))ValueError: n_samples=1 should be >= n_clusters=2So you could see that it REALLY thinks that it is just one giant sample.But: <code>  df = pd.read_csv('D:\\Apps\\DataSciense\\Kaggle Challenges\\Titanic\\Source Data\\train.csv', header = 0)median_ages = np.zeros((2,3))for i in range(0,2): for j in range (0,3): median_ages[i, j] =df[(df.Gender == i) &(df.Pclass == j+1)].Age.dropna().median()df['AgeFill'] = df['Age']for i in range(0, 2): for j in range(0,3): df.loc[ (df.Age.isnull()) & (df.Gender == i) & (df.Pclass == j+1), 'AgeFill'] = median_ages[i, j] df.AgeFillName: AgeFill, Length: 891, dtype: float64 k_means = cluster.KMeans(n_clusters=1, init='random')k_means.fit(df.AgeFill) k_means.cluster_centers_ k_means.labels_ array([0]) k_means = cluster.KMeans(n_clusters=2, init='random')k_means.fit(df.AgeFill) df.AgeFill.shape(891,)",Pandas + scikit-learn K-means not working properly - treats all of dataframe rows as one big multi-dimensional example
Setup Spyder to work with Anaconda python," I installed Spyder IDE before I have installed the Anaconda (on Ubuntu 14.04), on the terminal when I type python I have the anaconda python and I can import all of its packages. But on Spyder I still have the original python and I cannot import the Anaconda packages, how can I setup Spyder to use the Anaconda python? <code> ",Setup Spyder to work with Anaconda python on Ubuntu 14.04
python how to safely handle an exception inside a context manager," I think I've read that exceptions inside a with do not allow __exit__ to be call correctly. If I am wrong on this note, pardon my ignorance.So I have some pseudo code here, my goal is to use a lock context that upon __enter__ logs a start datetime and returns a lock id, and upon __exit__ records an end datetime and releases the lock: How can I still raise errors in addition to existing the context safely?Note: I intentionally raise the base exception in this pseudo-code as I want to exit safely upon any exception, not just expected exceptions.Note: Alternative/standard concurrency prevention methods are irrelevant, I want to apply this knowledge to any general context management. I do not know if different contexts have different quirks.PS. Is the finally block relevant? <code>  def main(): raise Exceptionwith cron.lock() as lockid: print('Got lock: %i' % lockid) main()",How to safely handle an exception inside a context manager
Restrict access to file download in django," I have multiple FileFields in my django app, which can belong to different users.I am looking for a good way to restrict access to files for user who aren't the owner of the file.What is the best way to achieve this? Any ideas? <code> ",Restricting access to private file downloads in Django
"Why does pip say ""No module names commands.install""?"," I have installed pip, and I have moved my directory in cmd to C:\Python27\Scripts. I try this code:'pip install whatever.whl'It says at the bottom of random code(looks like python):'ImportError: No module named commands.install'What is happening? <code> ","Why does pip say ""No module named commands.install""?"
Determine if an attribute is `DeferredAttribute` in django," The ContextI have located a rather critical bug in Django Cache Machine that causes it's invalidation logic to lose its mind after a upgrading from Django 1.4 to 1.7. The bug is localized to invocations of only() on models that extend cache machine's CachingMixin. It results in deep recursions that occasionally bust the stack, but otherwise create huge flush_lists that cache machine uses for bi-directional invalidation for models in ForeignKey relationships. The BugThe bug occurs in the following lines(https://github.com/jbalogh/django-cache-machine/blob/f827f05b195ad3fc1b0111131669471d843d631f/caching/base.py#L253-L254). In this case self is a instance of MyModel with a mix of deferred and undeferred attributes: Cache Machine does bidirectional invalidation across ForeignKey relationships. It does this by looping over all the fields in a Model and storing a series of pointers in cache that point to objects that need invalidated when the object in question is invalidated.The use of only() in the Django ORM does some meta programming magic that overrides the unfetched attributes with Django's DeferredAttribute implementation. Under normal circumstances an access to favorite_color would invoke DeferredAttribute.__get__(https://github.com/django/django/blob/18f3e79b13947de0bda7c985916d5a04e28936dc/django/db/models/query_utils.py#L121-L146) and fetch the attribute either from the result cache or the data source. It does this by fetching the undeferred representation of the Model in question and calling another only() query on it. This is the problem when looping over the foreign keys in the Model and accessing their values, Cachine Machine introduces an unintentional recursion. getattr(self, f.attname) on an attribute that is deferred induces a fetch of a Model that has the CachingMixin applied and has deferred attributes. This starts the whole caching process over again.The QuestionI would like to open a PR to fix this and I believe the answer to this is as simple as skipping over the deferred attributes, but I'm not sure how to do it because accessing the attribute causes the fetch process to start.If all I have is a handle on an instance of a Model with a mix of deferred and undeferred attributes, Is there a way to determine if an attribute is a DeferredAttribute without accessing it? <code>  class MyModel(CachingMixin): id = models.CharField(max_length=50, blank=True) nickname = models.CharField(max_length=50, blank=True) favorite_color = models.CharField(max_length=50, blank=True) content_owner = models.ForeignKey(OtherModel) m = MyModel.objects.only('id').all() fks = dict((f, getattr(self, f.attname)) for f in self._meta.fields if isinstance(f, models.ForeignKey)) fks = dict((f, getattr(self, f.attname)) for f in self._meta.fields if (isinstance(f, models.ForeignKey) and <f's value isn't a Deferred attribute))",Determine if an attribute is a `DeferredAttribute` in django
Is there a recursive version of Python's dict.get() built-in?," I have a nested dictionary object and I want to be able to retrieve values of keys with an arbitrary depth. I'm able to do this by subclassing dict: However, I don't want to have to subclass dict to get this behavior. Is there some built-in method that has equivalent or similar behavior? If not, are there any standard or external modules that provide this behavior?I'm using Python 2.7 at the moment, though I would be curious to hear about 3.x solutions as well. <code>  >>> class MyDict(dict):... def recursive_get(self, *args, **kwargs):... default = kwargs.get('default')... cursor = self... for a in args:... if cursor is default: break... cursor = cursor.get(a, default)... return cursor... >>> d = MyDict(foo={'bar': 'baz'})>>> d{'foo': {'bar': 'baz'}}>>> d.get('foo'){'bar': 'baz'}>>> d.recursive_get('foo'){'bar': 'baz'}>>> d.recursive_get('foo', 'bar')'baz'>>> d.recursive_get('bogus key', default='nonexistent key')'nonexistent key'",Is there a recursive version of the dict.get() built-in?
How do I debug efficiently with spyder in Python?," I like Python and I like Spyder but I find debugging with Spyder terrible!Every time I put a break point, I need to press two buttons: firstthe debug and then the continue button (it pauses at first lineautomatically) which is annoying.Moreover, rather than having the standard iPython console with auto completion etc I have a lousy ipdb>> console which is just garbage.The worst thing is that this console freezes very frequently even if I write prints or simple evaluation to try to figure out what is the bug. This is much worse than MATLAB.Last but not least, if I call a function from within theipdb>> console, and put a breakpoint in it, it will not stop there.It seems like I have to put the breakpoint there before I start thedebugging (Ctrl+F5).Do you have a solution or maybe can you tell me how you debug Python scripts and functions?I am using fresh install of Anaconda on a Windows 8.1 64bit. <code> ",How do I debug efficiently with Spyder in Python?
Python 3 and PyInstaller: Single-File Executable Doesn't Run," I'm trying to create a single-file executable for Windows from a Python application, using pyinstaller.I downloaded the experimental Python 3 branch of pyinstaller from here (the file was python3.zip, but the link is now dead). And I installed it using python setup.py install.Then I created a test python script called test.py, with the following content: Afterwards, I ran the following command to create a single-file executable: The command succeeded, and I verified that the file dist/test.exe had been generated. However, when I try to run it, all I get is an empty console window. Nothing ever appears, and the program never terminates. It just hangs there forever, until I force close it.Calling pyinstaller test.py (without the --onefile option) works fine. So what is the problem?Notice that using py2exe or cx_freeze is not an option. It has to be pyinstaller.UPDATE: I just tested it under Python 2 (using the normal PyInstaller version), and I ran into the same problem. So, this is not just a Python 3 problem. <code>  print('Hello, World!') pyinstaller --onefile test.py",PyInstaller: Single-file executable doesn't run
PyInstaller: Single-File Executable Doesn't Run," I'm trying to create a single-file executable for Windows from a Python application, using pyinstaller.I downloaded the experimental Python 3 branch of pyinstaller from here (the file was python3.zip, but the link is now dead). And I installed it using python setup.py install.Then I created a test python script called test.py, with the following content: Afterwards, I ran the following command to create a single-file executable: The command succeeded, and I verified that the file dist/test.exe had been generated. However, when I try to run it, all I get is an empty console window. Nothing ever appears, and the program never terminates. It just hangs there forever, until I force close it.Calling pyinstaller test.py (without the --onefile option) works fine. So what is the problem?Notice that using py2exe or cx_freeze is not an option. It has to be pyinstaller.UPDATE: I just tested it under Python 2 (using the normal PyInstaller version), and I ran into the same problem. So, this is not just a Python 3 problem. <code>  print('Hello, World!') pyinstaller --onefile test.py",PyInstaller: Single-file executable doesn't run
Twitter API - get tweets with specifc id," I have a list of tweet ids for which I would like to download their text content. Is there any easy solution to do this, preferably through a Python script? I had a look at other libraries like Tweepy and things don't appear to work so simple, and downloading them manually is out of the question since my list is very long. <code> ",Twitter API - get tweets with specific id
Django admin: use radio buttons in list view in list_filter()," I have a model Transaction with a ForeignKey to another model (TransactionState) on state field. So in admin.py I have: In TransactionState I have records like ""paid"", ""unpaid"", ""delivered"", ""canceled"", Etc. and it works fine but I want to be able to filter using checkboxes to allow multiple selection like ""paid"" OR ""delivered"". It's possible? <code>  class TransactionAdmin(admin.ModelAdmin): ... list_filter = ('state') ...",Django admin: use checkboxes in list view in list_filter()
Python | Why is accessing instance attribute is slower than local?," Results from my computer:Accessing Local Attribute: 0.686281020000024Accessing Instance Attribute: 3.7962001440000677Why does this happen? Moreover, is it a good practice to localise the instance variable before using it? <code>  import timeitclass Hello(): def __init__(self): self.x = 5 def get_local_attr(self): x = self.x # 10x10 x;x;x;x;x;x;x;x;x;x; x;x;x;x;x;x;x;x;x;x; x;x;x;x;x;x;x;x;x;x; x;x;x;x;x;x;x;x;x;x; x;x;x;x;x;x;x;x;x;x; x;x;x;x;x;x;x;x;x;x; x;x;x;x;x;x;x;x;x;x; x;x;x;x;x;x;x;x;x;x; x;x;x;x;x;x;x;x;x;x; x;x;x;x;x;x;x;x;x;x; def get_inst_attr(self): # 10x10 self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x; self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x; self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x; self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x; self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x; self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x; self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x; self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x; self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x; self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;self.x;if __name__ == '__main__': obj = Hello() print('Accessing Local Attribute:', min(timeit.Timer(obj.get_local_attr) .repeat(repeat=5))) print('Accessing Instance Attribute:', min(timeit.Timer(obj.get_inst_attr) .repeat(repeat=5)))",Python | Why is accessing instance attribute slower than local?
Jinja2 - Keep undefined variables," I am interested in rendering a template in multiple steps or keeping the tags for the undefined variables in Jinja2. I believe this would mean not only creating the 'UndefinedSilent"" class (so the template won't crash on missing data) but also keeping the tags with the appropriate variable names if they are missing.Example:Let's say we have the name = ""Test"" included in the context, but quantity is missing.Givent the following template: After rendering, I need the template to become: Does anyone know if this is achievable? <code>  <p>{{name}} has {{quantity}}</p> <p>test has {{quantity}}</p>",Keep undefined variables
Python CSV module handling double quotes inside field," I'm trying to parse CSV files from an external system which I have no control of.comma is used as a separatorwhen cell contains comma then it's wrapped in quotes and all other quotes are escaped with another quote character.(my problem) when cell was not wrapped in quotes then all quote characters are escaped with another quote nonetheless.Example CSV: qw""""erty,""a""""b""""c""""d,ef""""""""g""Should be parsed as: However, I think that Python's csv module does not expect quote characters to be escaped when cell was not wrapped in quote chars in the first place.csv.reader(my_file) (with default doublequote=True) returns: Is there any way to parse this with python csv module ? <code>  [['qw""erty', 'a""b""c""d,ef""""g']] ['qw""""erty', 'a""b""c""d,ef""""g']",How to handle double quotes inside field values with csv module?
setup.py checking for non-python dependencies," I'm trying to make a setup.py for cgal-bindings. To install this, the user needs to have at least a certain version of CGAL. In addition, CGAL has a few optional targets that should be built if the user has some libraries (like Eigen3). Is there a cross-platform way in Python to check for this? I can use find_library in ctypes.util to check if the library exists, but I don't see any easy way to get the version. <-- This doesn't actually work all the time, some libraries are header-only like eigen3, which is a C++ template library. Using the install_requires argument of setup() only works for Python libraries and CGAL is a C/C++ library.  <code> ",setup.py check if non-python library dependency exists
setup.py check if non-python library exists," I'm trying to make a setup.py for cgal-bindings. To install this, the user needs to have at least a certain version of CGAL. In addition, CGAL has a few optional targets that should be built if the user has some libraries (like Eigen3). Is there a cross-platform way in Python to check for this? I can use find_library in ctypes.util to check if the library exists, but I don't see any easy way to get the version. <-- This doesn't actually work all the time, some libraries are header-only like eigen3, which is a C++ template library. Using the install_requires argument of setup() only works for Python libraries and CGAL is a C/C++ library.  <code> ",setup.py check if non-python library dependency exists
How to get group name of matche regular expression in Python?," Question is very basic whatever I do not know how to figure out group name from match. Let me explain in code: How to get group name of a[0].group(0) match - assume that number of named patterns can be larger? Example is simplified to learn basics.I can invert match a[0].groupdict() but it will be slow. <code>  import re a = list(re.finditer('(?P<name>[^\W\d_]+)|(?P<number>\d+)', 'Ala ma kota'))",How to get group name of match regular expression in Python?
how to add elements to a numpy array recursively," I want to do the equivalent to adding elements in a python list recursively in Numpy, As in the following code I have tried the following: It does not return the desired output, as in the list. Edit this is the sample code: This is just the simplest case of what I want to do, but there is more data processing in the loop, I am putting it this way so the example is clear. <code>  matrix = open('workfile', 'w')A = []for row in matrix: A.append(row)print A matrix = open('workfile', 'w')A = np.array([])for row in matrix: A = numpy.append(row)print A mat = scipy.io.loadmat('file.mat')var1 = mat['data1']A = np.array([])for row in var1: np.append(A, row)print A",How to append elements to a numpy array
Python Pandas: How to replace a characters in a column of a dataframe?," I have a column in my dataframe like this: and I want to replace the , comma with - dash. I'm currently using this method but nothing is changed. Can anybody help? <code>  range""(2,30)""""(50,290)""""(400,1000)""... org_info_exc['range'].replace(',', '-', inplace=True)",How to replace text in a string column of a Pandas dataframe?
How to replace a characters in a column of a Pandas dataframe?," I have a column in my dataframe like this: and I want to replace the , comma with - dash. I'm currently using this method but nothing is changed. Can anybody help? <code>  range""(2,30)""""(50,290)""""(400,1000)""... org_info_exc['range'].replace(',', '-', inplace=True)",How to replace text in a string column of a Pandas dataframe?
How to replace text in a column of a Pandas dataframe?," I have a column in my dataframe like this: and I want to replace the , comma with - dash. I'm currently using this method but nothing is changed. Can anybody help? <code>  range""(2,30)""""(50,290)""""(400,1000)""... org_info_exc['range'].replace(',', '-', inplace=True)",How to replace text in a string column of a Pandas dataframe?
"Django Rest Framework nested resource key ""id"" unaccessible"," So I have the following Structure:A ClientFile belongs to an Owner (class name = Contact).I'm trying to create a Clientfile using the API. The request contains the following data: I created the serializer according to my structure. Hoping that this API call would create a clientfile with the name ""Hello!"" and Contact id 1 as the owner: I do get into the create method. However, the only field I need (['owner']['id']) is not accessible. If I do print ['owner']['first_name'] it does return 'Charlie'. But the ID for some reasons doesn't seem to be accessible...Any reasons why this can be happening? Am i missing something? (I'm new to Django)SOLUTION: Just found out that the reason why ID didn't show in the first place was because I had to declare it in the fields like so: Hope this helps. <code>  { name: ""Hello!"" owner: { id: 1, first_name: ""Charlie"", last_name: ""Watson"" }} class ContactSerializer(serializers.ModelSerializer): class Meta: model = Contact fields = ( 'id', 'first_name', 'last_name', )class ClientfileSerializer(serializers.ModelSerializer): owner = ContactSerializer(read_only=False) class Meta(): model = Clientfile fields = ( 'id', 'name', 'owner', ) def create(self, validated_data): owner = Contact.objects.get(pk=validated_data['owner']['id']) class ContactSerializer(serializers.ModelSerializer): id = serializers.IntegerField() # Here class Meta: model = Contact fields = ( 'id', 'first_name', 'last_name', )","Django REST Framework nested resource key ""id"" unaccessible"
pyGTK image type error (save pixmap)," How can I save a pixmap as a .png file? I do this: I get this error: <code>  image = gtk.Image() image.set_from_pixmap(disp.pixmap, disp.mask) pixbf=image.get_pixbuf() pixbf.save('path.png') pixbf=image.get_pixbuf() ValueError: image should be a GdkPixbuf or empty",ValueError while trying to save a pixmap as a png file
How to supress pylint logging-not-lazy?," I am using prospector to examine my code. Pylint returned a logging-not-lazy warning about my debug message. My code is: How do I fix logging-not-lazy in pylint? <code>  Line: 31 pylint: logging-not-lazy / Specify string format arguments as logging function parameters (col 16) Line: 42 pylint: logging-not-lazy / Specify string format arguments as logging function parameters (col 12) logging.debug(""detect mimetypes faild because %s"" % e )",How to fix pylint logging-not-lazy?
Python Pandas Use if-else to populate new column," I have a DataFrame like this: I'd like to add a column that is a 1 if col2 is > 0 or 0 otherwise. If I was using R I'd do something like How would I do this in python / pandas? <code>  col1 col2 1 0 0 1 0 0 0 0 3 3 2 0 0 4 df1[,'col3'] <- ifelse(df1$col2 > 0, 1, 0)",pandas: Use if-else to populate new column
Any way to bind click event int Tkinter?," I was just wondering if there was any possible way to bind a click event to a canvas using Tkinter.I would like to be able to click anywhere on a canvas and have an object move to it. I am able to make the motion, but I have not found a way to bind clicking to the canvas. <code> ",How to bind a click event to a Canvas in Tkinter?
Any way to bind click event in Tkinter?," I was just wondering if there was any possible way to bind a click event to a canvas using Tkinter.I would like to be able to click anywhere on a canvas and have an object move to it. I am able to make the motion, but I have not found a way to bind clicking to the canvas. <code> ",How to bind a click event to a Canvas in Tkinter?
Group and Average Numpy Matrix, Say I have an arbitrary numpy matrix that looks like this: What would be an efficient way of averaging rows that are grouped by their third column number?The expected output would be: <code>  arr = [[ 6.0 12.0 1.0] [ 7.0 9.0 1.0] [ 8.0 7.0 1.0] [ 4.0 3.0 2.0] [ 6.0 1.0 2.0] [ 2.0 5.0 2.0] [ 9.0 4.0 3.0] [ 2.0 1.0 4.0] [ 8.0 4.0 4.0] [ 3.0 5.0 4.0]] result = [[ 7.0 9.33 1.0] [ 4.0 3.0 2.0] [ 9.0 4.0 3.0] [ 4.33 3.33 4.0]],Group and average NumPy matrix
Forcing a thread to block all other threads from executing?," UPDATE:This answer states that what I'm trying to do is impossible as of April 2013. This, however, seems to contradict what Alex Martelli says in Python Cookbook (p. 624, 3rd ed.): Upon return, PyGILState_Ensure() always guarantees that the calling thread has exclusive access to the Python interpreter. This is true even if the calling C code is running a different thread that is unknown to the interpreter.The docs also seem to suggest GIL can be acquired, which would give me hope (except I don't think I can call PyGILState_Ensure() from pure python code, and if I create a C extension to call it, I'm not sure how to embed my memory_daemon() in that).Perhaps I'm misreading either the answer or Python Cookbook and the docs.ORIGINAL QUESTION:I want a given thread (from threading module) to prevent any other thread from running while a certain segment of its code is executing. What's the easiest way to achieve it? Obviously, it would be great to minimize code changes in the other threads, to avoid using C and direct OS calls, and to make it cross-platform for windows and linux. But realistically, I'll be happy to just have any solution whatsoever for my actual environment (see below).Environment:CPythonpython 3.4 (but can upgrade to 3.5 if it helps)Ubuntu 14.04Use case:For debugging purposes, I calculate memory used by all the objects (as reported by gc.get_objects()), and print some summary report to sys.stderr. I do this in a separate thread, because I want this summary delivered asynchronously from other threads; I put time.sleep(10) at the end of the while True loop that does the actual memory usage calculation. However, the memory reporting thread takes a while to complete each report, and I don't want all the other threads to move ahead before the memory calculation is finished (otherwise, the memory snapshot will be really hard to interpret).Example (to clarify the question): <code>  import threading as thimport timedef report_memory_consumption(): # go through `gc.get_objects()`, check their size and print a summary # takes ~5 min to rundef memory_daemon(): while True: # all other threads should not do anything until this call is complete report_memory_consumption() # sleep for 10 sec, then update memory summary # this sleep is the only time when other threads should be executed time.sleep(10)def f1(): # do something, including calling many other functions # takes ~3 min to rundef f2(): # do something, including calling many other functions # takes ~3 min to rundef main(): t_mem = th.Thread(target = memory_daemon) t1 = th.Thread(target = f1) t2 = th.Thread(target = f2) t_mem.start() t1.start() t2.start()# requirement: no other thread is running while t_mem is not sleeping",Forcing a thread to block all other threads from executing
Ineffective multiprocessing of numpy-based calculations," I'm trying to parallelize some calculations that use numpy with the help of Python's multiprocessing module. Consider this simplified example: When I execute it, the multicore_time is roughly equal to single_time * n_par, while I would expect it to be close to single_time. Indeed, if I replace numpy calculations with just time.sleep(10), this is what I get perfect efficiency. But for some reason it does not work with numpy. Can this be solved, or is it some internal limitation of numpy?Some additional info which may be useful:I'm using OSX 10.9.5, Python 3.4.2 and the CPU is Core i7 with (as reported by the system info) 4 cores (although the above program only takes 50% of CPU time in total, so the system info may not be taking into account hyperthreading).when I run this I see n_par processes in top working at 100% CPUif I replace numpy array operations with a loop and per-index operations, the efficiency rises significantly (to about 75% for n_par = 4). <code>  import timeimport numpyfrom multiprocessing import Pooldef test_func(i): a = numpy.random.normal(size=1000000) b = numpy.random.normal(size=1000000) for i in range(2000): a = a + b b = a - b a = a - b return 1t1 = time.time()test_func(0)single_time = time.time() - t1print(""Single time:"", single_time)n_par = 4pool = Pool()t1 = time.time()results_async = [ pool.apply_async(test_func, [i]) for i in range(n_par)]results = [r.get() for r in results_async]multicore_time = time.time() - t1print(""Multicore time:"", multicore_time)print(""Efficiency:"", single_time / multicore_time)",Inefficient multiprocessing of numpy-based calculations
pandas split string into columns," I have the following DataFrame, where Track ID is the row index. How can I split the string in the stats column into 5 columns of numbers? <code>  Track ID stats14.0 (-0.00924175824176, 0.41, -0.742016492568, 0.0036830094242, 0.00251748449963)28.0 (0.0411538461538, 0.318230769231, 0.758717081514, 0.00264000622468, 0.0106535783677)42.0 (-0.0144351648352, 0.168438461538, -0.80870348637, 0.000816872566404, 0.00316572586742)56.0 (0.0343461538462, 0.288730769231, 0.950844962874, 6.1608706775e-07, 0.00337262030771)70.0 (0.00905164835165, 0.151030769231, 0.670257006716, 0.0121790506745, 0.00302182567957)84.0 (-0.0047967032967, 0.171615384615, -0.552879463981, 0.0500316517755, 0.00217970256969)","Split strings in tuples into columns, in Pandas"
"How get a (x,y) position pointing with mouse in a interactive plot (Python)?"," I use ipython notebook (with the magic %matplotlib nbagg). I was reviewing the matplotlib.widget.Cursor but the cursor is only viewed widgets.Cursor. So I'd like to select two points clicking in the plot and get the initial and final x,y-position (e.g. time vs Temperature, selecting to points must return initial and final time). I need it for selecting manually an arbitrary interval. I think it's similar to get global x,y position, but I didn't understand well in that post.How can I get a (x,y) position pointing with mouse in a interactive plot (Python)?Obs. Something similar to CURSOR Procedure in IDL <code> ","How to get a (x,y) position pointing with mouse in a interactive plot (Python)?"
uWSGI python highload configuration 5K+ qps," We have a big EC2 instance with 32 cores, currently running Nginx, Tornado and Redis, serving on average 5K requests per second. Everything seems to work fine, but the CPU load already reaching 70% and we have to support even more requests. One of the thoughts was to replace Tornado with uWSGI because we don't really use async features of Tornado. Our application consist from one function, it receives a JSON (~=4KB), doing some blocking but very fast stuff (Redis) and return JSON. Proxy HTTP request to one of the Tornado instances (Nginx) Parse HTTP request (Tornado) Read POST body string (stringified JSON) and convert it to python dictionary (Tornado) Take data out of Redis (blocking) located on same machine (py-redis with hiredis) Process the data (python3.4) Update Redis on same machine (py-redis with hiredis) Prepare stringified JSON for response (python3.4) Send response to proxy (Tornado) Send response to client (Nginx)We thought the speed improvement will come from uwsgi protocol, we can install Nginx on separate server and proxy all requests to uWSGI with uwsgi protocol. But after trying all possible configurations and changing OS parameters we still can't get it working even on current load.Most of the time nginx log contains 499 and 502 errors. In some configurations it just stopped receiving new requests like it hit some OS limit.So as I said, we have 32 cores, 60GB free memory and very fast network. We don't do heavy stuff, only very fast blocking operations. What is the best strategy in this case? Processes, Threads, Async? What OS parameters should be set?Current configuration is: Nginx config: OS config: I know the limits are too high, started to try every value possible.UPDATE:I ended up with the following configuration: <code>  [uwsgi]master = 2processes = 100socket = /tmp/uwsgi.sockwsgi-file = app.pydaemonize = /dev/nullpidfile = /tmp/uwsgi.pidlisten = 64000stats = /tmp/stats.socketcpu-affinity = 1max-fd = 20000memory-report = 1gevent = 1000thunder-lock = 1threads = 100post-buffering = 1 user www-data;worker_processes 10;pid /run/nginx.pid;events { worker_connections 1024; multi_accept on; use epoll;} sysctl net.core.somaxconnnet.core.somaxconn = 64000 [uwsgi]chdir = %dmaster = 1processes = %ksocket = /tmp/%c.sockwsgi-file = app.pylazy-apps = 1touch-chain-reload = %dreloadvirtualenv = %d.envdaemonize = /dev/nullpidfile = /tmp/%c.pidlisten = 40000stats = /tmp/stats-%c.socketcpu-affinity = 1max-fd = 200000memory-report = 1post-buffering = 1threads = 2",uWSGI python highload configuration
Exclude certain dependency version ranges in setuptools," Currently the Django Project supports 1.4, 1.7 and 1.8. In my setup.py I want to reflect these versions as being supported. However this still allows 1.5.x and 1.6.x releases. How can I exclude a complete range?Setuptools lists the following valid requirements as an example: However this doesn't work with pip (it should at least match 1.4.x / 1.5.x): No matching distribution found for PickyThing!=1.9.6,<1.6,<2.0a0,==2.4c1,>1.9Update with exampleFor example; I only want to include currently supported versions of Django. This would be 1.4.x, 1.7.x and 1.8.x. So I would write; However if I run pip install -e . on this project, it fails with; It is obvious that a version number 1.4.20 cannot satisfy >=1.7 and 1.8.1 cannot satisfy <1.4.99. However the documentation from Setuptools (see above) does suggest that something along these lines should be possible. However, this is non-obvious to me. <code>  install_requires=['Django>=1.4.2,<1.8.99,!=1.5,!=1.6'] PickyThing<1.6,>1.9,!=1.9.6,<2.0a0,==2.4c1 #setup.pyinstall_requires=['Django>=1.4.2,<1.4.99,>=1.7,<1.8.99'] Collecting Django<1.4.99,<1.8.99,>=1.4.2,>=1.7 (from ...)Could not find a version that satisfies the requirement Django<1.4.99,<1.8.99,>=1.4.2,>=1.7 (from django-two-factor-auth==1.2.0) (from versions: 1.1.3, 1.1.4, 1.2, 1.2.1, 1.2.2, 1.2.3, 1.2.4, 1.2.5, 1.2.6, 1.2.7, 1.3, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5, 1.3.6, 1.3.7, 1.4, 1.4.1, 1.4.2, 1.4.3, 1.4.4, 1.4.5, 1.4.6, 1.4.7, 1.4.8, 1.4.9, 1.4.10, 1.4.11, 1.4.12, 1.4.13, 1.4.14, 1.4.15, 1.4.16, 1.4.17, 1.4.18, 1.4.19, 1.4.20, 1.5, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.5.5, 1.5.6, 1.5.7, 1.5.8, 1.5.9, 1.5.10, 1.5.11, 1.5.12, 1.6, 1.6.1, 1.6.2, 1.6.3, 1.6.4, 1.6.5, 1.6.6, 1.6.7, 1.6.8, 1.6.9, 1.6.10, 1.6.11, 1.7, 1.7.1, 1.7.2, 1.7.3, 1.7.4, 1.7.5, 1.7.6, 1.7.7, 1.7.8, 1.8a1, 1.8b1, 1.8b2, 1.8rc1, 1.8, 1.8.1)No matching distribution found for Django<1.4.99,<1.8.99,>=1.4.2,>=1.7 (from ...)",Exclude certain dependency version ranges in setuptools/pip
load session and cookies from selenium browser to requests library in Python," How can I load session and cookies from Selenium browser? The following code: gives me the following exception: <code>  import requestscookies = [{u'domain': u'academics.vit.ac.in', u'name': u'ASPSESSIONIDAEQDTQRB', u'value': u'ADGIJGJDDGLFIIOCEZJHJCGC', u'expiry': None, u'path': u'/', u'secure': True}]response = requests.get(url2, cookies=cookies) Traceback (most recent call last): File ""F:\PYTHON\python_scripts\cookies\cookies3.py"", line 23, in <module> response = requests.get(url2, cookies=cookies) File ""C:\Python27\lib\site-packages\requests\api.py"", line 68, in get return request('get', url, **kwargs)<br/> File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 450, in request prep = self.prepare_request(req) cookies = cookiejar_from_dict(cookies) File ""C:\Python27\lib\site-packages\requests\cookies.py"", line 439, in cookiejar_from_dict cookiejar.set_cookie(create_cookie(name, cookie_dict[name]))TypeError: list indices must be integers, not dict",How do I load session and cookies from Selenium browser to requests library in Python?
"Got ""No installed app with label 'admin'"" when running migration on Django. Why?"," I am trying to use admin.LogEntry objects during a datamigration on Django 1.7The 'django.contrib.admin' app is listed on INSTALLED_APPS.On the shell, it works: But during the migration, it fails: Fails like this: Using a debugger, I got that the 'admin' is not installed: WHY?? <code>  >>> from django.apps import apps>>> apps.get_model('admin', 'LogEntry')django.contrib.admin.models.LogEntry def do_it(apps, schema_editor): LogEntry = apps.get_model('admin', 'LogEntry') django-admin migrate(...)LookupError: No installed app with label 'admin'. ipdb> apps.get_apps()[]ipdb> apps.all_models.keys()['website', 'google', 'allauth', 'twitter', 'busca', 'conteudo', 'django_mobile', 'django_filters', 'videocenter', 'tinymce', 'oferta', 'programacaotv', 'contenttypes', 'suit', 'haystack', 'destaque', 'filer', 'galeria', 'auth', 'facebook', 'paintstore', 'critica', 'disqus', 'fichas', 'omeletop', 'autocomplete_light', 'modelsv1', 'temas', 'django_extensions', 'adv_cache_tag', 'taggit', 'social', 'personalidade']","""No installed app with label 'admin'"" running Django migration. The app is installed correctly"
Converting complex raw postgresql query to django orm," Given PostgreSQL 9.2.10, Django 1.8, python 2.7.5 and the following models: And the following raw query which returns exactly what I am looking for: Which returns like this: What would be the best method for making this work using Django's model and orm structure?I have been looking around for possible methods for joining the two tables entirely without a relationship but there does not seem to be a clean or efficient way to do this. I have also tried looking for methods to do left outer joins in django, but again documentation is sparse or difficult to decipher.I know I will probably have to use Q objects to do the or clause I have in there. Additionally I have looked at relationships and it looks like a foreignkey() may work but I am unsure if this is the best method of doing it. Any and all help would be greatly appreciated. Thank you in advance.** EDIT 1 **So far Todor has offered a solution that uses a INNER JOIN that works. I may have found a solution HERE if anyone can decipher that mess of inline raw html.** EDIT 2 **Is there a way to filter on a field (where something = 'something') like my query above given, Todor's answer? I tried the following but it is still including all records even though my equivalent postresql query is working as expected. It seems I cannot have everything in the where that I do because when I remove one of the or statements and just do a and statement it applies the excluded filter. ** EDIT 3 / CURRENT SOLUTION IN PLACE **To date Todor has provided the most complete answer, using an INNER JOIN, but the hope is that this question will generate thought into how this still may be accomplished. As this does not seem to be inherently possible, any and all suggestions are welcome as they may possibly lead to better solutions. That being said, using Todor's answer, I was able accomplish the exact query I needed: ** TLDR **I would like to convert this PostGreSQL query to the ORM provided by Django WITHOUT using .raw() or any raw query code at all. I am completely open to changing the model to having a foreignkey if that facilitates this and is, from a performance standpoint, the best method. I am going to be using the objects returned in conjunction with django-datatables-view if that helps in terms of design. <code>  class restProdAPI(models.Model): rest_id = models.PositiveIntegerField(primary_key=True) rest_host = models.CharField(max_length=20) rest_ip = models.GenericIPAddressField(default='0.0.0.0') rest_mode = models.CharField(max_length=20) rest_state = models.CharField(max_length=20)class soapProdAPI(models.Model): soap_id = models.PositiveIntegerField(primary_key=True) soap_host = models.CharField(max_length=20) soap_ip = models.GenericIPAddressField(default='0.0.0.0') soap_asset = models.CharField(max_length=20) soap_state = models.CharField(max_length=20) SELECT app_restProdAPI.rest_id, app_soapProdAPI.soap_id, app_restProdAPI.rest_host, app_restProdAPI.rest_ip, app_soapProdAPI.soap_asset, app_restProdAPI.rest_mode, app_restProdAPI.rest_stateFROM app_soapProdAPILEFT OUTER JOIN app_restProdAPION ((app_restProdAPI.rest_host = app_soapProdAPI.soap_host)OR (app_restProdAPI.rest_ip = app_soapProdAPI.soap_ip))WHERE app_restProdAPI.rest_mode = 'Excluded'; rest_id | soap_id | rest_host | rest_ip | soap_asset | rest_mode | rest_state---------+---------+---------------+----------------+------------+-----------+----------- 1234 | 12345 | 1G24019123ABC | 123.123.123.12 | A1234567 | Excluded | Up soapProdAPI.objects.extra( select = { 'rest_id' : 'app_restprodapi.rest_id', 'rest_host' : 'app_restprodapi.rest_host', 'rest_ip' : 'app_restprodapi.rest_ip', 'rest_mode' : 'app_restprodapi.rest_mode', 'rest_state' : 'app_restprodapi.rest_state' }, tables = ['app_restprodapi'], where = ['app_restprodapi.rest_mode=%s \ AND app_restprodapi.rest_host=app_soapprodapi.soap_host \ OR app_restprodapi.rest_ip=app_soapprodapi.soap_ip'], params = ['Excluded'] ) restProdAPI.objects.extra( select = { 'soap_id' : 'app_soapprodapi.soap_id', 'soap_asset' : 'app_soapprodapi.soap_asset' }, tables = ['app_soapprodapi'], where = ['app_restprodapi.rest_mode = %s', 'app_soapprodapi.soap_host = app_restprodapi.rest_host OR \ app_soapprodapi.soap_ip = app_restprodapi.rest_ip' ], params = ['Excluded'] )",Converting LEFT OUTER JOIN query to Django orm queryset/query
Converting complex raw query to django orm," Given PostgreSQL 9.2.10, Django 1.8, python 2.7.5 and the following models: And the following raw query which returns exactly what I am looking for: Which returns like this: What would be the best method for making this work using Django's model and orm structure?I have been looking around for possible methods for joining the two tables entirely without a relationship but there does not seem to be a clean or efficient way to do this. I have also tried looking for methods to do left outer joins in django, but again documentation is sparse or difficult to decipher.I know I will probably have to use Q objects to do the or clause I have in there. Additionally I have looked at relationships and it looks like a foreignkey() may work but I am unsure if this is the best method of doing it. Any and all help would be greatly appreciated. Thank you in advance.** EDIT 1 **So far Todor has offered a solution that uses a INNER JOIN that works. I may have found a solution HERE if anyone can decipher that mess of inline raw html.** EDIT 2 **Is there a way to filter on a field (where something = 'something') like my query above given, Todor's answer? I tried the following but it is still including all records even though my equivalent postresql query is working as expected. It seems I cannot have everything in the where that I do because when I remove one of the or statements and just do a and statement it applies the excluded filter. ** EDIT 3 / CURRENT SOLUTION IN PLACE **To date Todor has provided the most complete answer, using an INNER JOIN, but the hope is that this question will generate thought into how this still may be accomplished. As this does not seem to be inherently possible, any and all suggestions are welcome as they may possibly lead to better solutions. That being said, using Todor's answer, I was able accomplish the exact query I needed: ** TLDR **I would like to convert this PostGreSQL query to the ORM provided by Django WITHOUT using .raw() or any raw query code at all. I am completely open to changing the model to having a foreignkey if that facilitates this and is, from a performance standpoint, the best method. I am going to be using the objects returned in conjunction with django-datatables-view if that helps in terms of design. <code>  class restProdAPI(models.Model): rest_id = models.PositiveIntegerField(primary_key=True) rest_host = models.CharField(max_length=20) rest_ip = models.GenericIPAddressField(default='0.0.0.0') rest_mode = models.CharField(max_length=20) rest_state = models.CharField(max_length=20)class soapProdAPI(models.Model): soap_id = models.PositiveIntegerField(primary_key=True) soap_host = models.CharField(max_length=20) soap_ip = models.GenericIPAddressField(default='0.0.0.0') soap_asset = models.CharField(max_length=20) soap_state = models.CharField(max_length=20) SELECT app_restProdAPI.rest_id, app_soapProdAPI.soap_id, app_restProdAPI.rest_host, app_restProdAPI.rest_ip, app_soapProdAPI.soap_asset, app_restProdAPI.rest_mode, app_restProdAPI.rest_stateFROM app_soapProdAPILEFT OUTER JOIN app_restProdAPION ((app_restProdAPI.rest_host = app_soapProdAPI.soap_host)OR (app_restProdAPI.rest_ip = app_soapProdAPI.soap_ip))WHERE app_restProdAPI.rest_mode = 'Excluded'; rest_id | soap_id | rest_host | rest_ip | soap_asset | rest_mode | rest_state---------+---------+---------------+----------------+------------+-----------+----------- 1234 | 12345 | 1G24019123ABC | 123.123.123.12 | A1234567 | Excluded | Up soapProdAPI.objects.extra( select = { 'rest_id' : 'app_restprodapi.rest_id', 'rest_host' : 'app_restprodapi.rest_host', 'rest_ip' : 'app_restprodapi.rest_ip', 'rest_mode' : 'app_restprodapi.rest_mode', 'rest_state' : 'app_restprodapi.rest_state' }, tables = ['app_restprodapi'], where = ['app_restprodapi.rest_mode=%s \ AND app_restprodapi.rest_host=app_soapprodapi.soap_host \ OR app_restprodapi.rest_ip=app_soapprodapi.soap_ip'], params = ['Excluded'] ) restProdAPI.objects.extra( select = { 'soap_id' : 'app_soapprodapi.soap_id', 'soap_asset' : 'app_soapprodapi.soap_asset' }, tables = ['app_soapprodapi'], where = ['app_restprodapi.rest_mode = %s', 'app_soapprodapi.soap_host = app_restprodapi.rest_host OR \ app_soapprodapi.soap_ip = app_restprodapi.rest_ip' ], params = ['Excluded'] )",Converting LEFT OUTER JOIN query to Django orm queryset/query
Converting complex raw query to Django orm queryset/query," Given PostgreSQL 9.2.10, Django 1.8, python 2.7.5 and the following models: And the following raw query which returns exactly what I am looking for: Which returns like this: What would be the best method for making this work using Django's model and orm structure?I have been looking around for possible methods for joining the two tables entirely without a relationship but there does not seem to be a clean or efficient way to do this. I have also tried looking for methods to do left outer joins in django, but again documentation is sparse or difficult to decipher.I know I will probably have to use Q objects to do the or clause I have in there. Additionally I have looked at relationships and it looks like a foreignkey() may work but I am unsure if this is the best method of doing it. Any and all help would be greatly appreciated. Thank you in advance.** EDIT 1 **So far Todor has offered a solution that uses a INNER JOIN that works. I may have found a solution HERE if anyone can decipher that mess of inline raw html.** EDIT 2 **Is there a way to filter on a field (where something = 'something') like my query above given, Todor's answer? I tried the following but it is still including all records even though my equivalent postresql query is working as expected. It seems I cannot have everything in the where that I do because when I remove one of the or statements and just do a and statement it applies the excluded filter. ** EDIT 3 / CURRENT SOLUTION IN PLACE **To date Todor has provided the most complete answer, using an INNER JOIN, but the hope is that this question will generate thought into how this still may be accomplished. As this does not seem to be inherently possible, any and all suggestions are welcome as they may possibly lead to better solutions. That being said, using Todor's answer, I was able accomplish the exact query I needed: ** TLDR **I would like to convert this PostGreSQL query to the ORM provided by Django WITHOUT using .raw() or any raw query code at all. I am completely open to changing the model to having a foreignkey if that facilitates this and is, from a performance standpoint, the best method. I am going to be using the objects returned in conjunction with django-datatables-view if that helps in terms of design. <code>  class restProdAPI(models.Model): rest_id = models.PositiveIntegerField(primary_key=True) rest_host = models.CharField(max_length=20) rest_ip = models.GenericIPAddressField(default='0.0.0.0') rest_mode = models.CharField(max_length=20) rest_state = models.CharField(max_length=20)class soapProdAPI(models.Model): soap_id = models.PositiveIntegerField(primary_key=True) soap_host = models.CharField(max_length=20) soap_ip = models.GenericIPAddressField(default='0.0.0.0') soap_asset = models.CharField(max_length=20) soap_state = models.CharField(max_length=20) SELECT app_restProdAPI.rest_id, app_soapProdAPI.soap_id, app_restProdAPI.rest_host, app_restProdAPI.rest_ip, app_soapProdAPI.soap_asset, app_restProdAPI.rest_mode, app_restProdAPI.rest_stateFROM app_soapProdAPILEFT OUTER JOIN app_restProdAPION ((app_restProdAPI.rest_host = app_soapProdAPI.soap_host)OR (app_restProdAPI.rest_ip = app_soapProdAPI.soap_ip))WHERE app_restProdAPI.rest_mode = 'Excluded'; rest_id | soap_id | rest_host | rest_ip | soap_asset | rest_mode | rest_state---------+---------+---------------+----------------+------------+-----------+----------- 1234 | 12345 | 1G24019123ABC | 123.123.123.12 | A1234567 | Excluded | Up soapProdAPI.objects.extra( select = { 'rest_id' : 'app_restprodapi.rest_id', 'rest_host' : 'app_restprodapi.rest_host', 'rest_ip' : 'app_restprodapi.rest_ip', 'rest_mode' : 'app_restprodapi.rest_mode', 'rest_state' : 'app_restprodapi.rest_state' }, tables = ['app_restprodapi'], where = ['app_restprodapi.rest_mode=%s \ AND app_restprodapi.rest_host=app_soapprodapi.soap_host \ OR app_restprodapi.rest_ip=app_soapprodapi.soap_ip'], params = ['Excluded'] ) restProdAPI.objects.extra( select = { 'soap_id' : 'app_soapprodapi.soap_id', 'soap_asset' : 'app_soapprodapi.soap_asset' }, tables = ['app_soapprodapi'], where = ['app_restprodapi.rest_mode = %s', 'app_soapprodapi.soap_host = app_restprodapi.rest_host OR \ app_soapprodapi.soap_ip = app_restprodapi.rest_ip' ], params = ['Excluded'] )",Converting LEFT OUTER JOIN query to Django orm queryset/query
scoring must return a number cross_val_score error in Sklearn," Maybe it is a dumb question, but I don't understand the error that the function cross_val_score in the code below give me. Perhaps the answer is in the format of X sample, seeing that this is exactly what was shown in the crash message, but I don't know how to fix. This is a piece of code from my project with some random values. Gives me the error: <code>  import numpy as npfrom sklearn import mixture,cross_validationnp.random.seed(0)n_samples = 300C = np.array([[0., -0.7], [3.5, .7]])X = np.r_[np.dot(np.random.randn(n_samples, 2), C), np.random.randn(n_samples, 2) + np.array([20, 20])]clf = mixture.GMM(n_components=2, covariance_type='full')score = cross_validation.cross_val_score(clf, X) ValueError: scoring must return a number, got (<type 'numpy.ndarray'>) instead","""scoring must return a number"" cross_val_score error in scikit-learn"
Using Inheritance and encounting by non-nullable field error," I used inheritance model in my project after changing the model; but I give non-nullable field error. What should I do?I am using Django 1.7 After migrations: <code>  class Questions(models.Model): question_category = models.ForeignKey(Course, blank=False) question_author = models.ForeignKey(Author, blank=False) question_details = models.CharField(max_length=100, blank=False, default='') timestamp = models.DateTimeField(auto_now_add=True)class TypeFive(Questions): question_title = models.CharField(max_length=100, blank=False, default=generator(5), unique=True, editable=False) def __str__(self): return ""{}"".format(self.question_title)class TypeFiveChoice(models.Model): question_choice = models.ForeignKey(TypeFive) is_it_question = models.BooleanField(default=False) word = models.CharField(default='', blank=False, max_length=20) translate = models.CharField(default='', blank=False, max_length=20) timestamp = models.DateTimeField(auto_now_add=True) def __str__(self): return ""{} : {}, {}"".format(self.question_choice, self.word, self.translate) You are trying to add a non-nullable field 'questions_ptr' to typefive without a default; we can't do that (the database needs something to populate existing rows).Please select a fix: 1) Provide a one-off default now (will be set on all existing rows) 2) Quit, and let me add a default in models.py",Using model inheritance and encounting by non-nullable field error
Memory leak in my python program," I am trying to embed a matplotlib graph that updates every second into a PyQt GUI main window.In my program I call an update function every second using threading.Timer via the timer function shown below. I have a problem: my program grows bigger every second - at a rate of about 1k every 4 seconds. My initial thoughts are that the append function (that returns a new array in update_figure) does not delete the old array? Is it possible this is the cause of my problem? This is my timer function - this is triggered by the click of a button in my PyQt GUI and then calls itself as you can see: EDIT: I cant post my entire code because it requires a lot of .dll includes. So i'll try to explain what this program does.In my GUI I want to show the my CO2 value over time. My get_co22 function just returns a float value and I'm 100% sure this works fine. With my timer, shown above, I want to keep append a value to a matplotlib graph - the Axes object is available to me as self.axes. I try to plot the last 10 values of the data.EDIT 2: After some discussion in chat, I tried putting the call to update_figure() in a while loop and using just one thread to call it and was able to make this minimal example http://pastebin.com/RXya6Zah. This changed the structure of the code to call update_figure() to the following: but now the program crashes after 5 iterations or so. <code>  def update_figure(self): self.yAxis = np.append(self.yAxis, (getCO22())) self.xAxis = np.append(self.xAxis, self.i) # print(self.xAxis) if len(self.yAxis) > 10: self.yAxis = np.delete(self.yAxis, 0) if len(self.xAxis) > 10: self.xAxis = np.delete(self.xAxis, 0) self.axes.plot(self.xAxis, self.yAxis, scaley=False) self.axes.grid(True) self.i = self.i + 1 self.draw() def timer(self): getCH4() getCO2() getConnectedDevices() self.dc.update_figure() t = threading.Timer(1.0, self.timer) t.start() def task(self): while True: ui.dc.update_figure() time.sleep(1.0)def timer(self): t = Timer(1.0, self.task()) t.start()",Memory leak when embedding and updating a matplotlib graph in a PyQt GUI
Using distplot in Python," What is the unit of the y-axis when using distplot to plot a histogram? I have plotted different histograms together with a normal fit and I see that in one case, it has a range of 0 to 0.9 while in another a range of 0 to 4.5. <code> ",What is the unit of the y-axis when using distplot to plot a histogram?
Slenium Python how to get text from tag <strong>," I'm trying to get text $27.5 inside tag <div>, I located the element by id and the element is called ""price"". The snippet of html is as follows: Here is what I've tried Both of the above doesn't work.Update:Thanks for everyone that tries to help.I combined your answers together and got the solution:) <code>  <div id=""PPP,BOSSST,NYCPAS,2015-04-26T01:00:00-04:00,2015-04-26T05:20:00-04:00,_price"" class=""price inlineBlock strong mediumText"">$27.50</div> price.textprice.get_attribute('value') price = driver.find_element_by_xpath(""//div[@class='price inlineBlock strong mediumText']"") price_content = price.get_attribute('innerHTML') print price_content.strip()",Selenium Python how to get text(html source) from <div>
Selenium Python how to get text from tag <strong>," I'm trying to get text $27.5 inside tag <div>, I located the element by id and the element is called ""price"". The snippet of html is as follows: Here is what I've tried Both of the above doesn't work.Update:Thanks for everyone that tries to help.I combined your answers together and got the solution:) <code>  <div id=""PPP,BOSSST,NYCPAS,2015-04-26T01:00:00-04:00,2015-04-26T05:20:00-04:00,_price"" class=""price inlineBlock strong mediumText"">$27.50</div> price.textprice.get_attribute('value') price = driver.find_element_by_xpath(""//div[@class='price inlineBlock strong mediumText']"") price_content = price.get_attribute('innerHTML') print price_content.strip()",Selenium Python how to get text(html source) from <div>
Using both Pygame and wxpython simultaniously," I need to to build an application that has multiple windows. In one of these windows, I need to be able to play a simple game and another window has to display questions and get response from a user that influences the game.(1) I was wanting to use pygame in order to make the game. Is there a simple way to have pygame operate with multiple windows?(2) If there is no easy way to solve (1), is there a simple way to use some other python GUI structure that would allow for me to run pygame and another window simultaneously? <code> ",Pygame with Multiple Windows
Pandas DataFrame to List of Dictionaries (Dics)," I have the following DataFrame: which I want to translate it to list of dictionaries per row <code>  customer item1 item2 item31 apple milk tomato2 water orange potato3 juice mango chips rows = [ { 'customer': 1, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato' }, { 'customer': 2, 'item1': 'water', 'item2': 'orange', 'item3': 'potato' }, { 'customer': 3, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips' }]",Pandas DataFrame to List of Dictionaries
Dense Dot Sparse," If I have a numpy.ndarray A and a scipy.sparse.csc_matrix B, how do I take A dot B? I can do B dot A by saying B.dot(A), but the other way I can only think of this: Is there a more direct method to do this?  <code>  B.T.dot(A.T).T",Right multiplication of a dense array with a sparse matrix
Find the column name which has maximum value for each row [pandas]," I have a DataFrame like this one: In here, I want to ask how to get column name which has maximum value for each row, the desired output is like this: <code>  In [7]:frame.head()Out[7]:Communications and Search Business General Lifestyle0 0.745763 0.050847 0.118644 0.0847460 0.333333 0.000000 0.583333 0.0833330 0.617021 0.042553 0.297872 0.0425530 0.435897 0.000000 0.410256 0.1538460 0.358974 0.076923 0.410256 0.153846 In [7]: frame.head() Out[7]: Communications and Search Business General Lifestyle Max 0 0.745763 0.050847 0.118644 0.084746 Communications 0 0.333333 0.000000 0.583333 0.083333 Business 0 0.617021 0.042553 0.297872 0.042553 Communications 0 0.435897 0.000000 0.410256 0.153846 Communications 0 0.358974 0.076923 0.410256 0.153846 Business ",Find the column name which has the maximum value for each row
"Calculating the averages for each KEY in a Pairwise (K,V) RDD"," I want to share this particular Apache Spark with Python solution because documentation for it is quite poor.I wanted to calculate the average value of K/V pairs (stored in a Pairwise RDD), by KEY. Here is what the sample data looks like: Now the following code sequence is a less than optimal way to do it, but it does work. It is what I was doing before I figured out a better solution. It's not terrible but -- as you'll see in the answer section -- there is a more concise, efficient way. <code>  >>> rdd1.take(10) # Show a small sample.[(u'2013-10-09', 7.60117302052786),(u'2013-10-10', 9.322709163346612),(u'2013-10-10', 28.264462809917358),(u'2013-10-07', 9.664429530201343),(u'2013-10-07', 12.461538461538463),(u'2013-10-09', 20.76923076923077),(u'2013-10-08', 11.842105263157894),(u'2013-10-13', 32.32514177693762),(u'2013-10-13', 26.249999999999996),(u'2013-10-13', 10.693069306930692)] >>> import operator>>> countsByKey = sc.broadcast(rdd1.countByKey()) # SAMPLE OUTPUT of countsByKey.value: {u'2013-09-09': 215, u'2013-09-08': 69, ... snip ...}>>> rdd1 = rdd1.reduceByKey(operator.add) # Calculate the numerators (i.e. the SUMs).>>> rdd1 = rdd1.map(lambda x: (x[0], x[1]/countsByKey.value[x[0]])) # Divide each SUM by it's denominator (i.e. COUNT)>>> print(rdd1.collect()) [(u'2013-10-09', 11.235365503035176), (u'2013-10-07', 23.39500642456595), ... snip ... ]","Calculating the averages for each KEY in a Pairwise (K,V) RDD in Spark with Python"
"How to more efficiently calculate the averages for each KEY in a Pairwise (K,V) RDD in Apache Spark with Python"," I want to share this particular Apache Spark with Python solution because documentation for it is quite poor.I wanted to calculate the average value of K/V pairs (stored in a Pairwise RDD), by KEY. Here is what the sample data looks like: Now the following code sequence is a less than optimal way to do it, but it does work. It is what I was doing before I figured out a better solution. It's not terrible but -- as you'll see in the answer section -- there is a more concise, efficient way. <code>  >>> rdd1.take(10) # Show a small sample.[(u'2013-10-09', 7.60117302052786),(u'2013-10-10', 9.322709163346612),(u'2013-10-10', 28.264462809917358),(u'2013-10-07', 9.664429530201343),(u'2013-10-07', 12.461538461538463),(u'2013-10-09', 20.76923076923077),(u'2013-10-08', 11.842105263157894),(u'2013-10-13', 32.32514177693762),(u'2013-10-13', 26.249999999999996),(u'2013-10-13', 10.693069306930692)] >>> import operator>>> countsByKey = sc.broadcast(rdd1.countByKey()) # SAMPLE OUTPUT of countsByKey.value: {u'2013-09-09': 215, u'2013-09-08': 69, ... snip ...}>>> rdd1 = rdd1.reduceByKey(operator.add) # Calculate the numerators (i.e. the SUMs).>>> rdd1 = rdd1.map(lambda x: (x[0], x[1]/countsByKey.value[x[0]])) # Divide each SUM by it's denominator (i.e. COUNT)>>> print(rdd1.collect()) [(u'2013-10-09', 11.235365503035176), (u'2013-10-07', 23.39500642456595), ... snip ... ]","Calculating the averages for each KEY in a Pairwise (K,V) RDD in Spark with Python"
Ignore divide by 0 warning in python," I have a function for statistic issues: Sometimes I get from the shell the following warning: I use the numpy isinf function to correct the results of the function in other files, so I do not need this warning. Is there a way to ignore the message?In other words, I do not want the shell to print this message.I do not want to disable all python warnings, just this one. <code>  import numpy as npfrom scipy.special import gamma as Gammadef Foo(xdata): ... return x1 * ( ( #R is a numpy vector ( ((R - x2)/beta) ** (x3 -1) ) * ( np.exp( - ((R - x2) / x4) ) ) / ( x4 * Gamma(x3)) ).real ) RuntimeWarning: divide by zero encountered in...",Ignore divide by 0 warning in NumPy
Match a word in a string using regex - python," I am looking to see whether a word occurs in a sentence using regex. Words are separated by spaces, but may have punctuation on either side. If the word is in the middle of the string, the following match works (it prevents part-words from matching, allows punctuation on either side of the word). This won't however match the first or last word, since there is no trailing/leading space. So, for these cases, I have also been using: and then combining with Is there a simple way to avoid the need of three match terms. Specifically, is there a way of specifying 'ether a space or the start of file (i.e. ""^"") and similar, 'either a space or the end of the file (i.e. ""$"")? <code>  match_middle_words = "" [^a-zA-Z\d ]{0,}"" + word + ""[^a-zA-Z\d ]{0,} "" match_starting_word = ""^[^a-zA-Z\d]{0,}"" + word + ""[^a-zA-Z\d ]{0,} ""match_end_word = "" [^a-zA-Z\d ]{0,}"" + word + ""[^a-zA-Z\d]{0,}$"" match_string = match_middle_words + ""|"" + match_starting_word +""|"" + match_end_word ",Match a whole word in a string using dynamic regex
How to find the first index of any of a set of characters in a string (Python)," I'd like to find the index of the first occurrence of any special character in a string, like so: except that's not valid Python syntax. Of course, I can write a function that emulates this behavior: I could also use regular expressions, but both solutions seem to be a bit overkill. Is there any sane way to do this in Python? <code>  >>> ""Hello world!"".index([' ', '!'])5 def first_index(s, characters): i = [] for c in characters: try: i.append(s.index(c)) except ValueError: pass if not i: raise ValueError return min(i)",How to find the first index of any of a set of characters in a string
how to handle long path name for pep8?, How would I handle long path name like below for pep8 compliance? Is 79 characters per line a must even if it becomes somewhat unreadable? <code>  def setUp(self): self.patcher1 = patch('projectname.common.credential.CredentialCache.mymethodname') ,how to handle long path name for pep8 compliance?
Run function with arguments in parallel in python (follow up)," This is a follow up question to: Python: How can I run python functions in parallel?Minimal Working Example: Which leeds to this (and it's exactly what i want): func1: starting 1430920678.09 func1: starting 1430920678.53 func1: starting 1430920679.02 func1: starting 1430920679.57 func1: starting 1430920680.55 func1: finishing 1430920729.68 duration 51.1449999809 func1: finishing 1430920729.78 duration 51.6889998913 func1: finishing 1430920730.69 duration 51.1239998341 func1: finishing 1430920748.64 duration 69.6180000305 func1: finishing 1430920749.25 duration 68.7009999752 71.5629999638However, my function has quite a load of arguments, so i tested it like this:-> func1(a) now gets an argument passed. So now this happens: func1: starting 1430921299.08 func1: finishing 1430921327.84 duration 28.760999918 func1: starting 1430921327.84 func1: finishing 1430921357.68 duration 29.8410000801 func1: starting 1430921357.68 func1: finishing 1430921387.14 duration 29.4619998932 func1: starting 1430921387.14 func1: finishing 1430921416.52 duration 29.3849999905 func1: starting 1430921416.52 func1: finishing 1430921447.39 duration 30.864000082 151.392999887The process is now sequential and no longer parallel, and i don't get why! What am I missing and doing wrong?EDIT: Additionally, how would an example look like, whre a few arguments are positional and others which are optional? <code>  '''Created on 06.05.2015https://stackoverflow.com/questions/7207309/python-how-can-i-run-python-functions-in-parallel'''from multiprocessing import Processimport timedef runInParallel(*fns): proc = [] for fn in fns: p = Process(target=fn) p.start() proc.append(p) for p in proc: p.join()def func1(): s=time.time() print 'func1: starting', s for i in xrange(1000000000): if i==i: pass e = time.time() print 'func1: finishing', e print 'duration', e-sif __name__ == '__main__': s =time.time() runInParallel(func1, func1, func1, func1, func1) print time.time()-s '''Created on 06.05.2015https://stackoverflow.com/questions/7207309/python-how-can-i-run-python-functions-in-parallel'''from multiprocessing import Processimport timedef runInParallel(*fns): proc = [] for fn in fns: p = Process(target=fn) p.start() proc.append(p) for p in proc: p.join()def func1(a): s=time.time() print 'func1: starting', s for i in xrange(a): if i==i: pass e = time.time() print 'func1: finishing', e print 'duration', e-sif __name__ == '__main__': s =time.time() g=s runInParallel(func1(1000000000), func1(1000000000), func1(1000000000), func1(1000000000), func1(1000000000)) print time.time()-s",Run function with positional and optional arguments in parallel in python (follow up)
how to convert list of lists to a set in python," I have a list users_with_invites_ids_list, formed by loop where I append values to the list, in python that looks like this: when I try: I get: How do I convert this list of lists to a set?EDITbased on answer I've done the following: Which yields the following: How do I get each ObjectId without the () around each one. It's keeping me from comparing this set to other set's of ids. <code>  ...[ObjectId('55119e14bf2e4e010d8b48f2')], [ObjectId('54624128bf2e4e5e558b5a52')], [ObjectId('53a6e7bc763f4aa0308b4569')], [ObjectId('55241823bf2e4e59508b494c')]] users_with_invites_ids_set = set(users_with_invites_ids_list) TypeError: unhashable type: 'list' #convert list to setfirst_tuple_list = [tuple(lst) for lst in users_with_invites_ids_list]users_with_invites_ids_set = set(first_tuple_list) (ObjectId('542ac5a6763f4a82188b4a51'),), (ObjectId('54496fe6bf2e4efe348bd344'),), (ObjectId('54c96339bf2e4ee62c8b48e0'),)])",How to convert list of lists to a set in python so I can compare to other sets?
how to convert list of lists to a set in python so I can compare to other sets?," I have a list users_with_invites_ids_list, formed by loop where I append values to the list, in python that looks like this: when I try: I get: How do I convert this list of lists to a set?EDITbased on answer I've done the following: Which yields the following: How do I get each ObjectId without the () around each one. It's keeping me from comparing this set to other set's of ids. <code>  ...[ObjectId('55119e14bf2e4e010d8b48f2')], [ObjectId('54624128bf2e4e5e558b5a52')], [ObjectId('53a6e7bc763f4aa0308b4569')], [ObjectId('55241823bf2e4e59508b494c')]] users_with_invites_ids_set = set(users_with_invites_ids_list) TypeError: unhashable type: 'list' #convert list to setfirst_tuple_list = [tuple(lst) for lst in users_with_invites_ids_list]users_with_invites_ids_set = set(first_tuple_list) (ObjectId('542ac5a6763f4a82188b4a51'),), (ObjectId('54496fe6bf2e4efe348bd344'),), (ObjectId('54c96339bf2e4ee62c8b48e0'),)])",How to convert list of lists to a set in python so I can compare to other sets?
Is there a Python 'shortcut' to define a variable equal to a string version of its own name?," This is a bit of a silly thing, but I want to know if there is concise way in Python to define class variables that contain string representations of their own names. For example, one can define: Probably a more concise way to write it in terms of lines consumed is: Even there, though, I still have to type each identifier twice, once on each side of the assignment, and the opportunity for typos is rife.What I want is something like what sympy provides in its var method: The above injects into the namespace the variables a, b, and c, defined as the corresponding sympy symbolic variables.Is there something comparable that would do this for plain strings? EDIT: To note, I want to be able to access these as separate identifiers in code that uses foo: ADDENDUM: Given the interest in the question, I thought I'd provide more context on why I want to do this. I have two use-cases at present: (1) typecodes for a set of custom exceptions (each Exception subclass has a distinct typecode set); and (2) lightweight enum. My desired feature set is:Only having to type the typecode / enum name (or value) once in the source definition. class foo(object): bar = 'bar' works fine but means I have to type it out twice in-source, which gets annoying for longer names and exposes a typo risk.Valid typecodes / enum values exposed for IDE autocomplete.Values stored internally as comprehensible strings:For the Exception subclasses, I want to be able to define myError.__str__ as just something like return self.typecode + "": "" + self.message + "" ("" + self.source + "")"", without having to do a whole lot of dict-fu to back-reference an int value of self.typecode to a comprehensible and meaningful string.For the enums, I want to just be able to obtain widget as output from e = myEnum.widget; print(e), again without a lot of dict-fu. I recognize this will increase overhead. My application is not speed-sensitive (GUI-based tool for driving a separate program), so I don't think this will matter at all.Straightforward membership testing, by also including (say) a frozenset containing all of the typecodes / enum string values as myError.typecodes/myEnum.E classes. This addresses potential problems from accidental (or intentional.. but why?!) use of an invalid typecode / enum string via simple sanity checks like if not enumVal in myEnum.E: raise(ValueError('Invalid enum value: ' + str(enumVal))).Ability to import individual enum / exception subclasses via, say, from errmodule import squirrelerror, to avoid cluttering the namespace of the usage environment with non-relevant exception subclasses. I believe this prohibits any solutions requiring post-twiddling on the module level like what Sinux proposed.For the enum use case, I would rather avoid introducing an additional package dependency since I don't (think I) care about any extra functionality available in the official enum class. In any event, it still wouldn't resolve #1.I've already figured out implementation I'm satisfied with for all of the above but #1. My interest in a solution to #1 (without breaking the others) is partly a desire to typo-proof entry of the typecode / enum values into source, and partly plain ol' laziness. (Says the guy who just typed up a gigantic SO question on the topic.) <code>  class foo(object): bar = 'bar' baz = 'baz' baf = 'baf' class foo(object): bar, baz, baf = 'bar', 'baz', 'baf' sympy.var('a,b,c') class foo(object): [nifty thing]('bar', 'baz', 'baf') >>> f = foo(); print(f.bar)bar",Is there a Python 'shortcut' to define a class variable equal to a string version of its own name?
Why are some float-to-integer comparisons four times slower than others?," When comparing floats to integers, some pairs of values take much longer to be evaluated than other values of a similar magnitude.For example: But if the float or integer is made smaller or larger by a certain amount, the comparison runs much more quickly: Changing the comparison operator (e.g. using == or > instead) does not affect the times in any noticeable way. This is not solely related to magnitude because picking larger or smaller values can result in faster comparisons, so I suspect it is down to some unfortunate way the bits line up. Clearly, comparing these values is more than fast enough for most use cases. I am simply curious as to why Python seems to struggle more with some pairs of values than with others. <code>  >>> import timeit>>> timeit.timeit(""562949953420000.7 < 562949953421000"") # run 1 million times0.5387085462592742 >>> timeit.timeit(""562949953420000.7 < 562949953422000"") # integer increased by 10000.1481498428446173>>> timeit.timeit(""562949953423001.8 < 562949953421000"") # float increased by 3001.10.1459577925548956",Why are some float < integer comparisons four times slower than others?
How to create a hyperlink to a different excel sheet in the same workbook using openpyxl," I'm using the module openpyxl for Python and am trying to create a hyperlink that will take me to a different tab in the same Excel workbook. Doing something similar to the following creates the hyperlink; however, when I click on it, it tells me it can't open the file. I'm assuming the issue lies in the value of 'link_to'; however, I don't know what would need changed or what kind of path I would have to write.I'm using Python 2.7.6 and Excel 2013. <code>  from openpyxl import Workbookwb = Workbook()first_sheet = wb.create_sheet(title='first')second_sheet = wb.create_sheet(title='second')first_sheet['A1'] = ""hello""second_sheet['B2'] = ""goodbye""link_from = first_sheet['A1']link_to = second_sheet['B2'].valuelink_from.hyperlink = link_towb.save(""C:/somepath/workbook.xlsx"")",How to create a hyperlink to a different Excel sheet in the same workbook
How to store floating point values using less than 8 bytes?," I need to store a massive numpy vector to disk. Right now the vector that I am trying to store is ~2.4 billion elements long and the data is float64. This takes about 18GB of space when serialized out to disk.If I use struct.pack() and use float32 (4 bytes) I can reduce it to ~9GB. I don't need anywhere near this amount of precision disk space is going to quickly becomes an issue as I expect the number of values I need to store could grow by an order of magnitude or two.I was thinking that if I could access the first 4 significant digits I could store those values in an int and only use 1 or 2 bytes of space. However, I have no idea how to do this efficiently. Does anyone have any idea or suggestions? <code> ",Binary storage of floating point values (between 0 and 1) using less than 4 bytes?
How to store floating point values using less than 4 bytes?," I need to store a massive numpy vector to disk. Right now the vector that I am trying to store is ~2.4 billion elements long and the data is float64. This takes about 18GB of space when serialized out to disk.If I use struct.pack() and use float32 (4 bytes) I can reduce it to ~9GB. I don't need anywhere near this amount of precision disk space is going to quickly becomes an issue as I expect the number of values I need to store could grow by an order of magnitude or two.I was thinking that if I could access the first 4 significant digits I could store those values in an int and only use 1 or 2 bytes of space. However, I have no idea how to do this efficiently. Does anyone have any idea or suggestions? <code> ",Binary storage of floating point values (between 0 and 1) using less than 4 bytes?
why my python script is taking a screenshot?," I'm writing a script in Python, but when I attempt to run it a cross cursor appears and lets me take screenshots. But that's not part of my program, and the rest of the script never executes at all!The minimal code that produces this behavior is: <code>  import fionaimport scipy",Why does running my Python script start taking a screenshot?
Is there a faster way to clean out controlled characters in a file?," Previously, I had been cleaning out data using the code snippet below There are newline characters in the file that i want to keep.The following records the time taken for cc_re.sub('', s) to substitute the first few lines (1st column is the time taken and 2nd column is len(s)): As @ashwinichaudhary suggested, using s.translate(dict.fromkeys(control_chars)) and the same time taken log outputs: But the code is really slow for my 1GB of text. Is there any other way to clean out controlled characters? <code>  import unicodedata, re, ioall_chars = (unichr(i) for i in xrange(0x110000))control_chars = ''.join(c for c in all_chars if unicodedata.category(c)[0] == 'C')cc_re = re.compile('[%s]' % re.escape(control_chars))def rm_control_chars(s): # see http://www.unicode.org/reports/tr44/#General_Category_Values return cc_re.sub('', s)cleanfile = []with io.open('filename.txt', 'r', encoding='utf8') as fin: for line in fin: line =rm_control_chars(line) cleanfile.append(line) 0.275146961212 2510.672796010971 6140.178567171097 1630.200030088425 1800.236430883408 2150.343492984772 3130.317672967911 2900.160616159439 1420.0732028484344 650.533437013626 4680.260229110718 2360.231380939484 2040.197766065598 1810.283867120743 2580.229172945023 208 0.464188098907 2520.366552114487 6150.407374858856 1640.322507858276 1810.35142993927 2160.319973945618 3140.324357032776 2910.371646165848 1430.354818105698 660.351796150208 4690.388131856918 2370.374715805054 2050.363368988037 1820.425950050354 2590.382766962051 209",Is there a faster way to clean out control characters in a file?
Use options defined in setup.cfg in setup.py," I am using Python 2.7 with Distutils to distribute and install my self-created package. My setup.cfg looks like this: I have two questions: Is it possible to refer to variables set in setup.cfg (but also using command line options) when defining other setup.cfg options? For example, for: I want PREFIX to be the prefix defined either inside setup.cfg or using --prefix command line argument, similar to the interpolation of variables when using ConfigParser.Is it possible to refer to the variables set in the setup.cfg from within my setup.py, without manually parsing the config file using ConfigParser?  <code>  [install]prefix=/usr/local/MODULENAMErecord=installation.txt install-scripts=PREFIX/my-scripts",Referring to existing distutils options inside setup.cfg and setup.py
Given a (python) selenium WebElement can I get the innerText," The following find me my element (a div that ONLY contains some text), but: returns an empty string (which is a surprise). From the Java script console: So, my question is, given that I have some WebElement, can I find the innerText? (For boring reasons, I want to operate with a found webelement, not the original query).I'm including the surrounding HTML. However, I don't think its useful (since element is found and is unique) <code>  element = driver.execute_script(""return $('.theelementclass')"")[0] element.text $('.theelementclass').text # also (consistently) empty$('.theelementclass').innerText # YES! gets the div's text. <first-panel on-click=""onClickLearnMore()"" class=""ng-isolate-scope""> <div class=""comp-onboarding-first-thought-panel""> <div class=""onboarding-text-container""> <div class=""theelementclass""> Congratulations. You found the text is here! </div> <div class=""button-cont""> <div ng-click=""onClickButton()"">Learn more about The Thing</div> </div> </div></div></first-panel>",Given a (python) selenium WebElement can I get the innerText?
Get parent of parent directory from python script, I want to get the parent of current directory from Python script. For example I launch the script from /home/kristina/desire-directory/scripts the desire path in this case is /home/kristina/desire-directoryI know sys.path[0] from sys. But I don't want to parse sys.path[0] resulting string. Is there any another way to get parent of current directory in Python? <code> ,Get parent of current directory from Python script
Get parent of current directory from python script, I want to get the parent of current directory from Python script. For example I launch the script from /home/kristina/desire-directory/scripts the desire path in this case is /home/kristina/desire-directoryI know sys.path[0] from sys. But I don't want to parse sys.path[0] resulting string. Is there any another way to get parent of current directory in Python? <code> ,Get parent of current directory from Python script
Django TEMPLATE_DIRS being ignored," This is driving me crazy. I've done something weird and it appears that my TEMPLATE_DIRS entries are being ignored. I have only one settings.py file, located in the project directory, and it contains: I'm putting project-level templates in the /templates folder, and then have folders for different view categories in my app folder (e.g. authentication views, account views, etc.). For example, my main index page view is in web_app/views/main/views_main.py and looks like where an AppView is just an extension of TemplateView. Here's my problem: when I try to visit the page, I get a TemplateDoesNotExist exception and the part that's really confusing me is the Template-Loader Postmortem: Why in the world are the 'templates' and 'web_app/views' directories not being searched? I've checked Settings via the debugger and a breakpoint in views_main.py and it looks like they're in there. Has anyone had a similar problem? Thanks. <code>  TEMPLATE_DIRS = ( os.path.join(BASE_DIR, 'templates'), os.path.join(BASE_DIR, 'web_app/views/'),) from web_app.views.view_classes import AuthenticatedView, AppViewclass Index(AppView): template_name = ""main/templates/index.html"" Template-loader postmortemDjango tried loading these templates, in this order:Using loader django.template.loaders.filesystem.Loader:Using loader django.template.loaders.app_directories.Loader:C:\Python34\lib\site-packages\django\contrib\admin\templates\main\templates\index.html (File does not exist)C:\Python34\lib\site-packages\django\contrib\auth\templates\main\templates\index.html (File does not exist)",Django 1.8 TEMPLATE_DIRS being ignored
Python: create a new column from existing columns," I am trying to create a new column based on both columns. Say I want to create a new column z, and it should be the value of y when it is not missing and be the value of x when y is indeed missing. So in this case, I expect z to be [1, 8, 10, 8]. <code>  x y0 1 NaN1 2 82 4 103 8 NaN",Python: create a new column from existing columns
Create a new column from existing columns," I am trying to create a new column based on both columns. Say I want to create a new column z, and it should be the value of y when it is not missing and be the value of x when y is indeed missing. So in this case, I expect z to be [1, 8, 10, 8]. <code>  x y0 1 NaN1 2 82 4 103 8 NaN",Python: create a new column from existing columns
pyhton: Use a list as the key in a dict," I have two lists and I need to rank their elements with the Borda positional ranking. so I made this function, but I have this error: As indicated by other answers The problem is that I can't use a list as the key in a dict, since dict keys need to be immutable.So I used a tuple instead, but the error remains. I open the two files as list in this way Can someone help? <code>  TypeError: unhashable type: 'list'. The files of the two lists look like thislist1 = [([('diritti', 'S'), ('umani', 'A')]), ([('violazioni', 'S'), ('dei', 'E'), ('diritti', 'S'), ('umani', 'A')]), ([('forze', 'S'), ('di', 'E'), ('sicurezza', 'S')]), ([('violazioni', 'S'), ('dei', 'E'), ('diritti', 'S')]), ([('Nazioni', 'SP'), ('Unite', 'SP')]), ([('anni', 'S'), ('di', 'E'), ('carcere', 'S')])] list2 = [([('anni', 'S'), ('di', 'E'), ('carcere', 'S')]), ([('diritti', 'S'), ('umani', 'A')]), ([('forze', 'S'), ('di', 'E'), ('sicurezza', 'S')]), ([('violazioni', 'S'), ('dei', 'E'), ('diritti', 'S'), ('umani', 'A')]), ([('violazioni', 'S'), ('dei', 'E'), ('diritti', 'S')]), ([('Nazioni', 'SP'), ('Unite', 'SP')]), ([('uso', 'S'), ('eccessivo', 'A'), ('della', 'E'), ('forza', 'S')])] list1 = codecs.open('/home/list1', 'r', 'utf-8').read()list2 = codecs.open('/home/list2', 'r', 'utf-8').read()li = ast.literal_eval(list1)lii = ast.literal_eval(list2)def borda_sort(lists):###Bordas positional ranking combines ranked lists using information of the ordinal ranks of the elements in each list.Given lists t1, t2, t3 ... tk, for each candidate c and list ti, the score B ti (c) is the number of candidates ranked below c in ti. So The total Borda score is B(c) = B ti (c) The candidates are then sorted by descending Borda scores. Given the lists = [ ['a', 'c'], ['b', 'd', 'a'], ['b', 'a', 'c', 'd'] ], the output will be ['b', 'a', 'c', 'd'] scores = {} for l in lists: for idx, elem in enumerate(reversed(l)): if not elem in scores: scores[elem] = 0 scores[elem] += idx return sorted(scores.keys(), key=lambda elem: scores[elem], reverse=True)lists = zip(li, lii)print borda_sort(lists)",Use a list as the key in a Python dict
Python: Use a list as the key in a dict," I have two lists and I need to rank their elements with the Borda positional ranking. so I made this function, but I have this error: As indicated by other answers The problem is that I can't use a list as the key in a dict, since dict keys need to be immutable.So I used a tuple instead, but the error remains. I open the two files as list in this way Can someone help? <code>  TypeError: unhashable type: 'list'. The files of the two lists look like thislist1 = [([('diritti', 'S'), ('umani', 'A')]), ([('violazioni', 'S'), ('dei', 'E'), ('diritti', 'S'), ('umani', 'A')]), ([('forze', 'S'), ('di', 'E'), ('sicurezza', 'S')]), ([('violazioni', 'S'), ('dei', 'E'), ('diritti', 'S')]), ([('Nazioni', 'SP'), ('Unite', 'SP')]), ([('anni', 'S'), ('di', 'E'), ('carcere', 'S')])] list2 = [([('anni', 'S'), ('di', 'E'), ('carcere', 'S')]), ([('diritti', 'S'), ('umani', 'A')]), ([('forze', 'S'), ('di', 'E'), ('sicurezza', 'S')]), ([('violazioni', 'S'), ('dei', 'E'), ('diritti', 'S'), ('umani', 'A')]), ([('violazioni', 'S'), ('dei', 'E'), ('diritti', 'S')]), ([('Nazioni', 'SP'), ('Unite', 'SP')]), ([('uso', 'S'), ('eccessivo', 'A'), ('della', 'E'), ('forza', 'S')])] list1 = codecs.open('/home/list1', 'r', 'utf-8').read()list2 = codecs.open('/home/list2', 'r', 'utf-8').read()li = ast.literal_eval(list1)lii = ast.literal_eval(list2)def borda_sort(lists):###Bordas positional ranking combines ranked lists using information of the ordinal ranks of the elements in each list.Given lists t1, t2, t3 ... tk, for each candidate c and list ti, the score B ti (c) is the number of candidates ranked below c in ti. So The total Borda score is B(c) = B ti (c) The candidates are then sorted by descending Borda scores. Given the lists = [ ['a', 'c'], ['b', 'd', 'a'], ['b', 'a', 'c', 'd'] ], the output will be ['b', 'a', 'c', 'd'] scores = {} for l in lists: for idx, elem in enumerate(reversed(l)): if not elem in scores: scores[elem] = 0 scores[elem] += idx return sorted(scores.keys(), key=lambda elem: scores[elem], reverse=True)lists = zip(li, lii)print borda_sort(lists)",Use a list as the key in a Python dict
"Python: scatterplot with different size, marker, and color from pandas dataframe"," I am trying to do a scatter plot with speed over meters for each point where marker indicate different types, size indicate different weights and color indicate how old a point is over 10 minutes scale. However, I was only able to plot by size so far.Any help is highly appreciated. Updated question:I am trying to add colorbar to the color scale based on old. it worked when I plot against the entire dataset but failed after trying to add marker for each subset. Any idea? TypeError: You must first set_array for mappable <code>  x = {'speed': [10, 15, 20, 18, 19], 'meters' : [122, 150, 190, 230, 300], 'type': ['phone', 'phone', 'gps', 'gps', 'car'], 'weight': [0.2, 0.3, 0.1, 0.85, 0.0], 'old': [1, 2, 4, 5, 8]}m = pd.DataFrame(x)plt.scatter(m.meters, m.speed, s = 30* m.weight)mkr_dict = {'gps': 'x', 'phone': '+', 'car': 'o'} meters speed type weight old0 122 10 phone 0.20 11 150 15 phone 0.30 22 190 20 gps 0.10 43 230 18 gps 0.85 54 300 19 car 0.00 8 plt.scatter(m.meters, m.speed, s = 30* m.weight, c=m.old)cbar = plt.colorbar(ticks = [0, 5, 10])cbar.ax.set_yticklabels(['New','5mins', '10mins'])","Scatterplot with different size, marker, and color from pandas dataframe"
How to remove python in /usr/local/bins/, I have installed Python 2.7.9 in /usr/local/bin. Now it doesn't work any more. I have another Python in /usr/bin/ but in the path is /usr/local/bin/ first. How can i remove the 2.7.9 Python? <code> ,How to remove python in /usr/local/bin/
How to remove python in /usr/local/bin/, I have installed Python 2.7.9 in /usr/local/bin. Now it doesn't work any more. I have another Python in /usr/bin/ but in the path is /usr/local/bin/ first. How can i remove the 2.7.9 Python? <code> ,How to remove python in /usr/local/bin/
How to remove python in /usr/local/bins/, I have installed Python 2.7.9 in /usr/local/bin. Now it doesn't work any more. I have another Python in /usr/bin/ but in the path is /usr/local/bin/ first. How can i remove the 2.7.9 Python? <code> ,How to remove python in /usr/local/bin/
DJango forms - how to override field validation," In a form I have certain fields, that are not validated correctly. I wish to override the django validation and put my own instead. When I override the clean() method, the field self.errors is already filled with errors from the wrongly validated fields. Which method should I override, where are these errors generated?With overriding clean() and similar methods, one can achieve only extension to the django default validation. I want to prevent this default validation from happening.Edit: tried validatorsHere is what I tried: Those validators are not even called. Seems like the error-seeking process ends at the validate() function call, which happens before any validators are called.Edit: posting the whole class In clean() I can see that the errors are there, save() is never executed. <code>  253 def validate_gallery(value):254 print 'validate galley'255 return True256 257 def validate_cover_photo(value):258 print 'validate_cf'259 return True260 261 cover_photo_widget = SelectWithDefaultOptions(attrs={262 'class': 'chosen-select-no-single',263 'id': 'select-cover-photo',264 'data-placeholder': 'Select Cover Photo',265 'style': 'width: 200px;',266 'tabindex': '-1',267 });268 269 gallery_widget = SelectWithDefaultOptions(attrs={270 'class': 'chosen-select-no-single',271 'id': 'select-galley',272 'data-placeholder': 'Select Gallery',273 'style': 'width: 200px;',274 'gallery-select': '',275 'tabindex': '-1',276 'organisator-profile-specific': '',277 });278 279 gallery = forms.ChoiceField(widget = gallery_widget, validators = [validate_gallery])280 cover_photo = forms.ChoiceField(widget = cover_photo_widget, validators = [validate_cover_photo]) 240 def validate_gallery(value):241 print 'validate galley'242 return True243 244 def validate_cover_photo(value):245 print 'validate_cf'246 return True247 248 class EventDetailForm(NgFormValidationMixin, NgModelForm):249 def __init__(self, *args, **kwargs):250 super(EventDetailForm, self).__init__(*args, **kwargs)251 self.fields['end_date'].required = False252 self.fields['description'].required = False253 self.fields['start_date'].input_formats = DATE_TIME_INPUT_FORMATS254 self.fields['end_date'].input_formats = DATE_TIME_INPUT_FORMATS255 256 arguments_length = len(args)257 if arguments_length > 0:258 post_data = args[0]259 self.old_title = post_data.get('old_title', None)260 261 cover_photo_widget = SelectWithDefaultOptions(attrs={262 'class': 'chosen-select-no-single',263 'id': 'select-cover-photo',264 'data-placeholder': 'Select Cover Photo',265 'style': 'width: 200px;',266 'tabindex': '-1',267 });268 269 gallery_widget = SelectWithDefaultOptions(attrs={270 'class': 'chosen-select-no-single',271 'id': 'select-galley',272 'data-placeholder': 'Select Gallery',273 'style': 'width: 200px;',274 'gallery-select': '',275 'tabindex': '-1',276 'organisator-profile-specific': '',277 });278 279 gallery = forms.ChoiceField(widget = gallery_widget, validators = [validate_gallery])280 cover_photo = forms.ChoiceField(widget = cover_photo_widget, validators = [validate_cover_photo])281 282 class Meta:283 model = Event284 fields = ('title', 'description', 'end_date', 'start_date')285 widgets = {286 'title': forms.TextInput(attrs={287 'editable-detail': '',288 }),289 'description': forms.TextInput(attrs={290 'class': 'panel-body',291 'id': 'event-description-editable',292 'editable-detail': '',293 }),294 'start_date': DateTimeWidget(attrs = {295 'class': 'datetimepicker col-xs-6',296 'id': 'event-start-date-editable',297 'editable-detail': '',298 }),299 'end_date': DateTimeWidget(attrs = {300 'class': 'datetimepicker col-xs-6',301 'id': 'event-end-date-editable',302 'editable-detail': '',303 }),304 }305 306 def clean(self):307 cleaned_data = self.cleaned_data308 309 print self.errors310 311 return cleaned_data312 313 def save(self, commit=True):314 old_title = self.old_title315 event = Event()316 317 cover_photo_title = self.cleaned_data['cover_photo']318 cover_photo = Photo.objects.filter(title=cover_photo_title)319 320 gallery_title = self.cleaned_data['gallery']321 gallery = Gallery.objects.filter(title=gallery_title)322 323 event.title = self.cleaned_data['title']324 event.description = self.cleaned_data['desription']325 event.start_date = self.cleaned_date['start_date']326 event.end_date = self.cleaned_data['end_date']327 event.cover_photo = cover_photo328 event.gallery = gallery329 330 if commit:331 event.save()332 333 return event334 ",Django forms - how to override field validation
python fuzzy text search," I am wondering if there is a Python library can conduct fuzzy text search. For example:I have three keywords ""letter"", ""stamp"", and ""mail"".I would like to have a function to check if those three words are withinthe same paragraph (or certain distances, one page).In addition, those words have to maintain the same order. It is fine that other words appear between those three words.I have tried fuzzywuzzy which did not solve my problem. Another library, Whoosh, looks powerful, but I did not find the proper function. <code> ",Fuzzy text search in Python
Fuzzy text search in python," I am wondering if there is a Python library can conduct fuzzy text search. For example:I have three keywords ""letter"", ""stamp"", and ""mail"".I would like to have a function to check if those three words are withinthe same paragraph (or certain distances, one page).In addition, those words have to maintain the same order. It is fine that other words appear between those three words.I have tried fuzzywuzzy which did not solve my problem. Another library, Whoosh, looks powerful, but I did not find the proper function. <code> ",Fuzzy text search in Python
"What is this syntax ""..."" in blinder"," I was looking at the source code of a Blender add-on and I saw a new syntax: What is the meaning of ...? <code>  def elem_name_ensure_class(elem, clss=...): elem_name, elem_class = elem_split_name_class(elem) if clss is not ...: assert(elem_class == clss) return elem_name.decode('utf-8')","What is this syntax ""..."" (ellipsis)?"
"What is this syntax ""..."" in blender"," I was looking at the source code of a Blender add-on and I saw a new syntax: What is the meaning of ...? <code>  def elem_name_ensure_class(elem, clss=...): elem_name, elem_class = elem_split_name_class(elem) if clss is not ...: assert(elem_class == clss) return elem_name.decode('utf-8')","What is this syntax ""..."" (ellipsis)?"
text posisiton by axes fraction," Is there a way to position text in a figure by the fraction of the axis? I want the text in the same position for all my plots regardless of their differing ranges in x and y. This functionality is in ax.annotate() but I need to put in the extra 'xy' argument which makes my code harder to read.  <code>  import matplotlib.pyplot as pltimport numpy as npfig = plt.figure() ax = fig.add_subplot(1, 1, 1)ax.plot(np.arange(10),12*np.arange(10)) ax.annotate('Correct Position', xy=(0, 0), xytext=(0.4, 0.7), textcoords='axes fraction')ax.text(0.4, 0.7, 'Incorrect Position')plt.show()",Positioning text by axis fraction
Pyhton: No Module named suds," I have python 2.7.9 installed.When I run my script, I get the following error: So I downloaded the suds library from: https://fedorahosted.org/releases/s/u/suds/python-suds-0.4.tar.gzand extracted it and installed it using command (python setup.py install).The installed suds file is placed in Directory C:\Python27\Lib\site-packages as suds-0.4-py2.7.egg.But when i again run my script, I found the same error. Am I missing something ? <code>  from suds.client import client ImportError: No module named suds.client",Python: No Module named suds
Returning a String from Rust Function to Python," I'm very new to Rust. How would I return a String from a Rust function that can be used in Python?Here is my Rust implementation: And the Python code that calls it: I get a segmentation fault when its run.EDIT: Using Vladimir Matveev's Rust code below I was able to get it to work with the changes to my python code: <code>  use std::ffi::CString;#[no_mangle]pub extern fn query() -> CString { let s = CString::new(""Hello!"").unwrap(); return s;} from ctypes import cdll, c_char_plib = cdll.LoadLibrary(""target/release/libtest.so"")result = lib.query()print(c_char_p(result).value) from ctypes import *lib = cdll.LoadLibrary(""target/release/libtest.so"")lib.query.restype = c_char_presult = lib.query()print cast(result, c_char_p).valuelib.free_query(result)",Returning a String from Rust function to Python
How to include a multi-index when creating a pandas dataframe with .concat," UPDATE: This is no longer an issue since at least pandas version 0.18.1. Concatenating empty series doesn't drop them anymore so this question is out of date.I want to create a pandas dataframe from a list of series using .concat. The problem is that when one of the series is empty it doesn't get included in the resulting dataframe but this makes the dataframe be the wrong dimensions when I then try to rename its columns with a multi-index.UPDATE: Here's an example... This produces the following dataframe: But I want it to produce something like this: It does this if I put a single nan value anywhere in ser1 but it seems like this should be possible automatically even if some of my series are totally empty. <code>  import pandas as pdsers1 = pd.Series()sers2 = pd.Series(['a', 'b', 'c'])df1 = pd.concat([sers1, sers2], axis=1) >>> df10 a1 b2 cdtype: object >>> df2 0 10 NaN a1 NaN b2 NaN c",Include empty series when creating a pandas dataframe with .concat
How to include empty series when creating a pandas dataframe with .concat," UPDATE: This is no longer an issue since at least pandas version 0.18.1. Concatenating empty series doesn't drop them anymore so this question is out of date.I want to create a pandas dataframe from a list of series using .concat. The problem is that when one of the series is empty it doesn't get included in the resulting dataframe but this makes the dataframe be the wrong dimensions when I then try to rename its columns with a multi-index.UPDATE: Here's an example... This produces the following dataframe: But I want it to produce something like this: It does this if I put a single nan value anywhere in ser1 but it seems like this should be possible automatically even if some of my series are totally empty. <code>  import pandas as pdsers1 = pd.Series()sers2 = pd.Series(['a', 'b', 'c'])df1 = pd.concat([sers1, sers2], axis=1) >>> df10 a1 b2 cdtype: object >>> df2 0 10 NaN a1 NaN b2 NaN c",Include empty series when creating a pandas dataframe with .concat
understudying asyncio already running forever loop and pending tasks," I'm having problems understanding how to pend a new task to an already running event loop. This code: Changing run_forever() to run_until_complete(asyncio.async(blocking(""ls"")) works fine. But I'm really confused - why I can't pend a task on the already running loop? <code>  import asyncioimport logging@asyncio.coroutinedef blocking(cmd): while True: logging.info(""in blocking coroutine"") yield from asyncio.sleep(0.01) print(""ping"")def main(): logging.info(""in main funciton"") loop = asyncio.get_event_loop() logging.info(""new loop created"") logging.info(""loop running forever"") loop.run_forever() asyncio.async(blocking(""ls""))logging.basicConfig(level = logging.INFO)main()",understanding asyncio already running forever loop and pending tasks
Flask request.args.get() url parameters - Only getting 1st parameter," I am using curl to make a request to a Flask route that expects multiple query params. However, the log shows only the first param in the url, and Flask doesn't see the second param. What is going wrong?  <code>  @app.route('/path', methods=['GET'])def foo(): print request.args.get('param2') req = request.args.items() print req curl http://localhost:5000/path?param1=1&param2=2 127.0.0.1 - - [01/Jun/2015 21:35:10] ""GET /path?param1=1 HTTP/1.1"" 200 -None[('param1', u'1')]",Flask only sees first parameter from multiple parameters sent with curl
Python.h fatal error: limits.h: No such file or directory #include <limits.h>," I'm running alpine-linux on a Raspberry Pi 2. I'm trying to install Pillow via this command: This is the output from the command: I think this is probably the relevant section: My research shows it's probably something with the header files. I have installed these: <code>  pip install pillow Installing collected packages: pillowRunning setup.py install for pillow Complete output from command /usr/bin/python -c ""import setuptools, tokenize;__file__='/tmp/pip-build-gNq0WA/pillow/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-nDKwei-record/install-record.txt --single-version-externally-managed --compile: running install running build running build_py creating build creating build/lib.linux-armv7l-2.7 creating build/lib.linux-armv7l-2.7/PIL copying PIL/XVThumbImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/XpmImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/XbmImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/WmfImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/WebPImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/WalImageFile.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/TiffTags.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/TiffImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/TgaImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/TarIO.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/SunImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/SpiderImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/SgiImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/PyAccess.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/PSDraw.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/PsdImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/PpmImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/PngImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/PixarImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/PdfImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/PcxImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/PcfFontFile.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/PcdImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/PalmImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/PaletteFile.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/OleFileIO.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/MspImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/MpoImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/MpegImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/MicImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/McIdasImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/JpegPresets.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/JpegImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/Jpeg2KImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/IptcImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImtImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageWin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageTransform.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageTk.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageStat.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageShow.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageSequence.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageQt.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImagePath.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImagePalette.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageOps.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageMorph.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageMode.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageMath.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageGrab.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageFont.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageFilter.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageFileIO.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageFile.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageEnhance.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageDraw2.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageDraw.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageColor.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageCms.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ImageChops.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/Image.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/IcoImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/IcnsImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/Hdf5StubImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/GribStubImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/GimpPaletteFile.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/GimpGradientFile.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/GifImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/GdImageFile.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/GbrImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/FpxImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/FontFile.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/FliImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/FitsStubImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ExifTags.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/EpsImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/DcxImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/CurImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/ContainerIO.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/BufrStubImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/BmpImagePlugin.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/BdfFontFile.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/_util.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/_binary.py -> build/lib.linux-armv7l-2.7/PIL copying PIL/__init__.py -> build/lib.linux-armv7l-2.7/PIL running egg_info writing Pillow.egg-info/PKG-INFO writing top-level names to Pillow.egg-info/top_level.txt writing dependency_links to Pillow.egg-info/dependency_links.txt warning: manifest_maker: standard file '-c' not found reading manifest file 'Pillow.egg-info/SOURCES.txt' reading manifest template 'MANIFEST.in' warning: no files found matching 'LICENSE' under directory 'docs' writing manifest file 'Pillow.egg-info/SOURCES.txt' copying PIL/OleFileIO-README.md -> build/lib.linux-armv7l-2.7/PIL running build_ext building 'PIL._imaging' extension creating build/temp.linux-armv7l-2.7/libImaging gcc -fno-strict-aliasing -Os -fomit-frame-pointer -DNDEBUG -Os -fomit-frame-pointer -fPIC -DHAVE_LIBJPEG -I/tmp/pip-build-gNq0WA/pillow/libImaging -I/usr/include -I/usr/include/python2.7 -c _imaging.c -o build/temp.linux-armv7l-2.7/_imaging.o gcc -fno-strict-aliasing -Os -fomit-frame-pointer -DNDEBUG -Os -fomit-frame-pointer -fPIC -DHAVE_LIBJPEG -I/tmp/pip-build-gNq0WA/pillow/libImaging -I/usr/include -I/usr/include/python2.7 -c outline.c -o build/temp.linux-armv7l-2.7/outline.o gcc -fno-strict-aliasing -Os -fomit-frame-pointer -DNDEBUG -Os -fomit-frame-pointer -fPIC -DHAVE_LIBJPEG -I/tmp/pip-build-gNq0WA/pillow/libImaging -I/usr/include -I/usr/include/python2.7 -c libImaging/Bands.c -o build/temp.linux-armv7l-2.7/libImaging/Bands.o gcc -fno-strict-aliasing -Os -fomit-frame-pointer -DNDEBUG -Os -fomit-frame-pointer -fPIC -DHAVE_LIBJPEG -I/tmp/pip-build-gNq0WA/pillow/libImaging -I/usr/include -I/usr/include/python2.7 -c libImaging/ConvertYCbCr.c -o build/temp.linux-armv7l-2.7/libImaging/ConvertYCbCr.o In file included from _imaging.c:76:0: /usr/include/python2.7/Python.h:19:20: fatal error: limits.h: No such file or directory #include <limits.h> ^ compilation terminated. In file included from outline.c:20:0: /usr/include/python2.7/Python.h:19:20: fatal error: limits.h: No such file or directory #include <limits.h> ^ compilation terminated. In file included from libImaging/ImPlatform.h:10:0, from libImaging/Imaging.h:14, from libImaging/ConvertYCbCr.c:15: /usr/include/python2.7/Python.h:19:20: fatal error: limits.h: No such file or directory #include <limits.h> ^ compilation terminated. In file included from libImaging/ImPlatform.h:10:0, from libImaging/Imaging.h:14, from libImaging/Bands.c:19: /usr/include/python2.7/Python.h:19:20: fatal error: limits.h: No such file or directory #include <limits.h> ^ compilation terminated. gcc -fno-strict-aliasing -Os -fomit-frame-pointer -DNDEBUG -Os -fomit-frame-pointer -fPIC -DHAVE_LIBJPEG -I/tmp/pip-build-gNq0WA/pillow/libImaging -I/usr/include -I/usr/include/python2.7 -c libImaging/Draw.c -o build/temp.linux-armv7l-2.7/libImaging/Draw.o gcc -fno-strict-aliasing -Os -fomit-frame-pointer -DNDEBUG -Os -fomit-frame-pointer -fPIC -DHAVE_LIBJPEG -I/tmp/pip-build-gNq0WA/pillow/libImaging -I/usr/include -I/usr/include/python2.7 -c libImaging/Filter.c -o build/temp.linux-armv7l-2.7/libImaging/Filter.o gcc -fno-strict-aliasing -Os -fomit-frame-pointer -DNDEBUG -Os -fomit-frame-pointer -fPIC -DHAVE_LIBJPEG -I/tmp/pip-build-gNq0WA/pillow/libImaging -I/usr/include -I/usr/include/python2.7 -c libImaging/GifEncode.c -o build/temp.linux-armv7l-2.7/libImaging/GifEncode.o gcc -fno-strict-aliasing -Os -fomit-frame-pointer -DNDEBUG -Os -fomit-frame-pointer -fPIC -DHAVE_LIBJPEG -I/tmp/pip-build-gNq0WA/pillow/libImaging -I/usr/include -I/usr/include/python2.7 -c libImaging/LzwDecode.c -o build/temp.linux-armv7l-2.7/libImaging/LzwDecode.o In file included from libImaging/ImPlatform.h:10:0, from libImaging/Imaging.h:14, from libImaging/Draw.c:35: /usr/include/python2.7/Python.h:19:20: fatal error: limits.h: No such file or directory #include <limits.h> ^ compilation terminated. In file included from libImaging/ImPlatform.h:10:0, from libImaging/Imaging.h:14, from libImaging/Filter.c:27: /usr/include/python2.7/Python.h:19:20: fatal error: limits.h: No such file or directory #include <limits.h> ^ compilation terminated. In file included from libImaging/ImPlatform.h:10:0, from libImaging/Imaging.h:14, from libImaging/GifEncode.c:20: /usr/include/python2.7/Python.h:19:20: fatal error: limits.h: No such file or directory #include <limits.h> ^ compilation terminated. In file included from libImaging/ImPlatform.h:10:0, from libImaging/Imaging.h:14, from libImaging/LzwDecode.c:31: /usr/include/python2.7/Python.h:19:20: fatal error: limits.h: No such file or directory #include <limits.h> ^ compilation terminated. gcc -fno-strict-aliasing -Os -fomit-frame-pointer -DNDEBUG -Os -fomit-frame-pointer -fPIC -DHAVE_LIBJPEG -I/tmp/pip-build-gNq0WA/pillow/libImaging -I/usr/include -I/usr/include/python2.7 -c libImaging/Offset.c -o build/temp.linux-armv7l-2.7/libImaging/Offset.o gcc -fno-strict-aliasing -Os -fomit-frame-pointer -DNDEBUG -Os -fomit-frame-pointer -fPIC -DHAVE_LIBJPEG -I/tmp/pip-build-gNq0WA/pillow/libImaging -I/usr/include -I/usr/include/python2.7 -c libImaging/Quant.c -o build/temp.linux-armv7l-2.7/libImaging/Quant.o gcc -fno-strict-aliasing -Os -fomit-frame-pointer -DNDEBUG -Os -fomit-frame-pointer -fPIC -DHAVE_LIBJPEG -I/tmp/pip-build-gNq0WA/pillow/libImaging -I/usr/include -I/usr/include/python2.7 -c libImaging/PcxDecode.c -o build/temp.linux-armv7l-2.7/libImaging/PcxDecode.o gcc -fno-strict-aliasing -Os -fomit-frame-pointer -DNDEBUG -Os -fomit-frame-pointer -fPIC -DHAVE_LIBJPEG -I/tmp/pip-build-gNq0WA/pillow/libImaging -I/usr/include -I/usr/include/python2.7 -c libImaging/RawEncode.c -o build/temp.linux-armv7l-2.7/libImaging/RawEncode.o In file included from libImaging/ImPlatform.h:10:0, from libImaging/Imaging.h:14, from libImaging/Offset.c:18: /usr/include/python2.7/Python.h:19:20: fatal error: limits.h: No such file or directory #include <limits.h> ^ compilation terminated. In file included from libImaging/ImPlatform.h:10:0, from libImaging/Imaging.h:14, from libImaging/Quant.c:21: /usr/include/python2.7/Python.h:19:20: fatal error: limits.h: No such file or directory #include <limits.h> ^ compilation terminated. In file included from libImaging/ImPlatform.h:10:0, from libImaging/Imaging.h:14, from libImaging/PcxDecode.c:17: /usr/include/python2.7/Python.h:19:20: fatal error: limits.h: No such file or directory #include <limits.h> ^ compilation terminated. In file included from libImaging/ImPlatform.h:10:0, from libImaging/Imaging.h:14, from libImaging/RawEncode.c:21: /usr/include/python2.7/Python.h:19:20: fatal error: limits.h: No such file or directory #include <limits.h> ^ compilation terminated. gcc -fno-strict-aliasing -Os -fomit-frame-pointer -DNDEBUG -Os -fomit-frame-pointer -fPIC -DHAVE_LIBJPEG -I/tmp/pip-build-gNq0WA/pillow/libImaging -I/usr/include -I/usr/include/python2.7 -c libImaging/UnpackYCC.c -o build/temp.linux-armv7l-2.7/libImaging/UnpackYCC.o gcc -fno-strict-aliasing -Os -fomit-frame-pointer -DNDEBUG -Os -fomit-frame-pointer -fPIC -DHAVE_LIBJPEG -I/tmp/pip-build-gNq0WA/pillow/libImaging -I/usr/include -I/usr/include/python2.7 -c libImaging/ZipEncode.c -o build/temp.linux-armv7l-2.7/libImaging/ZipEncode.o gcc -fno-strict-aliasing -Os -fomit-frame-pointer -DNDEBUG -Os -fomit-frame-pointer -fPIC -DHAVE_LIBJPEG -I/tmp/pip-build-gNq0WA/pillow/libImaging -I/usr/include -I/usr/include/python2.7 -c libImaging/BoxBlur.c -o build/temp.linux-armv7l-2.7/libImaging/BoxBlur.o In file included from libImaging/ImPlatform.h:10:0, from libImaging/Imaging.h:14, from libImaging/UnpackYCC.c:17: /usr/include/python2.7/Python.h:19:20: fatal error: limits.h: No such file or directory #include <limits.h> ^ compilation terminated. In file included from libImaging/ImPlatform.h:10:0, from libImaging/Imaging.h:14, from libImaging/ZipEncode.c:18: /usr/include/python2.7/Python.h:19:20: fatal error: limits.h: No such file or directory #include <limits.h> ^ compilation terminated. In file included from libImaging/BoxBlur.c:1:0: /usr/include/python2.7/Python.h:19:20: fatal error: limits.h: No such file or directory #include <limits.h> ^ compilation terminated. Building using 4 processes gcc -shared -Wl,--as-needed build/temp.linux-armv7l-2.7/_imaging.o build/temp.linux-armv7l-2.7/decode.o build/temp.linux-armv7l-2.7/encode.o build/temp.linux-armv7l-2.7/map.o build/temp.linux-armv7l-2.7/display.o build/temp.linux-armv7l-2.7/outline.o build/temp.linux-armv7l-2.7/path.o build/temp.linux-armv7l-2.7/libImaging/Access.o build/temp.linux-armv7l-2.7/libImaging/AlphaComposite.o build/temp.linux-armv7l-2.7/libImaging/Resample.o build/temp.linux-armv7l-2.7/libImaging/Bands.o build/temp.linux-armv7l-2.7/libImaging/BitDecode.o build/temp.linux-armv7l-2.7/libImaging/Blend.o build/temp.linux-armv7l-2.7/libImaging/Chops.o build/temp.linux-armv7l-2.7/libImaging/Convert.o build/temp.linux-armv7l-2.7/libImaging/ConvertYCbCr.o build/temp.linux-armv7l-2.7/libImaging/Copy.o build/temp.linux-armv7l-2.7/libImaging/Crc32.o build/temp.linux-armv7l-2.7/libImaging/Crop.o build/temp.linux-armv7l-2.7/libImaging/Dib.o build/temp.linux-armv7l-2.7/libImaging/Draw.o build/temp.linux-armv7l-2.7/libImaging/Effects.o build/temp.linux-armv7l-2.7/libImaging/EpsEncode.o build/temp.linux-armv7l-2.7/libImaging/File.o build/temp.linux-armv7l-2.7/libImaging/Fill.o build/temp.linux-armv7l-2.7/libImaging/Filter.o build/temp.linux-armv7l-2.7/libImaging/FliDecode.o build/temp.linux-armv7l-2.7/libImaging/Geometry.o build/temp.linux-armv7l-2.7/libImaging/GetBBox.o build/temp.linux-armv7l-2.7/libImaging/GifDecode.o build/temp.linux-armv7l-2.7/libImaging/GifEncode.o build/temp.linux-armv7l-2.7/libImaging/HexDecode.o build/temp.linux-armv7l-2.7/libImaging/Histo.o build/temp.linux-armv7l-2.7/libImaging/JpegDecode.o build/temp.linux-armv7l-2.7/libImaging/JpegEncode.o build/temp.linux-armv7l-2.7/libImaging/LzwDecode.o build/temp.linux-armv7l-2.7/libImaging/Matrix.o build/temp.linux-armv7l-2.7/libImaging/ModeFilter.o build/temp.linux-armv7l-2.7/libImaging/MspDecode.o build/temp.linux-armv7l-2.7/libImaging/Negative.o build/temp.linux-armv7l-2.7/libImaging/Offset.o build/temp.linux-armv7l-2.7/libImaging/Pack.o build/temp.linux-armv7l-2.7/libImaging/PackDecode.o build/temp.linux-armv7l-2.7/libImaging/Palette.o build/temp.linux-armv7l-2.7/libImaging/Paste.o build/temp.linux-armv7l-2.7/libImaging/Quant.o build/temp.linux-armv7l-2.7/libImaging/QuantOctree.o build/temp.linux-armv7l-2.7/libImaging/QuantHash.o build/temp.linux-armv7l-2.7/libImaging/QuantHeap.o build/temp.linux-armv7l-2.7/libImaging/PcdDecode.o build/temp.linux-armv7l-2.7/libImaging/PcxDecode.o build/temp.linux-armv7l-2.7/libImaging/PcxEncode.o build/temp.linux-armv7l-2.7/libImaging/Point.o build/temp.linux-armv7l-2.7/libImaging/RankFilter.o build/temp.linux-armv7l-2.7/libImaging/RawDecode.o build/temp.linux-armv7l-2.7/libImaging/RawEncode.o build/temp.linux-armv7l-2.7/libImaging/Storage.o build/temp.linux-armv7l-2.7/libImaging/SunRleDecode.o build/temp.linux-armv7l-2.7/libImaging/TgaRleDecode.o build/temp.linux-armv7l-2.7/libImaging/Unpack.o build/temp.linux-armv7l-2.7/libImaging/UnpackYCC.o build/temp.linux-armv7l-2.7/libImaging/UnsharpMask.o build/temp.linux-armv7l-2.7/libImaging/XbmDecode.o build/temp.linux-armv7l-2.7/libImaging/XbmEncode.o build/temp.linux-armv7l-2.7/libImaging/ZipDecode.o build/temp.linux-armv7l-2.7/libImaging/ZipEncode.o build/temp.linux-armv7l-2.7/libImaging/TiffDecode.o build/temp.linux-armv7l-2.7/libImaging/Incremental.o build/temp.linux-armv7l-2.7/libImaging/Jpeg2KDecode.o build/temp.linux-armv7l-2.7/libImaging/Jpeg2KEncode.o build/temp.linux-armv7l-2.7/libImaging/BoxBlur.o -L/usr/lib -L/usr/local/lib -L/usr/lib -ljpeg -lpython2.7 -o build/lib.linux-armv7l-2.7/PIL/_imaging.so gcc: error: build/temp.linux-armv7l-2.7/_imaging.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/decode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/encode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/map.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/display.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/outline.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/path.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Access.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/AlphaComposite.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Resample.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Bands.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/BitDecode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Blend.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Chops.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Convert.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/ConvertYCbCr.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Copy.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Crc32.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Crop.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Dib.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Draw.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Effects.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/EpsEncode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/File.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Fill.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Filter.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/FliDecode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Geometry.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/GetBBox.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/GifDecode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/GifEncode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/HexDecode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Histo.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/JpegDecode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/JpegEncode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/LzwDecode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Matrix.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/ModeFilter.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/MspDecode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Negative.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Offset.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Pack.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/PackDecode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Palette.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Paste.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Quant.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/QuantOctree.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/QuantHash.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/QuantHeap.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/PcdDecode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/PcxDecode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/PcxEncode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Point.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/RankFilter.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/RawDecode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/RawEncode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Storage.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/SunRleDecode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/TgaRleDecode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Unpack.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/UnpackYCC.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/UnsharpMask.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/XbmDecode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/XbmEncode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/ZipDecode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/ZipEncode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/TiffDecode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Incremental.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Jpeg2KDecode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/Jpeg2KEncode.o: No such file or directory gcc: error: build/temp.linux-armv7l-2.7/libImaging/BoxBlur.o: No such file or directory error: command 'gcc' failed with exit status 1 ----------------------------------------Command ""/usr/bin/python -c ""import setup tools, tokenize;__file__='/tmp/pip-build-gNq0WA/pillow/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-nDKwei-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-gNq0WA/pillow In file included from libImaging/BoxBlur.c:1:0: /usr/include/python2.7/Python.h:19:20: fatal error: limits.h: No such file or directory #include <limits.h> ^ compilation terminated. apk add py-configobj libusb py-pip python-dev gcc linux-headerspip install --upgrade pippip install -U setuptoolspip install Cheetahpip install pyusb","No such file or directory ""limits.h"" when installing Pillow on Alpine Linux"
Anaconda python accessible to normal and root users in shell but not root user in command line," I installed Anaconda in a Google Cloud Compute environment and can use it successfully from the shell as a normal user: However, when I start an interpreter via sudo python, anaconda is not the interpreter used, and I would like it to be. Confusingly, when I start a shell as root and then start an interpreter, anaconda is the interpreter used. I have export PATH=""/anaconda/bin:$PATH"" in both the root's and my normal account's .bashrc files. At first I thought the issue was sudo python not actually starting a root shell, and thus the export PATH=""/anaconda/bin:$PATH"" not actually being done. But when from my normal account I do sudo echo $PATH, it shows anaconda in there: The anaconda installation was installed as root in /anaconda and I did a chmod -R 770 /anaconda to make it accessible to normal users, but I don't think this problem has anything to do with that.How can I get anaconda to be the default interpreter when run from a sudo command line? <code>  curt@lamp-v5mi:~$ pythonPython 2.7.9 |Anaconda 2.2.0 (64-bit)| (default, Mar 9 2015, 16:20:48) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.Anaconda is brought to you by Continuum Analytics.Please check out: http://continuum.io/thanks and https://binstar.org curt@lamp-v5mi:~$ sudo pythonPython 2.7.3 (default, Mar 13 2014, 11:03:55) [GCC 4.7.2] on linux2Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. curt@lamp-v5mi:~$ sudo -sroot@lamp-v5mi:/home/curt# pythonPython 2.7.9 |Anaconda 2.2.0 (64-bit)| (default, Mar 9 2015, 16:20:48) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.Anaconda is brought to you by Continuum Analytics.Please check out: http://continuum.io/thanks and https://binstar.org curt@lamp-v5mi:~$ sudo echo $PATH/anaconda/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games",Anaconda python not available from sudo
Python - Splitting a large string based on number of a specified delimeter," I'm still learning Python, and I have a question I haven't been able to solve. I have a very long string (millions of lines long) which I would like to be split into a smaller string length based on a specified number of occurrences of a delimeter.For instance: In this case I would want to split based on ""//"" and return a string of all lines before the nth occurrence of the delimeter.So an input of splitting the string by // by 1 would return: an input of splitting the string by // by 2 would return: an input of splitting the string by // by 3 would return: And so on... However, The length of the original 2 million line string appeared to be a problem when I simply tried to split the entire string and by ""//"" and just work with the individual indexes. (I was getting a memory error) Perhaps Python can't handle so many lines in one split? So I can't do that. I'm looking for a way that I don't need to split the entire string into a hundred-thousand indexes when I may only need 100, but instead just start from the beginning until a certain point, stop and return everything before it, which I assume may also be faster? I hope my question is as clear as possible.Is there a simple or elegant way to achieve this? Thanks! <code>  ABCDEF//GHIJKLMN//OPQ//RSTLN//OPQR//STUVW//XYZ// ABCDEF ABCDEF//GHIJKLMN ABCDEF//GHIJKLMN//OPQ",Python - Splitting a large string by number of delimiter occurrences
How to display a variable determined by a random number?," I have a list of names, and I would like my program to randomly select one of those names. I tried using the following: I also tried making each of the numbers as strings, using the eval()function for randrange(), but none of this worked. <code>  import randomdef main(): Arkansas = 1 Manchuria = 2 Bengal = ""3"" Baja_California = 4 Tibet = 5 Indonesia = 6 Cascade_Range = 7 Hudson_Bay = 8 High_Plains = 9 map = random.randrange(1, 10) print(map)main()",Print a variable selected by a random number
Fitting a straight line to a log-log curve on pyplot," I have a plot with me which is logarithmic on both the axes. I have pyplot's loglog function to do this. It also gives me the logarithmic scale on both the axes.Now, using numpy I fit a straight line to the set of points that I have. However, when I plot this line on the plot, I cannot get a straight line. I get a curved line. The blue line is the supposedly ""straight line"". It is not getting plotted straight. I want to fit this straight line to the curve plotted by red dotsHere is the code I am using to plot the points: <code>  import numpyfrom matplotlib import pyplot as pltimport mathfp=open(""word-rank.txt"",""r"")a=[]b=[]for line in fp: string=line.strip().split() a.append(float(string[0])) b.append(float(string[1]))coefficients=numpy.polyfit(b,a,1)polynomial=numpy.poly1d(coefficients)ys=polynomial(b)print polynomialplt.loglog(b,a,'ro')plt.plot(b,ys)plt.xlabel(""Log (Rank of frequency)"")plt.ylabel(""Log (Frequency)"")plt.title(""Frequency vs frequency rank for words"")plt.show()",Fitting a straight line to a log-log curve in matplotlib
Hidden Dictonary Key," I have a dictionary that is passed to me from a function that I do not have access to the code. In this dictionary there is a key called 'time'. I can print d['time'] and it prints the value I expect. However, when I iterate through the dictionary, this key is skipped. Also d.keys() does not include it. If it matters, the other keys are numerical.How would I recreate this? How do you see hidden keys without knowing the name? Can this be undone?print type(d) returns <type 'dict'> <code> ",Hidden Dictionary Key
How to make tesseract shape with 4 Dimensional Hasse Diagram in NetworkX?," So I created a really naive (probably inefficient) way of generating hasse diagrams. Question:I have 4 dimensions... p q r s .I want to display it uniformly (tesseract) but I have no idea how to reshape it. How can one reshape a networkx graph in Python?I've seen some examples of people using spring_layout() and draw_circular() but it doesn't shape in the way I'm looking for because they aren't uniform.Is there a way to reshape my graph and make it uniform? (i.e. reshape my hasse diagram into a tesseract shape (preferably using nx.draw() )Here's what mine currently look like:Here's my code to generate the hasse diagram of N dimensions I want to reshape it like this:If anyone knows of an easier way to make Hasse Diagrams, please share some wisdom but that's not the main aim of this post. <code>  #!/usr/bin/pythonimport networkx as nximport matplotlib.pyplot as pltimport itertoolsH = nx.DiGraph()axis_labels = ['p','q','r','s']D_len_node = {}#Iterate through axis labelsfor i in xrange(0,len(axis_labels)+1): #Create edge from empty set if i == 0: for ax in axis_labels: H.add_edge('O',ax) else: #Create all non-overlapping combinations combinations = [c for c in itertools.combinations(axis_labels,i)] D_len_node[i] = combinations #Create edge from len(i-1) to len(i) #eg. pq >>> pqr, pq >>> pqs if i > 1: for node in D_len_node[i]: for p_node in D_len_node[i-1]: #if set.intersection(set(p_node),set(node)): Oops if all(p in node for p in p_node) == True: #should be this! H.add_edge(''.join(p_node),''.join(node))#Show Plotnx.draw(H,with_labels = True,node_shape = 'o')plt.show() ",How to reshape a networkx graph in Python?
How to get pixel coordinates from Feature Matching," I need to get the list of the x and y coordinates of the pixels that the feature matcher selects in the code provided. I'm using Python and OpenCV. Can anyone help me? <code>  img1=cv2.imread('DSC_0216.jpg',0)img2=cv2.imread('DSC_0217.jpg',0)orb=cv2.ORB(nfeatures=100000)kp1,des1=orb.detectAndCompute(img1,None)kp2,des2=orb.detectAndCompute(img2,None)img1kp=cv2.drawKeypoints(img1,kp1,color=(0,255,0),flags=0)img2kp=cv2.drawKeypoints(img2,kp2,color=(0,255,0),flags=0)cv2.imwrite('m_img1.jpg',img1kp)cv2.imwrite('m_img2.jpg',img2kp)bf=cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)matches=bf.match(des1,des2)matches=sorted(matches, key= lambda x:x.distance)",How to get pixel coordinates from Feature Matching in OpenCV Python
How to execute python script on schedule?," I have two Python scripts on my machine that I want to execute two times a day on specific time period. How do I automate this task? Since I will be away from home and thus my computer for a while, I want to upload them to a site and be executed from there automatic without me doing anything.How can I do this? <code> ",How to execute script on schedule?
Calculating Euclidean distances between unique Python array regions?," I have a raster with a set of unique ID patches/regions which I've converted into a two-dimensional Python numpy array. I would like to calculate pairwise Euclidean distances between all regions to obtain the minimum distance separating the nearest edges of each raster patch. As the array was originally a raster, a solution needs to account for diagonal distances across cells (I can always convert any distances measured in cells back to metres by multiplying by the raster resolution). I've experimented with the cdist function from scipy.spatial.distance as suggested in this answer to a related question, but so far I've been unable to solve my problem using the available documentation. As an end result I would ideally have a 3 by X array in the form of ""from ID, to ID, distance"", including distances between all possible combinations of regions. Here's a sample dataset resembling my input data:  <code>  import numpy as npimport matplotlib.pyplot as plt# Sample study area arrayexample_array = np.array([[0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0], [0, 0, 2, 0, 2, 2, 0, 6, 0, 3, 3, 3], [0, 0, 0, 0, 2, 2, 0, 0, 0, 3, 3, 3], [0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3], [1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3], [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 3], [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0], [1, 1, 1, 0, 0, 0, 3, 3, 3, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0, 5, 5, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]])# Plot arrayplt.imshow(example_array, cmap=""spectral"", interpolation='nearest')",Calculating distances between unique Python array regions?
django runserver from python script," The normal way to start the Django server is to run the following command from a terminal or a bash script: e.g. How can I start a Django server from a Python script?One option is the following Is there a better way? <code>  python manage.py runserver [Ip.addr]:[port] python manage.py runserver 0.0.0.0:8000 import osif __name__ == ""__main__"": os.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""server.settings"") from django.core.management import execute_from_command_line args = ['name', 'runserver', '0.0.0.0:8000'] execute_from_command_line(args)",Django runserver from Python script
Non-Blocking raw_input() in python," After digging around in SO for a while, I still haven't found a good answer to what I would hope is a fairly common need. Basically I need a main thread to do ""stuff"" until it receives input and then act on that input, then return to the original ""stuff"". My problem every time seems to be that my program execution seems to halt completely at the raw input, whether I call it in a thread or anywhere else. Forwarning I'm pretty novice to python, but I'd hope this shouldn't be too nasty to implement. Here is what I'm playing with (pulled from my other question where my threading question was answered handily)So I'm trying to write a program that looks for keyboard presses and then does something in the main program based upon what the user inputs. I'm trying to run the keyboard listening in a thread and then compare whats in the variable in my main loop, but I don't ever seem to be getting the threaded keyboard input. In the below code, the print maybe updating line never happens, just the else block from the main while loop. What do i need to do so that my main loop is aware of the keys pressed by the user? <code>  import threadingimport timekbdInput = ''playingID = ''def kbdListener(): global kbdInput kbdInput = rawInput() print ""maybe updating...the kbdInput variable is: "",kbdInputlistener = threading.Thread(target=kbdListener)while True: print ""kbdInput: "",kbdInput print ""playingID: "",playingID if playingID != kbdInput: print ""Recieved new keyboard Input. Setting playing ID to keyboard input value"" playingID = kbdInput else: print ""No input from keyboard detected. Sleeping 2 seconds"" time.sleep(2)",Non-Blocking raw_input()
pandas: move given row to end of df," I would like to take a given row from a DataFrame and prepend or append to the same DataFrame. My code below does just that, but I'm not sure if I'm doing it the right way or if there is an easier, better, faster way? Thanks <code>  testdf = df.copy()#get row target_row = testdf.ix[[2],:]#del row from dftestdf.drop([testdf.index[2]], axis=0, inplace=True)#concat original row to end or start of dfnewdf = pd.concat([testdf, target_row], axis=0)",Move given row to end of DataFrame
Count occurrences of each key in python dictionary.," I have a python dictionary object that looks somewhat like this: Now for each 'sign' key, I'd like to count how many times each value occurs. This however, prints number of times 'sign' appears in the dictionary, instead of getting the value of the sign and counting the number of times a particular value appears.For example, the output I'd like to see is: And so on. What should I change to get the desired output? <code>  [{""house"": 4, ""sign"": ""Aquarius""}, {""house"": 2, ""sign"": ""Sagittarius""}, {""house"": 8, ""sign"": ""Gemini""}, {""house"": 3, ""sign"": ""Capricorn""}, {""house"": 2, ""sign"": ""Sagittarius""}, {""house"": 3, ""sign"": ""Capricorn""}, {""house"": 10, ""sign"": ""Leo""}, {""house"": 4, ""sign"": ""Aquarius""}, {""house"": 10, ""sign"": ""Leo""}, {""house"": 1, ""sign"": ""Scorpio""}] def predominant_sign(data): signs = [k['sign'] for k in data if k.get('sign')] print len(signs) Aquarius: 2Sagittarius: 2Gemini: 1...",Count occurrences of each key in python dictionary
Difference between set difference and set subtraction," What distinguishes - and .difference() on sets? Obviously the syntax is not the same, one is a binary operator, the other is an instance method. What else? <code>  s1 = set([1,2,3])s2 = set([3,4,5])>>> s1 - s2set([1, 2])>>> s1.difference(s2)set([1, 2])",Set difference versus set subtraction
pandas: reindex a dataframe with duplicate index values," So I imported and merged 4 csv's into one dataframe called data. However, upon inspecting the dataframe's index with: I see that multiple index entries have 4 counts. I want to completely reindex the data dataframe so each row now has a unique index value. I tried: which gave the error ""ValueError: cannot reindex from a duplicate axis."" A google search leads me to think this error is because the there are up to 4 rows that share a same index value. Any idea how I can do this reindexing without dropping any rows? I don't particularly care about the order of the rows either as I can always sort it.UPDATE:So in the end I did find a way to reindex like I wanted. As I understand it, I just added a new column called 'index' to my data frame, and then set that column as my index.As for my csv's, they were the four csv's under ""download loan data"" on this page of Lending Club loan stats. <code>  index_series = pd.Series(data.index.values)index_series.value_counts() data.reindex(np.arange(len(data))) data['index'] = np.arange(len(data))data = data.set_index('index')",Reindex a dataframe with duplicate index values
python iterate over list of objects," I have a dictionary of objects which contains the ""Names/Ranges"" within a spreadsheet. As I process the spreadsheet I need to update the value associated with a range.The class to hold this info looks like this: I create the dictionary as follows: It seems to work fine. I can see the dictionary and contained objects in debug. When I go back to update the objects within the dictionary based on processing the spreadsheet, I get lost.I call this function: staticNames refers to the dictionary containing the Name/Range objects.I am expecting sName to contain an object of type varName. But alas it contains a string. What am I doing wrong? <code>  class varName: name = None refersTo = None refersToR1C1 = None value = None def __init__(self, name, refersTo, refersToR1C1, value): self.name = name self.refersTo = refersTo self.refersToR1C1 = refersToR1C1 self.value = value staticNames = {}wbNames = wb.Namesfor name in wbNames: (nSheet, nAddr) = name.RefersTo.split(""!"") print ""Name: %s Refers to: %s Refers to R1C1: %s Value: %s "" % (name.Name, name.RefersTo, name.RefersToR1C1, wSheets(nSheet.replace('=', '')).Range(nAddr).value) staticNames[name.Name] = varName(name.Name, name.RefersTo, name.RefersToR1C1, wSheets(nSheet.replace('=', '') ).Range(nAddr).value) def updateStaticNames(ws, r, c, val_in, staticNames): for sName in staticNames: if sName.refersToR1C1() == ""="" + ws.Name + ""!R"" + str(r) + ""C"" + str(c): sName.value = val_in return None ",Iterate over dictionary of objects
Why is pymongo giving ServerSelectionTimeoutError?," I'm using:Python 3.4.2PyMongo 3.0.2mongolab running mongod 2.6.9uWSGI 2.0.10CherryPy 3.7.0nginx 1.6.2uWSGI start params: I setup my MongoClient ONE time: I try and save a JSON dict to MongoDB: It works via a unit test that executes the same code path to mongodb. However when I execute via CherryPy and uWSGI using an HTTP POST, I get this: Why am I seeing this behavior when run via CherryPy and uWSGI? Is this perhaps the new thread model in PyMongo 3?Update:If I run without uWSGI and nginx by using the CherryPy built-in server, the insert_one() works.Update 1/25 4:53pm EST:After adding some debug in PyMongo, it appears that topology._update_servers() knows that the server_type = 2 for server 'myserver-a.mongolab.com'. However server_description.known_servers() has the server_type = 0 for server 'myserver.mongolab.com'This leads to the following stack trace: <code>  --socket 127.0.0.1:8081 --daemonize --enable-threads --threads 2 --processes 2 self.mongo_client = MongoClient('mongodb://user:pw@host.mongolab.com:port/mydb')self.db = self.mongo_client['mydb'] result = self.db.jobs.insert_one(job_dict) pymongo.errors.ServerSelectionTimeoutError: No servers found yet result = self.db.jobs.insert_one(job_dict)File ""/usr/local/lib/python3.4/site-packages/pymongo/collection.py"", line 466, in insert_onewith self._socket_for_writes() as sock_info:File ""/usr/local/lib/python3.4/contextlib.py"", line 59, in __enter__return next(self.gen)File ""/usr/local/lib/python3.4/site-packages/pymongo/mongo_client.py"", line 663, in _get_socketserver = self._get_topology().select_server(selector)File ""/usr/local/lib/python3.4/site-packages/pymongo/topology.py"", line 121, in select_serveraddress))File ""/usr/local/lib/python3.4/site-packages/pymongo/topology.py"", line 97, in select_serversself._error_message(selector))pymongo.errors.ServerSelectionTimeoutError: No servers found yet",Why is PyMongo 3 giving ServerSelectionTimeoutError?
Why is pymongo 3 giving ServerSelectionTimeoutError?," I'm using:Python 3.4.2PyMongo 3.0.2mongolab running mongod 2.6.9uWSGI 2.0.10CherryPy 3.7.0nginx 1.6.2uWSGI start params: I setup my MongoClient ONE time: I try and save a JSON dict to MongoDB: It works via a unit test that executes the same code path to mongodb. However when I execute via CherryPy and uWSGI using an HTTP POST, I get this: Why am I seeing this behavior when run via CherryPy and uWSGI? Is this perhaps the new thread model in PyMongo 3?Update:If I run without uWSGI and nginx by using the CherryPy built-in server, the insert_one() works.Update 1/25 4:53pm EST:After adding some debug in PyMongo, it appears that topology._update_servers() knows that the server_type = 2 for server 'myserver-a.mongolab.com'. However server_description.known_servers() has the server_type = 0 for server 'myserver.mongolab.com'This leads to the following stack trace: <code>  --socket 127.0.0.1:8081 --daemonize --enable-threads --threads 2 --processes 2 self.mongo_client = MongoClient('mongodb://user:pw@host.mongolab.com:port/mydb')self.db = self.mongo_client['mydb'] result = self.db.jobs.insert_one(job_dict) pymongo.errors.ServerSelectionTimeoutError: No servers found yet result = self.db.jobs.insert_one(job_dict)File ""/usr/local/lib/python3.4/site-packages/pymongo/collection.py"", line 466, in insert_onewith self._socket_for_writes() as sock_info:File ""/usr/local/lib/python3.4/contextlib.py"", line 59, in __enter__return next(self.gen)File ""/usr/local/lib/python3.4/site-packages/pymongo/mongo_client.py"", line 663, in _get_socketserver = self._get_topology().select_server(selector)File ""/usr/local/lib/python3.4/site-packages/pymongo/topology.py"", line 121, in select_serveraddress))File ""/usr/local/lib/python3.4/site-packages/pymongo/topology.py"", line 97, in select_serversself._error_message(selector))pymongo.errors.ServerSelectionTimeoutError: No servers found yet",Why is PyMongo 3 giving ServerSelectionTimeoutError?
How to load default profile in chrome using Python Selenium Webdriver?, I'd like to launch Chrome with its default profile using Python's webdriver so that cookies and site preferences persist across sessions.How can I do that? <code> ,How to load default profile in Chrome using Python Selenium Webdriver?
python3 error:initial_value must be str or None," While porting code from python2 to 3, I get this error when reading from a URL TypeError: initial_value must be str or None, not bytes. The exception is thrown at this line If I use python2, it works fine. <code>  import urllibimport jsonimport gzipfrom urllib.parse import urlencodefrom urllib.request import Requestservice_url = 'https://babelfy.io/v1/disambiguate'text = 'BabelNet is both a multilingual encyclopedic dictionary and a semantic network'lang = 'EN'Key = 'KEY' params = { 'text' : text, 'key' : Key, 'lang' :'EN' }url = service_url + '?' + urllib.urlencode(params)request = Request(url)request.add_header('Accept-encoding', 'gzip')response = urllib.request.urlopen(request)if response.info().get('Content-Encoding') == 'gzip': buf = StringIO(response.read()) f = gzip.GzipFile(fileobj=buf) data = json.loads(f.read()) buf = StringIO(response.read()) ","Python3 error: initial_value must be str or None, with StringIO"
Python3 error: initial_value must be str or None," While porting code from python2 to 3, I get this error when reading from a URL TypeError: initial_value must be str or None, not bytes. The exception is thrown at this line If I use python2, it works fine. <code>  import urllibimport jsonimport gzipfrom urllib.parse import urlencodefrom urllib.request import Requestservice_url = 'https://babelfy.io/v1/disambiguate'text = 'BabelNet is both a multilingual encyclopedic dictionary and a semantic network'lang = 'EN'Key = 'KEY' params = { 'text' : text, 'key' : Key, 'lang' :'EN' }url = service_url + '?' + urllib.urlencode(params)request = Request(url)request.add_header('Accept-encoding', 'gzip')response = urllib.request.urlopen(request)if response.info().get('Content-Encoding') == 'gzip': buf = StringIO(response.read()) f = gzip.GzipFile(fileobj=buf) data = json.loads(f.read()) buf = StringIO(response.read()) ","Python3 error: initial_value must be str or None, with StringIO"
How do i get id of an object in pymongo?," I am trying to get the id from the document which I have in MongoDB, using PyMongo.Here is my code: I have tried this too: Neither of them works! <code>  docQuery = db.doctors.find({""email"":doc_mail})doc_id = docQuery[0][""_id""][""$oid""] doc_id = docQuery[0][""_id""]","How do I get the id from an ObjectID, using Python?"
How does flask's sqlalchemy extension discover models, Flask-SQLAlchemy's db.create_all() method creates each table corresponding to my defined models. I never instantiate or register instances of the models. They're just class definitions that inherit from db.Model. How does it know which models I have defined? <code> ,How does Flask-SQLAlchemy create_all discover the models to create?
Error import spatial data in GeoDjango - KeyError for mpoly field," I was following the tutorial on https://docs.djangoproject.com/en/1.8/ref/contrib/gis/tutorial/#importing-spatial-data for setting up GeoDjango on my machine. But it seems like there is some issue there. While importing data using LayerMapping by running load.run(), I get the following error: Then I found out that, there is no 'MULTIPOLYGON' field in the .shp file: But it's there in the WorldBorder model, as type MultiPolygonField.So, definitely in the world_mapping file, importing will fail for the 'mpoly': 'MULTIPOLYGON' mapping. Has anyone else faced this issue? I hope so, as I've followed the tutorial step-by-step. But it doesn't say anything about such issue. What effect it will have, if I load data by removing mpoly mapping?Here's my load.py file: Just an update: After going through source code, via stack trace, I figured that I'm unable to access field_types propery of layer module. So, from python shell, when I access that property, I get the same error: Now, this is strange, because now I've also removed mpoly field from WorldBorder model.Update 2:After digging through the source code, I found out that, OGDFieldTypes in my version of gdal, might not have the key 12, as in the source code here: https://github.com/django/django/blob/master/django/contrib/gis/gdal/field.py.But it says that keys 12, and 13 will be available for GDAL 2, and that is what I've installed. Really seems to be some conflict among libraries now.I've installed the following libraries:geos-3.4.2.tar.bz2proj-datumgrid-1.5.tar.gzproj-4.8.0.tar.gzgdal-2.0.0.tar.gzAnd PostGIS version 2.1.5 is installed in Amazon RDS instance. <code>  Traceback (most recent call last): File ""<console>"", line 1, in <module> File ""/home/ubuntu/src/django/world/load.py"", line 23, in run lm = LayerMapping(WorldBorder, world_shp, world_mapping, transform=False, encoding='iso-8859-1') File ""/home/ubuntu/Envs/vir-env/local/lib/python2.7/site-packages/django/contrib/gis/utils/layermapping.py"", line 105, in __init__ self.check_layer() File ""/home/ubuntu/Envs/vir-env/local/lib/python2.7/site-packages/django/contrib/gis/utils/layermapping.py"", line 178, in check_layer ogr_field_types = self.layer.field_types File ""/home/ubuntu/Envs/vir-env/local/lib/python2.7/site-packages/django/contrib/gis/gdal/layer.py"", line 153, in field_types for i in range(self.num_fields)]KeyError: 12 >>> from django.contrib.gis.gdal import DataSource>>> ds = DataSource('world/data/TM_WORLD_BORDERS-0.3.shp')>>> layer = ds[0]>>> layer.fields[u'FIPS', u'ISO2', u'ISO3', u'UN', u'NAME', u'AREA', u'POP2005', u'REGION', u'SUBREGION', u'LON', u'LAT'] 1 import os 2 from django.contrib.gis.utils import LayerMapping 3 from models import WorldBorder 4 5 world_mapping = { 6 'fips' : 'FIPS', 7 'iso2' : 'ISO2', 8 'iso3' : 'ISO3', 9 'un' : 'UN', 10 'name' : 'NAME', 11 'area' : 'AREA', 12 'pop2005' : 'POP2005', 13 'region' : 'REGION', 14 'subregion' : 'SUBREGION', 15 'lon' : 'LON', 16 'lat' : 'LAT', 17 'mpoly' : 'MULTIPOLYGON', 18 } 19 20 world_shp = os.path.abspath(os.path.join(os.path.dirname(__file__), 'data/TM_WORLD_BORDERS-0.3.shp')) 21 22 def run(verbose=True): 23 lm = LayerMapping(WorldBorder, world_shp, world_mapping, transform=False, encoding='iso-8859-1') 24 25 lm.save(strict=True, verbose=verbose) >>> from django.contrib.gis.gdal import DataSource>>> ds = DataSource(wshp)>>> layer = ds[0]>>> layer.fields[u'FIPS', u'ISO2', u'ISO3', u'UN', u'NAME', u'AREA', u'POP2005', u'REGION', u'SUBREGION', u'LON', u'LAT']>>> layer.field_typesTraceback (most recent call last): File ""<console>"", line 1, in <module> File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/django/contrib/gis/gdal/layer.py"", line 153, in field_types for i in range(self.num_fields)]KeyError: 12",Error importing spatial data in GeoDjango - KeyError for mpoly field
Python web server push data," I've written an algorithm in python and a web interface around that. After you submit the form and start the algorithm, I'd like to push and update data on the page as it's running. How can I accomplish this? <code> ",Pushing updates from Python server to a web interface
Python: count the duplicates in a list of tuples," I have a list of tuples: a = [(1,2),(1,4),(1,2),(6,7),(2,9)] I want to check if one of the individual elements of each tuple matches the same position/element in another tuple, and how many times this occurs.For example: If only the 1st element in some tuples has a duplicate, return the tuple and how many times it's duplicated.I can do that with the following code: I want to know if there is a more effective way to do this? <code>  a = [(1,2), (1,4), (1,2), (6,7), (2,9)]coll_list = []for t in a: coll_cnt = 0 for b in a: if b[0] == t[0]: coll_cnt = coll_cnt + 1 print ""%s,%d"" %(t,coll_cnt) coll_list.append((t,coll_cnt))print coll_list",Count the duplicates in a list of tuples
Trying to write a generalized Python function to add item to a SQLalchemy relationship," Given a couple of simple tables in sqlalchemy which have a simple one to many relationship, I am trying to write a generalized function to add children to the relationship collection. The tables look like this: So with the given setup, I can add items to the images collection for a given StockItem: Ok. so far so good. In reality, my app is going to have several tables with relationships. My issue arises when I try to generalize this out to a function to handle arbitrary relationships. Here'e what I wrote (note, add_item() is just a method which wraps an object construction with a try/except to handle IntegrityError's): I call the function like: which obviously errors because the parent table has no collection attribute. Traceback: I think it would throw an error anyway since the collection name is passed as a string.So my question, is how can I add an item to the collection by key word instead of using 'dot' notation? <code>  class StockItem(Base): __tablename__ = 'stock_items' stock_id = Column(Integer, primary_key=True) description = Column(String, nullable=False, unique=True) department = Column(String) images = relationship('ImageKey', backref='stock_item', lazy='dynamic') def __repr__(self): return '<StockItem(Stock ID:{}, Description: {}, Department: {})>'.\ format(self.stock_id, self.description, self.department)class ImageKey(Base): __tablename__ = 'image_keys' s3_key = Column(String, primary_key=True) stock_id = Column(Integer, ForeignKey('stock_items.stock_id')) def __repr__(self): return '<ImageKey(AWS S3 Key: {}, Stock Item: {})>'.\ format(self.s3_key, self.stock_id) item = StockItem(stock_id=42, description='Frobnistication for Foozlebars', department='Books')image = ImageKey(s3_key='listings/images/Frob1.jpg', stock_id=42)item.images.append(image) @session_managerdef _add_collection_item(self, Parent, Child, key, collection, session=None, **kwargs): """"""Add a Child object to the collection object of Parent."""""" child = self.add_item(Child, session=session, **kwargs) parent = session.query(Parent).get(key) parent.collection.append(child) # This line obviously throws error. session.add(parent) db._add_collection_item(StockItem, ImageKey, 42, 'images', s3_key='listings/images/Frob1.jpg', stock_id=42) Traceback (most recent call last): File ""C:\Code\development\pyBay\demo1.py"", line 25, in <module> stock_id=1) File ""C:\Code\development\pyBay\pybay\database\client.py"", line 67, in add_context_manager result = func(self, *args, session=session, **kwargs) File ""C:\Code\development\pyBay\pybay\database\client.py"", line 113, in _add_collection_item parent.collection.append(child)AttributeError: 'StockItem' object has no attribute 'collection'",Indirectly accessing Python instance attribute without using dot notation
Numpy: raise to 1/3, I cannot understand the following output. I would expect Numpy to return -10 (or an approximation). Why is it a complex number? Numpy answer Numpy official tutorial says the answer is nan. You can find it in the middle of this tutorial.  <code>  print((-1000)**(1/3.)) (5+8.660254037844384j),Raise to 1/3 gives complex number
How to throw exception if script is run with python 2," I have a script that should only be run with Python 3. I want to give a nice error message saying that this script should not be run with python2 if a user tries to run it with Python 2.xHow do I do this? When I try checking the Python version, it still throws an error, as Python parses the whole file before executing my if condition. If possible, I'd rather not make another script.  <code> ",How to throw exception if script is run with Python 2?
How to download multiple files and images from a wesite using python, So I am trying to download multiple files from a give a website and saving into a folder. I am trying to get highway data and in their website (http://www.wsdot.wa.gov/mapsdata/tools/InterchangeViewer/SR5.htm) is a list of pdf links. I want to create a code that will extract the numerous pdfs found on their website. Maybe creating a loop that will go through the website and extract and save each file into a local folder onto my desktop.does anyone know how I can do that? <code> ,How to download multiple files and images from a website using python
What is the best approach to use Sockets with Django projects?, I am starting to work on a new Django project that requires sockets. I've searched internet and found this and this tutorials. There is also a lot of outdated stuff out there. Can anyone recommend the best approach to use sockets with Django? I am using Django 1.7. Thanks. <code> ,What is the best approach to use Web Sockets with Django projects?
Counting the number of line crosses in a pandas data series," Consider the following series Is there an easy way of knowing how many times the 2 value is reached/crossed (without the obvious iterating solution)?The expected result for the example above should be 4 (the 2 line is crossed up or down 4 times in the series).Edit: updated example case <code>  s = pd.Series([0,1,2,3,4,1,5,4,3,2,1])",Finding when a value in a pandas.Series crosses/reaches a threshold
Finding when a value in a pandas.Series crosses a threshold," Consider the following series Is there an easy way of knowing how many times the 2 value is reached/crossed (without the obvious iterating solution)?The expected result for the example above should be 4 (the 2 line is crossed up or down 4 times in the series).Edit: updated example case <code>  s = pd.Series([0,1,2,3,4,1,5,4,3,2,1])",Finding when a value in a pandas.Series crosses/reaches a threshold
what is the name parameter in Pandas Series?," In the doc of Series, the use parameter of name and fastpath is not explained. What do they do? <code> ",What is the name parameter in Pandas Series?
Using nested serializers with explicit ForeignKey binding replaced with IntegerField," I'm trying to get rid of explicit binding between my models. Thus instead of using ForeignKey, I'll use IntegerField to just store the primary key of target model as a field. Thus I'll handle the relationship manually at code level. This is because, I'll have to move my some schemas to different database instances. So they can't have linkage.Now, I'm facing issue with my nested serializer. I'm trying to create an instance of the below model: And this is my CustomerAddress model, which references the Customer model: Below is my serializers: My views: When I try to fire the post request to above view from my test: My test is failing with the following error trace: All this was working fine when I had my CustomerAddress have ForeignKey binding to Customer. I'm not getting any clue on how to fix this thing. I tried to look through the source code to see whether some customization needs to be done. But I'm at loss. I feel like I've to somehow tweak with my serializer, by may be overriding to_representation method, but I'm not sure.BTW, error only comes while creating the model instance. For GET request, I get the proper json, with nested serializer.Have anyone else tried to do something like this and succeeded? What should be done to make this work? And yes, I've to remove the explicit foreign key binding. <code>  17 class Customer(models.Model): 18 id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False) 19 phone_no = models.CharField(max_length=15, unique=True) 47 class CustomerAddress(models.Model): 48 # customer = models.ForeignKey(Customer, related_name='cust_addresses') 49 customer_id = models.UUIDField(default=uuid.uuid4) 50 address = models.CharField(max_length=1000) 7 class CustomerAddressSerializer(serializers.ModelSerializer): 8 9 class Meta: 10 model = CustomerAddress 11 depth = 1 31 class CustomerSerializer(serializers.ModelSerializer): 32 cust_addresses = CustomerAddressSerializer(many=True) 33 34 class Meta: 35 model = Customer 36 depth = 1 37 fields = ('id', 'phone_no', 'cust_addresses',) 38 39 def create(self, validated_data): 40 cust = Customer.objects.create(id=uuid.uuid4()) 41 42 for addr in validated_data['cust_addresses']: 43 address = addr['address'] 44 cust_addr = CustomerAddress.objects.create(address=address, customer_id=cust.id) 45 46 return cust 12 class CustomerView(generics.RetrieveAPIView, generics.CreateAPIView): 13 serializer_class = CustomerSerializer 14 22 def get_object(self): 23 session = self.request.session 24 if session.has_key('uuid'): 25 id = session['uuid'] 26 cust = Customer.objects.get(pk=uuid.UUID(id)) 27 return cust 28 return None 71 def test_create_customer_address(self): 72 cust_url = reverse('user_v1:customer') 73 # Now we create a customer, and use it's UID in the ""customer"" data of /cust-address/ 74 cust_data = {""first_name"": ""Rohit"", ""last_name"": ""Jain"", ""phone_no"": ""xxxxxx"", ""email_id"": ""test@gmail.com"", ""cust_addresses"": [{""city_id"": 1, ""address"": ""addr"", ""pin_code"": ""123124"", ""address_tag"": ""XYZ""}]} 75 cust_response = self.client.post(cust_url, cust_data, format='json') 76 print 'Post Customer' 77 print cust_response 78 self.assertEqual(cust_response.data['id'], str(cust_id)) ======================================================================ERROR: test_create_customer_address (app.tests.CustomerViewTest)----------------------------------------------------------------------Traceback (most recent call last): File ""/home/ubuntu/src/django-proj/app/tests.py"", line 75, in test_create_customer_address cust_response = self.client.post(cust_url, cust_data, format='json') File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/rest_framework/test.py"", line 168, in post path, data=data, format=format, content_type=content_type, **extra) File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/rest_framework/test.py"", line 90, in post return self.generic('POST', path, data, content_type, **extra) File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/rest_framework/compat.py"", line 231, in generic return self.request(**r) File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/rest_framework/test.py"", line 157, in request return super(APIClient, self).request(**kwargs) File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/rest_framework/test.py"", line 109, in request request = super(APIRequestFactory, self).request(**kwargs) File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/django/test/client.py"", line 466, in request six.reraise(*exc_info) File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/django/core/handlers/base.py"", line 132, in get_response response = wrapped_callback(request, *callback_args, **callback_kwargs) File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/django/views/decorators/csrf.py"", line 58, in wrapped_view return view_func(*args, **kwargs) File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/django/views/generic/base.py"", line 71, in view return self.dispatch(request, *args, **kwargs) File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/rest_framework/views.py"", line 456, in dispatch response = self.handle_exception(exc) File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/rest_framework/views.py"", line 453, in dispatch response = handler(request, *args, **kwargs) File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/rest_framework/generics.py"", line 190, in post return self.create(request, *args, **kwargs) File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/rest_framework/mixins.py"", line 21, in create headers = self.get_success_headers(serializer.data) File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/rest_framework/serializers.py"", line 470, in data ret = super(Serializer, self).data File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/rest_framework/serializers.py"", line 217, in data self._data = self.to_representation(self.instance) File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/rest_framework/serializers.py"", line 430, in to_representation attribute = field.get_attribute(instance) File ""/home/ubuntu/Envs/rj-venv/local/lib/python2.7/site-packages/rest_framework/fields.py"", line 317, in get_attribute raise type(exc)(msg)AttributeError: Got AttributeError when attempting to get a value for field `cust_addresses` on serializer `CustomerSerializer`.The serializer field might be named incorrectly and not match any attribute or key on the `Customer` instance.Original exception text was: 'Customer' object has no attribute 'cust_addresses'.----------------------------------------------------------------------",Using nested serializers with explicit ForeignKey binding replaced with raw IntegerField
Python to JavaScript Equivalence, I'm practicing/studying both JavaScript and Python. I'm wondering if Javascript has the equivalence to this type of coding.I'm basically trying to get an array from each individual integer from the string for practice purposes. I'm more proficient in Python than JavaScriptPython: Does Javascript have something similar for me to do above? <code>  string = '1234-5'forbidden = '-'print([int(i) for i in str(string) if i not in forbidden]),Does JavaScript support array/list comprehensions like Python?
how to export a table dataframe in pyspark to csv?," I am using Spark 1.3.1 (PySpark) and I have generated a table using a SQL query. I now have an object that is a DataFrame. I want to export this DataFrame object (I have called it ""table"") to a csv file so I can manipulate it and plot the columns. How do I export the DataFrame ""table"" to a csv file?Thanks! <code> ",How to export a table dataframe in PySpark to csv?
Importing an svg file a matplotlib figure," I like to produce high quality plots and therefore avoid rasterized graphics as much as possible. I am trying to import an svg file on to a matplotlib figure: This works with png perfectly. Can somebody tell me how to do it with svg or at least point my to proper documentation. I know that a similar question has been asked (but not answered): here. Has anything changed since?P.S. I know that I could just export a high resolution png and achieve a similar effect. This is not the solution I am looking for.Here is the image I would like to import: . <code>  import matplotlib.pyplot as pltearth = plt.imread('./gfx/earth.svg')fig, ax = plt.subplots()im = ax.imshow(earth)plt.show()",Importing an svg file into a matplotlib figure
Constructing numpy 2D array from C++ 2D vector," I am trying to build a Python module in C++ that transforms a 2D vector into a Numpy 2D array. What is incorrect here - presumably there is some transformation needed to a boost python object from PyObject*? <code>  boost::python::object build_day(int year, int day) { PyObject* arr; const int HEIGHT = 5; const int WIDTH = 5; std::vector<std::vector<float> > array(WIDTH, std::vector<float>(HEIGHT)); npy_intp dims[2] = {WIDTH, HEIGHT}; arr = PyArray_SimpleNewFromData(2, dims, NPY_FLOAT, &array); return arr; }BOOST_PYTHON_MODULE(sumpar) { using namespace boost::python; def(""build_day"", build_day, args(""year"", ""day""));}",Python* to boost::python::object
Command line options in Python," I am trying to use my program with command line option. Here is my code: When I try to run my program from terminal (python script.py -u), I expect to get the message Starting with upgrade procedure, but instead I get the error message unrecognized arguments -u. <code>  import argparsedef main(): parser = argparse.ArgumentParser() parser.add_argument(""-u"",""--upgrade"", help=""fully automatized upgrade"") args = parser.parse_args() if args.upgrade: print ""Starting with upgrade procedure""main()","Python argparse: ""unrecognized arguments"""
How to batch process grayscale images with PIL/Pillow/NumPy to force highlight/midtone/shadow ratios," I have a large collection of 7 mega pixel grayscale images and I want batch process them to adjust the contrast and brightness so that each image contains about:50% highlights (pixels with a luminescence value of 200-255)30% midtones (pixels with a luminescence value of 55-199)20% shadows (pixels with a luminescence value of 0-54)It needs to be reasonably efficient as I only have a 1.8GHz and many images. I understand that with NumPy you can get PIL/Pillow to process images much more efficiently than without, but I have never used it before. <code> ",How can I transform the histograms of grayscale images to enforce a particular ratio of highlights/midtones/shadows?
how to get pandas get_dummies to emit N-1 variables to avoid co-lineraity?," pandas.get_dummies emits a dummy variable per categorical value. Is there some automated, easy way to ask it to create only N-1 dummy variables? (just get rid of one ""baseline"" variable arbitrarily)? Needed to avoid co-linearity in our dataset.  <code> ",how to get pandas get_dummies to emit N-1 variables to avoid collinearity?
asyncio python module takes 100% CPU," When I run this little main from rfxcom python library : I see that CPU usage is getting very hight (around 100%).I can't understand why : there is very few messages (~ 1 message every 5 seconds) received by the module, and I thought that when epoll_wait was called then the CPU should be idled, waiting for the next event.I launched the main with python cProfile, and it shows this : So the first three function calls in term of elapsed time are python3.4/asyncio/base_events.py, python3.4/selectors.py and python3.4/asyncio/selector_events.py.EDIT : the time command on a similar run gives : Can someone explain me why ?EDIT2: As the the number of fonctions calls is very high, I made a strace of the process and found that there is a loop on epoll_wait with a timeout value of 2ms : I saw in base_event._run_once that the timeout is calculated, but I can't figure it out. I don't know how to set this timeout higher to lower the CPU.If someone has a clue...Thank you for your answers. <code>  from asyncio import get_event_loopfrom rfxcom.transport import AsyncioTransportdev_name = '/dev/serial/by-id/usb-RFXCOM_RFXtrx433_A1XZI13O-if00-port0'loop = get_event_loop()def handler(packet): print(packet.data)try: rfxcom = AsyncioTransport(dev_name, loop, callback=handler) loop.run_forever()finally: loop.close() In [4]: s.sort_stats('time', 'module').print_stats(50)Mon Jul 20 22:20:55 2015 rfxcom_profile.log 263629453 function calls (263628703 primitive calls) in 145.437 seconds Ordered by: internal time, file name List reduced from 857 to 50 due to restriction <50> ncalls tottime percall cumtime percall filename:lineno(function) 13178675 37.280 0.000 141.337 0.000 /usr/local/lib/python3.4/asyncio/base_events.py:1076(_run_once) 13178675 31.114 0.000 53.230 0.000 /usr/local/lib/python3.4/selectors.py:415(select) 13178674 15.115 0.000 32.253 0.000 /usr/local/lib/python3.4/asyncio/selector_events.py:479(_process_events) 13178675 12.582 0.000 12.582 0.000 {method 'poll' of 'select.epoll' objects} 13178699 11.462 0.000 17.138 0.000 /usr/local/lib/python3.4/asyncio/base_events.py:1058(_add_callback) 13178732 6.397 0.000 11.397 0.000 /usr/local/lib/python3.4/asyncio/events.py:118(_run) 26359349 4.872 0.000 4.872 0.000 {built-in method isinstance} 1 4.029 4.029 145.365 145.365 /usr/local/lib/python3.4/asyncio/base_events.py:267(run_forever) 13178669 4.010 0.000 4.913 0.000 /home/bruno/src/DomoPyc/venv/lib/python3.4/site-packages/rfxcom-0.3.0-py3.4.egg/rfxcom/transport/asyncio.py:85(_writer) time python -m cProfile -o rfxcom_profile.log rfxcom_profile.pyreal 2m24.548suser 2m19.892ssys 0m4.113s // many lines like this :epoll_wait(4, {{EPOLLOUT, {u32=7, u64=537553536922157063}}}, 2, -1) = 1 ",asyncio python with serial device takes 100% CPU
\d not working from Psycopg2 (Postgresql)," I wish to list all the column names of a table using psycopg2 package of Python (2.7). But I am unable to execute the following query - Is there an alternate as to how I can list the column names of a table using psycopg2 ? Please point out any duplicates. Thanks ! <code>  cur.execute(""\d my_table"");psycopg2.ProgrammingError: syntax error at or near ""\""",Meta commands in Psycopg2 - \d not working
Form submit -- then re-route to another page -- Flask," I have a survey form. After submitting the form, I'd like to handle saving the data then redirect to a ""success"" view. I'm using the following code right now, but it just stays on the current url, while I'd like to go to /success. How can I do this? <code>  @app.route('/surveytest', methods=['GET', 'POST'])def surveytest(): if request.method == 'GET': return render_template('test.html', title='Survey Test', year=datetime.now().year, message='This is the survey page.') elif request.method == 'POST': name = request.form['name'] address = request.form['address'] phone = request.form['phone'] email = request.form['email'] company = request.form['company'] return render_template('success.html', name=name, address=address, phone = phone, email = email, company = company)",Redirect to other view after submitting form
Error when importing Flask-MySQL into Flask Application: image not found," I'm trying to import a MySQL module with python, more specifically Flask, though I receive an error. I'm using a virtual environment with my application. Here is the error: I can see in the error it says Library not loaded: /Library/Python/2.7/site-packages/_mysql.so. As I'm using a virtual environment that path is incorrect. It should be /lib/python2.7/site-packages/_mysql.so.Is there a way to change this? Thanks.EDIT:I found there was a terminal command on OSX to change the library location: though after hitting enter I get this: I don't appear to be entering the command wrong, what is the issue? <code>  Traceback (most recent call last): File ""../myapp/application.py"", line 9, in <module> from flask.ext.mysql import MySQL File ""/Users/pavsidhu/Documents/Web-Development/app/env/lib/python2.7/site-packages/flask/exthook.py"", line 81, in load_module reraise(exc_type, exc_value, tb.tb_next) File ""/Users/pavsidhu/Documents/Web-Development/app/env/lib/python2.7/site-packages/flaskext/mysql.py"", line 3, in <module> import MySQLdb File ""/Users/pavsidhu/Documents/Web-Development/app/env/lib/python2.7/site-packages/MySQLdb/__init__.py"", line 19, in <module> import _mysqlImportError: dlopen(/Users/pavsidhu/Documents/Web-Development/app/env/lib/python2.7/site-packages/_mysql.so, 2): Library not loaded: /Library/Python/2.7/site-packages/_mysql.so Referenced from: /Users/pavsidhu/Documents/Web-Development/app/env/lib/python2.7/site-packages/_mysql.so Reason: image not found sudo install_name_tool -change libmysqlclient.18.dylib /lib/python2.7/site-packages/MySQLdb/ Usage: /Library/Developer/CommandLineTools/usr/bin/install_name_tool [-change old new] ... [-rpath old new] ... [-add_rpath new] ... [-delete_rpath old] ... [-id name] input",Error importing MySQL package for Python
Writing to JSON file in python AttributeError: 'str' object has no attribute 'write'," I am trying to load in a json file. Update it and write it back. Here is my attempt at it but I am getting an error: TypeError: dump() takes at least 2 arguments (1 given) <code>  with open('employees.json') as data_file: employees = json.load(data_file) data_file.closeemployees['employees'].append({ ""id"": ""2"", ""name"": ""Rob Croft"", ""key"": ""0003837852""})with open('employees.json', 'w') as data_file: json.dump(employees) data_file.close",Writing to JSON produces TypeError: dump() takes at least 2 arguments (1 given)
"How can save and close an Excel file using Python 3.4, after i have added data?"," I am trying to open an existing Excel 2013 file, add data and then save it(same name) and then close it and then close Excel. The code will open the file, select the correct worksheet and write the data, but when I try save it I get an attribute error. Am I missing a library or something? Here is the code: <code>  import win32com.client as win32def Inventory_Status(): excel = win32.gencache.EnsureDispatch('Excel.Application') # opens Excel wb = excel.Workbooks.Open(r'C:/pytest/Test.xlsx') # opens ""Test"" file wb.Sheets(2).Select() # select 2nd worksheet ""Aisle_2"" excel.Visible = True excel.Range(""A1"").Select() excel.ActiveCell.Value = ""1234"" # Fill in test data # wb.save() wb.Close() excel.Quit()Inventory_Status()raise AttributeError(""'%s' object has no attribute '%s'"" % (repr(self), attr))AttributeError: '<win32com.gen_py.Microsoft Excel 15.0 Object Library._Workbook instance at 0x5901424>' object has no attribute 'save'",Save and close an Excel file after adding data?
Djang Rest Framework JWT," I'm using Django Rest Framework to build the API for my application and would like to implement DjangoRestFramework-JWT for token authentication. The steps seem simple enough but when I test the endpoint I get a 500 error. The terminal output is a ton of html saying a csrf_token wasn't provided. Code & errors are below. Your help is greatly appreciated. CSRF Error Settings.py Global Urls.py <code>  curl -X POST -d ""username=admin&password=123abc"" http://127.0.0.1:8000/api/token/auth/ <!DOCTYPE html><html lang=""en""> <head> <meta http-equiv=""content-type"" content=""text/html; charset=utf-8""> <meta name=""robots"" content=""NONE,NOARCHIVE""> <title>403 Forbidden</title> </head>",Django Rest Framework JWT
matplotlib plot dashed line interrupted with Numbers (similar to contour plot)," I am stuck with a (hopefully) simple problem.My aim is to plot a dashed line interrupted with data (not only text).As I only found out to create a dashed line via linestyle = 'dashed', any help is appreciated to put the data between the dashes.Something similar, regarding the labeling, is already existing with Matplotlib - as I saw in the contour line demo.Update:The question link mentioned by Richard in comments was very helpful, but not the 100% like I mentioned via comment.Currently, I do it this way: Here you can see a snapshot of the result. The minus 20C is without BBox. <code>  line_string2 = '-10 ' + u""\u00b0"" +""C""l, = ax1.plot(T_m10_X_Values,T_m10_Y_Values)pos = [(T_m10_X_Values[-2]+T_m10_X_Values[-1])/2., (T_m10_Y_Values[-2]+T_m10_Y_Values[-1])/2.]# transform data points to screen spacexscreen = ax1.transData.transform(zip(T_m10_Y_Values[-2::],T_m10_Y_Values[-2::]))rot = np.rad2deg(np.arctan2(*np.abs(np.gradient(xscreen)[0][0][::-1])))ltex = plt.text(pos[0], pos[1], line_string2, size=9, rotation=rot, color='b',ha=""center"", va=""bottom"",bbox = dict(ec='1',fc='1', alpha=0.5))",Plot dashed line interrupted with data (similar to contour plot)
Matplotlip: Multiple Subplots," I am a little confused about how this code works: How does the fig, axes work in this case? What does it do?Also why wouldn't this work to do the same thing: <code>  fig, axes = plt.subplots(nrows=2, ncols=2)plt.show() fig = plt.figure()axes = fig.subplots(nrows=2, ncols=2)",How to plot in multiple subplots
How do I get multiple subplots in matplotlib?," I am a little confused about how this code works: How does the fig, axes work in this case? What does it do?Also why wouldn't this work to do the same thing: <code>  fig, axes = plt.subplots(nrows=2, ncols=2)plt.show() fig = plt.figure()axes = fig.subplots(nrows=2, ncols=2)",How to plot in multiple subplots
extracting text outside of a <div> tag BeautifulSoup," So Im practicing my scraping and I came across something like this: and I need the number outside of the <div> tag:My code is: but the output of that is Mobile : only it's the text inside the <div> tag not the text outside of it.so how do we extract the text outside of the <div> tag? <code>  <div class=""profileDetail""> <div class=""profileLabel"">Mobile : </div> 021 427 399 </div> num = soup.find(""div"",{""class"":""profileLabel""}).text",Extracting text outside of a <div> tag BeautifulSoup
is there any best way change given no of days to years months weeks days in python," I am giving number of days to convert them to years, months, weeks and days, but I am taking default days to 365 and month days to 30. How do I do it in an effective way? <code>  def get_year_month_week_day(days): year = days / 365 days = days % 365 month = days / 30 days = days % 30 week = days / 7 day = days % 7 return year,month,week,daydef add_s(num): if num > 1: return 's ' return ' '@register.filterdef daysleft(fdate): cdate = datetime.datetime.now().date() days = (fdate.date() - cdate).days if days == 0: return ""Today"" elif days == 1: return ""Tomorrow"" elif days > 0: year, month, week, day = get_year_month_week_day(days) print year, month, week, day days_left = """" if year > 0: days_left += str(year) + "" year"" + add_s(year) if month > 0: days_left += str(month) + "" month"" + add_s(month) if week > 0: days_left += str(week) + "" week"" + add_s(week) if day > 0: days_left += str(day) + "" day"" + add_s(day) return days_left + "" left"" else: return ""No time left""",Is there a best way to change given number of days to years months weeks days in Python?
How to close Pillow opened image - Python," I have a python file with the Pillow library imported. I can open an image with But how do I close that image? I'm not using Pillow to edit the image, just to show the image and allow the user to choose to save it or delete it. <code>  Image.open(test.png)",How do I close an image opened in Pillow?
Import error when executing exe file," I've tried to create an exe file using py2exe. I've recently updated Python from 2.7.7 to 2.7.10 to be able to work with requests - proxies. Before the update everything worked fine but now, the exe file recently created, raising this error: It could be probably repaired by changing options in setup.py file but I can't figure out what I have to write there. I've tried options = {'py2exe': {'packages': ['requests','urllib2']}}) but with no success.It works as a Python script but not as an exe.Do anybody knows what to do?EDIT: I've tried to put into setup.py file this import: from _ssl import RAND_egdand it says that it can't be imported.EDIT2: Setup.py: <code>  Traceback (most recent call last): File ""puoka_2.py"", line 1, in <module> import mLib File ""mLib.pyc"", line 4, in <module> File ""urllib2.pyc"", line 94, in <module File ""httplib.pyc"", line 71, in <module File ""socket.pyc"", line 68, in <module>ImportError: cannot import name RAND_egd from distutils.core import setupimport py2exe# from _ssl import RAND_egdsetup( console=['puoka_2.py'], options = {'py2exe': {'packages': ['requests']}})",ImportError: cannot import name RAND_egd
How to parallelized file downloads? - python3," I can download a file at a time with: I could try to subprocess it as such: Is there any way to parallelize urlretrieve without using os.system or subprocess to cheat?Given that I must resort to the ""cheat"" for now, is subprocess.Popen the right way to download the data? When using the parallelized_commandline() above, it's using multi-thread but not multi-core for the wget, is that normal? Is there a way to make it multi-core instead of multi-thread? <code>  import urllib.requesturls = ['foo.com/bar.gz', 'foobar.com/barfoo.gz', 'bar.com/foo.gz']for u in urls: urllib.request.urlretrieve(u) import subprocessimport osdef parallelized_commandline(command, files, max_processes=2): processes = set() for name in files: processes.add(subprocess.Popen([command, name])) if len(processes) >= max_processes: os.wait() processes.difference_update( [p for p in processes if p.poll() is not None]) #Check if all the child processes were closed for p in processes: if p.poll() is None: p.wait()urls = ['http://www.statmt.org/wmt15/training-monolingual-nc-v10/news-commentary-v10.en.gz','http://www.statmt.org/wmt15/training-monolingual-nc-v10/news-commentary-v10.cs.gz', 'http://www.statmt.org/wmt15/training-monolingual-nc-v10/news-commentary-v10.de.gz']parallelized_commandline('wget', urls)",How to parallelize file downloads?
How to parallelized file downloads?," I can download a file at a time with: I could try to subprocess it as such: Is there any way to parallelize urlretrieve without using os.system or subprocess to cheat?Given that I must resort to the ""cheat"" for now, is subprocess.Popen the right way to download the data? When using the parallelized_commandline() above, it's using multi-thread but not multi-core for the wget, is that normal? Is there a way to make it multi-core instead of multi-thread? <code>  import urllib.requesturls = ['foo.com/bar.gz', 'foobar.com/barfoo.gz', 'bar.com/foo.gz']for u in urls: urllib.request.urlretrieve(u) import subprocessimport osdef parallelized_commandline(command, files, max_processes=2): processes = set() for name in files: processes.add(subprocess.Popen([command, name])) if len(processes) >= max_processes: os.wait() processes.difference_update( [p for p in processes if p.poll() is not None]) #Check if all the child processes were closed for p in processes: if p.poll() is None: p.wait()urls = ['http://www.statmt.org/wmt15/training-monolingual-nc-v10/news-commentary-v10.en.gz','http://www.statmt.org/wmt15/training-monolingual-nc-v10/news-commentary-v10.cs.gz', 'http://www.statmt.org/wmt15/training-monolingual-nc-v10/news-commentary-v10.de.gz']parallelized_commandline('wget', urls)",How to parallelize file downloads?
How does this decorator work?," I found this decorator that times out a function here on Stack Overflow, and I am wondering if someone could explain in detail how it works, as the code is very elegant but not clear at all. Usage is @timeout(timelimit). <code>  from functools import wrapsimport errnoimport osimport signalclass TimeoutError(Exception): passdef timeout(seconds=100, error_message=os.strerror(errno.ETIME)): def decorator(func): def _handle_timeout(signum, frame): raise TimeoutError(error_message) def wrapper(*args, **kwargs): signal.signal(signal.SIGALRM, _handle_timeout) signal.alarm(seconds) try: result = func(*args, **kwargs) finally: signal.alarm(0) return result return wraps(func)(wrapper) return decorator",How does the @timeout(timelimit) decorator work?
pandas How to eliminate rows with value ending with a specific character," I have a pandas DataFrame as follows: that looks like: What I want to do is to filter out (elimiante) all those rows in which the value in the column mail ends with '@gmail.com'. <code>  mail = DataFrame({'mail' : ['adv@gmail.com', 'fhngn@gmail.com', 'foinfo@yahoo.com', 'njfjrnfjrn@yahoo.com', 'nfjebfjen@hotmail.com', 'gnrgiprou@hotmail.com', 'jfei@hotmail.com']}) mail0 adv@gmail.com1 fhngn@gmail.com2 foinfo@yahoo.com3 njfjrnfjrn@yahoo.com4 nfjebfjen@hotmail.com5 gnrgiprou@hotmail.com6 jfei@hotmail.com",pandas: how to eliminate rows with value ending with a specific character?
Python read from subprocess stdout and stderr together," I have a python subprocess that I'm trying to read output and error streams from. Currently I have it working, but I'm only able to read from stderr after I've finished reading from stdout. Here's what it looks like: As you can see, the stderr for loop can't start until the stdout loop completes. How can I modify this to be able to read from both in the correct order the lines come in?To clarify: I still need to be able to tell whether a line came from stdout or stderr because they will be treated differently in my code. <code>  process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)stdout_iterator = iter(process.stdout.readline, b"""")stderr_iterator = iter(process.stderr.readline, b"""")for line in stdout_iterator: # Do stuff with line print linefor line in stderr_iterator: # Do stuff with line print line",Python read from subprocess stdout and stderr separately while preserving order
Python read from subprocess stdout and stderr separately," I have a python subprocess that I'm trying to read output and error streams from. Currently I have it working, but I'm only able to read from stderr after I've finished reading from stdout. Here's what it looks like: As you can see, the stderr for loop can't start until the stdout loop completes. How can I modify this to be able to read from both in the correct order the lines come in?To clarify: I still need to be able to tell whether a line came from stdout or stderr because they will be treated differently in my code. <code>  process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)stdout_iterator = iter(process.stdout.readline, b"""")stderr_iterator = iter(process.stderr.readline, b"""")for line in stdout_iterator: # Do stuff with line print linefor line in stderr_iterator: # Do stuff with line print line",Python read from subprocess stdout and stderr separately while preserving order
Python: Convert XML to CSV file," I have an XML file like this: I'm trying to convert it to a CSV file like this: My problem is, both the parent and child names are the same - 'att' and 'attval'. How do I tell Python to distinguish between them both and give me the output?I tried this: and it printed the same things twice. <code>  <hierachy> <att> <Order>1</Order> <attval>Data</attval> <children> <att> <Order>1</Order> <attval>Studyval</attval> </att> <att> <Order>2</Order> <attval>Site</attval> </att> </children> </att> <att> <Order>2</Order> <attval>Info</attval> <children> <att> <Order>1</Order> <attval>age</attval> </att> <att> <Order>2</Order> <attval>gender</attval> </att> </children> </att></hierachy> Data,StudyvalDate,SiteInfo,ageInfo,gender import xml.etree.cElementTree as ETtree = ET.parse('input.xml')rebase = tree.getroot()list = []for att in rebase.findall('att'): name = att.find('attval').text for each_att in att.findall('attval'): try: val = att.find('attval').text print name, val except AttributeError: print name",Convert XML to CSV file
What is this python code?," I just came across this line of python: I have no idea what it is doing, other than it is looping over the list child_orders and placing the result in order.messages.What does it do and what is it called? <code>  order.messages = {c.Code:[] for c in child_orders}",What is this python expression containing curly braces and a for in loop?
Store and retrieve image file as binary data in PostgreSQL using SQLAlchemy, I want to upload a file and store it in the database. I created a LargeBinary column. I read the uploaded file and store it in the database. Is this the proper way to store an image as binary in the database? How do I convert the binary data into an image again to display it? <code>  logo = db.Column(db.LargeBinary) files = request.files.getlist('file')if files: event.logo = files[0].file.read(),Serve image stored in SQLAlchemy LargeBinary column
Parsing a pdf(Devanagari script) using PDFminer gives in correct output, I am trying to parse a pdf file containing Indian voters list which is in hindi(Devanagari script). PDF displays all the text correctly but when I tried dumping this pdf into text format using PDFminer it output the characters which are different from the original pdf charactersFor exampleDisplayed/Correct word is But the output word is Now I want to know why this is happening and how do I correctly parse this type of pdf fileI am also including the sample pdf file-http://164.100.180.82/Rollpdf/AC276/S24A276P001.pdf <code> ,Parsing a pdf(Devanagari script) using PDFminer gives incorrect output
Numpy: all indices of unique coordinate positions in matrix," So, I have been browsing stackoverflow for quite some time now, but I can't seem to find the solution for my problemConsider this The coo array contains the (x, y) coordinate positionsx = (1, 2, 3, 3, 1, 5, 1)y = (2, 3, 4, 4, 2, 6, 2)and the values array some sort of data for this grid point.Now I want to get the average of all values for each unique grid point.For example the coordinate (1, 2) occurs at the positions (0, 4, 6), so for this point I want values[[0, 4, 6]].How could I get this for all unique grid points? <code>  import numpy as npcoo = np.array([[1, 2], [2, 3], [3, 4], [3, 4], [1, 2], [5, 6], [1, 2]])values = np.array([1, 2, 4, 2, 1, 6, 1])",Numpy: Average of values corresponding to unique coordinate positions
Python re.findall behaves weird," The source string is: and here is my pattern: however, re.search can give me correct result: re.findall just dump out an empty list: why can't re.findall give me the expected list: <code>  # Python 3.4.3s = r'abc123d, hello 3.1415926, this is my book' pattern = r'-?[0-9]+(\\.[0-9]*)?|-?\\.[0-9]+' m = re.search(pattern, s)print(m) # output: <_sre.SRE_Match object; span=(3, 6), match='123'> L = re.findall(pattern, s)print(L) # output: ['', '', ''] ['123', '3.1415926']",re.search and re.findall return different values although using the same arguments
re.findall behaves weird," The source string is: and here is my pattern: however, re.search can give me correct result: re.findall just dump out an empty list: why can't re.findall give me the expected list: <code>  # Python 3.4.3s = r'abc123d, hello 3.1415926, this is my book' pattern = r'-?[0-9]+(\\.[0-9]*)?|-?\\.[0-9]+' m = re.search(pattern, s)print(m) # output: <_sre.SRE_Match object; span=(3, 6), match='123'> L = re.findall(pattern, s)print(L) # output: ['', '', ''] ['123', '3.1415926']",re.search and re.findall return different values although using the same arguments
Pass variable from Flask to HTML via yield," I have a view that generates data and streams it in real time. I can't figure out how to send this data to a variable that I can use in my HTML template. My current solution just outputs the data to a blank page as it arrives, which works, but I want to include it in a larger page with formatting. How do I update, format, and display the data as it is streamed to the page? <code>  import flaskimport time, mathapp = flask.Flask(__name__)@app.route('/')def index(): def inner(): # simulate a long process to watch for i in range(500): j = math.sqrt(i) time.sleep(1) # this value should be inserted into an HTML template yield str(i) + '<br/>\n' return flask.Response(inner(), mimetype='text/html')app.run(debug=True)",Display data streamed from a Flask view as it updates
"Given a pandas dataframe row, what is the fastest way to find the column holding nth element of the row that is not NaN"," I have a Python pandas DataFrame in which each element is a float or NaN. For each row, I will need to find the column which holds the nth number of the row. That is, I need to get the column holding the nth element of the row that is not NaN. I know that the nth such column always exists.So if n was 4 and a pandas dataframe called myDF was the following: I would want to obtain: I could do: I'm sure this is very slow and not very Pythonic. Is there an approach that will be faster if I am dealing with a very large DataFrame and large values of n? <code>  10 20 30 40 50 60 70 80 90 100'A' 4.5 5.5 2.5 NaN NaN 2.9 NaN NaN 1.1 1.8'B' 4.7 4.1 NaN NaN NaN 2.0 1.2 NaN NaN NaN'C' NaN NaN NaN NaN NaN 1.9 9.2 NaN 4.4 2.1'D' 1.1 2.2 3.5 3.4 4.5 NaN NaN NaN 1.9 5.5 'A' 60'B' 70'C' 100 'D' 40 import pandas as pdimport mathn = some arbitrary intfor row in myDF.indexes: num_not_NaN = 0 for c in myDF.columns: if math.isnan(myDF[c][row]) == False: num_not_NaN +=1 if num_not_NaN==n: print row, c break","For each row, what is the fastest way to find the column holding nth element that is not NaN?"
Python: How do you convert datetime/timestamp from one timezone to another timezone?," Specifically, given the timezone of my server (system time perspective) and a timezone input, how do I calculate the system time as if it were in that new timezone (regardless of daylight savings, etc)? <code>  import datetimecurrent_time = datetime.datetime.now() #system timeserver_timezone = ""US/Eastern""new_timezone = ""US/Pacific""current_time_in_new_timezone = ???",Python: How do you convert a datetime/timestamp from one timezone to another timezone?
Get value of a form ID python/flask," How do you get the actual value of the input id after you send it in Flask?form: like, what I am trying to say is when the form is sent (i.e. when you do this): the value of the text field is passed. What I can't figure out is how to get the value of the id.So, when the form is sent we could then tell from which form the info was coming from, because each form has a unique id. In this case the id is number_one.So, how do we go about getting the actual literal value of the id and not the text input?  <code>  <form action="""" method=""post""> <input id = ""number_one"" type=""text"" name=""comment""> <input type=""submit"" value = ""comment""></form> request.form.get(""comment"")",Get value of a form input by ID python/flask
how dose numpy.ndarray.transpose() permute the axis of n-d array?," When we pass a tuple of integers to the transpose() function, what happens? To be specific, this is a 3D array: how does NumPy transform the array when I pass the tuple of axes (1, 0 ,2)? Can you explain which row or column these integers refer to? And what are axis numbers in the context of NumPy? <code>  In [28]: arr = np.arange(16).reshape((2, 2, 4))In [29]: arrOut[29]: array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]])In [32]: arr.transpose((1, 0, 2))Out[32]: array([[[ 0, 1, 2, 3], [ 8, 9, 10, 11]], [[ 4, 5, 6, 7], [12, 13, 14, 15]]])",How does NumPy's transpose() method permute the axes of an array?
How does numpy.ndarray.transpose() permute the axis of n-d array?," When we pass a tuple of integers to the transpose() function, what happens? To be specific, this is a 3D array: how does NumPy transform the array when I pass the tuple of axes (1, 0 ,2)? Can you explain which row or column these integers refer to? And what are axis numbers in the context of NumPy? <code>  In [28]: arr = np.arange(16).reshape((2, 2, 4))In [29]: arrOut[29]: array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]]])In [32]: arr.transpose((1, 0, 2))Out[32]: array([[[ 0, 1, 2, 3], [ 8, 9, 10, 11]], [[ 4, 5, 6, 7], [12, 13, 14, 15]]])",How does NumPy's transpose() method permute the axes of an array?
Delecting a div with a particlular class using beautiful soup, I want to delete the specific div from soup object. I am using python 2.7 and bs4. According to documentation we can use div.decompose(). But that would delete all the div. How can I delete a div with specific class? <code> ,Deleting a div with a particlular class using BeautifulSoup
aggregating and sorting text file in python," I have the file named ""names.txt"" having the following contents: Problem statement : File ""names.txt"" contains some student records in the format - {""number"": [year of birth, ""name rank""]}Parse this file and Segregate them according to year and then sort the names according to rank. First segregation and then sorting.Output should be in the format - So the expected output is - First How to store this file content in a dictionary object? Then Grouping by year & then ordering names by rank? How to achieve this in Python?Thanks.. <code>  {""1"":[1988, ""Anil 4""], ""2"":[2000, ""Chris 4""], ""3"":[1988, ""Rahul 1""],""4"":[2001, ""Kechit 3""], ""5"":[2000, ""Phil 3""], ""6"":[2001, ""Ravi 4""],""7"":[1988, ""Ramu 3""], ""8"":[1988, ""Raheem 5""], ""9"":[1988, ""Kranti 2""],""10"":[2000, ""Wayne 1""], ""11"":[2000, ""Javier 2""], ""12"":[2000, ""Juan 2""],""13"":[2001, ""Gaston 2""], ""14"":[2001, ""Diego 5""], ""15"":[2001, ""Fernando 1""]} {year : [Names of students in sorted order according to rank]} {1988:[""Rahul 1"",""Kranti 2"",""Rama 3"",""Anil 4"",""Raheem 5""],2000:[""Wayne 1"",""Javier 2"",""Jaan 2"",""Phil 3"",""Chris 4""],2001:[""Fernando 1"",""Gaston 2"",""Kechit 3"",""Ravi 4"",""Diego 5""]}","Parsing, Aggregating & Sorting text file in Python"
Python PIL bitmap from array with mode=1," Playing with PIL (and numpy) for the first time ever. I was trying to generate a black and white checkerboard image through mode='1', but it doesn't work. Sorry browser is probably going to try to interpolate this, but it is an 8x8 PNG.What am I missing?Relevant PIL docs: https://pillow.readthedocs.org/handbook/concepts.html#concept-modes <code>  from PIL import Imageimport numpy as npif __name__ == '__main__': g = np.asarray(dtype=np.dtype('uint8'), a=[ [0, 1, 0, 1, 0, 1, 0, 1, ], [1, 0, 1, 0, 1, 0, 1, 0, ], [0, 1, 0, 1, 0, 1, 0, 1, ], [1, 0, 1, 0, 1, 0, 1, 0, ], [0, 1, 0, 1, 0, 1, 0, 1, ], [1, 0, 1, 0, 1, 0, 1, 0, ], [0, 1, 0, 1, 0, 1, 0, 1, ], [1, 0, 1, 0, 1, 0, 1, 0, ], ]) print(g) i = Image.fromarray(g, mode='1') i.save('checker.png') $ pip freezenumpy==1.9.2Pillow==2.9.0wheel==0.24.0",Python PIL bitmap/png from array with mode=1
Is there a multi-dimensional version of arange/linspace in numpy?," I would like a list of 2d NumPy arrays (x,y) , where each x is in {-5, -4.5, -4, -3.5, ..., 3.5, 4, 4.5, 5} and the same for y.I could do and then iterate through all possible pairs, but I'm sure there's a nicer way...I would like something back that looks like: but the order does not matter. <code>  x = np.arange(-5, 5.1, 0.5)y = np.arange(-5, 5.1, 0.5) [[-5, -5], [-5, -4.5], [-5, -4], ... [5, 5]]",Is there a multi-dimensional version of arange/linspace in numpy?
How to calculate a Cartesian product of two arrays in NumPy?," I would like a list of 2d NumPy arrays (x,y) , where each x is in {-5, -4.5, -4, -3.5, ..., 3.5, 4, 4.5, 5} and the same for y.I could do and then iterate through all possible pairs, but I'm sure there's a nicer way...I would like something back that looks like: but the order does not matter. <code>  x = np.arange(-5, 5.1, 0.5)y = np.arange(-5, 5.1, 0.5) [[-5, -5], [-5, -4.5], [-5, -4], ... [5, 5]]",Is there a multi-dimensional version of arange/linspace in numpy?
python outlook email signiture, Does anyone know how to add an email signature to an email using win32com? <code>  import win32com.client as win32outlook = win32.Dispatch('outlook.application')mail = outlook.CreateItem(0)mail.To = 'TO'mail.Subject = 'SUBJECT'mail.HTMLbody = 'BODY'mail.send,Add signature to outlook email with python using win32com
Add signature to outlook email with python on win32com, Does anyone know how to add an email signature to an email using win32com? <code>  import win32com.client as win32outlook = win32.Dispatch('outlook.application')mail = outlook.CreateItem(0)mail.To = 'TO'mail.Subject = 'SUBJECT'mail.HTMLbody = 'BODY'mail.send,Add signature to outlook email with python using win32com
pattern for saving newline-delimited json with python," With Python, I'm saving json documents onto separate lines like this: and then reading like this: The ease and simplicity make me think that there must be a gotcha? Is this all there is to linejson, aka jsonlines? <code>  from bson import json_util # pymongowith open('test.json', 'ab') as f: for document in documents: f.write(json_util.dumps(document)+'\n') with open('test.json') as f: for line in f: document = json_util.loads(line)","pattern for saving newline-delimited json (aka linejson, jsonlines, .jsonl files) with python"
pattern for saving newline-delimited json aka linejson aka jsonlines with python," With Python, I'm saving json documents onto separate lines like this: and then reading like this: The ease and simplicity make me think that there must be a gotcha? Is this all there is to linejson, aka jsonlines? <code>  from bson import json_util # pymongowith open('test.json', 'ab') as f: for document in documents: f.write(json_util.dumps(document)+'\n') with open('test.json') as f: for line in f: document = json_util.loads(line)","pattern for saving newline-delimited json (aka linejson, jsonlines, .jsonl files) with python"
How to get process name by pid on Linux?," I want to get the process name, given it's pid in python.Is there any direct method in python? <code> ",How to get the process name by pid in Linux using Python?
How to get the process name by pid in python on Linux?," I want to get the process name, given it's pid in python.Is there any direct method in python? <code> ",How to get the process name by pid in Linux using Python?
How do I get old IPython profile functionality out of Jupyter?," There was official(?) recommendation of running an IPython Notebook server, and creating a profile via as recommended in http://ipython.org/ipython-doc/1/interactive/public_server.html. This allowed for very different and very useful behavior when starting an IPython Notebook via ipython notebook and ipython notebook --profile=nbserver.With Jupyter 4.0, there's a change and there are no longer profiles. I've found the conversation https://gitter.im/ipython/ipython/archives/2015/05/29 which has user minrk saying:The .ipython directory has several things in it:multiple config directories (called profiles)one 'data' directory, containing things like kernelspecs, nbextensionsruntime info scattered throughout, but mostly in profilesJupyter follows more platform-appropriate conventions:one config dir at JUPYTER_CONFIG_DIR, default: .jupyterone data dir at JUPYTER_DATA_DIR, default: platform-specificone runtime dir at JUPYTER_RUNTIME_DIR, default: platform-specificAnd a rather cryptic remark:If you want to use different config, specify a different config directory with JUPYTER_CONFIG_DIR=whateverWhat's the best way to get different behavior (say, between when running as a server vs normal usage)?Will it involve running something like: whenever a server 'profile' needs to be run? and whenever a 'normal' profile needs to run? Because that seems terrible. What's the best way to do this in Jupyter 4.0? <code>  $ ipython profile create nbserver $ export JUPYTER_CONFIG_DIR=~/.jupyter-nbserver$ jupyter notebook $ export JUPYTER_CONFIG_DIR=~/.jupyter$ jupyter notebook",How do I get IPython profile behavior from Jupyter 4.x?
Incorrect bin orientation," I have the following code to plot a histogram. The values in time_new are the hours when something occurred. This produces a histogram, but the bins are not aligned as I would like. I want the hour to be in the centre of the bin, not on the edge.I referred to this question / answer, but it seems not to answer the question either.I tried the following code for the histogram plot instead, but it didn't plot a bar for the value 23 Can anyone help me to get 24 bins, with the hour at the centre of each? <code>  time_new=[9, 23, 19, 9, 1, 2, 19, 5, 4, 20, 23, 10, 20, 5, 21, 17, 4, 13, 8, 13, 6, 19, 9, 14, 9, 10, 23, 19, 23, 20, 19, 6, 5, 24, 20, 19, 15, 14, 19, 14, 15, 21] hour_list = time_new print hour_list numbers=[x for x in xrange(0,24)] labels=map(lambda x: str(x), numbers) plt.xticks(numbers, labels) plt.xlim(0,24) pdb.set_trace() plt.hist(hour_list,bins=24) plt.show() plt.hist(hour_list, bins=np.arange(24)-0.5)",Put value at centre of bins for histogram
Is this valid use of if expression?, I'm trying to figure out what the best way of doing this is: I think the code is quite simple; if the rows have a value then add them to the list. Is this approach considered OK? Is there any other approach that would be better? The toPython method will return the string description of the contained object. <code>  resource['contents'][media_type] = []resource['contents'][media_type].append(row[0].toPython()) if row[0] is not None else Noneresource['contents'][media_type].append(row[2].toPython()) if row[2] is not None else None,Is it valid to use conditional expressions for side effects?
Is this a valid use of a conditional expression?, I'm trying to figure out what the best way of doing this is: I think the code is quite simple; if the rows have a value then add them to the list. Is this approach considered OK? Is there any other approach that would be better? The toPython method will return the string description of the contained object. <code>  resource['contents'][media_type] = []resource['contents'][media_type].append(row[0].toPython()) if row[0] is not None else Noneresource['contents'][media_type].append(row[2].toPython()) if row[2] is not None else None,Is it valid to use conditional expressions for side effects?
Networkx never finish calculating Betweenness centrality for 2 mil nodes," I have a simple twitter users graph with around 2 million nodes and 5 million edges. I'm trying to play around with Centrality. However, the calculation takes a really long time (more than an hour). I don't consider my graph to be super large so I'm guessing there might be something wrong with my code. Here's my code. The data is in MongoDB. Here's the sample of data. I tried using betweenness centrality parallel to speed up but it's still super slow. https://networkx.github.io/documentation/latest/examples/advanced/parallel_betweenness.html The import process from Mongodb to networkx is relatively fast, less than a minute. <code>  %matplotlib inlineimport pymongoimport networkx as nximport timeimport itertoolsfrom multiprocessing import Poolfrom pymongo import MongoClientfrom sweepy.get_config import get_configconfig = get_config()MONGO_URL = config.get('MONGO_URL')MONGO_PORT = config.get('MONGO_PORT')MONGO_USERNAME = config.get('MONGO_USERNAME')MONGO_PASSWORD = config.get('MONGO_PASSWORD')client = MongoClient(MONGO_URL, int(MONGO_PORT))db = client.tweetsdb.authenticate(MONGO_USERNAME, MONGO_PASSWORD)users = db.usersgraph = nx.DiGraph()for user in users.find(): graph.add_node(user['id_str']) for friend_id in user['friends_ids']: if not friend_id in graph: graph.add_node(friend_id) graph.add_edge(user['id_str'], friend_id) { ""_id"" : ObjectId(""55e1e425dd232e5962bdfbdf""), ""id_str"" : ""246483486"", ... ""friends_ids"" : [ // a bunch of ids ]} """"""Example of parallel implementation of betweenness centrality using themultiprocessing module from Python Standard Library.The function betweenness centrality accepts a bunch of nodes and computesthe contribution of those nodes to the betweenness centrality of the wholenetwork. Here we divide the network in chunks of nodes and we compute theircontribution to the betweenness centrality of the whole network.""""""def chunks(l, n): """"""Divide a list of nodes `l` in `n` chunks"""""" l_c = iter(l) while 1: x = tuple(itertools.islice(l_c, n)) if not x: return yield xdef _betmap(G_normalized_weight_sources_tuple): """"""Pool for multiprocess only accepts functions with one argument. This function uses a tuple as its only argument. We use a named tuple for python 3 compatibility, and then unpack it when we send it to `betweenness_centrality_source` """""" return nx.betweenness_centrality_source(*G_normalized_weight_sources_tuple)def betweenness_centrality_parallel(G, processes=None): """"""Parallel betweenness centrality function"""""" p = Pool(processes=processes) node_divisor = len(p._pool)*4 node_chunks = list(chunks(G.nodes(), int(G.order()/node_divisor))) num_chunks = len(node_chunks) bt_sc = p.map(_betmap, zip([G]*num_chunks, [True]*num_chunks, [None]*num_chunks, node_chunks)) # Reduce the partial solutions bt_c = bt_sc[0] for bt in bt_sc[1:]: for n in bt: bt_c[n] += bt[n] return bt_cprint(""Computing betweenness centrality for:"")print(nx.info(graph))start = time.time()bt = betweenness_centrality_parallel(graph, 2)print(""\t\tTime: %.4F"" % (time.time()-start))print(""\t\tBetweenness centrality for node 0: %.5f"" % (bt[0]))",Networkx never finishes calculating Betweenness centrality for 2 mil nodes
"Add a + sign before every word in a DF column (Pandas,Python 3)"," I have a DataFrame called ""Animals"" that looks like this: I want to add a plus sign before each word so that it looks like this: I have tried this using regex but it did not work: <code>  Words The Black Cat The Red Dog Words +The +Black +Cat +The +Red +Dog df = re.sub(r'([a-z]+)', r'+\1', Animals)",How do I apply a regex substitution in a string column of a DataFrame?
Add a + sign before every word in a DataFrame column," I have a DataFrame called ""Animals"" that looks like this: I want to add a plus sign before each word so that it looks like this: I have tried this using regex but it did not work: <code>  Words The Black Cat The Red Dog Words +The +Black +Cat +The +Red +Dog df = re.sub(r'([a-z]+)', r'+\1', Animals)",How do I apply a regex substitution in a string column of a DataFrame?
How do I apply a regex substitution in a string column of a data frame?," I have a DataFrame called ""Animals"" that looks like this: I want to add a plus sign before each word so that it looks like this: I have tried this using regex but it did not work: <code>  Words The Black Cat The Red Dog Words +The +Black +Cat +The +Red +Dog df = re.sub(r'([a-z]+)', r'+\1', Animals)",How do I apply a regex substitution in a string column of a DataFrame?
DRY aproche for Django," In my urls.py I have some Entries like these: This repeats for a lot of standard models where I just have to get the information, list it and be able to edit and delete it.In my views.py: It all repeats after this pattern. So for 3 models I will have 3 times mostly identical code with just minor changes.How can I stop repeating myself? <code>  url(r'auftragsarten/list/$', generic.ListView.as_view( queryset=Auftragsart.objects.order_by('name'), paginate_by=25), name='auftragsarten_liste'),url(r'^auftragsarten/form/$', views.auftragsarten_form, name='auftragsarten_form'),url(r'auftragsarten/update/(?P<pk>[\d]+)/$', views.AuftragsartUpdateView.as_view(), name='auftragsarten_update'),url(r'auftragsarten/delete/(?P<pk>[\d]+)/$', views.AuftragsartDeleteView.as_view(), name='auftragsarten_delete'), def auftragsarten_form(request): form = AuftragsartenForm(request.POST or None) if form.is_valid(): form.save() return redirect('auftragsverwaltung:auftragsarten_liste') else: print(form.errors) return render(request, 'generic_form_template.html', {'form': form})class AuftragsartUpdateView(UpdateView): model = Auftragsart fields = '__all__' success_url = reverse_lazy('auftragsverwaltung:auftragsarten_liste') template_name = 'generic_update_view.html'class AuftragsartDeleteView(DeleteView): model = Auftragsart success_url = reverse_lazy('auftragsverwaltung:auftragsarten_liste') template_name = 'generic_confirm_delete.html'",DRY approach for Django
python 3.x filter items by value in dict," I have a dict setup like so: This is a basic example of what it looks like, I need a way to filter (copy only the {characters}) that match certain values, such as I want only characters that are level 23, or that are carrying a sword.I was looking at doing something like this: and return: I am not sure how to filter either a specific item like k:v or k:[v1,v2,v3] when I do not know if it is a single value, or a list of values, or how to filter multiple values.I am not sure how I can filter character's with multiple keys. Say that I want to sort out characters that are lvl 23, or have items['sword'] or items['mace']. How would I have it sort in a way filter_cards(deck, ['lvl'=23, 'items'=['sword','mace'])So if any character is lvl 23, or carries a mace or a sword, they are on that list. <code>  deck = [{ 'name': 'drew', 'lvl': 23, 'items': ['sword', 'axe', 'mana_potion']}, { 'name': 'john', 'lvl': 23, 'items': ['sword', 'mace', 'health_potion']}] filtered = filter_deck(deck, 'mace')def filter_deck(self, deck, filt): return [{k:v for (k,v) in deck.items() if filt in k}] filtered = [{ 'name': 'john', 'lvl': 23, 'items': ['sword', 'mace', 'health_potion']}]",filter items by value in dict
Turning of Info Logging in PySpark without changing the Log4j.properties File," I working in a cluster where I do not have the permission to change the file log4j.properties to stop the info logging while using pyspark (as explained in first answer here.) The following solution as explained in the above question's first answer work for spark-shell (scala) But for spark with python (ie pyspark), it didn't work nor the following How can I stop the verbose printing of info in pyspark WITHOUT changing log4j.properties file? <code>  import org.apache.log4j.Loggerimport org.apache.log4j.Level Logger.getLogger(""org"").setLevel(Level.OFF)Logger.getLogger(""akka"").setLevel(Level.OFF)",How to turn off INFO from logs in PySpark with no changes to log4j.properties?
django makemigrations not creating tables in the db," I used makemigrations earlier in order to make my Django app aware of the tables in my legacy MySql database, and it worked fine. It generated models.py. Now, I want to add a new ""UserDetails"" model to my app: After saving the file, I ran the following command in the command prompt: But makemigrations is not seeing the new class I wrote inside the models.py, as it always outputs 'No changes detected'Am I doing it correctly or is there anything wrong with it? I checked my MySQL db and confirmed that no table named UserDetails already exists. <code>  class UserDetails(models.Model): user = models.CharField(max_length=255) telephone = models.CharField(max_length=100) python manage.py makemigrations",django makemigrations not detecting new model
How to get the physical length of a text string in python," Similar to this question, I'm not asking how to find the number of characters in a string. I would like to determine the visual length of a string as rendered or compare it to another string.For example, both 'iiii' and 'WWWW' have four characters. However, 'iiii' is shorter visually. I'm aware that this is determined by font, and I'm not working with monospaced fonts. So, for the purposes of this problem, I'll be using Arial 10pt.Are there any built-in modules which will provide the visual dimensions of a string given a font?  <code> ",How to get the visual length of a text string in python
create labeledPoints from spark dataframe in python," What .map() function in python do I use to create a set of labeledPoints from a spark dataframe? What is the notation if The label/outcome is not the first column but I can refer to its column name, 'status'?I create the Python dataframe with this .map() function: I convert it to a Spark dataframe after the reduce function has recombined all the Pandas dataframes. But now how do I create labledPoints from this in Python? I assume it may be another .map() function? <code>  def parsePoint(line): listmp = list(line.split('\t')) dataframe = pd.DataFrame(pd.get_dummies(listmp[1:]).sum()).transpose() dataframe.insert(0, 'status', dataframe['accepted']) if 'NULL' in dataframe.columns: dataframe = dataframe.drop('NULL', axis=1) if '' in dataframe.columns: dataframe = dataframe.drop('', axis=1) if 'rejected' in dataframe.columns: dataframe = dataframe.drop('rejected', axis=1) if 'accepted' in dataframe.columns: dataframe = dataframe.drop('accepted', axis=1) return dataframe parsedData=sqlContext.createDataFrame(parsedData)",Create labeledPoints from Spark DataFrame in Python
Does spark predicate pushdown work with jdbc?," According to this Catalyst applies logical optimizations such as predicate pushdown. The optimizer can push filter predicates down into the data source, enabling the physical execution to skip irrelevant data.Spark supports push down of predicates to the data source.Is this feature also available / expected for JDBC?(From inspecting the DB logs I can see it's not the default behavior right now - the full query is passed to the DB, even if it's later limited by spark filters)MORE DETAILSRunning Spark 1.5 with PostgreSQL 9.4code snippet: SQL Trace: I would expect that the last select will include a limit 1 clause - but it doesn't  <code>  from pyspark import SQLContext, SparkContext, Row, SparkConffrom data_access.data_access_db import REMOTE_CONNECTIONsc = SparkContext()sqlContext = SQLContext(sc)url = 'jdbc:postgresql://{host}/{database}?user={user}&password={password}'.format(**REMOTE_CONNECTION)sql = ""dummy""df = sqlContext.read.jdbc(url=url, table=sql)df = df.limit(1)df.show() < 2015-09-15 07:11:37.718 EDT >LOG: execute <unnamed>: SET extra_float_digits = 3 < 2015-09-15 07:11:37.771 EDT >LOG: execute <unnamed>: SELECT * FROM dummy WHERE 1=0 < 2015-09-15 07:11:37.830 EDT >LOG: execute <unnamed>: SELECT c.oid, a.attnum, a.attname, c.relname, n.nspname, a.attnotnull OR (t.typtype = 'd' AND t.typnotnull), pg_catalog.pg_get_expr(d.adbin, d.adrelid) LIKE '%nextval(%' FROM pg_catalog.pg_class c JOIN pg_catalog.pg_namespace n ON (c.relnamespace = n.oid) JOIN pg_catalog.pg_attribute a ON (c.oid = a.attrelid) JOIN pg_catalog.pg_type t ON (a.atttypid = t.oid) LEFT JOIN pg_catalog.pg_attrdef d ON (d.adrelid = a.attrelid AND d.adnum = a.attnum) JOIN (SELECT 15218474 AS oid , 1 AS attnum UNION ALL SELECT 15218474, 3) vals ON (c.oid = vals.oid AND a.attnum = vals.attnum) < 2015-09-15 07:11:40.936 EDT >LOG: execute <unnamed>: SET extra_float_digits = 3 < 2015-09-15 07:11:40.964 EDT >LOG: execute <unnamed>: SELECT ""id"",""name"" FROM dummy ",Does spark predicate pushdown work with JDBC?
read 16bit png image file using python," I'm trying to read a PNG image file written in 16-bit data type. The data should be converted to a NumPy array. But I have no idea how to read the file in '16-bit'. I tried with PIL and SciPy, but they converted the 16-bit data to 8-bit when they load it. Could anyone please let me know how to read data from a 16-bit PNG file and convert it to NumPy array without changing the datatype?The following is the script that I used. <code>  from scipy import miscimport numpy as npfrom PIL import Image#make a png file a = np.zeros((1304,960), dtype=np.uint16)a[:] = np.arange(960)misc.imsave('16bit.png',a)#read the png file using scipyb = misc.imread('16bit.png')print ""scipy:"" ,b.dtype#read the png file using PILc = Image.open('16bit.png') d = np.array(c)print ""PIL:"", d.dtype ",Read 16-bit PNG image file using Python
Spark DataFrame: Taking mean (or any aggregate operation) over columns," I have a Spark DataFrame loaded up in memory, and I want to take the mean (or any aggregate operation) over the columns. How would I do that? (In numpy, this is known as taking an operation over axis=1).If one were calculating the mean of the DataFrame down the rows (axis=0), then this is already built in: But is there a way to programmatically do this against the entries in the columns? For example, from the DataFrame below Omitting id, the means would be <code>  from pyspark.sql import functions as FF.mean(...) +--+--+---+---+|id|US| UK|Can|+--+--+---+---+| 1|50| 0| 0|| 1| 0|100| 0|| 1| 0| 0|125|| 2|75| 0| 0|+--+--+---+---+ +------+| mean|+------+| 16.66|| 33.33|| 41.67|| 25.00|+------+",Spark DataFrame: Computing row-wise mean (or any aggregate operation)
Spark DataFrame: Computing mean (or any aggregate operation) row-wise," I have a Spark DataFrame loaded up in memory, and I want to take the mean (or any aggregate operation) over the columns. How would I do that? (In numpy, this is known as taking an operation over axis=1).If one were calculating the mean of the DataFrame down the rows (axis=0), then this is already built in: But is there a way to programmatically do this against the entries in the columns? For example, from the DataFrame below Omitting id, the means would be <code>  from pyspark.sql import functions as FF.mean(...) +--+--+---+---+|id|US| UK|Can|+--+--+---+---+| 1|50| 0| 0|| 1| 0|100| 0|| 1| 0| 0|125|| 2|75| 0| 0|+--+--+---+---+ +------+| mean|+------+| 16.66|| 33.33|| 41.67|| 25.00|+------+",Spark DataFrame: Computing row-wise mean (or any aggregate operation)
How to sample Poisson processes in Python faster if the rate is non-constant?," I'm sampling a Poisson process at a millisecond time scale where the rate is not fixed. I discretise the sampling process by checking in each interval of size delta whether there is an event there or not based on the average rate in that interval. Since I'm using Python it's running a bit slower than I would hope it to be. The code I'm currently using is the following: The rate function can be arbitrary, I'm not looking for a closed form solution given a rate function.If you want some parameters to play with you can try: <code>  import numpydef generate_times(rate_function,max_t,delta): times = [] for t in numpy.arange(delta,max_t,delta): avg_rate = (rate_function(t)+rate_function(t+delta))/2.0 if numpy.random.binomial(1,1-math.exp(-avg_rate*delta/1000.0))>0: times.extend([t]) return times max_t = 1000.0delta = 0.1high_rate = 100.0low_rate = 0.0phase_length = 25.0rate_function = (lambda x: low_rate + (high_rate-low_rate)*0.5*(1+math.sin(2*math.pi*x/phase_length)))",How to sample inhomogeneous Poisson processes in Python faster than this?
Where Is My Flask Application's Stacktrace?," I'm running my Flask application with uWSGI and nginx. There's a 500 error, but the traceback doesn't appear in the browser or the logs. How do I log the traceback from Flask? The uWSGI log only shows the 500 status code, not the traceback. There's also nothing in the nginx log. <code>  uwsgi --http-socket 127.0.0.1:9000 --wsgi-file /var/webapps/magicws/service.py --module service:app --uid www-data --gid www-data --logto /var/log/magicws/magicapp.log [pid: 18343|app: 0|req: 1/1] 127.0.0.1 () {34 vars in 642 bytes} [Tue Sep 22 15:50:52 2015] GET /getinfo?color=White => generated 291 bytes in 64 msecs (HTTP/1.0 500) 2 headers in 84 bytes (1 switches on core 0)",Flask application traceback doesn't show up in server log
preferred block size when reading/writing big binary files," I need to read and write huge binary files. Is there a preferred or even optimal number of bytes (what I call BLOCK_SIZE) I should read() at a time?One byte is certainly too little, and I do not think reading 4 GB into the RAM is a good idea either - is there a 'best' block size? or does that even depend on the file-system (I'm on ext4)? What do I need to consider?Python's open() even provides a buffering argument. Would I need to tweak that as well?This is sample code that just joins the two files in-0.data and in-1.data into out.data (in real life there is more processing that is irrelevant to the question at hand). The BLOCK_SIZE is chosen equal to io.DEFAULT_BUFFER_SIZE which seems to be the default for buffering: <code>  from pathlib import Pathfrom functools import partialDATA_PATH = Path(__file__).parent / '../data/'out_path = DATA_PATH / 'out.data'in_paths = (DATA_PATH / 'in-0.data', DATA_PATH / 'in-1.data')BLOCK_SIZE = 8192def process(data): passwith out_path.open('wb') as out_file: for in_path in in_paths: with in_path.open('rb') as in_file: for data in iter(partial(in_file.read, BLOCK_SIZE), b''): process(data) out_file.write(data)# while True:# data = in_file.read(BLOCK_SIZE)# if not data:# break# process(data)# out_file.write(data)",Preferred block size when reading/writing big binary files
Pandas: How to create a data frame of random integers?," I know that if I use randn, gives me what I am looking for, but with elements from a normal distribution. But what if I just wanted random integers?randint works by providing a range, but not an array like randn does. So how do I do this with random integers between some range? <code>  import pandas as pdimport numpy as npdf = pd.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))",How to create a DataFrame of random integers with Pandas?
How to create a data frame of random integers with Pandas?," I know that if I use randn, gives me what I am looking for, but with elements from a normal distribution. But what if I just wanted random integers?randint works by providing a range, but not an array like randn does. So how do I do this with random integers between some range? <code>  import pandas as pdimport numpy as npdf = pd.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))",How to create a DataFrame of random integers with Pandas?
Next and previous elements in Django template forloop," What is the best way to get previous and next elements in Django forloop? I'm printing a list of elements and want child element to be in div-block. So I want something like this: As you can see my problem is list[forloop.counter-1]. How can I do it? <code>  {% for element in list %} {% if element.is_child and not list[forloop.counter-1].is_child %} <div class=""children-block""> {% endif %} {{ element.title }} {% if element.is_child and not list[forloop.counter+1].is_child %} </div> {% endif %}{% endfor %}",How to access the next and the previous elements in a Django template forloop?
Formatting Floats So They Allign On Decimal Point," In Python, I need to format numbers so they align on the decimal point, like so: Is there a straighforward way to do this? <code>  4.8 49.723456.781-72.18 5 13",Formatting Numbers So They Align On Decimal Point
Formatting Numbers So They Allign On Decimal Point," In Python, I need to format numbers so they align on the decimal point, like so: Is there a straighforward way to do this? <code>  4.8 49.723456.781-72.18 5 13",Formatting Numbers So They Align On Decimal Point
How to override default python functions like round(), I Want to override default round() function of python because I have to convert the result of round() function into integer. By default round() returns value in float.The code given below is returning error message.RuntimeError: maximum recursion depth exceeded while calling a Python object. <code>  def round(number): if type(number) is float: return int(round(number)) return None,How to override default python functions like round()?
removing the name of a pandas dataframe index after appendin a total row to a dataframe," I have calculated a series of totals tips by day of a week and appended it to the bottom of totalspt dataframe.I have set the index.name for the totalspt dataframe to None.However while the dataframe is displaying the default 0,1,2,3 index it doesn't display the default empty cell in the top left directly above the index. How could I make this cell empty in the dataframe? <code>  total_bill tip sex smoker day time size tip_pct0 16.54 1.01 F N Sun D 2 0.0618841 12.54 1.40 F N Mon D 2 0.1116432 10.34 3.50 M Y Tue L 4 0.3384913 20.25 2.50 M Y Wed D 2 0.1234574 16.54 1.01 M Y Thu D 1 0.0610645 12.54 1.40 F N Fri L 2 0.1116436 10.34 3.50 F Y Sat D 3 0.3384917 23.25 2.10 M Y Sun B 3 0.090323pivot = tips.pivot_table('total_bill', index=['sex', 'size'],columns=['day'],aggfunc='sum').fillna(0)print pivotday Fri Mon Sat Sun Thu Tue Wedsex sizeF 2 12.54 12.54 0.00 16.54 0.00 0.00 0.00 3 0.00 0.00 10.34 0.00 0.00 0.00 0.00M 1 0.00 0.00 0.00 0.00 16.54 0.00 0.00 2 0.00 0.00 0.00 0.00 0.00 0.00 20.25 3 0.00 0.00 0.00 23.25 0.00 0.00 0.00 4 0.00 0.00 0.00 0.00 0.00 10.34 0.00totals_row = tips.pivot_table('total_bill',columns=['day'],aggfunc='sum').fillna(0).astype('float')totalpt = pivot.reset_index('sex').reset_index('size')totalpt.index.name = Nonetotalpt = totalpt[['Fri', 'Mon','Sat', 'Sun', 'Thu', 'Tue', 'Wed']]totalpt = totalpt.append(totals_row)print totalpt**day** Fri Mon Sat Sun Thu Tue Wed #problem text day 0 12.54 12.54 0.00 16.54 0.00 0.00 0.001 0.00 0.00 10.34 0.00 0.00 0.00 0.002 0.00 0.00 0.00 0.00 16.54 0.00 0.003 0.00 0.00 0.00 0.00 0.00 0.00 20.254 0.00 0.00 0.00 23.25 0.00 0.00 0.005 0.00 0.00 0.00 0.00 0.00 10.34 0.00total_bill 12.54 12.54 10.34 39.79 16.54 10.34 20.25",removing the name of a pandas dataframe index after appending a total row to a dataframe
"Why does open(True, 'w') will print the text like sys.stdout.write?"," I have the following code: Why does this code print the text Hello instead of raise an error? <code>  with open(True, 'w') as f: f.write('Hello')","Why does open(True, 'w') print the text like sys.stdout.write?"
How do I get ALL Combination of Sequential elements of list Pythonically?," Given an array say x = ['A','I','R']I would want output as an What I don't want as output is : Below is the code which gives the output I don't want: My point is I only want combinations of sequences. Is there any way to use itertools or should I write custom function ? <code>  [['A','I','R'],['A','I'],['I','R'],['A'],['I'],['R']] [['A','I','R'],['A','I'],['I','R'],['A','R'],['A'],['I'],['R']] # extra ['A','R'] which is not in sequence . letter_list = [a for a in str]all_word = []for i in xrange(0,len(letter_list)): all_word = all_word + (map(list, itertools.combinations(letter_list,i))) # dont use append. gives wrong result.all_word = filter(None,all_word) # remove empty combinationall_word = all_word + [letter_list] # add original list",Python: Get all combinations of sequential elements of list
How do I get all combinations of sequential elements of list pythonically?," Given an array say x = ['A','I','R']I would want output as an What I don't want as output is : Below is the code which gives the output I don't want: My point is I only want combinations of sequences. Is there any way to use itertools or should I write custom function ? <code>  [['A','I','R'],['A','I'],['I','R'],['A'],['I'],['R']] [['A','I','R'],['A','I'],['I','R'],['A','R'],['A'],['I'],['R']] # extra ['A','R'] which is not in sequence . letter_list = [a for a in str]all_word = []for i in xrange(0,len(letter_list)): all_word = all_word + (map(list, itertools.combinations(letter_list,i))) # dont use append. gives wrong result.all_word = filter(None,all_word) # remove empty combinationall_word = all_word + [letter_list] # add original list",Python: Get all combinations of sequential elements of list
Is there a way to get the argument of the median in python?," Is there something like numpy.argmin(x), but for median? <code> ",Is there a way to get the index of the median in python in one command?
Is there a way to get the argument of the median in numpy?," Is there something like numpy.argmin(x), but for median? <code> ",Is there a way to get the index of the median in python in one command?
"Django urlize() method adding escaped html, want it unescaped"," I have a python method (thank to this snippet) that takes some html and wraps <a> tags around ONLY unformatted links, using BeautifulSoup and Django's urlize: Sample input text (as output by the first print statement) is this: The resulting return text (as output by the second print statement) is this: As you can see, it is formatting the link, but it's doing it with escaped html, so when I print it in a template {{ my.html|safe }} it doesn't render as html.So how can I get these tags that are added with urlize to be unescaped, and render properly as html? I suspect this has something do do with me using it as a method instead of a template filter? I can't actually find the docs on this method, it doesn't appear in django.utils.html.Edit: It appears the escaping actually happen in this line: textNode.replaceWith(urlizedText). <code>  from django.utils.html import urlizefrom bs4 import BeautifulSoupdef html_urlize(self, text): soup = BeautifulSoup(text, ""html.parser"") print(soup) textNodes = soup.findAll(text=True) for textNode in textNodes: if textNode.parent and getattr(textNode.parent, 'name') == 'a': continue # skip already formatted links urlizedText = urlize(textNode) textNode.replaceWith(urlizedText) print(soup) return str(soup) this is a formatted link <a href=""http://google.ca"">http://google.ca</a>, this one is unformatted and should become formatted: http://google.ca this is a formatted link <a href=""http://google.ca"">http://google.ca</a>, this one is unformatted and should become formatted: &lt;a href=""http://google.ca""&gt;http://google.ca&lt;/a&gt;","BeautifulSoup replaceWith() method adding escaped html, want it unescaped"
Under the hoods what's the difference between inherits the user and create a one to one relationship?," I want to implement users in my system. I know that Django already has an authentication system, and I've been reading the documentation. But I don't know yet the difference between And I don't want to know why to use one or another, but what happens under the hoods. What's the difference?  <code>  from django.contrib.auth.models import User class Profile(User): # others fields from django.contrib.auth.models import User class Profile(models.Model): user = models.OneToOneField(User) # others fields ",Under the hoods what's the difference between subclassing the user and creating a one to one field?
Under the hoods what's the difference between inherits from the user and create a one to one relationship?," I want to implement users in my system. I know that Django already has an authentication system, and I've been reading the documentation. But I don't know yet the difference between And I don't want to know why to use one or another, but what happens under the hoods. What's the difference?  <code>  from django.contrib.auth.models import User class Profile(User): # others fields from django.contrib.auth.models import User class Profile(models.Model): user = models.OneToOneField(User) # others fields ",Under the hoods what's the difference between subclassing the user and creating a one to one field?
Python - differnece between using requests.get() and requests.session().get()?, Sometimes I see people invoke web API using requests.Session object: But sometimes they don't: Can somebody explain when should we use Session and when we don't need them? <code>  client = requests.session()resp = client.get(url='...') resp = requests.get(url='...'),Difference between using requests.get() and requests.session().get()?
Make a 2d array with all possible columns," In numpy I would like to make a 2d arrray (r, by 2**r) where the columns are all possible binary columns. For example, if height of the columns is 5, the columns would be My solution is This seems very ugly. Is there a more elegant way? <code>  [0,0,0,0,0], [0,0,0,0,1], [0,0,0,1,0], [0,0,0,1,1], [0,0,1,0,0], ... np.array(list(itertools.product([0,1],repeat = c))).T",An elegant way to make a 2d array with all possible columns
"pyQt4 ""TypeError: native Qt signal is not callable"" with custom slots"," The EnvironmentI am running an Anaconda environment with Python 3.4. I am using PyCharm as my IDE.The ObjectiveI am trying to make a pyQt4 QPushButton connect to a custom function: AttemptsI have tried using the pyqtSlot() decorator but when I run the code it throws: I have used the following imports which should include that decorator: I also attempted to change my method into its own callable class containing a call method.The general error message that I'm getting for various attempts is this: The QuestionHonestly, at this point I have pretty much no idea where to go with this or what details you may need to diagnose the problem. Could anyone give me an idea how to put this together? <code>  button.clicked().connect([method reference or whatever]) NameError: name 'pyqtSlot' is not defined from PyQt4 import QtCore, QtGui TypeError: native Qt signal is not callable","""TypeError: native Qt signal is not callable"" with custom slots"
Call A Fucntion Through A Variable PYTHON," I'm trying to make a game engine in Python as a ""weekend project"" but I have a problem.I'm trying to make it that a user can declare a keybind by typing the key and the function into a text file what they want to run but when I run the code from that sheet with exec it runs the function and I have no idea on how to call the function through the variable. (Also I don't want it to run the function when it runs the code.)Here is the code I use for executing the code from binds.zdata Here's the text in binds.zdata <code>  for line in bind: try: exec line except: try: error.write(localtime + "" : "" + ""ERROR: Could not bind key from the bind file : "" + line) except: pass _w = Functions.motion.move().forward()_a = Functions.motion.move().left()_s = Functions.motion.move().back()_d = Functions.motion.move().right()",Call a function through a variable in Python
match a regular expression with optinal lookahead," I have the following strings:NAME John Nash FROM CaliforniaNAME John NashI want a regular expression capable of extracting 'John Nash' for both strings.Here is what I tried but none of these works for both strings. <code>  ""NAME(.*)(?:FROM)""""NAME(.*)(?:FROM)?""""NAME(.*?)(?:FROM)?""",match a regular expression with optional lookahead
"Why is it not possible to convert ""1.7"" to integer directly?"," When I type int(""1.7"") Python returns error (specifically, ValueError). I know that I can convert it to integer by int(float(""1.7"")). I would like to know why the first method returns error. <code> ","Why is it not possible to convert ""1.7"" to integer directly, without converting to float first?"
Set metadata in Google Cloud Storage using python-gcloud," I am trying to upload a file to Google Cloud Storage using gcloud-python and set some custom metadata properties. To try this I have created a simple script. I am able to upload the file contents. After uploading the file I am able to manually set metadata from the developer console and retrieve it.I can't figure out how to upload the metadata programmatically. <code>  import osfrom gcloud import storageclient = storage.Client('super secret app id')bucket = client.get_bucket('super secret bucket name')blob = bucket.get_blob('kirby.png')blob.metadata = blob.metadata or {}blob.metadata['Color'] = 'Pink'with open(os.path.expanduser('~/Pictures/kirby.png'), 'rb') as img_data: blob.upload_from_file(img_data)",Set metadata in Google Cloud Storage using gcloud-python
How can get current function name inside that function in python," For my logging purpose i want to log all the names of functions where my code is goingDoes not matter who is calling the function , i want the the function name in which i declare this line currently it prints foo , i want to print whoami <code>  import inspectdef whoami(): return inspect.stack()[1][3]def foo(): print(whoami())",get current function name inside that function using python
"Blueprints, PyMongo in Flask"," What is the correct way of picking up mongo object inside Blueprints?Here is how I have my parent login.py: in my child.py child2.py is the same structure as child: This is the error message I get: I've tried the following in the blueprint but that raises error. Here is full traceback: Once my app has created the connection, how should I pick it up in my blueprints?So the question is how can one structure the connection strings to the db in each of the blueprints. Here is the file structure: here is the config.py I've tried the suggestion below in answers, but this did not work. See my comments below that prospective answer. <code>  app.config.from_object('config')from flask.ext.pymongo import PyMongofrom child import childfrom child2 import child2app = Flask(__name__)app.register_blueprint(child2.child2)app.register_blueprint(child.child) from app import appfrom flask.ext.pymongo import PyMongomongo = PyMongo(app)child = Blueprint('child', __name__) from app import appfrom flask.ext.pymongo import PyMongo mongo = PyMongo(app)child2 = Blueprint('child2', __name__) raise Exception('duplicate config_prefix ""%s""' % config_prefix)Exception: duplicate config_prefix ""MONGO"" mongo = app.data.driver Traceback (most recent call last): File ""login.py"", line 12, in <module> from child import child File ""/home/xxx/xxx/child/child.py"", line 13, in <module> mongo = PyMongo(app) #blueprint File ""/home/xxx/xxx/lib/python3.4/site-packages/flask_pymongo/__init__.py"", line 97, in __init__ self.init_app(app, config_prefix) File ""/home/xxx/xxx/lib/python3.4/site-packages/flask_pymongo/__init__.py"", line 121, in init_app raise Exception('duplicate config_prefix ""%s""' % config_prefix)Exception: duplicate config_prefix ""MONGO""(xxx)xxx@linux:~/xxx$ python login.py Traceback (most recent call last): File ""login.py"", line 12, in <module> from courses import courses File ""/home/xxx/xxx/child/child.py"", line 13, in <module> mongo = PyMongo(app) #blueprint File ""/home/xxx/xxx/lib/python3.4/site-packages/flask_pymongo/__init__.py"", line 97, in __init__ self.init_app(app, config_prefix) File ""/home/xxx/xxx/lib/python3.4/site-packages/flask_pymongo/__init__.py"", line 121, in init_app raise Exception('duplicate config_prefix ""%s""' % config_prefix)Exception: duplicate config_prefix ""MONGO"" login.pyconfig.py/child/child.py/child2/child2.py MONGO_DBNAME = 'xxx'MONGO_URL = os.environ.get('MONGO_URL')if not MONGO_URL: MONGO_URL = ""mongodb://xxx:xxxx@xxxx.mongolab.com:55822/heroku_xxx"";MONGO_URI = MONGO_URL",How to use PyMongo with Flask Blueprints?
Python: printing text in form of circle," I'm trying to form figures of the text a person chooses. I already made a square, pyramid and a parallelogram. Where I get stuck is when I'm trying to make a circle. My thought was to use a for i in range (1, height/2) where it would print the text (f.e. --) * i times. Then I wanted another for loop to print the exact opposite, so from height/2 to height, it should first print height/2* text en for every time the for loop starts again, it should print the text one time less.I'm not completely sure if this will look like a circle somehow. I'll give my code for a pyramid and a circle. I'm aware that for the circle, I still need to do something with "" "", but as I don't really know how to get everything working in the first place, I haven't began thinking about how I'm gonna need blank spaces in that code. Example of output, see link <code>  def print_pyramid(height): text = raw_input(""Please give in what your pyramid needs to be formed from, you can choose two **, two --, or two letters"") for i in range(1,height+1): print (height-i+1)*"" "", text * idef print_circle(height): text = raw_input(""Give in what your circle is made of: **, -- or two letters."") for i in range(1,height/2): print text*i for j in range ((height/2)-1,(height/2)+1): print text*j for h in range((height/2)+2, height+1): print text*((height/2)-h)",Printing text in form of circle
"Django:how to concatenate the seperated integer field (Year, month) as date range to filter the database"," In forms, there are two fields: In sql database, there two columns: year ; month, ....I want to based on what users have entered in form (start_year,start_month ; end_year,end_month) as a date range to filter in database (year, month).XX.objects.filter(date_range=[]), or can I put in this data_range function?Following are some related code if you need.the app with form where user enter the data - views.py to filter the database based on user's entry - views.py <code>  class Input(models.Model): start_year=models.CharField(max_length=100) start_month=models.CharField(max_length=100) end_year=models.CharField(max_length=100) end_month=models.CharField(max_length=100) .... def input(request): if request.method == 'POST': form = InputForm(request.POST) if form.is_valid(): ... start_year=form.cleaned_data['start_year'] start_month=form.cleaned_data['start_month'] end_year=form.cleaned_data['end_year'] end_month=form.cleaned_data['end_month'] ... form.save() return redirect('FilterResult') class XXXView(ListView): context_object_name = 'XXX' template_name = 'XXX.html' queryset = XXX.objects.all() start_year=self.request.query_params.get('start_year', None) /*get from the form what the user has entered start_month=self.request.query_params.get('start_month', None) end_year=self.request.query_params.get('end_year', None) end_month=self.request.query_params.get('end_month', None) objects.filter(date_range=[.....]) /*how to concatenate the year and month to put here? if start_year,start_month,end_year,end_month are not None: queryset=queryset.filter(start_month=start_month,start_year=start_year,end_year=end_year,end_month=end_year) sales=XXX.objects.filter(queryset).aggregate(Sum('sales')) def get_context_data(self, **kwargs): context = super(XXXView, self).get_context_data(**kwargs) context['input'] = Input.objects.order_by('-id')[:1] return context","Django: how to concatenate the separated integer field (Year, month) as date range to filter the database"
"In python, when using sqlite3 where does databases are stored phisically?"," Sorry for the basic questions. I'm starting to work with Python. I'm using Windows 10, Python 3.5, Notepad++ as editor. Python is installed in z:\python35 and scripts are in z:\python\sqlite, then my script is really simple: When I run the script inside Notepad++ (run as administrator) I execute z:\python35\python.exe -i ""$(FULL_CURRENT_PATH)"". Looks like the script runs correctly, since it does nothing the first time run, but the next time it says: sqlite3.OperationalError: table example already exitsAnd that's okay because I wanted to create the database and the table. Thing is, where does this sample.db is stored? I don't find it in the python directory or my projects directory. Not even by searching in Windows.How can I delete the whole database? <code>  import sqlite3conn = sqlite3.connect('sample.db')c = conn.cursor()c.execute(""CREATE TABLE example (name VARCHAR)"")conn.close()","In python, when using sqlite3 where are databases stored physically?"
Pythono: get a frequency count based on two columns (variables) in pandas dataframe, Hello I have the following dataframe. I want to count the frequency of how many time the same row appears in the dataframe. <code>  Group Size Short Small Short Small Moderate Medium Moderate Small Tall Large Group Size Time Short Small 2 Moderate Medium 1 Moderate Small 1 Tall Large 1,Python: get a frequency count based on two columns (variables) in pandas dataframe some row appers
Python: get a frequency count based on two columns (variables) in pandas dataframe, Hello I have the following dataframe. I want to count the frequency of how many time the same row appears in the dataframe. <code>  Group Size Short Small Short Small Moderate Medium Moderate Small Tall Large Group Size Time Short Small 2 Moderate Medium 1 Moderate Small 1 Tall Large 1,Python: get a frequency count based on two columns (variables) in pandas dataframe some row appers
Scrape just the text within an html element with class," I'm trying to scrape a page using BeatifulSoup The problem is the text I would like to return is not enclosed in it's own html tag What I want to returnChuck Ragan - Rotterdam - Folkadelphia SessionBonus Points: The data returned is of the format Artist/Song/Album. What would be the proper data structure to use to store and manipulate this info? <code>  import urllib2from bs4 import BeautifulSoupurl='http://www.xpn.org/playlists/xpn-playlist'page = urllib2.urlopen(url)soup = BeautifulSoup(page.read())for link in soup.find_all(""li"", class_=""song""): print link <li class=""song""> <a href=""/default.htm"" onclick=""return clickreturnvalue()"" onmouseout=""delayhidemenu()"" onmouseover=""dropdownmenu(this, event, menu1, '100px','Death Vessel','Mandan Dink','Stay Close')"">Buy</a> Chuck Ragan - Rotterdam - Folkadelphia Session</li>","Scrape just the text, within an html element that has a class, using beautiful soup"
How to make an axes transparent in matplotlib," So I have the following example that plots a figure and an inset: This produces the following figure:What I would like to have thought is the inset to be a little bit transparent (say alpha=0.5). I want something along the lines of what they do in the documentation for the legends in the matplotlib documentation:http://matplotlib.org/users/recipes.html#transparent-fancy-legendsIs that possible? does anyone know how to do that?All the best,P.D. As mentioned in the comments the answer to this questions can be derived from the answer to the linked question. It is just a little bit different conceptually (figure vs axis) and far more straightforward here IMO.  <code>  import matplotlib.pyplot as pltimport numpy as npfig = plt.figure()ax1 = fig.add_subplot(1, 1, 1)x = np.arange(100).reshape((10, 10))ax1.imshow(x)ax2 = fig.add_axes([0.5, 0.5, 0.3, 0.3])t = np.arange(0, 1, 0.01)s = np.sin(t)ax2.plot(t, s, linewidth=2)",How to make axes transparent in matplotlib?
EMR Step failing when submitted using boto3 client," I'm trying to execute spark-submit using boto3 client for EMR.After executing the code below, EMR step submitted and after few seconds failed. The actual command line from step logs is working if executed manually on EMR master.Controller log shows hardly readable garbage, looking like several processes writing there concurrently.UPD: Tried command-runner.jar and EMR versions 4.0.0 and 4.1.0Any idea appreciated.The code fragment: <code>  class ProblemExample: def run(self): session = boto3.Session(profile_name='emr-profile') client = session.client('emr') response = client.add_job_flow_steps( JobFlowId=cluster_id, Steps=[ { 'Name': 'string', 'ActionOnFailure': 'CONTINUE', 'HadoopJarStep': { 'Jar': 's3n://elasticmapreduce/libs/script-runner/script-runner.jar', 'Args': [ '/usr/bin/spark-submit', '--verbose', '--class', 'my.spark.job', '--jars', '<dependencies>', '<my spark job>.jar' ] } }, ] )",spark-submit EMR Step failing when submitted using boto3 client
Pandas Convert Mixed Types to String," Given the following dataframe: I want to convert 'mixed' to an object such that all numbers are integers as strings and all strings remain, of course, strings.The desired output is as follows: Background info: Originally, 'mixed' was part of a data frame taken from a CSV that mainly consisted of numbers, with some strings here and there. When I tried converting it to string, some numbers ended up with '.0' at the end. <code>  DF = pd.DataFrame({'COL1': ['A', 'B', 'C', 'D','D','D'], 'mixed': [2016.0, 2017.0, 'sweatervest', 20, 209, 21]})DF COL1 mixed0 A 2016.01 B 2017.02 C sweatervest3 D 204 D 209 5 D 21 COL1 mixed0 A 20161 B 20172 C sweatervest3 D 204 D 209 5 D 21",Pandas convert mixed types to string
How can I attach a horizontal and vertical scrollbar to a treeview using Tkinter?," I have been trying to attach a horizontal and vertical scrollbar for the tkinter treeview. In my main application all the data comes from a sql database so I need to be able to scroll through lots of data within a sperate window. I have managed to place the treeview within the child window, however I am still stuck on how to attach scrollbars that work.Although there is a vertical scrollbar at the moment it doesn't seem to be attached to the treeview and does not scroll through any data that I input.Is there a way of putting vertical and horizontal scrollbars within my application? <code>  import Tkinter as tkimport osimport sysimport reimport ttkfrom Tkinter import *import tkFontclass application(tk.Tk): def __init__(self, *args, **kwargs): tk.Tk.__init__(self, *args, **kwargs) container = tk.Frame(self) container.pack(side=""top"", fill=""both"", expand = True) container.grid_rowconfigure(0, weight=1) container.grid_columnconfigure(0, weight=1) self.frames = {} for F in (app_one, app_two): frame = F(container, self) self.frames[F] = frame frame.grid(row=0, column=0, sticky=""nsew"") self.show_frame(app_one) def show_frame(self, cont): frame = self.frames[cont] frame.tkraise()class app_one(tk.Frame): def __init__(self, parent, controller): tk.Frame.__init__(self,parent) button = ttk.Button(self, text=""Page One"", command=lambda: controller.show_frame(app_two)) button.pack()class app_two(tk.Frame): def __init__(self, parent, controller): tk.Frame.__init__(self, parent) self.controller=controller self.msgText = tk.StringVar() button = ttk.Button(self, text=""Open"", command= self.Child_Window) button.pack() def Child_Window(self): win2 = Toplevel() new_element_header=['1st','2nd','3rd','4th'] treeScroll = ttk.Scrollbar(win2) treeScroll.pack(side=RIGHT, fill=Y) tree = ttk.Treeview(win2,columns=new_element_header, show=""headings"", yscrollcommand = treeScroll) tree.heading(""1st"", text=""1st"") tree.heading(""2nd"", text=""2nd"") tree.heading(""3rd"", text=""3rd"") tree.heading(""4th"", text=""4th"") tree.insert("""" , 0, text=""Line 1"", values=(""1A"",""1b"")) tree.insert("""" , 0, text=""Line 2"", values=(""1A"",""1b"")) tree.insert("""" , 0, text=""Line 3"", values=(""1A"",""1b"")) tree.insert("""" , 0, text=""Line 4"", values=(""1A"",""1b"")) tree.insert("""" , 0, text=""Line 5"", values=(""1A"",""1b"")) tree.insert("""" , 0, text=""Line 6"", values=(""1A"",""1b"")) tree.insert("""" , 0, text=""Line 7"", values=(""1A"",""1b"")) tree.pack(side=LEFT, fill=BOTH) treeScroll.config(command=tree.yview)app = application()app.wm_geometry(""420x200"")app.wm_title(""Test"")app.mainloop()",How can I attach a vertical scrollbar to a treeview using Tkinter?
Weird behavior of Flask application," I use gunicorn --workers 3 wsgi to run my Flask app. If I change the variable application to myapp, Gunicorn gives the error AppImportError: Failed to find application: 'wsgi'. Why am I getting this error and how do I fix it?myproject.py: wsgi.py: <code>  from flask import Flaskmyapp = Flask(__name__)@myapp.route(""/"")def hello(): return 'Test!'if __name__ == ""__main__"": myapp.run(host='0.0.0.0') from myproject import myappif __name__ == ""__main__"": myapp.run()","Gunicorn can't find app when name changed from ""application"""
Converting some columns of matrix from float to matrix," I have a matrix tempsyntheticGroup2 with 6 columns. I want to change the value of columns (0,1,2,3,5) from float to int. This is my code: but it doesn't work properly and I loose the other columns. <code>  tempsyntheticGroup2=tempsyntheticGroup2[:,[0,1,2,3,5]].astype(int)",Converting some columns of a matrix from float to int
How to Find the diffrence between 3 lista have a duplicate number in python," I have 3 lists and I want to find the difference between the 1st/2nd and 2nd/3rd and print them. Here is my code: but i get only 3 as output.How to make it output 1 and 3? I tried using sets as well, but it only printed 3 again. <code>  n1 = [1,1,1,2,3] n2 = [1,1,1,2] # Here the 3 is not found (""3"" is not present in n1 at all)n3 = [1,1,2] # here 1 is not found (there are fewer ""1""s in n3 than in n2)for x in n1: if x not in n2: print x for m in n2: if m not in n3: print m ",How to find the difference between 3 lists that may have duplicate numbers
Python: Remove a single entry in a list based on the position of the entry.," Is there an easy way to delete an entry in a list? I would like to only remove the first entry. In every forum that I have looked at, the only way that I can delete one entry is with the list.remove() function. This would be perfect, but I can only delete the entry if I know it's name. This doesn't work because you can only remove an entry based on it's name. I would have to run list.remove('hey'). I can't do this in this particular instance.If you require any additional information, ask. <code>  list = ['hey', 'hi', 'hello', 'phil', 'zed', 'alpha'] list.remove(0)",Python: Remove a single entry in a list based on the position of the entry
unequal bin widths python," Here is the histogramTo generate this plot, I did: However, as you noticed, I want to plot the histogram of the relative frequency of each data point with only 3 bins that have different sizes:bin1 = 0.03 -> 0.3bin2 = 0.3 -> 2bin3 = 2 -> 100The histogram looks ugly since the size of the last bin is extremely large relative to the other bins. How can I fix the histogram? I want to change the width of the bins but I do not want to change the range of each bin.  <code>  bins = np.array([0.03, 0.3, 2, 100])plt.hist(m, bins = bins, weights=np.zeros_like(m) + 1. / m.size)",display a histogram with very non-uniform bin widths
Getting Data of a boxplot - Pandas," I need to get the statistical data which were generated to draw a box plot in Pandas(using dataframe to create boxplots). i.e. Quartile1,Quartile2,Quartile3, lower whisker value, upper whisker value and outliers.I tried the following query to draw the boxplot. Is there a way to do it instead of manually calculating the values? <code>  import pandas as pddf = pd.DataFrame(np.random.rand(100, 5), columns=['A', 'B', 'C', 'D', 'E'])pd.DataFrame.boxplot(df,return_type = 'both')",How to get boxplot data for matplotlib boxplots
How to get boxplot data for pandas box plots?," I need to get the statistical data which were generated to draw a box plot in Pandas(using dataframe to create boxplots). i.e. Quartile1,Quartile2,Quartile3, lower whisker value, upper whisker value and outliers.I tried the following query to draw the boxplot. Is there a way to do it instead of manually calculating the values? <code>  import pandas as pddf = pd.DataFrame(np.random.rand(100, 5), columns=['A', 'B', 'C', 'D', 'E'])pd.DataFrame.boxplot(df,return_type = 'both')",How to get boxplot data for matplotlib boxplots
How to get boxplot data for matplotlib boxplots?," I need to get the statistical data which were generated to draw a box plot in Pandas(using dataframe to create boxplots). i.e. Quartile1,Quartile2,Quartile3, lower whisker value, upper whisker value and outliers.I tried the following query to draw the boxplot. Is there a way to do it instead of manually calculating the values? <code>  import pandas as pddf = pd.DataFrame(np.random.rand(100, 5), columns=['A', 'B', 'C', 'D', 'E'])pd.DataFrame.boxplot(df,return_type = 'both')",How to get boxplot data for matplotlib boxplots
Multiprocessing pool with pandas dataframe," So what I am trying to do with the following code is to read a list of lists and put them through function called checker and then have log_result deal with the result of the function checker. I am trying to do this using multithreading because the variable name rows_to_parse in reality has millions of rows, so using multiple cores should speed up this process by a considerable amount. The code at present moment doesn't work and crashes Python.Concerns and Issues I have:Want the existing df which held in the variable df to maintain theindex throughout process because otherwise log_result will getconfused as to which row needs updating.I am quite certain that apply_async is not the appropriatemultiprocessing function to perform this duty because I believe theorder at which the computer reads and writes the df can possibly corrupt it??? I think that a queue may need to be set up to write and read dfbut I am unsure as to how I would go about doing that.Thank you for any assistance. <code>  import pandas as pdimport multiprocessingfrom functools import partialdef checker(a,b,c,d,e): match = df[(df['a'] == a) & (df['b'] == b) & (df['c'] == c) & (df['d'] == d) & (df['e'] == e)] index_of_match = match.index.tolist() if len(index_of_match) == 1: #one match in df return index_of_match elif len(index_of_match) > 1: #not likely because duplicates will be removed prior to: if ""__name__"" == __main__: return [index_of_match[0]] else: #no match, returns a result which then gets processed by the else statement in log_result. this means that [a,b,c,d,e] get written to the df return [a,b,c,d,e]def log_result(result, dataf): if len(result) == 1: # dataf.loc[result[0]]['e'] += 1 else: #append new row to exisiting df new_row = pd.DataFrame([result],columns=cols) dataf = dataf.append(new_row,ignore_index=True)def apply_async_with_callback(parsing_material, dfr): pool = multiprocessing.Pool() for var_a, var_b, var_c, var_d, var_e in parsing_material: pool.apply_async(checker, args = (var_a, var_b, var_c, var_d, var_e), callback = partial(log_result,dataf=dfr)) pool.close() pool.join()if __name__ == '__main__': #setting up main dataframe cols = ['a','b','c','d','e'] existing_data = [[""YES"",""A"",""16052011"",""13031999"",3], [""NO"",""Q"",""11022003"",""15081999"",3], [""YES"",""A"",""22082010"",""03012001"",9]] #main dataframe df = pd.DataFrame(existing_data,columns=cols) #new data rows_to_parse = [['NO', 'A', '09061997', '06122003', 5], ['YES', 'W', '17061992', '26032012', 6], ['YES', 'G', '01122006', '07082014', 2], ['YES', 'N', '06081992', '21052008', 9], ['YES', 'Y', '18051995', '24011996', 6], ['NO', 'Q', '11022003', '15081999', 3], ['NO', 'O', '20112004', '28062008', 0], ['YES', 'R', '10071994', '03091996', 8], ['NO', 'C', '09091998', '22051992', 1], ['YES', 'Q', '01051995', '02012000', 3], ['YES', 'Q', '26022015', '26092007', 5], ['NO', 'F', '15072002', '17062001', 8], ['YES', 'I', '24092006', '03112003', 2], ['YES', 'A', '22082010', '03012001', 9], ['YES', 'I', '15072016', '30092005', 7], ['YES', 'Y', '08111999', '02022006', 3], ['NO', 'V', '04012016', '10061996', 1], ['NO', 'I', '21012003', '11022001', 6], ['NO', 'P', '06041992', '30111993', 6], ['NO', 'W', '30081992', '02012016', 6]] apply_async_with_callback(rows_to_parse, df)",Multiprocessing writing to pandas dataframe
Multiprocessing with pandas dataframe," So what I am trying to do with the following code is to read a list of lists and put them through function called checker and then have log_result deal with the result of the function checker. I am trying to do this using multithreading because the variable name rows_to_parse in reality has millions of rows, so using multiple cores should speed up this process by a considerable amount. The code at present moment doesn't work and crashes Python.Concerns and Issues I have:Want the existing df which held in the variable df to maintain theindex throughout process because otherwise log_result will getconfused as to which row needs updating.I am quite certain that apply_async is not the appropriatemultiprocessing function to perform this duty because I believe theorder at which the computer reads and writes the df can possibly corrupt it??? I think that a queue may need to be set up to write and read dfbut I am unsure as to how I would go about doing that.Thank you for any assistance. <code>  import pandas as pdimport multiprocessingfrom functools import partialdef checker(a,b,c,d,e): match = df[(df['a'] == a) & (df['b'] == b) & (df['c'] == c) & (df['d'] == d) & (df['e'] == e)] index_of_match = match.index.tolist() if len(index_of_match) == 1: #one match in df return index_of_match elif len(index_of_match) > 1: #not likely because duplicates will be removed prior to: if ""__name__"" == __main__: return [index_of_match[0]] else: #no match, returns a result which then gets processed by the else statement in log_result. this means that [a,b,c,d,e] get written to the df return [a,b,c,d,e]def log_result(result, dataf): if len(result) == 1: # dataf.loc[result[0]]['e'] += 1 else: #append new row to exisiting df new_row = pd.DataFrame([result],columns=cols) dataf = dataf.append(new_row,ignore_index=True)def apply_async_with_callback(parsing_material, dfr): pool = multiprocessing.Pool() for var_a, var_b, var_c, var_d, var_e in parsing_material: pool.apply_async(checker, args = (var_a, var_b, var_c, var_d, var_e), callback = partial(log_result,dataf=dfr)) pool.close() pool.join()if __name__ == '__main__': #setting up main dataframe cols = ['a','b','c','d','e'] existing_data = [[""YES"",""A"",""16052011"",""13031999"",3], [""NO"",""Q"",""11022003"",""15081999"",3], [""YES"",""A"",""22082010"",""03012001"",9]] #main dataframe df = pd.DataFrame(existing_data,columns=cols) #new data rows_to_parse = [['NO', 'A', '09061997', '06122003', 5], ['YES', 'W', '17061992', '26032012', 6], ['YES', 'G', '01122006', '07082014', 2], ['YES', 'N', '06081992', '21052008', 9], ['YES', 'Y', '18051995', '24011996', 6], ['NO', 'Q', '11022003', '15081999', 3], ['NO', 'O', '20112004', '28062008', 0], ['YES', 'R', '10071994', '03091996', 8], ['NO', 'C', '09091998', '22051992', 1], ['YES', 'Q', '01051995', '02012000', 3], ['YES', 'Q', '26022015', '26092007', 5], ['NO', 'F', '15072002', '17062001', 8], ['YES', 'I', '24092006', '03112003', 2], ['YES', 'A', '22082010', '03012001', 9], ['YES', 'I', '15072016', '30092005', 7], ['YES', 'Y', '08111999', '02022006', 3], ['NO', 'V', '04012016', '10061996', 1], ['NO', 'I', '21012003', '11022001', 6], ['NO', 'P', '06041992', '30111993', 6], ['NO', 'W', '30081992', '02012016', 6]] apply_async_with_callback(rows_to_parse, df)",Multiprocessing writing to pandas dataframe
error while reading JSON file, I am trying to read JSON file using pandas: I get ValueError: arrays must all be same lengthSome other JSON pages show this error: How do I somehow read the values? I am not particular about data validity.  <code>  import pandas as pddf = pd.read_json('https://data.gov.in/node/305681/datastore/export/json') ValueError: Mixing dicts with non-Series may lead to ambiguous ordering.,ValueError errors while reading JSON file with pd.read_json
Split big file to chunks in python," I have a csv large file that I cannot handle in memory with python. I am splitting it into multiple chunks after grouping by the value of a specific column, using the following logic: The logic is working but it's slow. I am wondering how can I optimize this? For instance with pandas? EditFurther explanation: I am not looking for a simple splitting to same size chunks (like each one having 1000 rows), I want to split by the value of a column, that's why I am using groupby.  <code>  def splitDataFile(self, data_file): self.list_of_chunk_names = [] csv_reader = csv.reader(open(data_file, ""rb""), delimiter=""|"") columns = csv_reader.next() for key,rows in groupby(csv_reader, lambda row: (row[1])): file_name = ""data_chunk""+str(key)+"".csv"" self.list_of_chunk_names.append(file_name) with open(file_name, ""w"") as output: output.write(""|"".join(columns)+""\n"") for row in rows: output.write(""|"".join(row)+""\n"") print ""message: list of chunks "", self.list_of_chunk_names return",Split big csv file by the value of a column in python
Unable to import Tensorflow," El Capitan OS here. I've been trying to find a workaround with import Tensorflow into my ipython notebook, but so far no luck.Like many people in the forums, I've also had issues with install tensorflow because of the six package. I was able to install after some fidgeting with brew I got a message that tensorflow was installed correctly. Even when I did sudo pip install tensorflow I get the message: However, when I'm on my ipython notebook and I did an import tensorflow I get the message: ImportError: No module named tensorflowI've dug further and found this error on the import as well: <code>  brew link gdbmbrew install pythonrew linkapps pythonsudo pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl Requirement already satisfied (use --upgrade to upgrade): tensorflow in /usr/local/lib/python2.7/site-packagesRequirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in /Library/Python/2.7/site-packages (from tensorflow)Requirement already satisfied (use --upgrade to upgrade): numpy>=1.9.2 in /usr/local/lib/python2.7/site-packages (from tensorflow) In [1]: import tensorflow---------------------------------------------------------------------------ImportError Traceback (most recent call last)<ipython-input-1-a649b509054f> in <module>()----> 1 import tensorflow/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py in <module>() 2 # module. 3 # pylint: disable=wildcard-import----> 4 from tensorflow.python import */usr/local/lib/python2.7/site-packages/tensorflow/python/__init__.py in <module>() 11 12 import tensorflow.python.platform---> 13 from tensorflow.core.framework.graph_pb2 import * 14 from tensorflow.core.framework.summary_pb2 import * 15 from tensorflow.core.framework.config_pb2 import */usr/local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py in <module>() 6 from google.protobuf import descriptor as _descriptor 7 from google.protobuf import message as _message----> 8 from google.protobuf import reflection as _reflection 9 from google.protobuf import symbol_database as _symbol_database 10 from google.protobuf import descriptor_pb2/usr/local/lib/python2.7/site-packages/google/protobuf/reflection.py in <module>() 56 from google.protobuf.pyext import cpp_message as message_impl 57 else:---> 58 from google.protobuf.internal import python_message as message_impl 59 60 # The type of all Message classes./usr/local/lib/python2.7/site-packages/google/protobuf/internal/python_message.py in <module>() 57 58 import six---> 59 import six.moves.copyreg as copyreg 60 61 # We use ""as"" to avoid name collisions with variables.ImportError: No module named copyreg","Unable to import Tensorflow ""No module named copyreg"""
Unable to import Tensorflow into ipython notebook," El Capitan OS here. I've been trying to find a workaround with import Tensorflow into my ipython notebook, but so far no luck.Like many people in the forums, I've also had issues with install tensorflow because of the six package. I was able to install after some fidgeting with brew I got a message that tensorflow was installed correctly. Even when I did sudo pip install tensorflow I get the message: However, when I'm on my ipython notebook and I did an import tensorflow I get the message: ImportError: No module named tensorflowI've dug further and found this error on the import as well: <code>  brew link gdbmbrew install pythonrew linkapps pythonsudo pip install https://storage.googleapis.com/tensorflow/mac/tensorflow-0.5.0-py2-none-any.whl Requirement already satisfied (use --upgrade to upgrade): tensorflow in /usr/local/lib/python2.7/site-packagesRequirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in /Library/Python/2.7/site-packages (from tensorflow)Requirement already satisfied (use --upgrade to upgrade): numpy>=1.9.2 in /usr/local/lib/python2.7/site-packages (from tensorflow) In [1]: import tensorflow---------------------------------------------------------------------------ImportError Traceback (most recent call last)<ipython-input-1-a649b509054f> in <module>()----> 1 import tensorflow/usr/local/lib/python2.7/site-packages/tensorflow/__init__.py in <module>() 2 # module. 3 # pylint: disable=wildcard-import----> 4 from tensorflow.python import */usr/local/lib/python2.7/site-packages/tensorflow/python/__init__.py in <module>() 11 12 import tensorflow.python.platform---> 13 from tensorflow.core.framework.graph_pb2 import * 14 from tensorflow.core.framework.summary_pb2 import * 15 from tensorflow.core.framework.config_pb2 import */usr/local/lib/python2.7/site-packages/tensorflow/core/framework/graph_pb2.py in <module>() 6 from google.protobuf import descriptor as _descriptor 7 from google.protobuf import message as _message----> 8 from google.protobuf import reflection as _reflection 9 from google.protobuf import symbol_database as _symbol_database 10 from google.protobuf import descriptor_pb2/usr/local/lib/python2.7/site-packages/google/protobuf/reflection.py in <module>() 56 from google.protobuf.pyext import cpp_message as message_impl 57 else:---> 58 from google.protobuf.internal import python_message as message_impl 59 60 # The type of all Message classes./usr/local/lib/python2.7/site-packages/google/protobuf/internal/python_message.py in <module>() 57 58 import six---> 59 import six.moves.copyreg as copyreg 60 61 # We use ""as"" to avoid name collisions with variables.ImportError: No module named copyreg","Unable to import Tensorflow ""No module named copyreg"""
where is the ./configure of TensorFlow ?," When installing TensorFlow on my Ubuntu, I would like to use GPU with CUDA. But I am stopped at this step in the Official Tutorial :Where exactly is this ./configure ? Or where is my root of source tree.My TensorFlow is located here /usr/local/lib/python2.7/dist-packages/tensorflow. But I still did not find ./configure. EDITI have found the ./configure according to Salvador Dali's answer. But when doing the example code, I got the following error: The cuda device cannot be found. AnswerSee the answer about how did I enable GPU support here. <code>  >>> import tensorflow as tf>>> hello = tf.constant('Hello, TensorFlow!')>>> sess = tf.Session()I tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 8E tensorflow/stream_executor/cuda/cuda_driver.cc:466] failed call to cuInit: CUDA_ERROR_NO_DEVICEI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:86] kernel driver does not appear to be running on this host (cliu-ubuntu): /proc/driver/nvidia/version does not existI tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: I tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 8",where is the ./configure of TensorFlow and how to enable the GPU support?
where is the ./configure of TensorFlow?," When installing TensorFlow on my Ubuntu, I would like to use GPU with CUDA. But I am stopped at this step in the Official Tutorial :Where exactly is this ./configure ? Or where is my root of source tree.My TensorFlow is located here /usr/local/lib/python2.7/dist-packages/tensorflow. But I still did not find ./configure. EDITI have found the ./configure according to Salvador Dali's answer. But when doing the example code, I got the following error: The cuda device cannot be found. AnswerSee the answer about how did I enable GPU support here. <code>  >>> import tensorflow as tf>>> hello = tf.constant('Hello, TensorFlow!')>>> sess = tf.Session()I tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 8E tensorflow/stream_executor/cuda/cuda_driver.cc:466] failed call to cuInit: CUDA_ERROR_NO_DEVICEI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:86] kernel driver does not appear to be running on this host (cliu-ubuntu): /proc/driver/nvidia/version does not existI tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: I tensorflow/core/common_runtime/local_session.cc:45] Local session inter op parallelism threads: 8",where is the ./configure of TensorFlow and how to enable the GPU support?
how to download surf and sift detector in opencv 2.4 or 2.7?," I was trying a code for feature matching which uses the function SURF(). Upon execution it gives an error saying ""AttributeError: 'module' object has no attribute 'SURF'"".How can I download this module for Python (Windows) and fix this error? <code> ",How to use surf and sift detector in OpenCV for Python
How to download surf and sift detector in OpenCV 2.4 Or 2.7?," I was trying a code for feature matching which uses the function SURF(). Upon execution it gives an error saying ""AttributeError: 'module' object has no attribute 'SURF'"".How can I download this module for Python (Windows) and fix this error? <code> ",How to use surf and sift detector in OpenCV for Python
Flip numpy array," I have a lower triangular array, like B: I want to flip it to look like: That is, I want to take all the positive values, and reverse within the positive values, leaving the trailing zeros in place. This is not what fliplr does: Any tips? Also, the actual array I am working with would be something like B.shape = (200,20,4,4) instead of (4,4). Each (4,4) block looks like the above example (with different numbers across the 200, 20 different entries). <code>  B = np.array([[1,0,0,0],[.25,.75,0,0], [.1,.2,.7,0],[.2,.3,.4,.1]])>>> Barray([[ 1. , 0. , 0. , 0. ], [ 0.25, 0.75, 0. , 0. ], [ 0.1 , 0.2 , 0.7 , 0. ], [ 0.2 , 0.3 , 0.4 , 0.1 ]]) array([[ 1. , 0. , 0. , 0. ], [ 0.75, 0.25, 0. , 0. ], [ 0.7 , 0.2 , 0.1 , 0. ], [ 0.1 , 0.4 , 0.3 , 0.2 ]]) >>> np.fliplr(B)array([[ 0. , 0. , 0. , 1. ], [ 0. , 0. , 0.75, 0.25], [ 0. , 0.7 , 0.2 , 0.1 ], [ 0.1 , 0.4 , 0.3 , 0.2 ]])",Flip non-zero values along each row of a lower triangular numpy array
How do I allow a user to view other users profiles using django?," I am new to django and I am currently trying to build a website that allows users to sign in and view other users profiles. So far I have managed to let users sign in but I can't work out how to view other peoples profiles.Each profile uses the users username to create a url for their profile. Currently if I sign in as one user and change the URL to another users profile URL, it still displays the current users profile. I want something similar to pinterest where any person whether they are signed in or not can view peoples profiles.Any help would be appreciated!View project url app url template <code>  from django.http import HttpResponsefrom django.shortcuts import renderfrom howdidu.forms import UserProfileFormfrom howdidu.models import UserProfilefrom django.contrib.auth.decorators import login_requiredfrom django.shortcuts import get_object_or_404from django.contrib.auth.models import Userdef index(request): context_dict = {'boldmessage': ""I am bold font from the context""} return render(request, 'howdidu/index.html', context_dict)#user profile form@login_requireddef register_profile(request): profile = UserProfile.objects.get(user=request.user) if request.method == 'POST': form = UserProfileForm(request.POST, request.FILES, instance=profile) if form.is_valid(): form.save() return index(request) else: print form.errors else: form = UserProfileForm() return render(request, 'howdidu/register_profile.html', {'form': form})#profile page using user name as url@login_requireddef profile_page(request, username): user = get_object_or_404(User, username=username) return render(request, 'howdidu/profile.html', {'profile_user': user}) from django.conf.urls import patterns, include, urlfrom django.contrib import adminfrom django.conf import settingsfrom registration.backends.simple.views import RegistrationViewclass MyRegistrationView(RegistrationView): #redirects to home page after registration def get_success_url(self,request, user): return '/register_profile'urlpatterns = patterns('', # Examples: # url(r'^$', 'howdidu_project.views.home', name='home'), # url(r'^blog/', include('blog.urls')), url(r'^admin/', include(admin.site.urls)), url(r'', include('howdidu.urls')), url(r'^accounts/register/$', MyRegistrationView.as_view(), name='registration_register'), #redirects to home page after registration (r'^accounts/', include('registration.backends.simple.urls')), url(r'^(?P<username>\w+)/', include('howdidu.urls')), #do i need this?)# mediaif settings.DEBUG: urlpatterns += patterns( 'django.views.static', (r'^media/(?P<path>.*)', 'serve', {'document_root': settings.MEDIA_ROOT}), ) from django.conf.urls import patterns, urlfrom howdidu import viewsurlpatterns = patterns('', url(r'^$', views.index, name='index'), url(r'^register_profile/$', views.register_profile, name='register_profile'), url(r'^(?P<username>\w+)/$', views.profile_page, name='user_profile'), ) {% extends 'howdidu/base.html' %}{% load staticfiles %}{% block title %}{{ user.username }}{% endblock %}{% block body_block %} {% if user.is_authenticated %} <h1>{{ user.username }} welcome to your profile page</h1> <img src=""{{ user.userprofile.profile_picture.url }}"" width = ""150"" height = ""150"" /> <h2>{{ user.userprofile.first_name }}</h2> <h2>{{ user.userprofile.second_name }}</h2> <h2>{{ user.userprofile.user_country }}</h2> {% endif %}{% endblock %}",How can I display a user profile using Django?
spark dataframe transform multiple rows to column," I am a novice to spark, and I want to transform below source dataframe (load from JSON file): Into below result dataframe: Here is the Transformation Rule:The result dataframe is consisted with A + (n major columns) where the major columns names are specified by: The result dataframe contains m rows where the values for A column are provided by: The value for each major column in result dataframe is the value from source dataframe on the corresponding A and major(e.g. the count in Row 1 in source dataframe is mapped to the box where A is a and column m1)The combinations of A and major in source dataframe has no duplication (please consider it a primary key on the two columns in SQL) <code>  +--+-----+-----+|A |count|major|+--+-----+-----+| a| 1| m1|| a| 1| m2|| a| 2| m3|| a| 3| m4|| b| 4| m1|| b| 1| m2|| b| 2| m3|| c| 3| m1|| c| 4| m3|| c| 5| m4|| d| 6| m1|| d| 1| m2|| d| 2| m3|| d| 3| m4|| d| 4| m5|| e| 4| m1|| e| 5| m2|| e| 1| m3|| e| 1| m4|| e| 1| m5|+--+-----+-----+ +--+--+--+--+--+--+|A |m1|m2|m3|m4|m5|+--+--+--+--+--+--+| a| 1| 1| 2| 3| 0|| b| 4| 2| 1| 0| 0|| c| 3| 0| 4| 5| 0|| d| 6| 1| 2| 3| 4|| e| 4| 5| 1| 1| 1|+--+--+--+--+--+--+ sorted(src_df.map(lambda x: x[2]).distinct().collect()) sorted(src_df.map(lambda x: x[0]).distinct().collect())",Spark dataframe transform multiple rows to column
Tensorflow: How to restore a previously saved model (python), After you train a model in Tensorflow: How do you save the trained model?How do you later restore this saved model? <code> ,How to save/restore a model after training?
Tensorflow: How to save/restore a model? (python), After you train a model in Tensorflow: How do you save the trained model?How do you later restore this saved model? <code> ,How to save/restore a model after training?
Tensorflow: how to save/restore a model?, After you train a model in Tensorflow: How do you save the trained model?How do you later restore this saved model? <code> ,How to save/restore a model after training?
How exactly does someone declare a model in a flask application?," I am attempting to declare a User model in my flask application in order to implement login with the Flask-Login extension. From the flask documentation regarding sql alchemy there is this example which I have used for another model called employees. Here is the code: This is taken from this page as an exmample Flask SQL-Alchemy DocsI am confused because I am also using alembic to run migrations so by using alembic revision -m ""create user table I have already created a 'users' table. I created my first model's table (employees) by using the recommendation of the Flask Sql-Alchemy guide as such: Here is the confusion. I now need to establish a User model for authentication. How do I do this? Here is my code so far: Notice that I have the User class inherit from flask_login.Usermixin. I need this in order for Flask Login to work so now its not instantiated like the Employee model from db.Model. But the thing is is that I created an alembic migration to establish my User model. Why am I not able to query the database. What piece of the puzzle am I missing? <code>  class Employee(db.Model): __tablename__ = ""employees"" id = db.Column(db.Integer, primary_key=True) name = db.Column(db.String(200)) title = db.Column(db.String(200)) email = db.Column(db.String(200)) department = db.Column(db.String(200)) def __init__(self, name, title, email, department): self.name = name self.title = title self.email = email self.department = department def __repr__(self): return '<Employee %r>' % self.name from yourapplication.database import init_dbinit_db() class User(flask_login.UserMixin): def __init__(self, username, password): self.id = username self.password = password",How do I define and create a User model for use with Flask-Login?
Write Large Pandas DataFrames to MSSQL Database," I have 74 relatively large Pandas DataFrames (About 34,600 rows and 8 columns) that I am trying to insert into a SQL Server database as quickly as possible. After doing some research, I learned that the good ole pandas.to_sql function is not good for such large inserts into a SQL Server database, which was the initial approach that I took (very slow - almost an hour for the application to complete vs about 4 minutes when using mysql database.)This article, and many other StackOverflow posts have been helpful in pointing me in the right direction, however I've hit a roadblock:I am trying to use SQLAlchemy's Core rather than the ORM for reasons explained in the link above. So, I am converting the dataframe to a dictionary, using pandas.to_dict and then doing an execute() and insert(): The problem is that insert is not getting any values -- they appear as a bunch of empty parenthesis and I get this error: There are values in the list of dictionaries that I passed in, so I can't figure out why the values aren't showing up.EDIT:Here's the example I'm going off of: <code>  self._session_factory.engine.execute( TimeSeriesResultValues.__table__.insert(), data)# 'data' is a list of dictionaries. (pyodbc.IntegretyError) ('23000', ""[23000] [FreeTDS][SQL Server]Cannotinsert the value NULL into the column... def test_sqlalchemy_core(n=100000): init_sqlalchemy() t0 = time.time() engine.execute( Customer.__table__.insert(), [{""name"": 'NAME ' + str(i)} for i in range(n)] ) print(""SQLAlchemy Core: Total time for "" + str(n) + "" records "" + str(time.time() - t0) + "" secs"")",Write Large Pandas DataFrames to SQL Server database
python tarfile recurcive extract in memory," I have a tar file with that contains compressed tar files. Like this: tarfile expects a string as the file to open. Is there anyway to pass it a file object? Thanks. <code>  gnomeAware@devserv:~$ tar tf test.tarFile1.tar.gzFile2.tar.gzFile3.tar.gzFile4.tar.gz tar = tarfile.open('test.tar', 'r') # Unpack tarfor item in tar: Bundle=tar.extractfile(item) # Pull out the file t = tarfile.open(Bundle, ""r:gz"") # Unpack tar for tItem in t: ...",python tarfile recursive extract in memory
Compute linear regression beta coefficient with Python," I would like to compute the beta or standardized coefficient of a linear regression model using standard tools in Python (numpy, pandas, scipy.stats, etc.).A friend of mine told me that this is done in R with the following command: Currently, I am computing it in Python like this: Is there a more straightforward function to compute this figure in Python? <code>  lm(scale(y) ~ scale(x)) from scipy.stats import linregressfrom scipy.stats.mstats import zscore(beta_coeff, intercept, rvalue, pvalue, stderr) = linregress(zscore(x), zscore(y))print('The Beta Coeff is: %f' % beta_coeff)",Compute linear regression standardized coefficient (beta) with Python
"python / QT / Qlistwidget, Getting selected indices from extendedselection in Qlistwidget"," I have a Qlistwidget in which I can select multiple items. I can get a list with all the selected items in the listwidget but can not find a way to get a list of the corresponding rows. To get a list of the selected items in the listwidget I used the following code: To retrieve the rows I am looking for something like: But this does not work. I have also tried some code which resulted in outputs like this, which is not very useful: <code>  print [str(x.text()) for x in self.listWidget.selectedItems()] a = self.listWidget.selectedIndexes()print a <PyQt4.QtGui.QListWidgetItem object at 0x0000000013048B88><PyQt4.QtCore.QModelIndex object at 0x0000000014FBA7B8>",Getting selected rows in QListWidget
"python / QT / Qlistwidget, Getting selected rows from extendedselection in Qlistwidget"," I have a Qlistwidget in which I can select multiple items. I can get a list with all the selected items in the listwidget but can not find a way to get a list of the corresponding rows. To get a list of the selected items in the listwidget I used the following code: To retrieve the rows I am looking for something like: But this does not work. I have also tried some code which resulted in outputs like this, which is not very useful: <code>  print [str(x.text()) for x in self.listWidget.selectedItems()] a = self.listWidget.selectedIndexes()print a <PyQt4.QtGui.QListWidgetItem object at 0x0000000013048B88><PyQt4.QtCore.QModelIndex object at 0x0000000014FBA7B8>",Getting selected rows in QListWidget
To merge two dictionaries of list in python," There are two dictionaries I want another dictionary z which is a merged one of x and y such that Is it possible to do this operation? I tried update operation But it gives me the following result <code>  x={1:['a','b','c']}y={1:['d','e','f'],2:['g']} z = {1:['a','b','c','d','e','f'],2:['g']} x.update(y) z= {1:['d','e','f'],2:['g']}",To merge two dictionaries of list in Python
Streaming server issue with gunicorn and flask," I am using gunicorn and flask for a web service. I am trying to get my head around running a streaming route (not sure if that is the correct terminology).my route looks like this: I expect that the server would yield the output each time that delay_inner does a yield. But, what I am getting is all the json responses at once, and only when the delay_inner finishes execution.What am I missing here?--EDIT--I have fixed the issue for Flask and Gunicorn, I am able to run it as expected by using the flask server, and by going to the Gunicorn port. It streams the data as expected. However, and I should have mentioned this in the original post, I am also running behind nginx. And that is not set up correctly to stream. Can anyone help with that? <code>  @app.route('/delay')def delay(): from time import sleep def delay_inner(): for i in range(10): sleep(5) yield json.dumps({'delay': i}) return Response(delay_inner(), mimetype=""text/event-stream"")",Streaming server issue with gunicorn and flask and Nginx
How to remove a dimension in a numpy matrix?," Imagine we have a 5x4 matrix. We need to remove only the first dimension. How can we do it with numpy? I tried: It looks a bit clunky. Am I doing it correctly?If yes, is there a cleaner way to remove the dimension without reshaping?  <code>  array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [ 12., 13., 14., 15.], [ 16., 17., 18., 19.]], dtype=float32) arr = np.arange(20, dtype=np.float32)matrix = arr.reshape(5, 4)new_arr = numpy.delete(matrix, matrix[:,0])trimmed_matrix = new_arr.reshape(5, 3)",How to remove a column in a numpy array?
How to remove a column in a numpy matrix?," Imagine we have a 5x4 matrix. We need to remove only the first dimension. How can we do it with numpy? I tried: It looks a bit clunky. Am I doing it correctly?If yes, is there a cleaner way to remove the dimension without reshaping?  <code>  array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [ 12., 13., 14., 15.], [ 16., 17., 18., 19.]], dtype=float32) arr = np.arange(20, dtype=np.float32)matrix = arr.reshape(5, 4)new_arr = numpy.delete(matrix, matrix[:,0])trimmed_matrix = new_arr.reshape(5, 3)",How to remove a column in a numpy array?
filter strings by regex in a list," I'd like to filter a list of strings in python by using regex. In the following case, keeping only the files with a '.npy' extension.The code that doesn't work: The same regex works for me in Ruby: What's wrong with the Python code? <code>  import refiles = [ '/a/b/c/la_seg_x005_y003.png', '/a/b/c/la_seg_x005_y003.npy', '/a/b/c/la_seg_x004_y003.png', '/a/b/c/la_seg_x004_y003.npy', '/a/b/c/la_seg_x003_y003.png', '/a/b/c/la_seg_x003_y003.npy', ]regex = re.compile(r'_x\d+_y\d+\.npy')selected_files = filter(regex.match, files)print(selected_files) selected = files.select { |f| f =~ /_x\d+_y\d+\.npy/ }",Failure when filtering string list with re.match
Python: Best way to reference class name in class attribute," Unfortunately, in Python, a class attribute cannot reference its class name. The following raises a NameError: In a situation where a class attribute must reference its class name (in my case, it's part of a validation scheme for a library). What is the clearest way to do so?The following is my current attempt: This works, but even with comments is likely to cause confusion, since you expect a class attribute to be initialized when declared, not after the class is declared.Does anyone have a better workaround? <code>  class Foo(object): bar = Foo class Foo(object): bar = NoneFoo.bar = Foo",What is the best way for a class to reference itself in a class attribute?
What is the best way to reference class name in class attribute?," Unfortunately, in Python, a class attribute cannot reference its class name. The following raises a NameError: In a situation where a class attribute must reference its class name (in my case, it's part of a validation scheme for a library). What is the clearest way to do so?The following is my current attempt: This works, but even with comments is likely to cause confusion, since you expect a class attribute to be initialized when declared, not after the class is declared.Does anyone have a better workaround? <code>  class Foo(object): bar = Foo class Foo(object): bar = NoneFoo.bar = Foo",What is the best way for a class to reference itself in a class attribute?
using IFF in python," Is there a way to write an iff statement (i.e., if and only if) in Python?I want to use it like in However, there isn't any iff statement in Python. Wikipedia defines the truth table for iff as this: How do I accomplish this in Python? <code>  for i in range(x) iff x%2==0 and x%i==0: a | b | iff a and b-----------------------T | T | TT | F | FF | T | FF | F | T",Using IFF in Python
Normal equation and Numpy least-squares methods difference in regression?," I am doing linear regression with multiple variables/features. I try to get thetas (coefficients) by using normal equation method (that uses matrix inverse), Numpy least-squares numpy.linalg.lstsq tool and np.linalg.solve tool. In my data I have n = 143 features and m = 13000 training examples.For normal equation method with regularization I use this formula: Sources: Regularization (Andrew Ng, Stanford) Normal equations (Andrew Ng, Stanford) Regularization is used to solve the potential problem of matrix non-invertibility (XtX matrix may become singular/non-invertible) Data preparation code: For least squares method I use Numpy's numpy.linalg.lstsq. Here is Python code: Also I used np.linalg.solve tool of numpy: For normal equation I use: In all methods I used regularization. Here is results (theta coefficients) to see difference between these three approaches: As you can see normal equation, least squares and np.linalg.solve tool methods give to some extent different results. The question is why these three approaches gives noticeably different results and which method gives more efficient and more accurate result? Assumption:Results of Normal equation method and results of np.linalg.solve are very close to each other. And results of np.linalg.lstsq differ from both of them. Since normal equation uses inverse we do not expect very accurate results of it and therefore results of np.linalg.solve tool also. Seem to be that better results are given by np.linalg.lstsq. Upd:As Dave Hensley mentioned:After the line np.fill_diagonal(IdentityMatrix, 1) this code IdentityMatrix[0,0] = 0 should be added.DB2.csv is available on DropBox: DB2.csv Full Python code is available on DropBox: Full code <code>  import pandas as pdimport numpy as nppath = 'DB2.csv' data = pd.read_csv(path, header=None, delimiter="";"")data.insert(0, 'Ones', 1)cols = data.shape[1]X = data.iloc[:,0:cols-1] y = data.iloc[:,cols-1:cols] IdentitySize = X.shape[1]IdentityMatrix= np.zeros((IdentitySize, IdentitySize))np.fill_diagonal(IdentityMatrix, 1) lamb = 1th = np.linalg.lstsq(X.T.dot(X) + lamb * IdentityMatrix, X.T.dot(y))[0] lamb = 1XtX_lamb = X.T.dot(X) + lamb * IdentityMatrixXtY = X.T.dot(y)x = np.linalg.solve(XtX_lamb, XtY); lamb = 1xTx = X.T.dot(X) + lamb * IdentityMatrixXtX = np.linalg.inv(xTx)XtX_xT = XtX.dot(X.T)theta = XtX_xT.dot(y) Normal equation: np.linalg.lstsq np.linalg.solve[-27551.99918303] [-27551.95276154] [-27551.9991855][-940.27518383] [-940.27520138] [-940.27518383][-9332.54653964] [-9332.55448263] [-9332.54654461][-3149.02902071] [-3149.03496582] [-3149.02900965][-1863.25125909] [-1863.2631435] [-1863.25126344][-2779.91105618] [-2779.92175308] [-2779.91105347][-1226.60014026] [-1226.61033117] [-1226.60014192][-920.73334259] [-920.74331432] [-920.73334194][-6278.44238081] [-6278.45496955] [-6278.44237847][-2001.48544938] [-2001.49566981] [-2001.48545349][-715.79204971] [-715.79664124] [-715.79204921][ 4039.38847472] [ 4039.38302499] [ 4039.38847515][-2362.54853195] [-2362.55280478] [-2362.54853139][-12730.8039209] [-12730.80866036] [-12730.80392076][-24872.79868125] [-24872.80203459] [-24872.79867954][-3402.50791863] [-3402.5140501] [-3402.50793382][ 253.47894001] [ 253.47177732] [ 253.47892472][-5998.2045186] [-5998.20513905] [-5998.2045184][ 198.40560401] [ 198.4049081] [ 198.4056042][ 4368.97581411] [ 4368.97175688] [ 4368.97581426][-2885.68026222] [-2885.68154407] [-2885.68026205][ 1218.76602731] [ 1218.76562838] [ 1218.7660275][-1423.73583813] [-1423.7369068] [-1423.73583793][ 173.19125007] [ 173.19086525] [ 173.19125024][-3560.81709538] [-3560.81650156] [-3560.8170952][-142.68135768] [-142.68162508] [-142.6813575][-2010.89489111] [-2010.89601322] [-2010.89489092][-4463.64701238] [-4463.64742877] [-4463.64701219][ 17074.62997704] [ 17074.62974609] [ 17074.62997723][ 7917.75662561] [ 7917.75682048] [ 7917.75662578][-4234.16758492] [-4234.16847544] [-4234.16758474][-5500.10566329] [-5500.106558] [-5500.10566309][-5997.79002683] [-5997.7904842] [-5997.79002634][ 1376.42726683] [ 1376.42629704] [ 1376.42726705][ 6056.87496151] [ 6056.87452659] [ 6056.87496175][ 8149.0123667] [ 8149.01209157] [ 8149.01236827][-7273.3450484] [-7273.34480382] [-7273.34504827][-2010.61773247] [-2010.61839251] [-2010.61773225][-7917.81185096] [-7917.81223606] [-7917.81185084][ 8247.92773739] [ 8247.92774315] [ 8247.92773722][ 1267.25067823] [ 1267.24677734] [ 1267.25067832][ 2557.6208133] [ 2557.62126916] [ 2557.62081337][-5678.53744654] [-5678.53820798] [-5678.53744647][ 3406.41697822] [ 3406.42040997] [ 3406.41697836][-8371.23657044] [-8371.2361594] [-8371.23657035][ 15010.61728285] [ 15010.61598236] [ 15010.61728304][ 11006.21920273] [ 11006.21711213] [ 11006.21920284][-5930.93274062] [-5930.93237071] [-5930.93274048][-5232.84459862] [-5232.84557665] [-5232.84459848][ 3196.89304277] [ 3196.89414431] [ 3196.8930428][ 15298.53309912] [ 15298.53496877] [ 15298.53309919][ 4742.68631183] [ 4742.6862601] [ 4742.68631172][ 4423.14798495] [ 4423.14765013] [ 4423.14798546][-16153.50854089] [-16153.51038489] [-16153.50854123][-22071.50792741] [-22071.49808389] [-22071.50792408][-688.22903323] [-688.2310229] [-688.22904006][-1060.88119863] [-1060.8829114] [-1060.88120546][-101.75750066] [-101.75776411] [-101.75750831][ 4106.77311898] [ 4106.77128502] [ 4106.77311218][ 3482.99764601] [ 3482.99518758] [ 3482.99763924][-1100.42290509] [-1100.42166312] [-1100.4229119][ 20892.42685103] [ 20892.42487476] [ 20892.42684422][-5007.54075789] [-5007.54265501] [-5007.54076473][ 11111.83929421] [ 11111.83734144] [ 11111.83928704][ 9488.57342568] [ 9488.57158677] [ 9488.57341883][-2992.3070786] [-2992.29295891] [-2992.30708529][ 17810.57005982] [ 17810.56651223] [ 17810.57005457][-2154.47389712] [-2154.47504319] [-2154.47390285][-5324.34206726] [-5324.33913623] [-5324.34207293][-14981.89224345] [-14981.8965674] [-14981.89224973][-29440.90545197] [-29440.90465897] [-29440.90545704][-6925.31991443] [-6925.32123144] [-6925.31992383][ 104.98071593] [ 104.97886085] [ 104.98071152][-5184.94477582] [-5184.9447972] [-5184.94477792][ 1555.54536625] [ 1555.54254362] [ 1555.5453638][-402.62443474] [-402.62539068] [-402.62443718][ 17746.15769322] [ 17746.15458093] [ 17746.15769074][-5512.94925026] [-5512.94980649] [-5512.94925267][-2202.8589276] [-2202.86226244] [-2202.85893056][-5549.05250407] [-5549.05416936] [-5549.05250669][-1675.87329493] [-1675.87995809] [-1675.87329255][-5274.27756529] [-5274.28093377] [-5274.2775701][-5424.10246845] [-5424.10658526] [-5424.10247326][-1014.70864363] [-1014.71145066] [-1014.70864845][ 12936.59360437] [ 12936.59168749] [ 12936.59359954][ 2912.71566077] [ 2912.71282628] [ 2912.71565599][ 6489.36648506] [ 6489.36538259] [ 6489.36648021][ 12025.06991281] [ 12025.07040848] [ 12025.06990358][ 17026.57841531] [ 17026.56827742] [ 17026.57841044][ 2220.1852193] [ 2220.18531961] [ 2220.18521579][-2886.39219026] [-2886.39015388] [-2886.39219394][-18393.24573629] [-18393.25888463] [-18393.24573872][-17591.33051471] [-17591.32838012] [-17591.33051834][-3947.18545848] [-3947.17487999] [-3947.18546459][ 7707.05472816] [ 7707.05577227] [ 7707.0547217][ 4280.72039079] [ 4280.72338194] [ 4280.72038435][-3137.48835901] [-3137.48480197] [-3137.48836531][ 6693.47303443] [ 6693.46528167] [ 6693.47302811][-13936.14265517] [-13936.14329336] [-13936.14267094][ 2684.29594641] [ 2684.29859601] [ 2684.29594183][-2193.61036078] [-2193.63086307] [-2193.610366][-10139.10424848] [-10139.11905454] [-10139.10426049][ 4475.11569903] [ 4475.12288711] [ 4475.11569421][-3037.71857269] [-3037.72118246] [-3037.71857265][-5538.71349798] [-5538.71654224] [-5538.71349794][ 8008.38521357] [ 8008.39092739] [ 8008.38521361][-1433.43859633] [-1433.44181824] [-1433.43859629][ 4212.47144667] [ 4212.47368097] [ 4212.47144686][ 19688.24263706] [ 19688.2451694] [ 19688.2426368][ 104.13434091] [ 104.13434349] [ 104.13434091][-654.02451175] [-654.02493111] [-654.02451174][-2522.8642551] [-2522.88694451] [-2522.86424254][-5011.20385919] [-5011.22742915] [-5011.20384655][-13285.64644021] [-13285.66951459] [-13285.64642763][-4254.86406891] [-4254.88695873] [-4254.86405637][-2477.42063206] [-2477.43501057] [-2477.42061727][ 0.] [ 1.23691279e-10] [ 0.][-92.79470071] [-92.79467095] [-92.79470071][ 2383.66211583] [ 2383.66209637] [ 2383.66211583][-10725.22892185] [-10725.22889937] [-10725.22892185][ 234.77560283] [ 234.77560254] [ 234.77560283][ 4739.22119578] [ 4739.22121432] [ 4739.22119578][ 43640.05854156] [ 43640.05848841] [ 43640.05854157][ 2592.3866707] [ 2592.38671547] [ 2592.3866707][-25130.02819215] [-25130.05501178] [-25130.02819515][ 4966.82173096] [ 4966.7946407] [ 4966.82172795][ 14232.97930665] [ 14232.9529959] [ 14232.97930363][-21621.77202422] [-21621.79840459] [-21621.7720272][ 9917.80960029] [ 9917.80960571] [ 9917.80960029][ 1355.79191536] [ 1355.79198092] [ 1355.79191536][-27218.44185748] [-27218.46880642] [-27218.44185719][-27218.04184348] [-27218.06875423] [-27218.04184318][ 23482.80743869] [ 23482.78043029] [ 23482.80743898][ 3401.67707434] [ 3401.65134677] [ 3401.67707463][ 3030.36383274] [ 3030.36384909] [ 3030.36383274][-30590.61847724] [-30590.63933424] [-30590.61847706][-28818.3942685] [-28818.41520495] [-28818.39426833][-25115.73726772] [-25115.7580278] [-25115.73726753][ 77174.61695995] [ 77174.59548773] [ 77174.61696016][-20201.86613672] [-20201.88871113] [-20201.86613657][ 51908.53292209] [ 51908.53446495] [ 51908.53292207][ 7710.71327865] [ 7710.71324194] [ 7710.71327865][-16206.9785119] [-16206.97851993] [-16206.9785119]","Normal equation and Numpy 'least-squares', 'solve' methods difference in regression?"
python args not working unless it has a position reference? General learning," And for reference sake - args is: for this query to work i need to reference args as because if I use I get the error: int() argument must be a string, a bytes-like object or a number, not 'tuple'I understand that it thinks it's a tuple even though it's only one value but when I do the following in terminal I get class back.Why do I have to reference position here when it looks to be just one value returning? <code>  def test_stats(team, *args): if not args: [do some stuff] else: team_fixtures = (Fixtures.objects.filter(home_team=team_details.id) | Fixtures.objects.filter(away_team=team_details.id))/.filter(fixture_datetime__lt=datetime.now()).filter(fixture_datetime__year=args[0]) date_year = datetime.now().year .filter(fixture_datetime__year=args[0]) .filter(fixture_datetime__year=args) type(date_year)",python args not working unless it has a position reference
How to setup Pycharm for multiple projects," I want to set up PyCharm to work on a set of Python projects in a single window.Lets say I have this projects structure. A system PYTHONPATH of /Library/Python/2.7/site-packages:/usr/local/lib/python2.7/site-packages:/opt/proprietary/packagesand some packages at How do I add the ~/src/py_project* projects to a single PyCharm window and have them see all of the packages in PYTHONPATH and /opt/proprietary/packages/project*?Moving directories is not optional, and I don't want virtualenv. I want to configure PyCharm once for all of the open projects. For the record the PYTHONPATH works just fine for this setup everywhere but PyCharm. <code>  ~/src py_project1 py_project2 py_project3 other_lang_proj1 other_lang_proj2 /opt/proprietary/packages project1 project2 project3",How to setup PyCharm for multiple projects
efficient way expanding an array in python," My question is how to efficiently expand an array, by copying itself many times. I am trying to expand my survey samples to the full size dataset, by copying every sample N times. N is the influence factor that signed to the sample. So I wrote two loops to do this task (script pasted below). It works, but is slow. My sample size is 20,000, and try to expand it into 3 million full size.. is there any function I can try? Thank you for your help! ----My script---- <code>  lines = np.asarray(person.read().split('\n'))df_array = np.asarray(lines[0].split(' '))for j in range(1,len(lines)-1): subarray = np.asarray(lines[j].split(' ')) factor = int(round(float(subarray[-1]),0)) for i in range(1,factor): df_array = np.vstack((df_array, subarray))print len(df_array)",How to efficiently expanding arrays in python?
"npm - ""Can't find Python executable ""python"", you can set the PYT HON env variable."""," I'm trying to run the following command: npm install -g bower gulp cordova ionic tsd@next karma-cli protractor node-gyp coffee-script js-beautify typescript npm-checkI have installed Python, Visual Studio Express and node-gyp so thought I'd be good to go, however I get the following errors:Regarding the ""Can't find Python executable ""python"", you can set the PYTHON env variable."" error, I'm a little confused because I have set the PYTHON environmental variable like so:Any ideas please? <code> ","npm - ""Can't find Python executable ""python"", you can set the PYTHON env variable."""
pandas IndexError/TypeError inconsistency," I have several series of lists of variable length with some nulls. One example is: but another contains all NaNs: I need the last item in each list, which is straightforward: But whilst getting to this I discovered that, without the isinstance, when the indexing chokes on the NaNs it does so differently on s0 and s1: Can anyone explain why? Is this a bug? I'm using Pandas 0.16.2 and Python 3.4.3. <code>  In [108]: s0 = pd.Series([['a', 'b'],['c'],np.nan])In [109]: s0Out[109]: 0 [a, b]1 [c]2 NaNdtype: object In [110]: s1 = pd.Series([np.nan,np.nan])In [111]: s1Out[111]: 0 NaN1 NaNdtype: float64 In [112]: s0.map(lambda x: x[-1] if isinstance(x,list) else x)Out[112]: 0 b1 c2 NaNdtype: object In [113]: s0.map(lambda x: x[-1])...TypeError: 'float' object is not subscriptableIn [114]: s1.map(lamda x: x[-1])...IndexError: invalid index to scalar variable.",pandas IndexError/TypeError inconsistency with NaN values
How can I dispatch different implementations based on characteristics of the input parameters?," I'd like to be able to dispatch different implementations of a function, based not only on the type of the first parameter, but based on arbitrary predicates. Currently I have to do it like so: Here's something in the spirit of what I'd like to be able to do: It is similar to Python 3's singledispatch, however singledispatch only supports dispatch on types, not on arbitrary predicates.TL;DR: Is there a library that allows a predicate-based dispatch of a function based on arbitrary predicates (not only the parameter's type)? <code>  def f(param): try: if param > 0: # do something except TypeError: pass try: if all(isinstance(item, str) for item in param): # do something else except TypeError: raise TypeError('Illegal input.') @genericdef f(param): raise TypeError('Illegal input.') # default@f.when(lambda param: param > 0)def f_when_param_positive(param): # do something@f.when(lambda param: all(isinstance(item, str) for item in param))def f_when_param_iterable_of_strings(param): # do something else",Function pattern/predicate matching in Python
Create a directed graph using python-igragh," I have 2 Node types, where TypeA will always point to TypeB, TypeB has no outbound edges. How can I indicate this as a directed graph using igraph? <code> ",Create a directed graph using python-igraph
What is the use of conftest.py files?," I recently discovered pytest. It seems great. However, I feel the documentation could be better.I'm trying to understand what conftest.py files are meant to be used for.In my (currently small) test suite I have one conftest.py file at the project root. I use it to define the fixtures that I inject into my tests.I have two questions:Is this the correct use of conftest.py? Does it have other uses?Can I have more than one conftest.py file? When would I want to do that? Examples will be appreciated.More generally, how would you define the purpose and correct use of conftest.py file(s) in a py.test test suite? <code> ","In pytest, what is the use of conftest.py files?"
"In py.test, what is the use of conftest.py files?"," I recently discovered pytest. It seems great. However, I feel the documentation could be better.I'm trying to understand what conftest.py files are meant to be used for.In my (currently small) test suite I have one conftest.py file at the project root. I use it to define the fixtures that I inject into my tests.I have two questions:Is this the correct use of conftest.py? Does it have other uses?Can I have more than one conftest.py file? When would I want to do that? Examples will be appreciated.More generally, how would you define the purpose and correct use of conftest.py file(s) in a py.test test suite? <code> ","In pytest, what is the use of conftest.py files?"
beautifulsoup - scrape items after a certain element," I have an Html document that look like this: I want to scrape only links that immediately follows the code tag. If I do soup.findAll('a') it returns all hyperlinks.How can I make BS4 to start scraping after that specific code element? <code>  <div id=""whatever""> <a href=""unwanted link""></a> <a href=""unwanted link""></a> ... <code>blah blah</code> ... <a href=""interesting link""></a> <a href=""interesting link""></a> ...</div>",How to scrape elements that immediately follows a certain element?
SQLAlchemy: access to joined model fields," I have a model with a foreign key to another model. I query the first model, and want to access the model related to it. In Django I can access the model from the foreign key. How do I do this in SQLAlchemy? Django works like this: <code>  class Model(Base): field_id = Column(Integer, ForeignKey('Model2.id'))class Model2(Base): id = Column(Integer) needed_field = Column(Integer) models = Model.query.all()return render to template('templ.html', models=models) models = Model.objects.all()model.field_id.needed_field # loop in template",Access SQLAlchemy model linked with foreign key
Create open bounds indicators from get_dummies on discretize numerical," From a numeric age pandas column, discretize as ageD with qcut, we create open bounds from the qcut bounds: From Index([u'[5, 30]', u'(30, 70]'], dtype='object') we make bopens: Then we convert categorical variable into dummy/indicator variables with get_dummies: I want to enrich the data frame with the open bounds columns, df.shape will bequite big, ~(10e6, 32). What is the best way to make for each line the 6 bopen cols ?The target df will look like this one: PS: the get_open_bounds used to make bopens: <code>  import pandas as pdfrom itertools import chaind = {'age': {0: 5, 1: 23, 2: 43, 3: 70, 4: 30}}df = pd.DataFrame.from_dict(d)df['ageD'] = pd.qcut(df.iloc[:, 0], 2)df.ageD.cat.categories# Index([u'[5, 30]', u'(30, 70]'], dtype='object') >>> bopens = get_open_bounds(df)>>> bopens# ['(-inf, 5]', '(-inf, 30]', '(-inf, 70]', '(5, +inf)', '(30, +inf)', '(70, +inf)'] df = pd.get_dummies(df)print df# age ageD_[5, 30] ageD_(30, 70]# 0 5 1 0# 1 23 1 0# 2 43 0 1# 3 70 0 1# 4 30 1 0 >>> df age age_[5, 30] age_(30, 70] (-inf, 5] (-inf, 30] (-inf, 70] (5, +inf) (30, +inf) (70, +inf)0 5 1 0 1 1 1 0 0 01 23 1 0 0 1 1 1 0 02 43 0 1 0 0 1 1 1 03 70 0 1 0 0 1 1 1 04 30 1 0 0 1 1 1 0 0 def get_open_bounds(df): bounds = [(int(x[1:]), int(y[:-1])) for x, y in [c.split(', ') for c in df.ageD.cat.categories]] bounds = list(chain(*bounds)) bounds # [5, 30, 30, 70] # to get uniques, keeping the order bounds = [b for idx, b in enumerate(bounds) if b not in bounds[:idx]] # make the open bounds bopens = [""(-inf, {}]"".format(b) for b in bounds] + \ [""({}, +inf)"".format(b) for b in bounds] return bopens",Create open bounds indicators from pandas get_dummies on discretized numerical
Create open bounds indicators from pandas get_dummies on discretize numerical," From a numeric age pandas column, discretize as ageD with qcut, we create open bounds from the qcut bounds: From Index([u'[5, 30]', u'(30, 70]'], dtype='object') we make bopens: Then we convert categorical variable into dummy/indicator variables with get_dummies: I want to enrich the data frame with the open bounds columns, df.shape will bequite big, ~(10e6, 32). What is the best way to make for each line the 6 bopen cols ?The target df will look like this one: PS: the get_open_bounds used to make bopens: <code>  import pandas as pdfrom itertools import chaind = {'age': {0: 5, 1: 23, 2: 43, 3: 70, 4: 30}}df = pd.DataFrame.from_dict(d)df['ageD'] = pd.qcut(df.iloc[:, 0], 2)df.ageD.cat.categories# Index([u'[5, 30]', u'(30, 70]'], dtype='object') >>> bopens = get_open_bounds(df)>>> bopens# ['(-inf, 5]', '(-inf, 30]', '(-inf, 70]', '(5, +inf)', '(30, +inf)', '(70, +inf)'] df = pd.get_dummies(df)print df# age ageD_[5, 30] ageD_(30, 70]# 0 5 1 0# 1 23 1 0# 2 43 0 1# 3 70 0 1# 4 30 1 0 >>> df age age_[5, 30] age_(30, 70] (-inf, 5] (-inf, 30] (-inf, 70] (5, +inf) (30, +inf) (70, +inf)0 5 1 0 1 1 1 0 0 01 23 1 0 0 1 1 1 0 02 43 0 1 0 0 1 1 1 03 70 0 1 0 0 1 1 1 04 30 1 0 0 1 1 1 0 0 def get_open_bounds(df): bounds = [(int(x[1:]), int(y[:-1])) for x, y in [c.split(', ') for c in df.ageD.cat.categories]] bounds = list(chain(*bounds)) bounds # [5, 30, 30, 70] # to get uniques, keeping the order bounds = [b for idx, b in enumerate(bounds) if b not in bounds[:idx]] # make the open bounds bopens = [""(-inf, {}]"".format(b) for b in bounds] + \ [""({}, +inf)"".format(b) for b in bounds] return bopens",Create open bounds indicators from pandas get_dummies on discretized numerical
How to plot correlation matrix in Python," The figure below is plotted using the open-air R package:I know matplotlib has the plt.matshow function,but it can't clearly show the relation between variables at the same time. Here is my early work df is a pandas dataframe with 7 variables shows like below: I don't know how to attach a .csv file to StackOverflow. Using plt.matshow(df.corr(),cmap = plt.cm.Greens), the figure shows like this: The second figure can't represent the correlation relations of the variables as clearly as the first one.Edit:I upload the csv file to Google docs here. <code> ","How can I plot a correlation matrix as a set of ellipses, similar to the R open-air package?"
cannot download scikit-learn," I am pretty new to python. I want to use KMean code, and I want to install scikit-learn or sklearn. I used this code to attempt install these packages: But I got this error: What is the cause of the problem?  <code>  pip install -U sklearnpip install -U scikit-learn Command /usr/bin/python -c ""import setuptools, tokenize;__file__='/tmp/pip_build_reihaneh/sklearn/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-89YQB7-record/install-record.txt --single-version-externally-managed --compile failed with error code 1 in /tmp/pip_build_reihaneh/sklearnStoring debug log for failure in /home/reihaneh/.pip/pip.log",Cannot download and install scikit-learn
Reverse the order of legend," I use the following code to plot the bar graph and need to present a legend in reverse order. How can I do it? <code>  colorsArr = plt.cm.BuPu(np.linspace(0, 0.5, len(C2)))p = numpy.empty(len(C2), dtype=object)plt.figure(figsize=(11, 11))prevBar = 0for index in range(len(C2)): plt.bar(ind, C2[index], width, bottom=prevBar, color=colorsArr[index], label=C0[index]) prevBar = prevBar + C2[index]# Positions of the x-axis ticks (center of the bars as bar labels)tick_pos = [i + (width/2) for i in ind]plt.ylabel('Home Category')plt.title('Affinity - Retail Details(Home category)')# Set the x ticks with namesplt.xticks(tick_pos, C1)plt.yticks(np.arange(0, 70000, 3000))plt.legend(title=""Line"", loc='upper left')# Set a buffer around the edgeplt.xlim(-width*2, width*2)plt.show()",Reverse the order of a legend
Run python script every 10 seconds," I have a function to do some work. The code should repeat 130 million times.Currently, I use Crontab to run a python script every 1 min. It takes too long a time, I want that this python script run when I run it the first time and keep repeating continuously until the work gets over. I want a 10 seconds break between 2 task. How can I do that? <code> ",Run Python script every 10 seconds
str() VS json.dumps() when converting json to string in python," I did not explain my questions clearly at beginning.Try to use str() and json.dumps() when converting JSON to string in python. My question is: My expected output: ""{'jsonKey': 'jsonValue','title': 'hello world''}"" My expected output: ""{'jsonKey': 'jsonValue','title': 'hello world\""'}""It is not necessary to change the output string to json (dict) again for me.How to do this? <code>  >>> data = {'jsonKey': 'jsonValue',""title"": ""hello world""}>>> print json.dumps(data){""jsonKey"": ""jsonValue"", ""title"": ""hello world""}>>> print str(data){'jsonKey': 'jsonValue', 'title': 'hello world'}>>> json.dumps(data)'{""jsonKey"": ""jsonValue"", ""title"": ""hello world""}'>>> str(data)""{'jsonKey': 'jsonValue', 'title': 'hello world'}"" >>> data = {'jsonKey': 'jsonValue',""title"": ""hello world'""}>>> str(data)'{\'jsonKey\': \'jsonValue\', \'title\': ""hello world\'""}'>>> json.dumps(data)'{""jsonKey"": ""jsonValue"", ""title"": ""hello world\'""}'>>> >>> data = {'jsonKey': 'jsonValue',""title"": ""hello world""""} File ""<stdin>"", line 1 data = {'jsonKey': 'jsonValue',""title"": ""hello world""""} ^SyntaxError: EOL while scanning string literal>>> data = {'jsonKey': 'jsonValue',""title"": ""hello world\""""}>>> json.dumps(data)'{""jsonKey"": ""jsonValue"", ""title"": ""hello world\\""""}'>>> str(data)'{\'jsonKey\': \'jsonValue\', \'title\': \'hello world""\'}'",converting JSON to string in Python
converting json to string in python," I did not explain my questions clearly at beginning.Try to use str() and json.dumps() when converting JSON to string in python. My question is: My expected output: ""{'jsonKey': 'jsonValue','title': 'hello world''}"" My expected output: ""{'jsonKey': 'jsonValue','title': 'hello world\""'}""It is not necessary to change the output string to json (dict) again for me.How to do this? <code>  >>> data = {'jsonKey': 'jsonValue',""title"": ""hello world""}>>> print json.dumps(data){""jsonKey"": ""jsonValue"", ""title"": ""hello world""}>>> print str(data){'jsonKey': 'jsonValue', 'title': 'hello world'}>>> json.dumps(data)'{""jsonKey"": ""jsonValue"", ""title"": ""hello world""}'>>> str(data)""{'jsonKey': 'jsonValue', 'title': 'hello world'}"" >>> data = {'jsonKey': 'jsonValue',""title"": ""hello world'""}>>> str(data)'{\'jsonKey\': \'jsonValue\', \'title\': ""hello world\'""}'>>> json.dumps(data)'{""jsonKey"": ""jsonValue"", ""title"": ""hello world\'""}'>>> >>> data = {'jsonKey': 'jsonValue',""title"": ""hello world""""} File ""<stdin>"", line 1 data = {'jsonKey': 'jsonValue',""title"": ""hello world""""} ^SyntaxError: EOL while scanning string literal>>> data = {'jsonKey': 'jsonValue',""title"": ""hello world\""""}>>> json.dumps(data)'{""jsonKey"": ""jsonValue"", ""title"": ""hello world\\""""}'>>> str(data)'{\'jsonKey\': \'jsonValue\', \'title\': \'hello world""\'}'",converting JSON to string in Python
Calculate the mode of a pyspark dataframe column?," Ultimately what I want is the mode of a column, for all the columns in the DataFrame. For other summary statistics, I see a couple of options: use DataFrame aggregation, or map the columns of the DataFrame to an RDD of vectors (something I'm also having trouble doing) and use colStats from MLlib. But I don't see mode as an option there. <code> ",Calculate the mode of a PySpark DataFrame column?
Numpy unique array value manipulation," I have a set of data (X,Y). My independent variable values X are not unique, so there are multiple repeated values, I want to output a new array containing : X_unique, which is a list of unique values of X. Y_mean, the mean of all of the Y values corresponding to X_unique. Y_std, the standard deviation of all the Y values corresponding to X_unique. <code>  x = data[:,0]y = data[:,1]",Performing grouped average and standard deviation with NumPy arrays
Odoo. Best way to support/synchronize db," I have some modules which expand add-ons of Odoo. For example, models in my_module which expand crm: The same situation is for modules which expand hr, product, etc.I need to make some changes to the models. For example, in my_module_1, I need to change a couple of fields(type, relation), in my_module_2, just to remove a few fields etc. Of course I also need to change views of each module. And of course I have my custom models which have dependencies with models from different apps/modules. But I have data on production which must be stored. I did not find any information about migrations(or synchronization of modules) in Odoo.My question is: What is the best way to update modules/apps on production(if we have many changes in fields of models and views)? Thanks in advance. <code>  class Lead(models.Model): _inherit = 'crm.lead' # exmaple fields field_1 = fields.Char(...) field_2 = fields.Many2one(...) # ... field 99class Stage(models.Model): _inherit = 'crm.stage' # exmaple fields field_1 = fields.Char(...) field_2 = fields.Many2one(...) # ... field 99",Odoo Migrations
Odoo. Migrations," I have some modules which expand add-ons of Odoo. For example, models in my_module which expand crm: The same situation is for modules which expand hr, product, etc.I need to make some changes to the models. For example, in my_module_1, I need to change a couple of fields(type, relation), in my_module_2, just to remove a few fields etc. Of course I also need to change views of each module. And of course I have my custom models which have dependencies with models from different apps/modules. But I have data on production which must be stored. I did not find any information about migrations(or synchronization of modules) in Odoo.My question is: What is the best way to update modules/apps on production(if we have many changes in fields of models and views)? Thanks in advance. <code>  class Lead(models.Model): _inherit = 'crm.lead' # exmaple fields field_1 = fields.Char(...) field_2 = fields.Many2one(...) # ... field 99class Stage(models.Model): _inherit = 'crm.stage' # exmaple fields field_1 = fields.Char(...) field_2 = fields.Many2one(...) # ... field 99",Odoo Migrations
Logging Format Interpolation," For the following code: pylint produces the following warning: logging-format-interpolation (W1202): Use % formatting in logging functions and pass the % parameters as arguments Used when a logging statement has a call form of logging.(format_string.format(format_args...)). Such calls should use % formatting instead, but leave interpolation to the logging function by passing the parameters as arguments.I know I can turn off this warning, but I'd like to understand it. I assumed using format() is the preferred way to print out statements in Python 3. Why is this not true for logger statements? <code>  logger.debug('message: {}'.format('test'))",PyLint message: logging-format-interpolation
multiple assigments with a comma in python," I tried to find an explanation of this, the Gotcha part: returns: I understand what happens with multiple equals: but using it together with a comma, I cannot understand the behaviour, ideas in why it works that way?  <code>  b = ""1984""a = b, c = ""AB""print(a, b, c) ('AB', 'A', 'B') a = b = 1",multiple assignments with a comma in python
Python find the indexes of a list given a condition," I have a list, let's say: I would like to find the minimum and maximum indices of this list where list_A > 0, i.e. in the above example, it would be 3 and 7. For other lists, which increase monotonically, I have been using np.searchsorted, like np.searchsorted(list,[0.5,1.0]) to find the indices wherein the list is between 0.5 and 1.0 respectively. But this case is quite different and the np.searchsorted doesn't work here, or maybe it does in a way which I don't know ! <code>  list_A = [0,0,0,1.0,2.0,3.0,2.0,1.0,0,0,0]",Find the minimum and maximum indices of a list given a condition
Generate 1d numpy with chuncks of random length," I need to generate 1D array where repeated sequences of integers are separated by a random number of zeros.So far I am using next code for this: It works but looks very slow when I need a lot of long sequences. So, how can I optimize it?  <code>  from random import normalvariateregular_sequence = np.array([1,2,3,4,5], dtype=np.int)n_iter = 10lag_mean = 10 # mean length of zeros sequencelag_sd = 1 # standard deviation of zeros sequence length# Sequence of lags lengthslag_seq = [int(round(normalvariate(lag_mean, lag_sd))) for x in range(n_iter)]# Generate list of concatenated zeros and regular sequencesseq = [np.concatenate((np.zeros(x, dtype=np.int), regular_sequence)) for x in lag_seq]seq = np.concatenate(seq)",Generate 1d numpy with chunks of random length
Python insert 0s into 2d array," I have an array x: and I want y: where the first row is x-1, the second row is x, and the third row is x+1. All even column indices are zero.I'm doing: I was thinking there might be a one-liner to do this instead of 4. <code>  x = [0, -1, 0, 3] y = [[0, -2, 0, 2], [0, -1, 0, 3], [0, 0, 0, 4]] y=np.vstack(x-1, x, x+1)y[0][::2] = 0y[1][::2] = 0y[2][::2] = 0",Insert 0s into 2d array
Python Pandas: remove group from the data when a value in the group meets a required condition," I have groupings of values in the data and within each group, I would like to check if a value within the group is below 8. If this condition is met, the entire group is removed from the data set.Please note the value I'm referring to lies in another column to the groupings column.Example Input: Output: <code>  Groups Count 1 7 1 11 1 9 2 12 2 15 2 21 Groups Count 2 12 2 15 2 21 ",Pandas: remove group from the data when a value in the group meets a required condition
how to separate spark python RDD?," I have an RDD structure like: and I want it to become: How do I write a map or reduce function to make it work? <code>  rdd = [[[1],[2],[3]], [[4],[5]], [[6]], [[7],[8],[9],[10]]] rdd = [1,2,3,4,5,6,7,8,9,10]",How to flatten nested lists in PySpark?
Do I need to change data shape if I change the number of layers in an artificial neural network?," The below code works perfectly okay. If I try to change all the 64s to 128s then I get an error about shape. Do I need to change the input data shape if I change the number of layers in an artificial neural network when using Keras? I didn't think so because it asks for input_dim which is correct. Works: Doesn't Work: <code>  model = Sequential()model.add(Dense(64, input_dim=14, init='uniform'))model.add(Activation('tanh'))model.add(Dropout(0.5))model.add(Dense(64, init='uniform'))model.add(Activation('tanh'))model.add(Dropout(0.5))model.add(Dense(64, init='uniform'))model.add(Activation('softmax'))sgd3 = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)model.compile(loss='binary_crossentropy', optimizer=sgd3)model.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2) model = Sequential()model.add(Dense(128, input_dim=14, init='uniform'))model.add(Activation('tanh'))model.add(Dropout(0.5))model.add(Dense(128, init='uniform'))model.add(Activation('tanh'))model.add(Dropout(0.5))model.add(Dense(128, init='uniform'))model.add(Activation('softmax'))sgd3 = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)model.compile(loss='binary_crossentropy', optimizer=sgd3)model.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2)",How do you change the number of units in a layer when using Keras?
Why are sockets closed in list comprehension but not for loop?," I'm trying to create a list of available ports in Python. I am following this tutorial, but instead of printing the open ports, I'm adding them to a list.Initially, I had something like the following: This clearly works fine, but it is well known that comprehensions are faster than loops, so I now have: I assumed the sockets wouldn't be closed, but I tested it with the following: and indeed the open ports were printed.Why are the sockets closed in the comprehension but not the for loop? Can I rely on this behavior or is this a red herring? <code>  available_ports = []try: for port in range(1,8081): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) result = sock.connect_ex((remoteServerIP, port)) if result == 0: available_ports.append(port) sock.close()# ... try: available_ports = [port for port in range(1, 8081) if not socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect_ex((remoteServerIP, port))]# ... try: available_ports = [port for port in range(1, 8081) if not socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect_ex((remoteServerIP, port))] for port in range(1,8081): sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) result = sock.connect_ex((remoteServerIP, port)) if result == 0: print(""Port {}: \t Open"".format(port)) sock.close()# ...",Why are sockets closed in list comprehension but not in for loop?
Python: calling list() has side-effect?," a evaluates to [ ]. a evaluates to [1, 2].The first result is unexpected to me. What semantics are going on here? <code>  a = range(1, 3)a = iter(a)list(a)a = list(a) a = range(1, 3)a = iter(a)a = list(a)",Calling list() empties my iterable object?
Calling list() has side-effect?," a evaluates to [ ]. a evaluates to [1, 2].The first result is unexpected to me. What semantics are going on here? <code>  a = range(1, 3)a = iter(a)list(a)a = list(a) a = range(1, 3)a = iter(a)a = list(a)",Calling list() empties my iterable object?
how to lanuch window shortcut using python," I want to launch a shortcut named blender.ink located at ""D://games//blender.ink"". I have tryed using:- But it failed, it only launches exe files. <code>  os.startfile (""D://games//blender.ink"")",How to launch a Window's shortcut using Python
Python write dataframe to excel with a title," I would like to print out a dataframe in Excel. I am using ExcelWriter as follows: This produces what I need but in addition I would like to add a title with some text (explanations) for the data on top of the table (startcol=0 ,startrow=0).How can I add a string title using ExcelWriter?  <code>  writer = pd.ExcelWriter('test.xlsx')df = DataFrame(C,ind) # C is the matrix and ind is the list of corresponding indices df.to_excel(writer, startcol = 0, startrow = 5)writer.save()",Write dataframe to excel with a title
How to avoid NLTK's sentence tokenizer spliting on abbreviations?," I'm currently using NLTK for language processing, but I have encountered a problem of sentence tokenizing.Here's the problem:Assume I have a sentence: ""Fig. 2 shows a U.S.A. map.""When I use punkt tokenizer, my code looks like this: It returns this: The tokenizer can't detect the abbreviation ""U.S.A."", but it worked on ""fig"".Now when I use the default tokenizer NLTK provides: This time I get: It recognizes the more common ""U.S.A."" but fails to see ""fig""!How can I combine these two methods? I want to use default abbreviation choices as well as adding my own abbreviations. <code>  from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameterspunkt_param = PunktParameters()abbreviation = ['U.S.A', 'fig']punkt_param.abbrev_types = set(abbreviation)tokenizer = PunktSentenceTokenizer(punkt_param)tokenizer.tokenize('Fig. 2 shows a U.S.A. map.') ['Fig. 2 shows a U.S.A.', 'map.'] import nltknltk.tokenize.sent_tokenize('Fig. 2 shows a U.S.A. map.') ['Fig.', '2 shows a U.S.A. map.']",How to avoid NLTK's sentence tokenizer splitting on abbreviations?
shortcut evalution of numpy's array comparison," In numpy if I want to compare two arrays, say for example I want to test if all elements in A are less than values in B, I use if (A < B).all():. But in practice this requires allocation and evaluation of complete array C = A < B and then calling C.all() on it. This is a bit of waste. Is there any way to 'shortcut' the comparison, i.e. directly evaluate A < B element by element (without allocation and calculation of temporary C) and stop and return False when first invalid element comparison is found? <code> ",shortcut evaluation of numpy's array comparison
Seeking Elegant Python Short Replacemnet," Is there an elegant way to iterate through possible dice rolls with up to five dice?I want to replace this hacky Python: Desired result: <code>  rolls = [ [str(a) for a in range(1,7)], [''.join([str(a), str(b)]) for a in range(1, 7) for b in range(1, 7) if a <= b], [''.join([str(a), str(b), str(c)]) for a in range(1, 7) for b in range(1, 7) for c in range(1, 7) if a <= b <= c], [''.join([str(a), str(b), str(c), str(d)]) for a in range(1, 7) for b in range(1, 7) for c in range(1, 7) for d in range(1, 7) if a <= b <= c <= d], [''.join([str(a), str(b), str(c), str(d), str(e)]) for a in range(1, 7) for b in range(1, 7) for c in range(1, 7) for d in range(1, 7) for e in range(1, 7) if a <= b <= c <= d <= e]]print(rolls) [['1', '2', '3', '4', '5', '6'], ['11', '12', '13', '14', '15', '16', '22', '23', '24', '25', '26', '33', '34', '35', '36', '44', '45', '46', '55', '56', '66'], ['111', '112', '113', '114', '115', '116', '122', '123', '124', '125', '126', '133', '134', '135', '136', '144', '145', '146', '155', '156', '166', '222', '223', '224', '225', '226', '233', '234', '235', '236', '244', '245', '246', '255', '256', '266', '333', '334', '335', '336', '344', '345', '346', '355', '356', '366', '444', '445', '446', '455', '456', '466', '555', '556', '566', '666'], ['1111', '1112', '1113', '1114', '1115', '1116', '1122', '1123', '1124', '1125', '1126', '1133', '1134', '1135', '1136', '1144', '1145', '1146', '1155', '1156', '1166', '1222', '1223', '1224', '1225', '1226', '1233', '1234', '1235', '1236', '1244', '1245', '1246', '1255', '1256', '1266', '1333', '1334', '1335', '1336', '1344', '1345', '1346', '1355', '1356', '1366', '1444', '1445', '1446', '1455', '1456', '1466', '1555', '1556', '1566', '1666', '2222', '2223', '2224', '2225', '2226', '2233', '2234', '2235', '2236', '2244', '2245', '2246', '2255', '2256', '2266', '2333', '2334', '2335', '2336', '2344', '2345', '2346', '2355', '2356', '2366', '2444', '2445', '2446', '2455', '2456', '2466', '2555', '2556', '2566', '2666', '3333', '3334', '3335', '3336', '3344', '3345', '3346', '3355', '3356', '3366', '3444', '3445', '3446', '3455', '3456', '3466', '3555', '3556', '3566', '3666', '4444', '4445', '4446', '4455', '4456', '4466', '4555', '4556', '4566', '4666', '5555', '5556', '5566', '5666', '6666'], ['11111', '11112', '11113', '11114', '11115', '11116', '11122', '11123', '11124', '11125', '11126', '11133', '11134', '11135', '11136', '11144', '11145', '11146', '11155', '11156', '11166', '11222', '11223', '11224', '11225', '11226', '11233', '11234', '11235', '11236', '11244', '11245', '11246', '11255', '11256', '11266', '11333', '11334', '11335', '11336', '11344', '11345', '11346', '11355', '11356', '11366', '11444', '11445', '11446', '11455', '11456', '11466', '11555', '11556', '11566', '11666', '12222', '12223', '12224', '12225', '12226', '12233', '12234', '12235', '12236', '12244', '12245', '12246', '12255', '12256', '12266', '12333', '12334', '12335', '12336', '12344', '12345', '12346', '12355', '12356', '12366', '12444', '12445', '12446', '12455', '12456', '12466', '12555', '12556', '12566', '12666', '13333', '13334', '13335', '13336', '13344', '13345', '13346', '13355', '13356', '13366', '13444', '13445', '13446', '13455', '13456', '13466', '13555', '13556', '13566', '13666', '14444', '14445', '14446', '14455', '14456', '14466', '14555', '14556', '14566', '14666', '15555', '15556', '15566', '15666', '16666', '22222', '22223', '22224', '22225', '22226', '22233', '22234', '22235', '22236', '22244', '22245', '22246', '22255', '22256', '22266', '22333', '22334', '22335', '22336', '22344', '22345', '22346', '22355', '22356', '22366', '22444', '22445', '22446', '22455', '22456', '22466', '22555', '22556', '22566', '22666', '23333', '23334', '23335', '23336', '23344', '23345', '23346', '23355', '23356', '23366', '23444', '23445', '23446', '23455', '23456', '23466', '23555', '23556', '23566', '23666', '24444', '24445', '24446', '24455', '24456', '24466', '24555', '24556', '24566', '24666', '25555', '25556', '25566', '25666', '26666', '33333', '33334', '33335', '33336', '33344', '33345', '33346', '33355', '33356', '33366', '33444', '33445', '33446', '33455', '33456', '33466', '33555', '33556', '33566', '33666', '34444', '34445', '34446', '34455', '34456', '34466', '34555', '34556', '34566', '34666', '35555', '35556', '35566', '35666', '36666', '44444', '44445', '44446', '44455', '44456', '44466', '44555', '44556', '44566', '44666', '45555', '45556', '45566', '45666', '46666', '55555', '55556', '55566', '55666', '56666', '66666']]",Elegant iteration over five dice
Seeking Elegant Python Dice Iteration," Is there an elegant way to iterate through possible dice rolls with up to five dice?I want to replace this hacky Python: Desired result: <code>  rolls = [ [str(a) for a in range(1,7)], [''.join([str(a), str(b)]) for a in range(1, 7) for b in range(1, 7) if a <= b], [''.join([str(a), str(b), str(c)]) for a in range(1, 7) for b in range(1, 7) for c in range(1, 7) if a <= b <= c], [''.join([str(a), str(b), str(c), str(d)]) for a in range(1, 7) for b in range(1, 7) for c in range(1, 7) for d in range(1, 7) if a <= b <= c <= d], [''.join([str(a), str(b), str(c), str(d), str(e)]) for a in range(1, 7) for b in range(1, 7) for c in range(1, 7) for d in range(1, 7) for e in range(1, 7) if a <= b <= c <= d <= e]]print(rolls) [['1', '2', '3', '4', '5', '6'], ['11', '12', '13', '14', '15', '16', '22', '23', '24', '25', '26', '33', '34', '35', '36', '44', '45', '46', '55', '56', '66'], ['111', '112', '113', '114', '115', '116', '122', '123', '124', '125', '126', '133', '134', '135', '136', '144', '145', '146', '155', '156', '166', '222', '223', '224', '225', '226', '233', '234', '235', '236', '244', '245', '246', '255', '256', '266', '333', '334', '335', '336', '344', '345', '346', '355', '356', '366', '444', '445', '446', '455', '456', '466', '555', '556', '566', '666'], ['1111', '1112', '1113', '1114', '1115', '1116', '1122', '1123', '1124', '1125', '1126', '1133', '1134', '1135', '1136', '1144', '1145', '1146', '1155', '1156', '1166', '1222', '1223', '1224', '1225', '1226', '1233', '1234', '1235', '1236', '1244', '1245', '1246', '1255', '1256', '1266', '1333', '1334', '1335', '1336', '1344', '1345', '1346', '1355', '1356', '1366', '1444', '1445', '1446', '1455', '1456', '1466', '1555', '1556', '1566', '1666', '2222', '2223', '2224', '2225', '2226', '2233', '2234', '2235', '2236', '2244', '2245', '2246', '2255', '2256', '2266', '2333', '2334', '2335', '2336', '2344', '2345', '2346', '2355', '2356', '2366', '2444', '2445', '2446', '2455', '2456', '2466', '2555', '2556', '2566', '2666', '3333', '3334', '3335', '3336', '3344', '3345', '3346', '3355', '3356', '3366', '3444', '3445', '3446', '3455', '3456', '3466', '3555', '3556', '3566', '3666', '4444', '4445', '4446', '4455', '4456', '4466', '4555', '4556', '4566', '4666', '5555', '5556', '5566', '5666', '6666'], ['11111', '11112', '11113', '11114', '11115', '11116', '11122', '11123', '11124', '11125', '11126', '11133', '11134', '11135', '11136', '11144', '11145', '11146', '11155', '11156', '11166', '11222', '11223', '11224', '11225', '11226', '11233', '11234', '11235', '11236', '11244', '11245', '11246', '11255', '11256', '11266', '11333', '11334', '11335', '11336', '11344', '11345', '11346', '11355', '11356', '11366', '11444', '11445', '11446', '11455', '11456', '11466', '11555', '11556', '11566', '11666', '12222', '12223', '12224', '12225', '12226', '12233', '12234', '12235', '12236', '12244', '12245', '12246', '12255', '12256', '12266', '12333', '12334', '12335', '12336', '12344', '12345', '12346', '12355', '12356', '12366', '12444', '12445', '12446', '12455', '12456', '12466', '12555', '12556', '12566', '12666', '13333', '13334', '13335', '13336', '13344', '13345', '13346', '13355', '13356', '13366', '13444', '13445', '13446', '13455', '13456', '13466', '13555', '13556', '13566', '13666', '14444', '14445', '14446', '14455', '14456', '14466', '14555', '14556', '14566', '14666', '15555', '15556', '15566', '15666', '16666', '22222', '22223', '22224', '22225', '22226', '22233', '22234', '22235', '22236', '22244', '22245', '22246', '22255', '22256', '22266', '22333', '22334', '22335', '22336', '22344', '22345', '22346', '22355', '22356', '22366', '22444', '22445', '22446', '22455', '22456', '22466', '22555', '22556', '22566', '22666', '23333', '23334', '23335', '23336', '23344', '23345', '23346', '23355', '23356', '23366', '23444', '23445', '23446', '23455', '23456', '23466', '23555', '23556', '23566', '23666', '24444', '24445', '24446', '24455', '24456', '24466', '24555', '24556', '24566', '24666', '25555', '25556', '25566', '25666', '26666', '33333', '33334', '33335', '33336', '33344', '33345', '33346', '33355', '33356', '33366', '33444', '33445', '33446', '33455', '33456', '33466', '33555', '33556', '33566', '33666', '34444', '34445', '34446', '34455', '34456', '34466', '34555', '34556', '34566', '34666', '35555', '35556', '35566', '35666', '36666', '44444', '44445', '44446', '44455', '44456', '44466', '44555', '44556', '44566', '44666', '45555', '45556', '45566', '45666', '46666', '55555', '55556', '55566', '55666', '56666', '66666']]",Elegant iteration over five dice
Certificate verification when using virtual enviroments," I have a root CA certificate installed on my machine and all is fine when issuing a requests when using the system install of the requests library: However if I issue the same request from within a virtual environment the certificate verification fails: Using requests.certs.where I can see the system install uses the systems CA bundle and the virtual environment uses the CA bundle shipped with requests: Is there another solution to picking up the system certs without providing the path on each request when using virtualenv, i.e: <code>  $ python -c 'import requests; print requests.get(""https://example.com"")'<Response [200]> $ python -c 'import requests; print requests.get(""https://example.com"")'requests.exceptions.SSLError: [Errno 1] _ssl.c:510: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed $ python -c ""import requests; print requests.certs.where()""/etc/ssl/certs/ca-certificates.crt$ (venv) python -c ""import requests; print requests.certs.where()"" .../venv/local/lib/python2.7/site-packages/requests/cacert.pem >>> requests.get(""https://example.com"" verify=""/etc/ssl/certs/ca-certificates.crt"")",Certificate verification when using virtual environments
Pandas: How can I use the apply() function for a single column?, I have a pandas data frame with two columns. I need to change the values of the first column without affecting the second one and get back the whole data frame with just first column values changed. How can I do that using apply in pandas? <code> ,How can I use the apply() function for a single column?
What `platforms` argument to `setup()` in `setup.py` does?," Looking through several projects recently, I noticed some of them use platforms argument to setup() in setup.py, though with only one value of any, i.e. OR From the name ""platforms"", I can make a guess about what this argument means, and it seems that the list variant is the right usage. So I googled, looked through setuptools docs, but I failed to find any explanation to what are the possible values to platforms and what it does/affects in package exactly.Please, explain or provide a link to explanation of what it does exactly and what values it accepts?P.S. Also tried to provide different values to it in my OS independent package and see what changes, when creating wheels, but it seems it does nothing. <code>  #setup.py file in project's package folder ... setup( ..., platforms=['any'], ...) #setup.py file in project's package folder... setup( ..., platforms='any', ...)",What does the `platforms` argument to `setup()` in `setup.py` do?
Weird for loop statment," I saw this for loop and I didn't quite understood why the last print is 2.Why it isn't 3 ? out: <code>  a = [0, 1, 2, 3]for a[-1] in a: print(a[-1]) 0122",Weird for loop statement
Is it possible to correct spelling in a Pandas DataFrame?," Using the TextBlob library it is possible to improve the spelling of strings by defining them as TextBlob objects first and then using the correct method. Example: Is it possible to do this to strings in a Pandas DataFrame series such as this one: To return this: Either using TextBlob or some other method.  <code>  from textblob import TextBlobdata = TextBlob('Two raods diverrged in a yullow waod and surry I culd not travl bouth')print (data.correct())Two roads diverged in a yellow wood and sorry I could not travel both data = [{'one': '3', 'two': 'two raods'}, {'one': '7', 'two': 'diverrged in a yullow'}, {'one': '8', 'two': 'waod and surry I'}, {'one': '9', 'two': 'culd not travl bouth'}]df = pd.DataFrame(data)df one two0 3 Two raods1 7 diverrged in a yullow2 8 waod and surry I3 9 culd not travl bouth one two0 3 Two roads1 7 diverged in a yellow2 8 wood and sorry I3 9 could not travel both",How to correct spelling in a Pandas DataFrame
Python: No module named joblib," I'm a new to Python, and I have a trouble in import library.I wrote a code then I got an error in IPython.I'm using ubuntu and I installed scikit_learn-0.18 by using ""sudo apt-get install python-sklearn"" command but encountered above error.I also tried to use ""sudo easy_install joblib"" but the error was not erased.What is wrong? Would you help me? Thank you. <code>  from sklearn.linear_model import LogisticRegression ImportError Traceback (most recent call last)<ipython-input-19-c84b03903d9e> in <module>()----> 1 from sklearn.linear_model import LogisticRegression/usr/lib/python2.7/dist-packages/sklearn/linear_model/__init__.py in <module>() 10 # complete documentation. 11 ---> 12 from .base import LinearRegression 13 14 from .bayes import BayesianRidge, ARDRegression/usr/lib/python2.7/dist-packages/sklearn/linear_model/base.py in <module>() 22 23 from ..externals import six---> 24 from ..externals.joblib import Parallel, delayed 25 from ..base import BaseEstimator, ClassifierMixin, RegressorMixin 26 from ..utils import as_float_array, atleast2d_or_csr, safe_asarray/usr/lib/python2.7/dist-packages/sklearn/externals/joblib/__init__.py in <module>() 1 # yoh: use system-wide joblib 2 ----> 3 from joblib import *ImportError: No module named joblib",Python: No module named ... How to use pip
"Is there an ERB-like templating language, but for Python?"," ERB, if you're not familiar with it, is the templating language used by Ruby On Rails and many other Ruby projects. In short, it allows you to evaluate raw ruby code inside HTML templates and render the result.Consider the following: The Ruby instance variable @name would get replaced and rendered out onto the page seen by users.Now, Python has a common templating language known as Jinja2 which works in almost the same way (mostly using {{ }}s instead of <% %>s), but there's one massive difference between the two:ERB allows you to use any valid Ruby code, while Jinja2 only has a very limited subset of Python-esque language, but not raw Python.The question:How do you template HTML with Python, using the entire language, rather than a limited subset? <code>  #hello.erb<html><body> <p>Hello, <%= @name %></p></body><html>",How to template like ERB in Python?
Can i install Django as offline in Windows7?," I'm new to Python and PyCharm. When I'm trying to make a Django project, it shows an error box. Later I found that it's due to no availability of internet connection on my PC.Is there any way to install Django as offline in Windows 7 PC? <code> ",Can I install Django as offline in Windows 7?
Assign op in TensorFlow: what is return value?," I was trying to build an autoincrementing graph in TensorFlow. I thought that the assign op might be suitable for that, but found no documentation for it.I assumed that this op returns its value—like in C-like languages—and wrote the following code: and this code works.The question is: is this the expected behavior? Why is the assign op not documented here: https://www.tensorflow.org/versions/0.6.0/api_docs/index.htmlIs it a non-recommended op? <code>  import tensorflow as tfcounter = tf.Variable(0, name=""counter"")one = tf.constant(1)ten = tf.constant(10)new_counter = tf.add(counter, one)assign = tf.assign(counter, new_counter)result = tf.add(assign, ten)init_op = tf.initialize_all_variables()with tf.Session() as sess: sess.run(init_op) for _ in range(3): print sess.run(result)",Assign op in TensorFlow: what is the return value?
Limiting threads to max of 20," How can I limit the maximum of open threads to 20 in the following code? I'm aware that there have been some similar questions asked in the past, but I specifically want to know how this is best done with a Queue and with a working example if possible. <code>  # b is a list with 10000 items threads = [threading.Thread(target=targetFunction, args=(ptf,anotherarg)) for ptf in b] for thread in threads: thread.start() for thread in threads: thread.join()",Threading queue working example
python code to calcualte angle between three point using thier 3D coordinates," I have write down a code to calculate angle between three points using their 3D coordinates. output of the code: but when i used one of the software to calculate the same it gives output bit different 120 degree. please help reference i have used to write the program:(How to calculate bond angle in protein db file?) <code>  import numpy as npa = np.array([32.49, -39.96,-3.86])b = np.array([31.39, -39.28, -4.66])c = np.array([31.14, -38.09,-4.49])f = a-b # normalization of vectorse = b-c # normalization of vectorsangle = dot(f, e) # calculates dot product print degrees(cos(angle)) # calculated angle in radians to degree degree 33.4118214995",python code to calculate angle between three point using their 3D coordinates
Python keyword arguments unpack and return dictonary," I have a function definition as below and I am passing keyword arguments. How do I get to return a dictionary with the same name as the keyword arguments?Manually I can do: But I don't want to do that. Is there any way that I can make this work without actually typing the dict? <code>  def generate_student_dict(first_name=None, last_name=None , birthday=None, gender =None): return { 'first_name': first_name, 'last_name': last_name, 'birthday': birthday, 'gender': gender } def generate_student_dict(self, first_name=None, last_name=None, birthday=None, gender=None): return # Packed value from keyword argument.",Python keyword arguments unpack and return dictionary
Twisted Deferred not raising Exception," I'm reading through McKellar and Fettig's Twisted Network Programming Essentials, 2nd Ed.I am running Twisted 15.5.0 on Python 2.7.10 on Windows 7.In the section about Deferred there's an example that is supposed to raise an Unhandled Error in Deferred - but I am only getting complete silence from the console when I run the minimal example below:Minimal example $ python test.py(no output)Minimal example from actual book textThe actual example from the book is along these lines: And the expected output is listed in the book as: callback3 raises an Exception, and because there is no registered errback to handle the Exception, the program terminates and reports an Unhandled Error to the user. The result is: Am I doing something wrong?EDIT:I have gotten the error to display correctly on my machine.To enable the error to get logged without having an errback handler on the Deferred object, I needed to add the following to my snippet: Now, when I run my minimal example from the first code snippet in my question, I get the following output: So, now I can verify that Twisted does indeed raise an error as it is meant to - it just didn't feel like telling me for some reason. If anyone can elaborate as to why that would be the default case for handling an exception without an errback defined, I'd love to know.I've changed the title to reflect my new question. <code>  from twisted.internet.defer import Deferreddef raiseErr(err): raise Exception(err)d = Deferred()d.addCallback(raiseErr)d.callback(""oh no"") from twisted.internet.defer import Deferreddef callback1(result): print ""Callback 1 said:"", result return resultdef callback2(result): print ""Callback 2 said:"", resultdef callback3(result): raise Exception(""Callback 3"")def errback1(failure): print ""Errback 1 had an an error on"", failure return failured = Deferred()d.addCallback(callback1)d.addCallback(callback2)d.addCallback(callback3)d.callback(""Test"") Callback 1 said: TestCallback 2 said: TestUnhandled error in Deferred:Unhandled ErrorTraceback (most recent call last):File ""/tmp/test.py"", line 33, in <module>d.callback(""Test"")<...>File ""/tmp/test.py"", line 11, in callback3raise Exception(""Callback 3"")exceptions.Exception: Callback 3 import sysfrom twisted.python import loglog.startLogging(sys.stdout)# rest of the code goes here 2016-02-05 09:45:43-0600 [-] Log opened.2016-02-05 09:45:43-0600 [-] Invalid format string or unformattable object in log message: '%(log_legacy)s', {'format': '%(log_legacy)s', 'log_legacy': <twisted.logger._stdlib.StringifiableFromEvent object at 0x038913F0>, 'time': 1454687143.778, 'message': (), 'log_time': 1454687143.778, 'log_namespace': 'twisted.internet.defer', 'log_level': <LogLevel=critical>, 'log_source': None, 'system': '-', 'isError': True, 'log_logger': <Logger 'twisted.internet.defer'>, 'log_format': 'Unhandled error in Deferred:'}2016-02-05 09:45:43-0600 [-] Unhandled Error Traceback (most recent call last): File ""testd.py"", line 13, in <module> d.callback(""oh no"") File ""C:\Swat\.virtualenvs\twisted\lib\site-packages\twisted\internet\defer.py"", line 393, in callback self._startRunCallbacks(result) File ""C:\Swat\.virtualenvs\twisted\lib\site-packages\twisted\internet\defer.py"", line 501, in _startRunCallbacks self._runCallbacks() --- <exception caught here> --- File ""C:\Swat\.virtualenvs\twisted\lib\site-packages\twisted\internet\defer.py"", line 588, in _runCallbacks current.result = callback(current.result, *args, **kw) File ""testd.py"", line 9, in raiseErr raise Exception(err) exceptions.Exception: oh no",Twisted Deferred not displaying unhandled Exception without errback
reverse dataframe's rows order with pandas," How can I reverse the order of the rows in my pandas.dataframe?I've looked everywhere and the only thing people are talking about is sorting the columns, reversing the order of the columns...What I want is simple :If my DataFrame looks like this: I want it to become this: I know I can iterate over my data in reverse order but that's not what I want. <code>  A B C ------------------ LOVE IS ALL THAT MAT TERS A B C ------------------ THAT MAT TERS LOVE IS ALL",reverse dataframe's rows' order with pandas
Combining conda environment.yml with pip requrements.txt," I work with conda environments and need some pip packages as well, e.g. pre-compiled wheels from ~gohlke.At the moment I have two files: environment.yml for conda with: and requirements.txt for pip which can be used after activating above conda environment: Is there a possibility to combine them in one file (for conda)? <code>  # run: conda env create --file environment.ymlname: test-envdependencies:- python>=3.5- anaconda # run: pip install -i requirements.txtdocxgooeyhttp://www.lfd.uci.edu/~gohlke/pythonlibs/bofhrmxk/opencv_python-3.1.0-cp35-none-win_amd64.whl",Combining conda environment.yml with pip requirements.txt
Enum usage in python," I want to use enums in python like in code below (java). I am a greenhorn in Python. I have the following code in Java and want to replicate the functionality in Python: How can I enforce users to only provide an enum to a Python method?  <code>  class Direction { public enum Direction {LEFT, RIGHT, UP, DOWN} public void navigate(Direction direction) switch(direction){ case Direction.LEFT: System.out.print(""left""); break; case Direction.RIGHT: System.out.print(""right""); break; case Direction.UP: System.out.print(""up""); break; case Direction.DOWN: System.out.print(""down""); break; }}",Enums in Python: How to enforce in method arguments
Selenium Python Select the first item from a drop down by index is not working. Unbound method select_by_index," I am trying to click the first item from a drop down.I want to use it's index value because the value could be different each time.I only need to select the 1st item in the drop down for this particular test.I have tried Select.select_by_index(1)I am getting the error: My code snippet to call the drop down is: <code>  Traceback (most recent call last): File ""C:\Webdriver\ClearCore 501 Regression Test\ClearCore - Regression Test\TestCases\DataPreviewsPage_TestCase.py"", line 398, in test_a2_sort_data_preview_advanced data_previews_view_page.select_option_from_new_sort_drop_down() # Select the sort from the sort drop down to view the sorted fields File ""C:\Webdriver\ClearCore 501 Regression Test\ClearCore - Regression Test\Pages\data_previews_view.py"", line 144, in select_option_from_new_sort_drop_down Select.select_by_index(1) # select the 1st item from the sort drop downTypeError: unbound method select_by_index() must be called with Select instance as first argument (got int instance instead) def select_option_from_new_sort_drop_down(self): # When sort is ready, select the 1st value from the drop to run the sort select = Select(WebDriverWait(self.driver, 20).until(EC.element_to_be_clickable((By.XPATH, '//option[contains(., ""(A-Z)"")]')))) Select.select_by_index(1) # select the 1st item from the sort drop down",Select the first item from a drop down by index is not working. Unbound method select_by_index
TensorFlow in production - how to use?, What is the right way to use TensorFlow for real time predictions in a high traffic application.Ideally I would have a server/cluster running tensorflow listening on a port(s) where I can connect from app servers and get predictions similar to the way databases are used.Training should be done by cron jobs feeding the training data through the network to the same server/cluster.How does one actually use tensorflow in production? Should I build a setup where the python is running as a server and use the python scripts to get predictions? I'm still new to this but I feel that such script will need to open sessions etc.. which is not scalable. (I'm talking about 100s of predictions/sec).Any pointer to relevant information will be highly appreciated. I could not find any. <code> ,TensorFlow in production for real time predictions in high traffic app - how to use?
how can I handle a list of integers in the url 's route in a flask application?," I'm trying to implement a basic calculator in Flask. I define two url parameters, which is manageable when I only want to add two values. However, I want to add any number of values. How can I get a list of integers without writing an infinitely long route? I tried to solve my problem with this code, but it's not working <code>  @app.route('/add/<int:n1>,<int:n2>')def add(n1,n2): sum = n1+n2 return ""%d"" % (sum) integer_list = [] @app.route('/add/integer_list') def fun (integer_list): sum = 0 for item in integer_list: sum = sum + item return '%d' % sum",Capture a list of integers with a Flask route
Python regex find all possible substrings begining with characters from capturing group," I have for example the string BANANA and want to find all possible substrings beginning with a vowel. The result I need looks like this: I tried this: re.findall(r""([AIEOU]+\w*)"", ""BANANA"")but it only finds ""ANANA"" which seems to be the longest match.How can I find all the other possible substrings? <code>  ""A"", ""A"", ""A"", ""AN"", ""AN"", ""ANA"", ""ANA"", ""ANAN"", ""ANANA""",Find all possible substrings beginning with characters from capturing group
Find all possible substrings begining with characters from capturing group," I have for example the string BANANA and want to find all possible substrings beginning with a vowel. The result I need looks like this: I tried this: re.findall(r""([AIEOU]+\w*)"", ""BANANA"")but it only finds ""ANANA"" which seems to be the longest match.How can I find all the other possible substrings? <code>  ""A"", ""A"", ""A"", ""AN"", ""AN"", ""ANA"", ""ANA"", ""ANAN"", ""ANANA""",Find all possible substrings beginning with characters from capturing group
matplotlib contour ClabelText clabel," I'm using Python Matplotlib to plot contours. Here's some code I have below as a basis. If you run this, you'll see that the labels are almost at vertical. I'd like to get the labels orientated horizontal, but I have no idea how can achieve this. I've tried with ClabelText, which the documentation suggests, but don't understand how this is supposed to work. I'd appreciate if someone could suggest a way to orientate the labels, either with or without ClabelText. <code>  import itertools as itimport numpy as npfrom matplotlib.ticker import FuncFormatterfrom matplotlib.contour import ClabelTextimport matplotlib.pyplot as pltfrom math import pi, log def getTime(data): M = data['weight'] Tei = data['temp'] Twasser = 99.8 Teikl = 86.0 ## max allowed temp k = 0.262 ## estimate was 0.3 W/(m.K), Crho = 3.18 # (KJ/kgC) const = pow(Crho, 1.0/3) / (pi*pi*k*pow(4*pi/3,2.0/3)) Tval = const*pow(M,2.0/3)*log(0.76*(Tei-Twasser)/(Teikl-Twasser)) return Tval # coo time in minutesdef contourFmt(val, posn): mins = int(val // 1) secs = int(val % 1 *60) return '{0:d}mm{1:d}ss'.format(mins, secs)def labeler(val): #is this any use?? print(val) return#weights = np.array(range(40, 80, 5))*1.0#temps = np.array(range(0, 30, 5))*1.0weights = np.arange(40.0, 80.0, 5.0)temps = np.arange(0.0, 25.01, 5.0)X = tempsY = weightsZ = np.zeros((len(X), len(Y))) xx = [{'temp':i} for i in X]yy = [{'weight':i} for i in Y]plt.figure()##zz = it.product(xx,yy)for i, xdicts in enumerate(xx): for j, ydicts in enumerate(yy): zd = {} zd.update(xdicts) zd.update(ydicts) zval = getTime(zd) Z[i,j] = zvaltimes = np.arange(4.00, 6.50, 0.25)CS = plt.contour(Y, X, Z, levels=times, colors='b')lbl = ClabelText(labeler)lbl.set_rotation('horizontal')formatter = FuncFormatter(contourFmt) #plt.clabel(CS, inline=True, fmt=formatter, fontsize=12)plt.clabel(CS, inline=True, use_clabeltext=True, fmt=formatter, fontsize=12)plt.grid(True)plt.clabel(CS, inline=1, fontsize=12)plt.show()",change orientation of contour clabel text objects
upgrading pip error UnicodeDecodeError: 'utf-8' codec can't decode byte ???? in position #####: invalid continuation byte," I've just installed python on windows 10, and I'm trying to upgrade pip. My windows user name has hebrew charecters...When I try to run: I get this error: I'm geussing this has to do with my Hebrew windows user name, Is that correct? Can I upgrade pip without opening a new windows user? <code>  python -m pip install --upgrade pip Collecting pipUsing cached pip-8.0.2-py2.py3-none-any.whlInstalling collected packages: pip Found existing installation: pip 7.1.2Exception:Traceback (most recent call last): File ""C:\Users\\AppData\Local\Programs\Python\Python35-32\lib\site-packages\pip\basecommand.py"", line 211, in main status = self.run(options, args) File ""C:\Users\\AppData\Local\Programs\Python\Python35-32\lib\site-packages\pip\commands\install.py"", line 311, in run root=options.root_path, File ""C:\Users\\AppData\Local\Programs\Python\Python35-32\lib\site-packages\pip\req\req_set.py"", line 640, in install requirement.uninstall(auto_confirm=True) File ""C:\Users\\AppData\Local\Programs\Python\Python35-32\lib\site-packages\pip\req\req_install.py"", line 673, in uninstall for path in pip.wheel.uninstallation_paths(dist): File ""C:\Users\\AppData\Local\Programs\Python\Python35-32\lib\site-packages\pip\wheel.py"", line 512, in unique for item in fn(*args, **kw): File ""C:\Users\\AppData\Local\Programs\Python\Python35-32\lib\site-packages\pip\wheel.py"", line 531, in uninstallation_paths r = csv.reader(FakeFile(dist.get_metadata_lines('RECORD'))) File ""C:\Users\\AppData\Local\Programs\Python\Python35-32\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 1619, in get_metadata_lines return yield_lines(self.get_metadata(name)) File ""C:\Users\\AppData\Local\Programs\Python\Python35-32\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 1616, in get_metadata return self._get(self._fn(self.egg_info, name)).decode(""utf-8"")UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf2 in position 22365: invalid continuation byteYou are using pip version 7.1.2, however version 8.0.2 is available.You should consider upgrading via the 'python -m pip install --upgrade pip' command.",Error while upgrading pip: UnicodeDecodeError: 'utf-8' codec can't decode byte
Error while upgrading pip: UnicodeDecodeError: 'utf-8' codec can't decode byte ???? in position #####," I've just installed python on windows 10, and I'm trying to upgrade pip. My windows user name has hebrew charecters...When I try to run: I get this error: I'm geussing this has to do with my Hebrew windows user name, Is that correct? Can I upgrade pip without opening a new windows user? <code>  python -m pip install --upgrade pip Collecting pipUsing cached pip-8.0.2-py2.py3-none-any.whlInstalling collected packages: pip Found existing installation: pip 7.1.2Exception:Traceback (most recent call last): File ""C:\Users\\AppData\Local\Programs\Python\Python35-32\lib\site-packages\pip\basecommand.py"", line 211, in main status = self.run(options, args) File ""C:\Users\\AppData\Local\Programs\Python\Python35-32\lib\site-packages\pip\commands\install.py"", line 311, in run root=options.root_path, File ""C:\Users\\AppData\Local\Programs\Python\Python35-32\lib\site-packages\pip\req\req_set.py"", line 640, in install requirement.uninstall(auto_confirm=True) File ""C:\Users\\AppData\Local\Programs\Python\Python35-32\lib\site-packages\pip\req\req_install.py"", line 673, in uninstall for path in pip.wheel.uninstallation_paths(dist): File ""C:\Users\\AppData\Local\Programs\Python\Python35-32\lib\site-packages\pip\wheel.py"", line 512, in unique for item in fn(*args, **kw): File ""C:\Users\\AppData\Local\Programs\Python\Python35-32\lib\site-packages\pip\wheel.py"", line 531, in uninstallation_paths r = csv.reader(FakeFile(dist.get_metadata_lines('RECORD'))) File ""C:\Users\\AppData\Local\Programs\Python\Python35-32\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 1619, in get_metadata_lines return yield_lines(self.get_metadata(name)) File ""C:\Users\\AppData\Local\Programs\Python\Python35-32\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 1616, in get_metadata return self._get(self._fn(self.egg_info, name)).decode(""utf-8"")UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf2 in position 22365: invalid continuation byteYou are using pip version 7.1.2, however version 8.0.2 is available.You should consider upgrading via the 'python -m pip install --upgrade pip' command.",Error while upgrading pip: UnicodeDecodeError: 'utf-8' codec can't decode byte
Running .ipynb from terminal?," I have some code in a .ipynb file and got it to the point where I don't really need the ""interactive"" feature of IPython Notebook. I would like to just run it straight from a Mac Terminal Command Line.Basically, if this were just a .py file, I believe I could just do python filename.py from the command line. Is there something similar for a .ipynb file? <code> ",How to run an .ipynb Jupyter Notebook from terminal?
How do I remove the brackets from the result," How do I remove the brackets from the result while keeping the function a single line of code? Output: <code>  day_list = [""Sunday"", ""Monday"", ""Tuesday"", ""Wednesday"", ""Thursday"", ""Friday"", ""Saturday""]def day_to_number(inp): return [day for day in range(len(day_list)) if day_list[day] == inp]print day_to_number(""Sunday"")print day_to_number(""Monday"")print day_to_number(""Tuesday"")print day_to_number(""Wednesday"")print day_to_number(""Thursday"")print day_to_number(""Friday"")print day_to_number(""Saturday"") [0][1][2][3][4][5][6]","My function returns a list with a single integer in it, how can I make it return only the integer?"
"How does ""all"" function in python works?"," I searched for understanding about the all function in Python, and I found this, according to here: all will return True only when all the elements are Truthy.But when I work with this function it's acting differently: Why is it that when all elements in input are False it returns True? Did I misunderstand its functionality or is there an explanation? <code>  '?' == True # False'!' == True # Falseall(['?','!']) # True","How does the ""all"" function in Python work?"
Distinction between str and unicode," After two questions regarding the distinction between the datatypes str and unicode, I'm still puzzled at the following.In Block 1 we see that the type of the city is unicode, as we're expecting.Yet in Block 2, after a round-trip through disk (redis), the type of the city is str (and the representation is different).The dogma of storing utf-8 on disk, reading into unicode, and writing back in utf-8 is failing somewhere.Why is the second instance of type(city) str rather than unicode?Just as importantly, does it matter? Do you care whether your variables are unicode or str, or are you oblivious to the difference just so long as the code ""does the right thing""? <code>  # -*- coding: utf-8 -*-# Block 1city = u'Dsseldorf'print city, type(city), repr(city)# Dsseldorf <type 'unicode'> u'D\xfcsseldorf'# Block 2import redisr_server = redis.Redis('localhost')r_server.set('city', city)city = r_server.get('city')print city, type(city), repr(city)# Dsseldorf <type 'str'> 'D\xc3\xbcsseldorf'",Distinction between str and unicode: why does Redis return binary data when passed unicode?
Distinction between str and unicode: why Redis returns binary data when passed unicode?," After two questions regarding the distinction between the datatypes str and unicode, I'm still puzzled at the following.In Block 1 we see that the type of the city is unicode, as we're expecting.Yet in Block 2, after a round-trip through disk (redis), the type of the city is str (and the representation is different).The dogma of storing utf-8 on disk, reading into unicode, and writing back in utf-8 is failing somewhere.Why is the second instance of type(city) str rather than unicode?Just as importantly, does it matter? Do you care whether your variables are unicode or str, or are you oblivious to the difference just so long as the code ""does the right thing""? <code>  # -*- coding: utf-8 -*-# Block 1city = u'Dsseldorf'print city, type(city), repr(city)# Dsseldorf <type 'unicode'> u'D\xfcsseldorf'# Block 2import redisr_server = redis.Redis('localhost')r_server.set('city', city)city = r_server.get('city')print city, type(city), repr(city)# Dsseldorf <type 'str'> 'D\xc3\xbcsseldorf'",Distinction between str and unicode: why does Redis return binary data when passed unicode?
Pandas cut method exludes lower bound," I am trying to bin a dataframe column that contains ages in the range 0 to 100.When I try and use a bin to include the zero ages it does not work.Here is an demo using a list with the range of my data: The zero value in the range returns NaN from the cut.Any way around this? <code>  pd.cut(pd.Series(range(101)), [0, 24, 49, 74, 100])",Pandas cut method excludes lower bound
"New to Python, replacing characters in a string"," I'm trying to manipulate a string. After extracting all the vowels from a string, I want to replace all the 'v' with 'b' and all the 'b' with 'v' from the same string (i.g. ""accveioub"" would become ccvb first, then ccbv).I'm having problem to swap the characters. I end up getting ccvv and I figured I'd get that based of this code. I'm thinking of iterating through the string and using an if statement basically staying if the character at index i .equals""v"" then replace it with ""b"" and an else statement that says ""b"" replace it with ""v"" and then append or join the characters together?Here's my code <code>  def Problem4(): volString = {""a"", ""e"", ""i"", ""o"", ""u"", ""A"", ""E"", ""I"", ""O"", ""U""} s = ""accveioub"" chars = [] index = 0 #Removes all the vowels with the for loop for i in s: if i not in volString: chars.append(i) s2 = """".join(chars) print(s2) print(s2.replace(""v"", ""b"")) print(s2.replace(""b"", ""v""))>>> Problem4()ccvbccbbccvv>>> ",Changing multiple characters by other characters in a string
Possible bug in numpy /= operator," So, I created an vertical numpy array, used the /= operator and the output seems to be incorrect. Basically if x is a vector, s a scalar. I would expect x /= s have every entry of x divided by s. However, I couldn't make much sense of the output. The operator is only applied on part of the entries in x, and I am not sure how they are chosen. <code>  In [8]: np.__version__Out[8]: '1.10.4'In [9]: x = np.random.rand(5,1)In [10]: xOut[10]:array([[ 0.47577008], [ 0.66127875], [ 0.49337183], [ 0.47195985], [ 0.82384023]]) ####In [11]: x /= x[2]In [12]: xOut[12]:array([[ 0.96432356], [ 1.3403253 ], [ 1. ], [ 0.95660073], [ 0.82384023]]) #### this entry is not changed.",numpy: unexpected result when dividing a vertical array by one of its own elements
Retrieving multiple urls with aiohttp in Python 3.5," Since Python 3.5 introduced async with the syntax recommended in the docs for aiohttp has changed. Now to get a single url they suggest: How can I modify this to fetch a collection of urls instead of just one url?In the old asyncio examples you would set up a list of tasks such as I tried to combine a list like this with the approach above but failed. <code>  import aiohttpimport asyncioasync def fetch(session, url): with aiohttp.Timeout(10): async with session.get(url) as response: return await response.text()if __name__ == '__main__': loop = asyncio.get_event_loop() with aiohttp.ClientSession(loop=loop) as session: html = loop.run_until_complete( fetch(session, 'http://python.org')) print(html) tasks = [ fetch(session, 'http://cnn.com'), fetch(session, 'http://google.com'), fetch(session, 'http://twitter.com') ]",Fetching multiple urls with aiohttp in Python 3.5
Scraping 101 with aiohttp in Python 3.5," Since Python 3.5 introduced async with the syntax recommended in the docs for aiohttp has changed. Now to get a single url they suggest: How can I modify this to fetch a collection of urls instead of just one url?In the old asyncio examples you would set up a list of tasks such as I tried to combine a list like this with the approach above but failed. <code>  import aiohttpimport asyncioasync def fetch(session, url): with aiohttp.Timeout(10): async with session.get(url) as response: return await response.text()if __name__ == '__main__': loop = asyncio.get_event_loop() with aiohttp.ClientSession(loop=loop) as session: html = loop.run_until_complete( fetch(session, 'http://python.org')) print(html) tasks = [ fetch(session, 'http://cnn.com'), fetch(session, 'http://google.com'), fetch(session, 'http://twitter.com') ]",Fetching multiple urls with aiohttp in Python 3.5
Spark Convert Data Frame Column to Vectors.dense," I am very new to Spark and I am trying to apply StandardScaler() to a column in a DataFrame. The problem is that applying it like this, gives me an error: requirement failed: Input column DF_column must be a vector column. I tried using UDF but still doesn't work. I did the example of the LIBSVM but that is easy a the TXT file is loading features as Vectors. <code>  +---------------+| DF_column|+---------------+| 0.114285714286|| 0.115702479339|| 0.267893660532||0.0730337078652|| 0.124309392265|| 0.365714285714|| 0.111747851003|| 0.279538904899|| 0.134670487106|| 0.523287671233|| 0.404011461318|| 0.375|| 0.125517241379||0.0143266475645|| 0.313684210526|| 0.381088825215|| 0.411428571429|| 0.327683615819|| 0.153409090909|| 0.344827586207|+---------------+ scaler = StandardScaler(inputCol='DF_column', outputCol=""scaledFeatures"",withStd=True, withMean=False)","Spark Convert Data Frame Column to dense Vector for StandardScaler() ""Column must be of type org.apache.spark.ml.linalg.VectorUDT"""
Is there any way to use variables in multiline comments in Python?," So I have this as part of a mail sending script: ... And I'd like to use different subjects each time (let's say it's the function argument).I know there are several ways to do this.However, I am also using ProbLog for some of my other scripts (a probabilistic programming language based in Prolog syntax).As far as I know, the only way to use ProbLog in Python is through strings, and if the string is broke in several parts; example = (""""""string"""""", variable, """"""string2""""""), as well as in the email example above, there's no way I can make it work.I actually have a few more scripts where using variables in multiline strings could be useful, but you get the idea.Is there any way to make this work?Thanks in advance! <code>  try: content = (""""""From: Fromname <fromemail> To: Toname <toemail> MIME-Version: 1.0 Content-type: text/html Subject: test This is an e-mail message to be sent in HTML format <b>This is HTML message.</b> <h1>This is headline.</h1> """""") mail.sendmail('from', 'to', content)",Is there any way to use variables in multiline string in Python?
Simple Python string slicing," Yeah I know there are a lot of similar questions up there. But I just cannot find what I was looking for.My confusion is about the backward slicing. Now I have found that the result will be So I thought that maybe I will test it out by changing the ends in that string slicing. I was really sure that Python would give me something like Instead it gave me this which made me totally lost... Can someone explain what is going on here? I am new to Python and find this very confusing.. Thanks for any help. <code>  my_jumble = ['jumbly', 'wumbly', 'number', 5]print(my_jumble[:1:-1]) [5, 'number'] print(my_jumble[:2:-1]) [5, 'number', 'wumbly'] [5]",Simple Python String (Backward) Slicing
return type of classmethod," I am playing around with python's 3.5 type hints. I was wondering how can I type hint the return type of a class method.This is what I had in mind: Clearly it does not work. I guess one way would be to do some sort of ""forward declaration"" like so: But that does not feel ""pythonic"". How do people normally solve this? <code>  >>> class A(): @classmethod def a(cls) -> A: passTraceback (most recent call last): File ""<pyshell#24>"", line 1, in <module> class A(): File ""<pyshell#24>"", line 3, in A def a(cls) -> A:NameError: name 'A' is not defined >>> class A(): pass>>> class A(): @classmethod def a(cls) -> A: pass",How to make a type hint forward reference
Convert PyUnicode String to Char * in C," I have a PyUnicode object I'm trying to convert back to a C string (char *).The way I am trying to do it does not seem to be working. Here is my code: Is there another/better way I should be doing this? <code>  PyObject * objectCompName = PyTuple_GET_ITEM(compTuple, (Py_ssize_t) 0);PyObject * ooCompName = PyUnicode_AsASCIIString(objectCompName);char * compName = PyBytes_AsString(ooCompName);Py_DECREF(ooCompName);",How Does String Conversion Between PyUnicode String and C String Work?
Efficiant way to swap bytes in python," I have some bytearray with length of 2*n: I need to switch bytes endian in each 2-byte word, and make: Now I use next approach but it is very slow for my task: Is it possible to switch endian of bytearray by calling some system/libc function?Ok, thanks to all, I timed some suggestions: and result is: So in-pace slice assignment with a step of 2 now is fastest. Also thanks to Mr. F for detailed explaining but I not yet tried it because of numpy <code>  a1 a2 b1 b2 c1 c2 a2 a1 b2 b1 c2 c1 converted = bytearray([])for i in range(int(len(chunk)/2)): converted += bytearray([ chunk[i*2+1], chunk[i*2] ]) import timeittest = [""""""converted = bytearray([])for i in range(int(len(chunk)/2)): converted += bytearray([ chunk[i*2+1], chunk[i*2] ])"""""",""""""for i in range(0, len(chunk), 2): chunk[i], chunk[i+1] = chunk[i+1], chunk[i]"""""",""""""byteswapped = bytearray([0]) * len(chunk)byteswapped[0::2] = chunk[1::2]byteswapped[1::2] = chunk[0::2]"""""",""""""chunk[0::2], chunk[1::2] = chunk[1::2], chunk[0::2]""""""]for t in test: print(timeit.timeit(t, setup='chunk = bytearray([1]*10)')) $ python ti.py11.62197613722.618831872943.471940994261.66421198845",Efficient way to swap bytes in python
Django 1.9 - makemigrations - No changes detected," I was trying to create migrations within an existing app using the makemigrations command but it outputs ""No changes detected"".Usually I create new apps using the startapp command but did not use it for this app when I created it.After debugging, I found that it is not creating migration because the migrations package/folder is missing from an app. Would it be better if it creates the folder if it is not there or am I missing something? <code> ",Django - makemigrations - No changes detected
finding max ocurrance of a columns value for groupby on another column, I have a pandas data-frame: I want to find id-wise the maximum occurring city name. So that for a given id I can tell that - this is his favorite city: Using groupby id and city gives: How to proceed further? I believe some group-by apply will do that but unaware of what exactly will do the trick. So please suggest.If some id has same count for two or three cities I am ok with returning any of those cities. <code>  id city 000.tushar@gmail.com Bangalore 00078r@gmail.com Mumbai0007ayan@gmail.com Jamshedpur0007ayan@gmail.com Jamshedpur000.tushar@gmail.com Bangalore 00078r@gmail.com Mumbai 00078r@gmail.com Vijayawada 00078r@gmail.com Vijayawada 00078r@gmail.com Vijayawada id city000.tushar@gmail.com Bangalore00078r@gmail.com Vijayawada0007ayan@gmail.com Jamshedpur id city count0 000.tushar@gmail.com Bangalore 21 00078r@gmail.com Mumbai 22 00078r@gmail.com Vijayawada 33 0007ayan@gmail.com Jamshedpur 2,"Finding max occurrence of a column's value, after group-by on another column"
"Reading a list stored in a text file, Python"," I have a file whose content is in the form of a python list such as the following: Is there any way to read the python file back into a list object? instead of using .read() and having the whole file just read as a string.EDIT: for those who may be interested i ran into a strange issue using (import ast) as suggested as a solution for the above problem. the program i used it in has a function which fetches historical stock data from the yahoo finance python module. this function is in no way related or dependent on the function which used ast.literal_eval(). anyways every night after market close i collect new batches of historical data from yahoo finance and last night i ran into an error : simplejson.scanner.jsondecodeerror expecting value. it was strange because it would collect data just fine for some companies but throw the error for others, and sometime work for the same company but a minute later it would not work. after trying all kinds of things to debug and solve the issue remembered that the import ast was recently added and thought i should try to see if it could have an effect, after removing the import ast the program went back to workin as it normally did. does anybody know why import ast caused issues? @Apero why did you initially warn against using eval or ast.literal_eval? <code>  ['hello','how','are','you','doing','today','2016','10.004']",Reading a list stored in a text file
"Reading a list stored in a text file, Python,"," I have a file whose content is in the form of a python list such as the following: Is there any way to read the python file back into a list object? instead of using .read() and having the whole file just read as a string.EDIT: for those who may be interested i ran into a strange issue using (import ast) as suggested as a solution for the above problem. the program i used it in has a function which fetches historical stock data from the yahoo finance python module. this function is in no way related or dependent on the function which used ast.literal_eval(). anyways every night after market close i collect new batches of historical data from yahoo finance and last night i ran into an error : simplejson.scanner.jsondecodeerror expecting value. it was strange because it would collect data just fine for some companies but throw the error for others, and sometime work for the same company but a minute later it would not work. after trying all kinds of things to debug and solve the issue remembered that the import ast was recently added and thought i should try to see if it could have an effect, after removing the import ast the program went back to workin as it normally did. does anybody know why import ast caused issues? @Apero why did you initially warn against using eval or ast.literal_eval? <code>  ['hello','how','are','you','doing','today','2016','10.004']",Reading a list stored in a text file
Inserting data into table using previous query SQLAlchemy," I have two identical tables post and old_post. I have a query that checks for old posts. I would like to move the rows returned by the query into the table old_post and delete the rows from table post.I could solve this by iterating through the results returned by the initial query and update my results this way, however I am worried this is very inefficient and will start to cause me problems when I have 1,000+ rows. How can I efficiently ""bulk move"" rows from one table to another? <code> ",Bulk move rows from one table to another with SQLAlchemy
Convert triangle matrix to square (Python | NumPy)?," I'm doing some computations on a full matrix that is redundant (i.e. can be a triangle matrix without losing info). I realized I can compute only the lower portion of the triangle for faster results. How can I project the lower triangle into the upper once I'm done?In other words, how can I reverse the np.tril method? How to convert it back to a full matrix? <code>  print DF_var.as_matrix()# [[1 1 0 1 1 1 0 1 0 0 0]# [1 1 1 1 1 0 1 0 1 1 1]# [0 1 1 0 0 0 0 0 0 0 0]# [1 1 0 1 0 0 0 0 0 0 0]# [1 1 0 0 1 0 0 0 0 0 0]# [1 0 0 0 0 1 1 0 0 0 0]# [0 1 0 0 0 1 1 0 0 0 0]# [1 0 0 0 0 0 0 1 1 0 0]# [0 1 0 0 0 0 0 1 1 0 0]# [0 1 0 0 0 0 0 0 0 1 0]# [0 1 0 0 0 0 0 0 0 0 1]]print np.tril(DF_var.as_matrix())# [[1 0 0 0 0 0 0 0 0 0 0]# [1 1 0 0 0 0 0 0 0 0 0]# [0 1 1 0 0 0 0 0 0 0 0]# [1 1 0 1 0 0 0 0 0 0 0]# [1 1 0 0 1 0 0 0 0 0 0]# [1 0 0 0 0 1 0 0 0 0 0]# [0 1 0 0 0 1 1 0 0 0 0]# [1 0 0 0 0 0 0 1 0 0 0]# [0 1 0 0 0 0 0 1 1 0 0]# [0 1 0 0 0 0 0 0 0 1 0]# [0 1 0 0 0 0 0 0 0 0 1]]",How to convert triangle matrix to square in NumPy?
Python permutation of two strings," I am trying to generate all possible ways to interleave any two arbitrary strings in Python.For example: If the two strings are 'ab' and 'cd', the output I wish to get is: See a is always before b (and c before d). I am struggling to find a solution to this. I have tried itertools as shown below: But as expected, this returns all possible permutations disregarding order of a and b (and c and d). <code>  ['abcd', 'acbd', 'acdb', 'cabd', 'cadb', 'cdab'] import itertoolsdef shuffle(s,t): string = s+t for i in itertools.permutations(string): print(''.join(i))shuffle('ab','cd')",All possible ways to interleave two strings
Permutation of two strings," I am trying to generate all possible ways to interleave any two arbitrary strings in Python.For example: If the two strings are 'ab' and 'cd', the output I wish to get is: See a is always before b (and c before d). I am struggling to find a solution to this. I have tried itertools as shown below: But as expected, this returns all possible permutations disregarding order of a and b (and c and d). <code>  ['abcd', 'acbd', 'acdb', 'cabd', 'cadb', 'cdab'] import itertoolsdef shuffle(s,t): string = s+t for i in itertools.permutations(string): print(''.join(i))shuffle('ab','cd')",All possible ways to interleave two strings
All Possible Ways to Interleave 2 Strings," I am trying to generate all possible ways to interleave any two arbitrary strings in Python.For example: If the two strings are 'ab' and 'cd', the output I wish to get is: See a is always before b (and c before d). I am struggling to find a solution to this. I have tried itertools as shown below: But as expected, this returns all possible permutations disregarding order of a and b (and c and d). <code>  ['abcd', 'acbd', 'acdb', 'cabd', 'cadb', 'cdab'] import itertoolsdef shuffle(s,t): string = s+t for i in itertools.permutations(string): print(''.join(i))shuffle('ab','cd')",All possible ways to interleave two strings
scraping google analytics with scrapy," I have been trying to use Scrapy to get some data from Google Analytics and despite the fact that I'm a complete Python newbie I have made some progress. I can now login to Google Analytics by Scrapy but I need to make an AJAX request to get the data what I want. I have tried to replicate my browser's HTTP request header with the code below but it doesn't seem to work, my error log says too many values to unpackCould somebody help? I've been worked on it for two days, I have the feeling that I'm very close but I'm also very confused.Here is the code: And here is part of the log: <code>  from scrapy.spider import BaseSpiderfrom scrapy.selector import HtmlXPathSelectorfrom scrapy.http import FormRequest, Requestfrom scrapy.selector import Selectorimport loggingfrom super.items import SuperItemfrom scrapy.shell import inspect_responseimport jsonclass LoginSpider(BaseSpider): name = 'super' start_urls = ['https://accounts.google.com/ServiceLogin?service=analytics&passive=true&nui=1&hl=fr&continue=https%3A%2F%2Fwww.google.com%2Fanalytics%2Fweb%2F%3Fhl%3Dfr&followup=https%3A%2F%2Fwww.google.com%2Fanalytics%2Fweb%2F%3Fhl%3Dfr#identifier'] def parse(self, response): return [FormRequest.from_response(response, formdata={'Email': 'Email'}, callback=self.log_password)] def log_password(self, response): return [FormRequest.from_response(response, formdata={'Passwd': 'Password'}, callback=self.after_login)] def after_login(self, response): if ""authentication failed"" in response.body: self.log(""Login failed"", level=logging.ERROR) return # We've successfully authenticated, let's have some fun! else: print(""Login Successful!!"") return Request(url=""https://analytics.google.com/analytics/web/getPage?id=trafficsources-all-traffic&ds=a5425w87291514p94531107&hl=fr&authuser=0"", method='POST', headers=[{'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8', 'Galaxy-Ajax': 'true', 'Origin': 'https://analytics.google.com', 'Referer': 'https://analytics.google.com/analytics/web/?hl=fr&pli=1', 'User-Agent': 'My-user-agent', 'X-GAFE4-XSRF-TOKEN': 'Mytoken'}], callback=self.parse_tastypage, dont_filter=True) def parse_tastypage(self, response): response = json.loads(jsonResponse) inspect_response(response, self) yield item 2016-03-28 19:11:39 [scrapy] INFO: Enabled item pipelines:[]2016-03-28 19:11:39 [scrapy] INFO: Spider opened2016-03-28 19:11:39 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)2016-03-28 19:11:39 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:60232016-03-28 19:11:40 [scrapy] DEBUG: Crawled (200) <GET https://accounts.google.com/ServiceLogin?service=analytics&passive=true&nui=1&hl=fr&continue=https%3A%2F%2Fwww.google.com%2Fanalytics%2Fweb%2F%3Fhl%3Dfr&followup=https%3A%2F%2Fwww.google.com%2Fanalytics%2Fweb%2F%3Fhl%3Dfr#identifier> (referer: None)2016-03-28 19:11:46 [scrapy] DEBUG: Crawled (200) <POST https://accounts.google.com/AccountLoginInfo> (referer: https://accounts.google.com/ServiceLogin?service=analytics&passive=true&nui=1&hl=fr&continue=https%3A%2F%2Fwww.google.com%2Fanalytics%2Fweb%2F%3Fhl%3Dfr&followup=https%3A%2F%2Fwww.google.com%2Fanalytics%2Fweb%2F%3Fhl%3Dfr)2016-03-28 19:11:50 [scrapy] DEBUG: Redirecting (302) to <GET https://accounts.google.com/CheckCookie?hl=fr&checkedDomains=youtube&pstMsg=0&chtml=LoginDoneHtml&service=analytics&continue=https%3A%2F%2Fwww.google.com%2Fanalytics%2Fweb%2F%3Fhl%3Dfr&gidl=CAA> from <POST https://accounts.google.com/ServiceLoginAuth>2016-03-28 19:11:57 [scrapy] DEBUG: Redirecting (302) to <GET https://www.google.com/analytics/web/?hl=fr> from <GET https://accounts.google.com/CheckCookie?hl=fr&checkedDomains=youtube&pstMsg=0&chtml=LoginDoneHtml&service=analytics&continue=https%3A%2F%2Fwww.google.com%2Fanalytics%2Fweb%2F%3Fhl%3Dfr&gidl=CAA>2016-03-28 19:12:01 [scrapy] DEBUG: Crawled (200) <GET https://www.google.com/analytics/web/?hl=fr> (referer: https://accounts.google.com/AccountLoginInfo)Login Successful!!2016-03-28 19:12:01 [scrapy] ERROR: Spider error processing <GET https://www.google.com/analytics/web/?hl=fr> (referer: https://accounts.google.com/AccountLoginInfo)Traceback (most recent call last): File ""/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/twisted/internet/defer.py"", line 577, in _runCallbacks current.result = callback(current.result, *args, **kw) File ""/Users/aminbouraiss/super/super/spiders/mySuper.py"", line 42, in after_login callback=self.parse_tastypage, dont_filter=True) File ""/Library/Python/2.7/site-packages/Scrapy-1.1.0rc3-py2.7.egg/scrapy/http/request/__init__.py"", line 35, in __init__ self.headers = Headers(headers or {}, encoding=encoding) File ""/Library/Python/2.7/site-packages/Scrapy-1.1.0rc3-py2.7.egg/scrapy/http/headers.py"", line 12, in __init__ super(Headers, self).__init__(seq) File ""/Library/Python/2.7/site-packages/Scrapy-1.1.0rc3-py2.7.egg/scrapy/utils/datatypes.py"", line 193, in __init__ self.update(seq) File ""/Library/Python/2.7/site-packages/Scrapy-1.1.0rc3-py2.7.egg/scrapy/utils/datatypes.py"", line 229, in update super(CaselessDict, self).update(iseq) File ""/Library/Python/2.7/site-packages/Scrapy-1.1.0rc3-py2.7.egg/scrapy/utils/datatypes.py"", line 228, in <genexpr> iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)ValueError: too many values to unpack2016-03-28 19:12:01 [scrapy] INFO: Closing spider (finished)2016-03-28 19:12:01 [scrapy] INFO: Dumping Scrapy stats:{'downloader/request_bytes': 6419, 'downloader/request_count': 5, 'downloader/request_method_count/GET': 3, 'downloader/request_method_count/POST': 2, 'downloader/response_bytes': 75986, 'downloader/response_count': 5, 'downloader/response_status_count/200': 3, 'downloader/response_status_count/302': 2, 'finish_reason': 'finished', 'finish_time': datetime.datetime(2016, 3, 28, 23, 12, 1, 824033), 'log_count/DEBUG': 6,",Scraping Google Analytics by Scrapy
Pythoncially check if a variable name is valid," tldr; see the final line; the rest is just preamble. I am developing a test harness, which parses user scripts and generates a Python script which it then runs. The idea is for non-techie folks to be able to write high-level test scripts.I have introduced the idea of variables, so a user can use the LET keyword in his script. E.g. LET X = 42, which I simply expand to X = 42. They can then use X later in their scripts - RELEASE CONNECTION XBut what if someone writes LET 2 = 3? That's going to generate invalid Python. If I have that X in a variable variableName, then how can I check whether variableName is a valid Python variable? <code> ",Pythonically check if a variable name is valid
Calculate required equipment," I would like to visualize the number of required machines in a jobshop at a certain time in graph with on the x-axis a continuous time axis and on the y-axis the number of shifts. In the dataframe below, you find an example of my data. Here, you see Shift_IDs (which are unique) and the start and end time of that shift. Over a period of a day, I would like to see how many machines are needed at a certain interval. This can be 5 minutes, quarter of an hour, half an hour and hours. In this example in the quarter 9:30-9:45 I would need 3 machines to be able to do every shift at that specific time. The desired output would look something like this: With this data frame i could round it to the the lowest boundary of the interval and then plot it in a graph.I get stuck on how to ""see"" whether a shift lies within multiple intervals. Do you have any ideas how to tackle this?NB: All date-time values are of course datetime typeEDIT after Solution of MaxU and knightofniI used the code of MaxU to plot both your codes. They both seem to do it well on 15min but please take a look at you results with 5 minutes:MaxU:knightofni:EDIT 2 4 april 2015 <code>  df: Shift_ID Shift_Time_Start Shift_Time_End0 1 2016-03-22 9:00:00 2016-03-22 9:35:001 2 2016-03-22 9:20:00 2016-03-22 10:20:002 3 2016-03-22 9:40:00 2016-03-22 10:14:003 4 2016-03-22 10:00:00 2016-03-22 10:31:00 df2: Interval Count0 2016-03-22 9:00:00 - 2016-03-22 9:15:00 11 2016-03-22 9:15:00 - 2016-03-22 9:30:00 22 2016-03-22 9:30:00 - 2016-03-22 9:45:00 33 2016-03-22 9:45:00 - 2016-03-22 10:00:00 24 2016-03-22 10:00:00 - 2016-03-22 10:15:00 25 2016-03-22 10:15:00 - 2016-03-22 10:30:00 26 2016-03-22 10:30:00 - 2016-03-22 10:45:00 1",Calculate required equipment on shifts in timespans
Return the include and libs directories from within Python," Lets say I want to use gcc from the command line in order to compile a C extension of Python. I'd structure the call something like this: I noticed that the -I, -L, and -l options are absolutely necessary, or else you will get an error that looks something like this. These commands tell gcc where to look for the headers (-I), where to look for the static libraries (-L), and which static library to actually use (python35, which actually translates to libpython35.a).Now, this is obviously really easy to get the libs and include directories if its your machine, as they never change if you don't want them to. However, I was writing a program that calls gcc from the command line, that other people will be using. The line where this call occurs looks something like this: However, others will have different platforms and different Python installation structures, so just joining the paths won't always work.Instead, I need a solution from within Python that will reliably return the include and libs directories.Edit:I looked at the module distutils.ccompiler, and found many useful functions that would in part use distutils, but make it customizable for me to make my compiler entirely cross platform. The only thing is, I need to pass it the include and runtime libraries...Edit 2:I looked at distutils.sysconfig an I am able to reliably return the 'include' directory including all the header files. I still have no idea how to get the runtime library.The distutils.ccompiler docs are hereThe program that needs this functionality is named Cyther <code>  gcc -o applesauce.pyd -I C:/Python35/include -L C:/Python35/libs -l python35 applesauce.c from subprocess import callimport sysfilename = applesauce.cinclude_directory = os.path.join(sys.exec_prefix, 'include')libs_directory = os.path.join(sys.exec_prefix, 'libs')call(['gcc', ..., '-I', include_direcory, '-L', libs_directory, ...])",Return the include and runtime lib directories from within Python
Using Python to connect to MySQL with a MySQL option file with a password," I used mysql_config_editor to create a .mylogin.cnf file with a password. I know it worked correctly because I can use it to connect through both the command line utility mysql and the R package RMySQL without a problem.However, when trying to connect using Mysql-Connector/Python: or with PyMySQL: I get the same error: Skimming over the source code, it looks like these are trying to read the files in cleartext. However, mysql_config_editor encrypts the login file it generates. Both modules work fine when entering the password manually in the code.How can I connect to Python using one of these generated config files? I'm using Python 3, so MySQLdb isn't an option.update: for now, I'm using RPy2 to run queries in R and pipe the results back into Python. The code is a little ugly but the workflow isn't so bad. <code>  # using mysql-connector-python-rfimport osimport mysql.connectorcon = mysql.connector.connect(option_files=os.path.expanduser('~/.mylogin.cnf')) # using pymysqlimport osimport pymysqlcon = pymysql.connect(option_files=os.path.expanduser('~/.mylogin.cnf')) ---------------------------------------------------------------------------UnicodeDecodeError Traceback (most recent call last)<ipython-input-64-d17e56ef7010> in <module>()----> 1 con = mysql.connector.connect(option_files=os.path.expanduser('~/.mylogin.cnf'))/usr/local/lib/python3.5/site-packages/mysql/connector/__init__.py in connect(*args, **kwargs) 140 # Option files 141 if 'option_files' in kwargs:--> 142 new_config = read_option_files(**kwargs) 143 return connect(**new_config) 144 /usr/local/lib/python3.5/site-packages/mysql/connector/optionfiles.py in read_option_files(**config) 66 config['option_files'] = [config['option_files']] 67 option_parser = MySQLOptionsParser(list(config['option_files']),---> 68 keep_dashes=False) 69 del config['option_files'] 70 /usr/local/lib/python3.5/site-packages/mysql/connector/optionfiles.py in __init__(self, files, keep_dashes) 162 self.files = files 163 --> 164 self._parse_options(list(self.files)) 165 self._sections = self.get_groups_as_dict() 166 /usr/local/lib/python3.5/site-packages/mysql/connector/optionfiles.py in _parse_options(self, files) 193 ""than once in the list"".format(file_)) 194 with open(file_, 'r') as op_file:--> 195 for line in op_file.readlines(): 196 if line.startswith('!includedir'): 197 _, dir_path = line.split(None, 1)/usr/local/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/encodings/ascii.py in decode(self, input, final) 24 class IncrementalDecoder(codecs.IncrementalDecoder): 25 def decode(self, input, final=False):---> 26 return codecs.ascii_decode(input, self.errors)[0] 27 28 class StreamWriter(Codec,codecs.StreamWriter):UnicodeDecodeError: 'ascii' codec can't decode byte 0x96 in position 28: ordinal not in range(128)",Connecting Python to MySQL using an encrypted option file
Import python code and library in swift," I want write a Skype application for OS X in Swift, but that application needs the skype4com library. I find that library in Python, which works on OS X.Is it possible to import Python code into Swift? <code> ",Import Python code and library in Swift
SyntaxError: Non-ASCII character but no encoding declared - Python," I am suppose to change some of these characters in English to Chinese in this setting.py file but then error happened.I know that it's saying because no encoding is declared in the error message but I have been reading a few posts and still have no idea how / where I can make it happen.in my setting.py I have something like this but need to change the navigation to Chinese so ended up looking like this Edit, I already read the post which thought might be duplicated and tried what's in that post and instead of getting errors, I get a no show in my page but regarding to Pedru's answer. It works like a charm now. <code>  OSCAR_DASHBOARD_NAVIGATION = [{ 'label': _('dashboard'), 'icon': 'icon-th-list', 'url_name': 'dashboard:index',}, OSCAR_DASHBOARD_NAVIGATION = [{ 'label': _(''), 'icon': 'icon-th-list', 'url_name': 'dashboard:index',},",SyntaxError: Non-ASCII character but no encoding declared
Pyinstaller doesnt import Queue," I'm trying to compile some script with twisted and Queue. Unfortunately, produced file fails with ImportError: No module named queue. <code>  pyinstaller sample.py --onefile # -*- coding: utf-8 -*-#from twisted import *import queue as Queuea = Queue.Queue()",PyInstaller doesn't import Queue
Python - How to elegantly cycle a list from alternating sides?," Given a list how can I get That is, produce a new list in which each successive element is alternately taken from the two sides of the original list? <code>  a = [0,1,2,3,4,5,6,7,8,9] b = [0,9,1,8,2,7,3,6,4,5]",Cycle a list from alternating sides
How to elegantly cycle a list from alternating sides?," Given a list how can I get That is, produce a new list in which each successive element is alternately taken from the two sides of the original list? <code>  a = [0,1,2,3,4,5,6,7,8,9] b = [0,9,1,8,2,7,3,6,4,5]",Cycle a list from alternating sides
python vs perl: performance reading a gzipped file," I have a gzipped data file containing a million lines: My Perl script which processes this file is as follows: In Python: For whatever reason, the Python script takes almost ~8x longer: I tried profiling both scripts, but there really isn't much code to profile. The Python script spends most of its time on for line in fh; the Perl script spends most of its time in if($_ eq ""1000000"").Now, I know that Perl and Python have some expected differences. For instance, in Perl, I open up the filehandle using a subproc to UNIX gzip command; in Python, I use the gzip library.What can I do to speed up the Python implementation of this script (even if I never reach the Perl performance)? Perhaps the gzip module in Python is slow (or perhaps I'm using it in a bad way); is there a better solution?EDIT #1Here's what the read_million.py line-by-line profiling looks like. EDIT #2:I have now also tried subprocess python module as per @Kirk Strauser, and others. It is faster:Python ""subproc"" solution: Here is a comparative table of all the things I've tried so far: <code>  $ zcat million_lines.txt.gz | head12345678910... # read_million.pluse strict; my $file = ""million_lines.txt.gz"" ;open MILLION, ""gzip -cdfq $file |"";while ( <MILLION> ) { chomp $_; if ($_ eq ""1000000"" ) { print ""This is the millionth line: Perl\n""; last; }} # read_million.pyimport gzipfilename = 'million_lines.txt.gz'fh = gzip.open(filename)for line in fh: line = line.strip() if line == '1000000': print ""This is the millionth line: Python"" break $ time perl read_million.pl ; time python read_million.pyThis is the millionth line: Perlreal 0m0.329suser 0m0.165ssys 0m0.019sThis is the millionth line: Pythonreal 0m2.663suser 0m2.154ssys 0m0.074s Line # Hits Time Per Hit % Time Line Contents============================================================== 2 @profile 3 def main(): 4 5 1 1 1.0 0.0 filename = 'million_lines.txt.gz' 6 1 472 472.0 0.0 fh = gzip.open(filename) 7 1000000 5507042 5.5 84.3 for line in fh: 8 1000000 582653 0.6 8.9 line = line.strip() 9 1000000 443565 0.4 6.8 if line == '1000000': 10 1 25 25.0 0.0 print ""This is the millionth line: Python"" 11 1 0 0.0 0.0 break # read_million_subproc.py import subprocessfilename = 'million_lines.txt.gz'gzip = subprocess.Popen(['gzip', '-cdfq', filename], stdout=subprocess.PIPE)for line in gzip.stdout: line = line.strip() if line == '1000000': print ""This is the millionth line: Python"" breakgzip.wait() method average_running_time (s)--------------------------------------------------read_million.py 2.708read_million_subproc.py 0.850read_million.pl 0.393",Python vs Perl: performance reading a gzipped file
Python Seaborn Facegrid change xlabels, I just can't figure out how to change the xlabels in a Seaborn Facetgrid. It offers a method for changing the x labels with set_xlabels() but unfortunately not individually for each subplot.I have two subplots which share the y-axis but have a different x-axes and i want to label them with different texts.Can anybody give me a hint. Thank you in advance. <code> ,Python Seaborn Facetgrid change xlabels
Python float precision," While working with float precision, I stumbled across a strange fact. Why does python prints only the integer part when formatted with ""%.f"". I am willing to know the mechanism behind this Thanks in advance for the explanation :) <code>  >>> a = float(2.12345) >>> a 2.12345 >>> print ""%.2f"" % a 2.12 >>> print ""%.1f"" % a 2.1 >>> print ""%f"" % a 2.123450 >>> print ""%.f"" % a 2 #why?",Python float precision with no integer precision after decimal point
Python: Compare two large dictionaries and return list of values for similar keys," I have a two dictionaries like: What I want as output is two list of values for the items which exist in both dictionaries: What I am doing right now is something like this: This code is taking too long running which is not something that I am looking forward to. I was wondering if there might be a more efficient way of doing it? <code>  dict1 = { (1,2) : 2, (2,3): 3, (1,3): 3}dict2 = { (1,2) : 1, (1,3): 2} [2,3][1,2] list1 = []list2 = []for key in dict1.keys(): if key in dict2.keys(): list1.append(dict1.get(key)) list2.append(dict2.get(key))",Compare two large dictionaries and create lists of values for keys they have in common
A decorator for a class method that caches the return after first run," My problem, and whyI'm trying to write a decorator for a class method, @cachedproperty. I want it to behave so that when the method is first called, the method is replaced with its return value. I also want it to behave like @property so that it doesn't need to be explicitly called. Basically, it should be indistinguishable from @property except that it's faster, because it only calculates the value once and then stores it. My idea is that this would not slow down instantiation like defining it in __init__ would. That's why I want to do this.What I triedFirst, I tried to override the fget method of the property, but it's read-only.Next, I figured I'd try to implement a decorator that does needs to be called the first time but then caches the values. This isn't my final goal of a property-type decorator that never needs to be called, but I thought this would be a simpler problem to tackle first.In other words, this is a not-working solution to a slightly simpler problem.I tried: However, this doesn't seem work. I tested this with: but I get an error about how the class didn't pass itself to the method: At this point, me and my limited knowledge of deep Python methods are very confused, and I have no idea where my code went wrong or how to fix it. (I've never tried to write a decorator before)The questionHow can I write a decorator that will return the result of calling a class method the first time it's accessed (like @property does), and be replaced with a cached value for all subsequent queries?I hope this question isn't too confusing, I tried to explain it as well as I could. <code>  def cachedproperty(func): """""" Used on methods to convert them to methods that replace themselves with their return value once they are called. """""" def cache(*args): self = args[0] # Reference to the class who owns the method funcname = inspect.stack()[0][3] # Name of the function, so that it can be overridden. setattr(self, funcname, func()) # Replace the function with its value return func() # Return the result of the function return cache >>> class Test:... @cachedproperty... def test(self):... print ""Execute""... return ""Return""... >>> Test.test<unbound method Test.cache>>>> Test.test() Traceback (most recent call last): File ""<stdin>"", line 1, in <module>TypeError: unbound method cache() must be called with Test instance as first argument (got nothing instead)",Decorator for a class method that caches return value after first access
Pandas to_dict modifying numbers," My code below takes in CSV data and uses pandas to_dict() function as one step in converting the data to JSON. The problem is it is modifying the float numbers (e.g. 1.6 becomes 1.6000000000000001). I am not concerned about the loss of accuracy, but because users will see the change in the numbers, it looks amateurish.I am aware:this is something that has come up before here, but it was two years ago, was not really answered in a great way,also I have an additional complication: the data frames I am looking to convert to dictionaries could be any combination of datatypesAs such the issue with the previous solutions are:Converting all the numbers to objects only works if you don't need to (numerically) use the numbers. I want the option to calculate sums and averages which reintroduces the addition decimal issue.Force rounding of numbers to x decimals will either reduce accuracy or add additional unnecessary 0s depending on the data the user providesMy question:Is there a better way to ensure the numbers are not being modified, but are kept in a numeric datatype? Is it a question of changing how I import the CSV data in the first place? Surely there is a simple solution I am overlooking?Here is a simple script that will reproduce this bug: <code>  import pandas as pdimport sysif sys.version_info[0] < 3: from StringIO import StringIOelse: from io import StringIOCSV_Data = ""Index,Column_1,Column_2,Column_3,Column_4,Column_5,Column_6,Column_7,Column_8\nindex_1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8\nindex_2,2.1,2.2,2.3,2.4,2.5,2.6,2.7,2.8\nindex_3,3.1,3.2,3.3,3.4,3.5,3.6,3.7,3.8\nindex_4,4.1,4.2,4.3,4.4,4.5,4.6,4.7,4.8""input_data = StringIO(CSV_Data)df = pd.DataFrame.from_csv(path = input_data, header = 0, sep=',', index_col=0, encoding='utf-8')print(df.to_dict(orient = 'records'))",Pandas to_dict unwantedly modifying float numbers
Automatic type conversions of user defined classes," I want to create a class that wraps an int and allows some things not normally allowed with int types. Here is my code: The output was. So, the first print statement ran nicely, and gave what I expected, but the second one gave an error. I think this is because the first one is using tInt's add function because a appeared before + ""5"" and the second one used the string ""5""'s add function first because it appeared first. I know this but I don't really know how to either force a's add function or allow the tInt class to be represented as a string/int/etc.. when a normal type appears before it in an operation. <code>  class tInt(int): def __add__(self, other): if type(other) == str: return str(self) + str(other) elif type(other) == int: return int(self) + other elif type(other) == float: return float(self) + float(other) else: return self + othera = tInt(2)print (a + ""5"")print (""5"" + a) >> 25Traceback (most recent call last): File ""C:\example.py"", line 14, in <module> print (""5"" + a)TypeError: Can't convert 'tInt' object to str implicitly",User defined int class that supports addition
restricting class based views to user type (custom decorator for class base)," I have few users in my application say A, B and C. Once any type of user authenticates I don't want this user to access all my API'sSo for function based views I have implemented a decorator: so suppose I want one of my API's to be accesed to A type of Users I do: This works fine but I am having trouble to convert this for class based views. I tried to decorate the dispatch method: But I get an error:  <code>  from functools import wrapsfrom rest_framework import statusfrom rest_framework.response import Responsedef permit(user_type): class Permission(object): def __init__(self, view_func): self.view_func = view_func wraps(view_func)(self) def __call__(self, request, *args, **kwargs): if request.user.user_type in user_type: return self.view_func(request, *args, **kwargs) else: return Response(status=status.HTTP_403_FORBIDDEN) return Permission @permit([""A""])def myview(request): # return some reponse @method_decorator(permit_only([""A"",]))def dispatch(self, request, *args, **kwargs): return super(UserList, self).dispatch(*args, **kwargs) AssertionError(u'.accepted_renderer not set on Response',)",Custom decorator for class based views in django rest framework
Use python's typing library to specify more than one possible type, I want to specify a type hint for a function can receive either a list of strs or a list of ints. Is there a way to do this using Python? Something like: <code>  from typing import Listdef my_function(list_arg: List[str|int]): ... ...,Use python's typing library to specify more than one possible type
Use python&#39;s typing library to specify more than one possible type, I want to specify a type hint for a function can receive either a list of strs or a list of ints. Is there a way to do this using Python? Something like: <code>  from typing import Listdef my_function(list_arg: List[str|int]): ... ...,Use python's typing library to specify more than one possible type
Hide axis values in matplotlib," I have this image: I want to hide the numbers; if I use: ...I get this image:It also hide the labels, V and t. How can I keep the labels while hiding the values? <code>  plt.plot(sim_1['t'],sim_1['V'],'k')plt.ylabel('V')plt.xlabel('t')plt.show() plt.axis('off')",Hide tick label values but keep axis labels
Hide axis values but keep axis tick labels in matplotlib," I have this image: I want to hide the numbers; if I use: ...I get this image:It also hide the labels, V and t. How can I keep the labels while hiding the values? <code>  plt.plot(sim_1['t'],sim_1['V'],'k')plt.ylabel('V')plt.xlabel('t')plt.show() plt.axis('off')",Hide tick label values but keep axis labels
How to get list of bucket in a given bucket using google cloud api," I wanted to get all the folders inside a given Google Cloud bucket or folder using Google Cloud Storage API.For example if gs://abc/xyz contains three folders gs://abc/xyz/x1, gs://abc/xyz/x2 and gs://abc/xyz/x3. The API should return all three folder in gs://abc/xyz. It can easily be done using gsutilgsutil ls gs://abc/xyzBut I need to do it using python and Google Cloud Storage API. <code> ",How to get list of folders in a given bucket using Google Cloud API
How to get list of folders in a given bucket using google cloud api," I wanted to get all the folders inside a given Google Cloud bucket or folder using Google Cloud Storage API.For example if gs://abc/xyz contains three folders gs://abc/xyz/x1, gs://abc/xyz/x2 and gs://abc/xyz/x3. The API should return all three folder in gs://abc/xyz. It can easily be done using gsutilgsutil ls gs://abc/xyzBut I need to do it using python and Google Cloud Storage API. <code> ",How to get list of folders in a given bucket using Google Cloud API
"Pip ""Could not find a that satisfies the requirement"""," When I try to install PyGame with:pip install pygame it saysCollecting pygameCould not find a version that satisfies therequirement pygame (from versions: )No matching distribution foundI believe that I am using the most recent version, 8.1.1. I am using Python 3.5.1, on Windows 8.1. I have looked at other answers for this problem and none worked for me. Thanks for any help. <code> ","Pip ""Could not find a version that satisfies the requirement pygame"""
What is the pythonic way to pass arguments to methods?," I'm working on a project that almost everywhere arguments are passed by key. There are functions with positional params only, with keyword (default value) params or mix of both. For example the following function: This function in the current code would be called like this: For me there is no point to name arguments whose name is obvious by the context of the function execution / by the variable names. I would call it like this: In certain cases where it's not clear what the a call argument is from the context, or inferred from the variable names, I might do: So this got me wondering if there is a convention or ""pythonic"" way to call functions with positional/keyword params, when there is no need to rearrange method arguments or use default values for some of the keyword params.  <code>  def complete_task(activity_task, message=None, data=None): pass complete_task(activity_task=activity_task, message=""My massage"", data=task_data) complete_task(activity_task, ""My message"", task_data) complete_task(activity_task, message=""success"", task_data=json_dump)",What is the pythonic way to pass arguments to functions?
Is __init__.py not required for packaes in Python 3?," I am using Python 3.5.1. I read the document and the package section here: https://docs.python.org/3/tutorial/modules.html#packagesNow, I have the following structure: module.py: Now, while in /home/wujek/Playground: Similarly, now in home, superfolder of Playground: Actually, I can do all kinds of stuff: Why does this work? I though there needed to be __init__.py files (empty ones would work) in both a and b for module.py to be importable when the Python path points to the Playground folder?This seems to have changed from Python 2.7: With __init__.py in both ~/Playground/a and ~/Playground/a/b it works fine. <code>  /home/wujek/Playground/a/b/module.py class Foo: def __init__(self): print('initializing Foo') ~/Playground $ python3>>> import a.b.module>>> a.b.module.Foo()initializing Foo<a.b.module.Foo object at 0x100a8f0b8> ~ $ PYTHONPATH=Playground python3>>> import a.b.module>>> a.b.module.Foo()initializing Foo<a.b.module.Foo object at 0x10a5fee10> ~ $ PYTHONPATH=Playground python3>>> import a>>> import a.b>>> import Playground.a.b ~ $ PYTHONPATH=Playground python>>> import aImportError: No module named a>>> import a.bImportError: No module named a.b>>> import a.b.moduleImportError: No module named a.b.module",Is __init__.py not required for packages in Python 3.3+
Is __init__.py not required for packages in Python 3?," I am using Python 3.5.1. I read the document and the package section here: https://docs.python.org/3/tutorial/modules.html#packagesNow, I have the following structure: module.py: Now, while in /home/wujek/Playground: Similarly, now in home, superfolder of Playground: Actually, I can do all kinds of stuff: Why does this work? I though there needed to be __init__.py files (empty ones would work) in both a and b for module.py to be importable when the Python path points to the Playground folder?This seems to have changed from Python 2.7: With __init__.py in both ~/Playground/a and ~/Playground/a/b it works fine. <code>  /home/wujek/Playground/a/b/module.py class Foo: def __init__(self): print('initializing Foo') ~/Playground $ python3>>> import a.b.module>>> a.b.module.Foo()initializing Foo<a.b.module.Foo object at 0x100a8f0b8> ~ $ PYTHONPATH=Playground python3>>> import a.b.module>>> a.b.module.Foo()initializing Foo<a.b.module.Foo object at 0x10a5fee10> ~ $ PYTHONPATH=Playground python3>>> import a>>> import a.b>>> import Playground.a.b ~ $ PYTHONPATH=Playground python>>> import aImportError: No module named a>>> import a.bImportError: No module named a.b>>> import a.b.moduleImportError: No module named a.b.module",Is __init__.py not required for packages in Python 3.3+
Find all possible matchings of bipartite graph," I am using networkx to find the maximum cardinality matching of a bipartite graph.The matched edges are not unique for the particular graph.Is there a way for me to find all the maximum matchings? For the following example, all edges below can be the maximum matching:{1: 2, 2: 1} or {1: 3, 3: 1} or {1: 4, 4: 1} Unfortunately, only returns Is there a way I can get the other combinations as well? <code>  import networkx as nximport matplotlib.pyplot as pltG = nx.MultiDiGraph()edges = [(1,3), (1,4), (1,2)]nx.is_bipartite(G)Truenx.draw(G, with_labels=True)plt.show() nx.bipartite.maximum_matching(G) {1: 2, 2: 1}",All possible maximum matchings of a bipartite graph
python multiprocessing map mishandle of last processes," There is a strange behavior of map when using Python's multiprocessing.Pool. In the example below a pool of 4 processors will work on 28 tasks. This should take seven passes, each taking 4 seconds.However, it takes 8 passes. In the first six passes all processors are engaged. In the 7th pass only two tasks are completed (two idling processors). The remaining 2 tasks are finished in the 8th pass (two idling processors, again). This behavior appears for seemingly random combinations of number of cpus and number of tasks, unnecessarily losing time.This example has been reproduced on both Intel Xeon Haswell (20 cores) and Intel i7 (4 cores).Any ideas on how to force Pool to make use of all available processors in all the passes? The output of the run is given below This is related to the following question, which does not have a clear or correct answer.Python: Multiprocessing Map takes longer to complete last few processes <code>  import timeimport multiprocessingfrom multiprocessing import Poolimport datetimedef f(values): now = str(datetime.datetime.now()) proc_id = str(multiprocessing.current_process()) print(proc_id+' '+now) a=values**2 time.sleep(4) return a if __name__ == '__main__': p = Pool(4) #number of processes processed_values= p.map( f, range(28)) p.close() p.join() print processed_values <Process(PoolWorker-1, started daemon)> 2016-05-13 17:08:49.604065<Process(PoolWorker-2, started daemon)> 2016-05-13 17:08:49.604189<Process(PoolWorker-3, started daemon)> 2016-05-13 17:08:49.604252<Process(PoolWorker-4, started daemon)> 2016-05-13 17:08:49.604866<Process(PoolWorker-1, started daemon)> 2016-05-13 17:08:53.608475<Process(PoolWorker-2, started daemon)> 2016-05-13 17:08:53.608878<Process(PoolWorker-3, started daemon)> 2016-05-13 17:08:53.608931<Process(PoolWorker-4, started daemon)> 2016-05-13 17:08:53.609503<Process(PoolWorker-1, started daemon)> 2016-05-13 17:08:57.612831<Process(PoolWorker-2, started daemon)> 2016-05-13 17:08:57.613135<Process(PoolWorker-3, started daemon)> 2016-05-13 17:08:57.613555<Process(PoolWorker-4, started daemon)> 2016-05-13 17:08:57.614065<Process(PoolWorker-1, started daemon)> 2016-05-13 17:09:01.616974<Process(PoolWorker-2, started daemon)> 2016-05-13 17:09:01.617273<Process(PoolWorker-3, started daemon)> 2016-05-13 17:09:01.617699<Process(PoolWorker-4, started daemon)> 2016-05-13 17:09:01.618190<Process(PoolWorker-1, started daemon)> 2016-05-13 17:09:05.621284<Process(PoolWorker-2, started daemon)> 2016-05-13 17:09:05.621489<Process(PoolWorker-3, started daemon)> 2016-05-13 17:09:05.622130<Process(PoolWorker-4, started daemon)> 2016-05-13 17:09:05.622404<Process(PoolWorker-1, started daemon)> 2016-05-13 17:09:09.625522<Process(PoolWorker-2, started daemon)> 2016-05-13 17:09:09.625631<Process(PoolWorker-3, started daemon)> 2016-05-13 17:09:09.626555<Process(PoolWorker-4, started daemon)> 2016-05-13 17:09:09.626566<Process(PoolWorker-1, started daemon)> 2016-05-13 17:09:13.629761<Process(PoolWorker-2, started daemon)> 2016-05-13 17:09:13.629846<Process(PoolWorker-1, started daemon)> 2016-05-13 17:09:17.634003<Process(PoolWorker-2, started daemon)> 2016-05-13 17:09:17.634317[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441, 484, 529, 576, 625, 676, 729]",python multiprocessing map mishandling of last processes
"Django | Displaying an image from model form like {% static img.thumbnail.url %}, Error 400"," I've finish my first app in Django and works perfectly, but still have pre-deployment problems since I set DEGUG=False ...Here is just to display an image in a template... T_TI was using this, but now it does'nt work when I use whitenoise to serve my image localy... And it return a Bad Request(400) error... Models.py template.py urls.py settings.py Here my error log : Thanks a lot for helping ! :)EDIT :principal structureproject folder <code>  class GalleryItem(models.Model): thumbnail = models.ImageField(blank=True,upload_to='gallery/thumb') img_wide = models.ImageField(blank=True,upload_to='gallery') {% load staticfiles %}{% for img in img_to_display %} <a href=""{{ img.img_wide.url}}"" class=""swipebox"" title=""""> <img src=""{% static img.thumbnail.url %}"" alt=""{{ img.alt}}""> </a>{% endfor %} from django.conf.urls import url, includefrom django.contrib import adminfrom django.conf import settingsimport osfrom django.conf import settingsfrom django.conf.urls.static import staticurlpatterns = [ url(r'^gallery/', include('gallery.urls')), url(r'^shop/', include('shop.urls')), url(r'^events/', include('events.urls')), url(r'^page/', include('paginator.urls')), url(r'^news/', include('blog.urls')), url(r'^ckeditor/', include('ckeditor_uploader.urls')), url(r'^admin/', admin.site.urls),] + static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT) import osimport dj_database_urlBASE_DIR = os.path.dirname(os.path.dirname(__file__))print(""BASE_DIR = "",BASE_DIR)MEDIA_ROOT = os.path.join(BASE_DIR, 'wt/static/media/')MEDIA_URL = '/media/'SECRET_KEY = 'SECRET_KEY'DEBUG = FalseINSTALLED_APPS = [ 'ckeditor', 'ckeditor_uploader', 'team.apps.TeamConfig', 'gallery.apps.GalleryConfig', 'shop.apps.ShopConfig', 'events.apps.EventsConfig', 'blog.apps.BlogConfig', 'paginator.apps.paginatorConfig', 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles',]MIDDLEWARE_CLASSES = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.auth.middleware.SessionAuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'whitenoise.middleware.WhiteNoiseMiddleware',]ROOT_URLCONF = 'wt.urls'TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates')], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ ""django.contrib.auth.context_processors.auth"", ""django.core.context_processors.request"", ""django.core.context_processors.debug"", ""django.core.context_processors.i18n"", ""django.core.context_processors.media"", ""django.core.context_processors.static"", ""django.core.context_processors.tz"", ""django.contrib.messages.context_processors.messages"", ], }, },]WSGI_APPLICATION = 'wt.wsgi.application'DATABASES = { 'default': { 'ENGINE': 'django.db.backends.postgresql', 'NAME': 'wt_db', 'USER': 'postgres', 'PASSWORD': 'PASSWORD', 'HOST': '127.0.0.1', 'PORT': '5432', }}AUTH_PASSWORD_VALIDATORS = [ { 'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator', }, { 'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator', }, { 'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator', }, { 'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator', },]LANGUAGE_CODE = 'fr-fr'TIME_ZONE = 'UTC'USE_I18N = TrueUSE_L10N = TrueUSE_TZ = Truedb_from_env = dj_database_url.config(conn_max_age=500)DATABASES['default'].update(db_from_env)ALLOWED_HOSTS = ['localhost', '127.0.0.1',]STATIC_ROOT = os.path.join(BASE_DIR, 'wt/staticfiles')STATIC_URL = '/static/'STATICFILES_DIRS = [ os.path.join(BASE_DIR, 'wt/static'), os.path.join(BASE_DIR, 'wt/staticfiles'),]STATICFILES_STORAGE = 'whitenoise.django.GzipManifestStaticFilesStorage'CKEDITOR_UPLOAD_PATH = 'uploads'CKEDITOR_IMAGE_BACKEND = 'pillow'CKEDITOR_BROWSE_SHOW_DIRS = True The joined path (E:\media\gallery\thumb\lost-thumb.jpg) is located outside of the base path component (E:\dev\wt\wt\wt\staticfiles)[15/May/2016 20:01:41] ""GET /page/gallery HTTP/1.1"" 400 26","Django | joined path is located outside of the base path component {% static img.thumbnail.url %}, Error 400 with whitenoise"
"Django | joined path is located outside of the base path component {% static img.thumbnail.url %}, Error 400"," I've finish my first app in Django and works perfectly, but still have pre-deployment problems since I set DEGUG=False ...Here is just to display an image in a template... T_TI was using this, but now it does'nt work when I use whitenoise to serve my image localy... And it return a Bad Request(400) error... Models.py template.py urls.py settings.py Here my error log : Thanks a lot for helping ! :)EDIT :principal structureproject folder <code>  class GalleryItem(models.Model): thumbnail = models.ImageField(blank=True,upload_to='gallery/thumb') img_wide = models.ImageField(blank=True,upload_to='gallery') {% load staticfiles %}{% for img in img_to_display %} <a href=""{{ img.img_wide.url}}"" class=""swipebox"" title=""""> <img src=""{% static img.thumbnail.url %}"" alt=""{{ img.alt}}""> </a>{% endfor %} from django.conf.urls import url, includefrom django.contrib import adminfrom django.conf import settingsimport osfrom django.conf import settingsfrom django.conf.urls.static import staticurlpatterns = [ url(r'^gallery/', include('gallery.urls')), url(r'^shop/', include('shop.urls')), url(r'^events/', include('events.urls')), url(r'^page/', include('paginator.urls')), url(r'^news/', include('blog.urls')), url(r'^ckeditor/', include('ckeditor_uploader.urls')), url(r'^admin/', admin.site.urls),] + static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT) import osimport dj_database_urlBASE_DIR = os.path.dirname(os.path.dirname(__file__))print(""BASE_DIR = "",BASE_DIR)MEDIA_ROOT = os.path.join(BASE_DIR, 'wt/static/media/')MEDIA_URL = '/media/'SECRET_KEY = 'SECRET_KEY'DEBUG = FalseINSTALLED_APPS = [ 'ckeditor', 'ckeditor_uploader', 'team.apps.TeamConfig', 'gallery.apps.GalleryConfig', 'shop.apps.ShopConfig', 'events.apps.EventsConfig', 'blog.apps.BlogConfig', 'paginator.apps.paginatorConfig', 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles',]MIDDLEWARE_CLASSES = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.auth.middleware.SessionAuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'whitenoise.middleware.WhiteNoiseMiddleware',]ROOT_URLCONF = 'wt.urls'TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates')], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ ""django.contrib.auth.context_processors.auth"", ""django.core.context_processors.request"", ""django.core.context_processors.debug"", ""django.core.context_processors.i18n"", ""django.core.context_processors.media"", ""django.core.context_processors.static"", ""django.core.context_processors.tz"", ""django.contrib.messages.context_processors.messages"", ], }, },]WSGI_APPLICATION = 'wt.wsgi.application'DATABASES = { 'default': { 'ENGINE': 'django.db.backends.postgresql', 'NAME': 'wt_db', 'USER': 'postgres', 'PASSWORD': 'PASSWORD', 'HOST': '127.0.0.1', 'PORT': '5432', }}AUTH_PASSWORD_VALIDATORS = [ { 'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator', }, { 'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator', }, { 'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator', }, { 'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator', },]LANGUAGE_CODE = 'fr-fr'TIME_ZONE = 'UTC'USE_I18N = TrueUSE_L10N = TrueUSE_TZ = Truedb_from_env = dj_database_url.config(conn_max_age=500)DATABASES['default'].update(db_from_env)ALLOWED_HOSTS = ['localhost', '127.0.0.1',]STATIC_ROOT = os.path.join(BASE_DIR, 'wt/staticfiles')STATIC_URL = '/static/'STATICFILES_DIRS = [ os.path.join(BASE_DIR, 'wt/static'), os.path.join(BASE_DIR, 'wt/staticfiles'),]STATICFILES_STORAGE = 'whitenoise.django.GzipManifestStaticFilesStorage'CKEDITOR_UPLOAD_PATH = 'uploads'CKEDITOR_IMAGE_BACKEND = 'pillow'CKEDITOR_BROWSE_SHOW_DIRS = True The joined path (E:\media\gallery\thumb\lost-thumb.jpg) is located outside of the base path component (E:\dev\wt\wt\wt\staticfiles)[15/May/2016 20:01:41] ""GET /page/gallery HTTP/1.1"" 400 26","Django | joined path is located outside of the base path component {% static img.thumbnail.url %}, Error 400 with whitenoise"
Selenium use of firefox profile," I try to use Selenium Webdriver and Python on Windows 10 system to make some automation of browser actions. But I have this problem: Selenium-started Firefox window doesn't ""see"" that I am already logged in and target site sends me to login page. So I assumed that Selenium not really uses the profile, but just a copy of it.I would like to know:Is my conclusion about actual use of copy of profile true?If 1. is true, is there a way to use really everything from existing profile?If my conclusion is not true, please prove it and point me to the direction where I can find out what information can be used for session, why Selenium could fail to send it and how to force it to do so actually. Edit: <code>  from selenium import webdriverfp = webdriver.FirefoxProfile('C:/Users/<user name>/AppData/Roaming/Mozilla/Firefox/Profiles/abc3defghij2.ProfileName')driver = webdriver.Firefox(fp)driver.get(""https://www.example.com/membersarea"")",Selenium use of Firefox profile
pandas applying sqrt function on a column," I have following data frame How I can apply sqrt function after I do sum to the columns? <code>  data = {'year': [2010, 2011, 2012, 2011, 2012, 2010, 2011, 2012], 'team': ['Bears', 'Bears', 'Bears', 'Packers', 'Packers', 'Lions', 'Lions', 'Lions'], 'wins': [11, 8, 10, 15, 11, 6, 10, 4], 'losses': [5, 8, 6, 1, 5, 10, 6, 12]}football = pd.DataFrame(data, columns=['year', 'team', 'wins', 'losses'])football.set_index(['team', 'year'], inplace=True) football[['wins', 'losses']].sum(axis=1)",Applying sqrt function on a column
How to see full requested URL in wireShark," I did enough research and failed to find a conclusive answer(version 1.12.7).My local server is making a get call to https://hacker-news.firebaseio.com/v0/item/12345.json when I see this packet in wireshark I see the destination URL as hacker-news.firebaseio.com. My ask is simple 1) how can I see the entire URI including /v0/item/12345.json part.2) Why are somany dots in the following TCP stream and what does this TCP stream actually trying to say with jabbered English letters. And description 244 19.329480000 sapy hacker-news.firebaseio.com TCP 435 47965https [PSH, ACK] Seq=471 Ack=3948 Win=40576 Len=369 TSval=20106775 TSecr=4020547278 hacker-news.firebaseio.com 244 <code>  ...........Z..d.2......j.q..n8..{.""RE...wT...../.+.0.,...'.g.(.k.$............j.i.h.9.8.7.6.2...*.&.......=.5.#...........@.?.>.3.2.1.0.1.-.).%.....f.........localhost................3t......5...1..W:...."".q.........m....r.>?t w..............#.................0...0..|.......%e.x.#s.0..*.H.......0I1.0...U....US1.0...U..Google Inc1%0#..U....Google Internet Authority G20..151217200354Z.161216000000Z0j1.0...U....US1.0...U...California1.0...U...Mountain View1.0...U...{.<...[L.......V..7.`?Yb.oF.z.....aC#..FtqyG0..p....-s...........d0.pe....3....5s.....T..y.F.I..w{.zX!...ou.db..j<..+......y.]..`.{V ..q..z""P<T.,<.}............._j.B.*F.\.0 gJa.E..........V).-z...].......t@..S!n......<......`...........>....:Q",How to see full HTTPS URL in wireShark
Drag and Drop widgets tkinter," I am trying to make a Python program in which you can move around widgets.This is my code: But, this gets super glitchy and the widget jumps back and forth. <code>  import tkinter as tk main = tk.Tk()notesFrame = tk.Frame(main, bd = 4, bg = ""a6a6a6"")notesFrame.place(x=10,y=10)notes = tk.Text(notesFrame)notes.pack()notesFrame.bind(""<B1-Motion>"", lambda event: notesFrame.place(x = event.x, y = event.y)",tkinter - How to drag and drop widgets?
Mock timing in a context for models with a field DateTimeField with auto_now_add=True," I'd like to mock timing so that be able to set certain time to a field of type DateTimeField with auto_now_add=True during my tests e.g: I'm aware the current date is always used for this type of fields, that is why I'm looking for an alternative to mock somehow the system timing, but just in a context.I've tried some approaches, for instance, creating a context with freeze_time but didn't work: Ofc I guess, this is due to the machinery behind the scene to create the object when auto_now_add=True.I don't want to remove auto_now_add=True and/or use default values.Is there a way we can mock the timing so that we can make this type of field to get the time that I want in certain context?I'm using Django 1.9.6 and Python 3.4 <code>  class MyModel: ... created_at = models.DateTimeField(auto_now_add=True) ...class TestMyModel(TestCase): ... def test_something(self): # mock current time so that `created_at` be something like 1800-02-09T020000 my_obj = MyModel.objects.create(<whatever>) # and here my_obj.created_at == 1800-02-09T000000 with freeze_now(""1800-02-09""): MyModel.objects.create(<whatever>) # here the created_at doesn't fit 1800-02-09",Mock timing in a context to create models with a field DateTimeField with auto_now_add=True
Mock timing in a context to create models with a field DateTimeField with auto_now_add=True," I'd like to mock timing so that be able to set certain time to a field of type DateTimeField with auto_now_add=True during my tests e.g: I'm aware the current date is always used for this type of fields, that is why I'm looking for an alternative to mock somehow the system timing, but just in a context.I've tried some approaches, for instance, creating a context with freeze_time but didn't work: Ofc I guess, this is due to the machinery behind the scene to create the object when auto_now_add=True.I don't want to remove auto_now_add=True and/or use default values.Is there a way we can mock the timing so that we can make this type of field to get the time that I want in certain context?I'm using Django 1.9.6 and Python 3.4 <code>  class MyModel: ... created_at = models.DateTimeField(auto_now_add=True) ...class TestMyModel(TestCase): ... def test_something(self): # mock current time so that `created_at` be something like 1800-02-09T020000 my_obj = MyModel.objects.create(<whatever>) # and here my_obj.created_at == 1800-02-09T000000 with freeze_now(""1800-02-09""): MyModel.objects.create(<whatever>) # here the created_at doesn't fit 1800-02-09",Mock timing in a context to create models with a field DateTimeField with auto_now_add=True
Mock timing in a context to create models with a field `DateTimeField` with `auto_now_add=True`," I'd like to mock timing so that be able to set certain time to a field of type DateTimeField with auto_now_add=True during my tests e.g: I'm aware the current date is always used for this type of fields, that is why I'm looking for an alternative to mock somehow the system timing, but just in a context.I've tried some approaches, for instance, creating a context with freeze_time but didn't work: Ofc I guess, this is due to the machinery behind the scene to create the object when auto_now_add=True.I don't want to remove auto_now_add=True and/or use default values.Is there a way we can mock the timing so that we can make this type of field to get the time that I want in certain context?I'm using Django 1.9.6 and Python 3.4 <code>  class MyModel: ... created_at = models.DateTimeField(auto_now_add=True) ...class TestMyModel(TestCase): ... def test_something(self): # mock current time so that `created_at` be something like 1800-02-09T020000 my_obj = MyModel.objects.create(<whatever>) # and here my_obj.created_at == 1800-02-09T000000 with freeze_now(""1800-02-09""): MyModel.objects.create(<whatever>) # here the created_at doesn't fit 1800-02-09",Mock timing in a context to create models with a field DateTimeField with auto_now_add=True
How to filter and list the instances on tag values of different tag keys by custom tags values passed during script execution," I am trying to list the instances on tag values of different tag keysFor eg> one tag key - Environment, other tag key - Role. My code is given below : After running this script : python script.py arg1 arg2I am getting following error  <code>  import argparseimport boto3AWS_ACCESS_KEY_ID = '<Access Key>'AWS_SECRET_ACCESS_KEY = '<Secret Key>'def get_ec2_instances(Env,Role): ec2 = boto3.client(""ec2"", region) reservations = ec2.describe_instances(Filters={""tag:environment"" : Env, ""tag:role"" : Role}) for reservation in reservations[""Reservations""] : for instance in reservation[""Instances""]: print ""%s"" % (instance.tags['Name'])if __name__ == '__main__': regions = ['us-east-1','us-west-1','us-west-2','eu-west-1','sa-east-1', 'ap-southeast-1','ap-southeast-2','ap-northeast-1'] parser = argparse.ArgumentParser() parser.add_argument('Env', default=""environment"", help='value for tag:environment'); parser.add_argument('Role', default=""role"", help='value for tag:role'); args = parser.parse_args() for region in regions: get_ec2_instances(args.Env, args.Role) Traceback (most recent call last): File ""script.py"", line 27, in <module> for region in regions: get_ec2_instances(args.Env, args.Role) File ""script.py"", line 10, in get_ec2_instances reservations = ec2.describe_instances(Filters={""tag:environment"" : Env, ""tag:role"" : Role}) File ""/usr/local/lib/python2.7/dist-packages/botocore/client.py"", line 258, in _api_call return self._make_api_call(operation_name, kwargs) File ""/usr/local/lib/python2.7/dist-packages/botocore/client.py"", line 524, in _make_api_call api_params, operation_model, context=request_context) File ""/usr/local/lib/python2.7/dist-packages/botocore/client.py"", line 577, in _convert_to_request_dict api_params, operation_model) File ""/usr/local/lib/python2.7/dist-packages/botocore/validate.py"", line 270, in serialize_to_request raise ParamValidationError(report=report.generate_report())botocore.exceptions.ParamValidationError: Parameter validation failed:Invalid type for parameter Filters, value: {'tag:role': 'arg1', 'tag:environment': 'arg2'}, type: <type 'dict'>, valid types: <type 'list'>, <type 'tuple'>",What is the correct ways to write Boto3 filters to use customise tag name?
how to read first X bytes from binary file in python, I want to read all data from a binary file without the last 4 bytes. How can we do it in Python? <code> ,How do I read all but the last X bytes from a binary file?
How do I read the first X bytes from a binary file?, I want to read all data from a binary file without the last 4 bytes. How can we do it in Python? <code> ,How do I read all but the last X bytes from a binary file?
Python CMA-ES Algorithm to solve user-defined function with constant constraints," I am struggling to create a simple example of a CMA-ES optimization algorithm in python. What is the most streamlined way to optimize the function x**2 + 2*y**2 -4*x*y - 0.5*y, subject to constraints -2<x<2 and -1<2*(x**2)*y<1, using the CMA-ES algorithm?I looked into the DEAP library, but was not able to develop a cohesive attempt. I found their documentation less than intuitive. I also looked into the cma package, but it is not clear to me how I can implement constraints. <code> ",Python CMA-ES Algorithm to solve user-defined function and constraints
DRF Serializer: get data from nested fields to flat object," I want to serialize non-flat structure to a one flat object.Here's an example of an API call I receive (I cannot control it unfortunately): I'd like to serialize it to the models like these: As you can see, model Issue's field summary is located on the same object as id and key unlike in JSON data.My Serializer are next: I works well for fields in issue object in JSON.But how can I add to the JiraIssueSerializer some fields like summary? They are not direct fields of issue object, but located in substrcucture fields. I see these ways:Make one more Model Fields to keep them, but it's ridiculous. I do not need it and my API strictly depends on the foreign structure. Make some .to_internal_fields() convertors. But in this case I have to manually validate and prepare all the fields in my Issue and repeat myself several times. Also If field summary is not enlisted in Serializer, latter cannot use it or validation fails.Add DictField to Serializer (commented in code) and take data from it. Is it good? How to validate it? Again, I have n clean structure of code.Next I'd like to parse and save changelog data. How to deal better with such structures?Thank you! <code>  {""webhookEvent"": ""jira:issue_updated"",""user"": { ""id"": 2434, ""name"": ""Ben"", },""issue"": { ""id"": ""33062"", ""key"": ""jira-project-key-111"", ""fields"": { ""summary"": ""The week ahead"", },""changelog"": { ""id"": ""219580"", ""items"": [{ ""field"": ""status"", ""fieldtype"": ""jira"", ""from"": ""10127"", ""fromString"": ""Submitted"", ""to"": ""10128"", ""toString"": ""Staged"" }]},""timestamp"": 1423234723378} class Issue(models.Model): jira_id = models.IntegerField() jira_id = models.CharField() summary = models.CharField()class Change(models.Model): issue = models.ForeignKey(Issue) timestamp = models.DataTimeField() class ChangeSerializer(serializers.ModelSerializer): """"""Receives complex data from jira and converts into objects."""""" issue = JiraIssueSerializer() timestamp = TimestampField(source='created_at') class Meta: model = Change fields = ('issue', 'timestamp') def create(self, validated_data): super(serializers.ModelSerializer, self).create(validated_data=validated_data) jira_issue = JiraIssueSerializer(data=validated_data) issue = Issue.objects.get(jira_issue) self.created_at = datetime.utcnow() change = Change(**validated_data) return change class JiraIssueSerializer(serializers.ModelSerializer): """"""Issue serializer."""""" id = serializers.IntegerField(source='jira_id') key = serializers.CharField(source='jira_key') summary = serializers.CharField() ### I want field to work! # fields = serializers.DictField(child=serializers.CharField()) class Meta: model = Issue fields = ('id', 'key', 'summary', ) def to_internal_value(self, data): # ret = super(serializers.ModelSerializer, self).to_internal_value(data) ret = {} # ret = super().to_internal_value(data) ret['jira_id'] = data.get('id', None) ret['jira_key'] = data.get('key', None) jira_issue_fields_data = data.get('fields') if jira_issue_fields_data or 1: summary = jira_issue_fields_data.get('summary', None) ret.update(summary=summary) print('to_internal_value', ret) return ret def to_representation(self, instance): ret = {} ret = super().to_representation(instance) fields = {} fields['summary'] = instance.summary ret.update(fields=fields) print(ret) return ret",Django Rest Framework: Serialize data from nested json fields to plain object
Is it possible to have a local random seed in Python?," Python's random seems are global, so modules changing it will effect each other.While there are of course many 3rd party modules, is there a way using Python's standard library to have a random number local to a context.(without using random.get/setstate which may be problematic when mixing code from different modules).Something like... Where each module can use its own random context. <code>  r = random.context(seed=42)number = r.randint(10, 20)",How to use Python's random number generator with a local seed?
argparse - Arbitrary optional arguments, Is it possible to use argparse to capture an arbitrary set of optional arguments?For example both the following should be accepted as inputs: a priori I don't know what optional arguments would be specified receive but would handle them accordingly. <code>  python script.py required_arg1 --var1 value1 --var2 value2 --var3 value3python script.py required_arg1 --varA valueA --var2 value2 --varB valueB,Is it possible to use argparse to capture an arbitrary set of optional arguments?
numpy.polynomial.legendre.leggauss over intervals -x -> infinity? convert from scipy quad," Okay I know this has been asked before with a limited example for scaling [-1, 1] intervals [a, b] Different intervals for Gauss-Legendre quadrature in numpy BUT no one has posted how to generalize this for [-a, Infinity] (as is done below, but not (yet) fast). Also this shows how to call a complex function (in quantitative option pricing anyhow) with several implementations. There is the benchmark quad code, followed by leggauss, with links to code examples on how to implement an adaptive algorithm. I have worked through most of the linked adaptive algorithm difficulties - it currently prints the sum of the divided integral to show it works correctly. Here you will find functions to convert a range from [-1, 1] to [0, 1] to [a, Infinity] (thanks @AlexisClarembeau). To use the adaptive algorithm I had to create another function to convert from [-1, 1] to [a, b] which is fed back into the [a, Infinity] function. Still need to increase the speed and accuracy with less dimensions as I can't manually adjust the degrees range for -a to get convergence. To illustrate why this is a problem, put in these values instead: a=-20, F=50, then run. You can increase degrees=1000 and see that there is no benefit to this Gauss-Legendre algorithm if it is not applied intelligently. My requirement for speed is to get to 0.0004s per loop, whereas the last algorithm I Cythonized took about 0.75s, which is why I am trying to use a low degree, high accuracy algorithm with Gauss-Legendre. With Cython and multi-threading this requirement from a completely optimized Python implementation is roughly 0.007s per loop (a non-vectorized, loop ridden, inefficient routine could be 0.1s per loop, with degrees=20, i.e. %timeit adaptive_integration(x,w).A possible solution which I've half implemented is here http://online.sfsu.edu/meredith/Numerical_Analysis/improper_integrals on pages 5/6, adaptive integration whereas the interval a-b (in this case, I wrote the transform_integral_negative1_1_to_a_b function) where the interval is divided in 2 (@0.5), the function is then evaluated on these 1/2 intervals, and the sum of the two 0->0.5 + 0.5->1 are compared to the function results for the whole range 0->1. If accuracy is not within tolerance, the range is further subdivided at 0.25 and 0.75, the function is again evaluated for each subinterval, and compared to the prior 1/2 interval sums @0.5. If 1 side is within tolerance (e.g. abs(0->0.5 - (0->0.25 + 0.25->0.5)) < precision), but the other side is not, splitting stops on the side within tolerance, but continues on the other side until precision is reached. At this point the results for each slice of the interval are summed to obtain the full integral with higher accuracy.There are likely faster and better ways of approaching this problem. I don't care as long as it is fast and accurate. Here is the best description of integration routines I've come across for reference http://orion.math.iastate.edu/keinert/computation_notes/chapter5.pdf Award is 100pts bounty + 15pts for answer acceptance. Thank you for assisting in making this code FAST and ACCURATE!EDIT:Here is my change to the adaptive_integration code - if someone can make this work fast I can accept an answer and award bounty. This Mathematica code on page 7 http://online.sfsu.edu/meredith/Numerical_Analysis/improper_integrals does the routine I attempted. It has work on a routine that doesn't converge well, see the variables below. Right now my code errors out: RecursionError: maximum recursion depth exceeded in comparison on some inputs, or if the degrees are set too high, or doesn't get close to the quad result when it does work, so something is apparently wrong here. The output with degrees=100 (after calculating GL with degrees=10000 for a better initial estimate, otherwise, the algorithm always agrees with its own accuracy apparently and doesn't invoke the adaptive path which fails every time): <code>  import numpy as npfrom scipy.stats import norm, lognormfrom scipy.integrate import quada = 0degrees = 50flag=-1.0000F = 1.2075K = 0.1251vol = 0.43T2 = 0.0411T1 = 0.0047def integrand(x, flag, F, K, vol, T2, T1): d1 = (np.log(x / (x+K)) + 0.5 * (vol**2) * (T2-T1)) / (vol * np.sqrt(T2 - T1)) d2 = d1 - vol*np.sqrt(T2 - T1) mu = np.log(F) - 0.5 *vol **2 * T1 sigma = vol * np.sqrt(T1) return lognorm.pdf(x, mu, sigma) * (flag * x*norm.cdf(flag * d1) - flag * (x+K)*norm.cdf(flag * d2))def transform_integral_0_1_to_Infinity(x, a): return integrand(a+(x/(1-x)), flag, F, K, vol, T2, T1) *(1/(1-x)**2); def transform_integral_negative1_1_to_0_1(x, a): return 0.5 * transform_integral_0_1_to_Infinity((x+1)/2, a)def transform_integral_negative1_1_to_a_b(x, w, a, b): return np.sum(w*(0.5 * transform_integral_0_1_to_Infinity(((x+1)/2*(b-a)+a), a)))def adaptive_integration(x, w, a=-1, b=1, lastsplit=False, precision=1e-10): #split the integral in half assuming [-1, 1] range midpoint = (a+b)/2 interval1 = transform_integral_negative1_1_to_a_b(x, w, a, midpoint) interval2 = transform_integral_negative1_1_to_a_b(x, w, midpoint, b) return interval1+interval2 #just shows this is correct for splitting the intervaldef integrate(x, w, a): return np.sum(w*transform_integral_negative1_1_to_0_1(x, a))x, w = np.polynomial.legendre.leggauss(degrees) quadresult = quad(integrand, a, np.Inf, args=(flag, F, K, vol, T2, T1), epsabs=1e-1000)[0]GL = integrate(x, w, a)print(""Adaptive Sum Result:"")print(adaptive_integration(x, w))print(""GL result""); print(GL)print(""QUAD result"")print(quadresult) def adaptive_integration(x, w, a, b, integralA2B, remainingIterations, firstIteration, precision=1e-9): #split the integral in half assuming [-1, 1] range if remainingIterations == 0: print('Adaptive integration failed on the interval',a,'->',b) if np.isnan(integralA2B): return np.nan midpoint = (a+b)/2 interval1 = transform_integral_negative1_1_to_a_b(x, w, a, midpoint) interval2 = transform_integral_negative1_1_to_a_b(x, w, midpoint, b) if np.abs(integralA2B - (interval1 + interval2)) < precision : return(interval1 + interval2) else: return adaptive_integration(x, w, a, midpoint, interval1, (remainingIterations-1), False) + adaptive_integration(x, w, midpoint, b, interval2, (remainingIterations-1), False) #This example doesn't converge to Quad# non-converging interval inputsa = 0 # AND a = -250degrees = 10flag= 1F = 50K = 0.1251vol = 0.43T2 = 0.0411T1 = 0.0047print(adaptive_integration(x, w, -1, 1, GL, 500, False)) GL result:60.065205169286379Adaptive Sum Result:RecursionError: maximum recursion depth exceeded in comparisonQUAD result:68.72069173210338",Gauss-Legendre over intervals -x -> infinity: adaptive algorithm to transform weights and nodes efficiently
gauss-legendre over intervals -x -> infinity? algorithm to transform weights and nodes?," Okay I know this has been asked before with a limited example for scaling [-1, 1] intervals [a, b] Different intervals for Gauss-Legendre quadrature in numpy BUT no one has posted how to generalize this for [-a, Infinity] (as is done below, but not (yet) fast). Also this shows how to call a complex function (in quantitative option pricing anyhow) with several implementations. There is the benchmark quad code, followed by leggauss, with links to code examples on how to implement an adaptive algorithm. I have worked through most of the linked adaptive algorithm difficulties - it currently prints the sum of the divided integral to show it works correctly. Here you will find functions to convert a range from [-1, 1] to [0, 1] to [a, Infinity] (thanks @AlexisClarembeau). To use the adaptive algorithm I had to create another function to convert from [-1, 1] to [a, b] which is fed back into the [a, Infinity] function. Still need to increase the speed and accuracy with less dimensions as I can't manually adjust the degrees range for -a to get convergence. To illustrate why this is a problem, put in these values instead: a=-20, F=50, then run. You can increase degrees=1000 and see that there is no benefit to this Gauss-Legendre algorithm if it is not applied intelligently. My requirement for speed is to get to 0.0004s per loop, whereas the last algorithm I Cythonized took about 0.75s, which is why I am trying to use a low degree, high accuracy algorithm with Gauss-Legendre. With Cython and multi-threading this requirement from a completely optimized Python implementation is roughly 0.007s per loop (a non-vectorized, loop ridden, inefficient routine could be 0.1s per loop, with degrees=20, i.e. %timeit adaptive_integration(x,w).A possible solution which I've half implemented is here http://online.sfsu.edu/meredith/Numerical_Analysis/improper_integrals on pages 5/6, adaptive integration whereas the interval a-b (in this case, I wrote the transform_integral_negative1_1_to_a_b function) where the interval is divided in 2 (@0.5), the function is then evaluated on these 1/2 intervals, and the sum of the two 0->0.5 + 0.5->1 are compared to the function results for the whole range 0->1. If accuracy is not within tolerance, the range is further subdivided at 0.25 and 0.75, the function is again evaluated for each subinterval, and compared to the prior 1/2 interval sums @0.5. If 1 side is within tolerance (e.g. abs(0->0.5 - (0->0.25 + 0.25->0.5)) < precision), but the other side is not, splitting stops on the side within tolerance, but continues on the other side until precision is reached. At this point the results for each slice of the interval are summed to obtain the full integral with higher accuracy.There are likely faster and better ways of approaching this problem. I don't care as long as it is fast and accurate. Here is the best description of integration routines I've come across for reference http://orion.math.iastate.edu/keinert/computation_notes/chapter5.pdf Award is 100pts bounty + 15pts for answer acceptance. Thank you for assisting in making this code FAST and ACCURATE!EDIT:Here is my change to the adaptive_integration code - if someone can make this work fast I can accept an answer and award bounty. This Mathematica code on page 7 http://online.sfsu.edu/meredith/Numerical_Analysis/improper_integrals does the routine I attempted. It has work on a routine that doesn't converge well, see the variables below. Right now my code errors out: RecursionError: maximum recursion depth exceeded in comparison on some inputs, or if the degrees are set too high, or doesn't get close to the quad result when it does work, so something is apparently wrong here. The output with degrees=100 (after calculating GL with degrees=10000 for a better initial estimate, otherwise, the algorithm always agrees with its own accuracy apparently and doesn't invoke the adaptive path which fails every time): <code>  import numpy as npfrom scipy.stats import norm, lognormfrom scipy.integrate import quada = 0degrees = 50flag=-1.0000F = 1.2075K = 0.1251vol = 0.43T2 = 0.0411T1 = 0.0047def integrand(x, flag, F, K, vol, T2, T1): d1 = (np.log(x / (x+K)) + 0.5 * (vol**2) * (T2-T1)) / (vol * np.sqrt(T2 - T1)) d2 = d1 - vol*np.sqrt(T2 - T1) mu = np.log(F) - 0.5 *vol **2 * T1 sigma = vol * np.sqrt(T1) return lognorm.pdf(x, mu, sigma) * (flag * x*norm.cdf(flag * d1) - flag * (x+K)*norm.cdf(flag * d2))def transform_integral_0_1_to_Infinity(x, a): return integrand(a+(x/(1-x)), flag, F, K, vol, T2, T1) *(1/(1-x)**2); def transform_integral_negative1_1_to_0_1(x, a): return 0.5 * transform_integral_0_1_to_Infinity((x+1)/2, a)def transform_integral_negative1_1_to_a_b(x, w, a, b): return np.sum(w*(0.5 * transform_integral_0_1_to_Infinity(((x+1)/2*(b-a)+a), a)))def adaptive_integration(x, w, a=-1, b=1, lastsplit=False, precision=1e-10): #split the integral in half assuming [-1, 1] range midpoint = (a+b)/2 interval1 = transform_integral_negative1_1_to_a_b(x, w, a, midpoint) interval2 = transform_integral_negative1_1_to_a_b(x, w, midpoint, b) return interval1+interval2 #just shows this is correct for splitting the intervaldef integrate(x, w, a): return np.sum(w*transform_integral_negative1_1_to_0_1(x, a))x, w = np.polynomial.legendre.leggauss(degrees) quadresult = quad(integrand, a, np.Inf, args=(flag, F, K, vol, T2, T1), epsabs=1e-1000)[0]GL = integrate(x, w, a)print(""Adaptive Sum Result:"")print(adaptive_integration(x, w))print(""GL result""); print(GL)print(""QUAD result"")print(quadresult) def adaptive_integration(x, w, a, b, integralA2B, remainingIterations, firstIteration, precision=1e-9): #split the integral in half assuming [-1, 1] range if remainingIterations == 0: print('Adaptive integration failed on the interval',a,'->',b) if np.isnan(integralA2B): return np.nan midpoint = (a+b)/2 interval1 = transform_integral_negative1_1_to_a_b(x, w, a, midpoint) interval2 = transform_integral_negative1_1_to_a_b(x, w, midpoint, b) if np.abs(integralA2B - (interval1 + interval2)) < precision : return(interval1 + interval2) else: return adaptive_integration(x, w, a, midpoint, interval1, (remainingIterations-1), False) + adaptive_integration(x, w, midpoint, b, interval2, (remainingIterations-1), False) #This example doesn't converge to Quad# non-converging interval inputsa = 0 # AND a = -250degrees = 10flag= 1F = 50K = 0.1251vol = 0.43T2 = 0.0411T1 = 0.0047print(adaptive_integration(x, w, -1, 1, GL, 500, False)) GL result:60.065205169286379Adaptive Sum Result:RecursionError: maximum recursion depth exceeded in comparisonQUAD result:68.72069173210338",Gauss-Legendre over intervals -x -> infinity: adaptive algorithm to transform weights and nodes efficiently
gauss-legendre over intervals -x -> infinity: adaptive algorithm to transform weights and nodes efficiently," Okay I know this has been asked before with a limited example for scaling [-1, 1] intervals [a, b] Different intervals for Gauss-Legendre quadrature in numpy BUT no one has posted how to generalize this for [-a, Infinity] (as is done below, but not (yet) fast). Also this shows how to call a complex function (in quantitative option pricing anyhow) with several implementations. There is the benchmark quad code, followed by leggauss, with links to code examples on how to implement an adaptive algorithm. I have worked through most of the linked adaptive algorithm difficulties - it currently prints the sum of the divided integral to show it works correctly. Here you will find functions to convert a range from [-1, 1] to [0, 1] to [a, Infinity] (thanks @AlexisClarembeau). To use the adaptive algorithm I had to create another function to convert from [-1, 1] to [a, b] which is fed back into the [a, Infinity] function. Still need to increase the speed and accuracy with less dimensions as I can't manually adjust the degrees range for -a to get convergence. To illustrate why this is a problem, put in these values instead: a=-20, F=50, then run. You can increase degrees=1000 and see that there is no benefit to this Gauss-Legendre algorithm if it is not applied intelligently. My requirement for speed is to get to 0.0004s per loop, whereas the last algorithm I Cythonized took about 0.75s, which is why I am trying to use a low degree, high accuracy algorithm with Gauss-Legendre. With Cython and multi-threading this requirement from a completely optimized Python implementation is roughly 0.007s per loop (a non-vectorized, loop ridden, inefficient routine could be 0.1s per loop, with degrees=20, i.e. %timeit adaptive_integration(x,w).A possible solution which I've half implemented is here http://online.sfsu.edu/meredith/Numerical_Analysis/improper_integrals on pages 5/6, adaptive integration whereas the interval a-b (in this case, I wrote the transform_integral_negative1_1_to_a_b function) where the interval is divided in 2 (@0.5), the function is then evaluated on these 1/2 intervals, and the sum of the two 0->0.5 + 0.5->1 are compared to the function results for the whole range 0->1. If accuracy is not within tolerance, the range is further subdivided at 0.25 and 0.75, the function is again evaluated for each subinterval, and compared to the prior 1/2 interval sums @0.5. If 1 side is within tolerance (e.g. abs(0->0.5 - (0->0.25 + 0.25->0.5)) < precision), but the other side is not, splitting stops on the side within tolerance, but continues on the other side until precision is reached. At this point the results for each slice of the interval are summed to obtain the full integral with higher accuracy.There are likely faster and better ways of approaching this problem. I don't care as long as it is fast and accurate. Here is the best description of integration routines I've come across for reference http://orion.math.iastate.edu/keinert/computation_notes/chapter5.pdf Award is 100pts bounty + 15pts for answer acceptance. Thank you for assisting in making this code FAST and ACCURATE!EDIT:Here is my change to the adaptive_integration code - if someone can make this work fast I can accept an answer and award bounty. This Mathematica code on page 7 http://online.sfsu.edu/meredith/Numerical_Analysis/improper_integrals does the routine I attempted. It has work on a routine that doesn't converge well, see the variables below. Right now my code errors out: RecursionError: maximum recursion depth exceeded in comparison on some inputs, or if the degrees are set too high, or doesn't get close to the quad result when it does work, so something is apparently wrong here. The output with degrees=100 (after calculating GL with degrees=10000 for a better initial estimate, otherwise, the algorithm always agrees with its own accuracy apparently and doesn't invoke the adaptive path which fails every time): <code>  import numpy as npfrom scipy.stats import norm, lognormfrom scipy.integrate import quada = 0degrees = 50flag=-1.0000F = 1.2075K = 0.1251vol = 0.43T2 = 0.0411T1 = 0.0047def integrand(x, flag, F, K, vol, T2, T1): d1 = (np.log(x / (x+K)) + 0.5 * (vol**2) * (T2-T1)) / (vol * np.sqrt(T2 - T1)) d2 = d1 - vol*np.sqrt(T2 - T1) mu = np.log(F) - 0.5 *vol **2 * T1 sigma = vol * np.sqrt(T1) return lognorm.pdf(x, mu, sigma) * (flag * x*norm.cdf(flag * d1) - flag * (x+K)*norm.cdf(flag * d2))def transform_integral_0_1_to_Infinity(x, a): return integrand(a+(x/(1-x)), flag, F, K, vol, T2, T1) *(1/(1-x)**2); def transform_integral_negative1_1_to_0_1(x, a): return 0.5 * transform_integral_0_1_to_Infinity((x+1)/2, a)def transform_integral_negative1_1_to_a_b(x, w, a, b): return np.sum(w*(0.5 * transform_integral_0_1_to_Infinity(((x+1)/2*(b-a)+a), a)))def adaptive_integration(x, w, a=-1, b=1, lastsplit=False, precision=1e-10): #split the integral in half assuming [-1, 1] range midpoint = (a+b)/2 interval1 = transform_integral_negative1_1_to_a_b(x, w, a, midpoint) interval2 = transform_integral_negative1_1_to_a_b(x, w, midpoint, b) return interval1+interval2 #just shows this is correct for splitting the intervaldef integrate(x, w, a): return np.sum(w*transform_integral_negative1_1_to_0_1(x, a))x, w = np.polynomial.legendre.leggauss(degrees) quadresult = quad(integrand, a, np.Inf, args=(flag, F, K, vol, T2, T1), epsabs=1e-1000)[0]GL = integrate(x, w, a)print(""Adaptive Sum Result:"")print(adaptive_integration(x, w))print(""GL result""); print(GL)print(""QUAD result"")print(quadresult) def adaptive_integration(x, w, a, b, integralA2B, remainingIterations, firstIteration, precision=1e-9): #split the integral in half assuming [-1, 1] range if remainingIterations == 0: print('Adaptive integration failed on the interval',a,'->',b) if np.isnan(integralA2B): return np.nan midpoint = (a+b)/2 interval1 = transform_integral_negative1_1_to_a_b(x, w, a, midpoint) interval2 = transform_integral_negative1_1_to_a_b(x, w, midpoint, b) if np.abs(integralA2B - (interval1 + interval2)) < precision : return(interval1 + interval2) else: return adaptive_integration(x, w, a, midpoint, interval1, (remainingIterations-1), False) + adaptive_integration(x, w, midpoint, b, interval2, (remainingIterations-1), False) #This example doesn't converge to Quad# non-converging interval inputsa = 0 # AND a = -250degrees = 10flag= 1F = 50K = 0.1251vol = 0.43T2 = 0.0411T1 = 0.0047print(adaptive_integration(x, w, -1, 1, GL, 500, False)) GL result:60.065205169286379Adaptive Sum Result:RecursionError: maximum recursion depth exceeded in comparisonQUAD result:68.72069173210338",Gauss-Legendre over intervals -x -> infinity: adaptive algorithm to transform weights and nodes efficiently
Custom continuous color map in matplotlib (Pyhton)," I have read some questions on this subject but I have been unable to find a specific answer to my question.Let consider the image below:My goal is just to change the limit colors of the map, e.g. in this case the color map goes from dark red to dark blue, let's say I would like it to go from dark green to dark blue. Specifically, I would it to go from colors #244162 to #DCE6F1 (tonalities of blue) in the same continuous way as in the example above.How is it possible to do this?[EDIT]I have tried the following code: But I get the error message TypeError: list indices must be integers, not unicode. <code>  import matplotlib.pyplot as pltimport matplotlib.colors as clrsome_matrix = ...cmap = clr.LinearSegmentedColormap('custom blue', ['#244162','#DCE6F1'], N=256)plt.matshow(some_matrix, cmap=cmap)",Custom continuous color map in matplotlib
randint with a normal distribution," How to generate a random integer as with np.random.randint(), but with a normal distribution around 0.np.random.randint(-10, 10) returns integers with a discrete uniform distributionnp.random.normal(0, 0.1, 1) returns floats with a normal distributionWhat I want is a kind of combination between the two functions. <code> ",How to generate a random normal distribution of integers
Can't install Polyglot package," I'm trying to install Polyglot package by using the command: and I receive the following: At first I had a problem with the C++ Compiler which I resolved and now I get this problem. Iv'e searched for similar issues and most of the comments recommend to install Microsoft Windows SDK (in order to resolve a 64 bits issue with the compiler) which I did without luck.I'm working on Windows 10 64 machine on Python 3.4. <code>  pip install polyglot Collecting polyglot Using cached polyglot-15.10.03-py2.py3-none-any.whlCollecting pycld2>=0.3 (from polyglot)Requirement already satisfied (use --upgrade to upgrade): futures>=2.1.6 in d:\program files\winpython-64bit-3.4.4.2\python-3.4.4.amd64\lib\site-packages (from polyglot)Requirement already satisfied (use --upgrade to upgrade): wheel>=0.23.0 in d:\program files\winpython-64bit-3.4.4.2\python-3.4.4.amd64\lib\site-packages (from polyglot)Collecting PyICU>=1.8 (from polyglot) Using cached PyICU-1.9.3.tar.gzCollecting morfessor>=2.0.2a1 (from polyglot)Requirement already satisfied (use --upgrade to upgrade): six>=1.7.3 in d:\program files\winpython-64bit-3.4.4.2\python-3.4.4.amd64\lib\site-packages (from polyglot)Building wheels for collected packages: PyICU Running setup.py bdist_wheel for PyICU ... error Complete output from command ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\python.exe"" -u -c ""import setuptools, tokenize;__file__='C:\\Users\\revuze\\AppData\\Local\\Temp\\pip-build-h2bmp43j\\PyICU\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" bdist_wheel -d C:\Users\revuze\AppData\Local\Temp\tmpphoimlejpip-wheel- --python-tag cp34: running bdist_wheel running build running build_py creating build creating build\lib.win-amd64-3.4 copying icu.py -> build\lib.win-amd64-3.4 copying PyICU.py -> build\lib.win-amd64-3.4 copying docs.py -> build\lib.win-amd64-3.4 running build_ext building '_icu' extension Traceback (most recent call last): File ""<string>"", line 1, in <module> File ""C:\Users\revuze\AppData\Local\Temp\pip-build-h2bmp43j\PyICU\setup.py"", line 115, in <module> py_modules=['icu', 'PyICU', 'docs']) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\core.py"", line 148, in setup dist.run_commands() File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\dist.py"", line 955, in run_commands self.run_command(cmd) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\dist.py"", line 974, in run_command cmd_obj.run() File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\site-packages\wheel\bdist_wheel.py"", line 179, in run self.run_command('build') File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\cmd.py"", line 313, in run_command self.distribution.run_command(command) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\dist.py"", line 974, in run_command cmd_obj.run() File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\command\build.py"", line 126, in run self.run_command(cmd_name) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\cmd.py"", line 313, in run_command self.distribution.run_command(command) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\dist.py"", line 974, in run_command cmd_obj.run() File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\site-packages\setuptools\command\build_ext.py"", line 49, in run _build_ext.run(self) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\site-packages\Cython\Distutils\build_ext.py"", line 164, in run _build_ext.build_ext.run(self) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\command\build_ext.py"", line 339, in run self.build_extensions() File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\site-packages\Cython\Distutils\build_ext.py"", line 172, in build_extensions self.build_extension(ext) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\site-packages\setuptools\command\build_ext.py"", line 174, in build_extension _build_ext.build_extension(self, ext) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\command\build_ext.py"", line 503, in build_extension depends=ext.depends) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\msvc9compiler.py"", line 460, in compile self.initialize() File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\msvc9compiler.py"", line 371, in initialize vc_env = query_vcvarsall(VERSION, plat_spec) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\site-packages\setuptools\msvc9_support.py"", line 52, in query_vcvarsall return unpatched['query_vcvarsall'](version, *args, **kwargs) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\msvc9compiler.py"", line 287, in query_vcvarsall raise ValueError(str(list(result.keys()))) ValueError: ['path'] ---------------------------------------- Failed building wheel for PyICU Running setup.py clean for PyICUFailed to build PyICUInstalling collected packages: pycld2, PyICU, morfessor, polyglot Running setup.py install for PyICU ... error Complete output from command ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\python.exe"" -u -c ""import setuptools, tokenize;__file__='C:\\Users\\revuze\\AppData\\Local\\Temp\\pip-build-h2bmp43j\\PyICU\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record C:\Users\revuze\AppData\Local\Temp\pip-lkjedrat-record\install-record.txt --single-version-externally-managed --compile: running install running build running build_py creating build creating build\lib.win-amd64-3.4 copying icu.py -> build\lib.win-amd64-3.4 copying PyICU.py -> build\lib.win-amd64-3.4 copying docs.py -> build\lib.win-amd64-3.4 running build_ext building '_icu' extension Traceback (most recent call last): File ""<string>"", line 1, in <module> File ""C:\Users\revuze\AppData\Local\Temp\pip-build-h2bmp43j\PyICU\setup.py"", line 115, in <module> py_modules=['icu', 'PyICU', 'docs']) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\core.py"", line 148, in setup dist.run_commands() File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\dist.py"", line 955, in run_commands self.run_command(cmd) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\dist.py"", line 974, in run_command cmd_obj.run() File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\site-packages\setuptools\command\install.py"", line 61, in run return orig.install.run(self) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\command\install.py"", line 539, in run self.run_command('build') File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\cmd.py"", line 313, in run_command self.distribution.run_command(command) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\dist.py"", line 974, in run_command cmd_obj.run() File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\command\build.py"", line 126, in run self.run_command(cmd_name) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\cmd.py"", line 313, in run_command self.distribution.run_command(command) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\dist.py"", line 974, in run_command cmd_obj.run() File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\site-packages\setuptools\command\build_ext.py"", line 49, in run _build_ext.run(self) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\site-packages\Cython\Distutils\build_ext.py"", line 164, in run _build_ext.build_ext.run(self) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\command\build_ext.py"", line 339, in run self.build_extensions() File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\site-packages\Cython\Distutils\build_ext.py"", line 172, in build_extensions self.build_extension(ext) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\site-packages\setuptools\command\build_ext.py"", line 174, in build_extension _build_ext.build_extension(self, ext) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\command\build_ext.py"", line 503, in build_extension depends=ext.depends) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\msvc9compiler.py"", line 460, in compile self.initialize() File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\msvc9compiler.py"", line 371, in initialize vc_env = query_vcvarsall(VERSION, plat_spec) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\site-packages\setuptools\msvc9_support.py"", line 52, in query_vcvarsall return unpatched['query_vcvarsall'](version, *args, **kwargs) File ""D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\lib\distutils\msvc9compiler.py"", line 287, in query_vcvarsall raise ValueError(str(list(result.keys()))) ValueError: ['path'] ----------------------------------------Command """"D:\Program Files\WinPython-64bit-3.4.4.2\python-3.4.4.amd64\python.exe"" -u -c ""import setuptools, tokenize;__file__='C:\\Users\\revuze\\AppData\\Local\\Temp\\pip-build-h2bmp43j\\PyICU\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record C:\Users\revuze\AppData\Local\Temp\pip-lkjedrat-record\install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in C:\Users\revuze\AppData\Local\Temp\pip-build-h2bmp43j\PyICU\",Can't install python Polyglot package on Windows
"Python with Eclipse, dig into variables while debugging"," I am currently debugging code using Python. I have not been using Python for a while. I put some breakpoints on a variable which is an integer. Let's say this variable is X = 10. How can I:see what is in the variable? (I can highlight and a yellow case appears but if there are a lot of information it is not practical to display like this)do some manipulation of the variable, for example I would like to do X+2 and get the result? <code> ",Dig into variables while debugging
pandas column name assignment read_csv," I have a csv file as follows: I want to save it as a dataframe with x,y axes as names, then plot it. However when I assign x,y I get a messed up DataFrame, what is happening? I've tried without specifying None for header, to no avail. <code>  0 51 102 153 204 25 column_names = ['x','y']x = pd.read_csv('csv-file.csv', header = None, names = column_names)print(x) x y0 0 5 NaN1 1 10 NaN2 2 15 NaN3 3 20 NaN4 4 25 NaN",read_csv doesn't read the column names correctly on this file?
"Python, what's the `Enum` type good for?"," In Python 3.4, we got an Enum lib in the standard library: enum. We can get a backport for enum that works with Python 2.4 to 2.7 (and even 3.1 to 3.3), enum34 in pypi.But we've managed to get along for quite some time without this new module - so why do we now have it?I have a general idea about the purpose of enums from other languages. In Python, it has been common to use a bare class as follows and refer to this as an enum: This can be used in an API to create a canonical representation of the value, e.g.: If this has any criticisms, it's mutable, you can't iterate over it (easily), and how are we to know the semantics of the integer, 2?Then I suppose I could just use something like a namedtuple, which would be immutable? The creation of the namedtuple is a little redundant (we have to write each name twice), and so somewhat inelegant. Getting the ""number"" of the color is also a little inelegant (we have to write colors twice). Value checking will have to be done with strings, which will be a little less efficient.So back to enums.What's the purpose of enums? What value do they create for the language? When should I use them and when should I avoid them? <code>  class Colors: blue = 1 green = 2 red = 3 function_of_color(Colors.green) >>> Colors = namedtuple('Colors', 'blue green red')>>> colors = Colors('blue', 'green', 'red')>>> colorsColors(blue='blue', green='green', red='red')>>> list(colors)['blue', 'green', 'red']>>> len(colors)3>>> colors.blue'blue'>>> colors.index(colors.blue)0","Python, what's the Enum type good for?"
Opencv: 4 camera Bird's Eye view?," I am having quite a lot of trouble thinking of how to make a four camera bird's eye view like that seen in luxury cars. Here is the original that I will be using as an example for this question...Right now, I have made it so the image is skewed using .getPerspectiveTransform but that is just for one image. I obviously need four and am clueless on how to stitch those images together. I am also clueless if this is how the images are supposed to look like. Here is the code I currently have: and here is the end image that I would roughly like to have (A friend put together using Photoshop)... <code>  import cv2 as cvimport numpy as npimg1 = cv.imread(""testBird.jpg"", cv.IMREAD_COLOR)image = np.zeros((700, 700, 3), np.uint8)src = np.array([[0,200],[480,200],[480,360],[0,360]],np.float32)dst = np.array([[0,0],[480,0],[300,360],[180,360]],np.float32)M = cv.getPerspectiveTransform(src, dst)warp = cv.warpPerspective(img1.copy(), M, (480, 360))cv.imshow('transform', warp)cv.waitKey(0)cv.destroyAllWindows()",OpenCV: 4 camera Bird's Eye view?
How to convert an object to a tuple in Python?," If we define __str__ method in a class: We will be able to define how to convert the object to the str class (into a string): I know that we can define the string representation of a custom-defined object, but how do we define the list more precisely, tuple representation of an object? <code>  class Point(): def __init__(self, x, y): self.x = x self.y = y def __str__(self, key): return '{}, {}'.format(self.x, self.y) a = Point(1, 1) b = str(a) print(b)",How to convert an custom class object to a tuple in Python?
How to access the json content of http post request with Klein in python?," I have a simple http client in python that send the http post request like this: On my server side with Flask, I can define a simple function And the data on server will be the same dictionary as the data on the client side.However, now I am moving to Klein, so the server code looks like this: and the request that's been used in Klein does not support the same function. I wonder is there a way to get the json in Klein in the same way I got it in Flask? Thank you for reading this question. <code>  import jsonimport urllib2from collections import defaultdict as dddata = dd(str)req = urllib2.Request('http://myendpoint/test')data[""Input""] = ""Hello World!""response = urllib2.urlopen(req, json.dumps(data)) from flask import request@app.route('/test', methods = ['POST'])def test(): output = dd() data = request.json @app.route('/test', methods = ['POST'])@inlineCallbacksdef test(request): output = dd() data = request.json <=== This doesn't work",Access json content of http post request with Klein in python
Copy channels in Numpy array," I have a RGB image img which is of shape (2560L, 1920L, 3L) and another single channel image mask which is of shape (2560L, 1920L). Now, I want to make this mask of shape (2560L, 1920L, 3L) i.e. I want to copy this single channel data into all the three channels. I'm doing it as follows. Is there a faster way of doing this using numpy? <code>  np.array([[[j,j,j] for j in i] for i in mask])",Broadcast one channel in Numpy array into three channels
PyCharm change working directory of console, How do I change the default working directory when I open a new Python Console? I have multiple projects open in my PyCharm view and the Python Console seems to be defaulting to an arbitrary one. Of course I can work around by modifying sys.path but I want a definite solution. Using Windows. <code> ,Change working directory of console in PyCharm
Use glob to find arbirary length numbers," I'm looking for the glob-pattern that find all files containing the pattern myfile_[SomeNumber].txtMy naive attempt was but this also find all files on the form myfile_[SomeNumber][AnyStuff].txtThis answer shows how to do it for a fixed length, but that not what I want in this case.use python glob to find a folder that is a 14 digit number <code>  glob.glob(""myfile_[0-9]*.txt"")",Use glob to find arbitrary length numbers
python comparing boolean and int using isinstance," Can someone give me an explanation why isinstance() returns True in the following case? I expected False, when writing the code. My guess would be that its Python's internal subclassing, as zero and one - whether float or int - both evaluate when used as boolean, but don't know the exact reason.What would be the most pythonic way to solve such a situation? I could use type() but in most cases this is considered less pythonic. <code>  print isinstance(True, (float, int))True",Comparing boolean and int using isinstance
Shorter version of this numpy matrix indexing," I have the following code in python (numpy array or scipy.sparse.matrices), it works: But it doesn't look elegant. 'a' and 'b' are 1-D boolean mask.'a' has the same length as X.shape[0] and 'b' has the same length as X.shape[1]I tried X[a,b] but it doesn't work.What I am trying to accomplish is to select particular rows and columns at the same time. For example, select row 0,7,8 then from that result select all rows from column 2,3,4How would you make this shorter and more elegant? <code>  X[a,:][:,b]",Shorter version of this numpy array indexing
How do I use seaborns color_palette as a colormap in matplotlib?," Seaborn offers a function called color_palette, which allows you to easily create new color_palettes for plots. I want to transform color_palette to a cmap, which I can use in matplotlib, but I don't see how I can do this. Sadly just functions like ""cubehelix_palette"",""light_palette"", have an ""as_cmap"" paramater. ""color_palette"" doesn't, unfortunately. <code>  colors = [""#67E568"",""#257F27"",""#08420D"",""#FFF000"",""#FFB62B"",""#E56124"",""#E53E30"",""#7F2353"",""#F911FF"",""#9F8CA6""]color_palette = sns.color_palette(colors)",seaborn color_palette as matplotlib colormap
seaborn color_palette as a colormap in matplotlib," Seaborn offers a function called color_palette, which allows you to easily create new color_palettes for plots. I want to transform color_palette to a cmap, which I can use in matplotlib, but I don't see how I can do this. Sadly just functions like ""cubehelix_palette"",""light_palette"", have an ""as_cmap"" paramater. ""color_palette"" doesn't, unfortunately. <code>  colors = [""#67E568"",""#257F27"",""#08420D"",""#FFF000"",""#FFB62B"",""#E56124"",""#E53E30"",""#7F2353"",""#F911FF"",""#9F8CA6""]color_palette = sns.color_palette(colors)",seaborn color_palette as matplotlib colormap
Counting consecutive occurrences of a value," I have a df like so: and I want to return a 1 in a new column if there are two or more consecutive occurrences of 1 in Count and a 0 if there is not. So in the new column each row would get a 1 based on this criteria being met in the column Count. My desired output would then be: I am thinking I may need to use itertools but I have been reading about it and haven't come across what I need yet. I would like to be able to use this method to count any number of consecutive occurrences, not just 2 as well. For example, sometimes I need to count 10 consecutive occurrences, I just use 2 in the example here. <code>  Count1011001110 Count New_Value1 0 0 01 11 10 00 01 11 1 1 10 0",Identifying consecutive occurrences of a value in a column of a pandas DataFrame
Identifying consecutive occurrences of a value," I have a df like so: and I want to return a 1 in a new column if there are two or more consecutive occurrences of 1 in Count and a 0 if there is not. So in the new column each row would get a 1 based on this criteria being met in the column Count. My desired output would then be: I am thinking I may need to use itertools but I have been reading about it and haven't come across what I need yet. I would like to be able to use this method to count any number of consecutive occurrences, not just 2 as well. For example, sometimes I need to count 10 consecutive occurrences, I just use 2 in the example here. <code>  Count1011001110 Count New_Value1 0 0 01 11 10 00 01 11 1 1 10 0",Identifying consecutive occurrences of a value in a column of a pandas DataFrame
how to parallelize many string comparisons in Pandas?," I have the following problemI have a dataframe master that contains sentences, such as For every row in Master, I lookup into another Dataframe slave for the best match using fuzzywuzzy. I use fuzzywuzzy because the matched sentences between the two dataframes could differ a bit (extra characters, etc).For instance, slave could be Here is a fully-functional, wonderful, compact working example :) The 1 million dollars question is: can I parallelize my apply code above? After all, every row in master is compared to all the rows in slave (slave is a small dataset and I can hold many copies of the data into the RAM). I dont see why I could not run multiple comparisons (i.e. process multiple rows at the same time). Problem: I dont know how to do that or if thats even possible.Any help greatly appreciated! <code>  masterOut[8]: original0 this is a nice sentence1 this is another one2 stackoverflow is nice slaveOut[10]: my_value name0 2 hello world1 1 congratulations2 2 this is a nice sentence 3 3 this is another one4 1 stackoverflow is nice from fuzzywuzzy import fuzzimport pandas as pdimport numpy as npimport difflibmaster= pd.DataFrame({'original':['this is a nice sentence','this is another one','stackoverflow is nice']})slave= pd.DataFrame({'name':['hello world','congratulations','this is a nice sentence ','this is another one','stackoverflow is nice'],'my_value': [2,1,2,3,1]})def fuzzy_score(str1, str2): return fuzz.token_set_ratio(str1, str2)def helper(orig_string, slave_df): #use fuzzywuzzy to see how close original and name are slave_df['score'] = slave_df.name.apply(lambda x: fuzzy_score(x,orig_string)) #return my_value corresponding to the highest score return slave_df.ix[slave_df.score.idxmax(),'my_value']master['my_value'] = master.original.apply(lambda x: helper(x,slave))",how to parallelize many (fuzzy) string comparisons using apply in Pandas?
how to parallelize many (fuzzy) string comparisons in Pandas?," I have the following problemI have a dataframe master that contains sentences, such as For every row in Master, I lookup into another Dataframe slave for the best match using fuzzywuzzy. I use fuzzywuzzy because the matched sentences between the two dataframes could differ a bit (extra characters, etc).For instance, slave could be Here is a fully-functional, wonderful, compact working example :) The 1 million dollars question is: can I parallelize my apply code above? After all, every row in master is compared to all the rows in slave (slave is a small dataset and I can hold many copies of the data into the RAM). I dont see why I could not run multiple comparisons (i.e. process multiple rows at the same time). Problem: I dont know how to do that or if thats even possible.Any help greatly appreciated! <code>  masterOut[8]: original0 this is a nice sentence1 this is another one2 stackoverflow is nice slaveOut[10]: my_value name0 2 hello world1 1 congratulations2 2 this is a nice sentence 3 3 this is another one4 1 stackoverflow is nice from fuzzywuzzy import fuzzimport pandas as pdimport numpy as npimport difflibmaster= pd.DataFrame({'original':['this is a nice sentence','this is another one','stackoverflow is nice']})slave= pd.DataFrame({'name':['hello world','congratulations','this is a nice sentence ','this is another one','stackoverflow is nice'],'my_value': [2,1,2,3,1]})def fuzzy_score(str1, str2): return fuzz.token_set_ratio(str1, str2)def helper(orig_string, slave_df): #use fuzzywuzzy to see how close original and name are slave_df['score'] = slave_df.name.apply(lambda x: fuzzy_score(x,orig_string)) #return my_value corresponding to the highest score return slave_df.ix[slave_df.score.idxmax(),'my_value']master['my_value'] = master.original.apply(lambda x: helper(x,slave))",how to parallelize many (fuzzy) string comparisons using apply in Pandas?
"How to get syntax highlighting on Kivy, .kv, file in Pycharm?", What are the steps needed to get syntax highlighting on .kv files in PyCharm on OSX? <code> ,"How to get syntax highlighting on Kivy, .kv, file in Pycharm on OSX?"
Remove Non-Breaking Space from Scrapy result," I am currently trying to scrape a website for article prices but I have run into a problem (after having somehow solved the problem that the prices were dynamically generated, which was a huge pain). I am able to receive the prices and the article names without a problem, but every second result for 'price' is ""\xa0"". I have tried removing it using 'normalize-space()' but to no avail. My code:  <code>  import scrapyfrom scrapy import signalsfrom scrapy.http import TextResponsefrom scrapy.xlib.pydispatch import dispatcherfrom horni.items import HorniItemfrom selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.wait import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECimport timefrom selenium.webdriver.common.keys import Keysclass mySpider(scrapy.Spider): name = ""placeholder"" allowed_domains = [""placeholder.com""] start_urls = [""https://www.placeholder.com""] def __init__(self): self.driver = webdriver.Chrome() dispatcher.connect(self.spider_closed, signals.spider_closed) def spider_closed(self, spider): self.driver.close() def parse(self, response): self.driver.get(""https://www.placeholder.com"") response = TextResponse(url=self.driver.current_url, body=self.driver.page_source, encoding='utf-8') for post in response.xpath('//body'): item = myItem() item['article_name'] = post.xpath('//a[@class=""title-link""]/span/text()').extract() item['price'] = post.xpath('//p[@class=""display-price""]/span]/text()').extract() yield item",Remove/Exclude Non-Breaking Space from Scrapy result
"MVC the simplest example, python"," I'm struggling to understand the MVC pattern. I've been working with MVC frameworks like ASP.NET MVC and Django, but project structure there is pretty much forced, so it really didn't help to understand how to build my own apps based on this pattern. To clear things up i decided to write the simplest example of my understanding of MVC (console program in Python) and figure out if there is anything wrong. So this is my basic structure. What this program will do is display all people that are inside db.txt. I use db.txt(json) to simulate actual database.controller.py view.py model.py So this is the scenario when the user wants to see all people in the db:Is this approach correct? <code>  |- program:| controller.py| model.py| view.py| db.txt #simulates database from model import Personimport viewdef showAll(): #gets list of all Person objects people_in_db = Person.getAll() #calls view return view.showAllView(people_in_db)def start(): view.startView() input = raw_input() if input == 'y': return showAll() else: return view.endView()if __name__ == ""__main__"": #running controller function start() from model import Persondef showAllView(list): print 'In our db we have %i users. Here they are:' % len(list) for item in list: print item.name()def startView(): print 'MVC - the simplest example' print 'Do you want to see everyone in my db?[y/n]'def endView(): print 'Goodbye!' import jsonclass Person(object): def __init__(self, first_name = None, last_name = None): self.first_name = first_name self.last_name = last_name #returns Person name, ex: John Doe def name(self): return (""%s %s"" % (self.first_name,self.last_name)) @classmethod #returns all people inside db.txt as list of Person objects def getAll(self): database = open('db.txt', 'r') result = [] json_list = json.loads(database.read()) for item in json_list: item = json.loads(item) person = Person(item['first_name'], item['last_name']) result.append(person) return result",MVC the simplest example
Change mean pooling to max pooling in a framework-free CNN implementation with Python," I've implemented a simple CNN program with Python that can machine learn on the MNIST data set. I've implemented 3 layers: ConvPoolLayer, which convolves and then does mean pooling FullyConnectedLayer, which is a fully connected hidden layerSoftmaxLayer, which basically gives the softmax output of the network It's in the ConvPoolLayer that I've implemented mean pooling. Here's the line of code that does mean pooling during forward propagation: And here's the equivalent back-propagation code: All it's doing is just upscaling the error.My question is, how do I implement the backpropagation for max pooling without loss in performance? Or, is there a better way to do this without a function call? I get around ~90-95% accuracy after a few iterations with mean pooling, so I'd like to see how max pooling affects performance.If there are any NumPy tricks that can be applied here, I would be glad to learn them. I want to understand myself what happens in a CNN, why things work the way they do, and whether operations can be optimised, so using frameworks isn't an option for me.Thanks for the help!  <code>  # 'activation' is a numpy array of 3D activations from the convolutional code (not shown here) skimage.measure.block_reduce(activation, block_size=(1, 1, 2, 2), func=np.mean) # delta is a numpy array of 3D error matrices back-propagated from the upper layersdelta = delta.repeat(2, axis=2).repeat(2, axis=3)",Convolutional Neural Network (CNN) with max-pooling
How to recognize an music sample using Python?," I recently discovered the GNSDK (Gracenote SDK) that seems to provide examples in several programming languages to recognize music samples by fingerprinting them, and then to request their audio database to get the corresponding artist and song title.But the documentation is horrible.How can I, using Python and the GNSDK, perform a recognition of an audio sample file? There isn't any examples or tutorials in the provided docs.Edit: I really want to use the GNSDK with Python. Don't post anything unrelated, you'll waste your time. <code> ",How to recognize a music sample using Python and Gracenote?
using import in python class," I am completely new to the python class concept. After searching for a solution for some days, I hope I will get help here:I want a python class where I import a function and use it there. The main code should be able to call the function from the class. for that I have two files in the same folder.Thanks to @cdarke, @DeepSpace and @MosesKoledoye, I edited the mistake, but sadly that wasn't it.I still get the Error: @wombatz got the right tip: it must be self.zeit.sleep(2) or Test.zeit.sleep(2). the import could be also done above the class declaration. Test.Py andrun.py when I try to python run.py I get this error: What I understand from the error is that the import in the class is not recognized. But how can I achive that the import in the class is recognized? <code>  test 0Traceback (most recent call last): File ""run.py"", line 3, in <module> foo.doit() File ""/Users/ls/Documents/Entwicklung/RaspberryPi/test/test.py"", line 8, in doit self.timer(5) File ""/Users/ls/Documents/Entwicklung/RaspberryPi/test/test.py"", line 6, in timer zeit.sleep(2)NameError: global name 'zeit' is not defined class Test: import time as zeit def timer(self, count): for i in range(count): print(""test ""+str(i)) self.zeit.sleep(2) <-- self is importent, otherwise, move the import above the class declaration def doit(self): self.timer(5) from test import Testfoo = Test()foo.doit() test 0Traceback (most recent call last): File ""run.py"", line 3, in <module> foo.doit() File ""/Users/ls/Documents/Entwicklung/RaspberryPi/test/test.py"", line 8, in doit self.timer(5) File ""/Users/ls/Documents/Entwicklung/RaspberryPi/test/test.py"", line 6, in timer sleep(2)NameError: global name 'sleep' is not defined",using import inside class
How to convert a Scikit-learn dataset to a Pandas dataset?, How do I convert data from a Scikit-learn Bunch object to a Pandas DataFrame? <code>  from sklearn.datasets import load_irisimport pandas as pddata = load_iris()print(type(data))data1 = pd. # Is there a Pandas method to accomplish this?,How to convert a Scikit-learn dataset to a Pandas dataset
Can we make synchronized queries with SQLAlchemy," I'm trying to translate this SQL query into a Flask-SQLAlchemy call: As you can see, it uses subqueries and, most important part, one of the subqueries is a correlated query (it use d table defined in an outer query).I know how to use subqueries with subquery() function, but I can't find documentation about correlated queries with SQLAlchemy. Do you know a way to do it ? <code>  SELECT *FROM ""ENVOI""WHERE ""ID_ENVOI"" IN (SELECT d.""ID_ENVOI"" FROM ""DECLANCHEMENT"" d WHERE d.""STATUS"" = 0 AND d.""DATE"" = (SELECT max(""DECLANCHEMENT"".""DATE"") FROM ""DECLANCHEMENT"" WHERE ""DECLANCHEMENT"".""ID_ENVOI"" = d.""ID_ENVOI""))",Can we make correlated queries with SQLAlchemy
What does overrideredirect do?," I have come to understand overrideredirect helps removing the default window decorations like the toolbar.What are its other uses? I am not very sure and couldn't find much documentation.I am working on a mac. Using tkinter I want to obtain window which remains maxsize and cannot be resized,which i have achieved using geometry and resizable. What I need now is the guarantee that no random keystrokes by my user can close the window. Will overrideredirect help me in that? Is there any alternative? <code> ",What does overrideredirect() do?
"Parsing this line: list_of_tuples = [(x,y) for x, y, label in data_one]"," As you've already understood I'm a beginner and am trying to understand what the ""Pythonic way"" of writing this function is built on.I know that other threads might include a partial answer to this, but I don't know what to look for since I don't understand what is happening here.This line is a code that my friend sent me, to improve my code which is: The ""improved"" version: I wonder:What is happening here?Is it a better or worse way? since it is ""Pythonic"" I assume it wouldn't work with other languages and so perhaps it's better to get used to the more general way? <code>  import numpy as np#load_data:def load_data(): data_one = np.load ('/Users/usr/... file_name.npy') list_of_tuples = [] for x, y, label in data_one: list_of_tuples.append( (x,y) ) return list_of_tuplesprint load_data() import numpy as np#load_data:def load_data(): data_one = np.load ('/Users/usr.... file_name.npy') list_of_tuples = [(x,y) for x, y, label in data_one] return list_of_tuplesprint load_data()","Understanding this line: list_of_tuples = [(x,y) for x, y, label in data_one]"
Python Ref Counts to Object," In this answer I found a way to get a reference count of objects in Python. They mentioned using sys.getrefcount(). I tried it, but I'm getting an unexpected result. When there is 1 reference, it seems as though the count is 20. Why is that? I looked at the documentation but it doesn't seem to explain the reason.  <code> ",Why are Python Ref Counts to small integers surprisingly high?
exposing c++ class in Python ( only ET_DYN and ET_EXEC can be loaded)," I was looking at here to see how to expose c++ to Python. I have built Python deep learning code which uses boost-python to connect c++ and python and it is running ok, so my system has things for boost-python alread setup.Here is my hello.cpp code (where I used WorldC and WorldP to clearly show the C++ and Python class name usage in the declaration. I don't know why the original web page is using the same class name World to cause confusion to beginners.) and this is how I made hello.so When I run import hello in python, it gives me this error. Can anybody tell me what's wrong?(I'm using anaconda2 under my home directory for python environment, but since my deep learning code builds ok with boost-python, there should be no problem including boost/python.hpp in my system directory) <code>  #include <boost/python.hpp>using namespace boost::python;struct WorldC{ void set(std::string msg) { this->msg = msg; } std::string greet() { return msg; } std::string msg;};BOOST_PYTHON_MODULE(hello){ class_<WorldC>(""WorldP"") .def(""greet"", &WorldC::greet) .def(""set"", &WorldC::set) ;} g++ -shared -c -o hello.so -fPIC hello.cpp -lboostpython -lpython2.7 -I/usr/local/include/python2.7 >>> import helloTraceback (most recent call last): File ""<stdin>"", line 1, in <module>ImportError: ./hello.so: only ET_DYN and ET_EXEC can be loaded",exposing C++ class in Python ( only ET_DYN and ET_EXEC can be loaded)
Plot pie chart and a table using matplotlib in pandas dataframe," I have to plot pie-chart and a table side by side using matplotlib.For drawing the pie-chart, I use the below code: For drawing a table, I use the below code: df_result2 is a table with the list of MachineName's in it.Not sure whether we can place both pie chart and table side by side. Any help would be appreciated. <code>  import matplotlib.pyplot as pltdf1.EventLogs.value_counts(sort=False).plot.pie()plt.show() %%chart table --fields MachineName --data df_result2",Plot pie chart and table of pandas dataframe
Modify third party app in Django project," I'm using a third party app (django-social-share) in my Django project but I need to customize the templates. I'm not sure how to go about doing that--everything I try keeps using the default templates. The current default template is stored in: I've made my custom one in: My templates setting: <code>  django-social-share/django_social_share/templates/django_social_share/templatetags/post_to_facebook.html. {{project_root}}/{{app_name}}/templates/django_social_share/templatetags/post_to_facebook.html TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [], 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', 'django.core.context_processors.request', 'mainsite.context_processors.google_api' ], 'loaders': ( 'django.template.loaders.filesystem.Loader', 'django.template.loaders.app_directories.Loader', 'django.template.loaders.eggs.Loader', ), }, },]",Customize templates in a third party Django app
How do I create a Flask-wtf form that will validate a Boolean field and ensure it is checked?," I am creating a form using Flask-WTForms.I am using a BooleanField so that a user can indicate they agree terms.I cannot validate the BooleanField upon submission to ensure that it has been checked. I have tried using Required(), DataRequired() and custom validation but in each case I have not received a validation error.Here are the nuts and bolts of the application: And here is the index.html template... Any suggestions would be gratefully received. <code>  from flask import Flask, render_template, session, redirect, url_for, flashfrom flask_wtf import Formfrom wtforms import BooleanField, SubmitFieldfrom wtforms.validators import Required, DataRequiredfrom flask_bootstrap import Bootstrapapp = Flask(__name__)app.config['SECRET_KEY'] = 'impossibletoknow'bootstrap = Bootstrap(app)class AgreeForm(Form): agreement = BooleanField('I agree.', validators=[DataRequired()]) submit = SubmitField('Submit')@app.route('/', methods=['GET', 'POST'])def index(): form = AgreeForm() if form.validate_on_submit(): agreement = form.agreement.data if agreement is True: flash('You agreed!') return redirect(url_for('index', form=form)) form.agreement.data = None agreement = False return render_template('index.html', form=form)if __name__ == '__main__': app.run(debug=True) {% import ""bootstrap/wtf.html"" as wtf %}{% block content %}<div class=""container""> {% for message in get_flashed_messages() %} <div class=""alert alert-warning""> <button type=""button"" class=""close"" data-dismiss=""alert"">&times;</button> {{ message }} </div> {% endfor %} {{ wtf.quick_form(form) }}</div>{% endblock %}",Validate that a WTForms BooleanField is checked
Validate that a WTForms BooleanField is checked," I am creating a form using Flask-WTForms.I am using a BooleanField so that a user can indicate they agree terms.I cannot validate the BooleanField upon submission to ensure that it has been checked. I have tried using Required(), DataRequired() and custom validation but in each case I have not received a validation error.Here are the nuts and bolts of the application: And here is the index.html template... Any suggestions would be gratefully received. <code>  from flask import Flask, render_template, session, redirect, url_for, flashfrom flask_wtf import Formfrom wtforms import BooleanField, SubmitFieldfrom wtforms.validators import Required, DataRequiredfrom flask_bootstrap import Bootstrapapp = Flask(__name__)app.config['SECRET_KEY'] = 'impossibletoknow'bootstrap = Bootstrap(app)class AgreeForm(Form): agreement = BooleanField('I agree.', validators=[DataRequired()]) submit = SubmitField('Submit')@app.route('/', methods=['GET', 'POST'])def index(): form = AgreeForm() if form.validate_on_submit(): agreement = form.agreement.data if agreement is True: flash('You agreed!') return redirect(url_for('index', form=form)) form.agreement.data = None agreement = False return render_template('index.html', form=form)if __name__ == '__main__': app.run(debug=True) {% import ""bootstrap/wtf.html"" as wtf %}{% block content %}<div class=""container""> {% for message in get_flashed_messages() %} <div class=""alert alert-warning""> <button type=""button"" class=""close"" data-dismiss=""alert"">&times;</button> {{ message }} </div> {% endfor %} {{ wtf.quick_form(form) }}</div>{% endblock %}",Validate that a WTForms BooleanField is checked
way to convert image straight from url to base64 without saving as a file python, I'm looking to convert a web-based image to base64. I know how to do it currently by saving the image as a .jpg file and then using the base64 library to convert the .jpg file to a base64 string. I'm wondering whether I could skip out the step of saving the image first? Thanks! <code> ,Way to convert image straight from URL to base64 without saving as a file in Python
Apply StandardScaler on a partial part of a data set," I want to use sklearn's StandardScaler. Is it possible to apply it to some feature columns but not others?For instance, say my data is: I fit and transform the data But of course the names are not really integers but strings and I don't want to standardize them. How can I apply the fit and transform methods only on the columns Age and Weight? <code>  data = pd.DataFrame({'Name' : [3, 4,6], 'Age' : [18, 92,98], 'Weight' : [68, 59,49]}) Age Name Weight0 18 3 681 92 4 592 98 6 49col_names = ['Name', 'Age', 'Weight']features = data[col_names] scaler = StandardScaler().fit(features.values)features = scaler.transform(features.values)scaled_features = pd.DataFrame(features, columns = col_names) Name Age Weight0 -1.069045 -1.411004 1.2027031 -0.267261 0.623041 0.0429542 1.336306 0.787964 -1.245657",Apply StandardScaler to parts of a data set
giving a name to a pandas dataframe ?," When I look to a pd df :I tell myself I would like to fill the upper left empty space by a dataframe name. Is that possible ?(Here it would be to put a text on the empty cell on top of ""Lib_ze"" and on left of ""nunique"".) <code> ",giving a name to a pandas dataframe?
How to access application variable from flask in blueprint?," I am using the app factory pattern to set up my Flask application. My app uses the Flask-Babel extension, and that is set up in the factory as well. However, I want to access the extension in a blueprint in order to use it, The factory is in __init__.py. I want to add the following to main.py: Unfortunately, main.py doesn't have access to the babel variable from the application factory. How should I go about solving this?  <code>  def create_app(object_name): app = Flask(__name__) app.config.from_object(object_name) babel = Babel(app) app.register_blueprint(main_blueprint) app.register_blueprint(category_blueprint) app.register_blueprint(item_blueprint) db.init_app(app) return app @babel.localeselectordef get_locale(): if 'locale' in session: return session['locale'] return request.accept_languages.best_match(LANGUAGES.keys())@application.route('/locale/<locale>/', methods=['GET'])def set_locale(locale): session['locale'] = locale redirect_to = request.args.get('redirect_to', '/') return redirect(redirect_to) # Change this to previous url",Access a Flask extension that is defined in the app factory
Split string of digits into 2 lists of odd and even intgeres respectively," Having such code How can accomplish same on fewer lines? <code>  numbers = '1 2 3 4 5 6 7 8'nums = {'evens': [], 'odds': []}for number in numbers.split(' '): if int(number) % 2: nums['odds'].append(number) else: nums['evens'].append(number)",Split string of digits into lists of even and odd integers
Rename downloades files selenium," I'm using selenium to automatically download files in csv format from this page:https://catalog.data.gov/dataset?tags=crimeThis is the code I'm using: Here the download folder is set: How can I select the name with which the file is saved?Can be the name defined at the moment of the download?I meant something like this: <code>  profile = webdriver.FirefoxProfile()profile.set_preference(""browser.download.folderList"", 2)profile.set_preference(""browser.download.manager.showWhenStarting"", False)profile.set_preference(""browser.download.dir"", '/home/luis/Desktop/data/')profile.set_preference(""browser.helperApps.neverAsk.saveToDisk"", ""text/csv"")driver = webdriver.Firefox(firefox_profile=profile)driver.get(url)time.sleep(2)download_button = driver.find_element_by_xpath('//*[@id=""content""]/div[2]/div[2]/section[1]/div[2]/ul/li[14]/div/ul/li[1]/a')download_button.click() profile.set_preference(""browser.download.dir"", '/home/luis/Desktop/data/') For name in names: download_button = driver.find_element_by_xpath('//*[@id=""content""]/div[2]/div[2]/section[1]/div[2]/ul/li[14]/div/ul/li[{}]/a'.format(name)) download_button.click() save_file_as(name)",Rename downloaded files selenium
Flask - Check If Arbitrary URL Matches A url_rule," I have some url paths and want to check if they point to a url rule in my Flask app. How can I check this using Flask?  <code>  from flask import Flask, json, request, Responseapp = Flask('simple_app')@app.route('/foo/<bar_id>', methods=['GET'])def foo_bar_id(bar_id): if request.method == 'GET': return Response(json.dumps({'foo': bar_id}), status=200)@app.route('/bar', methods=['GET'])def bar(): if request.method == 'GET': return Response(json.dumps(['bar']), status=200) test_route_a = '/foo/1' # return foo_bar_id functiontest_route_b = '/bar' # return bar function",Get the Flask view function that matches a url
Create a if else condition column in dask dataframe," I need to create a column which is based on some condition on dask dataframe. In pandas it is fairly straightforward: While in dask I have to do same thing like below: Questions:Is there a better/more straightforward way to achieve it?I can't modify the first dataframe ddf, i need to create ddf1 to se the changes is dask dataframe Immutable object? <code>  ddf['TEST_VAR'] = ['THIS' if x == 200607 else 'NOT THIS' if x == 200608 else 'THAT' if x == 200609 else 'NONE' for x in ddf['shop_week'] ] def f(x): if x == 200607: y= 'THIS' elif x == 200608 : y= 'THAT' else : y= 1 return yddf1 = ddf.assign(col1 = list(ddf.shop_week.apply(f).compute()))ddf1.compute()",Create an if-else condition column in dask dataframe
How Django channels are different than celery?," Recently I came to know about Django channels.Can somebody tell me the difference between channels and celery, as well as where to use celery and channels. <code> ",How are Django channels different than celery?
how to load jinja template directly from filesystem," The jinja API document at pocoo.org states:The simplest way to configure Jinja2 to load templates for your application looks roughly like this: This will create a template environment with the default settings and a loader that looks up the templates in the templates folder inside the yourapplication python package.As it turns out, this isn't so simple because you have to make/install a python package with your templates in it, which introduces a lot of needless complexity, especially if you have no intention of distributing your code. You can refer to SO questions on the topic here and here, but the answers are vague and unsatisfying.What a naive newbie wants to do, obviously, is just load the template directly from the filesystem, not as a resource in a package. How is this done? <code>  from jinja2 import Environment, PackageLoaderenv = Environment(loader=PackageLoader('yourapplication', 'templates'))",How to load jinja template directly from filesystem
Sort pandas DataFrame with function over column values," Based on python, sort descending dataframe with pandas:Given: df then looks like this: I would like to have something like: This should order the complete dataframe with respect to the sum of the squared values of column 'x' and 'y' and give me: Ascending or descending order does not matter. Is there a nice and simple way to do that? I could not yet find a solution. <code>  from pandas import DataFrameimport pandas as pdd = {'x':[2,3,1,4,5], 'y':[5,4,3,2,1], 'letter':['a','a','b','b','c']}df = DataFrame(d) df: letter x y 0 a 2 5 1 a 3 4 2 b 1 3 3 b 4 2 4 c 5 1 f = lambda x,y: x**2 + y**2test = df.sort(f('x', 'y')) test: letter x y 2 b 1 3 3 b 4 2 1 a 3 4 4 c 5 1 0 a 2 5",DataFrame sorting based on a function of multiple column values
"Why is it possible to have low loss, but also very low accuracy in a convulational neural network?"," I am new to machine learning and am currently trying to train a convolutional neural net with 3 convolutional layers and 1 fully connected layer. I am using a dropout probability of 25% and a learning rate of 0.0001. I have 6000 150x200 training images and 13 output classes. I am using tensorflow. I am noticing a trend where my loss steadily decreases, but my accuracy increases only slightly and then drops back down again. My training images are the blue lines and my validation images are the orange lines. The x axis is steps. I am wondering if there is a something I am not understanding or what could be possible causes of this phenomenon? From the material I have read, I assumed low loss meant high accuracy. Here is my loss function. <code>  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))","Why is it possible to have low loss, but also very low accuracy, in a convolutional neural network?"
difference between scikit-learn and sklearn," On OS X 10.11.6 and python 2.7.10 I need to import from sklearn manifold. I have numpy 1.8 Orc1, scipy .13 Ob1 and scikit-learn 0.17.1 installed.I used pip to install sklearn(0.0), but when I try to import from sklearn manifold I get the following: Traceback (most recent call last): File """", line 1, in File ""/Library/Python/2.7/site-packages/sklearn/init.py"", line 57, in from .base import clone File ""/Library/Python/2.7/site-packages/sklearn/base.py"", line 11, in from .utils.fixes import signature File ""/Library/Python/2.7/site-packages/sklearn/utils/init.py"", line 10, in from .murmurhash import murmurhash3_32 File ""numpy.pxd"", line 155, in init sklearn.utils.murmurhash (sklearn/utils/murmurhash.c:5029) ValueError: numpy.dtype has the wrong size, try recompiling.What is the difference between scikit-learn and sklearn? Also, I cant import scikit-learn because of a syntax error <code> ",Difference between scikit-learn and sklearn
Pythonic way to hain python generator function to form a pipeline," I'm doing a pipeline code refactoring using python.Assuming we have a series of generator functions and we want to chain those to form a data processing pipeline. Example: Output: I do not think foo3(foo2(foo1(range(0, 5)))) is a pythonic way to achieve my pipeline goal. Especially when the number of stages in the pipeline is large.I wish I could rewrite it like chain in jquery. Something similar to : Or maybe But I'm new to generator topic and couldn't find a way to achieve this.Any help will be welcome. <code>  #!/usr/bin/pythonimport itertoolsdef foo1(g): for i in g: yield i + 1def foo2(g): for i in g: yield 10 + idef foo3(g): for i in g: yield 'foo3:' + str(i)res = foo3(foo2(foo1(range(0, 5))))for i in res: print i foo3:11foo3:12foo3:13foo3:14foo3:15 range(0, 5).foo1().foo2().foo3() l = [range(0, 5), foo1, foo2, foo3]res = runner.run(l)",Pythonic way to chain python generator function to form a pipeline
Grouping of Function Using Classes Python," I have been a Python Scientific Programmer for a few years now, and I find myself coming to a sort specific problem as my programs get larger and larger. I am self taught so I have never had any formal training and spent any time really on 'conventions' of coding in Python ""properly"".Anyways, to the point, I find myself always creating a utils.py file that I store all my defined functions in that my programs use. I then find myself grouping these functions into their respective purposes. One way of I know of grouping things is of course using Classes, but I am unsure as to whether my strategy goes against what classes should actually be used for.Say I have a bunch of functions that do roughly the same thing like this: Now obviously these 4 functions can be seen as doing two seperate purposes one is calculation and the other is formatting. This is what logic tells me to do, but I have to work around it since I don't want to initialise a variable that corresponds to the class evertime. This way I have now 'grouped' these methods together into a nice package that makes finding desired methods much faster based on their role in the program. However that being said, I feel this is a very 'unpython-y' way to do things, and it just feels messy. Am I going about thinking this the right way or is there an alternate method? <code>  def add(a,b): return a + bdef sub(a,b): return a -bdef cap(string): return string.title()def lower(string): return string.lower() class calc_funcs(object): def __init__(self): pass @staticmethod def add(a,b): return a + b @staticmethod def sub(a, b): return a - bclass format_funcs(object): def __init__(self): pass @staticmethod def cap(string): return string.title() @staticmethod def lower(string): return string.lower() print calc_funcs.add(1,2)print format_funcs.lower(""Hello Bob"")",Grouping Functions by Using Classes in Python
Python Panda Rounding up a column," I am new to pandas python and I am having difficulties trying to round up all the values in the column. For example, I tried using my current code below, but it gave me: AttributeError: 'str' object has no attribute 'rint' <code>  Example88.988.190.245.1 df.Example = df.Example.round()",Rounding up a column
Performance issues with pandas and filtering on datetime index," I've a pandas dataframe with a datetime64 object on one of the columns. My application follows the (simplified) structure below: I noticed when profiling the app, that calling the df[(df.time > start) & (df.time <= end)] causes the highest performance issues and I was wondering if there is a way to speed up these calls?EDIT: I'm adding some more info about the use-case here (also, source is available at: https://github.com/jmelett/pyFxTrader)The application will accept a list of instruments (e.g. EUR_USD, USD_JPY, GBP_CHF) and then pre-fetch ticks/candles for each one of them and their timeframes (e.g. 5 minutes, 30 minutes, 1 hour etc.). The initialised data is basically a dict of Instruments, each containing another dict with candle data for M5, M30, H1 timeframes. Each ""timeframe"" is a pandas dataframe like shown at the topA clock simulator is then used to query the individual candles for the specific time (e.g. at 15:30:00, give me the last x ""5-minute-candles"") for EUR_USDThis piece of data is then used to ""simulate"" specific market conditions (e.g. average price over last 1 hour increased by 10%, buy market position) <code>  time volume complete closeBid closeAsk openBid openAsk highBid highAsk lowBid lowAsk closeMid0 2016-08-07 21:00:00+00:00 9 True 0.84734 0.84842 0.84706 0.84814 0.84734 0.84842 0.84706 0.84814 0.847881 2016-08-07 21:05:00+00:00 10 True 0.84735 0.84841 0.84752 0.84832 0.84752 0.84846 0.84712 0.8482 0.847882 2016-08-07 21:10:00+00:00 10 True 0.84742 0.84817 0.84739 0.84828 0.84757 0.84831 0.84735 0.84817 0.8477953 2016-08-07 21:15:00+00:00 18 True 0.84732 0.84811 0.84737 0.84813 0.84737 0.84813 0.84721 0.8479 0.8477154 2016-08-07 21:20:00+00:00 4 True 0.84755 0.84822 0.84739 0.84812 0.84755 0.84822 0.84739 0.84812 0.8478855 2016-08-07 21:25:00+00:00 4 True 0.84769 0.84843 0.84758 0.84827 0.84769 0.84843 0.84758 0.84827 0.848066 2016-08-07 21:30:00+00:00 5 True 0.84764 0.84851 0.84768 0.84852 0.8478 0.84857 0.84764 0.84851 0.8480757 2016-08-07 21:35:00+00:00 4 True 0.84755 0.84825 0.84762 0.84844 0.84765 0.84844 0.84755 0.84824 0.84798 2016-08-07 21:40:00+00:00 1 True 0.84759 0.84812 0.84759 0.84812 0.84759 0.84812 0.84759 0.84812 0.8478559 2016-08-07 21:45:00+00:00 3 True 0.84727 0.84817 0.84743 0.8482 0.84743 0.84822 0.84727 0.84817 0.84772 class Runner(): def execute_tick(self, clock_tick, previous_tick): candles = self.broker.get_new_candles(clock_tick, previous_tick) if candles: run_calculations(candles)class Broker(): def get_new_candles(clock_tick, previous_tick) start = previous_tick - timedelta(minutes=1) end = clock_tick - timedelta(minutes=3) return df[(df.time > start) & (df.time <= end)]",Performance issues with pandas and filtering on datetime column
python numpy filter two dimentional array by condition," Python newbie here, I have read Filter rows of a numpy array? and the doc but still can't figure out how to code it the python way.Example array I have: (the real data is 50000 x 10) I need to find all rows in a with a[:, 1] in filter. Expected result: My current code is this: It works okay but I have read somewhere that it is not efficient. What is the proper numpy method for this?Edit:Thanks for all the correct answers! Unfortunately I can only mark one as accepted answer. I am surprised that numpy.in1d is not turned up in google searchs for numpy filter 2d array. <code>  a = numpy.asarray([[2,'a'],[3,'b'],[4,'c'],[5,'d']])filter = ['a','c'] [[2,'a'],[4,'c']] numpy.asarray([x for x in a if x[1] in filter ])",Python numpy filter two-dimensional array by condition
"unordered_map<int, vector<float>> equivelent in python"," I need a structure in Python which maps an integer index to a vector of floating point numbers. My data is like: If I were to write this in C++ I would use the following code for define / add / access as follows: What is the equivalent structure of this in Python?  <code>  [0] = {1.0, 1.0, 1.0, 1.0}[1] = {0.5, 1.0} std::unordered_map<int, std::vector<float>> VertexWeights;VertexWeights[0].push_back(0.0f);vertexWeights[0].push_back(1.0f);vertexWeights[13].push_back(0.5f);std::cout <<vertexWeights[0][0];","unordered_map<int, vector<float>> equivalent in Python"
"Simple, Ugly Function"," I wrote a function that takes a degree and returns the orientation as 'N', 'NE', ...etc. Very simple, but it's ugly - is there any way to rewrite this to make it...prettier? <code>  def orientation(tn): if 23 <= tn <= 67: o = 'NE' elif 68 <= tn <= 113: o = 'E' elif 114 <= tn <= 158: o = 'SE' elif 159 <= tn <= 203: o = 'S' elif 204 <= tn <= 248: o = 'SW' elif 249 <= tn <= 293: o = 'W' elif 294 <= tn <= 338: o = 'NW' else: o = 'N' return o","Simple, ugly function to produce an orientation from an angle."
PIP library function draw.text is Saving my string seprately," I have python code. """" is a string, consisting of two alphabets and .but they are combined in arabic. i am giving this word to PIL library function. But it is saving image separately both alphabets. How can i combine them.? output:that is and .  <code>  data2= """" draw.text(((W-w)/2,(H-h)/2),data2,(0,0,0),font=font)draw = ImageDraw.Draw(img)img.save(""abc""+"".png"")",PIL draw.text() is displaying string containing arabic ligature as two separate glyphs
Simple Django JSON Response," I have a file on my computer that I'm trying to serve up as JSON from a django view. What I get back is the path to the file when navigating to the URL What am I doing wrong here? <code>  def serve(request): file = os.path.join(BASE_DIR, 'static', 'files', 'apple-app-site-association') response = HttpResponse(content=file) response['Content-Type'] = 'application/json' /Users/myself/Developer/us/www/static/files/apple-app-site-association",How to put a JSON file's content in a response
install python modules in mac osx," I am new to python. I am using python 3.5 on mac os x el capitan.I tried using the command 'pip install requests' in the python interpreter IDLE. But it throws invalid 'syntax error'.I read about installing modules is only possible in the command line.So I moved to TERMINAL, but no command is working here also.(I tried 'python -m pip install requests')I read that mac os x comes with python 2.7 already installed and I ran 'easy_install pip' but it also works on the 2.7 version.Then there's discussion about the PATH settings also.Can please anybody explain to me how I can use my current version in TERMINAL window and what is the PATH scenario.I am familiar with the environment variable settings and adding pythonpath in windows but not on mac. <code> ",Install python modules in mac osx
X and Y or Z - ternary operator in Python," In Java or C we have <condition> ? X : Y, which translates into Python as X if <condition> else Y.But there's also this little trick: <condition> and X or Y. While I understand that it's equivalent to the aforementioned ternary operators, I find it difficult to grasp how and and or operators are able to produce correct result. What's the logic behind this? <code> ",X and Y or Z - ternary operator
mongoengine - do like/regex search," I know I can do a glob-type search on mongodb: or How do I do this with mongoengine without using a raw query (which is apparently the only way based on my searches)?I've blindly tried several variations like: etc, to no avail... <code>  db.person.find({ name: /*.bob.*/ }) db.person.find({ name: { $regex: '*.bob.*' }}) Person.objects(name='/.*bob.*/')Person.objects(name='/\.*bob\.*/')Person.objects(name='.*bob.*')Person.objects(name='\\.*bob\\.*')","python, mongoengine - do like/regex search"
ord function in python2.7 and python 3.4 are differenty?," I have been running a script where I use the ord() function and for whatever the reason in python 2.7, it accepts the unicode string character just as it requires and outputs an integer.In python 3.4, this is not so much the case. This is the output of error that is being produced : When I look in both documentations, the ord function is explained to be doing the same exact thing. This is the code that I am using for both python versions: Can anyone explain why python3.4 it says that c is an integer, rather than in Python 2.7 where it is actually a string, just as the ord() function requires? <code>  Traceback (most recent call last): File ""udpTransfer.py"", line 38, in <module> buf.append(ord(c))TypeError: ord() expected string of length 1, but int found import socket,sys, ast , os, structfrom time import ctimeimport timeimport csv# creating the udo socket necessary to receive datasock = socket.socket(socket.AF_INET,socket.SOCK_DGRAM)ip = '192.168.10.101' #i.p. of our computerport = 20000 # socket port opened to connect from the matlab udp send data streamserver_address = (ip, port)sock.bind(server_address) # bind socketsock.settimeout(2) # sock configurationsock.setblocking(1)print('able to bind')ii = 0shotNummer = 0client = ''Array = []byte = 8192filename = time.strftime(""%d_%m_%Y_%H-%M-%S"")filename = filename + '.csv'try : with open(filename,'wb') as csvfile : spamwriter = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL) # spamwriter.writerow((titles)) # as long as data comes in, well take it while True: data,client = sock.recvfrom(byte) buf = [] values = [] for c in data: # print(type(c)) buf.append(ord(c)) if len(buf) == 4 : ###",ord function in python2.7 and python 3.4 are different?
Python mixed string and numerical sort with regex or similar," How do I sort this list via the numerical values? Is a regex required to remove the numbers or is there a more Pythonic way to do this? Desired output is as follows: <code>  to_sort['12-foo', '1-bar', '2-bar', 'foo-11', 'bar-3', 'foo-4', 'foobar-5', '6-foo', '7-bar'] 1-bar2-barbar-3foo-4foobar-56-foo7-barfoo-1112-foo",Sort list of mixed strings based on digits
python shapely: check if a polygon is a multipolygon," How can I check if a polygon entity is actually a multipolygon?I've tried: but then get the error: I've tried Nill, None and others, nothing worked. <code>  if len(polygon) > 1: TypeError: object of type 'Polygon' has no len()",Check if a polygon is a multipolygon in Shapely
Sympy: Derivative of summations," I am using sympy from time to time, but am not very good at it. At the moment I am stuck with defining a list of indexed variables, i.e. n1 to nmax and performing a summation on it. Then I want to be able to take the derivative:So far I tried the following: However, if i try to take the derivative with respect to one variable, this fails: I also tried to avoid working with IndexedBase. However, here already the summation fails because mixing python tuples and the sympy summation.How I can perform indexedbase derivatives or some kind of workaround? <code>  numSpecies = 10n = IndexedBase('n')i = symbols(""i"",cls=Idx)nges = summation(n[i],[i,1,numSpecies]) diff(nges,n[5]) numSpecies = 10n = symbols('n0:%d'%numSpecies)k = symbols('k',integer=True)ntot = summation(n[k],[k,0,numSpecies])",Derivative of summations
what is calculator mode in python," I don't know about the meaning of calculator mode in Python, and I've put the portion of documentation below.If you quit from the Python interpreter and enter it again, the definitions you have made (functions and variables) are lost. Therefore, if you want to write a somewhat longer program, you are better off using a text editor to prepare the input for the interpreter and running it with that file as input instead. This is known as creating a script. As your program gets longer, you may want to split it into several files for easier maintenance. You may also want to use a handy function that youve written in several programs without copying its definition into each program.To support this, Python has a way to put definitions in a file and use them in a script or in an interactive instance of the interpreter. Such a file is called a module; definitions from a module can be imported into other modules or into the main module (the collection of variables that you have access to in a script executed at the top level and in calculator mode).(Emphasis mine)Here's the original document. <code> ",What is calculator mode?
(Python) How do i search directories and find files that match regex?," I recently started getting into Python and I am having a hard time searching through directories and matching files based on a regex that I have created. Basically I want it to scan through all the directories in another directory and find all the files that ends with .zip or .rar or .r01 and then run various commands based on what file it is.  <code>  import os, rerootdir = ""/mnt/externa/Torrents/completed""for subdir, dirs, files in os.walk(rootdir): if re.search('(w?.zip)|(w?.rar)|(w?.r01)', files): print ""match: "" . files",How do i search directories and find files that match regex?
coloring cells in excel with python," I need some help here. So i have something like this So basically this program load the ejemplo.xlsx (ejemplo is example in Spanish, just the name of the file) into df (a DataFrame), then checks for duplicate values in a specific column. It deletes the duplicates and saves the file again. That part works correctly. The problem is that instead of removing duplicates, I need highlight the cells containing them with a different color, like yellow. <code>  import pandas as pdpath = '/Users/arronteb/Desktop/excel/ejemplo.xlsx'xlsx = pd.ExcelFile(path)df = pd.read_excel(xlsx,'Sheet1')df['is_duplicated'] = df.duplicated('#CSR')df_nodup = df.loc[df['is_duplicated'] == False]df_nodup.to_excel('ejemplo.xlsx', encoding='utf-8')",coloring cells in excel with pandas
How can I write 1 byte to a binary file in python," I've tried everything to write just one byte to a file in python. will output '0x00 0x0a' will output '0x00 0x0a 0x00 0x00'I want to write a single byte with the value 10 to the file. <code>  i = 10fh.write( six.int2byte(i) ) fh.write( struct.pack('i', i) )",How to write 1 byte to a binary file?
Inserting an element before each element of a list in Python," I'm looking to insert a constant element before each of the existing element of a list, i.e. go from: to: I've tried using list comprehensions but the best thing I can achieve is an array of arrays using this statement: Which results in this: So not exactly what I want. Can it be achieved using list comprehension? Just in case it matters, I'm using Python 3.5. <code>  ['foo', 'bar', 'baz'] ['a', 'foo', 'a', 'bar', 'a', 'baz'] [['a', elt] for elt in stuff] [['a', 'foo'], ['a', 'bar'], ['a', 'baz']]",Inserting an element before each element of a list
show distinct column values in pyspark dataframe: python," With pyspark dataframe, how do you do the equivalent of Pandas df['col'].unique().I want to list out all the unique values in a pyspark dataframe column.Not the SQL type way (registertemplate then SQL query for distinct values).Also I don't need groupby then countDistinct, instead I want to check distinct VALUES in that column. <code> ",Show distinct column values in pyspark dataframe
How to penalize predictions ? Binary Cross Entropy and Conv nets," How to implement Weighted Binary CrossEntropy on theano?My Convolutional neural network only predict 0 ~~ 1 (sigmoid).I want to penalize my predictions in this way :Basically, i want to penalize MORE when the model predicts 0 but the truth was 1.Question : How can I create this Weighted Binary CrossEntropy function using theano and lasagne ?I tried this below But I get this error below :TypeError: New shape in reshape must be a vector or a list/tuple of scalar. Got Subtensor{int64}.0 after conversion to a vector.Reference : https://github.com/fchollet/keras/issues/2115Reference : https://groups.google.com/forum/#!topic/theano-users/R_Q4uG9BXp8 <code>  prediction = lasagne.layers.get_output(model)import theano.tensor as Tdef weighted_crossentropy(predictions, targets): # Copy the tensor tgt = targets.copy(""tgt"") # Make it a vector # tgt = tgt.flatten() # tgt = tgt.reshape(3000) # tgt = tgt.dimshuffle(1,0) newshape = (T.shape(tgt)[0]) tgt = T.reshape(tgt, newshape) #Process it so [index] < 0.5 = 0 , and [index] >= 0.5 = 1 # Make it an integer. tgt = T.cast(tgt, 'int32') weights_per_label = theano.shared(lasagne.utils.floatX([0.2, 0.4])) weights = weights_per_label[tgt] # returns a targets-shaped weight matrix loss = lasagne.objectives.aggregate(T.nnet.binary_crossentropy(predictions, tgt), weights=weights) return lossloss_or_grads = weighted_crossentropy(prediction, self.target_var)",How to implement Weighted Binary CrossEntropy on theano?
how to get the specific c compiler type from Python distutils?," I'd like to check the system's C compiler in Python so that I can add library links accordingly to compile my Cython code.I understand distutils.ccompiler.get_default_compiler() or something like compiler.compiler_type would return a compiler name. But it is too coarse just like ""unix"", etc.What I need is more specific information such as ""gcc"", ""icc"", ""clang"", etc., which are all shown as ""unix"" using the methods above.One possible way to get the information is to check the system's environment variable CC via os.environ[""CC""], but it is not guaranteed that every system has CC defined so it is not a universal solution.So, what should I do then? Thanks in advance!  <code> ",How to get the specific C compiler type from Python distutils?
Why calling Bootstrap(app)?," I'm learning Flask web development, and the tutorial I'm following introduces an extension called Flask-Bootstrap. To use this extension, you must initialize it first, like this: Weirdly enough to me, the variable bootstrap is not used in the rest of my module. However, if I comment out this line, a jinja2.exceptions.TemplateNotFound exception will be raised. Also, the templates used start with this line: But I don't have a directory named /bootstrap under /templates!I want to know what's going on:What does the bootstrap = Bootstrap(app) line do?Where does bootstrap/base.html reside?  <code>  from flask_bootstrap import Bootstrap# ...bootstrap = Bootstrap(app) {% extends ""bootstrap/base.html"" %}",How Flask-Bootstrap works?
Change an html line with selenium," I lost my credentials.. so I'm creating this new thread. The old question it here if it helps: How to click a button to vote with pythonI'd like to change this line: to this: So that the vote is set changing vote-link up to vote-link up voted.But the problem is that in that site, there are severals items to vote, and the element ""data-faucet"" changes. If I use this script: But it obsiusly doesn't print anything, cause the first attribute value has another number. How can I select my line with the input of the number of data-faucet element, so I can replace it with vote-link up voted?I only can do this selenium? Is there another way without using a real browser?Anyway, this is the structure of the webpage: The site is this: https://faucetbox.com/en/list/BTC <code>  <a data-original-title=""I&nbsp;like&nbsp;this&nbsp;faucet"" href=""#"" class=""vote-link up"" data-faucet=""39274"" data-vote=""up"" data-toggle=""tooltip"" data-placement=""top"" title=""""><span class=""glyphicon glyphicon-thumbs-up""></span></a> <a data-original-title=""I&nbsp;like&nbsp;this&nbsp;faucet"" href=""#"" class=""vote-link up voted"" data-faucet=""39274"" data-vote=""up"" data-toggle=""tooltip"" data-placement=""top"" title=""""><span class=""glyphicon glyphicon-thumbs-up""></span></a> from selenium import webdriverdriver = webdriver.Firefox()driver.get(""linkurl"")element = driver.find_element_by_css_selector("".vote-link.up"")element_attribute_value = element.get_attribute(""data-faucet"")if element_attribute_value == ""39274"": print (""Value: {0}"".format(element_attribute_value))driver.quit() <html><head></head><body role=""document""><div id=""static page"" class=""container-fluid""><div id=""page"" class=""row""></div><div id=""faucets-list""><tbody><tr class=""""></tr><tr class=""""></tr><tr class=""""></tr><tr class=""""></tr># an infinite number of nodes, until there's mine<tr class=""""><td class=""vote-col""><div class=""vote-box""><div class=""vote-links""><a class=""vote-link up"" data-original-title=""I like this faucet"" href=""#"" data-faucet""39274"" data-vote""up"" data-toggle""tooltip"" data-placement=""top"" title=""""></a>",How to change element class attribute value using selenium
"See stacktrace of hanging Python in futux(..., FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, ...)"," A Python process hangs in futex(): I want to see the stacktrace if the hanging process.Unfortunately ctrl-c does not work :-(How can I see the stacktrace if Python hangs like this? <code>  root@pc:~# strace -p 9042strace: Process 9042 attachedfutex(0x1e61900, FUTEX_WAIT_BITSET_PRIVATE|FUTEX_CLOCK_REALTIME, 0, NULL, ffffffff","See stacktrace of hanging Python in futex(..., FUTEX_WAIT_BITSET_PRIVATE|...)"
"pyspark,spark: how to select last row and also how to access pyspark dataframe by index", From a PySpark SQL dataframe like How to get the last row.(Like by df.limit(1) I can get first row of dataframe into new dataframe).And how can I access the dataframe rows by index.like row no. 12 or 200 .In pandas I can do I am just curious how to access pyspark dataframe in such ways or alternative ways.Thanks <code>  name age cityabc 20 Adef 30 B df.tail(1) # for last rowdf.ix[rowno or index] # by indexdf.loc[] or by df.iloc[],How to select last row and also how to access PySpark dataframe by index?
Why NotImplemented evaluated mutiple times with __eq__ operator," Dont mix up apples and orangesThe problemIm playing with the __eq__ operator and the NotImplemented value.Im trying to understand whats happen when obj1.__eq__(obj2) returns NotImplemented and obj2.__eq__(obj1) also returns NotImplemented.According to the answer to Why return NotImplemented instead of raising NotImplementedError, and the detailed article How to override comparison operators in Python in the ""LiveJournal"" blog, the runtime should fall back to the built-in behavior (which is based on identity for == and !=).Code sampleBut, trying the example bellow, it seems that I have multiple calls to __eq__ for each pair of objects. Expected behaviorI expected to have only: Then falling back to identity comparison id(apple) == id(orange) -> False. <code>  class Apple(object): def __init__(self, color): self.color = color def __repr__(self): return ""<Apple color='{color}'>"".format(color=self.color) def __eq__(self, other): if isinstance(other, Apple): print(""{self} == {other} -> OK"".format(self=self, other=other)) return self.color == other.color print(""{self} == {other} -> NotImplemented"".format(self=self, other=other)) return NotImplementedclass Orange(object): def __init__(self, usage): self.usage = usage def __repr__(self): return ""<Orange usage='{usage}'>"".format(usage=self.usage) def __eq__(self, other): if isinstance(other, Orange): print(""{self} == {other}"".format(self=self, other=other)) return self.usage == other.usage print(""{self} == {other} -> NotImplemented"".format(self=self, other=other)) return NotImplemented>>> apple = Apple(""red"")>>> orange = Orange(""juice"")>>> apple == orange<Apple color='red'> == <Orange usage='juice'> -> NotImplemented<Orange usage='juice'> == <Apple color='red'> -> NotImplemented<Orange usage='juice'> == <Apple color='red'> -> NotImplemented<Apple color='red'> == <Orange usage='juice'> -> NotImplementedFalse <Apple color='red'> == <Orange usage='juice'> -> NotImplemented<Orange usage='juice'> == <Apple color='red'> -> NotImplemented",Why is NotImplemented evaluated multiple times with __eq__ operator
Why is NotImplemented evaluated mutiple times with __eq__ operator," Dont mix up apples and orangesThe problemIm playing with the __eq__ operator and the NotImplemented value.Im trying to understand whats happen when obj1.__eq__(obj2) returns NotImplemented and obj2.__eq__(obj1) also returns NotImplemented.According to the answer to Why return NotImplemented instead of raising NotImplementedError, and the detailed article How to override comparison operators in Python in the ""LiveJournal"" blog, the runtime should fall back to the built-in behavior (which is based on identity for == and !=).Code sampleBut, trying the example bellow, it seems that I have multiple calls to __eq__ for each pair of objects. Expected behaviorI expected to have only: Then falling back to identity comparison id(apple) == id(orange) -> False. <code>  class Apple(object): def __init__(self, color): self.color = color def __repr__(self): return ""<Apple color='{color}'>"".format(color=self.color) def __eq__(self, other): if isinstance(other, Apple): print(""{self} == {other} -> OK"".format(self=self, other=other)) return self.color == other.color print(""{self} == {other} -> NotImplemented"".format(self=self, other=other)) return NotImplementedclass Orange(object): def __init__(self, usage): self.usage = usage def __repr__(self): return ""<Orange usage='{usage}'>"".format(usage=self.usage) def __eq__(self, other): if isinstance(other, Orange): print(""{self} == {other}"".format(self=self, other=other)) return self.usage == other.usage print(""{self} == {other} -> NotImplemented"".format(self=self, other=other)) return NotImplemented>>> apple = Apple(""red"")>>> orange = Orange(""juice"")>>> apple == orange<Apple color='red'> == <Orange usage='juice'> -> NotImplemented<Orange usage='juice'> == <Apple color='red'> -> NotImplemented<Orange usage='juice'> == <Apple color='red'> -> NotImplemented<Apple color='red'> == <Orange usage='juice'> -> NotImplementedFalse <Apple color='red'> == <Orange usage='juice'> -> NotImplemented<Orange usage='juice'> == <Apple color='red'> -> NotImplemented",Why is NotImplemented evaluated multiple times with __eq__ operator
How to use AirFlow to run a list of python tasks?," I have a series of Python tasks inside a folder of python files: file1.py, file2.py, ...I read the Airflow docs, but I don't see how to specify the folder and filename of the python files in the DAG?I would like to execute those python files (not the Python function through Python Operator).Task1: Execute file1.py (with some import package)Task2: Execute file2.py (with some other import package)It would be helpful. Thanks, regards <code> ",How to use AirFlow to run a folder of python files?
"Massive minimisation to fill in matrix (in Matlab, but open to alternatives)"," Consider the following (excel) dataset: My goal is to fill in missing values using the following condition: Denote as R the pairwise correlation between the above two columns (around 0.68). Denote as R* the correlation after the empty cells have been filled in. Fill in the table so that (R - R*)^2 = 0. This is, I want to keep the correlation structure of the data intact.So far I have done it using Matlab: where the function my_correl is: This function computes the correlation manually, where every missing data is a variable x(i).The problem: my actual dataset has more than 20,000 observations. There is no way I can create that rho formula manually.How can I fill in my dataset?Note 1: I am open to use alternative languages like Python, Julia, or R. Matlab it's just my default one.Note 2: a 100 points bounty will be awarded to the answer. Promise from now. <code>  m | r----|------2.0 | 3.30.8 | | 4.01.3 | 2.1 | 5.2 | 2.3 | 1.92.5 | 1.2 | 3.02.0 | 2.6 clear all;m = xlsread('data.xlsx','A2:A11') ;r = xlsread('data.xlsx','B2:B11') ;rho = corr(m,r,'rows','pairwise');x0 = [1,1,1,1,1,1];lb = [0,0,0,0,0,0];f = @(x)my_correl(x,rho);SOL = fmincon(f,x0,[],[],[],[],lb) function X = my_correl(x,rho)sum_m = (11.9 + x(1) + x(2) + x(3));sum_r = (22.3 + x(1) + x(2) + x(3));avg_m = (11.9 + x(1) + x(2) + x(3))/8;avg_r = (22.3 + x(4) + x(5) + x(6))/8;rho_num = 8*(26.32 + 4*x(1) + 2.3*x(2) + 1.9*x(3) + 0.8*x(4) + 1.3*x(5) + 2.5*x(6)) - sum_m*sum_r;rho_den = sqrt(8*(22.43 + (4*x(1))^2 + (2.3*x(2))^2 + (1.9*x(3))^2) - sum_m^2)*sqrt(8*(78.6 + (0.8*x(4))^2 + (1.3*x(5))^ + (2.5*x(6))^2) - sum_r^2);X = (rho - rho_num/rho_den)^2;end","Impute missing data, while forcing correlation coefficient to remain the same"
Override the python class patch with method patch (decorator)," I have several test methods in a class that use one type of patching for a object, so I have patched with class decorator. For a single another method i want to patch the same object differently. I tried the following approach, but the patch made as class decorator is in effect despite the method itself being decorated with different patch. I expected method patch to override class patch. Why is this not the case?In this particular case I can remove class patch and patch individual methods, but that would be repetitive. How can I implement such overriding (method overrides class patch) mechanism?  <code>  from unittest TestCasefrom unittest import mock@mock.patch('my_module.cls.method', mock.Mock(side_effect=RuntimeError('testing'))class SwitchViewTest(TestCase): def test_use_class_patching(self): # several other methods like this # test code .. @mock.patch('my_module.cls.method', mock.Mock(side_effect=RuntimeError('custom')) def test_override_class_patching(self): # test code ...",Override the class patch with method patch (decorator)
Is there any equivalent to the Perl's \K backslash sequence in Python?," Perl's regular expressions have the \K backslash sequence: \K This appeared in perl 5.10.0. Anything matched left of \K is not included in $&, and will not be replaced if the pattern is used in a substitution. This lets you write s/PAT1 \K PAT2/REPL/x instead of s/(PAT1) PAT2/${1}REPL/x or s/(?<=PAT1) PAT2/REPL/x. Mnemonic: Keep.Is there anything equivalent in Python? <code> ",Is there any equivalent to the Perl regexes' \K backslash sequence in Python?
"Pycharm: How to use the green ""Attach Debugger"" button in python console"," I was wondering how to use this green bug button on the left side of Python console. I've been searched official documents but there seems to be no description of this button. Note I am asking the button on the left side, not the button on the right top corner.I'm using IPython console and found %debug magic is not available anymore. I'd like to know what's the usage of that button and how to use %debug magic in IPython console. <code> ","How to use the green ""Attach Debugger"" button in Python console using PyCharm"
python string concatenation operation," I am reading The Hitchhikers Guide to Python and there is a short code snippet The author pointed out that ''.join() is not always faster than +, so he is not against using + for string concatenation. But why is foo += 'ooo' bad practice whereas foobar=foo+bar is considered good? is foo += bar good?is foo = foo + 'ooo' good?Before this code snippet, the author wrote: One final thing to mention about strings is that using join() is not always best. In the instances where you are creating a new string from a pre-determined number of strings, using the addition operator is actually faster, but in cases like above or in cases where you are adding to an existing string, using join() should be your preferred method. <code>  foo = 'foo'bar = 'bar'foobar = foo + bar # This is goodfoo += 'ooo' # This is bad, instead you should do:foo = ''.join([foo, 'ooo'])",Is python += string concatenation bad practice?
python string concatenation operation bad practice," I am reading The Hitchhikers Guide to Python and there is a short code snippet The author pointed out that ''.join() is not always faster than +, so he is not against using + for string concatenation. But why is foo += 'ooo' bad practice whereas foobar=foo+bar is considered good? is foo += bar good?is foo = foo + 'ooo' good?Before this code snippet, the author wrote: One final thing to mention about strings is that using join() is not always best. In the instances where you are creating a new string from a pre-determined number of strings, using the addition operator is actually faster, but in cases like above or in cases where you are adding to an existing string, using join() should be your preferred method. <code>  foo = 'foo'bar = 'bar'foobar = foo + bar # This is goodfoo += 'ooo' # This is bad, instead you should do:foo = ''.join([foo, 'ooo'])",Is python += string concatenation bad practice?
Possible to change directory in Python and have change persist when script finishes?," In trying to answer a question for another user, I came across something that piqued my curiosity: Will change the working directory as far as Python is concerned, so if I am in /home/username/, and I run os.chdir('..'), any subsequent code will work as though I am in /home/. For example, if I then do: files will be a list of .py files in /home/ rather than in /home/username/. However, as soon as the script exits, I will be back in /home/username/, or whichever directory I ran the script from originally.I have found the same thing happens with shell scripts. If I have the following script: Running the script from /home/username/ will create a file foo.txt in /tmp/, but when the script finishes, I will still be in /home/username/ not /tmp/. I am curious if there is some fundamental reason why the working directory is not changed ""permanently"" in these cases, and if there is a way to change it permanently, e.g., to run a script with ~$ python myscript.py, and have the terminal that script was run from end up in a different directory when the script finishes executing.  <code>  import osos.chdir('..') import globfiles = glob.glob('*.py') #!/bin/bashcd /tmptouch foo.txt",Possible to change directory and have change persist when script finishes?
How to Use Lagged Time-Sereis Variables in a Python Pandas Regression Model?," I'm creating time-series econometric regression models. The data is stored in a Pandas data frame.How can I do lagged time-series econometric analysis using Python? I have used Eviews in the past (which is a standalone econometric program i.e. not a Python package). To estimate an OLS equation using Eviews you can write something like: Note the lagged dependent and lagged price terms. It's these lagged variables which seem to be difficult to handle using Python e.g. using scikit or statmodels (unless I've missed something).Once I've created a model I'd like to perform tests and use the model to forecast.I'm not interested in doing ARIMA, Exponential Smoothing, or Holt Winters time-series projections - I'm mainly interested in time-series OLS. <code>  equation eq1.ls log(usales) c log(usales(-1)) log(price(-1)) tv_spend radio_spend",How to Use Lagged Time-Series Variables in a Python Pandas Regression Model?
better way convert json to SQLAlchemy object," These days I am learning SQLAlchemy. When I want to load an object from json and save it to MySQL, things get difficult because the fields in my model are more that 20 and I wonder whether there're better ways to do this.My original code follows as an example: It can work but as I add more fields, it gets awful. <code>  class User(Base): __tablename__ = 'parent' id = Column(Integer, primary_key=True) name = Column(String)json_str = """"""{""id"": 1, ""name"": ""yetship""}""""""obj = json.loads(json_str)user = User(id=obj.get(""id""), name=obj.get(""name""))",Better way convert json to SQLAlchemy object
keyboard shortcut to clear cell output in jupyter notebook, Does anyone know what is the keyboard shortcut to clear (not toggle) the cell output in Jupyter Notebook? <code> ,Keyboard shortcut to clear cell output in Jupyter notebook
How to convert numbers in a string without using lists? Using Python 3," My prof wants me to create a function that return the sum of numbers in a string but without using any lists or list methods.The function should look like this when operating: Usually a function like this would be easy to create when using lists and list methods. But trying to do so without using them is a nightmare.I tried the following code but they don't work: Instead of getting 1, 2, and 3 all converted into integers and added together, I instead get the string '11'. In other words, the numbers in the string still have not been converted to integers.I also tried using a map() function but I just got the same results: <code>  >>> sum_numbers('34 3 542 11') 590 >>> def sum_numbers(s): for i in range(len(s)): int(i) total = s[i] + s[i] return total>>> sum_numbers('1 2 3')'11' >>> def sum_numbers(s): for i in range(len(s)): map(int, s[i]) total = s[i] + s[i] return total>>> sum_numbers('1 2 3')'11'",How to convert numbers in a string without using lists?
python argparse in separate function inside a class and calling args from init," I have a question regarding passing args to variables inside initHere are my working version of code. The above code works, but what I am trying right now is to put all the argparse codes inside a function inside class A and assign arg to init.I looked in the web and I couldn't find a good solution. Ok, so to be exact, I'm not sure how I should approach this problem.In above example, I just called args.variable for args to work but in this case I call self.id = self.parse_args.id?parse_args function return args and I also tried self.id = self.parse_args(id) and this is giving me The part of reason why I want to separate args into a separate function is to simplify my unit test with argparse. <code>  class A: def __init__(self): self.id = args.id self.pw = args.pw self.endpoint = args.endpoint def B: ..do something..if __name__ == ""__main__"": parser = argparse.ArgumentParser() parser.add_argument('-i', '--id', type=str, help = 'username') parser.add_argument('-p', '--pw', type=str, help ='password') parser.add_argument('-e', '--end_point', type=str , help='end point of api') args = parser.parse_args() class A: def __init__(self): self.id = self.parse_args(id) self.pw = self.parse_args(pw) self.endpoint = self.parse_args(endpoint) def B: ..do something.. def parse_args(self,args): parser = argparse.ArgumentParser() parser.add_argument('-i', '--id', type=str, help = 'username') parser.add_argument('-p', '--pw', type=str, help ='password') parser.add_argument('-e', '--end_point', type=str , help='end point of api') return parser.parse_args(args) TypeError: 'builtin_function_or_method' object is not iterable",argparse in separate function inside a class and calling args from init
How to pack spheres in pyhton," I am trying to model random closed packing spheres of uniform size in a square using python. And the spheres should not overlap but I do not know how to do thisI have so far:Code: <code>  import random, math, pylabdef show_conf(L, sigma, title, fname): pylab.axes() for [x, y] in L: for ix in range(-1, 2): for iy in range(-1, 2): cir = pylab.Circle((x + ix, y + iy), radius=sigma, fc='r') pylab.gca().add_patch(cir) pylab.axis('scaled') pylab.xlabel('eixo x') pylab.ylabel('eixo y') pylab.title(title) pylab.axis([0.0, 1.0, 0.0, 1.0]) pylab.savefig(fname) pylab.close()L = []N = 8 ** 2for i in range(N): posx = float(random.uniform(0, 1)) posy = float(random.uniform(0, 1)) L.append([posx, posy])print LN = 8 ** 2eta = 0.3sigma = math.sqrt(eta / (N * math.pi))Q = 20ltilde = 5*sigmaN_sqrt = int(math.sqrt(N) + 0.5)titulo1 = '$N=$'+str(N)+', $\eta =$'+str(eta)nome1 = 'inicial'+'_N_'+str(N) + '_eta_'+str(eta) + '.png'show_conf(L, sigma, titulo1, nome1)",How to pack spheres in python?
Converting Days Since Epoch to Date in Python," How can one convert a serial date number, representing the number of days since epoch (1970), to the corresponding date string? I have seen multiple posts showing how to go from string to date number, but I haven't been able to find any posts on how to do the reverse.For example, 15951 corresponds to ""2013-09-02"". (The + 1 because whatever generated these date numbers followed the convention that Jan 1, 1970 = 1.)TL;DR: Looking for something to do the following: This is different from Python: Converting Epoch time into the datetime because I am starting with days since 1970. I not sure if you can just multiply by 86,400 due to leap seconds, etc. <code>  >>> import datetime>>> (datetime.datetime(2013, 9, 2) - datetime.datetime(1970,1,1)).days + 115951 >>> serial_date_to_string(15951) # arg is number of days since 1970""2013-09-02""",Converting days since epoch to date
Watch requset in gmail API doesnt work," I am trying to make a watch request using python as referred to in the google APIs but it does not work. I could not find a library or a package to use gmail.users() function. How do I make a watch request using an access token? <code>  request = { 'labelIds': ['INBOX'], 'topicName': 'projects/myproject/topics/mytopic'}gmail.users().watch(userId='me', body=request).execute()",Watch request in gmail API doesn't work
data transfer between c/c++ and python," I would like to share memory between C++ and Python.My problem:I am working with big data sets (up to 6 GB of RAM) in C++. All calculations are done in c++.Then, I want to ""paste"" all my results to a Python program. But I can only write my data on disk, and then read that file from Python, which is not efficient. Is there any way to ""map"" memory corresponding to C++ variables so that I may access the data from Python? I don't want to copy 6GB of data onto a hard drive.  <code> ",Data transfer between C++ and Python
"Python: TypeError: argument 1 must have a ""write"" method"," My problem at hand is to get the data from API's and input that data into csv. I'm able to get the data, however outputting the data into csv is where I'm getting the error. Can someone please help.Here is the sample code: Type of variable metrics is <class 'list'>Error that I'm getting is  <code>  import csv,sys def read_campaign_info(campaigns): myfile = csv.writer(open(""output.csv"", ""w"")) for insight in reach_insights: account_id = str(insight[AdsInsights.Field.account_id]) objective = str(insight[AdsInsights.Field.objective]) metrics =[account_id,objective] wr = csv.writer(myfile,quoting=csv.QUOTE_ALL) wr.writerows(metrics) wr = csv.writer(myfile,quoting=csv.QUOTE_ALL)TypeError: argument 1 must have a ""write"" method","TypeError: argument 1 must have a ""write"" method"
Python - Find elements NOT in the intersection of two lists," So I know how to find the intersection of two lists by doing: But what is the best way to find all the elements that are not included in the intersection. My initial idea is to create a union of the two lists and then remove all the elements from the intersection from the union, as such: Is this the best way to do this or is there another way? <code>  >>> a = [1,2,3,4,5]>>> b = [1,3,5,6]>>> list(set(a) & set(b))[1, 3, 5] >>> a = [1,2,3,4,5]>>> b = [1,3,5,6]>>> intersection = list(set(a) & set(b))>>> union = list(set(a) | set(b))>>> non_intersection = intersection - union[2, 4, 6]",Find elements NOT in the intersection of two lists
How to install scipy on windows 10?," I have windows 10 and i am trying to install scipy. Presently, I have tried pip, but it gave an error Then i downloaded scs-1.2.6-cp27-cp27m-win_amd64 from http://www.lfd.uci.edu/~gohlke/pythonlibs/and tried to run this pip install scs-1.2.6-cp27-cp27m-win_amd64but still got error How can i install scipy? I have numpyUpdate 1-: I was able to install scipy but now i got error -: So i downloaded numpy-1.11.2+mkl-cp27-cp27m-win_amd64.whl from the same link. But i get this error-: Why am i getting this error? <code>  Command ""c:\python27\python.exe -u -c ""import setuptools, tokenize;__file__='c:\\users\\kanika\\appdata\\local\\temp\\pip-build-4jzyxl\\scipy\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().repl Could not find a version that satisfies the requirement scs-1.2.6-cp27-cp27m-win_amd64 (from versions: )No matching distribution found for scs-1.2.6-cp27-cp27m-win_amd64 No module named numpy+mkl Invalid requirement: 'numpy-1.11.2+mkl-cp27-cp27m-win_amd64'Traceback (most recent call last): File ""c:\python27\lib\site-packages\pip\req\req_install.py"", line 78, in __init__ req = Requirement(req) File ""c:\python27\lib\site-packages\pip\_vendor\packaging\requirements.py"", line 96, in __init__ requirement_string[e.loc:e.loc + 8]))InvalidRequirement: Invalid requirement, parse error at ""'+mkl-cp2'""",How to install scipy on windows 10?
How to install scipy on windows 10 ?," I have windows 10 and i am trying to install scipy. Presently, I have tried pip, but it gave an error Then i downloaded scs-1.2.6-cp27-cp27m-win_amd64 from http://www.lfd.uci.edu/~gohlke/pythonlibs/and tried to run this pip install scs-1.2.6-cp27-cp27m-win_amd64but still got error How can i install scipy? I have numpyUpdate 1-: I was able to install scipy but now i got error -: So i downloaded numpy-1.11.2+mkl-cp27-cp27m-win_amd64.whl from the same link. But i get this error-: Why am i getting this error? <code>  Command ""c:\python27\python.exe -u -c ""import setuptools, tokenize;__file__='c:\\users\\kanika\\appdata\\local\\temp\\pip-build-4jzyxl\\scipy\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().repl Could not find a version that satisfies the requirement scs-1.2.6-cp27-cp27m-win_amd64 (from versions: )No matching distribution found for scs-1.2.6-cp27-cp27m-win_amd64 No module named numpy+mkl Invalid requirement: 'numpy-1.11.2+mkl-cp27-cp27m-win_amd64'Traceback (most recent call last): File ""c:\python27\lib\site-packages\pip\req\req_install.py"", line 78, in __init__ req = Requirement(req) File ""c:\python27\lib\site-packages\pip\_vendor\packaging\requirements.py"", line 96, in __init__ requirement_string[e.loc:e.loc + 8]))InvalidRequirement: Invalid requirement, parse error at ""'+mkl-cp2'""",How to install scipy on windows 10?
Check context in Flask," I want to log some data from context variables (request, session) when logging during a Flask request, but use default behavior if not.I'm using a try ... except block in logging.formatter. Is there a better way to check for a request context? <code>  try: record.user = session['user_name'] record.very_important_data = request.super_secretexcept Exception: record.user = None",Check if Flask request context is available
visualization data with 10 parameters data using matplotlib," I have this kind of data : what's the best method for visualizing this data, i'm using matplotlib to visualizing it, and read it from csv using pandasthanks <code>  ID x1 x2 x3 x4 x5 x6 x7 x8 x9 x101 -0.18 5 -0.40 -0.26 0.53 -0.66 0.10 2 -0.20 12 -0.58 5 -0.52 -1.66 0.65 -0.15 0.08 3 3.03 -23 -0.62 5 -0.09 -0.38 0.65 0.22 0.44 4 1.49 14 -0.22 -3 1.64 -1.38 0.08 0.42 1.24 5 -0.34 05 0.00 5 1.76 -1.16 0.78 0.46 0.32 5 -0.51 -2",Visualising 10 dimensional data with matplotlib
Understanding big numbers in Python," How does Python allocate memory for large integers?An int type has a size of 28 bytes and as I keep increasing the value of the int, the size increases in increments of 4 bytes.Why 28 bytes initially for any value as low as 1?Why increments of 4 bytes? PS: I am running Python 3.5.2 on a x86_64 (64 bit machine). Any pointers/resources/PEPs on how the (3.0+) interpreters work on such huge numbers is what I am looking for.Code illustrating the sizes: <code>  >>> a=1>>> print(a.__sizeof__())28>>> a=1024>>> print(a.__sizeof__())28>>> a=1024*1024*1024>>> print(a.__sizeof__())32>>> a=1024*1024*1024*1024>>> print(a.__sizeof__())32>>> a=1024*1024*1024*1024*1024*1024>>> a1152921504606846976>>> print(a.__sizeof__())36",Understanding memory allocation for large integers in Python
Behavior of python's select() with partial recv()," I have created a SSL socket (server side) and place the socket into a select() queue. The select() properly returns when the socket ""is ready"" to read.I then recv(1024) bytes. Under some circumstances this will get all the data, in others it may not. However, if there is still data in the socket buffer (because I didn't recv() it all), and I pass that same socket into select() again, it will not be returned as being ""ready"" to read even though I know there is data there.I suppose my question is really to confirm what ""ready to be read"" really means from select()'s perspective and what the best way to handle this would be. Continuing to recv() until EWOULDBLOCK seems sort of hack-ish given that I'm using select().Am I thinking about this incorrectly? I realize I could use a larger recv buffer, but there is always the possibility that there would be more to read than recv can pull -- so what is the ""right"" way to handle this coming out of a select()?Thanks in advance.EDIT: As noted in the comments, I neglected to mention that this is an SSL server and apparently select() behaves differently when using wrapped sockets. <code> ",Behavior of python's select() with partial recv() on SSL socket
comparing string with int in python, Given the following code: we get output as no.How is Python comparing a string value to an int here (if a == 1)? In C such a comparison would give an error because this is comparing different types. <code>  a = '1'if a == 1: print 'yes'else: print 'no',How can Python compare strings with integers?
How to solve this encoding issue in Python 3?," I'm trying to run the following: But I get the following error : UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6987: ordinal not in range(128)From the internet I've found that it should be because the encoding needs to be set to utf-8, but my issue is that it's already in utf-8. Also, it looks like my file is in utf-8, so I'm really confusedAlso, the following code works : Is there a way to solve this ?Thanks !EDIT:When I run the code in my console it works, but not when I run it in Spyder provided by Anaconda (https://www.continuum.io/downloads)Do you know what can go wrong ? <code>  import jsonpath = 'ch02/usagov_bitly_data2012-03-16-1331923249.txt' records = [json.loads(line) for line in open(path)] sys.getdefaultencoding() Out[43]: 'utf-8' In [15]: path = 'ch02/usagov_bitly_data2012-03-16-1331923249.txt'In [16]: open(path).readline()",How to solve this encoding issue in with Spyder in Anaconda (Python 3)?
How to solve this encoding issue in Python 3 with Anaconda," I'm trying to run the following: But I get the following error : UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 6987: ordinal not in range(128)From the internet I've found that it should be because the encoding needs to be set to utf-8, but my issue is that it's already in utf-8. Also, it looks like my file is in utf-8, so I'm really confusedAlso, the following code works : Is there a way to solve this ?Thanks !EDIT:When I run the code in my console it works, but not when I run it in Spyder provided by Anaconda (https://www.continuum.io/downloads)Do you know what can go wrong ? <code>  import jsonpath = 'ch02/usagov_bitly_data2012-03-16-1331923249.txt' records = [json.loads(line) for line in open(path)] sys.getdefaultencoding() Out[43]: 'utf-8' In [15]: path = 'ch02/usagov_bitly_data2012-03-16-1331923249.txt'In [16]: open(path).readline()",How to solve this encoding issue in with Spyder in Anaconda (Python 3)?
inf and -inf in Python 2.7," I am wondering how to define inf and -inf as an int in Python 2.7. I tried and it seems inf and -inf only work as a float.  <code>  a = float('-inf') # worksb = float('inf') # worksc = int('-inf') # compile error, ValueError: invalid literal for int() with base 10: 'inf'd = int('inf') # compile error, ValueError: invalid literal for int() with base 10: 'inf'",Represent infinity as an integer in Python 2.7
Represent infinity as an integer Python 2.7," I am wondering how to define inf and -inf as an int in Python 2.7. I tried and it seems inf and -inf only work as a float.  <code>  a = float('-inf') # worksb = float('inf') # worksc = int('-inf') # compile error, ValueError: invalid literal for int() with base 10: 'inf'd = int('inf') # compile error, ValueError: invalid literal for int() with base 10: 'inf'",Represent infinity as an integer in Python 2.7
how to get query a certain period of datetime in Flask sqlalchemy?," In Flask, I have a model named User, like this: I wanna query a certain period datetime, for example, I need to query all the posts posted between 2016-11-01 and 2016-11-30I tried to achieve this by Stitching string, like this: but this is awkward, is there any better way to to that? <code>  class Post(db.Model): __tablename__ = 'post' id = db.Column(db.Integer, primary_key=True) title = db.Column(db.String(255)) content = db.Column(db.Text) post_time = db.Column(db.DateTime(), index=True, default=datetime.now) posts = Post.query.filter(Post.post_time <= year_month+'-30').filter(Post.post_time >= year_month+'-01')",How to get query a certain period of datetime in Flask sqlalchemy?
add pip requirements to docker image in runtime," I want to be able to add some extra requirements to an own create docker image. My strategy is build the image from a dockerfile with a CMD command that will execute a ""pip install -r"" command using a mounted volume in runtime.This is my dockerfile: Having that dockerfile I build the image: And finally I try to attach my new requirements using this command: My local folder ""sourceCode"" has inside a valid requirements.txt file (it contains only one line with the value ""gunicorn""). When I get the prompt I can see that the requirements file is there, but if I execute a pip freeze command the gunicorn package is not listed.Why the requirements.txt file is been attached correctly but the pip command is not working properly? <code>  FROM ubuntu:14.04RUN apt-get updateRUN apt-get install -y python-pip python-dev build-essential RUN pip install --upgrade pipWORKDIR /rootCMD [""pip install -r /root/sourceCode/requirements.txt""] sudo docker build -t test . sudo docker run -v $(pwd)/sourceCode:/root/sourceCode -it test /bin/bash",Add pip requirements to docker image in runtime
Share a list between different processes in python," I have the following problem. I have written a function that takes a list as input and creates a dictionary for each element in the list. I then want to append this dictionary to a new list, so I get a list of dictionaries. I am trying to spawn multiple processes for this. My problem here is that I want the different processes to access the list of dictionaries as it is updated by other processes, for example to print something once the has reached a certain length.My example would be like this: Right now my problem is that each process creates its own new_list. Is there a way to share the list between processes, such that all dictionaries are appended to the same list? Or is the only way to define the new_list outside of the function? <code>  import multiprocessinglist=['A', 'B', 'C', 'D', 'E', 'F']def do_stuff(element): element_dict={} element_dict['name']=element new_list=[] new_list.append(element_dict) if len(new_list)>3: print 'list > 3'###Main###pool=multiprocessing.Pool(processes=6)pool.map(do_stuff, list)pool.close()",Share a list between different processes?
Why are mutable values in python Enums the same object?," While experimenting with different value types for Enum members, I discovered some odd behavior when the values are mutable.If I define the values of an Enum as different lists, the members still behave similarly to when the Enum values are typical immutable types like str or int, even though I can change the values of the members in place so that the values of the two Enum members are the same: However, if I define the values to be identical lists, each member's value seems to be the same object, and thus any mutation of one member's value affects all members: Why does Enum behave this way? Is it the intended behavior or is it a bug?NOTE: I'm not planning on actually using Enums this way, I was simply experimenting with using non-standard values for Enum members <code>  >>> class Color(enum.Enum): black = [1,2] blue = [1,2,3] >>> Color.blue is Color.blackFalse>>> Color.black == Color.blueFalse>>> Color.black.value.append(3)>>> Color.black<Color.black: [1, 2, 3]>>>> Color.blue<Color.blue: [1, 2, 3]>>>> Color.blue == Color.blackFalse>>> Color.black.value == Color.blue.valueTrue >>> class Color(enum.Enum): black = [1,2,3] blue = [1,2,3]>>> Color.blue is Color.blackTrue>>> Color.black == Color.blueTrue>>> Color.black.value.append(4)>>> Color.black<Color.black: [1, 2, 3, 4]>>>> Color.blue<Color.black: [1, 2, 3, 4]>>>> Color.blue == Color.blackTrue",Why are mutable values in Python Enums the same object?
Why '{0}'.format() is faster than str() and '{}'.format()?," So it's a CPython thing, not quite sure that it has same behaviour with other implementations. But '{0}'.format() is faster than str() and '{}'.format(). I'm posting results from Python 3.5.2, but, I tried it with Python 2.7.12 and the trend is the same. From the docs on object.__str__(self) Called by str(object) and the built-in functions format() and print() to compute the informal or nicely printable string representation of an object.So, str() and format() call same object.__str__(self) method, but where does that difference in speed come from?UPDATEas @StefanPochmann and @Leon noted in comments, they get different results. I tried to run it with python -m timeit ""..."" and, they are right, because the results are: So it seems that IPython is doing something strange...NEW QUESTION: What is preferred way to convert an object to str by speed?  <code>  %timeit q=['{0}'.format(i) for i in range(100, 100000, 100)]%timeit q=[str(i) for i in range(100, 100000, 100)]%timeit q=['{}'.format(i) for i in range(100, 100000, 100)]1000 loops, best of 3: 231 s per loop1000 loops, best of 3: 298 s per loop1000 loops, best of 3: 434 s per loop $ python3 -m timeit ""['{0}'.format(i) for i in range(100, 100000, 100)]""1000 loops, best of 3: 441 usec per loop$ python3 -m timeit ""[str(i) for i in range(100, 100000, 100)]""1000 loops, best of 3: 297 usec per loop$ python3 -m timeit ""['{}'.format(i) for i in range(100, 100000, 100)]""1000 loops, best of 3: 420 usec per loop",'{0}'.format() is faster than str() and '{}'.format() using IPython %timeit and otherwise using pure Python
"Why does pandas read_csv does not support multiple comments (#,@,...)?"," I found pandas read_csv method to be faster than numpy loadtxt. Unfortunatly now I find myself in a situation where I have to go back to numpy because loadtxt has the option of setting comments=['#','@']. Pandas read_csv method can only take one comment string like comment='#' as far as I can tell from the help site. Any suggestions or workarounds that could make my life easier and make me not pivot back to numpy? Also why does pandas not support multiple comment indicators? Minimal example: <code>  # save this in test.dat@ bla# bla1 2 3 4 # does work, but only one type of comment is accounted fordf = pd.read_csv('test.dat', index_col=0, header=None, comment='#')# does not work (not suprising reading the help)df = pd.read_csv('test.dat', index_col=0, header=None, comment=['#','@'])# does work but is slowdf = np.loadtxt('test.dat', comments=['#','@'])","Why does pandas read_csv not support multiple comments (#,@,...)?"
Pandas: Fill missing values by mean in each group faster than transfrom," I need to fill missing values in a pandas DataFrame by the mean value in each group. According to this question transform can achieve this. However, transform is too slow for my purposes.For example, take the following setting with a large DataFrame with 100 different groups and 70% NaN values: Using transform via takes already more than 3 seconds on my computer. I need something by an order of magnitude faster (buying a bigger machine is not an option :-D).So how can I fill the missing values any faster? <code>  import pandas as pdimport numpy as npsize = 10000000 # DataFrame lengthngroups = 100 # Number of Groupsrandgroups = np.random.randint(ngroups, size=size) # Creation of groupsrandvals = np.random.rand(size) * randgroups * 2 # Random values with mean like group numbernan_indices = np.random.permutation(range(size)) # NaN indicesnanfrac = 0.7 # Fraction of NaN valuesnan_indices = nan_indices[:int(nanfrac*size)] # Take fraction of NaN indicesrandvals[nan_indices] = np.NaN # Set NaN valuesdf = pd.DataFrame({'value': randvals, 'group': randgroups}) # Create data frame df.groupby(""group"").transform(lambda x: x.fillna(x.mean())) # Takes too long",Pandas: Fill missing values by mean in each group faster than transform
Artificial axis labels for seaborn heatmaps," I have a seaborn heatmap that looks like this:...generated from a pandas dataframe of randomly generated values a piece of which looks like this:The values along the y axis are all in the range [0,1], and the ones on the x axis in the range [0,2*pi], and I just want some short floats at regular intervals for my tick labels, but I can only seem to get values that are in my dataframe. When I try specifying the values I want, it doesn't put them in the right place, as seen in the plot above. He's my code right now. How can I get the axis labels that I tried specifying with xticks and yticks in this code in the correct places (which would be evenly spaced along the axes)? <code>  import pandas as pdimport numpy as npimport matplotlib as pltfrom matplotlib.mlab import griddatasns.set_style(""darkgrid"")PHI, COSTH = np.meshgrid(phis, cos_thetas)THICK = griddata(phis, cos_thetas, thicknesses, PHI, COSTH, interp='linear')thick_df = pd.DataFrame(THICK, columns=phis, index=cos_thetas)thick_df = thick_df.sort_index(axis=0, ascending=False)thick_df = thick_df.sort_index(axis=1)cmap = sns.cubehelix_palette(start=1.6, light=0.8, as_cmap=True, reverse=True)yticks = np.array([0,0.2,0.4,0.6,0.8,1.0])xticks = np.array([0,1,2,3,4,5,6])g = sns.heatmap(thick_df, linewidth=0, xticklabels=xticks, yticklabels=yticks, square=True, cmap=cmap)plt.show(g)",Artificial tick labels for seaborn heatmaps
What is pythononic way of slicing a set?," I have some list of data, for example: and I want to get unique values with fixed length (I don't care which I will get) and I also want it to be a set.I know that I can do set from some_data then make it list, crop it and then make it set again. I understand that I don't have __getitem__ method in set which wouldn't make the whole slice thing possible, but if there is a chance to make it look better?And I completely understand that set is unordered. So it doesn't matter which elements are in final set.Possible options are to use:ordered-setusing dict with None values:  <code>  some_data = [1, 2, 4, 1, 6, 23, 3, 56, 6, 2, 3, 5, 6, 32, 2, 12, 5, 3, 2] set(list(set(some_data))[:5]) # doesn't look so friendly set(dict(map(lambda x: (x, None), some_data)).keys()[:2]) # not that great",What is a pythonic way of slicing a set?
how to save the output of the console in Anaconda Spyder?," I have a bug in my program :(The problem is that:My .py code is long, and takes ages to runI don't know where the bug isThe good news is that I have a lot of print() in my py file, so I can potentially know where the bug lives. The bad news is that my bug makes my computer crash, so there is no way for me to look at the output of the ipython console and see what went wrong.How can I have the output be written to disk while the program runs? So that I can still open the file after reboot to understand what happened before the crash?This question is different from Redirect stdout to a file in Python?, because I needcontinuous writing to filesomething to use from within SpyderMany thanks! <code> ",How to save the output of an IPython console to a file in Spyder?
PANDAS count consecutive date observations within df.groupby object," This is an example of the data frame i'm working with: I'd like to count consecutive observations starting from 2016-11-22 that there are grouped by Comp_ID and item_number.Essentially, what I am looking to do, is count how many days in a row there is an observation counting back from todays date for each Comp_ID and item_number. (this example was put together on the 22nd of Nov) Consecutive observations observed weeks/ days prior to today are not relevant. Only sequences like today... yesterday... the day before yesterday... and so on are relevant. I got this to work on a smaller sample, but it seems to be getting tripped up on a larger data-set. Here is the code for the smaller sample. I need to find the consecutive dates with observations across thousands of sellers/ items. For some reason, the below code did not work on the larger data set. This gets the desired result for the smaller data-set: <code>  d = {'item_number':['bdsm1000', 'bdsm1000', 'bdsm1000', 'ZZRWB18','ZZRWB18', 'ZZRWB18', 'ZZRWB18', 'ZZHP1427BLK', 'ZZHP1427', 'ZZHP1427', 'ZZHP1427', 'ZZHP1427', 'ZZHP1427', 'ZZHP1427', 'ZZHP1427', 'ZZHP1427', 'ZZHP1427', 'ZZHP1427', 'ZZHP1427', 'ZZHP1427', 'ZZHP1414', 'ZZHP1414', 'ZZHP1414', 'WRM115WNTR', 'WRM115WNTR', 'WRM115WNTR', 'WRM115WNTR', 'WRM115WNTR', 'WRM115WNTR', 'WRM115WNTR', 'WRM115WNTR', 'WRM115WNTR', 'WRM115WNTR', 'WRM115WNTR', 'WRM115WNTR', 'WRM115WNTR', 'WRM115SCFRE', 'WRM115SCFRE', 'WRM115SCFRE', 'WRM115SCFRE', 'WRM115SCFRE', 'WRM115SCFRE', 'WRM115SCFRE', 'WRM115SCFRE', 'WRM115SCFRE', 'WRM115SCFRE', 'WRM115SCFRE', 'WRM115SCFRE', 'WRM115SCFRE', 'WRM115SCFRE'],'Comp_ID':[2454, 2454, 2454, 1395, 1395, 1395, 1395, 3378, 1266941, 660867, 43978, 1266941, 660867, 43978, 1266941, 660867, 43978, 1266941, 660867, 43978, 43978, 43978, 43978, 1197347907, 70745, 4737, 1197347907, 4737, 1197347907, 70745, 4737, 1197347907, 70745, 4737, 1197347907, 4737, 1197487704, 1197347907, 70745, 23872, 4737, 1197347907, 4737, 1197487704, 1197347907, 23872, 4737, 1197487704, 1197347907, 70745],'date':['2016-11-22', '2016-11-20', '2016-11-19', '2016-11-22', '2016-11-20', '2016-11-19', '2016-11-18', '2016-11-22', '2016-11-22', '2016-11-22', '2016-11-22', '2016-11-20', '2016-11-20', '2016-11-20', '2016-11-19', '2016-11-19', '2016-11-19', '2016-11-18', '2016-11-18', '2016-11-18', '2016-11-22', '2016-11-20', '2016-11-19', '2016-11-22', '2016-11-22', '2016-11-22', '2016-11-21', '2016-11-21', '2016-11-20', '2016-11-20', '2016-11-20', '2016-11-19', '2016-11-19', '2016-11-19', '2016-11-18', '2016-11-18', '2016-11-22', '2016-11-22', '2016-11-22', '2016-11-22', '2016-11-22', '2016-11-21', '2016-11-21', '2016-11-20', '2016-11-20', '2016-11-20', '2016-11-20', '2016-11-19', '2016-11-19', '2016-11-19']}df = pd.DataFrame(data=d)df.date = pd.to_datetime(df.date) d = {'item_number':['KIN005','KIN005','KIN005','KIN005','KIN005','A789B','A789B','A789B','G123H','G123H','G123H'],'Comp_ID':['1395','1395','1395','1395','1395','7787','7787','7787','1395','1395','1395'],'date':['2016-11-22','2016-11-21','2016-11-20','2016-11-14','2016-11-13','2016-11-22','2016-11-21','2016-11-12','2016-11-22','2016-11-21','2016-11-08']}df = pd.DataFrame(data=d)df.date = pd.to_datetime(df.date)d = pd.Timedelta(1, 'D')df = df.sort_values(['item_number','date','Comp_ID'],ascending=False)g = df.groupby(['Comp_ID','item_number'])sequence = g['date'].apply(lambda x: x.diff().fillna(0).abs().le(d)).reset_index()sequence.set_index('index',inplace=True)test = df.join(sequence)test.columns = ['Comp_ID','date','item_number','consecutive']g = test.groupby(['Comp_ID','item_number'])g['consecutive'].apply(lambda x: x.idxmin() - x.idxmax() ) Comp_ID item_number1395 G123H 2 KIN005 37787 KIN005 2Name: consecutive, dtype: int64",Pandas count consecutive date observations within groupby object
No module named imutils.pespective after pip installing," I am trying to follow this tutorial which requires imtools. It looks like I have the package installed, but the Python compiler cannot find it. What's going on here?Tutorial: http://www.pyimagesearch.com/2016/10/03/bubble-sheet-multiple-choice-scanner-and-test-grader-using-omr-python-and-opencv/ <code>  ~/py:. cat test_grader.pyfrom imutils.perspective import four_point_transform~~/py:. python test_grader.pyTraceback (most recent call last): File ""test_grader.py"", line 1, in <module> from imutils.perspective import four_point_transformImportError: No module named imutils.perspective~/py:pip install imtoolsRequirement already satisfied: imtools in /usr/local/lib/python2.7/site-packagesRequirement already satisfied: matplotlib in /usr/local/lib/python2.7/site-packages (from imtools)Requirement already satisfied: pyyaml in /usr/local/lib/python2.7/site-packages (from imtools)Requirement already satisfied: numpy in /usr/local/lib/python2.7/site-packages (from imtools)Requirement already satisfied: scipy in /usr/local/lib/python2.7/site-packages (from imtools)Requirement already satisfied: python-dateutil in /usr/local/lib/python2.7/site-packages (from matplotlib->imtools)Requirement already satisfied: pyparsing!=2.0.0,!=2.0.4,>=1.5.6 in /usr/local/lib/python2.7/site-packages (from matplotlib->imtools)Requirement already satisfied: cycler in /usr/local/lib/python2.7/site-packages (from matplotlib->imtools)Requirement already satisfied: pytz in /usr/local/lib/python2.7/site-packages (from matplotlib->imtools)Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/site-packages (from python-dateutil->matplotlib->imtools)~/py:.",No module named imutils.perspective after pip installing
Pandas Dataframe: set maximum value in column," I'm trying to set a maximum value of a pandas DataFrame column. For example: would yield: But it doesn't. There are a million solutions to find the maximum value, but nothing to set the maximum value... at least that I can find. I could iterate through the list, but I suspect there is a faster way to do it with pandas. My lists will be significantly longer and thus I would expect iteration to take relatively longer amount of time. Also, I'd like whatever solution to be able to handle NaN. <code>  my_dict = {'a':[10,12,15,17,19,20]}df = pd.DataFrame(my_dict)df['a'].set_max(15) a0 101 122 153 154 155 15",Set maximum value (upper bound) in pandas DataFrame
"Set y-axis scale for Pandas Dataframe Boxplot(), 3 Deviations?"," I'm trying to make a single boxplot chart area per month with different boxplots grouped by (and labeled) by industry and then have the Y-axis use a scale I dictate. In a perfect world this would be dynamic and I could set the axis to be a certain number of standard deviations from the overall mean. I could live with another type of dynamically setting the y axis but I would want it to be standard on all the 'monthly' grouped boxplots created. I don't know what the best way to handle this is yet and open to wisdom - all I know is the numbers being used now are way to large for the charts to be meaningful.I've tried all kinds of code and had zero luck with the scaling of axis and the code below was as close as I could come to the graph.Here's a link to some dummy data: https://drive.google.com/open?id=0B4xdnV0LFZI1MmlFcTBweW82V0kAnd for the code I'm using Python 3.5: <code>  import pandas as pdimport numpy as npimport matplotlibimport matplotlib.pyplot as pltmatplotlib.use('TkAgg')import pylab df = pd.read_csv('Query_Final_2.csv')df['Ship_Date'] = pd.to_datetime(df['Ship_Date'], errors = 'coerce')df1 = (df.groupby('Industry'))print(df1.boxplot(column='Gross_Margin',layout=(1,9), figsize=(20,10), whis=[5,95]),pylab.show())","Set y-axis scale for pandas Dataframe Boxplot(), 3 Deviations?"
What should I put in the body of an abstract method in Python," Say I have the following abstract class Foo: What should I put in the body of the bar method? I see a lot of code that has raise NotImplementedError, as shown above. However, this seems redundant, since any subclass that does not implement bar will raise the TypeError: Can't instantiate abstract class Foo with abstract methods bar when it is instantiated.Is it Pythonic to leave bar empty, as follows: This is what is done in the Python docs for Abstract Base Classes, but I'm not sure if that's just a placeholder or an actual example of how to write code.If it's ok to leave bar with only three dots (...), when should I use NotImplementedError? <code>  import abcclass Foo(abc.ABC): @abc.abstractmethod def bar(self): raise NotImplementedError import abcclass Foo(abc.ABC): @abc.abstractmethod def bar(self): ...",What should I put in the body of an abstract method?
Using SUM/Between with Flask-SQLalchemy?," I want to query services between two dates and sum their prices. When I try to use func.sum with Services.query, I get TypeError: BaseQuery object is not callable. How do I query using a function with Flask-SQLAlchemy? <code>  Services.query(func.sum(Services.price)).filter(Services.dateAdd.between(start, end))",Querying with function on Flask-SQLAlchemy model gives BaseQuery object is not callable error
How to read a utf-8 encoded Tamil text file using Python," I need to analyse a textfile in tamil (utf-8 encoded). Im using nltk package of Python on the interface IDLE. when i try to read the text file on the interface, this is the error i get. how do i avoid this? <code>  corpus = open('C:\\Users\\Customer\\Desktop\\DISSERTATION\\ettuthokai.txt').read()Traceback (most recent call last): File ""<pyshell#2>"", line 1, in <module> corpus = open('C:\\Users\\Customer\\Desktop\\DISSERTATION\\ettuthokai.txt').read() File ""C:\Users\Customer\AppData\Local\Programs\Python\Python35-32\lib\encodings\cp1252.py"", line 23, in decode return codecs.charmap_decode(input,self.errors,decoding_table)[0]UnicodeDecodeError: 'charmap' codec can't decode byte 0x8d in position 33: character maps to <undefined>",How to read a utf-8 encoded text file using Python
Print Rows in csv file python," Here is a sample of my csv file: I have written this code with the intent to print out the first row: However this only print out the first(technically second) column like so: How can I make it print out the first row like so? <code>  1 Ryan Giggs 13.50 23 Manchester Utd2 David Beckham 40.00 24 Manchester Utd3 Michael Owen 22.00 22 Liverpool4 Robbie Fowler 21.00 23 Leeds Utd import csvwith open(""Footballers.csv"") as f: reader = csv.reader(f) for row in reader: print(row[1]) RyanDavidMichealRobbie 1 Ryan Giggs 13.50 23 Manchester Utd",Print rows in csv file?
How to use OpenID connect SSO with Django rest-framework?," Is there a way to write the logic for both backend and frontend?In my current flow, We are using Django rest-framework login, now we need to integrate OpenID connect SSO in our project so guide me how we can integrate OpenID connect SSO with Django-rest framework.Is there a GitHub example?I am looking for a sample code. <code> ",OpenID connect based authentication in Angular.js with (drf oidc) Django rest framework backend
OpenID connect based authentication in Angular.js (with Django backend)," Is there a way to write the logic for both backend and frontend?In my current flow, We are using Django rest-framework login, now we need to integrate OpenID connect SSO in our project so guide me how we can integrate OpenID connect SSO with Django-rest framework.Is there a GitHub example?I am looking for a sample code. <code> ",OpenID connect based authentication in Angular.js with (drf oidc) Django rest framework backend
How to query an advanced search with google python API?," How can I programmatically using the Google Python client library do an advanced search with Google custom search API search engine in order to return a list of first n links based in some terms and parameters of an advanced search I queried?. I tried to check the documentation(I did not found any example), and this answer. However, the latter did not worked, since currently there is no support for the AJAX API. So far I tried this: And this: Thus, any idea of how to do and advanced search with google's search engine API?. This is how my credentials look at google console:credentials <code>  from googleapiclient.discovery import buildimport pprintmy_cse_id = ""test""def google_search(search_term, api_key, cse_id, **kwargs): service = build(""customsearch"", ""v1"",developerKey=""<My developer key>"") res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute() return res['items']results = google_search('dogs', my_api_key, my_cse_id, num=10)for result in results: pprint.pprint(result) import pprintfrom googleapiclient.discovery import builddef main(): service = build(""customsearch"", ""v1"",developerKey=""<My developer key>"") res = service.cse().list(q='dogs').execute() pprint.pprint(res)if __name__ == '__main__': main()",How to query an advanced search with google customsearch API?
Dont have permissions to install packages for anaconda3," I am trying to install cherrypy for anaconda3, using the following page's commands https://anaconda.org/anaconda/cherrypy, but am getting the following error message,The operating system I am using is Windows 10 and I have checked that I have administrator privileges with my login. Moreover I installed anaconda for all users and have checked that all mmy logins can modify the C:\Program Files\Anaconda3 folder.What should I do? <code> ",How do obtain permissions to install packages for Anaconda3 on Windows 10?
How to get permissions to install packages for anaconda3 on Windows 10," I am trying to install cherrypy for anaconda3, using the following page's commands https://anaconda.org/anaconda/cherrypy, but am getting the following error message,The operating system I am using is Windows 10 and I have checked that I have administrator privileges with my login. Moreover I installed anaconda for all users and have checked that all mmy logins can modify the C:\Program Files\Anaconda3 folder.What should I do? <code> ",How do obtain permissions to install packages for Anaconda3 on Windows 10?
Somehow I do not have permissions to install packages for Anaconda3 on Windows 10?," I am trying to install cherrypy for anaconda3, using the following page's commands https://anaconda.org/anaconda/cherrypy, but am getting the following error message,The operating system I am using is Windows 10 and I have checked that I have administrator privileges with my login. Moreover I installed anaconda for all users and have checked that all mmy logins can modify the C:\Program Files\Anaconda3 folder.What should I do? <code> ",How do obtain permissions to install packages for Anaconda3 on Windows 10?
Python type hinting with class," How do I do node: Node? Because when I run it, it says name 'Node' is not defined. Should I just remove the : Node and instance check it inside the function?But then how could I access node's properties (which I would expect to be instance of Node class)?I don't know how implement type casting in Python, BTW. <code>  class Node: def append_child(self, node: Node): if node != None: self.first_child = node self.child_nodes += [node]",type hinting within a class
Add/remove stop wrods with spacy, What is the best way to add/remove stop words with spacy? I am using token.is_stop function and would like to make some custom changes to the set. I was looking at the documentation but could not find anything regarding of stop words. Thanks! <code> ,Add/remove custom stop words with spacy
Add/remove stop words with spacy, What is the best way to add/remove stop words with spacy? I am using token.is_stop function and would like to make some custom changes to the set. I was looking at the documentation but could not find anything regarding of stop words. Thanks! <code> ,Add/remove custom stop words with spacy
SymPy: How evaluate constants sympy gives with initial condition?," How can I evaluate the constants C1 and C2 from a solution of a differential equation SymPy gives me? There are the initial condition f(0)=0 and f(pi/2)=3. I tried some ics stuff but it's not working. Example: By the way: C2 = 0 and C1 = 3.  <code>  >>> from sympy import *>>> f = Function('f')>>> x = Symbol('x')>>> dsolve(f(x).diff(x,2)+f(x),f(x))f(x) == C1*sin(x) + C2*cos(x) >>> dsolve(f(x).diff(x,2)+f(x),f(x), ics={f(0):0, f(pi/2):3})f(x) == C1*sin(x) + C2*cos(x)",How to evaluate the constants SymPy gives with initial condition?
How do I mock an class and control a return value in py.test with pytest-mock?," I'm trying to use py.test to test some code that does various LDAP searches, and modifications.I'm using pytest-mock, but I'm having trouble understanding how to mock out the creation of the an LDAP object, and control what it returns when a search_s() is called on the mocked object.I thought this would do what I wanted, but the test fails, the count shows the generator function find_users() never yields anything. <code>  import pytest# Here is some code to simply test mocking out ldap.initialize(), and# controlling the return value from calls to search_s()import ldapdef find_users(ldap_url, admin_user, admin_password, userbase): lobj = ldap.initialize(ldap_url) lobj.simple_bind_s(admin_user, admin_password) for i in lobj.search_s(userbase, ldap.SCOPE_SUBTREE, '*'): yield i[1]['uid'][0]class TestMocking: @pytest.fixture() def no_ldap(self, mocker): return mocker.patch('ldap.initialize') def test_ad_one_user(self, no_ldap): # try and modify how search_s() would return no_ldap.search_s.return_value = ('', {'uid': ['happy times']}) count = 0 for i in find_users('', '', '', ''): count += 1 assert i=='happy times' assert count == 1",How do I mock a class and control a returned value in py.test with pytest-mock?
Beautysoup how to get all link inside block with class?," I have the following HTML Dom: I need to get all links(url) with class dev-link inside block div.meta-info-wide.I tried this obvious way, but does not work: <code>  <div class=""meta-info meta-info-wide""> <div class=""title""></div> <div class=""content contains-text-link""> <a class=""dev-link"" href=""http://www.jourist.com&amp;sa=D&amp;usg=AFQjCNHiC-nLYHAJwNnvDyYhyoeB6n8YKg"" rel=""nofollow"" target=""_blank""> -</a> <a class=""dev-link"" href=""mailto:info@jourist.com"" rel=""nofollow"" target=""_blank"">: info@jourist.com</a> <div class=""content physical-address"">Diagonalstrae 41 20537 Hamburg</div> </div> </div> divTag = soup.find_all(""div"", {""class"":""meta-info-wide""}) print(len(divTag)) for tag in divTag: tdTags = tag.find_all(""a"", {""class"":""dev-link""}) for tag in tdTags: print tag.text",Beautifulsoup - How to get all links inside a block with a certain class?
pandas find first occurence," Suppose I have a structured dataframe as follows: The A column has previously been sorted. I wish to find the first row index of where df[df.A!='a']. The end goal is to use this index to break the data frame into groups based on A. Now I realise that there is a groupby functionality. However, the dataframe is quite large and this is a simplified toy example. Since A has been sorted already, it would be faster if I can just find the 1st index of where df.A!='a'. Therefore it is important that whatever method that you use the scanning stops once the first element is found. <code>  df = pd.DataFrame({""A"":['a','a','a','b','b'], ""B"":[1]*5})",pandas - find first occurrence
pandas find first occurrence," Suppose I have a structured dataframe as follows: The A column has previously been sorted. I wish to find the first row index of where df[df.A!='a']. The end goal is to use this index to break the data frame into groups based on A. Now I realise that there is a groupby functionality. However, the dataframe is quite large and this is a simplified toy example. Since A has been sorted already, it would be faster if I can just find the 1st index of where df.A!='a'. Therefore it is important that whatever method that you use the scanning stops once the first element is found. <code>  df = pd.DataFrame({""A"":['a','a','a','b','b'], ""B"":[1]*5})",pandas - find first occurrence
Declaring a single byte variable in python 3, How do we declare a single byte variable in Python? I would like to achieve the following result represented in C: I would like to know if it is possible to declare an 8-bit variable in Python. <code>  unsigned char = 0xFF;,Declaring a single byte variable in Python
How to explain asynch/await in python to my wife," I was trying to explain an example of async programming in python but I failed.Here is my code. My expectation is that I would see: With a wait of 10s before asyncFoo was displayed.But instead I got nothing for 10s, and then they both displayed.What am I doing wrong, and how can I explain it? <code>  import asyncioimport timeasync def asyncfoo(t): time.sleep(t) print(""asyncFoo"")loop = asyncio.get_event_loop()loop.run_until_complete(asyncfoo(10)) # I think Here is the problemprint(""Foo"")loop.close() FooasyncFoo",How to use async/await in python 3.5+
python how to print defaultdict variable without its type," In the following code: Output is: But, I need output to be: How can I do it? <code>  from collections import defaultdictconfusion_proba_dict = defaultdict(float)for i in xrange(10): confusion_proba_dict[i] = i + 10print confusion_proba_dict defaultdict(<type 'float'>, {0: 10, 1: 11, 2: 12, 3: 13, 4: 14, 5: 15, 6: 16, 7: 17, 8: 18, 9: 19}) {0: 10, 1: 11, 2: 12, 3: 13, 4: 14, 5: 15, 6: 16, 7: 17, 8: 18, 9: 19}",How to print defaultdict variable without its type?
python pandas - Edting DataFrame with a for loop," Considering the following 2 lists of 3 dicts and 3 empty DataFrames I want to recursively modify the 3 Dataframes within a loop, by using the following line: However, when trying to retrieve for instance 1 of the df outside of the loop, it is still empty will return When printing the df from within the for loop, we can see the data is correctly appended though.How to make the loop so that it is possible to print the 3 dfs with their changes outside of the loop? <code>  dict0={'actual': {'2013-02-20 13:30:00': 0.93}}dict1={'actual': {'2013-02-20 13:30:00': 0.85}}dict2={'actual': {'2013-02-20 13:30:00': 0.98}}dicts=[dict0, dict1, dict2]df0=pd.DataFrame()df1=pd.DataFrame()df2=pd.DataFrame()dfs=[df0, df1, df2] for df, dikt in zip(dfs, dicts): df = df.from_dict(dikt, orient='columns', dtype=None) print (df0) Empty DataFrameColumns: []Index: []",python pandas - Editing multiple DataFrames with a for loop
python pandas - Editing DataFrame within a for loop," Considering the following 2 lists of 3 dicts and 3 empty DataFrames I want to recursively modify the 3 Dataframes within a loop, by using the following line: However, when trying to retrieve for instance 1 of the df outside of the loop, it is still empty will return When printing the df from within the for loop, we can see the data is correctly appended though.How to make the loop so that it is possible to print the 3 dfs with their changes outside of the loop? <code>  dict0={'actual': {'2013-02-20 13:30:00': 0.93}}dict1={'actual': {'2013-02-20 13:30:00': 0.85}}dict2={'actual': {'2013-02-20 13:30:00': 0.98}}dicts=[dict0, dict1, dict2]df0=pd.DataFrame()df1=pd.DataFrame()df2=pd.DataFrame()dfs=[df0, df1, df2] for df, dikt in zip(dfs, dicts): df = df.from_dict(dikt, orient='columns', dtype=None) print (df0) Empty DataFrameColumns: []Index: []",python pandas - Editing multiple DataFrames with a for loop
python pandas - Editing DataFrame with a for loop," Considering the following 2 lists of 3 dicts and 3 empty DataFrames I want to recursively modify the 3 Dataframes within a loop, by using the following line: However, when trying to retrieve for instance 1 of the df outside of the loop, it is still empty will return When printing the df from within the for loop, we can see the data is correctly appended though.How to make the loop so that it is possible to print the 3 dfs with their changes outside of the loop? <code>  dict0={'actual': {'2013-02-20 13:30:00': 0.93}}dict1={'actual': {'2013-02-20 13:30:00': 0.85}}dict2={'actual': {'2013-02-20 13:30:00': 0.98}}dicts=[dict0, dict1, dict2]df0=pd.DataFrame()df1=pd.DataFrame()df2=pd.DataFrame()dfs=[df0, df1, df2] for df, dikt in zip(dfs, dicts): df = df.from_dict(dikt, orient='columns', dtype=None) print (df0) Empty DataFrameColumns: []Index: []",python pandas - Editing multiple DataFrames with a for loop
"Python: concat string if condition, else do nothing"," I want to concat few strings together, and add the last one only if a boolean condition is True.Like this (a, b and c are strings): But Python does not like it. Is there a nice way to do it without the else option?Thanks! :) <code>  something = a + b + (c if <condition>)","Concat string if condition, else do nothing"
Python - Local variable might be referenced," Why does PyCharm highlight the boolean variable nearby the return with Local variable ""boolean"" might be referenced before assignment?This code checks whether a number is prime or not: I've read that might be some problem with global variables, but there's no ones which might be used in prime_t(). I had similar thing - exception while executing the code, but I think it has been eliminated with if x == 2 and if x == 3.What else might be the problem? <code>  import randomimport mathimport timedef prime_t(x): print x if x < 2: return False if x == 2: return True if x == 3: return True for i in range(2, int(math.sqrt(x))+1): if x % i == 0: boolean = False break else: boolean = True return booleanrandom.seed()how_much = input()start = time.time()for i in range(0, how_much): print(prime_t(random.randint(0, 1000)))print time.time()-start",PyCharm warns local variable might be referenced
Keras Custom Metric for single class accuracy," I am building a custom metric to measure the accuracy of one class in my multi-class dataset during training. I am having trouble selecting the class. The targets are one hot (e.g: the class 0 label is [1 0 0 0 0]): The trouble is, we have to use Keras functions to index tensors. How do you create a boolean mask for a tensor? <code>  from keras import backend as Kdef single_class_accuracy(y_true, y_pred): idx = bool(y_true[:, 0]) # boolean mask for class 0 class_preds = y_pred[idx] class_true = y_true[idx] class_acc = K.mean(K.equal(K.argmax(class_true, axis=-1), K.argmax(class_preds, axis=-1))) # multi-class accuracy return class_acc",How do you create a boolean mask for a tensor in Keras?
Getting the pointed file of a symbolic link," Is there a way to get the target of a symbolic link using pathlib? I know that this can be done using os.readlink().I want to create a dictionary composed by links and their target files. Edit ... and I want to filter all paths that are not absoulute <code>  links = [link for link in root.rglob('*') if link.is_symlink()]files = [Path(os.readlink(str(pointed_file))) for pointed_file in links] link_table = {link : pointed_file for link, pointed_file in zip(links, files) if pointed_file.is_absolute()}",Getting the target of a symbolic link with pathlib
Cracking the coding interview #9.3," I am currently doing this problem from the book cracking the coding interview: 9.3. A magic index in an array A[0...n-1] is defined to be an index such thatA[i] = i. Given a sorted array of distinct integers, write a method to find amagic index, if one exists, in array A.And here is my code: However, when i run my code. I am not getting the proper output. Is there any help or advice I could get? Thanks in advance <code>  def magic_index(seq, start = None, end = None): if start is None: start = 0 if end is None: end = len(seq) - 1 if start > end: return -1 index = (start + end) // 2 if index == seq(index): print(""Equal to index. Value of index = "" + index) return index if index > seq[index]: print(""Greater than loop. Value of Index ="" + index) return magic_index(seq, start=index + 1, end=end) else: print(""Else part of Greater. Value of index = "" + index) return magic_index(seq, start=start, end=index - 1)def main(): magic_index(seq=[1, 2, 3, 4, 6], start=None, end=None). ",Cracking the coding interview #9.3: magic index algorithm
pyqt QTablewidget remove scrollbar to show full table," I have a scrollview to which I dynamically add QTableWidgets. However, the QTables themselves also have scrollbars and therefore don't show the full table. Is there a way to disable the scroll bar so that the table always gets shown in full?EDIT: I added As suggested. The scroll bar does disappear, but it still only shows the partial tables (Ican scroll with hovering iverthe table and using the mouse wheel, still). The code for the Widget is below <code>  self.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOff) self.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff) from PySide.QtGui import *from PySide.QtCore import *class MdTable(QTableWidget): def __init__(self, data, depth, *args): QTableWidget.__init__(self, *args) self.hheaders = [""c1"", ""c2"", ""c3"", ""c4""] self.depth = depth self.bids = data self.setData() def setData(self): self.setRowCount(self.depth) self.setColumnCount(5) for i in xrange(self.depth): if len(self.data) > i: d1= QTableWidgetItem(str(self.data[i][0])) d2= QTableWidgetItem(str(self.data[i][1])) self.setItem(i, 1, d1) self.setItem(i, 2, d2) self.setHorizontalHeaderLabels(self.hheaders) self.verticalHeader().setVisible(False) self.resizeRowsToContents() self.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOff) self.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)",Remove scrollbar to show full table
Is there a clean test for a dictionary element in Python, If I have an element I'm trying to get from a dictionary:my_dict[i]['level_1']['level_2']['my_var'].Is there a cleaner way than doing this to check for nulls? <code>  if 'level_1' in my_dict[i]: if 'level_2' in my_dict[i]['level_1']: if 'my_var' in my_dict[i]['level_1']['level_2']: my_var = my_dict[i]['level_1']['level_2']['my_var'],Is there a clean test for a dictionary element in Python?
Finding maximum value for each level of a multi-index dataframe," I have a DataFrame that looks like this: and I want to find the minimum value for each level of a ignoring the b level, so as an output I'm looking for something like <code>  dataa b1 1 0.1 2 0.2 3 0.32 1 0.5 2 0.6 3 0.7 a min1 0.12 0.5",Finding minimum value for each level of a multi-index dataframe
How to conver a string into list of lenth one in python," I have a string which I want to convert to a list with only one element in it.a = abc print list(a)output : ['a','b','c']Expected o/p = ['abc']What is the proper way to do it ? <code> ",How to convert a string into list with one element in python
Efficient computation of the least-squares alghorithm in NumPy," I need to solve a large set of linear systems, in the least-squares sense. I am having trouble in understanding the difference in computational efficiency of numpy.linalg.lstsq(a, b), np.dot(np.linalg.pinv(a), b) and the mathematical implementation.I use the following matrices: and the results of the algorithms are: 10 loops, best of 3: 36.3 ms per loop 10 loops, best of 3: 103 ms per loop 1 loop, best of 3: 216 ms per loopWhy is there a difference? <code>  h=np.random.random((50000,100))a=h[:,:-1].copy()b=-h[:,-1].copy() # mathematical implementation%%timeitnp.dot(np.dot(np.linalg.inv(np.dot(a.T,a)),a.T),b) # numpy.linalg.lstsq implementation%%timeitnp.linalg.lstsq(a, b)[0] %%timeitnp.dot(np.linalg.pinv(a), b)",Efficient computation of the least-squares algorithm in NumPy
Python - Executing functions within switch statement," I have encountered a problem when putting all the modules I've developed into the main program. The switch dictionary I've created can be seen below: The tank shape is set in the main file in a loop for each of the tanks. The first tank has Tank_Shape = 2 so I would expect it to execute the Calc_Strapped_Volume() function.I have tried testing it, and the switcher function is definitely reading Tank_Shape as 2. Also if I change the functions to strings, it will print out the correct string. The problem is that the functions seem to be executed sequentially, until the correct function is called. This results in errors as the data I'm using will only work with the correct function.Is there a way to only execute the correct function? <code>  def Tank_Shape_Calcs(Tank_Shape, level, area, dish, radius, length, Strapping_Table, Tank_Number): switcher = { 0: vertical.Vertical_Tank(level, area), 1: horiz.Horiz_Cylinder_Dished_Ends(dish, radius, level, length), 2: strapping.Calc_Strapped_Volume(Strapping_Table, level), 3: poly.Fifth_Poly_Calcs(Tank_Number) } return switcher.get(Tank_Shape, ""ERROR: Tank type not valid"")",Executing functions within switch dictionary
Comparing and getting difference between four lists," Consider API returning four lists as output. Let's consider output as Now, first we want to compare these lists are equal or not.Lists are equal only if elements and there indexes matches.For example, from above lists, a and b are equal. But a and c are not equal.If the lists are not equal, then output is expected as: this element at this index in this list is not same as other. For comparing and getting differences of two lists, I have written below code. Now question is how to achieve this for all four above lists? <code>  a = [1,2,3,4]b = [1,2,3,4]c = [1,2,4,3]d = [1,2,3,5] for i in range(len(a)): if a[i] != c[i]: print ""Expected value at "",i,"" is "",a[i] print ""But got value "",c[i],""in second list"" ",Simultaneously iterate over multiple list and capture difference in values
Python Beautifulsoup parsing html," I'm trying to parse for the 76561198134729239. and I can't figure out how to do it. what I tried: <code>  <td height=""16"" class=""listtable_1""><a href=""http://steamcommunity.com/profiles/76561198134729239"" target=""_blank"">76561198134729239</a></td> import requestsfrom lxml import htmlfrom bs4 import BeautifulSoupr = requests.get(""http://ppm.rep.tf/index.php?p=banlist&page=154"")content = r.contentsoup = BeautifulSoup(content, ""html.parser"")element = soup.find(""td"", { ""class"":""listtable_1"", ""target"":""_blank""})print(element.text)",Beautifulsoup: parsing html  get part of href
python keras cross_val_score error," I am trying to do this little tutorial on keras about regression:http://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/Unfortunately I am running into an error I cannot fix. If i just copy and paste the code I get the following error when running this snippet: The error says: Thanks for any help.Here is the full traceback: <code>  import numpyimport pandasfrom keras.models import Sequentialfrom keras.layers import Densefrom keras.wrappers.scikit_learn import KerasRegressorfrom sklearn.model_selection import cross_val_scorefrom sklearn.model_selection import KFoldfrom sklearn.preprocessing import StandardScalerfrom sklearn.pipeline import Pipeline# load datasetdataframe = pandas.read_csv(""housing.csv"", delim_whitespace=True,header=None)dataset = dataframe.values# split into input (X) and output (Y) variablesX = dataset[:,0:13]Y = dataset[:,13]# define base modedef baseline_model(): # create model model = Sequential() model.add(Dense(13, input_dim=13, init='normal', activation='relu')) model.add(Dense(1, init='normal')) # Compile model model.compile(loss='mean_squared_error', optimizer='adam') return model# fix random seed for reproducibilityseed = 7numpy.random.seed(seed)# evaluate model with standardized datasetestimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100,batch_size=5, verbose=0)kfold = KFold(n_splits=10, random_state=seed)results = cross_val_score(estimator, X, Y, cv=kfold) TypeError: get_params() got an unexpected keyword argument 'deep' Traceback (most recent call last):File ""<stdin>"", line 1, in <module> File ""C:\Users\myname\Anaconda3\lib\site-packages\sklearn\model_selection\_validation.py"", line 140, in cross_val_score for train, test in cv_iter) File ""C:\Users\myname\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 758, in __call__ while self.dispatch_one_batch(iterator): File ""C:\Users\myname\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 603, in dispatch_one_batch tasks = BatchedCalls(itertools.islice(iterator, batch_size)) File ""C:\Users\myname\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 127, in __init__ self.items = list(iterator_slice) File ""C:\Users\myname\Anaconda3\lib\site-packages\sklearn\model_selection\_validation.py"", line 140, in <genexpr> for train, test in cv_iter) File ""C:\Users\myname\Anaconda3\lib\site-packages\sklearn\base.py"", line 67, in clone new_object_params = estimator.get_params(deep=False)TypeError: get_params() got an unexpected keyword argument 'deep'",Python Keras cross_val_score Error
Inverse 2D arrays along the third axis in a 3D array without loops," I have an array A whose shape is (N, N, K) and I would like to compute another array B with the same shape where B[:, :, i] = np.linalg.inv(A[:, :, i]).As solutions, I see map and for loops but I am wondering if numpy provides a function to do this (I have tried np.apply_over_axes but it seems that it can only handle 1D array).with a for loop: with map: <code>  B = np.zeros(shape=A.shape)for i in range(A.shape[2]): B[:, :, i] = np.linalg.inv(A[:, :, i]) B = np.asarray(map(np.linalg.inv, np.squeeze(np.dsplit(A, A.shape[2])))).transpose(1, 2, 0)",Compute inverse of 2D arrays along the third axis in a 3D array without loops
How to extract public / private key from a x509 certificate in python?," Below shows the code example I followed, However I got error response as - ""Unable to load certificate"". Error response Please help me to sort this issue.  <code>  from cryptography.x509 import load_pem_x509_certificatefrom cryptography.hazmat.backends import default_backendcert_str = '-----BEGIN CERTIFICATE----- MIIDBTCCAe2gAwIBAgIQEsuEXXy6BbJCK3bMU6GZ/TANBgkqhkiG9w0BAQsFADAt... -----END CERTIFICATE-----';cert_obj = load_pem_x509_certificate(str.encode(cert_str), default_backend())public_key = cert_obj.public_key(); Traceback (most recent call last): File ""C:\xampp1\htdocs\TestWorkPlace\TestPython\src\test1.py"", line 10, in <module> cert_obj = load_pem_x509_certificate(str.encode(cert_str), default_backend()) File ""C:\Program Files (x86)\Python\lib\site-packages\cryptography\x509\base.py"", line 43, in load_pem_x509_certificate return backend.load_pem_x509_certificate(data) File ""C:\Program Files (x86)\Python\lib\site-packages\cryptography\hazmat\backends\multibackend.py"", line 341, in load_pem_x509_certificate return b.load_pem_x509_certificate(data) File ""C:\Program Files (x86)\Python\lib\site-packages\cryptography\hazmat\backends\openssl\backend.py"", line 1175, in load_pem_x509_certificate raise ValueError(""Unable to load certificate"")ValueError: Unable to load certificate",How to extract public key from a x509 certificate in python?
"Django admin nested form can't submit, connection was reset"," I have a django nested admin form and below code is my admin.py file content: When i developing and run devserver on localhost anything work nice, but on the server and by domain i can't submit this form by The connection was reset message.Below code is my apache2 configs: Also i tried to using uwsgi and mod_proxy together but my problem not resolved. After i monitor access.log and my server port 80 by tshark, tshard shows me this request but in access.log file i can't see any change...Apache logs in info mode: And my access.log file content: My related question on ServerFault site:https://serverfault.com/questions/827813/apache-respons-to-get-but-not-to-postUpdate:I ran tshark again and saw below important line: Below is my browser ""Request Payload"": But now my questions is how this problem happend and how can i resolve this? <code>  # -*- coding:utf-8 -*-from django.db.models import Qfrom django import formsfrom django.contrib.auth.admin import UserAdmin as AuthUserAdminfrom django.contrib import adminfrom django.contrib.auth.forms import UserCreationForm, UserChangeFormfrom django.contrib.auth.hashers import UNUSABLE_PASSWORD_PREFIX, identify_hasherfrom django.forms.utils import flatattfrom django.utils.html import format_htmlfrom django.utils.safestring import mark_safefrom django.utils.translation import ugettext as _, ugettextfrom django.contrib.auth.models import Group, Permissionfrom nested_admin.nested import NestedStackedInline, NestedModelAdminfrom HomeMakers.apps.system.models import Dependant, Benefit, User, \ Unit, Stack, Parkingfrom mtools.fields import UniqueValueWidget, PersianDateFieldclass DependantAdminForm(forms.ModelForm): model = Dependant birth_date = PersianDateField(label=u' ')class DependantAdmin(NestedStackedInline): model = Dependant form = DependantAdminForm extra = 0 exclude = ['changed_status', 'new_obj'] can_delete = Trueclass BenefitSubAdmin(admin.TabularInline): model = Benefit extra = 1 min_num = 0 exclude = ['changed_status', 'new_obj'] can_delete = Trueclass NewUserAdminForm(UserCreationForm): class Meta(UserCreationForm.Meta): model = User username = forms.RegexField(label=u"" "", max_length=30, regex=r'^\d{8,10}$', widget=UniqueValueWidget, error_messages={'invalid': u"" .""}) birth_date = PersianDateField(from_year=1290, to_year=1400, label=_('Birth Date')) start_date = PersianDateField(from_year=1290, to_year=1400, label=u"" "") def clean_username(self): # Since User.username is unique, this check is redundant, # but it sets a nicer error message than the ORM. See #13147. username = self.cleaned_data[""username""] if User.objects.filter(username=username).count() > 0: raise forms.ValidationError(self.error_messages['duplicate_username']) return usernameclass ReadOnlyPasswordHashWidget(forms.Widget): def render(self, name, value, attrs): encoded = value final_attrs = self.build_attrs(attrs) if not encoded or encoded.startswith(UNUSABLE_PASSWORD_PREFIX): summary = mark_safe(u""<strong>%s</strong>"" % _(""No password set."")) else: try: hasher = identify_hasher(encoded) except ValueError: summary = mark_safe(u""<strong>%s</strong>"" % _( ""Invalid password format or unknown hashing algorithm."")) else: summary = u'''format_html_join('', ""<strong>{0}</strong>: {1} "", ((ugettext(key), value) for key, value in hasher.safe_summary(encoded).items()) )''' return format_html(u""<div{0}>{1}</div>"", flatatt(final_attrs), summary)class ReadOnlyPasswordHashField(forms.Field): widget = ReadOnlyPasswordHashWidget def __init__(self, *args, **kwargs): kwargs.setdefault(""required"", False) super(ReadOnlyPasswordHashField, self).__init__(*args, **kwargs) def bound_data(self, data, initial): # Always return initial because the widget doesn't # render an input field. return initial def _has_changed(self, initial, data): return Falseclass EditUserAdminForm(UserChangeForm): class Meta(UserChangeForm.Meta): model = User birth_date = PersianDateField(from_year=1290, to_year=1400, label=_('Birth Date')) start_date = PersianDateField(from_year=1290, to_year=1400, label=u"" "") password = ReadOnlyPasswordHashField(label=_(""Password""), help_text=_(""Raw passwords are not stored, so there is no way to see "" ""this user's password, but you can change the password "" ""using <a href=\""password/\"">this form</a>."")) error_messages = {'duplicate_username': 'An user by this international code already exists.'} def __init__(self, *args, **kwargs): super(EditUserAdminForm, self).__init__(*args, **kwargs) self.fields['username'] = forms.RegexField(label=u"" "", max_length=30, regex=r'^\d{8,10}$', widget=UniqueValueWidget(), error_messages={'invalid': u"" .""}) self.fields['username'].widget.attrs['value'] = self.instance.username def clean_username(self): # Since User.username is unique, this check is redundant, # but it sets a nicer error message than the ORM. See #13147. username = self.cleaned_data[""username""] if username != self.instance.username and User.objects.filter(username=username).count() > 0: raise forms.ValidationError(self.error_messages['duplicate_username']) return usernameclass UnitForm(forms.ModelForm): model = Unit installment_begin_date = PersianDateField(label=u' ')class StackSubAdmin(NestedStackedInline): model = Stack extra = 1class ParkingSubAdmin(NestedStackedInline): model = Parking extra = 1class UnitSubAdmin(NestedStackedInline): #(admin.StackedInline): model = Unit form = UnitForm extra = 0 inlines = [ParkingSubAdmin, StackSubAdmin] exclude = ['sum_of_pays', 'parkings', 'warehouse']class UserAdmin(AuthUserAdmin, NestedModelAdmin): model = User ordering = ['last_name'] list_per_page = 10 add_form = NewUserAdminForm form = EditUserAdminForm list_display = ['user_thumb', 'first_name', 'last_name'] list_filter = ['units__project', 'groups'] formfield_overrides = { # models.DateField: {'widget': PersianDateWidget} } '''fields = ( 'username', 'password', 'first_name', 'last_name', 'email', 'gender', 'birth_date', 'picture', 'certificate_no', 'birth_place', 'address', 'home_phone', 'work_phone', 'mobile', 'personnel_code', 'international_code', 'job_field', 'self_employed_job_name', 'employment_type', 'start_date', 'is_retired' )''' inlines = [DependantAdmin, UnitSubAdmin] # & BenefitSubAdmin def get_fieldsets(self, request, obj=None): key_fields = {'fields': ('username', 'password1', 'password2')} if obj is not None: key_fields = {'fields': ('username', 'password')} fieldsets = ( (None, key_fields), (u' ', {'fields': ( 'first_name', 'last_name', 'email', 'gender', 'birth_date', 'academic_degree', 'picture', 'international_card_scaned_file', 'certificate_scaned_file_page1', 'certificate_scaned_file_page2', 'academic_degree_scaned_file', 'job_edict_document', 'certificate_no', 'birth_place', 'address', 'home_phone', 'work_phone', 'mobile', 'personnel_code', 'job_field', 'self_employed_job_name', 'employment_type', 'start_date', 'is_retired' )} ), (u' ', {'fields': ('is_active', 'is_staff', 'groups')}) # , 'user_permissions', 'is_superuser')}), # (u' ', {'fields': ('last_login', 'date_joined')}) ) if request.user.is_superuser: fieldsets = ( (None, key_fields), (u' ', {'fields': ( 'first_name', 'last_name', 'email', 'gender', 'birth_date', 'academic_degree', 'picture', 'international_card_scaned_file', 'certificate_scaned_file_page1', 'certificate_scaned_file_page2', 'academic_degree_scaned_file', 'job_edict_document', 'certificate_no', 'birth_place', 'address', 'home_phone', 'work_phone', 'mobile', 'personnel_code', 'job_field', 'self_employed_job_name', 'employment_type', 'start_date', 'is_retired' )} ), (u' ', {'fields': ( 'is_active', 'is_staff', 'is_superuser', 'user_permissions', 'groups' ) }) # (u' ', {'fields': ('last_login', 'date_joined')}) ) return fieldsets def get_queryset(self, request): if request.user.is_superuser: return User.objects.all() return User.objects.filter(is_superuser=False)class GroupAdmin(admin.ModelAdmin): model = Group filter_horizontal = ('permissions',) def formfield_for_manytomany(self, db_field, request, **kwargs): if db_field.name == ""permissions"" and not request.user.is_superuser: kwargs[""queryset""] = Permission.objects.exclude( Q(codename__startswith='add_') | Q(codename__startswith='change_') | Q(codename__startswith='delete_') ) return super(GroupAdmin, self).formfield_for_manytomany( db_field, request, **kwargs ) def save_model(self, request, group, form, change): perms = [] for p in group.permissions.all(): if p.codename.startswith('add_') or \ p.codename.startswith('change_') or \ p.codename.startswith('delete_'): perms.append(p) super(GroupAdmin, self).save_model(request, group, form, change) form.cleaned_data['permissions'] = list( form.cleaned_data['permissions'] ) if not request.user.is_superuser: form.cleaned_data['permissions'].extend(perms) form.cleaned_data['permissions'] = list(set( form.cleaned_data['permissions'])) group.save()# register new user adminadmin.site.unregister(User)admin.site.register(User, UserAdmin)admin.site.unregister(Group)admin.site.register(Group, GroupAdmin) <VirtualHost *:80> DocumentRoot ""/var/www/wordpress""# ServerName localhost# Alias /wordpress /var/www/wordpress# <Directory /var/www/wordpress># Options Indexes FollowSymLinks# AllowOverride None# Order Deny,Allow# Allow from all# </Directory> WSGIScriptAlias /m3s /var/www/m3s/HomeMakers/wsgi.py #ProxyPass /m3s/ http://127.0.0.1:8000/ #ProxyPassReverse /m3s/ http://127.0.0.1:8000/ #<Proxy http://127.0.0.1:8000/m3s/> # Order Allow,Deny # Allow from all #</Proxy> # WSGIDaemonProcess sentry python-path=/var/www/sentry/venv/lib/python2.7/site-packages # WSGIScriptAlias /exceptions/tracker /var/www/sentry/venv/lib/python2.7/site-packages/sentry/wsgi.py Alias /ufiles /var/www/m3s/media_files Alias /static /var/www/m3s/sfiles Alias /_static /var/www/sentry/venv/lib/python2.7/site-packages/sentry/static Alias /mydb/admin /usr/share/phpmyadmin <Directory ""/var/www/m3s/HomeMakers/""> Options +ExecCGI Order allow,deny Allow from all Require all granted </Directory> <Directory /var/www/m3s/sfiles/> Order allow,deny Allow from all </Directory> ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined</VirtualHost> [Sun Jan 29 21:15:47.896062 2017] [wsgi:warn] [pid 7596] mod_wsgi: Compiled for Python/2.7.11.[Sun Jan 29 21:15:47.896100 2017] [wsgi:warn] [pid 7596] mod_wsgi: Runtime using Python/2.7.11+.[Sun Jan 29 21:15:47.898887 2017] [mpm_prefork:notice] [pid 7596] AH00163: Apache/2.4.18 (Ubuntu) mod_wsgi/4.3.0 Python/2.7.11+ configured -- resuming normal operations[Sun Jan 29 21:15:47.898913 2017] [core:notice] [pid 7596] AH00094: Command line: '/usr/sbin/apache2'[Sun Jan 29 21:16:43.833245 2017] [wsgi:info] [pid 7599] [client 84.241.62.118:44316] mod_wsgi (pid=7599, process='', application='ut3taavoni.ir|/m3s'): Loading WSGI script '/var/www/m3s/HomeMakers/wsgi.py'.[Sun Jan 29 21:16:45.317557 2017] [wsgi:error] [pid 7599] DEBUG 2017-01-29 21:16:45,317 base 7599 -1220638144 Configuring Raven for host: <raven.conf.remote.RemoteConfig object at 0xada5a12c>[Sun Jan 29 21:16:47.484799 2017] [wsgi:info] [pid 7602] [client 84.241.62.118:42751] mod_wsgi (pid=7602, process='', application='ut3taavoni.ir|/m3s'): Loading WSGI script '/var/www/m3s/HomeMakers/wsgi.py'., referer: http://ut3taavoni.ir/m3s/m3s-panel/members/user/501/?_changelist_filters=q%3D0065231619[Sun Jan 29 21:16:48.899865 2017] [wsgi:error] [pid 7602] DEBUG 2017-01-29 21:16:48,899 base 7602 -1220638144 Configuring Raven for host: <raven.conf.remote.RemoteConfig object at 0xada5a12c>[Sun Jan 29 21:17:33.961983 2017] [wsgi:info] [pid 7603] [client 84.241.62.118:20515] mod_wsgi (pid=7603, process='', application='ut3taavoni.ir|/m3s'): Loading WSGI script '/var/www/m3s/HomeMakers/wsgi.py'., referer: http://ut3taavoni.ir/m3s/m3s-panel/members/user/501/?_changelist_filters=q%3D0065231619[Sun Jan 29 21:17:35.360116 2017] [wsgi:error] [pid 7603] DEBUG 2017-01-29 21:17:35,360 base 7603 -1220638144 Configuring Raven for host: <raven.conf.remote.RemoteConfig object at 0xada5a1ac> 192.0.102.40 - - [29/Jan/2017:22:37:30 +0330] ""HEAD / HTTP/1.1"" 200 372 ""-"" ""jetmon/1.0 (Jetpack Site Uptime Monitor by WordPress.com)""xxx.241.62.118 - - [29/Jan/2017:22:37:56 +0330] ""GET /m3s/m3s-panel/members/user/ HTTP/1.1"" 200 4627 ""-"" ""Mozilla/5.0 (X11; Linux x86_64; rv:50.0) Gecko/20100101 Firefox/50.0""xxx.241.62.118 - - [29/Jan/2017:22:37:57 +0330] ""GET /m3s/m3s-panel/jsi18n/ HTTP/1.1"" 200 10588 ""http://ut3taavoni.ir/m3s/m3s-panel/members/user/"" ""Mozilla/5.0 (X11; Linux x86_64; rv:50.0) Gecko/20100101 Firefox/50.0""xxx.241.62.118 - - [29/Jan/2017:22:38:51 +0330] ""GET /m3s/m3s-panel/members/user/?q=0065231619 HTTP/1.1"" 200 4195 ""http://ut3taavoni.ir/m3s/m3s-panel/members/user/"" ""Mozilla/5.0 (X11; Linux x86_64; rv:50.0) Gecko/20100101 Firefox/50.0""xxx.241.62.118 - - [29/Jan/2017:22:38:51 +0330] ""GET /m3s/m3s-panel/jsi18n/ HTTP/1.1"" 200 10588 ""http://ut3taavoni.ir/m3s/m3s-panel/members/user/?q=0065231619"" ""Mozilla/5.0 (X11; Linux x86_64; rv:50.0) Gecko/20100101 Firefox/50.0""xxx.241.62.118 - - [29/Jan/2017:22:38:54 +0330] ""GET /m3s/m3s-panel/members/user/501/?_changelist_filters=q%3D0065231619 HTTP/1.1"" 200 14967 ""http://ut3taavoni.ir/m3s/m3s-panel/members/user/?q=0065231619"" ""Mozilla/5.0 (X11; Linux x86_64; rv:50.0) Gecko/20100101 Firefox/50.0""xxx.241.62.118 - - [29/Jan/2017:22:38:55 +0330] ""GET /m3s/m3s-panel/jsi18n/ HTTP/1.1"" 200 10588 ""http://ut3taavoni.ir/m3s/m3s-panel/members/user/501/?_changelist_filters=q%3D0065231619"" ""Mozilla/5.0 (X11; Linux x86_64; rv:50.0) Gecko/20100101 Firefox/50.0""xxx.241.62.118 - - [29/Jan/2017:22:38:55 +0330] ""GET /m3s/_nested_admin/server-data.js HTTP/1.1"" 200 388 ""http://ut3taavoni.ir/m3s/m3s-panel/members/user/501/?_changelist_filters=q%3D0065231619"" ""Mozilla/5.0 (X11; Linux x86_64; rv:50.0) Gecko/20100101 Firefox/50.0"" 7 0.754812317 5.113.18.90 -> xxx.156.28.145 HTTP 1434 POST /m3s/m3s-panel/members/user/501/ HTTP/1.1 [Malformed Packet] Content-Type: multipart/form-data; boundary=---------------------------51995842320268179811054389612Content-Length: 4614-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""csrfmiddlewaretoken""STMAQ1bSTuWsl9CelQBK5S2QjUKIfZ1Z-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""username""9265291619-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""first_name""-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""last_name""-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""email""jafariphd@ut.ac.ir-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""gender""0-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""birth_date_0""15-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""birth_date_1""6-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""birth_date_2""1356-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""academic_degree""5-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""picture""; filename=""""Content-Type: application/octet-stream-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""international_card_scaned_file""; filename=""""Content-Type: application/octet-stream-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""certificate_scaned_file_page1""; filename=""""Content-Type: application/octet-stream-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""certificate_scaned_file_page2""; filename=""""Content-Type: application/octet-stream-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""academic_degree_scaned_file""; filename=""""Content-Type: application/octet-stream-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""job_edict_document""; filename=""""Content-Type: application/octet-stream-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""certificate_no""11909-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""birth_place""-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""address"" - - - - - 7- -----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""home_phone""66915902-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""work_phone""66409696-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""mobile""09125114282-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""personnel_code""26687-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""job_field""1-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""self_employed_job_name"" -----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""employment_type""3-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""start_date_0""1-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""start_date_1""1-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""start_date_2""1385-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""is_active""on-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""is_staff""on-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""groups""3-----------------------------51995842320268179811054389612Content-Disposition: form-data; name=""_continue"" -----------------------------51995842320268179811054389612--","Malformed Packet: Django admin nested form can't submit, connection was reset"
Upload CSV file into table Mirosoft Azure cloud table," I am trying to upload a .csv file into Microsoft Azure storage account using python. I have found C-sharp code to write a data to blob storage. But, I don't know C# language. I need to upload .csv file using python.Is there any example of python to upload contents of CSV file to Azure storage? <code> ",Upload CSV file into Microsoft Azure storage account using python
Upload CSV file into table Microsoft Azure cloud table," I am trying to upload a .csv file into Microsoft Azure storage account using python. I have found C-sharp code to write a data to blob storage. But, I don't know C# language. I need to upload .csv file using python.Is there any example of python to upload contents of CSV file to Azure storage? <code> ",Upload CSV file into Microsoft Azure storage account using python
Using np.where with 2D array," I would like to know how I use np.where with 2D arrayI have the following array: I want to find this array: But when I use np.where(): It returns: I can't understand what it means. Can someone explain this for me? <code>  arr1 = np.array([[ 3., 0.], [ 3., 1.], [ 3., 2.], [ 3., 3.], [ 3., 6.], [ 3., 5.]]) arr2 = np.array([3.,0.]) np.where(arr1 == arr2) (array([0, 0, 1, 2, 3, 4, 5]), array([0, 1, 0, 0, 0, 0, 0]))",Using np.where to find matching row in 2D array
Convert List to Pandas Dataframe," I need to Convert my list into a one column pandas dataframe Current List (len=3): Required Pandas DF (shape =3,): Please note the numbers represent index in Required Pandas DF above. <code>  ['Thanks You', 'Its fine no problem', 'Are you sure'] 0 Thank You1 Its fine no problem2 Are you sure",Convert List to Pandas Dataframe Column
How can I create an infinite iterator?," I've created a function that generates a list of alphabets incrementing continuously. A, B, C ..., Z. After Z, it goes to AA, AB, AC ...AZ. This pattern repeats. This is similar to MS Excel's column names. At the moment, this function generates a finite list of alphabets. I can then iterate over it in conjunction with some finite list, e.g. 0-10. See my code below. What I'd like is to create a generator that will give me an infinitely long list of incrementing alphabets. <code>  _column_name_generator() = ['A', 'B', ..., 'AA', 'AB', ..., 'BA', 'BB', ..., 'CV'] import stringdef _column_name_generator(): column_names = [] for x in range(0, 100): if x < 26: column_names.append(string.ascii_uppercase[x % 26]) else: column_names.append(column_names[x/26 - 1] + string.ascii_uppercase[x % 26]) return column_namescontainer = []for column_name, num in zip(_column_name_generator(), range(0, 10)): container.append(column_name + str(num))print _column_name_generator()print containercontainer = ['A0', 'B1', 'C2', 'D3', 'E4', 'F5', 'G6', 'H7', 'I8', 'J9']",How to create an infinite iterator to generate an incrementing alphabet pattern?
get data from form Python," I need get data from form.I use JavaScript to create form: then I need to get data from the input field which name=""json"".Here is my view function: But I get an error:builtins.KeyError KeyError: 'json'Help me get data from form. <code>  <script> function checkAuth() { var user = ADAL.getCachedUser(); if (user) { var form = $('<form style=""position: absolute; width: 0; height: 0; opacity: 0; display: none; visibility: hidden;"" method=""POST"" action= ""{{ url_for(""general.microsoft"") }}"">'); form.append('<input type=""hidden"" name=""token"" value=""' + ADAL.getCachedToken(ADAL.config.clientId) + '"">'); form.append('<input type=""hidden"" name=""json"" value=""' + encodeURIComponent(JSON.stringify(user)) + '"">'); $(""body"").append(form); form.submit(); } }</script> @general.route(""/microsoft/"", methods=[""GET"", ""POST""])@csrf.exemptdef microsoft(): form = cgi.FieldStorage() name = form['json'].value return name",How to get form data in Flask?
how to setup selenium python environment for firefox ?," How can I set up a Selenium Python environment for Firefox?I am using Firefox 50, Selenium 3, Python3.5. I tried with many things binary and copying the geckodriver in the environment variable PATH, etc. <code> ",How to set up a Selenium Python environment for Firefox
how to setup selenium python environment for firefox?," How can I set up a Selenium Python environment for Firefox?I am using Firefox 50, Selenium 3, Python3.5. I tried with many things binary and copying the geckodriver in the environment variable PATH, etc. <code> ",How to set up a Selenium Python environment for Firefox
How Can I install Twisted on Python3.6 and CentOs," I use the newest Python on Centos 7, and a dedicated virtualenv When I install scrapy, the error and when I install twisted independence, the error So why can't install Twisted on Python3.6? Does something wrong on my environmentKmike suggested me to ask twisted's developer  <code>  (ENV) [luoc@study ~ ]$ lsb_release -aLSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarchDistributor ID: CentOSDescription: CentOS Linux release 7.3.1611 (Core) Release: 7.3.1611Codename: Core(ENV) [luoc@study ~ ]$ python --versionPython 3.6.0 (ENV) [luoc@study ~ ]$ pip install scrapyCollecting scrapy Using cached Scrapy-1.3.2-py2.py3-none-any.whlCollecting cssselect>=0.9 (from scrapy) Using cached cssselect-1.0.1-py2.py3-none-any.whlRequirement already satisfied: six>=1.5.2 in ./ENV/lib/python3.6/site-packages (from scrapy)Collecting Twisted>=13.1.0 (from scrapy) Could not find a version that satisfies the requirement Twisted>=13.1.0 (from scrapy) (from versions: )No matching distribution found for Twisted>=13.1.0 (from scrapy) (ENV) [luoc@study ~ ]$ pip install TwistedCollecting Twisted Could not find a version that satisfies the requirement Twisted (from versions: )No matching distribution found for Twisted(ENV) [luoc@study ~ ]$ pip install --verbose TwistedCollecting Twisted 1 location(s) to search for versions of Twisted: * https://pypi.python.org/simple/twisted/ Getting page https://pypi.python.org/simple/twisted/ Looking up ""https://pypi.python.org/simple/twisted/"" in the cache Current age based on date: 40208 Freshness lifetime from max-age: 600 Freshness lifetime from request max-age: 600 The cached response is ""stale"" with no etag, purging Starting new HTTPS connection (1): pypi.python.org ""GET /simple/twisted/ HTTP/1.1"" 200 10196 Updating cache with response from ""https://pypi.python.org/simple/twisted/"" Caching b/c date exists and max-age > 0 Analyzing links from page https://pypi.python.org/simple/twisted/Skipping link ...Could not find a version that satisfies the requirement Twisted (from versions: )Cleaning up...No matching distribution found for TwistedException information:Traceback (most recent call last): File ""/home/luoc/ENV/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main status = self.run(options, args) File ""/home/luoc/ENV/lib/python3.6/site-packages/pip/commands/install.py"", line 335, in run wb.build(autobuilding=True) File ""/home/luoc/ENV/lib/python3.6/site-packages/pip/wheel.py"", line 749, in build self.requirement_set.prepare_files(self.finder) File ""/home/luoc/ENV/lib/python3.6/site-packages/pip/req/req_set.py"", line 380, in prepare_files ignore_dependencies=self.ignore_dependencies)) File ""/home/luoc/ENV/lib/python3.6/site-packages/pip/req/req_set.py"", line 554, in _prepare_file require_hashes File ""/home/luoc/ENV/lib/python3.6/site-packages/pip/req/req_install.py"", line 278, in populate_link self.link = finder.find_requirement(self, upgrade) File ""/home/luoc/ENV/lib/python3.6/site-packages/pip/index.py"", line 514, in find_requirement 'No matching distribution found for %s' % reqpip.exceptions.DistributionNotFound: No matching distribution found for Twisted",How Can I install Twisted + Scrapy on Python3.6 and CentOs
SystemExit: 2 error when calling parse_args()," I'm learning basics of Python and got already stuck at the beginning of argparse tutorial. I'm getting the following error: a %tb command gives the following output: How could I fix this problem? <code>  import argparseparser = argparse.ArgumentParser()args = parser.parse_args() usage: __main__.py [-h] echo__main__.py: error: unrecognized arguments: -fAn exception has occurred, use %tb to see the full traceback.SystemExit: 2 SystemExit Traceback (most recent call last)<ipython-input-16-843cc484f12f> in <module>()----> 1 args = parser.parse_args()C:\Users\Haik\Anaconda2\lib\argparse.pyc in parse_args(self, args, namespace) 1702 if argv: 1703 msg = _('unrecognized arguments: %s')-> 1704 self.error(msg % ' '.join(argv)) 1705 return args 1706 C:\Users\Haik\Anaconda2\lib\argparse.pyc in error(self, message) 2372 """""" 2373 self.print_usage(_sys.stderr)-> 2374 self.exit(2, _('%s: error: %s\n') % (self.prog, message))C:\Users\Haik\Anaconda2\lib\argparse.pyc in exit(self, status, message) 2360 if message: 2361 self._print_message(message, _sys.stderr)-> 2362 _sys.exit(status) 2363 2364 def error(self, message):SystemExit: 2",SystemExit: 2 error when calling parse_args() within ipython
Formatting two numbers in python - wrong results," I am coping with the formatting of numbers. I assumed that the .format allows to use multiple arguments: Returns: Instead of: On the other hand, this works fine: Returns: Any idea where is the problem? <code>  a = 1.11111111111b = 0.9s = '({0:.2f}, {0:.2f})'.format(a, b)print(s) '(1.11, 1.11)' '(1.11, 0.90)' '({}, {})'.format(a, b) '(1.11111111111111, 0.9)'",Formatting two numbers in Python
Check if a matrix consists of identical tiles," I have a mxn matrix A, where m%t = n%t = 0, so that a smaller txt matrix B tiles the matrix without borders or overlaps. I want to check if A consists entirely of tiles from B without calculating a tiling as an intermediate step as efficiently as possible. Furthermore for my special use case, it is not necessary to know B. It is enough to test if A strictly repeats itself every txt tile in every direction.Numeric examples: So far, I calculate a comparison matrix C, which simply is a tiling of B to fit the size of A: Is there a faster way, without calculating C? <code>  A = [[1, 0, 1, 0], [0, 1, 0, 1], [1, 0, 1, 0], [0, 1, 0, 1]]B.shape = [2,2]--> TrueB.shape = [1,1]--> False import numpy as npx,y = B.shapex_a, y_a = A.shapex_t = x_a/xy_t = y_a/yB_dash = A[:x, :y]C = np.tile(B_dash,(x_t, y_t))np.count_nonzero(A-C)",Count number of occurrences of an array without overlap in another array
Windows 10: How to uninstall Python and all packages, I wish to uninstall Python 2.7 and all packages connected to it. I initially installed Python from the official website and I installed all packages using the pip install command. Would uninstalling Python from the control panel also uninstall all packages automatically?The reason I want to uninstall Python is because I want to use Anaconda in order to be able to manage packages more easily and also be able to install both Python 2 and 3 to switch between them back and forth. <code> ,How to uninstall Python and all packages
seaboard plot replicating legend," I am plotting two subplots using seaborn like so: However, each subplot has its own legend dictated by colours. Is it possible to remove one of these, and preferably place the remaining outside of the plot? I have tried using ax1.legend_.remove() but that didn't work.  <code>  fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)sns.swarmplot(flowers[0], flowers[1], hue=colours, ax=ax1)ax1.set(xlabel='Sepal Length', ylabel='Sepal Width')plt.legend(loc=""upper left"", bbox_to_anchor=(1, 1))sns.swarmplot(flowers[2], flowers[3], hue=colours, ax=ax2)ax2.set(xlabel='Petal Length', ylabel='Petal Width')sns.plt.show()",How to keep only one legend in seaborn subplots
how to keep only one legend in seaborn subplots," I am plotting two subplots using seaborn like so: However, each subplot has its own legend dictated by colours. Is it possible to remove one of these, and preferably place the remaining outside of the plot? I have tried using ax1.legend_.remove() but that didn't work.  <code>  fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)sns.swarmplot(flowers[0], flowers[1], hue=colours, ax=ax1)ax1.set(xlabel='Sepal Length', ylabel='Sepal Width')plt.legend(loc=""upper left"", bbox_to_anchor=(1, 1))sns.swarmplot(flowers[2], flowers[3], hue=colours, ax=ax2)ax2.set(xlabel='Petal Length', ylabel='Petal Width')sns.plt.show()",How to keep only one legend in seaborn subplots
"Why do I need to deploy a ""default"" app before I can deploy multiple services in GCP?"," Reading this doc it says ""You must initially deploy a version of your app to the default service before you can create and deploy subsequent services.""I don't understand this because I thought the GAE microservices were separate things as in:But it seems this is not an accurate depiction of how GAE microservices work? Is there like a master controller ""default"" service that sets top level config or does some kind of routing? If I'm just running a bunch of non web apps (meaning apps that wil run on a scheduled and process data) and a frontend ""app"" for accepting web requests isn't necessary than why do I still need to create the default service? <code> ","Why do I need to deploy a ""default"" app before I can deploy multiple services in GAE?"
"Try using .loc[row_indexer,col_indexer] = value instead when creating a new column"," Trying to create a new column in the netc df but i get the warning whats the proper way to create a field in the newer version of Pandas to avoid getting the warning? <code>  netc[""DeltaAMPP""] = netc.LOAD_AM - netc.VPP12_AMC:\Anaconda\lib\site-packages\ipykernel\__main__.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc[row_indexer,col_indexer] = value instead pd.__version__Out[45]:u'0.19.2+0.g825876c.dirty'",Correct way to set new column in pandas DataFrame to avoid SettingWithCopyWarning
What is the time complexity of collections.Counter() in Python vs TreeMap in Java?," returns output: Since the result is in descended sorted order of values, does this mean the cost of building the Counter is O(nlogn) and not O(n)?Also, what is the equivalent of the collections.Counter in Java?  <code>  collection.Counter(""bcdefffaa"") Counter({'f': 3, 'a': 2, 'c': 1, 'b': 1, 'e': 1, 'd': 1})",What is the time complexity of collections.Counter() in Python?
How to use ImageDataGenerator without deforming aspect ratio," I have a folder with images of different sizes. I tried to use ImageDataGenerator with flow_from_directory for batch loading / data augmentation.Is there a way to keep the aspect ratios of my images ? It seems like the images are being stretched to target_size : I would like to ""pad"" my images without deforming them (filling the gaps with a constant value)Here's my code : Images are being stretched to (256,256). <code>  datagen = ImageDataGenerator( rescale = 1./255, fill_mode='constant')generator = datagen.flow_from_directory( 'data/images', target_size=(256,256), color_mode = 'grayscale', batch_size=99, class_mode=None, shuffle=False)",Keras - How to use ImageDataGenerator without deforming aspect ratio
How to parse a yaml file with multiple documents?," Here is my parsing code: Here is a sample of the file: In the traceback, note that it starts having a problem at the ... What I would like is for it to parse each of these documents as a separate object, perhaps all of them in the same list, or pretty much anything else that would work with the PyYAML module. I believe the ... is actually valid YAML so I am surprised that it doesn't handle it automatically.  <code>  import yamldef yaml_as_python(val): """"""Convert YAML to dict"""""" try: return yaml.load_all(val) except yaml.YAMLError as exc: return excwith open('circuits-small.yaml','r') as input_file: results = yaml_as_python(input_file) print results for value in results: print value ingests: - timestamp: 1970-01-01T00:00:00.000Z id: SwitchBank_35496721 attrs: Feeder: Line_928 Switch.normalOpen: 'true' IdentifiedObject.description: SwitchBank IdentifiedObject.mRID: SwitchBank_35496721 PowerSystemResource.circuit: '928' IdentifiedObject.name: SwitchBank_35496721 IdentifiedObject.aliasName: SwitchBank_35496721 loc: vector [43.05292, -76.126800000000003, 0.0] kind: SwitchBank - timestamp: 1970-01-01T00:00:00.000Z id: UndergroundDistributionLineSegment_34862802 attrs: Feeder: Line_928 status: de-energized IdentifiedObject.description: UndergroundDistributionLineSegment IdentifiedObject.mRID: UndergroundDistributionLineSegment_34862802 PowerSystemResource.circuit: '928' IdentifiedObject.name: UndergroundDistributionLineSegment_34862802 path: - vector [43.052942000000002, -76.126716000000002, 0.0] - vector [43.052585000000001, -76.126515999999995, 0.0] kind: UndergroundDistributionLineSegment - timestamp: 1970-01-01T00:00:00.000Z id: UndergroundDistributionLineSegment_34806014 attrs: Feeder: Line_928 status: de-energized IdentifiedObject.description: UndergroundDistributionLineSegment IdentifiedObject.mRID: UndergroundDistributionLineSegment_34806014 PowerSystemResource.circuit: '928' IdentifiedObject.name: UndergroundDistributionLineSegment_34806014 path: - vector [43.05292, -76.126800000000003, 0.0] - vector [43.052928999999999, -76.126766000000003, 0.0] - vector [43.052942000000002, -76.126716000000002, 0.0] kind: UndergroundDistributionLineSegment... ingests: - timestamp: 1970-01-01T00:00:00.000Z id: OverheadDistributionLineSegment_31168454 Traceback (most recent call last): File ""convert.py"", line 29, in <module> for value in results: File ""/Users/conduce-laptop/anaconda2/lib/python2.7/site-packages/yaml/__init__.py"", line 82, in load_all while loader.check_data(): File ""/Users/conduce-laptop/anaconda2/lib/python2.7/site-packages/yaml/constructor.py"", line 28, in check_data return self.check_node() File ""/Users/conduce-laptop/anaconda2/lib/python2.7/site-packages/yaml/composer.py"", line 18, in check_node if self.check_event(StreamStartEvent): File ""/Users/conduce-laptop/anaconda2/lib/python2.7/site-packages/yaml/parser.py"", line 98, in check_event self.current_event = self.state() File ""/Users/conduce-laptop/anaconda2/lib/python2.7/site-packages/yaml/parser.py"", line 174, in parse_document_start self.peek_token().start_mark)yaml.parser.ParserError: expected '<document start>', but found '<block mapping start>' in ""circuits-small.yaml"", line 42, column 1",How to parse a YAML file with multiple documents?
numpy multidemensional indexing and the function 'take'," On odd days of the week I almost understand multidimensional indexing in numpy. Numpy has a function 'take' which seems to do what I want but with the added bonus that I can control what happens if the indexing is out of rangectSpecifically, I have a 3-dimensional array to ask as the lookup-table and a 2x2 array of 3-long vectors to act as indexes into the table IIUC, if I were to write lut[arr] then arr is treated as a 2x2x3 array of numbers and when these are used as indexes into lut they each return a 13x13 array. This explains why lut[arr].shape is (2, 2, 3, 13, 13).I can make it do what I want by writing and now the three terms act as if they have been zipped to produce a 2x2 array of tuples and lut[<tuple>] produces a single element from lut. The final result is a 2x2 array of entries from lut, just what I want.I have read the documentation for the 'take' function ... This function does the same thing as fancy indexing (indexing arrays using arrays); however, it can be easier to use if you need elements along a given axis.and axis : int, optional The axis over which to select values.Perhaps naively, I thought that setting axis=2 I would get three values to use as a 3-tuple to perform the lookup but actually so it's clear I don't understand what is going on. Can anyone show me how to achieve what I want? <code>  lut = np.ones([13,13,13],np.bool) arr = np.arange(12).reshape([2,2,3]) % 13 lut[ arr[:,:,0],arr[:,:,1],arr[:,:,2] ] #(is there a better way to write this?) np.take(lut,arr).shape = (2, 2, 3)np.take(lut,arr,axis=0).shape = (2, 2, 3, 13, 13)np.take(lut,arr,axis=1).shape = (13, 2, 2, 3, 13)np.take(lut,arr,axis=2).shape = (13, 13, 2, 2, 3)",numpy multidimensional indexing and the function 'take'
Pygame: How to correctly use get-rect()," I'm trying to understand how get_rect() works. In this simple example, I have two images and want to obtain the location of the second and move the first image to the second image.I have looked at a variety of examples online and cannot get this to work. What am I doing wrong? <code>  import pygame, sysfrom pygame.locals import *import timepygame.init()FPS = 10 # frames per second settingfpsClock = pygame.time.Clock()# Set up the windowDISPLAYSURF = pygame.display.set_mode((600, 400), 0, 32)pygame.display.set_caption('Test program for get_rect()')WHITE = (255, 255, 255)# Load two imagesbaseImg = pygame.image.load('image1.jpg')spaceshipImg = pygame.image.load('image2.jpg')DISPLAYSURF.fill(WHITE)# Place one image at the bottom of the screenDISPLAYSURF.blit(baseImg, (300, 300))pygame.display.update()# Place the second image at the top of the screenDISPLAYSURF.blit(spaceshipImg, (300, 0))pygame.display.update()# Wait for one secondtime.sleep(1)# Obtain the rectangle for each imagebaseRect = baseImg.get_rect()spaceshipRect = spaceshipImg.get_rect()# This is where I believe I'm going wrong# I understand this to obtain the x,y of the spaceship image# Set the xy coordinates for the top image to the xy of the bottom imagespaceshipRect.x = baseRect.xspaceshipRect.y = baseRect.y# Move the top image to new xy position# However this doesn't workDISPLAYSURF.blit(spaceshipImg, (spaceshipRect.x, spaceshipRect.y))pygame.display.update()while True: for event in pygame.event.get(): if event.type == QUIT: pygame.quit() sys.exit()",Pygame: How to correctly use get_rect()
Use .corr to get the the correlation between two columns," I have the following pandas dataframe Top15: I create a column that estimates the number of citable documents per person: I want to know the correlation between the number of citable documents per capita and the energy supply per capita. So I use the .corr() method (Pearson's correlation): I want to return a single number, but the result is: <code>  Top15['PopEst'] = Top15['Energy Supply'] / Top15['Energy Supply per Capita']Top15['Citable docs per Capita'] = Top15['Citable documents'] / Top15['PopEst'] data = Top15[['Citable docs per Capita','Energy Supply per Capita']]correlation = data.corr(method='pearson')",Use .corr to get the correlation between two columns
redis await untill key exists," I'm new to Redis and was wondering if there is a way to be able to await geting a value by it's key until the key exists. Minimal code: As you know, if such key doesnt exist, it return's None. But since in my project, seting key value pair to redis takes place in another application, I want the redis_connection get method to block untill key exists.Is such expectation even valid?  <code>  async def handler(): data = await self._fetch(key)async def _fetch(key): return self.redis_connection.get(key)",redis block until key exists
redis block untill key exists," I'm new to Redis and was wondering if there is a way to be able to await geting a value by it's key until the key exists. Minimal code: As you know, if such key doesnt exist, it return's None. But since in my project, seting key value pair to redis takes place in another application, I want the redis_connection get method to block untill key exists.Is such expectation even valid?  <code>  async def handler(): data = await self._fetch(key)async def _fetch(key): return self.redis_connection.get(key)",redis block until key exists
How to create both short and long options for one option in click?," How do I specify both a short option and long option for the same option?e.g., for the following, I also want to use -c for --count: e.g., References:click documentation in a single pdfclick sourcecode on githubclick websiteclick PyPI page <code>  import click@click.command()@click.option('--count', default=1, help='count of something')def my_command(count): click.echo('count=[%s]' % count)if __name__ == '__main__': my_command() $ python my_command.py --count=2count=[2]$ python my_command.py -c 3count=[3]",How to create both short and long options for one option in click (python package)?
normalization misses polish caracters," I have a big dataframe with people data. I would like to flatten all weird diacritics and convert them to the closest ascii character. Based on a solution I found in SO I do the following: It works for most of the cases (haven't checked them all) however I have noticed it misses letter '' in Polish. For example Lech Wasa is translated to Lech Waesa while my expectation would be to see Lech Walesa. My guess would be that it's what ignore parameter does in str.encode method. Any idea how to make it work for any diacritic?  <code>  for column in df.columns: df[column] = df[column].astype(""str"").str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')",normalization misses polish characters
Best way to save a trained model in PyTroch?," I was looking for alternative ways to save a trained model in PyTorch. So far, I have found two alternatives.torch.save() to save a model and torch.load() to load a model.model.state_dict() to save a trained model and model.load_state_dict() to load the saved model.I have come across to this discussion where approach 2 is recommended over approach 1.My question is, why the second approach is preferred? Is it only because torch.nn modules have those two function and we are encouraged to use them? <code> ",Best way to save a trained model in PyTorch?
Angular2 headers incorrectly cause HEAD request and no response from API," Receiving these error when using the OPTIONS verb in Angular2 http.get(url, options), even though the appropriate CORS headers are set in Falcon Rest API. XMLHttpRequest cannot load http://localhost:8000/names. Request header field Authorization is not allowed by Access-Control-Allow-Headers in preflight response. For non OPTIONS / normal http.get() requests this works fine.  <code>  resp.set_header(""Access-Control-Allow-Origin"", ""*"") resp.set_header(""Access-Control-Allow-Credentials"", ""true"") resp.set_header(""Access-Control-Allow-Methods"", ""GET,HEAD,OPTIONS,POST,PUT"") resp.set_header(""Access-Control-Allow-Headers"", ""Access-Control-Allow-Headers, Origin,Accept, X-Requested-With, Content-Type, Access-Control-Request-Method, Access-Control-Request-Headers"")",CORS failure in with Python Falcon even with heads for Auth Pre-Flight
python re add character to matched string," I have a long string which is a paragraph, however there is no white space after periods. For example: I am trying to use re.sub to solve this problem, but the output is not what I expected.This is what I did: I am matching the first char of each sentence, and I want to put a white space before it. My match pattern is (?<=\.)., which (supposedly) checks for any character that appears after a period. I learned from other stackoverflow questions that \1 matches the last matched pattern, so I wrote my replace pattern as \1, a space followed by the previously matched string. Here is the output: Instead of matching any character preceded by a period and adding a space before it, re.sub replaced the matched character with \x01. Why? How do I add a character before a matched string? <code>  para = ""I saw this film about 20 years ago and remember it as being particularly nasty. I believe it is based on a true incident: a young man breaks into a nurses\' home and rapes, tortures and kills various women.It is in black and white but saves the colour for one shocking shot.At the end the film seems to be trying to make some political statement but it just comes across as confused and obscene.Avoid."" re.sub(""(?<=\.)."", "" \1"", para) ""I saw this film about 20 years ago and remember it as being particularly nasty. \x01I believe it is based on a true incident: a young man breaks into a nurses\' home and rapes, tortures and kills various women. \x01t is in black and white but saves the colour for one shocking shot. \x01t the end the film seems to be trying to make some political statement but it just comes across as confused and obscene. \x01void. \x01",Regex add character to matched string
Python abstract classes with varying amounts of parameters," I was wondering if its possible when creating an abstract class with abstract methods if its possible to allow the implementations of those methods in the derived classes to have different amounts of parameters for each function.I currently have for my abstract class But I want to be able to implement it in one class with set having 1 parameter and get having 2(set(param1) and get(param1, param2)), and then in another class also inherit it but have 0 parameters for set and 2 for get(set() and get(param1, param2)).Is this possible and if so how would I go about doing it <code>  from abc import ABCMeta, abstractmethodclass View(metaclass=ABCMeta): @abstractmethod def set(self): pass @abstractmethod def get(self): pass",Abstract classes with varying amounts of parameters
Python flask ajax get image from one view to an other view," Since yesterday I am trying to understand how I can use the encoded base64 image from a certain view in an other view.I need to replace my form.company_logo_image_path.data which is the original image with the new image which is resized. The new resized image is sent via AJAX to a new view.Here my AJAX: I created a new view where the image is decoded and stored in the body variable: I tested this by saving the image to a folder on my local machine and it worked so the body variable stores the resized image correctly.Now I need this image to be sent to an other view where it will be uploaded to AWS3 and I will use this image instead of form.company_logo_image_path.data: The problem here is I get a Bad Request site if I try to access the result of the resize_image function from the first view. How can I access the new Image?I am working on this problem since yesterday and it is the biggest issue I ever had so far, here is my old question with my progress:Old question with my progress and triesEDITIt doesnt matter what I try, sending to ""/profil/unternehmen-bearbeiten"" also results in a bad request error.Requesting the data a anywhere results in a bad request: The Exception here is the Bad RequestAlso requesting the canvas itself results in a bad request, the console in the browser doesnt tell me something useful which I can use/understand. Its the same as in the console in Eclipse, it gets a 400 Bad Request in the route where I try to send to: EDITFinally I made some serious progress! I was trying to request the data in if form.validate_on_submit():. I put the code now outside if form.validate_on_submit(): and I can now request the data, I am still getting problems, but from here I can keep on working! I am getting again a Bad Request here, but I understand now why. form.validate_on_submit(): itself is a POST request aswell, so I need the correct if condition and it will work (I guess).Basically the problem is:My ajax request and the form.validate_on_submit(): in the route where I am sending to are both POST requests, that is why I am getting Bad Request so often, there is a conflict. I was trying creating a custom form checkbox. I was trying moving the code and different other if conditions. I simply dont get it.My recent tries: <code>  var dataurl = canvas.toDataURL(""image/png"");$.ajax({ type: ""POST"", url: ""/profil/unternehmen-bearbeiten/resize-image"", data:{ a: dataurl }}).done(function() { console.log('sent');}); @app.route('/profil/unternehmen-bearbeiten/resize-image', methods=[""POST""])def resize_image(): data_url = request.values['a'] content = data_url.split(';')[1] image_encoded = content.split(',')[1] body = base64.decodestring(image_encoded.encode('utf-8')) return body @app.route('/profil/unternehmen-bearbeiten', methods=[""GET"", ""POST""])@login_required@check_confirmeddef edit_company(): the_company = Company.query.filter_by(users_id=current_user.id).first() form = EditCompanyForm(obj=the_company) used_file = the_company.company_logo_image_path if form.validate_on_submit(): form.populate_obj(the_company) imagedata = resize_image() print ""The"", imagedata if form.company_logo_image_path.data: upload_image_to_aws('baubedarf', ""userimages/"", form.company_logo_image_path, the_company,'company_logo_image_path', the_company.company_name)# ... try: print request.values['a']except Exception as e: print ""error"", e try: print request.form['uploading_canvas']except Exception as e: print ""error 1"", e if request.method == 'POST': print ""post"" file_data = request.values['a'] imagedata = resize_image(file_data) print ""The"", type(imagedata)if form.validate_on_submit(): # ... """"""if form.company_check_it.data == True: print ""post 1"" file_data = request.values['a'] imagedata = resize_image(file_data) print ""The"", type(imagedata)else: print ""post 3""""""""""""""if request.method == 'POST': print ""post"" file_data = request.values['a'] imagedata = resize_image(file_data) print ""The"", type(imagedata)""""""if form.validate_on_submit(): print ""post 2"" """""" if form.company_check_it.data == True: print ""post 1"" file_data = request.values['a'] imagedata = resize_image(file_data) print ""The"", type(imagedata) else: print ""post 3"" """""" if request.method == 'POST': print ""post"" file_data = request.values['a'] imagedata = resize_image(file_data) print ""The"", type(imagedata) form.populate_obj(the_company) """""" data_url = request.values['a'] print data_url content = data_url.split(';')[1] image_encoded = content.split(',')[1] body = base64.decodestring(image_encoded.encode('utf-8')) print type(body) """""" ",Python flask ajax get image - last EDIT is the issue
Getting empty predictions with multi-label classification in scikit learn," I am doing multilabel classification, where I try to predict correct labels for each document and here is my code: When running my code I get multiple warnings: When I print out predicted and true labels, cca half of all documents has it's predictions for labels empty.Why is this happening, is it related to warnings it prints out while training is running? How can I avoid those empty predictions?EDIT01:This is also happening when using other estimators than LinearSVC().I've tried RandomForestClassifier() and it gives empty predictions as well. Strange thing is, when I use cross_val_predict(classifier, X, y, method='predict_proba') for predicting probabilities for each label, instead of binary decisions 0/1, there is always at least one label per predicted set with probability > 0 for given document. So I dont know why is this label not chosen with binary decisioning? Or is binary decisioning evaluated in different way than probabilities?EDIT02:I have found an old post where OP was dealing with similar problem. Is this the same case? <code>  mlb = MultiLabelBinarizer()X = dataframe['body'].values y = mlb.fit_transform(dataframe['tag'].values)classifier = Pipeline([ ('vectorizer', CountVectorizer(lowercase=True, stop_words='english', max_df = 0.8, min_df = 10)), ('tfidf', TfidfTransformer()), ('clf', OneVsRestClassifier(LinearSVC()))])predicted = cross_val_predict(classifier, X, y) UserWarning: Label not :NUMBER: is present in all training examples.",UserWarning: Label not :NUMBER: is present in all training examples
Getting empty predictions with multi-label classification using scikit-learn," I am doing multilabel classification, where I try to predict correct labels for each document and here is my code: When running my code I get multiple warnings: When I print out predicted and true labels, cca half of all documents has it's predictions for labels empty.Why is this happening, is it related to warnings it prints out while training is running? How can I avoid those empty predictions?EDIT01:This is also happening when using other estimators than LinearSVC().I've tried RandomForestClassifier() and it gives empty predictions as well. Strange thing is, when I use cross_val_predict(classifier, X, y, method='predict_proba') for predicting probabilities for each label, instead of binary decisions 0/1, there is always at least one label per predicted set with probability > 0 for given document. So I dont know why is this label not chosen with binary decisioning? Or is binary decisioning evaluated in different way than probabilities?EDIT02:I have found an old post where OP was dealing with similar problem. Is this the same case? <code>  mlb = MultiLabelBinarizer()X = dataframe['body'].values y = mlb.fit_transform(dataframe['tag'].values)classifier = Pipeline([ ('vectorizer', CountVectorizer(lowercase=True, stop_words='english', max_df = 0.8, min_df = 10)), ('tfidf', TfidfTransformer()), ('clf', OneVsRestClassifier(LinearSVC()))])predicted = cross_val_predict(classifier, X, y) UserWarning: Label not :NUMBER: is present in all training examples.",UserWarning: Label not :NUMBER: is present in all training examples
"pithon 3, when i run this code get this error <filter object at 0x7fd83ff0> <filter object at 0x7feede10>"," When I run this code in Python 3: I get this error: I do not know what the error means - and it runs ok in Python 2.7.Can anyone suggestion a solution? <code>  languages = [""HTML"", ""JavaScript"", ""Python"", ""Ruby""]print( filter(lambda x: x == ""Python"",languages)) filter object at 0x7fd83ff0filter object at 0x7feede10",Filter object error in Python 3
Why is the ouput of print in python2 and python3 different with the same string?," In python2: In python3: Why does it have the byte ""\xc2"" here?Edit:I think when the string have a non-ascii character, python3 will append the byte ""\xc2"" to the string. (as @Ashraful Islam said)So how can I avoid this in python3? <code>  $ python2 -c 'print ""\x08\x04\x87\x18""' | hexdump -C00000000 08 04 87 18 0a |.....|00000005 $ python3 -c 'print(""\x08\x04\x87\x18"")' | hexdump -C00000000 08 04 c2 87 18 0a |......|00000006",Why is the output of print in python2 and python3 different with the same string?
How to install pandas from pip on windows cmd," I am trying to install pandas using pip to run some pandas-based Python programs. I already installed pip. I tried googling and SO'ing but didn't find a solution to this error. Can somebody share your inputs on this? Error: <code>  C:\> pip install pandas pip is not recognized as an internal or external command, operable program or batch file.",How to install pandas from pip on windows cmd?
"Replacing a text with \n in it, with a real \n output [Python]"," I am trying to get a config from a juniper router and I have the following problem:After setting this It brings me something like these and the problem is that I want to receive that information in this format: So I want the \n to actually be a new line, and not just to show me the ""\n"" string. <code>  stdin, stdout, stderr = client1.exec_command('show configuration interfaces %s' % SID) CONFIG = stdout.read() print CONFIG 'description El_otro_Puerto_de_la_Routing-instance;\nvlan-id 309;\nfamily inet {\n mtu 1600;\n address 10.100.10.10/24;\n}\n' 'description El_otro_Puerto_de_la_Routing-instance; nvlan-id 309; nfamily inet { mtu 1600; address 10.100.10.10/24; }","Replacing a text with \n in it, with a real \n output"
How can I locate something on my screen quickly in Python," I've tried using the pyautogui module and i's function that locates an image on the screen but it's processing time is about 5-10 seconds. Is there any other way for me to locate an image on the screen more quickly? Basically, I want a faster version of locateOnScreen(). <code>  pyautogui.locateOnScreen()",How can I locate something on my screen quickly in Python?
Deleteing related data in flask-sqlalchemy," I'm currently building a data model using sqlalchemy through flask-sqlalchemyThe database is on a Postgresql serverI am having trouble when deleting rows from a table that has relationships. In this case I have a number of treatment types, and one treatment. the treatment has a single treatment type assigned.As long as I have one or more treatments assigned a particular treatment Type, I wish that the treatment type cannot be deleted. As it is now it is deleted when I try.I have the following model: I can build some logic in my ""delete"" view that checks for assigned treatments, before deleting the treatment type, but in my opinion this should be a standard feature of a relational database. So in other words I must be doing something wrong.I delete the treatment type like so: As I said it is possible for me to do a check before deleting the treatment Type, but I would rather have sqlalchemy throw an error if there are relationship issues prior to deletion. <code>  class treatment(db.Model): __tablename__ = 'treatment' __table_args__ = (db.UniqueConstraint('title', 'tenant_uuid'),) id = db.Column(db.Integer, primary_key=True) uuid = db.Column(db.String(), nullable=False, unique=True) title = db.Column(db.String(), nullable=False) tenant_uuid = db.Column(db.String(), nullable=False) treatmentType_id = db.Column(db.Integer, db.ForeignKey('treatmentType.id')) riskResponse_id = db.Column(db.Integer, db.ForeignKey('riskResponse.id'))class treatmentType(db.Model): __tablename__ = 'treatmentType' __table_args__ = (db.UniqueConstraint('title', 'tenant_uuid'),) id = db.Column(db.Integer, primary_key=True) uuid = db.Column(db.String(), nullable=False, unique=True) title = db.Column(db.String(), nullable=False) tenant_uuid = db.Column(db.String(), nullable=False) treatments = db.relationship('treatment', backref='treatmentType', lazy='dynamic') entry = treatmentType.query.filter_by(tenant_uuid=session['tenant_uuid']).all() try: db.session.delete(entry) db.session.commit() return {'success': 'Treatment Type deleted'} except Exception as E: return {'error': unicode(E)}",Prevent deletion of parent row if it's child will be orphaned in SQLAlchemy
How to write a generator class in python," I see lot of examples of generator functions, but I want to know how to write generators for classes. Lets say, I wanted to write Fibonacci series as a class. Output: Why is the value self.a not getting printed? Also, how do I write unittest for generators?  <code>  class Fib: def __init__(self): self.a, self.b = 0, 1 def __next__(self): yield self.a self.a, self.b = self.b, self.a+self.bf = Fib()for i in range(3): print(next(f)) <generator object __next__ at 0x000000000A3E4F68><generator object __next__ at 0x000000000A3E4F68><generator object __next__ at 0x000000000A3E4F68>",How to write a generator class?
Check if <string> is in <string> python 3.6," In python, I'm trying to create a program which checks if 'numbers' are in a string, but I seem to get an error. Here's my code: Here's my error: I tried changing numbers into numbers = ""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""0"" which is basically removed the [] but this didn't work either. Hope I can get an answer; thanks :) <code>  numbers = [""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""0""]test = input()print(test)if numbers in test: print(""numbers"") TypeError: 'in <string>' requires string as left operand, not list",Check if any of a list of strings is in another string
Do python datetime objects need to be deep-copied?," So I noticed the other week through running an experiment, that, despite being a high-level language, while you can make copies of variables by just assigning them like this: ...if you treat dictionaries or possibly lists the same way, it comes unstuck! I created a bug in my code the other week thinking that dictionaries worked the same way.. Found out that to make a proper, deep copy you need to go: Anyway, I'm busy with datetime objects and I'm manipulating them around as if they were integers, now starting to get a bit nervous as to whether this is okay. It all seems a bit arbitrary where it works and where it doesn't, do I have to run an experiment every time just to check its behaviour? Can guess that strings probably work like integers but not sure where the behaviour changes.Can see someone has asked about this for PHP but for Python I'm inclined to think that any assignment of a datetime object would be a proper, deep copy and never mess accidentally with the original variable. Does anyone know for sure?  <code>  a = 5 b = aprint(b) # 5b = 3print(b) # 3print(a) # 5 b = dict(a)",Do datetime objects need to be deep-copied?
Why does map over an iterable return an iterator and not an iterable," Why does map when called with an object which can be iterated over multiple times not return an object which can also be iterated multiple times? I think the latter is much more reasonable.My use case is that I have a lot of data, such that it can only be iterated over. map is (in theory) perfect for operations on data, since it is lazy. However in the following example I would expect that the length is both times the same. How can I map over an iterable structure and get an iterable structure back?Edit:This is an example of how it imho should work, demonstrating the lazy evaluation: <code>  iterable = [1,2,3,4] # this can be iterated repeatedlym = map(lambda x:x**2, iterable) # this again should be iterable repeatedlyprint(len(list(m))) # 4print(len(list(m))) # 0 def g(): print('g() called')data = [g, g]# map is lazy, so nothing is calledm = map(lambda g: g(), data)print('m: %s' % len(list(m))) # g() is called hereprint('m: %s' % len(list(m))) # this should work, but doesnt# this imap returns an iterableclass imap(object): def __init__(self, fnc, iterable): self.fnc = fnc self.iterable = iterable def __iter__(self): return map(self.fnc, self.iterable)# imap is lazy, so nothing is calledim = imap(lambda g: g(), data) print('im: %s' % len(list(im))) # g() is called hereprint('im: %s' % len(list(im))) # works as expected",Why does map over an iterable return a one-shot iterable?
how to check if a string in python contains lower case letters and numbers ONLY?," How can I check if a string contains only numbers and lower case letters?I only managed to check if it contains numbers and lower case letters and doesn't contain upper case letters but I don't know how to check that it doesn't contain any symbols as ^&*(% etc.. EDIT:so apparently I need to do this without using any loops and mainly using functions like .islower() , .isdigit() , isalnum() etc.. and I have no idea how I can check if a string contains lower case letters and numbers only , without using loops or something that will check every single char in the string . we only started to learn the basics in python so they told us we can't use ""for"" and all that even if I know what it does.. now I can check if an entire string is only digits or lower/upper case letters but I don't know how to check the two conditions mentioned above in the simplest way possible  <code>  if(any(i.islower() for i in password) and any(i.isdigit() for i in password) and not any(i.isupper() for i in password)):",how to check if a string contains only lower case letters and numbers?
Linear regression with tenserflow [Ptyhon]," I trying to understand linear regression... here is script that I tried to understand: Question is what this part represent: And why are there random float numbers?Also could you show me some math with formals represents cost, pred, optimizer variables? <code>  '''A linear regression learning algorithm example using TensorFlow library.Author: Aymeric DamienProject: https://github.com/aymericdamien/TensorFlow-Examples/'''from __future__ import print_functionimport tensorflow as tffrom numpy import *import numpyimport matplotlib.pyplot as pltrng = numpy.random# Parameterslearning_rate = 0.0001training_epochs = 1000display_step = 50# Training Datatrain_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167, 7.042,10.791,5.313,7.997,5.654,9.27,3.1])train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221, 2.827,3.465,1.65,2.904,2.42,2.94,1.3])train_X=numpy.asarray(train_X)train_Y=numpy.asarray(train_Y)n_samples = train_X.shape[0]# tf Graph InputX = tf.placeholder(""float"")Y = tf.placeholder(""float"")# Set model weightsW = tf.Variable(rng.randn(), name=""weight"")b = tf.Variable(rng.randn(), name=""bias"")# Construct a linear modelpred = tf.add(tf.multiply(X, W), b)# Mean squared errorcost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)# Gradient descentoptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)# Initializing the variablesinit = tf.global_variables_initializer()# Launch the graphwith tf.Session() as sess: sess.run(init) # Fit all training data for epoch in range(training_epochs): for (x, y) in zip(train_X, train_Y): sess.run(optimizer, feed_dict={X: x, Y: y}) # Display logs per epoch step if (epoch+1) % display_step == 0: c = sess.run(cost, feed_dict={X: train_X, Y:train_Y}) print(""Epoch:"", '%04d' % (epoch+1), ""cost="", ""{:.9f}"".format(c), \ ""W="", sess.run(W), ""b="", sess.run(b)) print(""Optimization Finished!"") training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y}) print(""Training cost="", training_cost, ""W="", sess.run(W), ""b="", sess.run(b), '\n') # Graphic display plt.plot(train_X, train_Y, 'ro', label='Original data') plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line') plt.legend() plt.show() # Set model weightsW = tf.Variable(rng.randn(), name=""weight"")b = tf.Variable(rng.randn(), name=""bias"")",Linear regression with tensorflow
Linear regression with tensorflow [Python]," I trying to understand linear regression... here is script that I tried to understand: Question is what this part represent: And why are there random float numbers?Also could you show me some math with formals represents cost, pred, optimizer variables? <code>  '''A linear regression learning algorithm example using TensorFlow library.Author: Aymeric DamienProject: https://github.com/aymericdamien/TensorFlow-Examples/'''from __future__ import print_functionimport tensorflow as tffrom numpy import *import numpyimport matplotlib.pyplot as pltrng = numpy.random# Parameterslearning_rate = 0.0001training_epochs = 1000display_step = 50# Training Datatrain_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167, 7.042,10.791,5.313,7.997,5.654,9.27,3.1])train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221, 2.827,3.465,1.65,2.904,2.42,2.94,1.3])train_X=numpy.asarray(train_X)train_Y=numpy.asarray(train_Y)n_samples = train_X.shape[0]# tf Graph InputX = tf.placeholder(""float"")Y = tf.placeholder(""float"")# Set model weightsW = tf.Variable(rng.randn(), name=""weight"")b = tf.Variable(rng.randn(), name=""bias"")# Construct a linear modelpred = tf.add(tf.multiply(X, W), b)# Mean squared errorcost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)# Gradient descentoptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)# Initializing the variablesinit = tf.global_variables_initializer()# Launch the graphwith tf.Session() as sess: sess.run(init) # Fit all training data for epoch in range(training_epochs): for (x, y) in zip(train_X, train_Y): sess.run(optimizer, feed_dict={X: x, Y: y}) # Display logs per epoch step if (epoch+1) % display_step == 0: c = sess.run(cost, feed_dict={X: train_X, Y:train_Y}) print(""Epoch:"", '%04d' % (epoch+1), ""cost="", ""{:.9f}"".format(c), \ ""W="", sess.run(W), ""b="", sess.run(b)) print(""Optimization Finished!"") training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y}) print(""Training cost="", training_cost, ""W="", sess.run(W), ""b="", sess.run(b), '\n') # Graphic display plt.plot(train_X, train_Y, 'ro', label='Original data') plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line') plt.legend() plt.show() # Set model weightsW = tf.Variable(rng.randn(), name=""weight"")b = tf.Variable(rng.randn(), name=""bias"")",Linear regression with tensorflow
Creating/ uploading new file In google cloud storage bucket using Python," How to create new empty files in Google Cloud Storage using Python with client libraries available? Or how to upload a new file to a selected bucket using blob function ""upload_from_filename()"" ? To initialize the blob object we should have file already in the cloud bucket, but I want to create a new file name, and copy the content from the file stored locally. I want to do this using python.Thanks in advance <code> ",Creating/Uploading new file at Google Cloud Storage bucket using Python
Running scrapy with PyCharm - Debug works but Run not work," I met a very strange issue, running Scrapy with PyCharm: With the exact same configuration, Debug works, but Run with PyCharm does NOT work.Windows 10 PyCharm 2016.3.3 Scrapy 1.3.3 Python 3.6.0Configuration:PyCharm Edit Configuration PageWhen I attempt to debug scrapy, it works perfectly.When I attempt to run scrapy, I get below error: No need to attach any code because even running the command:python3.exe C:\Users\baib2\AppData\Local\Programs\Python\Python36\Lib\site-packages\scrapy\cmdline.py will produce the same error.I've checked my sys.path, comparing with run, debug has 1 more path:'C:\Program Files (x86)\JetBrains\PyCharm 2016.3.3\helpers\pydev'And I really don't think this shall make any difference.Hope someone can take a look, thanks! <code>  C:\Users\baib2\AppData\Local\Programs\Python\Python36\python.exe C:/Users/baib2/AppData/Local/Programs/Python/Python36/Lib/site-packages/scrapy/cmdline.py crawl scenelist_spiderTraceback (most recent call last): File ""C:/Users/baib2/AppData/Local/Programs/Python/Python36/Lib/site-packages/scrapy/cmdline.py"", line 8, in <module import scrapy File ""C:\Users\baib2\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\__init__.py"", line 27, in <module from . import _monkeypatches File ""C:\Users\baib2\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\_monkeypatches.py"", line 20, in <module import twisted.persisted.styles # NOQA File ""C:\Users\baib2\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\persisted\styles.py"", line 21, in <module from twisted.python.compat import _PY3, _PYPY File ""C:\Users\baib2\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\python\__init__.py"", line 11, in <module from .compat import unicode File ""C:\Users\baib2\AppData\Local\Programs\Python\Python36\lib\site-packages\twisted\python\compat.py"", line 612, in <module from http import cookiejar as cookielib File ""C:\Users\baib2\AppData\Local\Programs\Python\Python36\Lib\site-packages\scrapy\http\__init__.py"", line 8, in <module from scrapy.http.headers import Headers File ""C:\Users\baib2\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\http\__init__.py"", line 10, in <module from scrapy.http.request import Request File ""C:\Users\baib2\AppData\Local\Programs\Python\Python36\lib\site-packages\scrapy\http\request\__init__.py"", line 8, in <module from w3lib.url import safe_url_string File ""C:\Users\baib2\AppData\Local\Programs\Python\Python36\lib\site-packages\w3lib\url.py"", line 17, in <module from six.moves.urllib.request import pathname2url, url2pathname File ""C:\Users\baib2\AppData\Local\Programs\Python\Python36\lib\site-packages\six.py"", line 92, in __get__ result = self._resolve() File ""C:\Users\baib2\AppData\Local\Programs\Python\Python36\lib\site-packages\six.py"", line 160, in _resolve module = _import_module(self.mod) File ""C:\Users\baib2\AppData\Local\Programs\Python\Python36\lib\site-packages\six.py"", line 82, in _import_module __import__(name) File ""C:\Users\baib2\AppData\Local\Programs\Python\Python36\lib\urllib\request.py"", line 88, in <module import http.clientModuleNotFoundError: No module named 'http.client'Process finished with exit code 1",Running scrapy with PyCharm - Debug works but Run does not work
QtDesigner how to change background," This is an repost.I am kind of new in QtDesigner and wanted to do something about the uglyness of just some buttons.All i have done yesterday was looking on the internet on how to change the background color of a screen in Qt Designer. How to change the color of a button and how to make it flow into another color on the sides. Guess what.I found nothing.I hope that some of you die-hard coders know what i need..EDIT 1: What i mean with a color flowing into another:Here pink flows into purple, do you get what I mean? <code> ",Qt Designer how to change background
max of window in numpy array," I want to create an array which holds all the max()es of a window moving through a given numpy array. I'm sorry if this sounds confusing. I'll give an example. Input: My output with a window width of 5 shall be this: Each number shall be the max of a subarray of width 5 of the input array: I did not find an out-of-the-box function within numpy which would do this (but I would not be surprised if there was one; I'm not always thinking in the terms the numpy developers thought). I considered creating a shifted 2D-version of my input: Then I could apply np.max(input, 0) on this and would get my results. But this does not seem efficient in my case because both my array and my window width can be large (>1000000 entries and >100000 window width). The data would be blown up more or less by a factor of the window width.I also considered using np.convolve() in some fashion but couldn't figure out a way to achieve my goal with it.Any ideas how to do this efficiently? <code>  [ 6,4,8,7,1,4,3,5,7,2,4,6,2,1,3,5,6,3,4,7,1,9,4,3,2 ] [ 8,8,8,7,7,7,7,7,7,6,6,6,6,6,6,7,7,9,9,9,9 ] [ 6,4,8,7,1,4,3,5,7,2,4,6,2,1,3,5,6,3,4,7,1,9,4,3,2 ] \ / \ / \ / \ / \ / \ / \ / \ /[ 8,8,8,7,7,7,7,7,7,6,6,6,6,6,6,7,7,9,9,9,9 ] [ [ 6,4,8,7,1,4,3,5,7,8,4,6,2,1,3,5,6,3,4,7,1 ] [ 4,8,7,1,4,3,5,7,8,4,6,2,1,3,5,6,3,4,7,1,9 ] [ 8,7,1,4,3,5,7,8,4,6,2,1,3,5,6,3,4,7,1,9,4 ] [ 7,1,4,3,5,7,8,4,6,2,1,3,5,6,3,4,7,1,9,4,3 ] [ 1,4,3,5,7,8,4,6,2,1,3,5,6,3,4,7,1,9,4,3,2 ] ]",Max in a sliding window in NumPy array
where does Transformers come from," This questionAnd this oneThis one tooAll show the use of this import When I run it Googling only led me to the links I've already posted.Where does Transformers come from? <code>  from sklearn.pipeline import Pipeline, FeatureUnionfrom Transformers import TextTransformer ModuleNotFoundError Traceback (most recent call last)<ipython-input-6-1f277e1659bb> in <module>() 1 from sklearn.pipeline import Pipeline, FeatureUnion----> 2 from Transformers import TextTransformerModuleNotFoundError: No module named 'Transformers import sklearnimport sysprint(sklearn.__version__)print(sys.version)0.18.13.6.0 |Anaconda 4.3.1 (x86_64)| (default, Dec 23 2016, 13:19:00) [GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]",Where does class Transformers come from?
Pandas One hot encoding: Bundelling together less frequent categories," I'm doing one hot encoding over a categorical column which has some 18 different kind of values. I want to create new columns for only those values, which appear more than some threshold (let's say 1%), and create another column named other values which has 1 if value is other than those frequent values.I'm using Pandas with Sci-kit learn. I've explored pandas get_dummies and sci-kit learn's one hot encoder, but can't figure out how to bundle together less frequent values into one column. <code> ",Pandas One hot encoding: Bundling together less frequent categories
Pyspark: cast array to string," I have pyspark dataframe with a column named Filters: ""array>""I want to save my dataframe in csv file, for that i need to cast the array to string type.I tried to cast it: DF.Filters.tostring() and DF.Filters.cast(StringType()), but both solutions generate error message for each row in the columns Filters: org.apache.spark.sql.catalyst.expressions.UnsafeArrayData@56234c19The code is as follows Sample JSON data: Thanks !! <code>  from pyspark.sql.types import StringTypeDF.printSchema()|-- ClientNum: string (nullable = true)|-- Filters: array (nullable = true) |-- element: struct (containsNull = true) |-- Op: string (nullable = true) |-- Type: string (nullable = true) |-- Val: string (nullable = true)DF_cast = DF.select ('ClientNum',DF.Filters.cast(StringType())) DF_cast.printSchema()|-- ClientNum: string (nullable = true)|-- Filters: string (nullable = true)DF_cast.show()| ClientNum | Filters | 32103 | org.apache.spark.sql.catalyst.expressions.UnsafeArrayData@d9e517ce| 218056 | org.apache.spark.sql.catalyst.expressions.UnsafeArrayData@3c744494 {""ClientNum"":""abc123"",""Filters"":[{""Op"":""foo"",""Type"":""bar"",""Val"":""baz""}]}",Pyspark: cast array with nested struct to string
Python output above the last printed line," Is there a way in python to print something in the command line above the last line printed? Or, similarly to what I want to achieve, remain the last line intact, that is, not overwrite it.The goal of this is to let the last line in the command line a status/precentage bar.Output example: Next refresh: Next refresh: <code>  File 1 processed(0.1% Completed) File 1 processedFile 2 processed(0.2% Completed) File 1 processedFile 2 processedFile 3 processed(0.3% Completed)",How to output above the last printed line?
How to remove seconds from datatime?," I have the following date and I tried the following code, I get the following error: <code>  df['start_date_time'] = [""2016-05-19 08:25:00"",""2016-05-19 16:00:00"",""2016-05-20 07:45:00"",""2016-05-24 12:50:00"",""2016-05-25 23:00:00"",""2016-05-26 19:45:00""]df['start_date_time'] = pd.to_datetime([df['start_date_time']).replace(second = 0) TypeError: replace() got an unexpected keyword argument 'second'",How to remove seconds from datetime?
python3 type hint as logical-and of multiple types," I know that Union allows you to specify the logical-or of multiple types. I'm wondering if there is a way to do something analogous for the logical-and, something like: I know that one option is to just explicitly define a new type that inherits from both Bar and Baz, like this: In my context that option is not ideal. <code>  def foo(x: And[Bar, Baz]): class BarAndBaz(Bar, Baz): ...def foo(x: BarAndBaz):",Type hint as logical-and of multiple types
What is the purpose of __table_args__ when using sqlalchemy?," I have no experience with sqlalchemy, and I have the following code: Although I have checked this, I cannot understand what is the purpose of __table_args__, so I have no clue what this line is doing: Could somebody please explain me what is the purpose of __table_args__, and what the previous piece of code is doing. <code>  class ForecastBedSetting(Base): __tablename__ = 'forecast_bed_settings' __table_args__ = ( Index('idx_forecast_bed_settings_forecast_id_property_id_beds', 'forecast_id', 'property_id', 'beds'), ) forecast_id = Column(ForeignKey(u'forecasts.id'), nullable=False) property_id = Column(ForeignKey(u'properties.id'), nullable=False, index=True) # more definition of columns __table_args__ = ( Index('idx_forecast_bed_settings_forecast_id_property_id_beds', 'forecast_id', 'property_id', 'beds'),)",What is the purpose of __table_args__ in sqlalchemy?
QGraphicsScene / View require being shown for correct transformations?," Primary issue: the QGraphicsView.mapToScene method returns different answers depending on whether or not the GUI is shown. Why, and can I get around it?The context is I'm trying to write unit tests but I don't want to actually show the tools for the tests.The small example below illustrates the behavior. I use a sub-classed view that prints mouse click event positions in scene coordinates with the origin at the lower left (it has a -1 scale vertically) by calling mapToScene. However, mapToScene does not return what I am expecting before the dialog is shown. If I run the main section at the bottom, I get the following output: Before show(), there is a consistent offset of 34 pixels in x and 105 in y (and in y the offset moves in reverse as if the scale is not being applied). Those offset seem rather random, I have no idea where they are coming from.Here is the example code: This example is in PyQt5 on Windows, but PyQt4 on Linux does the same thing. <code>  Size is (150, 200)Putting in (50, 125) - This point should return (50.0, 75.0)Before show(): PyQt5.QtCore.QPointF(84.0, -20.0)After show() : PyQt5.QtCore.QPointF(50.0, 75.0) import numpy as npfrom PyQt5.QtCore import pyqtSignal, pyqtSlot, QPointF, QPointfrom PyQt5.QtWidgets import (QDialog, QGraphicsView, QGraphicsScene, QVBoxLayout, QPushButton, QApplication, QSizePolicy)from PyQt5.QtGui import QPixmap, QImageclass MyView(QGraphicsView): """"""View subclass that emits mouse events in the scene coordinates."""""" mousedown = pyqtSignal(QPointF) def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.setSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed) # This is the key thing I need self.scale(1, -1) def mousePressEvent(self, event): return self.mousedown.emit(self.mapToScene(event.pos()))class SimplePicker(QDialog): def __init__(self, data, parent=None): super().__init__(parent=parent) # Get a grayscale image bdata = ((data - data.min()) / (data.max() - data.min()) * 255).astype(np.uint8) wid, hgt = bdata.shape img = QImage(bdata.T.copy(), wid, hgt, wid, QImage.Format_Indexed8) # Construct a scene with pixmap self.scene = QGraphicsScene(0, 0, wid, hgt, self) self.scene.setSceneRect(0, 0, wid, hgt) self.px = self.scene.addPixmap(QPixmap.fromImage(img)) # Construct the view and connect mouse clicks self.view = MyView(self.scene, self) self.view.mousedown.connect(self.mouse_click) # End button self.doneb = QPushButton('Done', self) self.doneb.clicked.connect(self.accept) # Layout layout = QVBoxLayout(self) layout.addWidget(self.view) layout.addWidget(self.doneb) @pyqtSlot(QPointF) def mouse_click(self, xy): print((xy.x(), xy.y()))if __name__ == ""__main__"": # Fake data x, y = np.mgrid[0:4*np.pi:150j, 0:4*np.pi:200j] z = np.sin(x) * np.sin(y) qapp = QApplication.instance() if qapp is None: qapp = QApplication(['python']) pick = SimplePicker(z) print(""Size is (150, 200)"") print(""Putting in (50, 125) - This point should return (50.0, 75.0)"") p0 = QPoint(50, 125) print(""Before show():"", pick.view.mapToScene(p0)) pick.show() print(""After show() :"", pick.view.mapToScene(p0)) qapp.exec_()",mapToScene requires the view being shown for correct transformations?
How to drop the Year-Month-Date from a datetime series? [python]," I have a datetime series, and I want to only keep the ""Hour-Miniute-Second"" strings in the series, and drop the Year-Month-Date strings.So what should I do?original series: target: Preiviously the original series has been already translated from a unix timestamp series by using pandas.to_datetime(). But I'm not able to use this method to reach my target:(Any suggestions is appreciated! <code>  0 2000-12-31 22:12:401 2000-12-31 22:35:092 2000-12-31 22:32:483 2000-12-31 22:04:354 2001-01-06 23:38:115 2000-12-31 22:37:48 0 22:12:401 22:35:092 22:32:483 22:04:354 23:38:115 22:37:48",How to drop the Year-Month-Date from a datetime series in python?
How to drop the Year-Month-Date from a datetime series?," I have a datetime series, and I want to only keep the ""Hour-Miniute-Second"" strings in the series, and drop the Year-Month-Date strings.So what should I do?original series: target: Preiviously the original series has been already translated from a unix timestamp series by using pandas.to_datetime(). But I'm not able to use this method to reach my target:(Any suggestions is appreciated! <code>  0 2000-12-31 22:12:401 2000-12-31 22:35:092 2000-12-31 22:32:483 2000-12-31 22:04:354 2001-01-06 23:38:115 2000-12-31 22:37:48 0 22:12:401 22:35:092 22:32:483 22:04:354 23:38:115 22:37:48",How to drop the Year-Month-Date from a datetime series in python?
Merging two dataframes based on a date between to other dates without a common column," I have two dataframes that I need to merge based on whether or not a date value fits in between two other dates. Basically, I need to perform an outer join where B.event_date is between A.start_date and A.end_date. It seems that merge and join always assume a common column which in this case, I do not have.  <code>  A B start_date end_date event_date price0 2017-03-27 2017-04-20 0 2017-01-20 1001 2017-01-10 2017-02-01 1 2017-01-27 200Result start_date end_date event_date price0 2017-03-27 2017-04-20 1 2017-01-10 2017-02-01 2017-01-20 1002 2017-01-10 2017-02-01 2017-01-27 200",Merging two dataframes based on a date between two other dates without a common column
Merging Overlapping Intervals python," Currently, I have intervals of: in an ascending order by the lower bound. My task is to merge overlapping intervals so that the outcome comes out to be: My first attempt involved deleting intervals that are completely within previous intervals, like [-21, -16] which falls within [-25, -14]. But deleting objects within a list kept interfering with the loop condition. My second attempt at deleting unnecessary intervals was: but this doesn't delete all the unnecessary intervals for some reason.What should I do? <code>  temp_tuple = [[-25, -14], [-21, -16], [-20, -15], [-10, -7], [-8, -5], [-6, -3], [2, 4], [2, 3], [3, 6], [12, 15], [13, 18], [14, 17], [22, 27], [25, 30], [26, 29]] [-25, -14][-10, -3][2, 6][12, 18][22, 30] i = 0j = 1while i < len(temp_tuples): while j < len(temp_tuples): if temp_tuples[i][1] > temp_tuples[j][1]: del temp_tuples[j] j += 1 i += 1",Merging Overlapping Intervals
Python PIL Drawing a semi-transparent square overlay on image," This is giving me a giant black square. I want it to be a semi transparent black square with a cat. Any Ideas? <code>  from PIL import Imagefrom PIL import ImageDraw from io import BytesIOfrom urllib.request import urlopenurl = ""https://i.ytimg.com/vi/W4qijIdAPZA/maxresdefault.jpg""file = BytesIO(urlopen(url).read())img = Image.open(file)img = img.convert(""RGBA"")draw = ImageDraw.Draw(img, ""RGBA"")draw.rectangle(((0, 00), (img.size[0], img.size[1])), fill=(0,0,0,127))img.save('dark-cat.jpg')",PIL Drawing a semi-transparent square overlay on image
Pascals triangle with recursion," Can someone tell me if my current code is even possible? I have to create Pascal's Triangle with an input without using any loops. I am bound to recursion.I have spent 3 days on this, and this is the best output that I can come up with. <code>  def pascal(curlvl,newlvl,tri): if curlvl == newlvl: return """" else: tri.append(tri[curlvl]) print(tri) return pascal(curlvl+1,newlvl,tri)def triLvl(): msg = ""Please enter the number of levels to generate:"" triheight = int(input(msg)) if triheight < 1: print(""\n Sorry, the number MUST be positive\n Please try again."") return triLvl() else: return triheightdef main(): triangle = [1] curlvl = 0 print(""Welcome to the Pascal's triangle generator."") usrTri = triLvl() print(triangle) pascal(curlvl,usrTri,triangle)main()",Pascal's Triangle via Recursion
seaborn heatmap queries regarding data order & cbar labels matplot," So I have a heatmap created using seaborn which producesThere are two things which I want to do however, which I can't for the life of me work out how to do, despite consulting seaborn's helpfile (http://seaborn.pydata.org/generated/seaborn.heatmap.html)The thing I want to do is order the flavours differently. Despite the order being inputted in the textfile as orange, toffee, choc, malt, raisin, coffee, it doesn't generate this way when plotting. I tried to edit the yticklabs but that just edits the labels as opposed to moving the data with it.Thanks for any helpPS data looks like this: <code>  revels = rd.pivot(""Flavour"", ""Packet number"", ""Contents"")ax = sns.heatmap(revels, annot=True, fmt=""d"", linewidths=0.4, cmap=""YlOrRd"")plt.show() Packet number,Flavour,Contents1,orange,42,orange,33,orange,2...1,toffee,42,toffee,33,toffee,3...etc.",Data order in seaborn heatmap from pivot
Seaborn heatmap querie regarding data order," So I have a heatmap created using seaborn which producesThere are two things which I want to do however, which I can't for the life of me work out how to do, despite consulting seaborn's helpfile (http://seaborn.pydata.org/generated/seaborn.heatmap.html)The thing I want to do is order the flavours differently. Despite the order being inputted in the textfile as orange, toffee, choc, malt, raisin, coffee, it doesn't generate this way when plotting. I tried to edit the yticklabs but that just edits the labels as opposed to moving the data with it.Thanks for any helpPS data looks like this: <code>  revels = rd.pivot(""Flavour"", ""Packet number"", ""Contents"")ax = sns.heatmap(revels, annot=True, fmt=""d"", linewidths=0.4, cmap=""YlOrRd"")plt.show() Packet number,Flavour,Contents1,orange,42,orange,33,orange,2...1,toffee,42,toffee,33,toffee,3...etc.",Data order in seaborn heatmap from pivot
python json.loads changes the order of the object," I've got a file that contains a JSON object. It's been loaded the following way: At this point input_data contains just a string, and now I proceed to parse it into JSON: data_content has the JSON representation of the string which is what I need, but for some reason not clear to me after json.loads it is altering the order original order of the keys, so for instance, if my file contained something like: After json.loads the order is altered to let's say something like: Why is this happening? Is there a way to preserve the order? I'm using Python 2.7. <code>  with open('data.json', 'r') as input_file: input_data = input_file.read() data_content = json.loads(input_data.decode('utf-8')) { ""z_id"": 312312, ""fname"": ""test"", ""program"": ""none"", ""org"": null} { ""fname"": ""test"", ""program"": None, ""z_id"": 312312, ""org"": ""none""}",Python json.loads changes the order of the object
"getting multiple indices from a tensor at once, in tensorflow"," Let's consider a numpy matrix, o:If we want to use the following function by using numpy: I can get multiple indices from a numpy array at once.I have tried to do the same with tensorflow, but it doesn't work as what I have done. When o is a tensorflow tensor; I get the following error: What can I do? <code>  o[np.arange(x), column_array] o[tf.range(0, x, 1), column_array] TypeError: can only concatenate list (not ""int"") to list","getting values in multiple indices from a tensor at once, in tensorflow"
How do I initialize a Counter from a list of key-value pairs?," If I have a sequence of (key, value) pairs, I can quickly initialize a dictionary like this: I would like to do the same with a Counter dictionary; but how? Both the constructor and the update() method treat the ordered pairs as keys, not key-value pairs: The best I could manage was to use a temporary dictionary, which is ugly and needlessly circuitous: Is there a proper way to directly initialize a Counter from a list of (key, count) pairs? My use case involves reading lots of saved counts from files (with unique keys). <code>  >>> data = [ ('a', 1), ('b', 2) ]>>> dict(data) {'a': 1, 'b': 2} >>> from collections import Counter>>> Counter(data)Counter({('a', 1): 1, ('b', 2): 1}) >>> Counter(dict(data))Counter({'b': 2, 'a': 1})",How do I initialize a Counter from a list of key/initial counts pairs?
Python 3.7 math.remainder vs %(remainder operator)," From Whats New In Python 3.7we can see that there is new math.remainder. It saysReturn the IEEE 754-style remainder of x with respect to y. For finite x and finite nonzero y, this is the difference x - n*y, where n is the closest integer to the exact value of the quotient x / y. If x / y is exactly halfway between two consecutive integers, the nearest even integer is used for n. The remainder r = remainder(x, y) thus always satisfies abs(r) <= 0.5 * abs(y).Special cases follow IEEE 754: in particular, remainder(x, math.inf) is x for any finite x, and remainder(x, 0) and remainder(math.inf, x) raise ValueError for any non-NaN x. If the result of the remainder operation is zero, that zero will have the same sign as x.On platforms using IEEE 754 binary floating-point, the result of this operation is always exactly representable: no rounding error is introduced.But we also remember that there is % symbol which isremainder of x / yWe also see that there is a note to operator:Not for complex numbers. Instead convert to floats using abs() if appropriate.I haven't tried to run Python 3.7 if it's even possible.But i tried So difference would be, instead of nan and ZeroDivisionError we would get ValueError as it says in docs.So the question is what is the difference between % and math.remainder? Would math.remainder also work with complex numbers(% lacks from it)? What is the main advantage?Here is the source of math.remainder from official CPython github repo. <code>  Python 3.6.1 (v3.6.1:69c0db5050, Mar 21 2017, 01:21:04)[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwinType ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import math>>> 100 % math.inf100.0>>> math.inf % 100nan>>> 100 % 0Traceback (most recent call last): File ""<stdin>"", line 1, in <module>ZeroDivisionError: integer division or modulo by zero",Difference between Python 3.7 math.remainder and %(modulo operator)
Difference between Python 3.7 math.remainder and %(remainder operator)," From Whats New In Python 3.7we can see that there is new math.remainder. It saysReturn the IEEE 754-style remainder of x with respect to y. For finite x and finite nonzero y, this is the difference x - n*y, where n is the closest integer to the exact value of the quotient x / y. If x / y is exactly halfway between two consecutive integers, the nearest even integer is used for n. The remainder r = remainder(x, y) thus always satisfies abs(r) <= 0.5 * abs(y).Special cases follow IEEE 754: in particular, remainder(x, math.inf) is x for any finite x, and remainder(x, 0) and remainder(math.inf, x) raise ValueError for any non-NaN x. If the result of the remainder operation is zero, that zero will have the same sign as x.On platforms using IEEE 754 binary floating-point, the result of this operation is always exactly representable: no rounding error is introduced.But we also remember that there is % symbol which isremainder of x / yWe also see that there is a note to operator:Not for complex numbers. Instead convert to floats using abs() if appropriate.I haven't tried to run Python 3.7 if it's even possible.But i tried So difference would be, instead of nan and ZeroDivisionError we would get ValueError as it says in docs.So the question is what is the difference between % and math.remainder? Would math.remainder also work with complex numbers(% lacks from it)? What is the main advantage?Here is the source of math.remainder from official CPython github repo. <code>  Python 3.6.1 (v3.6.1:69c0db5050, Mar 21 2017, 01:21:04)[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwinType ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import math>>> 100 % math.inf100.0>>> math.inf % 100nan>>> 100 % 0Traceback (most recent call last): File ""<stdin>"", line 1, in <module>ZeroDivisionError: integer division or modulo by zero",Difference between Python 3.7 math.remainder and %(modulo operator)
How to determine a class's metaclass in Python," I have a class object, cls. I want to know its metaclass. How do I do this?(If I wanted to know its parent classes, I would do cls.__mro__. Is there something like this to get the metaclass?) <code> ",How to determine the metaclass of a class?
How to bind the backspace key to delete more than one character?," I am wanting to create bindings that let me press tab to insert a predefined number of spaces, and then press backspace to delete that many spaces, based on a variable. How can I delete a pre-determined number of spaces when the user presses the backspace key? I don't know how to delete multiple characters, and when I've attempted to solve this, the binding deletes the wrong number of characters. <code> ",How to bind the backspace key in tkinter to delete more than one character?
urllib.error.HTTPError: HTTP Error 500: INTERNAL SERVER ERROR python," I'm trying to make a request to the following view. However, I get the error TypeError: echoplaca() got an unexpected keyword argument 'placa'. <code>  @app.route(r'/enviaplaca/<placa>')def echoplaca(): return ""Numero de placa: {}"".format(placa)",Flask view raises TypeError got unexpected keyword argument
Load Unique NER Model Stanford CoreNLP," I have created my own NER model with Stanford's ""Stanford-NER"" software and by following these directions. I am aware that CoreNLP loads three NER models out of the box in the following order:edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gzedu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gzedu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gzI now want to include my NER model in the list above and have the text tagged by my NER model first.I have found two previous StackOverflow questions regarding this topic and they are 'Stanford OpenIE using customized NER model' and 'Why does Stanford CoreNLP NER-annotator load 3 models by default?'Both of these posts have good answers. The general message of the answers is that you have to edit code within a file. Stanford OpenIE using customized NER modelFrom this post it says to edit corenlpserver.sh but I cannot find this file within the Stanford CoreNLP downloaded software. Can anyone point me to this file's location? does Stanford CoreNLP NER-annotator load 3 models by default?This post says that I can use the argument of -ner.model to specifically call which NER models to load. I added this argument to the initial server command (java -mx4g -cp ""*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000 -ner.model *modlefilepathhere*). This did not work as the server still loaded all three models. It also states that you have to change some java code though it does not specifically call out where to make the change. Do I need to modify or add this code props.put(""ner.model"", ""model_path1,model_path2""); to a specific class file in the CoreNLP software? QUESTION: From my research it seems that I need to add/modify some code to call my unique NER model. These 'edits' are outlined above and this information has been pulled from other StackOverflow questions. What files specifically do I need to edit? Where exactly are these files located (i.e. edu/Stanford/nlp/...etc)?EDIT: My system is running on a local server and I'm using the API pycorenlp in order to open a pipeline to my local server and to make requests against it. the two critical lines of python/pycorenlp code are:nlp = StanfordCoreNLP('http://localhost:9000')output = nlp.annotate(evalList[line], properties={'annotators': 'ner, openie','outputFormat': 'json', 'openie.triple.strict':'True', 'openie.max_entailments_per_clause':'1'})I do NOT think this will affect my ability to call my unique NER model but I wanted to present all the situational data I can in order to obtain the best possible answer. <code> ",Load Custom NER Model Stanford CoreNLP
Scrap the absolute URL instead of a relative path in python, I'm trying to get all the href's from a HTML code and store it in a list for future processing such as this: Example URL: www.example-page-xl.com I'm using the following code to list the href's: However I would like to store the URL as:www.example-page-xl.com/helloworld/index.php and not just the relative path which is /helloworld/index.phpAppending/joining the URL with the relative path isn't required since the dynamic links may vary when I join the URL and the relative path.In a nutshell I would like to scrape the absolute URL and not relative paths alone (and without joining) <code> ,Scrape the absolute URL instead of a relative path in python
how to take two minimum values from the numpy where," I would like to take the two smallest values from an array x. But when I use np.where: I get this error: ValueError: need more than 1 value to unpackHow can I fix this error? And do I need to arange numbers in ascending order in array? <code>  A,B = np.where(x == x.min())[0:1]",How to get the two smallest values from a numpy array
how to take two smallest values from the numpy where," I would like to take the two smallest values from an array x. But when I use np.where: I get this error: ValueError: need more than 1 value to unpackHow can I fix this error? And do I need to arange numbers in ascending order in array? <code>  A,B = np.where(x == x.min())[0:1]",How to get the two smallest values from a numpy array
how to take two smallest values from variable in the numpy where," I would like to take the two smallest values from an array x. But when I use np.where: I get this error: ValueError: need more than 1 value to unpackHow can I fix this error? And do I need to arange numbers in ascending order in array? <code>  A,B = np.where(x == x.min())[0:1]",How to get the two smallest values from a numpy array
Python: Unable to find element with selenium in dynamic page," My purpose it to download a zip file from https://www.shareinvestor.com/prices/price_download_zip_file.zip?type=history_all&market=bursaIt is a link in this webpage https://www.shareinvestor.com/prices/price_download.html#/?type=price_download_all_stocks_bursa. Then save it into this directory ""/home/vinvin/shKLSE/ (I am using pythonaywhere). Then unzip it and the csv file extract in the directory.The code run until the end with no error but it does not downloaded.The zip file is automatically downloaded when click on https://www.shareinvestor.com/prices/price_download_zip_file.zip?type=history_all&market=bursa manually.My code with a working username and password is used. The real username and password is used so that it is easier to understand the problem. HTML snippet: Note that &amp is shown when I copy the snippet. It was hidden from view source, so I guess it is written in JavaScript.Observation I foundThe directory home/vinvin/shKLSE do not created even I run the code with no errorI try to download a much smaller zip file which can be completed in a second but still do not download after a wait of 30s. dl = browser.find_element_by_xpath(""//*[@href='/prices/price_download_zip_file.zip?type=history_daily&date=20170519&market=bursa']"").click() <code>  #!/usr/bin/python print ""hello from python 2"" import urllib2 from selenium import webdriver from selenium.webdriver.common.keys import Keys import time from pyvirtualdisplay import Display import requests, zipfile, os display = Display(visible=0, size=(800, 600)) display.start() profile = webdriver.FirefoxProfile() profile.set_preference('browser.download.folderList', 2) profile.set_preference('browser.download.manager.showWhenStarting', False) profile.set_preference('browser.download.dir', ""/home/vinvin/shKLSE/"") profile.set_preference('browser.helperApps.neverAsk.saveToDisk', '/zip') for retry in range(5): try: browser = webdriver.Firefox(profile) print ""firefox"" break except: time.sleep(3) time.sleep(1) browser.get(""https://www.shareinvestor.com/my"") time.sleep(10) login_main = browser.find_element_by_xpath(""//*[@href='/user/login.html']"").click() print browser.current_url username = browser.find_element_by_id(""sic_login_header_username"") password = browser.find_element_by_id(""sic_login_header_password"") print ""find id done"" username.send_keys(""bkcollection"") password.send_keys(""123456"") print ""log in done"" login_attempt = browser.find_element_by_xpath(""//*[@type='submit']"") login_attempt.submit() browser.get(""https://www.shareinvestor.com/prices/price_download.html#/?type=price_download_all_stocks_bursa"") print browser.current_url time.sleep(20) dl = browser.find_element_by_xpath(""//*[@href='/prices/price_download_zip_file.zip?type=history_all&market=bursa']"").click() time.sleep(30) browser.close() browser.quit() display.stop() zip_ref = zipfile.ZipFile(/home/vinvin/sh/KLSE, 'r') zip_ref.extractall(/home/vinvin/sh/KLSE) zip_ref.close() os.remove(zip_ref) <li><a href=""/prices/price_download_zip_file.zip?type=history_all&amp;market=bursa"">All Historical Data</a> <span>About 220 MB</span></li>",Python: Unable to download with selenium in webpage
Python: Unable to download with selenium in dynamic page," My purpose it to download a zip file from https://www.shareinvestor.com/prices/price_download_zip_file.zip?type=history_all&market=bursaIt is a link in this webpage https://www.shareinvestor.com/prices/price_download.html#/?type=price_download_all_stocks_bursa. Then save it into this directory ""/home/vinvin/shKLSE/ (I am using pythonaywhere). Then unzip it and the csv file extract in the directory.The code run until the end with no error but it does not downloaded.The zip file is automatically downloaded when click on https://www.shareinvestor.com/prices/price_download_zip_file.zip?type=history_all&market=bursa manually.My code with a working username and password is used. The real username and password is used so that it is easier to understand the problem. HTML snippet: Note that &amp is shown when I copy the snippet. It was hidden from view source, so I guess it is written in JavaScript.Observation I foundThe directory home/vinvin/shKLSE do not created even I run the code with no errorI try to download a much smaller zip file which can be completed in a second but still do not download after a wait of 30s. dl = browser.find_element_by_xpath(""//*[@href='/prices/price_download_zip_file.zip?type=history_daily&date=20170519&market=bursa']"").click() <code>  #!/usr/bin/python print ""hello from python 2"" import urllib2 from selenium import webdriver from selenium.webdriver.common.keys import Keys import time from pyvirtualdisplay import Display import requests, zipfile, os display = Display(visible=0, size=(800, 600)) display.start() profile = webdriver.FirefoxProfile() profile.set_preference('browser.download.folderList', 2) profile.set_preference('browser.download.manager.showWhenStarting', False) profile.set_preference('browser.download.dir', ""/home/vinvin/shKLSE/"") profile.set_preference('browser.helperApps.neverAsk.saveToDisk', '/zip') for retry in range(5): try: browser = webdriver.Firefox(profile) print ""firefox"" break except: time.sleep(3) time.sleep(1) browser.get(""https://www.shareinvestor.com/my"") time.sleep(10) login_main = browser.find_element_by_xpath(""//*[@href='/user/login.html']"").click() print browser.current_url username = browser.find_element_by_id(""sic_login_header_username"") password = browser.find_element_by_id(""sic_login_header_password"") print ""find id done"" username.send_keys(""bkcollection"") password.send_keys(""123456"") print ""log in done"" login_attempt = browser.find_element_by_xpath(""//*[@type='submit']"") login_attempt.submit() browser.get(""https://www.shareinvestor.com/prices/price_download.html#/?type=price_download_all_stocks_bursa"") print browser.current_url time.sleep(20) dl = browser.find_element_by_xpath(""//*[@href='/prices/price_download_zip_file.zip?type=history_all&market=bursa']"").click() time.sleep(30) browser.close() browser.quit() display.stop() zip_ref = zipfile.ZipFile(/home/vinvin/sh/KLSE, 'r') zip_ref.extractall(/home/vinvin/sh/KLSE) zip_ref.close() os.remove(zip_ref) <li><a href=""/prices/price_download_zip_file.zip?type=history_all&amp;market=bursa"">All Historical Data</a> <span>About 220 MB</span></li>",Python: Unable to download with selenium in webpage
Python3 - how isinstance works for List?," I'm trying to understand how Python's type annotations work (e.g. List and Dict - not list or dict). Specifically I'm interested in how isinstance(list(), List) works, so that I can create my own custom annotations.I see that List is defined as: I'm familiar with metaclass = xxx but I can't find any documentation on this extra = xxx. Is this a keyword or just an argument, and if so, where does it come from and does it do what I'm after? Is it even relevant for isinstance? <code>  class List(list, MutableSequence[T], extra=list): . . .",How does isinstance work for List?
OrderedDict in python 3 - how to get the keys in order?," In python 2 when using an OrderedDict I was able to get the keys in the order they were inserted by simply using the keys method that returned a list. In python 3 however: I get: I certainly can do list(rows)[:2] as advised for instance here - but is this guaranteed to get me the keys in order ? Is this the one right way ?UPDATE: the python 2 code would be better as: which of course still blows with the same error on python 3 <code>  rows = OrderedDict()rows[0]=[1,2,3]rows[1]=[1,2,3]image = [rows[k] for k in rows.keys()[:2]] Traceback (most recent call last): File ""<input>"", line 1, in <module>TypeError: 'odict_keys' object is not subscriptable image = [v for v in rows.values()[:2]]",OrderedDict in python 3 - how to get the keys in order?
OrderedDict - how to get the keys in order?," In python 2 when using an OrderedDict I was able to get the keys in the order they were inserted by simply using the keys method that returned a list. In python 3 however: I get: I certainly can do list(rows)[:2] as advised for instance here - but is this guaranteed to get me the keys in order ? Is this the one right way ?UPDATE: the python 2 code would be better as: which of course still blows with the same error on python 3 <code>  rows = OrderedDict()rows[0]=[1,2,3]rows[1]=[1,2,3]image = [rows[k] for k in rows.keys()[:2]] Traceback (most recent call last): File ""<input>"", line 1, in <module>TypeError: 'odict_keys' object is not subscriptable image = [v for v in rows.values()[:2]]",OrderedDict in python 3 - how to get the keys in order?
Scikit learn sklearn. NotFittedError: TfidfVectorizer - Vocabulary wasn't fitted," I am trying to build a sentiment analyzer using scikit-learn/pandas. Building and evaluating the model works, but attempting to classify new sample text does not. My code: The error: I'm not sure what the issue could be. In my classify method, I create a brand new vectorizer to process the text I want to classify, separate from the vectorizer used to create training and test data from the model.Thanks  <code>  import csvimport pandas as pdimport numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.naive_bayes import BernoulliNBfrom sklearn.metrics import classification_reportfrom sklearn.metrics import accuracy_scoreinfile = 'Sentiment_Analysis_Dataset.csv'data = ""SentimentText""labels = ""Sentiment""class Classifier(): def __init__(self): self.train_set, self.test_set = self.load_data() self.counts, self.test_counts = self.vectorize() self.classifier = self.train_model() def load_data(self): df = pd.read_csv(infile, header=0, error_bad_lines=False) train_set, test_set = train_test_split(df, test_size=.3) return train_set, test_set def train_model(self): classifier = BernoulliNB() targets = self.train_set[labels] classifier.fit(self.counts, targets) return classifier def vectorize(self): vectorizer = TfidfVectorizer(min_df=5, max_df = 0.8, sublinear_tf=True, ngram_range = (1,2), use_idf=True) counts = vectorizer.fit_transform(self.train_set[data]) test_counts = vectorizer.transform(self.test_set[data]) return counts, test_counts def evaluate(self): test_counts,test_set = self.test_counts, self.test_set predictions = self.classifier.predict(test_counts) print (classification_report(test_set[labels], predictions)) print (""The accuracy score is {:.2%}"".format(accuracy_score(test_set[labels], predictions))) def classify(self, input): input_text = input input_vectorizer = TfidfVectorizer(min_df=5, max_df = 0.8, sublinear_tf=True, ngram_range = (1,2), use_idf=True) input_counts = input_vectorizer.transform(input_text) predictions = self.classifier.predict(input_counts) print(predictions)myModel = Classifier()text = ['I like this I feel good about it', 'give me 5 dollars']myModel.classify(text)myModel.evaluate() Traceback (most recent call last): File ""sentiment.py"", line 74, in <module> myModel.classify(text) File ""sentiment.py"", line 66, in classify input_counts = input_vectorizer.transform(input_text) File ""/home/rachel/Sentiment/ENV/lib/python3.5/site-packages/sklearn/feature_extraction/text.py"", line 1380, in transform X = super(TfidfVectorizer, self).transform(raw_documents) File ""/home/rachel/Sentiment/ENV/lib/python3.5/site-packages/sklearn/feature_extraction/text.py"", line 890, in transform self._check_vocabulary() File ""/home/rachel/Sentiment/ENV/lib/python3.5/site-packages/sklearn/feature_extraction/text.py"", line 278, in _check_vocabulary check_is_fitted(self, 'vocabulary_', msg=msg), File ""/home/rachel/Sentiment/ENV/lib/python3.5/site-packages/sklearn/utils/validation.py"", line 690, in check_is_fitted raise _NotFittedError(msg % {'name': type(estimator).__name__})sklearn.exceptions.NotFittedError: TfidfVectorizer - Vocabulary wasn't fitted.",NotFittedError: TfidfVectorizer - Vocabulary wasn't fitted
PyCharm 2017.1: How do I get each python import on a different line when using Alt+Enter to magically import," Currently if i do Alt+Enter on a function in a different module which isn't imported yet it simply adds it to a an existing import line.Say I have: Then I type: I love that I can simply Alt+Enter on do_something_else and it gets imported. But what happens is this: While what I would like to happen is this: I looked into the settings, but none of the ones I saw seemed right.These are the ones I've looked at for now:Where else can I look? Seems like something which should be possible. Maybe one of the options above is the one I'm looking for, but just didn't understand was the right one? <code>  from my_package.my_module import do_somethingmy_module.do_something() from my_package.my_module import do_somethingdo_something()do_something_else() # My new line from my_package.my_module import do_something, do_something_elsedo_something()do_something_else() from my_package.my_module import do_somethingfrom my_package.my_module import do_something_elsedo_something()do_something_else()",How do I get each python import on a different line when using Alt+Enter to magically import in Pycharm?
Add tag after current BeautifulSoup, I have a loop: I need to add a new tag after each tag in this loop. I tried to use the insert() method to no avail. How can I solve this task with BeautifulSoup?  <code>  for tag in soup.find('article'):,Add new HTML tag after current tag
Pandas df.resample with column-specific," With pandas.DataFrame.resample I can downsample a DataFrame: This resamples a data frame with a datetime-like index such that all values within 3 seconds are aggregated into one row. The values of the columns are averaged.Question: I have a data frame with multiple columns. Is it possible to specify a different aggregation function for different columns, e.g. I want to ""sum"" column x, ""mean"" column y and pick the ""last"" for column z? How can I achieve that effect?I know I could create a new empty data frame, and then call resample three times, but I would prefer a faster in-place solution. <code>  df.resample(""3s"", how=""mean"")",Pandas df.resample with column-specific aggregation function
Selenium3.4.0-Python3.6.1 : In Selenium-Python binding using unittest how do I decide to use self.assertIn or assert," I am working with Selenium 3.4.0 with Python 3.6.1. I have written a script following the Python documentation through unittest module which is a built-in Python based on Javas JUnit using geckodriver 0.16.1 and Mozilla Firefox 57.0 on Windows 8 Pro machine, 64 bit OS, x-64 processor. In my test method test_search_in_python_org() I have the following lines which works well: When I am asserting the ""page title"" I am using: self.assertIn(""Python"", driver.title)But, when I am asserting a string (my assumption), within the page source I am using: assert ""No results found."" not in driver.page_sourceMy question is what are the factors/conditions which decides whether I should use self.assertIn or simply assert?Any suggestions or pointers will be helpful.  <code>  def test_search_in_python_org(self): driver = self.driver driver.get(""http://www.python.org"") self.assertIn(""Python"", driver.title) elem = driver.find_element_by_name(""q"") elem.send_keys(""pycon"") elem.send_keys(Keys.RETURN) assert ""No results found."" not in driver.page_source",Selenium3.4.0-Python3.6.1 : In Selenium-Python binding using unittest how do I decide when to use self.assertIn or assert
Understanding LSTM model tensorflow for sentiment analysis," I am trying to learn LSTM model for sentiment analysis using Tensorflow, I have gone through the LSTM model.Following code (create_sentiment_featuresets.py) generates the lexicon from 5000 positive sentences and 5000 negative sentences. The following code (sentiment_analysis.py) is for sentiment analysis using simple neural network model and is working fine I am trying to modify the above (sentiment_analysis.py) for LSTM modelafter reading the RNN w/ LSTM cell example in TensorFlow and Python which is for LSTM on mnist image dataset:Some how through many hit and run trails, I was able to get the below running code (sentiment_demo_lstm.py) : Output of len(train_x)= 9596, len(train_x[0]) = 423 meaning train_x is a list of 9596x423 ?Tough I have a running code now, I still have lots of doubts.In sentiment_demo_lstm, I am not able to understand the following part I have print the following shapes: Here I took the number of hidden layers as 128, does it need to be same as the number of inputs i.e. len(train_x)= 9596The value 1 in and is because train_x[0] is 428x1 ?The following is in order to match the placeholder x = tf.placeholder('float', [None, input_vec_size, 1]) dimensions, right?If I modified the code: as I get the following error: => I can't include the last 124 records/feature-sets while training? <code>  import nltkfrom nltk.tokenize import word_tokenizeimport numpy as npimport randomfrom collections import Counterfrom nltk.stem import WordNetLemmatizerlemmatizer = WordNetLemmatizer()def create_lexicon(pos, neg): lexicon = [] with open(pos, 'r') as f: contents = f.readlines() for l in contents[:len(contents)]: l= l.decode('utf-8') all_words = word_tokenize(l) lexicon += list(all_words) f.close() with open(neg, 'r') as f: contents = f.readlines() for l in contents[:len(contents)]: l= l.decode('utf-8') all_words = word_tokenize(l) lexicon += list(all_words) f.close() lexicon = [lemmatizer.lemmatize(i) for i in lexicon] w_counts = Counter(lexicon) l2 = [] for w in w_counts: if 1000 > w_counts[w] > 50: l2.append(w) print(""Lexicon length create_lexicon: "",len(lexicon)) return l2def sample_handling(sample, lexicon, classification): featureset = [] print(""Lexicon length Sample handling: "",len(lexicon)) with open(sample, 'r') as f: contents = f.readlines() for l in contents[:len(contents)]: l= l.decode('utf-8') current_words = word_tokenize(l.lower()) current_words= [lemmatizer.lemmatize(i) for i in current_words] features = np.zeros(len(lexicon)) for word in current_words: if word.lower() in lexicon: index_value = lexicon.index(word.lower()) features[index_value] +=1 features = list(features) featureset.append([features, classification]) f.close() print(""Feature SET------"") print(len(featureset)) return featuresetdef create_feature_sets_and_labels(pos, neg, test_size = 0.1): global m_lexicon m_lexicon = create_lexicon(pos, neg) features = [] features += sample_handling(pos, m_lexicon, [1,0]) features += sample_handling(neg, m_lexicon, [0,1]) random.shuffle(features) features = np.array(features) testing_size = int(test_size * len(features)) train_x = list(features[:,0][:-testing_size]) train_y = list(features[:,1][:-testing_size]) test_x = list(features[:,0][-testing_size:]) test_y = list(features[:,1][-testing_size:]) return train_x, train_y, test_x, test_ydef get_lexicon(): global m_lexicon return m_lexicon from create_sentiment_featuresets import create_feature_sets_and_labelsfrom create_sentiment_featuresets import get_lexiconimport tensorflow as tfimport numpy as np# extras for testingfrom nltk.tokenize import word_tokenize from nltk.stem import WordNetLemmatizerlemmatizer = WordNetLemmatizer()#- end extrastrain_x, train_y, test_x, test_y = create_feature_sets_and_labels('pos.txt', 'neg.txt')# pt A-------------n_nodes_hl1 = 1500n_nodes_hl2 = 1500n_nodes_hl3 = 1500n_classes = 2batch_size = 100hm_epochs = 10x = tf.placeholder(tf.float32)y = tf.placeholder(tf.float32)hidden_1_layer = {'f_fum': n_nodes_hl1, 'weight': tf.Variable(tf.random_normal([len(train_x[0]), n_nodes_hl1])), 'bias': tf.Variable(tf.random_normal([n_nodes_hl1]))}hidden_2_layer = {'f_fum': n_nodes_hl2, 'weight': tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])), 'bias': tf.Variable(tf.random_normal([n_nodes_hl2]))}hidden_3_layer = {'f_fum': n_nodes_hl3, 'weight': tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])), 'bias': tf.Variable(tf.random_normal([n_nodes_hl3]))}output_layer = {'f_fum': None, 'weight': tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])), 'bias': tf.Variable(tf.random_normal([n_classes]))}def nueral_network_model(data): l1 = tf.add(tf.matmul(data, hidden_1_layer['weight']), hidden_1_layer['bias']) l1 = tf.nn.relu(l1) l2 = tf.add(tf.matmul(l1, hidden_2_layer['weight']), hidden_2_layer['bias']) l2 = tf.nn.relu(l2) l3 = tf.add(tf.matmul(l2, hidden_3_layer['weight']), hidden_3_layer['bias']) l3 = tf.nn.relu(l3) output = tf.matmul(l3, output_layer['weight']) + output_layer['bias'] return output# pt B--------------def train_neural_network(x): prediction = nueral_network_model(x) cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits= prediction, labels= y)) optimizer = tf.train.AdamOptimizer(learning_rate= 0.001).minimize(cost) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for epoch in range(hm_epochs): epoch_loss = 0 i = 0 while i < len(train_x): start = i end = i+ batch_size batch_x = np.array(train_x[start: end]) batch_y = np.array(train_y[start: end]) _, c = sess.run([optimizer, cost], feed_dict= {x: batch_x, y: batch_y}) epoch_loss += c i+= batch_size print('Epoch', epoch+ 1, 'completed out of ', hm_epochs, 'loss:', epoch_loss) correct= tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1)) accuracy = tf.reduce_mean(tf.cast(correct, 'float')) print('Accuracy:', accuracy.eval({x:test_x, y:test_y})) # testing -------------- m_lexicon= get_lexicon() print('Lexicon length: ',len(m_lexicon)) input_data= ""David likes to go out with Kary"" current_words= word_tokenize(input_data.lower()) current_words = [lemmatizer.lemmatize(i) for i in current_words] features = np.zeros(len(m_lexicon)) for word in current_words: if word.lower() in m_lexicon: index_value = m_lexicon.index(word.lower()) features[index_value] +=1 features = np.array(list(features)).reshape(1,-1) print('features length: ',len(features)) result = sess.run(tf.argmax(prediction.eval(feed_dict={x:features}), 1)) print(prediction.eval(feed_dict={x:features})) if result[0] == 0: print('Positive: ', input_data) elif result[0] == 1: print('Negative: ', input_data)train_neural_network(x) import tensorflow as tffrom tensorflow.contrib import rnnfrom create_sentiment_featuresets import create_feature_sets_and_labelsfrom create_sentiment_featuresets import get_lexiconimport numpy as np# extras for testingfrom nltk.tokenize import word_tokenize from nltk.stem import WordNetLemmatizerlemmatizer = WordNetLemmatizer()#- end extrastrain_x, train_y, test_x, test_y = create_feature_sets_and_labels('pos.txt', 'neg.txt')n_steps= 100input_vec_size= len(train_x[0])hm_epochs = 8n_classes = 2batch_size = 128n_hidden = 128x = tf.placeholder('float', [None, input_vec_size, 1])y = tf.placeholder('float')def recurrent_neural_network(x): layer = {'weights': tf.Variable(tf.random_normal([n_hidden, n_classes])), # hidden_layer, n_classes 'biases': tf.Variable(tf.random_normal([n_classes]))} h_layer = {'weights': tf.Variable(tf.random_normal([1, n_hidden])), # hidden_layer, n_classes 'biases': tf.Variable(tf.random_normal([n_hidden], mean = 1.0))} x = tf.transpose(x, [1,0,2]) x = tf.reshape(x, [-1, 1]) x= tf.nn.relu(tf.matmul(x, h_layer['weights']) + h_layer['biases']) x = tf.split(x, input_vec_size, 0) lstm_cell = rnn.BasicLSTMCell(n_hidden, state_is_tuple=True) outputs, states = rnn.static_rnn(lstm_cell, x, dtype= tf.float32) output = tf.matmul(outputs[-1], layer['weights']) + layer['biases'] return outputdef train_neural_network(x): prediction = recurrent_neural_network(x) cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits= prediction, labels= y)) optimizer = tf.train.AdamOptimizer(learning_rate= 0.001).minimize(cost) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for epoch in range(hm_epochs): epoch_loss = 0 i = 0 while (i+ batch_size) < len(train_x): start = i end = i+ batch_size batch_x = np.array(train_x[start: end]) batch_y = np.array(train_y[start: end]) batch_x = batch_x.reshape(batch_size ,input_vec_size, 1) _, c = sess.run([optimizer, cost], feed_dict= {x: batch_x, y: batch_y}) epoch_loss += c i+= batch_size print('--------Epoch', epoch+ 1, 'completed out of ', hm_epochs, 'loss:', epoch_loss) correct= tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1)) accuracy = tf.reduce_mean(tf.cast(correct, 'float')) print('Accuracy:', accuracy.eval({x:np.array(test_x).reshape(-1, input_vec_size, 1), y:test_y})) # testing -------------- m_lexicon= get_lexicon() print('Lexicon length: ',len(m_lexicon)) input_data= ""Mary does not like pizza"" #""he seems to to be healthy today"" #""David likes to go out with Kary"" current_words= word_tokenize(input_data.lower()) current_words = [lemmatizer.lemmatize(i) for i in current_words] features = np.zeros(len(m_lexicon)) for word in current_words: if word.lower() in m_lexicon: index_value = m_lexicon.index(word.lower()) features[index_value] +=1 features = np.array(list(features)).reshape(-1, input_vec_size, 1) print('features length: ',len(features)) result = sess.run(tf.argmax(prediction.eval(feed_dict={x:features}), 1)) print('RESULT: ', result) print(prediction.eval(feed_dict={x:features})) if result[0] == 0: print('Positive: ', input_data) elif result[0] == 1: print('Negative: ', input_data)train_neural_network(x) print(train_x[0])[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]print(train_y[0])[0, 1] x = tf.transpose(x, [1,0,2])x = tf.reshape(x, [-1, 1])x = tf.split(x, input_vec_size, 0) x = tf.placeholder('float', [None, input_vec_size, 1]) ==> TensorShape([Dimension(None), Dimension(423), Dimension(1)]))x = tf.transpose(x, [1,0,2]) ==> TensorShape([Dimension(423), Dimension(None), Dimension(1)]))x = tf.reshape(x, [-1, 1]) ==> TensorShape([Dimension(None), Dimension(1)]))x = tf.split(x, input_vec_size, 0) ==> ? x = tf.placeholder('float', [None, input_vec_size, 1]) x = tf.reshape(x, [-1, 1]) batch_x = np.array(train_x[start: end]) ==> (128, 423)batch_x = batch_x.reshape(batch_size ,input_vec_size, 1) ==> (128, 423, 1) while (i+ batch_size) < len(train_x): while i < len(train_x): Traceback (most recent call last): File ""sentiment_demo_lstm.py"", line 131, in <module> train_neural_network(x) File ""sentiment_demo_lstm.py"", line 86, in train_neural_network batch_x = batch_x.reshape(batch_size ,input_vec_size, 1)ValueError: cannot reshape array of size 52452 into shape (128,423,1)",Understanding LSTM model using tensorflow for sentiment analysis
Applying Spacy Parser to Pandas DataFrame returns blank col with Multiprocessing," Say I have a dataset, like I can use Spacy and .apply to parse a string column into tokens (my real dataset has >1 word/token per entry of course) result: I can also use this convenient multiprocessing function (thanks to this blogpost) to do most arbitrary apply functions on a dataframe in parallel: for example: result: ...But for some reason, I can't apply the Spacy parser to a dataframe using multiprocessing this way. result: Is there some other way to do this? I'm loving Spacy for NLP but I have a lot of text data and so I'd like to parallelize some processing functions, but ran into this issue. <code>  iris = pd.DataFrame(sns.load_dataset('iris')) import spacy # (I have version 1.8.2)nlp = spacy.load('en')iris['species_parsed'] = iris['species'].apply(nlp) sepal_length ... species species_parsed0 1.4 ... setosa (setosa)1 1.4 ... setosa (setosa)2 1.3 ... setosa (setosa) from multiprocessing import Pool, cpu_countdef parallelize_dataframe(df, func, num_partitions): df_split = np.array_split(df, num_partitions) pool = Pool(num_partitions) df = pd.concat(pool.map(func, df_split)) pool.close() pool.join() return df def my_func(df): df['length_of_word'] = df['species'].apply(lambda x: len(x)) return dfnum_cores = cpu_count()iris = parallelize_dataframe(iris, my_func, num_cores) sepal_length species length_of_word0 5.1 setosa 61 4.9 setosa 62 4.7 setosa 6 def add_parsed(df): df['species_parsed'] = df['species'].apply(nlp) return dfiris = parallelize_dataframe(iris, add_parsed, num_cores) sepal_length species length_of_word species_parsed0 5.1 setosa 6 ()1 4.9 setosa 6 ()2 4.7 setosa 6 ()",Applying Spacy Parser to Pandas DataFrame w/ Multiprocessing
Can I access class variable using self?, I have a class Foo with a class variable remote. Can I access the class variable remote using self.remote? <code>  class Foo: remote = False def __init__(self): self.remote = True @classmethod def print_remote(cls): print(cls.remote) #prints False but why? ,Can I access class variables using self?
python - end loop with counter and condition," In Python I can implement a loop with step counter and a stop condition as a classical case of for loop : where fun(x) is some arbitrary function from integers to integers. I always in doubts if that is the best way to code it (Pythonically, and in terms of readability and efficiency) or is it better to run it as a while loop: which approach is better? In particular - I'm concerned about the usage of break statement which doesn't feel right. <code>  for i in range(50): result = fun(i) print(i, result) if result == 0: break i = 0result = 1while result != 0 and i < 50: result = fun(i) print(i, result) i += 1",End loop with counter and condition
End loop with counter and condition?," In Python I can implement a loop with step counter and a stop condition as a classical case of for loop : where fun(x) is some arbitrary function from integers to integers. I always in doubts if that is the best way to code it (Pythonically, and in terms of readability and efficiency) or is it better to run it as a while loop: which approach is better? In particular - I'm concerned about the usage of break statement which doesn't feel right. <code>  for i in range(50): result = fun(i) print(i, result) if result == 0: break i = 0result = 1while result != 0 and i < 50: result = fun(i) print(i, result) i += 1",End loop with counter and condition
CV2: reading frames from VideoCapture advances the video to bizarrely wrong location," (I will put a 500 reputation bounty on this question as soon as it's eligible - unless the question got closed.)Problem in one sentenceReading frames from a VideoCapture advances the video much further than it's supposed to.ExplanationI need to read and analyze frames from a 100 fps (according to cv2 and VLC media player) video between certain time-intervals. In the minimal example that follows I am trying to read all the frames for the first ten seconds of a three minute video.I am creating a cv2.VideoCapture object from which I read frames until the desired position in milliseconds is reached. In my actual code each frame is analyzed, but that fact is irrelevant in order to showcase the error.Checking the current frame and millisecond position of the VideoCapture after reading the frames yields correct values, so the VideoCapture thinks it is at the right position - but it is not. Saving an image of the last read frame reveals that my iteration is grossly overshooting the destination time by over two minutes.What's even more bizarre is that if I manually set the millisecond position of the capture with VideoCapture.set to 10 seconds (the same value VideoCapture.get returns after reading the frames) and save an image, the video is at (almost) the right position!Demo video fileIn case you want to run the MCVE, you need the demo.avi video file.You can download it HERE.MCVEThis MCVE is carefully crafted and commented. Please leave a comment under the question if anything remains unclear.If you are using OpenCV 3 you have to replace all instances of cv2.cv.CV_ with cv2.. (The problem occurs in both versions for me.) MCVE outputThe print statements produce the following output. cv2 version = 2.4.9.1 initial attributes: fps = 100.0, pos_msec = 0.0, pos_frames = 0.0 attributes after reading: pos_msec = 10010.0, pos_frames = 1001.0 attributes after setting msec pos manually: pos_msec = 10010.0, pos_frames = 1001.0As you can see, all properties have the expected values.imwrite saves the following pictures.first_frame.pngafter_iteration.pngafter_setting.pngYou can see the problem in the second picture. The target of 9:26:15 (real time clock in picture) is missed by over two minutes. Setting the target time manually (third picture) sets the video to (almost) the correct position. What am I doing wrong and how do I fix it?Tried so farcv2 2.4.9.1 @ Ubuntu 16.04cv2 2.4.13 @ Scientific Linux 7.3 (three computers)cv2 3.1.0 @ Scientific Linux 7.3 (three computers)Creating the capture with and in OpenCV 3 (version 2 does not seem to have the apiPreference argument).Using cv2.CAP_GSTREAMER takes extremely long (about 2-3 minutes to run the MCVE) but both api-preferences produce the same incorrect images.When using ffmpeg directly to read frames (credit to this tutorial) the correct output images are produced. This produces the correct result! (Correct timestamp 9:26:15)frame_1001_ffmpeg_only.png:Additional informationIn the comments I was asked for my cvconfig.h file. I only seem to have this file for cv2 version 3.1.0 under /opt/opencv/3.1.0/include/opencv2/cvconfig.h. HERE is a paste of this file.In case it helps, I was able to extract the following video information with VideoCapture.get. brightness 0.0 contrast 0.0 convert_rgb 0.0 exposure 0.0 format 0.0 fourcc 1684633187.0 fps 100.0 frame_count 18000.0 frame_height 593.0 frame_width 792.0 gain 0.0 hue 0.0 mode 0.0 openni_baseline 0.0 openni_focal_length 0.0 openni_frame_max_depth 0.0 openni_output_mode 0.0 openni_registration 0.0 pos_avi_ratio 0.01 pos_frames 0.0 pos_msec 0.0 rectification 0.0 saturation 0.0 <code>  import cv2# set up capture and print propertiesprint 'cv2 version = {}'.format(cv2.__version__)cap = cv2.VideoCapture('demo.avi')fps = cap.get(cv2.cv.CV_CAP_PROP_FPS)pos_msec = cap.get(cv2.cv.CV_CAP_PROP_POS_MSEC)pos_frames = cap.get(cv2.cv.CV_CAP_PROP_POS_FRAMES)print ('initial attributes: fps = {}, pos_msec = {}, pos_frames = {}' .format(fps, pos_msec, pos_frames))# get first frame and save as picture_, frame = cap.read()cv2.imwrite('first_frame.png', frame)# advance 10 seconds, that's 100*10 = 1000 frames at 100 fpsfor _ in range(1000): _, frame = cap.read() # in the actual code, the frame is now analyzed# save a picture of the current framecv2.imwrite('after_iteration.png', frame)# print properties after iterationpos_msec = cap.get(cv2.cv.CV_CAP_PROP_POS_MSEC)pos_frames = cap.get(cv2.cv.CV_CAP_PROP_POS_FRAMES)print ('attributes after iteration: pos_msec = {}, pos_frames = {}' .format(pos_msec, pos_frames))# assert that the capture (thinks it) is where it is supposed to be# (assertions succeed)assert pos_frames == 1000 + 1 # (+1: iteration started with second frame)assert pos_msec == 10000 + 10# manually set the capture to msec position 10010# note that this should change absolutely nothing in theorycap.set(cv2.cv.CV_CAP_PROP_POS_MSEC, 10010)# print properties again to be extra surepos_msec = cap.get(cv2.cv.CV_CAP_PROP_POS_MSEC)pos_frames = cap.get(cv2.cv.CV_CAP_PROP_POS_FRAMES)print ('attributes after setting msec pos manually: pos_msec = {}, pos_frames = {}' .format(pos_msec, pos_frames))# save a picture of the next frame, should show the same clock as# previously taken image - but does not_, frame = cap.read()cv2.imwrite('after_setting.png', frame) cap = cv2.VideoCapture('demo.avi', apiPreference=cv2.CAP_FFMPEG) cap = cv2.VideoCapture('demo.avi', apiPreference=cv2.CAP_GSTREAMER) import numpy as npimport subprocess as spimport pylab# video propertiespath = './demo.avi'resolution = (593, 792)framesize = resolution[0]*resolution[1]*3# set up pipeFFMPEG_BIN = ""ffmpeg""command = [FFMPEG_BIN, '-i', path, '-f', 'image2pipe', '-pix_fmt', 'rgb24', '-vcodec', 'rawvideo', '-']pipe = sp.Popen(command, stdout = sp.PIPE, bufsize=10**8)# read first frame and save as imageraw_image = pipe.stdout.read(framesize)image = np.fromstring(raw_image, dtype='uint8')image = image.reshape(resolution[0], resolution[1], 3)pylab.imshow(image)pylab.savefig('first_frame_ffmpeg_only.png')pipe.stdout.flush()# forward 1000 framesfor _ in range(1000): raw_image = pipe.stdout.read(framesize) pipe.stdout.flush()# save frame 1001image = np.fromstring(raw_image, dtype='uint8')image = image.reshape(resolution[0], resolution[1], 3)pylab.imshow(image)pylab.savefig('frame_1001_ffmpeg_only.png')pipe.terminate()",OpenCV: reading frames from VideoCapture advances the video to bizarrely wrong location
python populate list with for loop, I'm trying to populate a list with a for loop. This is what I have so far: and at this point I am stumped. I was hoping the loops would give me a list of 10 lists. <code>  newlist = []for x in range(10): for y in range(10): newlist.append(y),How to create and fill a list of lists in a for loop
Installing pocketsphinx python module," I'm getting something like this. Can anyone please tell me how to fix this. <code>  C:\Users\krush\Documents\ML using Python>pip install pocketsphinxCollecting pocketsphinx Using cached pocketsphinx-0.1.3.zipBuilding wheels for collected packages: pocketsphinx Running setup.py bdist_wheel for pocketsphinx: started Running setup.py bdist_wheel for pocketsphinx: finished with status 'error' Complete output from command C:\Users\krush\Anaconda3\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\krush\\AppData\\Local\\Temp\\pip-build-cns2i_wb\\pocketsphinx\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d C:\Users\krush\AppData\Local\Temp\tmp3tyvnl9wpip-wheel- --python-tag cp36: running bdist_wheel running build_ext building 'sphinxbase._ad' extension swigging swig/sphinxbase/ad.i to swig/sphinxbase/ad_wrap.c swig.exe -python -modern -Ideps/sphinxbase/include -Ideps/sphinxbase/include/sphinxbase -Ideps/sphinxbase/include/win32 -Ideps/sphinxbase/swig -outdir sphinxbase -o swig/sphinxbase/ad_wrap.c swig/sphinxbase/ad.i error: command 'swig.exe' failed: No such file or directory ---------------------------------------- Failed building wheel for pocketsphinx Running setup.py clean for pocketsphinxFailed to build pocketsphinxInstalling collected packages: pocketsphinx Running setup.py install for pocketsphinx: started Running setup.py install for pocketsphinx: finished with status 'error' Complete output from command C:\Users\krush\Anaconda3\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\krush\\AppData\\Local\\Temp\\pip-build-cns2i_wb\\pocketsphinx\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\krush\AppData\Local\Temp\pip-x5mxeczy-record\install-record.txt --single-version-externally-managed --compile: running install running build_ext building 'sphinxbase._ad' extension swigging swig/sphinxbase/ad.i to swig/sphinxbase/ad_wrap.c swig.exe -python -modern -Ideps/sphinxbase/include -Ideps/sphinxbase/include/sphinxbase -Ideps/sphinxbase/include/win32 -Ideps/sphinxbase/swig -outdir sphinxbase -o swig/sphinxbase/ad_wrap.c swig/sphinxbase/ad.i error: command 'swig.exe' failed: No such file or directory ----------------------------------------Command ""C:\Users\krush\Anaconda3\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\krush\\AppData\\Local\\Temp\\pip-build-cns2i_wb\\pocketsphinx\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\krush\AppData\Local\Temp\pip-x5mxeczy-record\install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in C:\Users\krush\AppData\Local\Temp\pip-build-cns2i_wb\pocketsphinx\",Installing pocketsphinx python module: command 'swig.exe' failed
how to plot additional points on the top of scatter plot?," I have panda dataframe as df with two attributes df.one (=x) and df.two (=y). Now, I want to plot scatter plot for these data points. I used Now, I want to plot centroid of the data points give by C. How should I overlay centroid on the above scatter plot? I tried: But it overrides the scatter plot, I want to overlay on that. Is there any hold on option, similar to matlab?  <code>  ax1 = fig.add_subplot(111)ax1.scatter(df.one,df.two,c = 'g',marker = 'o',alpha = 0.2) ax1.scatter(C[:,0],C[:,1],c = 'r',marker = 'x')",How to plot additional points on the top of scatter plot?
How to get the coordinates of the bounding box in yolo object detection?, I need to get the bounding box coordinates generated in the above image using YOLO object detection. <code> ,How to get the coordinates of the bounding box in YOLO object detection?
Python - How to use a return statement in a for loop?," So I am working on a chat-bot for discord, and right now on a feature that would work as a todo-list. I have a command to add tasks to the list, where they are stored in a dict. However my problem is returning the list in a more readable format (see pictures). The tasks are stored in a dict called cal. But in order for the bot to actually send the message I need to use a return statement, otherwise it'll just print it to the console and not to the actual chat (see pictures). Here is how I tried to fix it, but since I used return the for-loop does not work properly.So how do I fix this? How can I use a return statement so that it would print into the chat instead of the console? <code>  def show_todo(): for key, value in cal.items(): print(value[0], key) def show_todo(): for key, value in cal.items(): return(value[0], key)",How to use a return statement in a for loop?
"How do I ""correctly"" copy a types.SimpleNamespace object in Python 3?"," I was expecting to be able to do something like: or maybe: My current solution, which seems to work fine is But it looks somewhat hacky. Is there a more ""correct"" way?I don't need a deep copy. <code>  a = SimpleNamespace(x='test')b = a.copy() b = SimpleNamespace(a) b = SimpleNamespace(**a.__dict__)","How to ""correctly"" copy a types.SimpleNamespace object?"
Python pysftp not getting the data," I'm trying to load (and directly save locally) a .csv file stored on a FTP Server (SFTP protocol). I'm using Python in combination with pysftp library. When I check if the file exists, it returns TRUE. But when trying to load the file, it seems to be empty, whatever I try.How can I get (and store) the file to my local environment? Do I miss something obvious? <code>  import pysftpcnopts = pysftp.CnOpts()cnopts.hostkeys = None# Make connection to sFTPwith pysftp.Connection(hostname, username=sftp_username, password=sftp_pw, cnopts = cnopts ) as sftp: sftp.isfile('directory/file.csv')) ## TRUE file = sftp.get('directory/file.csv') print(file) ## Nonesftp.close()",Downloading file with pysftp
Pandas: Ignore string columns while doing normalization," I am using the following code to normalize a pandas DataFrame: This works fine when all columns are numeric. However, now I have some string columns in df and the above normalization got errors. Is there a way to perform such normalization only on numeric columns of a data frame (keeping string column unchanged)? <code>  df_norm = (df - df.mean()) / (df.max() - df.min())",Ignore string columns while doing
produce a shorter legend or colorbar for plots within seaborn FacetGrid," I am trying to generate multi-panel figure using seaborn in python and I want the color of the points in my multi-panel figure to be specified by a continuous variable. Here's an example of what I am trying to do with the ""iris"" dataset: This makes the following figure:Which is nice, but the legend is way too long. I'd like to sample out like 1/4 of these values (ideally) or barring that display a colorbar instead.For instance, something like this might be acceptable, but I'd still want to split it over the three species. Any idea about how I can get the best of both of these plots?Edit:This topic seems like a good start.https://github.com/mwaskom/seaborn/issues/582Somehow, for this user, simply appending plt.colorbar after everything else ran seemed to somehow work. Doesn't seem to help in this case though. <code>  import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib as mplimport matplotlib.pyplot as pltiris = sns.load_dataset('iris')g = sns.FacetGrid(iris, col = 'species', hue = 'petal_length', palette = 'seismic')g = g.map(plt.scatter, 'sepal_length', 'sepal_width', s = 100, alpha = 0.5)g.add_legend() plt.scatter(iris.sepal_length, iris.sepal_width, alpha = .8, c = iris.petal_length, cmap = 'seismic')cbar = plt.colorbar()",Scatterplot with point colors representing a continuous variable in seaborn FacetGrid
What are the contraints for tensorflow scope names?," I'm running a tensorflow model and getting the following error:ValueError: 'Cement (component 1)(kg in a m^3 mixture)' is not a valid scope name.I get that tf probably doesn't like special chars and spaces in its scope names, but I'm trying to find an actual doc on what chars are allowed. Does anyone know where I could find this?Thanks. <code> ",What are the constraints for tensorflow scope names?
How can I prevent Python from inserting a new line after taking user input?," Spyder inserts a blank line in the console output between every call of the input() function but I don't want that (i.e. I want the input() prompts to be on contiguous lines in the console instead of separated by a blank line). Is there a way to do that? I tried using input(""foo"", end="""") thinking it may work like the print() function but that's not the case...Code: Output: Desired output: Edit:As others have pointed out in the comments section, this issue is non-reproducible, even for me, except through the use of the IPython interface within the Spyder IDE. If anyone is running IPython outside of Spyder, please run the above code and let me know whether that produces the same output. I can reproduce the undesired output through Spyder's IPython interface but not through a Terminal session so this is something specific to either IPython or Spyder. <code>  fname = input(""Please enter your first name: "")lname = input(""Please enter your last name: "")print(""Pleased to meet you, "" + str(fname) + "" "" + str(lname) + ""!"") Please enter your first name: JanePlease enter your last name: DoePleased to meet you, Jane Doe! Please enter your first name: JanePlease enter your last name: DoePleased to meet you, Jane Doe!",How can I prevent Spyder from inserting a new line after taking user input?
What path to install Python 3 to on Windows," The 3.6 installer suggests C:\Users\MyUserName\AppData\Local\Programs\Python\Python36-32 which is unlike any other software on Windows. I remember that earlier versions installed to C:\PythonXY which is also unusual on Windows. Is any of that really a good idea?In particular, I don't see why I would want to install this only for my user account. The checkbox ""Install launcher for all users (recommended)"" is default checked which seems incompatible with installing into %APPDATA%.What is a good path to install Python to?I'm a complete Python amateur and I don't want to cause myself problems. I am fearful of adding a space to the path for example.Clicking further through the installer it turns out there is a checkbox to install for all users. This immediately sets a Program Files (x86) based path to the checkbox. <code> ",What path to install Python 3.6 to on Windows?
What path to install Python 3.6 to on Windows," The 3.6 installer suggests C:\Users\MyUserName\AppData\Local\Programs\Python\Python36-32 which is unlike any other software on Windows. I remember that earlier versions installed to C:\PythonXY which is also unusual on Windows. Is any of that really a good idea?In particular, I don't see why I would want to install this only for my user account. The checkbox ""Install launcher for all users (recommended)"" is default checked which seems incompatible with installing into %APPDATA%.What is a good path to install Python to?I'm a complete Python amateur and I don't want to cause myself problems. I am fearful of adding a space to the path for example.Clicking further through the installer it turns out there is a checkbox to install for all users. This immediately sets a Program Files (x86) based path to the checkbox. <code> ",What path to install Python 3.6 to on Windows?
Plotly and Dash - overwriting tick labels," I cannot get the following to plot the ticklabels So basically I have a numerical representation of the a bar chart, however when I change the tick values and ticklabels, those numerical labels dissappear, however I do not see the dates that I would be expected to be there. Am I missing a switch to display these labels? <code>  self.months = [2017-01-01', 2017-02-01', ...]def plot_bar(self): print self.data app.layout = html.Div(children=[html.H1(children=''), html.Div(children='Discovered monthly'), dcc.Graph( figure=go.Figure( data = self.data, layout=go.Layout( title='Streams', showlegend=True, barmode='stack', margin=go.Margin(l=200, r=0, t=40, b=20), xaxis=dict(tickvals = self.months, ticktext = self.months, title='months') ) ), style={'height': 300}, id='my-graph') ])",Plotly (Dash) tick label overwriting
Increase the number of statically nested block," The number of statically nested blocks in Python is limited to 20.That is, nesting 19 for loops will be fine (although excessively time consuming; O(n^19) is insane), but nesting 20 will fail with: What is the underlying reason for having such a limit?Is there a way to increase the limit? <code>  SyntaxError: too many statically nested blocks",Why does Python have a limit on the number of static blocks that can be nested?
Increase the number of statically nested blocks," The number of statically nested blocks in Python is limited to 20.That is, nesting 19 for loops will be fine (although excessively time consuming; O(n^19) is insane), but nesting 20 will fail with: What is the underlying reason for having such a limit?Is there a way to increase the limit? <code>  SyntaxError: too many statically nested blocks",Why does Python have a limit on the number of static blocks that can be nested?
Increase the number of statically nested blocks," The number of statically nested blocks in Python is limited to 20.That is, nesting 19 for loops will be fine (although excessively time consuming; O(n^19) is insane), but nesting 20 will fail with: What is the underlying reason for having such a limit?Is there a way to increase the limit? <code>  SyntaxError: too many statically nested blocks",Why does Python have a limit on the number of static blocks that can be nested?
Increase the number of statically nested blocks in Python," The number of statically nested blocks in Python is limited to 20.That is, nesting 19 for loops will be fine (although excessively time consuming; O(n^19) is insane), but nesting 20 will fail with: What is the underlying reason for having such a limit?Is there a way to increase the limit? <code>  SyntaxError: too many statically nested blocks",Why does Python have a limit on the number of static blocks that can be nested?
Why does Python have a limit on the number of statically nested blocks?," The number of statically nested blocks in Python is limited to 20.That is, nesting 19 for loops will be fine (although excessively time consuming; O(n^19) is insane), but nesting 20 will fail with: What is the underlying reason for having such a limit?Is there a way to increase the limit? <code>  SyntaxError: too many statically nested blocks",Why does Python have a limit on the number of static blocks that can be nested?
What is the type of the super object retured by `super`?," From here: Return a proxy object that delegates method calls to a parent or sibling class of type. This is useful for accessing inherited methods that have been overridden in a class. The search order is same as that used by getattr() except that the type itself is skipped. If the second argument is omitted, the super object returned is unbound. If the second argument is an object, isinstance(obj, type) must be true. If the second argument is a type, issubclass(type2, type) must be true (this is useful for classmethods).If I am correct, a type is a class, and a class is a type. A classis an object, so a type is also an object. Why does the quotedistinguish the two cases when the second argument being an object when it is a type?When the second argument is a type, why is issubclass(type2, type)required to be true?What is the type of the super object returned by super in each of the three cases respectively? Or how do you determine the type of the super object returned by super?When the second argument is an object, because ""The search order is same as that used by getattr() except that the type itself is skipped"", I guessed that the type of the superobject returned by super function should be a subclass of any ancestry class of the first argument type, but I found that it is actually not by testing with issubclass. So did I misunderstand something? <code>  super( [ type [ , object-or-type ]] )",What is the type of the super object returned by super()?
How to use tf.contrib.data's initializable iterators within a tf.estimator's input_fn?," I would like to manage my training with a tf.estimator.Estimator but have some trouble to use it alongside the tf.data API.I have something like this: As I can't use a make_one_shot_iterator for my use case, my issue is that input_fn contains an iterator that should be initialized within model_fn (here, I use tf.train.Scaffold to initialize local ops).Also, I understood that we can't only use input_fn = iterator.get_next otherwise the other ops will not be added to the same graph.What is the recommended way to initialize the iterator? <code>  def model_fn(features, labels, params, mode): # Defines model's ops. # Initializes with tf.train.Scaffold. # Returns an tf.estimator.EstimatorSpec.def input_fn(): dataset = tf.data.TextLineDataset(""test.txt"") # map, shuffle, padded_batch, etc. iterator = dataset.make_initializable_iterator() return iterator.get_next()estimator = tf.estimator.Estimator(model_fn)estimator.train(input_fn)",How to use tf.data's initializable iterators within a tf.estimator's input_fn?
Reconstruct datapoint for predicting with a Regression model after using One-Hot-Encoding in training," I am writing an application which uses Linear Regression. In my case sklearn.linear_model.Ridge. I have trouble bringing my datapoint I like to predict in the correct shape for Ridge. I briefly describe my two applications and how the problem turns up:1RST APPLICATION:My datapoints have just 1 feature each, which are all Strings, so I am using One-Hot-Encoding to be able to use them with Ridge. After that, the datapoints (X_hotEncoded) have 9 features each: After fitting Ridge to X_hotEncoded and labels y I save the trained model with: 2ND APPLICATION:Now that I have a trained model saved on disk, I like to retrieve it in my 2nd application and predict y (Label) for just one datapoint. That's where I encounter above mentioned problem: This gives me the following Error in the last line of code:ValueError: shapes (1,1) and (9,) not aligned: 1 (dim 1) != 9 (dim 0)Ridge was trained with 9 features because of the use of One-Hot-Encoding I used on all the datapoints. Now, when I like to predict just one datapoint (with just 1 feature) I have trouble bringing this datapoint in the correct shape for Ridge to be able to handle it. One-Hot-Encoding has no affect on jsut one datapoint with just one feature.Does anybody know a neat solution to this problem?A possible solution might be to write the column names to disk in the 1rst Application and retrieve it in the 2nd and then rebuild the datapoint there. The column names of one-hot-encoded arrays could be retrieved like stated here: Reversing 'one-hot' encoding in Pandas <code>  import pandas as pdX_hotEncoded = pd.get_dummies(X) from sklearn.externals import joblibjoblib.dump(ridge, ""ridge.pkl"") # X = one datapoint I like to predict y for ridge= joblib.load(""ridge.pkl"")X_hotEncoded = pd.get_dummies(X)ridge.predict(X_hotEncoded) # this should give me the prediction",Getting correct shape for datapoint to predict with a Regression model after using One-Hot-Encoding in training
"PyCharm cannot resolve template, but Flask can"," PyCharm's code editor shows the popup message Template file 'index.html' not found when passing ""index.html"" to render_template. The template templates/index.html exists. Accessing http://localhost:5000/ renders the template. How do I tell PyCharm that the template exists?  <code>  @app.route('/')def index(): return render_template('index.html')",PyCharm can't find Flask template that exists
"PyCharm cannot resolve template, while Flask can"," PyCharm's code editor shows the popup message Template file 'index.html' not found when passing ""index.html"" to render_template. The template templates/index.html exists. Accessing http://localhost:5000/ renders the template. How do I tell PyCharm that the template exists?  <code>  @app.route('/')def index(): return render_template('index.html')",PyCharm can't find Flask template that exists
relu prime with numpy array," I want to pass a multidimensionnal array into the relu prime function ... where the x is the whole array. It returns ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()I've had this problem with the normal relu function, and instead of using the python function max() I used np.max() and it worked. But with the relu prime, it is not working either way. I tried: ... and it still returned the same error. How can I fix this? Thank you. <code>  def reluprime(x): if x > 0: return 1 else: return 0 def reluprime(x): if np.greater(x, 0): return 1 else: return 0",ReLU Prime with NumPy array
Pivot multiple columns - pyspark," I need to pivot more than one column in a pyspark dataframe. Sample dataframe, Now,if I need to get price column into a row for each id based on day, then I can use pivot method as, so when I need units column as well to be transposed as price, either I got to create one more dataframe as above for units and then join both using id.But, when I have more columns as such, I tried a function to do it, Need suggestions on ,if it is good practice to do so and if any other better way of doing it. Thanks in advance! <code>  >>> d = [(100,1,23,10),(100,2,45,11),(100,3,67,12),(100,4,78,13),(101,1,23,10),(101,2,45,13),(101,3,67,14),(101,4,78,15),(102,1,23,10),(102,2,45,11),(102,3,67,16),(102,4,78,18)]>>> mydf = spark.createDataFrame(d,['id','day','price','units'])>>> mydf.show()+---+---+-----+-----+| id|day|price|units|+---+---+-----+-----+|100| 1| 23| 10||100| 2| 45| 11||100| 3| 67| 12||100| 4| 78| 13||101| 1| 23| 10||101| 2| 45| 13||101| 3| 67| 14||101| 4| 78| 15||102| 1| 23| 10||102| 2| 45| 11||102| 3| 67| 16||102| 4| 78| 18|+---+---+-----+-----+ >>> pvtdf = mydf.withColumn('combcol',F.concat(F.lit('price_'),mydf['day'])).groupby('id').pivot('combcol').agg(F.first('price'))>>> pvtdf.show()+---+-------+-------+-------+-------+| id|price_1|price_2|price_3|price_4|+---+-------+-------+-------+-------+|100| 23| 45| 67| 78||101| 23| 45| 67| 78||102| 23| 45| 67| 78|+---+-------+-------+-------+-------+ >>> def pivot_udf(df,*cols):... mydf = df.select('id').drop_duplicates()... for c in cols:... mydf = mydf.join(df.withColumn('combcol',F.concat(F.lit('{}_'.format(c)),df['day'])).groupby('id').pivot('combcol').agg(F.first(c)),'id')... return mydf...>>> pivot_udf(mydf,'price','units').show()+---+-------+-------+-------+-------+-------+-------+-------+-------+| id|price_1|price_2|price_3|price_4|units_1|units_2|units_3|units_4|+---+-------+-------+-------+-------+-------+-------+-------+-------+|100| 23| 45| 67| 78| 10| 11| 12| 13||101| 23| 45| 67| 78| 10| 13| 14| 15||102| 23| 45| 67| 78| 10| 11| 16| 18|+---+-------+-------+-------+-------+-------+-------+-------+-------+",How to pivot on multiple columns in Spark SQL?
Get the type of the key of a dictionary," Is there a ""clean"" way to take the type of the keys of a dictionary in python3?For example, I want to decide if one of this dictionaries has keys of type str: There is several ways to achieve this, for example, using some as: But this is quite annoying because d2.keys() is not indexable, so you need to convert it into a list just to extract the value of one element of the list and check the type.So has python3 something as get_key_type(d2)?If not, is there a better (cleaner) way to ask if the key of a dictionary is of type str? <code>  d1 = { 1:'one', 2:'two', 5:'five' }d2 = { '1':'one', '2':'two', '5':'five' } isinstance(list(d2.keys())[0], type('str'))",Get the types of the keys in a dictionary
"Python Requests returning 403 forbidden, despite using correct user-agent headers."," response.status_code is returning 403. I can browse the website using firefox/chrome, so It seems to be a coding error.I can't figure out what mistake I'm making. Thank you.  <code>  import requestsimport webbrowserfrom bs4 import BeautifulSoupurl = 'https://www.gamefaqs.com'#headers={'User-Agent': 'Mozilla/5.0'} headers ={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'}response = requests.get(url, headers)",Python requests - 403 forbidden - despite setting `User-Agent` headers
Python: reformatting file names," I have the following code: In the dictionary, the dataframes are named as ""folder/name(csv)"" what I would like to do is remove the prefix ""staging/"" from the keys in the dictionary. How can I do this? <code>  os.listdir(""staging"")# Seperate filename from extensionsep = os.sep# Change the casingfor n in os.listdir(""staging""): print(n) if os.path.isfile(""staging"" + sep + n): filename_one, extension = os.path.splitext(n) os.rename(""staging"" + sep + n, ""staging"" + sep + filename_one.lower() + extension)# Show the new file namesprint ('\n--------------------------------\n')for n in os.listdir(""staging""): print (n)# Remove the blanks, -, %, and /for n in os.listdir(""staging""): print (n) if os.path.isfile(""staging"" + sep + n): filename_zero, extension = os.path.splitext(n) os.rename(""staging"" + sep + n , ""staging"" + sep + filename_zero.replace(' ','_').replace('-','_').replace('%','pct').replace('/','_') + extension)# Show the new file namesprint ('\n--------------------------------\n')for n in os.listdir(""staging""): print (n)""""""In order to fix all of the column headers and to solve the encoding issues and remove nulls, first read in all of the CSV's to python as dataframes, then make changes and rewrite the old files""""""import osimport globimport pandas as pdfiles = glob.glob(os.path.join(""staging"" + ""/*.csv""))print(files)# Create an empty dictionary to hold the dataframes from csvsdict_ = {}# Write the files into the dictionaryfor file in files: dict_[file] = pd.read_csv(file, header = 0, dtype = str, encoding = 'cp1252').fillna('')",How to extract the file name from a file path?
How to replace multiple matches / groups with Python regexes?," Normally we would write the following to replace one match: What i want is to replace, probably using back references, each group with a specific text. Namely i want to replace the first group (is) with ""are"" and the second group (life) with ""butterflies"". Maybe something like that. But the following is not working code. Is there a way to replace multiple groups in one statement in python? <code>  namesRegex = re.compile(r'(is)|(life)', re.I)replaced = namesRegex.sub(r""butter"", ""There is no life in the void."")print(replaced)output:There butter no butter in the void. namesRegex = re.compile(r'(is)|(life)', re.I)replaced = namesRegex.sub(r""(are) (butterflies)"", r""\1 \2"", ""There is no life in the void."")print(replaced)",How to replace multiple matches / groups with regexes?
Import a simple tensorflow frozen_model.pb file and make prediction in c++," I am trying to import a graph I exported from Tensorflow Python into Tensorflow C++. I've already successfully re-imported the graph into Python. The only thing I want now is to write the same code in C++ but I am not sure about the C++ api functions and there usage as the documentation on the Tensorflow website is not good enough.Here's the C++ code I found so far.C++: The problem I am having with the current c++ code above is that it says it cannot find any operation by the name of prefix/input_neurons:0. Although there is an operation in the graph because when i import this graph in the Python code (shown below), it works perfectly fine.Here's the Python code to import the graph successfully.Python: ( Works perfectly fine ) UpdateI can print the operations from the python script. Here's the screenshot.Here's the error I get. <code>  namespace tf = tensorflow;tf::Session* session;tf::Status status = tf::NewSession(tf::SessionOptions(), &session);checkStatus(status);tf::GraphDef graph_def;status = ReadBinaryProto(tf::Env::Default(), ""./models/frozen_model.pb"", &graph_def);checkStatus(status);status = session->Create(graph_def);checkStatus(status);tf::Tensor x(tf::DT_FLOAT, tf::TensorShape());tf::Tensor y(tf::DT_FLOAT, tf::TensorShape());x.scalar<float>()() = 23.0;y.scalar<float>()() = 19.0;std::vector<std::pair<tf::string, tf::Tensor>> input_tensors = {{""x"", x}, {""y"", y}};std::vector<string> vNames; // vector of names for required graph nodesvNames.push_back(""prefix/input_neurons:0"");vNames.push_back(""prefix/prediction_restore:0"");std::vector<tf::Tensor> output_tensors;status = session->Run({}, vNames, {}, &output_tensors);checkStatus(status);tf::Tensor output = output_tensors[0];std::cout << ""Success: "" << output.scalar<float>() << ""!"" << std::endl;session->Close();return 0; def load_graph(frozen_graph_filename): # We load the protobuf file from the disk and parse it to retrieve the # unserialized graph_def with tf.gfile.GFile(frozen_graph_filename, ""rb"") as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) # Then, we can use again a convenient built-in function to import a graph_def into the # current default Graph with tf.Graph().as_default() as graph: tf.import_graph_def( graph_def, input_map=None, return_elements=None, name=""prefix"", op_dict=None, producer_op_list=None ) return graph# We use our ""load_graph"" functiongraph = load_graph(""./models/frozen_model.pb"")# We can verify that we can access the list of operations in the graphfor op in graph.get_operations(): print(op.name) # <--- printing the operations snapshot below # prefix/Placeholder/inputs_placeholder # ... # prefix/Accuracy/predictions# We access the input and output nodesx = graph.get_tensor_by_name('prefix/input_neurons:0')y = graph.get_tensor_by_name('prefix/prediction_restore:0')# We launch a Sessionwith tf.Session(graph=graph) as sess: test_features = [[0.377745556,0.009904444,0.063231111,0.009904444,0.003734444,0.002914444,0.008633333,0.000471111,0.009642222,0.05406,0.050163333,7e-05,0.006528889,0.000314444,0.00649,0.043956667,0.016816667,0.001644444,0.016906667,0.00204,0.027342222,0.13864]] # compute the predicted output for test_x pred_y = sess.run( y, feed_dict={x: test_features} ) print(pred_y)",Import a simple Tensorflow frozen_model.pb file and make prediction in C++
adding extra column as the culumative time difference.," How to add an extra column that is the cumulative value of the time differences for each course? For example, the initial table is: The expected result is: <code>  id_A course weight ts_A value id1 cotton 3.5 2017-04-27 01:35:30 150.000000 id1 cotton 3.5 2017-04-27 01:36:00 416.666667 id1 cotton 3.5 2017-04-27 01:36:30 700.000000 id1 cotton 3.5 2017-04-27 01:37:00 950.000000 id2 cotton blue 5.0 2017-04-27 02:35:30 150.000000 id2 cotton blue 5.0 2017-04-27 02:36:00 450.000000 id2 cotton blue 5.0 2017-04-27 02:36:30 520.666667 id2 cotton blue 5.0 2017-04-27 02:37:00 610.000000 id_A course weight ts_A value cum_delta_sec id1 cotton 3.5 2017-04-27 01:35:30 150.000000 0 id1 cotton 3.5 2017-04-27 01:36:00 416.666667 30 id1 cotton 3.5 2017-04-27 01:36:30 700.000000 60 id1 cotton 3.5 2017-04-27 01:37:00 950.000000 90 id2 cotton blue 5.0 2017-04-27 02:35:30 150.000000 0 id2 cotton blue 5.0 2017-04-27 02:36:00 450.000000 30 id2 cotton blue 5.0 2017-04-27 02:36:30 520.666667 60 id2 cotton blue 5.0 2017-04-27 02:37:00 610.000000 90",Add extra column as the cumulative time difference
Add extra column as the culumative time difference," How to add an extra column that is the cumulative value of the time differences for each course? For example, the initial table is: The expected result is: <code>  id_A course weight ts_A value id1 cotton 3.5 2017-04-27 01:35:30 150.000000 id1 cotton 3.5 2017-04-27 01:36:00 416.666667 id1 cotton 3.5 2017-04-27 01:36:30 700.000000 id1 cotton 3.5 2017-04-27 01:37:00 950.000000 id2 cotton blue 5.0 2017-04-27 02:35:30 150.000000 id2 cotton blue 5.0 2017-04-27 02:36:00 450.000000 id2 cotton blue 5.0 2017-04-27 02:36:30 520.666667 id2 cotton blue 5.0 2017-04-27 02:37:00 610.000000 id_A course weight ts_A value cum_delta_sec id1 cotton 3.5 2017-04-27 01:35:30 150.000000 0 id1 cotton 3.5 2017-04-27 01:36:00 416.666667 30 id1 cotton 3.5 2017-04-27 01:36:30 700.000000 60 id1 cotton 3.5 2017-04-27 01:37:00 950.000000 90 id2 cotton blue 5.0 2017-04-27 02:35:30 150.000000 0 id2 cotton blue 5.0 2017-04-27 02:36:00 450.000000 30 id2 cotton blue 5.0 2017-04-27 02:36:30 520.666667 60 id2 cotton blue 5.0 2017-04-27 02:37:00 610.000000 90",Add extra column as the cumulative time difference
Why does an inner join/merge in pandas dataframe give more rows than left dataframe?," Here are how the dataframes columns look like. df1='device number', 'date', ....<<10 other columns>> 3500 recordsdf2='device number', 'date', ....<<9 other columns>> 14,000 recordsIn each data frame, neither 'device number', nor 'date' are unique. However, their combination is unique to identify a row. I am trying to form a new data frame which matches the rows from df1 and df2 where both device number and date are equal, and have all the columns from these df1 and df2. The pandas command I am trying is However, df3 gives me a dataframe of shape (14,000, 21). The column number makes sense, but how can the inner join has more rows than any of the left dataframes? Does it mean I have a flaw in my understanding of inner join? Also, how can I achieve the result I described?  <code>  df3=pd.merge(df1, df2, how='inner', on=['device number', 'date'])",inner join/merge in pandas dataframe give more rows than left dataframe
Python regular expression to split list into lists based on a character occurring inside of an element," In a list like the one below: There could be some numerical elements preceded by a character. I would like to break this into sub-lists like below: As you can tell, depending upon the character, the lists could look similar. Otherwise they could have a different number of elements, or dissimilar elements altogether. The main separator is the ""|"" character. I have tried to run the following code to split up the list, but all I get is the same, larger, list within a list. I.e., list of len(list) == 1. Any ideas how to split it up successfully? <code>  biglist = ['X', '1498393178', '1|Y', '15496686585007', '-82', '-80', '-80', '3', '3', '2', '|Y', '145292534176372', '-87', '-85', '-85', '3', '3', '2', '|Y', '11098646289856', '-91', '-88', '-89', '3', '3', '2', '|Y', '35521515162112', '-82', '-74', '-79', '3', '3', '2', '|Z', '0.0', '0.0', '0', '0', '0', '0', '0', '4', '0', '154'] smallerlist = [ ['X', '1498393', '1'], ['Y', '1549668', '-82', '-80', '-80', '3', '3', '2', ''], ['Y', '1452925', '-87', '-85', '-85', '3', '3', '2', ''], ['Y', '3552151', '-82', '-74', '-79', '3', '3', '2', ''], ['Z', '0.0', '0.0', '0', '0', '0', '0', '0', '4', '0', '154']] import itertoolsdelim = '|'smallerlist = [list(y) for x, y in itertools.groupby(biglist, lambda z: z == delim) if not x]",Split list into lists based on a character occurring inside of an element
Decompose a float into mantissa and exponent without strings," Are there functions in the Python library or numpy that take a float as input and return its decimal scientific notation decomposition, i.e. mantissa and exponent? Or is there a BRIEF way to accomplish this without resorting to string conversions or using a for loop to determine the exponent? Writing such a function wouldn't be difficult, I'm just shocked that I'm having trouble finding an existing one in math, decimal or numpy.e.g. if fexp and fman are the functions giving the exponent and mantissa of the decimal floating point representation of a float then we'd expect the following statements to all return true: In short, this would be a ""decimal version"" of math.frexp. <code>  fexp(154.3) == 2.0fman(154.3) == 1.543fexp(-1000) == 3.0fman(-1000) == -1.0",Decompose a float into mantissa and exponent in base 10 without strings
TensorFlow crashes when fitting model," I am trying to fit at TensorForestEstimator model with numerical floating-point data representing 7 features and 7 labels. That is, the shape of both features and labels is (484876, 7). I set num_classes=7 and num_features=7 in ForestHParamsappropriately. The format of the data is as follows: When calling fit() Python crashes with the following message: Python quit unexpectedly while using the _pywrap_tensorflow_internal.so plug-in.Here is the output when enabling tf.logging.set_verbosity('INFO'): I'm not sure what this error means, it doesn't really make sense since num_classes=7, not 8 and as the shape of features and labels is (484876, 7), I don't know where the 39001 is coming from.Here is the code to reproduce: It also doesn't work if I wrap it with SKCompat, the same error occur. What is the cause of this crash? <code>  f1 f2 f3 f4 f5 f6 f7 l1 l2 l3 l4 l5 l6 l739000.0 120.0 65.0 1000.0 25.0 0.69 3.94 39000.0 39959.0 42099.0 46153.0 49969.0 54127.0 55911.032000.0 185.0 65.0 1000.0 75.0 0.46 2.19 32000.0 37813.0 43074.0 48528.0 54273.0 60885.0 63810.0 30000.0 185.0 65.0 1000.0 25.0 0.41 1.80 30000.0 32481.0 35409.0 39145.0 42750.0 46678.0 48595.0 INFO:tensorflow:training graph for tree: 0INFO:tensorflow:training graph for tree: 1... INFO:tensorflow:training graph for tree: 9998INFO:tensorflow:training graph for tree: 9999INFO:tensorflow:Create CheckpointSaverHook.2017-07-26 10:25:30.908894: F tensorflow/contrib/tensor_forest/kernels/count_extremely_random_stats_op.cc:404] Check failed: column < num_classes_ (39001 vs. 8)Process finished with exit code 134 (interrupted by signal 6: SIGABRT) import numpy as npimport pandas as pdimport osdef get_training_data(): training_file = ""data.txt"" data = pd.read_csv(training_file, sep='\t') X = np.array(data.drop('Result', axis=1), dtype=np.float32) y = [] for e in data.ResultStr: y.append(list(np.array(str(e).replace('[', '').replace(']', '').split(',')))) y = np.array(y, dtype=np.float32) features = tf.constant(X) labels = tf.constant(y) return features, labelshyperparameters = ForestHParams( num_trees=100, max_nodes=10000, bagging_fraction=1.0, num_splits_to_consider=0, feature_bagging_fraction=1.0, max_fertile_nodes=0, split_after_samples=250, min_split_samples=5, valid_leaf_threshold=1, dominate_method='bootstrap', dominate_fraction=0.99, # All parameters above are default num_classes=7, num_features=7)estimator = TensorForestEstimator( params=hyperparameters, # All parameters below are default device_assigner=None, model_dir=None, graph_builder_class=RandomForestGraphs, config=None, weights_name=None, keys_name=None, feature_engineering_fn=None, early_stopping_rounds=100, num_trainers=1, trainer_id=0, report_feature_importances=False, local_eval=False)estimator.fit( input_fn=lambda: get_training_data(), max_steps=100, monitors=[ TensorForestLossHook( early_stopping_rounds=30 ) ])",TensorFlow crashes when fitting TensorForestEstimator
Django REST Swagger https requests," How configure django-rest-swagger to get a HTTPS requests?upd:SSL cert is present and ALL app working with it, but swagger make a http requests. <code> ",Django REST Swagger HTTPS requests
cosine similarity between each row in a dataframe in python," I have a DataFrame containing multiple vectors each having 3 entries. Each row is a vector in my representation. I needed to calculate the cosine similarity between each of these vectors. Converting this to a matrix representation is better or is there a cleaner approach in DataFrame itself?Here is the code that I have tried. <code>  import pandas as pdfrom scipy import spatialdf = pd.DataFrame([X,Y,Z]).Tsimilarities = df.values.tolist()for x in similarities: for y in similarities: result = 1 - spatial.distance.cosine(x, y)",Cosine similarity between each row in a Dataframe in Python
Cosine similarity between each row in a Dataframe in Python.," I have a DataFrame containing multiple vectors each having 3 entries. Each row is a vector in my representation. I needed to calculate the cosine similarity between each of these vectors. Converting this to a matrix representation is better or is there a cleaner approach in DataFrame itself?Here is the code that I have tried. <code>  import pandas as pdfrom scipy import spatialdf = pd.DataFrame([X,Y,Z]).Tsimilarities = df.values.tolist()for x in similarities: for y in similarities: result = 1 - spatial.distance.cosine(x, y)",Cosine similarity between each row in a Dataframe in Python
get original keyd from defaultdict," Is there a way to get the original/consistent list of keys from defaultdict even when non existing keys were requested? <code>  from collections import defaultdict>>> d = defaultdict(lambda: 'default', {'key1': 'value1', 'key2' :'value2'})>>>>>> d.keys()['key2', 'key1']>>> d['bla']'default'>>> d.keys() # how to get the same: ['key2', 'key1']['key2', 'key1', 'bla']",get original key set from defaultdict
How to plot multiple subplotted plots in the same page in python," I have a data set like this:Sample Dataframe: I only know how to generate individual plot: How can I plot all four figures in the same page?This is what I want:Update on 8/2/2017:I would also like to apply it to larger datasets. Here is @Phlya's code I tried but it doesn't give me what I want:A larger dataset: This dataset has 11 sets of data and clearly you can see the code made a mistake: <code>  import pandas as pdimport numpy as npimport matplotlib.pyplot as pltdf = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD')) for k, m in zip('ABCD', 'mbry'): plt.figure(k) for i in range(5): plt.subplot(5,1,i+1) plt.bar(range(20), df[k][20*i: 20*(i+1)], color = m) plt.subplots_adjust(wspace=0, hspace=0)plt.show() import pandas as pdimport numpy as npimport matplotlib.pyplot as pltfrom matplotlib import gridspecdf = pd.DataFrame(np.random.randint(0,100,size=(100, 11)), columns=list('ABCDEFGHIJK'))from mpl_toolkits.axes_grid1 import axes_gridf = plt.figure()for i, (k, m) in enumerate(zip('ABCDEFGHIJK', 'mbrygrygybr')): ag = axes_grid.Grid(f, 261+i, (5, 1), axes_pad=0) for j in range(5): ag[j].bar(range(20), df[k][20*j: 20*(j+1)], color = m) ag[j].set_ylim(0, df.max().max()) if i%2==0: if j == 4: ag[j].yaxis.set_ticks([0, ag[j].get_yticks()[-1]]) else: ag[j].yaxis.set_ticks([ag[j].get_yticks()[-1]]) else: ag[j].yaxis.set_ticks([]) if i in (0, 1): ag[j].xaxis.set_ticks([])plt.subplots_adjust(wspace=0.5, hspace=0.5)plt.show()",How to do nested subplots in python
How to correctly use scipy's kurtosis?," The skewness is a parameter to measure the symmetry of a data set and the kurtosis to measure how heavy its tails are compared to a normal distribution, see for example here.scipy.stats provides an easy way to calculate these two quantities, see scipy.stats.kurtosis and scipy.stats.skew.In my understanding, the skewness and kurtosis of a normal distribution should both be 0 using the functions just mentioned. That is, however, not the case with my code: The output is:excess kurtosis of normal distribution (should be 0): -0.307393087742skewness of normal distribution (should be 0): 1.11082371392What am I doing wrong ?The versions I am using are <code>  import numpy as npfrom scipy.stats import kurtosisfrom scipy.stats import skewx = np.linspace( -5, 5, 1000 )y = 1./(np.sqrt(2.*np.pi)) * np.exp( -.5*(x)**2 ) # normal distributionprint( 'excess kurtosis of normal distribution (should be 0): {}'.format( kurtosis(y) ))print( 'skewness of normal distribution (should be 0): {}'.format( skew(y) )) python: 2.7.6scipy : 0.17.1numpy : 1.12.1",How to correctly use scipy's skew and kurtosis functions?
groupby and count the number of unique value," I have a dataframe with 2 variables: ID and outcome. I'm trying to groupby ID first, and count the number of unique values of outcome within that ID. Expected output: My code df[['PID', 'outcome']].groupby('PID')['outcome'].nunique() gives the number of the unique value itself, such that: But I need the counts of the yes and no, how can I achieve that? Thanks! <code>  dfID outcome1 yes1 yes1 yes2 no2 yes2 no ID yes no1 3 02 1 2 ID1 22 2",Groupby and count the number of unique values (Pandas)
Set chrome browser binary to chrome webdriver in Python," I used Selenium with Python Chrome webdriver.In my code I used: to point the webdriver to the webdriver executable. Is there a way to point webdriver to the Chrome Browser binaries?In https://sites.google.com/a/chromium.org/chromedriver/capabilities they have the following (which I assume it what I'm looking for): Anyone has an example for Python? <code>  driver = webdriver.Chrome(executable_path = PATH_TO_WEBDRIVER) ChromeOptions options = new ChromeOptions();options.setBinary(""/path/to/other/chrome/binary"");",Set chrome browser binary through chromedriver in Python
How to easily parallelize numpy.apply_along_axis() on multiple cores?," How could the application of a function to the elements of a NumPy array through numpy.apply_along_axis() be parallelized so as to take advantage of multiple cores? This seems to be a natural thing to do, in the common case where all the calls to the function being applied are independent.In my particular caseif this matters, the axis of application is axis 0: np.apply_along_axis(func, axis=0, arr=param_grid) (np being NumPy).I had a quick look at Numba, but I can't seem to get this parallelization, with a loop like: There is also apparently a compilation option in NumPy for parallelization through OpenMP, but it does not seem to be accessible through MacPorts.One can also think of maybe cutting the array in a few pieces and using threads (so as to avoid copying the data) and applying the function on each piece in parallel. This is more complex than what I am looking for (and might not work if the Global Interpreter Lock is not released enough).It would be very nice to be able to use multiple cores in a simple way for simple parallelizable tasks like applying a function to all the elements of an array (which is essentially what is needed here, with the small complication that function func() takes a 1D array of parameters). <code>  @numba.jit(parallel=True)result = np.empty(shape=params.shape[1:])for index in np.ndindex(*result.shape)): # All the indices of params[0,...] result[index] = func(params[(slice(None),) + index]) # Applying func along axis 0",Easy parallelization of numpy.apply_along_axis()?
Easy parallelization of the application of a function on each element of a NumPy array?," How could the application of a function to the elements of a NumPy array through numpy.apply_along_axis() be parallelized so as to take advantage of multiple cores? This seems to be a natural thing to do, in the common case where all the calls to the function being applied are independent.In my particular caseif this matters, the axis of application is axis 0: np.apply_along_axis(func, axis=0, arr=param_grid) (np being NumPy).I had a quick look at Numba, but I can't seem to get this parallelization, with a loop like: There is also apparently a compilation option in NumPy for parallelization through OpenMP, but it does not seem to be accessible through MacPorts.One can also think of maybe cutting the array in a few pieces and using threads (so as to avoid copying the data) and applying the function on each piece in parallel. This is more complex than what I am looking for (and might not work if the Global Interpreter Lock is not released enough).It would be very nice to be able to use multiple cores in a simple way for simple parallelizable tasks like applying a function to all the elements of an array (which is essentially what is needed here, with the small complication that function func() takes a 1D array of parameters). <code>  @numba.jit(parallel=True)result = np.empty(shape=params.shape[1:])for index in np.ndindex(*result.shape)): # All the indices of params[0,...] result[index] = func(params[(slice(None),) + index]) # Applying func along axis 0",Easy parallelization of numpy.apply_along_axis()?
Jinja unable to convert html form data to json," I want to take input from an HTML form and give the output in JSON format. When multiple values are selected they are not converted into JSON arrays, only the first value is used. form.html: result.html: When multiple values for owner are selected, ""thor"" and ""flash"", the output shows only one value: I expect owners to be a list: How do I display the form as JSON without losing list values? <code>  @app.route('/form')def show_form(): return render_template('form.html')@app.route(""/result"", methods=['POST'])def show_result(): result = request.form return render_template('result.html', result=result) <form method=POST> <input name=server> <select name=owners multiple> <option value=""thor"">thor</option> <option value=""loki"">loki</option> <option value=""flash"">flash</option> <option value=""batman"">batman</option> </select> <input type=submit></form> {{ result|tojson }} {""server"": ""app-srv"", ""owners"": ""thor""} {""server"": ""app-srv"", ""owners"": [""thor"", ""flash""]}",Converting Flask form data to JSON only gets first value
Why is an IndentionError being raised here rather than a SyntaxError?," Why in the following program is an IndentationError being raised rather than SyntaxError? To make sure the IDLE wasn't just acting funny, I also tested this code by running it from a normal source file. The same exception type is still being raised. The versions of Python I used to test this were Python 3.5.2 and Python 3.6.1. It is my understanding that missing parenthesis when using print was considered a SyntaxError, not an IndentationError. The top answer in the post What does SyntaxError: Missing parentheses in call to 'print' mean in Python? also seems to support this: SyntaxError: Missing parentheses in call to 'print' is a new error message that was added in Python 3.4.2 primarily to help users that are trying to follow a Python 2 tutorial while running Python 3.Is this a bug? If so, what's causing it? <code>  >>> if True:... print ""just right!"" File ""<stdin>"", line 2 print ""just right!"" ^IndentationError: Missing parentheses in call to 'print'",Why is an IndentationError being raised here rather than a SyntaxError?
PyQt draggable line with multiple break points," I have a application very similar to the following question:Draw half infinite lines?I would like to have a infinite line with multiple thresholds. The solution provided in the question is a great starting point: https://stackoverflow.com/a/37836348/7163293I attempted to make the lines movable by modifying the movable attribute in __init__ and add a setMovable method just as the source code in source: http://www.pyqtgraph.org/documentation/_modules/pyqtgraph/graphicsItems/InfiniteLine.html#InfiniteLine However, the line is still not movable after the modifications.So I am kind of stuck here. Would someone be able to provide some pointers?Also, ideally, the line on the applications should be movable by segments. So when the user drag a line, only the portion in between break points are moving. So ideally I would like to have something like:Draggable line with draggable pointsin my application. Ideally it would look something likewith the threshold point level (TH_Px_L1) draggable but not the timing (TH_Px_T1), so the points can only move vertically.If someone can also help on the second item and provide some pointers or solution that will be very helpful. <code>  from pyqtgraph.Qt import QtGuiimport numpy as npimport pyqtgraph as pgclass InfiniteLineWithBreak(pg.GraphicsObject): def __init__(self, changeX, levelsY, pen=None): pg.GraphicsObject.__init__(self) self.changeX = changeX self.levelsY = levelsY self.maxRange = [None, None] self.moving = False self.movable = True self.setMovable(self.movable) self.mouseHovering = False pen = (200, 200, 100) self.setPen(pen) self.setHoverPen(color=(255,0,0), width=self.pen.width()) self.currentPen = self.pen def setMovable(self, m): """"""Set whether the line is movable by the user."""""" self.movable = m self.setAcceptHoverEvents(m) def setBounds(self, bounds): self.maxRange = bounds self.setValue(self.value()) def setPen(self, *args, **kwargs): self.pen = pg.fn.mkPen(*args, **kwargs) if not self.mouseHovering: self.currentPen = self.pen self.update() def setHoverPen(self, *args, **kwargs): self.hoverPen = pg.fn.mkPen(*args, **kwargs) if self.mouseHovering: self.currentPen = self.hoverPen self.update() def boundingRect(self): br = self.viewRect() return br.normalized() def paint(self, p, *args): br = self.boundingRect() p.setPen(self.currentPen) # three lines (left border to change point, change point vertical, change point to right) p.drawLine(pg.Point(br.left(), self.levelsY[0]), pg.Point(self.changeX, self.levelsY[0])) p.drawLine(pg.Point(self.changeX, self.levelsY[0]), pg.Point(self.changeX, self.levelsY[1])) p.drawLine(pg.Point(self.changeX, self.levelsY[1]), pg.Point(br.right(), self.levelsY[1])) def dataBounds(self, axis, frac=1.0, orthoRange=None): if axis == 0: return None ## x axis should never be auto-scaled else: return (0,0) def setMouseHover(self, hover): passapp = QtGui.QApplication([])w = pg.GraphicsWindow()w.resize(1000, 600)v = w.addPlot(y=np.random.normal(size=100))v.addItem(InfiniteLineWithBreak(changeX=50, levelsY=(-1, 1)))app.exec_()",draggable line with multiple break points
Check if a rows exists in pandas," I want to check if a row exists in dataframe, following is my code: This is the output: Does if 'entry' in df2: only check if 'entry' exists as a column? It must be the case, I guess. We can see that the row 'entry' exists but we still land in the else condition(if it had landed in if the statement sum for Apr 2016 would be 23).If I check it for the file which don't have the row 'entry', it again lands in else statement(as I expect), so I assume it always enters the else condition.How do I check if a row exists in pandas? <code>  df = pd.read_csv('dbo.Access_Stat_all.csv',error_bad_lines=False, usecols=['Name','Format','Resource_ID','Number'])df1 = df[df['Resource_ID'] == 30957]df1 = df1[['Format','Name','Number']]df1 = df1.groupby(['Format','Name'], as_index=True).last()pd.options.display.float_format = '{:,.0f}'.formatdf1 = df1.unstack()df1.columns = df1.columns.droplevel()if 'entry' in df1: df2 = df1[1:4].sum(axis=0)else: df2 = df1[0:3].sum(axis=0)df2.name = 'sum'df2 = df1.append(df2)print(df2) Name Apr 2013 Apr 2014 Apr 2015 Apr 2016 Apr 2017 Aug 2010 Aug 2013 Format entry 0 0 0 1 4 1 0 pdf 13 12 4 23 7 1 9 sum 13 12 4 24 11 2 9 ",Check if a row exists in pandas
Unresolved reference with scapy," I am working on a network tool that I write in python using scapy.As IDE I am using Pycharm.My Code works. So if I run it, everything works just as intended.My problem is that PyCharm is giving me some errors.It marks every use of IP, TCP, Ether, ... as Undefined Reference to ... The relevant parts of my Code look like this I tried many things I found using google, like adding my src folder as source root, I refreshed all caches I could find and restarted PyCharm dozens of times, but nothing worked... Since the code works it's a minor problem, but still I'd like to have my IDE working as intended I am working under MacOS and I use a Virtual Environment <code>  #!/usr/bin/env pythonfrom scapy.all import *... ... syn = IP(src=src_ip, dst=dst_ip) / TCP(sport=src_port, dport=dst_port, seq=src_seq, flags=""S"")...",PyCharm: Unresolved reference with Scapy
backward - fill increment by 12 months," I have a dataframe with course names for each year. I need to find the duration in months starting from year 2016. How do I add months in a new column starting from highest (i.e. bottom like bfill?)The final data-frame will look like this... Some of the answers do not consider that there can be multiple courses. Updating sample data... <code>  from io import StringIOimport pandas as pdu_cols = ['page_id','web_id']audit_trail = StringIO('''year_id | web_id2012|efg2013|abc 2014| xyz2015| pqr2016| mnp''')df11 = pd.read_csv(audit_trail, sep=""|"", names = u_cols ) u_cols = ['page_id','web_id' , 'months']audit_trail = StringIO('''year_id | web_id | months2012|efg | 602013|abc | 482014| xyz | 362015| pqr | 242016| mnp | 12''')df12 = pd.read_csv(audit_trail, sep=""|"", names = u_cols ) from io import StringIOimport pandas as pdu_cols = ['course_name','page_id','web_id']audit_trail = StringIO('''course_name| year_id | web_ida|2012|efga|2013|abc a|2014| xyza|2015| pqra|2016| mnpb|2014| xyzb|2015| pqrb|2016| mnp''')df11 = pd.read_csv(audit_trail, sep=""|"", names = u_cols )",Pandas backward fill increment by 12 months
Shorter alternative for `lambda` keyword in python," Background:Python is about simplicity and readable code. It has gotten better over the versions and I am a huge fan! However, typing l a m b d a every time I have to define a lambda is not fun (you may disagree).The problem is, these 6 characters l a m b d a make my statements longer, especially if I nest a couple of lambdas inside maps and filters.I am not nesting more than 2 or three, because it takes away the readability of python, even then typing l a m b d a feels too verbose.The actual question (is in comments): I am happy to add import like this: <code>  # How to rename/alias a keyword to a nicer one? lines = map(lmd x: x.strip(), sys.stdin)# OR, better yet, how to define my own operator like -> in python?lines = map(x -> x.strip(), sys.stdin)# Or may be :: operator is pythoniclines = map(x :: x.strip(), sys.stdin)# INSTEAD of this ugly one. Taking out this is my goal!lines = map(lambda x: x.strip(), sys.stdin) from myfuture import lmd_as_lambda# ORfrom myfuture import lambda_operator",Shorter alternative for 'lambda' keyword?
Keras: How to use predict_generator with ImageDataGenerator?," I'm very new to Keras. I trained a model and would like to predict some images stored in subfolders (like for training). For testing, I want to predict 2 images from 7 classes (subfolders). The test_generator below sees 14 images, but I get 196 predictions. Where is the mistake? Thanks a lot! <code>  test_datagen = ImageDataGenerator(rescale=1./255)test_generator = test_datagen.flow_from_directory( test_dir, target_size=(200, 200), color_mode=""rgb"", shuffle = ""false"", class_mode='categorical')filenames = test_generator.filenamesnb_samples = len(filenames)predict = model.predict_generator(test_generator,nb_samples)",How to use predict_generator with ImageDataGenerator?
What are different options for objectives in XGBClassifier?," Apart from binary:logistic (which is the default objective function), Is there any other built-in objective function that can be used in xbgoost.XGBClassifier() ? <code> ",What are different options for objective functions available in xgboost.XGBClassifier?
OpenCV Python Count Pixels, I'm working with a little project with application of OpenCV and I'm stuck with something that I don't know how to implement. Suppose I have an image (1024x768). In this image there is a red bounding box at the center.Is it possible to count the pixels inside the red box using OpenCS? given that the image is 1024x768 in dimension.I tried to use bounding rectangle by thresholding the red color and tried using convexhull but then I can't extract how many pixels are inside the red marker. <code> ,OpenCV Python count pixels
How to use conditional on every element of array using [:] syntax?," If i need to ask a condition on every element of a numpy.ndarray of integers, do I have to use a for loop or can I ask the question using [:] syntax I know the previous is wrong, but is there any way of doing something similar? <code>  for i in range(n): if a[i] == 0: a[i] = 1 if a[:] == 0: #...",(Python) How to use conditional statements on every element of array using [:] syntax?
Python NLP - When to lowercase text during preprocessing," I want to build a model for language modelling, which should predict the next words in a sentence, given the previous word(s) and/or the previous sentence.Use case: I want to automate writing reports. So the model should automatically complete the sentence I am writing. Therefore, it is important that nouns and the words at the beginning of a sentence are capitalized. Data: The data is in German and contains a lot of technical jargon.My text corpus is in German and I am currently working on the preprocessing. Because my model should predict gramatically correct sentences I have decided to use/not use the following preprocessing steps:no stopword removalno lemmatizationreplace all expressions with numbers by NUMBERnormalisation of synonyms and abbreviations replace rare words with RAREHowever, I am not sure whether to convert the corpus to lowercase. When searching the web I found different opinions. Although lower-casing is quite common it will cause my model to wrongly predict the capitalization of nouns, sentence beginnings etc.I also found the idea to convert only the words at the beginning of a sentence to lower-case on the following Stanford page.What is the best strategy for this use-case? Should I convert the text to lower-case and change the words to the correct case after prediction? Should I leave the capitalization as it is? Should I only lowercase words at the beginning of a sentence?Thanks a lot for any suggestions and experiences! <code> ",NLP - When to lowercase text during preprocessing
OTSU Binarization method not functioning on a PNG image," I am currently working to solve this, any help would be appreciated. As mentioned in the comments, the PIL image needs to be converted to CV2 accepted format, can anyone provide an explanation using the example given below?  <code>  gray_image = cv2.cvtColor(contrast, cv2.COLOR_BGR2GRAY)TypeError: src is not a numpy array, neither a scalar import cv2import numpy as npfrom matplotlib import pyplot as pltfrom cycler import cyclerfrom PIL import Image, ImageEnhance# Loads the image then enhances itimage = Image.open('lineCapture.png')contrast = ImageEnhance.Contrast(image)# Reads the enhanced image and converts it to grayscale, creates new filegray_image = cv2.cvtColor(contrast, cv2.COLOR_BGR2GRAY) //there is a problem herecv2.imwrite('enhancedGrayscaleLineCapture.png', gray_image)# Adaptive Gaussian Thresholdingth1 = cv2.adaptiveThreshold(gray_image,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\ cv2.THRESH_BINARY,11,2)# Otsu's thresholdingret2,th2 = cv2.threshold(gray_image,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)# Otsu's thresholding after Gaussian filteringblur = cv2.GaussianBlur(gray_image,(5,5),0)ret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)# writes enhanced and thresholded imgcv2.imwrite('enhancedGrayscaleThresholdLineCapture.png', th2)","TypeError: src is not a numpy array, neither a scalar"
Python:Dataframe apply doesn't accept axis argument," I have two dataframes: data and rules . I am trying to add two new columns into the data dataframe after computing the Levenshtein similarity between each vendor and rule. So my dataframe should ideally contain columns looking like this: So far I am trying to perform an apply function that will return me this structure, but the dataframe apply is not accepting the axis argument. Could someone please help me figure out what I am doing wrong? Any change I make is just creating new errors.Thank you <code>  >>>data >>>rules vendor rule0 googel 0 google1 google 1 dell2 googly 2 macbook >>>data vendor rule similarity0 googel google 0.8 >>> for index,r in rules.iterrows():... data[['rule','similarity']]=data['vendor'].apply(lambda row:[r[0],ratio(row[0],r[0])],axis=1)...Traceback (most recent call last):File ""<stdin>"", line 2, in <module>File ""/home/mnnr/test/env/test-1.0/runtime/lib/python3.4/site-packages/pandas/core/series.py"", line 2220, in applymapped = lib.map_infer(values, f, convert=convert_dtype)File ""pandas/src/inference.pyx"", line 1088, in pandas.lib.map_infer (pandas/lib.c:62658)File ""/home/mnnr/test/env/test-1.0/runtime/lib/python3.4/site-packages/pandas/core/series.py"", line 2209, in <lambda>f = lambda x: func(x, *args, **kwds)TypeError: <lambda>() got an unexpected keyword argument 'axis'",Dataframe apply doesn't accept axis argument
Efficiently select minimum value associated to a non-unique pair of arrays," I have two arrays of N floats (which act as (x,y) coordinates and might have duplicates) and and associated z array of N floats (which act as weights for the coordinates).For each (x,y) pair of floats I need to select the pair with the smallest associated z value. I've defined a selectMinz() function which does this (see code below) but it takes too long.How could I improve the performance of this function? <code>  import numpy as npimport timedef getData(): N = 100000 x = np.arange(0.0005, 0.03, 0.001) y = np.arange(6., 10., .05) # Select N values for x,y, where values can be repeated x = np.random.choice(x, N) y = np.random.choice(y, N) z = np.random.uniform(10., 15., N) return x, y, zdef selectMinz(x, y, z): """""" Select the minimum z for each (x,y) pair. """""" xy_unq, z_unq = [], [] # For each (x,y) pair for i, xy in enumerate(zip(*[x, y])): # If this xy pair was already stored in the xy_unq list if xy in xy_unq: # If the stored z value associated with this xy pair is # larger than this new z[i] value if z_unq[xy_unq.index(xy)] > z[i]: # Store this smaller value instead z_unq[xy_unq.index(xy)] = z[i] else: # Store the xy pair, and its associated z value xy_unq.append(xy) z_unq.append(z[i]) return xy_unq, z_unq# Define data with the proper format.x, y, z = getData()s = time.clock()xy_unq, z_unq = selectMinz(x, y, z) # <-- TAKES TOO LONG (~15s in my system)print(time.clock() - s)",Efficiently get minimum values for each pair of elements from two arrays in a third array
"In Python, rowwise min() and max() fails when containing a NaN"," I am trying to take the rowwise max (and min) of two columns containing dates as expected. However, if the dataframe contains a single NaN value, the whole operation fails What is going on here? I was expecting this result How can this be achieved? <code>  from datetime import dateimport pandas as pdimport numpy as np df = pd.DataFrame({'date_a' : [date(2015, 1, 1), date(2012, 6, 1), date(2013, 1, 1), date(2016, 6, 1)], 'date_b' : [date(2012, 7, 1), date(2013, 1, 1), date(2014, 3, 1), date(2013, 4, 1)]})df[['date_a', 'date_b']].max(axis=1)Out[46]: 0 2015-01-011 2013-01-012 2014-03-013 2016-06-01 df_nan = pd.DataFrame({'date_a' : [date(2015, 1, 1), date(2012, 6, 1), np.NaN, date(2016, 6, 1)], 'date_b' : [date(2012, 7, 1), date(2013, 1, 1), date(2014, 3, 1), date(2013, 4, 1)]})df_nan[['date_a', 'date_b']].max(axis=1)Out[49]: 0 NaN 1 NaN2 NaN3 NaNdtype: float64 0 2015-01-011 2013-01-012 NaN3 2016-06-01",Rowwise min() and max() fails for column with NaNs
Rowwise min() and max() fails when containing a NaN," I am trying to take the rowwise max (and min) of two columns containing dates as expected. However, if the dataframe contains a single NaN value, the whole operation fails What is going on here? I was expecting this result How can this be achieved? <code>  from datetime import dateimport pandas as pdimport numpy as np df = pd.DataFrame({'date_a' : [date(2015, 1, 1), date(2012, 6, 1), date(2013, 1, 1), date(2016, 6, 1)], 'date_b' : [date(2012, 7, 1), date(2013, 1, 1), date(2014, 3, 1), date(2013, 4, 1)]})df[['date_a', 'date_b']].max(axis=1)Out[46]: 0 2015-01-011 2013-01-012 2014-03-013 2016-06-01 df_nan = pd.DataFrame({'date_a' : [date(2015, 1, 1), date(2012, 6, 1), np.NaN, date(2016, 6, 1)], 'date_b' : [date(2012, 7, 1), date(2013, 1, 1), date(2014, 3, 1), date(2013, 4, 1)]})df_nan[['date_a', 'date_b']].max(axis=1)Out[49]: 0 NaN 1 NaN2 NaN3 NaNdtype: float64 0 2015-01-011 2013-01-012 NaN3 2016-06-01",Rowwise min() and max() fails for column with NaNs
Filtering a pandas data frame: unknown exec behaviour," I am trying to filter a pandas data frame using thresholds for three columns However, I want to do this inside a function where the names of the columns and their thresholds are given to me in a dictionary. Here's my first try that works ok. Essentially I am putting the filter inside cond variable and just run it: Now, finally I put everything inside a function and it stops working (perhaps exec function does not like to be used inside a function!): I know that exec function acts differently when used inside a function but was not sure how to address the problem. Also, I am wondering there must be a more elegant way to define a function to do the filtering given two input: 1)df and 2)limits_dic = {""A"" : 0, ""B"" : 2, ""C"" : -1}. I would appreciate any thoughts on this. <code>  import pandas as pddf = pd.DataFrame({""A"" : [6, 2, 10, -5, 3], ""B"" : [2, 5, 3, 2, 6], ""C"" : [-5, 2, 1, 8, 2]})df = df.loc[(df.A > 0) & (df.B > 2) & (df.C > -1)].reset_index(drop = True)df A B C0 2 5 21 10 3 12 3 6 2 df = pd.DataFrame({""A"" : [6, 2, 10, -5, 3], ""B"" : [2, 5, 3, 2, 6], ""C"" : [-5, 2, 1, 8, 2]})limits_dic = {""A"" : 0, ""B"" : 2, ""C"" : -1}cond = ""df = df.loc[""for key in limits_dic.keys(): cond += ""(df."" + key + "" > "" + str(limits_dic[key])+ "") & ""cond = cond[:-2] + ""].reset_index(drop = True)""exec(cond)df A B C0 2 5 21 10 3 12 3 6 2 df = pd.DataFrame({""A"" : [6, 2, 10, -5, 3], ""B"" : [2, 5, 3, 2, 6], ""C"" : [-5, 2, 1, 8, 2]})limits_dic = {""A"" : 0, ""B"" : 2, ""C"" : -1}def filtering(df, limits_dic): cond = ""df = df.loc["" for key in limits_dic.keys(): cond += ""(df."" + key + "" > "" + str(limits_dic[key])+ "") & "" cond = cond[:-2] + ""].reset_index(drop = True)"" exec(cond) return(df)df = filtering(df, limits_dic)df A B C0 6 2 -51 2 5 22 10 3 13 -5 2 84 3 6 2",Dynamically filtering a pandas dataframe
Make a list of list as a row in pandas dataframe," I have a list of list that I would like to make it as a row. The closest I got was using this post. However, I could not get my answer.For example lets say I have a testarray of values, I need like as a pandas DataFrame row like this, I tried the following, I get an error, ValueError: If using all scalar values, you must pass an indexHow can I pass those values with an index? <code>  ([[ 26.85406494], [ 27.85406494], [ 28.85406494], [ 29.85406494], [ 30.85406494], [ 31.85406494], [ 32.85406494], [ 33.85406494], [ 34.85406494], [ 35.85406494], [ 36.85406494], [ 37.85406494], [ 38.85406494], [ 39.85406494], [ 40.85406494], [ 41.85406494]]) Row_value0 26.854064941 27.854064942 29.85406494... df = pd.DataFrame({'Row_value':testarray})",Converting a 2D numpy array to dataframe rows
Convert timestamp to date in spark dataframe," I've seen (here: How to convert Timestamp to Date format in DataFrame?) the way to convert a timestamp in datetype, but,at least for me, it doesn't work.Here is what I've tried: But this returns null in the column date_again: Any idea of what's failing? <code>  # Create dataframedf_test = spark.createDataFrame([('20170809',), ('20171007',)], ['date',])# Convert to timestampdf_test2 = df_test.withColumn('timestamp',func.when((df_test.date.isNull() | (df_test.date == '')) , '0')\.otherwise(func.unix_timestamp(df_test.date,'yyyyMMdd')))\# Convert timestamp to date againdf_test2.withColumn('date_again', df_test2['timestamp'].cast(stypes.DateType())).show() +--------+----------+----------+| date| timestamp|date_again|+--------+----------+----------+|20170809|1502229600| null||20171007|1507327200| null|+--------+----------+----------+",Convert timestamp to date in Spark dataframe
Having trouble with functions and their use," I am currently having trouble completing this challenge in ""Automate the boring stuff"": My code is: And I am getting this error: I know I am doing SOMETHING wrong with how I wrote my code but I don't understand what it is exactly. Any and all help is greatly appreciated!Also I am using python 3. <code>  def collatz(number): global seqNum if (seqNum % 2 == 0): return seqNum // 2 elif (seqNum % 2 == 1): return 3 * seqNum + 1print('What number would you like to use?')seqNum = input()number = int(seqNum)i = numberwhile i > 1: collatz(seqNum) print(number) ""Traceback (most recent call last): File ""C:/Users/Administrative/AppData/Local/Programs/Python/Python36-32/collatzSeq.py"", line 15, in <module> collatz(seqNum) File ""C:/Users/Administrative/AppData/Local/Programs/Python/Python36-32/collatzSeq.py"", line 3, in collatz if (seqNum % 2 == 0):TypeError: not all arguments converted during string formatting""",Implementing the collatz function using Python
Problems with my implementation of a collatz function," I am currently having trouble completing this challenge in ""Automate the boring stuff"": My code is: And I am getting this error: I know I am doing SOMETHING wrong with how I wrote my code but I don't understand what it is exactly. Any and all help is greatly appreciated!Also I am using python 3. <code>  def collatz(number): global seqNum if (seqNum % 2 == 0): return seqNum // 2 elif (seqNum % 2 == 1): return 3 * seqNum + 1print('What number would you like to use?')seqNum = input()number = int(seqNum)i = numberwhile i > 1: collatz(seqNum) print(number) ""Traceback (most recent call last): File ""C:/Users/Administrative/AppData/Local/Programs/Python/Python36-32/collatzSeq.py"", line 15, in <module> collatz(seqNum) File ""C:/Users/Administrative/AppData/Local/Programs/Python/Python36-32/collatzSeq.py"", line 3, in collatz if (seqNum % 2 == 0):TypeError: not all arguments converted during string formatting""",Implementing the collatz function using Python
"VISUAL STUDIO 2017 IronPython Hello World gets error: ""the environment ironpython 2.7-32 appears to be incorrectly configured or missing"""," Steps to the Reproduce:Open Visual Studio 2017 (Pro on Windows 10 64-bit)File > New Project > IronPython ApplicationRun the default program: print('Hello world')When I run it, I get the following error: The environment ""IronPython|2.7-32"" appears to be incorrectly configured or missing. You may need to install it or create a virtual environmentI went to VS Installer, Individual Components tab, Compilers section, and checked on the Python 2.7 & 3.6 selections, but I'm still getting the problem <code> ","Visual Studio - ""The environment IronPython|2.7-32 appears to be incorrectly configured or missing"""
"Visual Studio - ""The environment ironpython 2.7-32 appears to be incorrectly configured or missing"""," Steps to the Reproduce:Open Visual Studio 2017 (Pro on Windows 10 64-bit)File > New Project > IronPython ApplicationRun the default program: print('Hello world')When I run it, I get the following error: The environment ""IronPython|2.7-32"" appears to be incorrectly configured or missing. You may need to install it or create a virtual environmentI went to VS Installer, Individual Components tab, Compilers section, and checked on the Python 2.7 & 3.6 selections, but I'm still getting the problem <code> ","Visual Studio - ""The environment IronPython|2.7-32 appears to be incorrectly configured or missing"""
Pandas : Map columns from one dataframe to another dataframe creating a new column, i have a dataframe i have another dataframe df2 I want my final dataframe to look like: i.e map from one dataframe onto another creating new column <code>  id store address1 100 xyz2 200 qwe3 300 asd4 400 zxc5 500 bnm serialNo store_code warehouse 1 300 Land 2 500 Sea 3 100 Land 4 200 Sea 5 400 Land id store address warehouse1 100 xyz Land2 200 qwe Sea3 300 asd Land4 400 zxc Land5 500 bnm Sea,Mapping columns from one dataframe to another to create a new column
PYTHON - How to update rows in a CSV file," Hello I'm trying to make a program that updates the values in a csv. The user searches for the ID, and if the ID exists, it gets the new values you want to replace on the row where that ID number is. Here row[0:9] is the length of my ID. My idea was to scan each row from 0-9 or where my ID number is, and when its found, I will replace the values besides it using the .replace() method. This how i did it: But it's not working, I need ideas on how to get through this. <code>  def update_thing(): replace = stud_ID +','+ stud_name +','+ stud_course +','+ stud_year empty = [] with open(fileName, 'r+') as upFile: for row in f: if row[0:9] == stud_ID: row=row.replace(row,replace) msg = Label(upd_win, text=""Updated Successful"", font=""fixedsys 12 bold"").place(x=3,y=120) if not row[0:9] == getID: empty.append(row) upFile.close() upFile = open(fileName, 'w') upFile.writelines(empty) upFile.close() ",How to update rows in a CSV file
How to stop sound in pygame," Ok so basically I'm working on this pygame game and I'm using the mixer sub module for sounds. However, in pygame if you want to stop a sound, the sound must finish its cycle until it can be stopped. For all my sounds this is ok as they can be a looped 1 second sound. However, I use a falling sound which needs to constantly play in a cycle, but cycling a falling sound that is only 1 second long doesn't sound very effective. So, ultimately I want to know if there is a way to stop a 7 second sound loop instantly without it finishing a cycle. <code> ",How to stop sound in pygame?
Perfomance of map vs starmap?," I was trying to make a pure-python (without external dependencies) element-wise comparison of two sequences. My first solution was: Then I found starmap function from itertools, which seemed pretty similar to me. But it turned out to be 37% faster on my computer in worst case. As it was not obvious to me, I measured the time necessary to retrieve 1 element from a generator (don't know if this way is correct): In retrieving elements the second solution is 24% more performant. After that, they both produce the same results for list. But from somewhere we gain extra 13% in time: I don't know how to dig deeper in profiling of such nested code? So my question is why the first generator so faster in retrieving and from where we gain extra 13% in list function?EDIT:My first intention was to perform element-wise comparison instead of all, so the all function was replaced with list. This replacement does not affect the timing ratio.CPython 3.6.2 on Windows 10 (64bit)  <code>  list(map(operator.eq, seq1, seq2)) from operator import eqfrom itertools import starmapseq1 = [1,2,3]*10000seq2 = [1,2,3]*10000seq2[-1] = 5gen1 = map(eq, seq1, seq2))gen2 = starmap(eq, zip(seq1, seq2))%timeit -n1000 -r10 next(gen1)%timeit -n1000 -r10 next(gen2)271 ns 1.26 ns per loop (mean std. dev. of 10 runs, 1000 loops each)208 ns 1.72 ns per loop (mean std. dev. of 10 runs, 1000 loops each) %timeit list(map(eq, seq1, seq2))%timeit list(starmap(eq, zip(seq1, seq2)))5.24 ms 29.4 s per loop (mean std. dev. of 7 runs, 100 loops each)3.34 ms 84.8 s per loop (mean std. dev. of 7 runs, 100 loops each)",Performance of map vs starmap?
Filtering pandas df with multiple boolean variables," I am trying to filter a df using several Boolean variables that are a part of the df, but have been unable to do so. Sample data: The dtype for columns C and D is Boolean. I want to create a new df (df1) with only the rows where either C or D is True. It should look like this: I've tried something like this, which faces issues because it cant handle the Boolean type: Any ideas? <code>  A | B | C | DJohn Doe | 45 | True | FalseJane Smith | 32 | False | FalseAlan Holmes | 55 | False | TrueEric Lamar | 29 | True | True A | B | C | DJohn Doe | 45 | True | FalseAlan Holmes | 55 | False | TrueEric Lamar | 29 | True | True df1 = df[(df['C']=='True') or (df['D']=='True')]",Filtering pandas dataframe with multiple Boolean columns
Filtering pandas dataframe with multiple boolean columns," I am trying to filter a df using several Boolean variables that are a part of the df, but have been unable to do so. Sample data: The dtype for columns C and D is Boolean. I want to create a new df (df1) with only the rows where either C or D is True. It should look like this: I've tried something like this, which faces issues because it cant handle the Boolean type: Any ideas? <code>  A | B | C | DJohn Doe | 45 | True | FalseJane Smith | 32 | False | FalseAlan Holmes | 55 | False | TrueEric Lamar | 29 | True | True A | B | C | DJohn Doe | 45 | True | FalseAlan Holmes | 55 | False | TrueEric Lamar | 29 | True | True df1 = df[(df['C']=='True') or (df['D']=='True')]",Filtering pandas dataframe with multiple Boolean columns
lazy evaluation and late binding of python," when is lazy evaluation? (generator, if, iterator?),when is late binding? (closure, regular functions?) <code>  a = [1,2,3,4] b = [lambda y: x for x in a] c = (lambda y: x for x in a) #lazy evaluation d = map(lambda m: lambda y:m, a) #closure for i in b: print i(None) # 4 4 4 4 for i in c: print i(None) # 1 2 3 4 for i in d: print i(None) # 1 2 3 4",lazy evaluation and late binding of python?
Extracting column in Jupyter notebook (pandas DataFrame)," I create DataFrame object in Jupyter notebook: When I'm extracting column 'year', it's ok: But when I'm extracting column 'pop' (frame.pop), result is: Why the result is not the same as for ""frame.year""? <code>  data = {'state':['Ohio','Ohio','Ohio','Nevada','Nevada'], 'year':[2000, 2001, 2002, 2000, 2001], 'pop':[1.5, 2.0, 3.6, 2.4, 2.9]}frame = DataFrame(data) In [30]: frame.yearOut[30]: 0 2000 1 2001 2 2002 3 2000 4 2001 Name: year, dtype: int64 Out[31]:<bound method NDFrame.pop of pop state year0 1.5 Ohio 20001 2.0 Ohio 20012 3.6 Ohio 20023 2.4 Nevada 20004 2.9 Nevada 2001>","Attempt to access dataframe column displays ""<bound method NDFrame.xxx..."""
dajngo 'AnonymousUser' object has no attribute '_meta'," I am using social login in my Django app. So, I have added additional backends in my settings.py file. ]I have also used UserCreationForm for signup, This is the views file, Now, I get this error when i click signup button on my form, at the line, Why so ?I can see in my admin panel that user has been saved.What is causing this error ? and how to solve it ?EDIT -  <code>  AUTHENTICATION_BACKENDS = [ 'django.contrib.auth.backends.ModelBackend', 'social_core.backends.open_id.OpenIdAuth', 'social_core.backends.google.GoogleOpenId', 'social_core.backends.google.GoogleOAuth2', 'social_core.backends.google.GoogleOAuth', 'social_core.backends.twitter.TwitterOAuth', 'social_core.backends.facebook.FacebookOAuth2', 'social_core.backends.github.GithubOAuth2', class SignupForm(UserCreationForm): first_name = forms.CharField(max_length=30, required=True, help_text='Required.') last_name = forms.CharField(max_length=30, required=True, help_text='Required.') email = forms.EmailField(max_length=254, help_text='Required. Inform a valid email address.') class Meta: model = User fields = ('username', 'first_name', 'last_name', 'email', 'password1', 'password2' ) def signup(request): if request.method == 'POST': form = SignupForm(request.POST) if form.is_valid(): form.save() username = form.cleaned_data.get('username') raw_pass = form.cleaned_data.get('password') user = authenticate(request, username=username, password=raw_pass) login(request,user,backend='django.contrib.auth.backends.ModelBackend') url = reverse('location:get_location') print(""location_url "", url) return HttpResponseRedirect(url) else: form = SignupForm() return render(request, 'signup.html', {'form':form}) 'AnonymousUser' object has no attribute '_meta' login(request,user,backend='django.contrib.auth.backends.ModelBackend') Internal Server Error: /signup/Traceback (most recent call last): File ""/home/luvpreet/Envs/weather/local/lib/python2.7/site-packages/django/core/handlers/exception.py"", line 41, in inner response = get_response(request) File ""/home/luvpreet/Envs/weather/local/lib/python2.7/site-packages/django/core/handlers/base.py"", line 187, in _get_response response = self.process_exception_by_middleware(e, request) File ""/home/luvpreet/Envs/weather/local/lib/python2.7/site-packages/django/core/handlers/base.py"", line 185, in _get_response response = wrapped_callback(request, *callback_args, **callback_kwargs) File ""/home/luvpreet/Desktop/drf-vogo/weather/weather/pilot/views.py"", line 45, in signup login(request,user,backend='django.contrib.auth.backends.ModelBackend') File ""/home/luvpreet/Envs/weather/local/lib/python2.7/site-packages/django/contrib/auth/__init__.py"", line 154, in login request.session[SESSION_KEY] = user._meta.pk.value_to_string(user) File ""/home/luvpreet/Envs/weather/local/lib/python2.7/site-packages/django/utils/functional.py"", line 239, in inner return func(self._wrapped, *args)AttributeError: 'AnonymousUser' object has no attribute '_meta'",Django 'AnonymousUser' object has no attribute '_meta'
django 'AnonymousUser' object has no attribute '_meta'," I am using social login in my Django app. So, I have added additional backends in my settings.py file. ]I have also used UserCreationForm for signup, This is the views file, Now, I get this error when i click signup button on my form, at the line, Why so ?I can see in my admin panel that user has been saved.What is causing this error ? and how to solve it ?EDIT -  <code>  AUTHENTICATION_BACKENDS = [ 'django.contrib.auth.backends.ModelBackend', 'social_core.backends.open_id.OpenIdAuth', 'social_core.backends.google.GoogleOpenId', 'social_core.backends.google.GoogleOAuth2', 'social_core.backends.google.GoogleOAuth', 'social_core.backends.twitter.TwitterOAuth', 'social_core.backends.facebook.FacebookOAuth2', 'social_core.backends.github.GithubOAuth2', class SignupForm(UserCreationForm): first_name = forms.CharField(max_length=30, required=True, help_text='Required.') last_name = forms.CharField(max_length=30, required=True, help_text='Required.') email = forms.EmailField(max_length=254, help_text='Required. Inform a valid email address.') class Meta: model = User fields = ('username', 'first_name', 'last_name', 'email', 'password1', 'password2' ) def signup(request): if request.method == 'POST': form = SignupForm(request.POST) if form.is_valid(): form.save() username = form.cleaned_data.get('username') raw_pass = form.cleaned_data.get('password') user = authenticate(request, username=username, password=raw_pass) login(request,user,backend='django.contrib.auth.backends.ModelBackend') url = reverse('location:get_location') print(""location_url "", url) return HttpResponseRedirect(url) else: form = SignupForm() return render(request, 'signup.html', {'form':form}) 'AnonymousUser' object has no attribute '_meta' login(request,user,backend='django.contrib.auth.backends.ModelBackend') Internal Server Error: /signup/Traceback (most recent call last): File ""/home/luvpreet/Envs/weather/local/lib/python2.7/site-packages/django/core/handlers/exception.py"", line 41, in inner response = get_response(request) File ""/home/luvpreet/Envs/weather/local/lib/python2.7/site-packages/django/core/handlers/base.py"", line 187, in _get_response response = self.process_exception_by_middleware(e, request) File ""/home/luvpreet/Envs/weather/local/lib/python2.7/site-packages/django/core/handlers/base.py"", line 185, in _get_response response = wrapped_callback(request, *callback_args, **callback_kwargs) File ""/home/luvpreet/Desktop/drf-vogo/weather/weather/pilot/views.py"", line 45, in signup login(request,user,backend='django.contrib.auth.backends.ModelBackend') File ""/home/luvpreet/Envs/weather/local/lib/python2.7/site-packages/django/contrib/auth/__init__.py"", line 154, in login request.session[SESSION_KEY] = user._meta.pk.value_to_string(user) File ""/home/luvpreet/Envs/weather/local/lib/python2.7/site-packages/django/utils/functional.py"", line 239, in inner return func(self._wrapped, *args)AttributeError: 'AnonymousUser' object has no attribute '_meta'",Django 'AnonymousUser' object has no attribute '_meta'
"How to add new column start to the dataframe, Python 3.6", I have dataframe with 30 columns and want to add one new column to start. <code> ,Insert a column at the beginning (leftmost end) of a DataFrame
Insert a column at the beginning in a pandas dataframe, I have dataframe with 30 columns and want to add one new column to start. <code> ,Insert a column at the beginning (leftmost end) of a DataFrame
PySpark - String matching to create new binary column," I have a dataframe like: Let's say for example there are only 3 employees to check: John, Stacy, or Marsha. I'd like to make a new column like so: Is regex or grep better here? What kind of function should I try? Thanks!EDIT: I've been trying a bunch of solutions, but nothing seems to work. Should I give up and instead create columns for each employee, with a binary value? IE: <code>  ID Notes2345 Checked by John2398 Verified by Stacy3983 Double Checked on 2/23/17 by Marsha ID Notes Employee2345 Checked by John John2398 Verified by Stacy Stacy3983 Double Checked on 2/23/17 by Marsha Marsha ID Notes John Stacy Marsha2345 Checked by John 1 0 02398 Verified by Stacy 0 1 03983 Double Checked on 2/23/17 by Marsha 0 0 1",PySpark - String matching to create new column
Tensorflow- Feature Column," I'm relatively new to Tensor Flow. What is this feature column and how does it affect the training? When I implement a code like below, this numeric column is created as a feature column. I would like to understand the use. <code>  feature_columns = [tf.feature_column.numeric_column(""x"", shape=[1])]estimator = tf.estimator.LinearRegressor(feature_columns=feature_columns)x_train = np.array([1., 2., 3., 4.])y_train = np.array([0., -1., -2., -3.])x_eval = np.array([2., 5., 8., 1.])y_eval = np.array([-1.01, -4.1, -7, 0.])input_fn = tf.estimator.inputs.numpy_input_fn( {""x"": x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)train_input_fn = tf.estimator.inputs.numpy_input_fn({""x"": x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)eval_input_fn = tf.estimator.inputs.numpy_input_fn({""x"": x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)estimator.train(input_fn=input_fn, steps=10000)train_metrics = estimator.evaluate(input_fn=train_input_fn)eval_metrics = estimator.evaluate(input_fn=eval_input_fn)print(""\n\ntrain metrics: %r""% train_metrics)print(""eval metrics: %r""% eval_metrics)",What is this feature column and how does it affect the training?
Django custom Func," In the process of finding a solution for Django ORM order by exact, I created a custom django Func: which works as follows: But as @hynekcer commented: ""It crashes easily by ') in '') from myapp_suburb; drop ... expected that the name of the app is ""myapp and autocommit is enabled.""The main problem is that extra data (substring) got into the template without sqlescape which leaves the app vulnerable to SQL injection attacks.I cannot find which is the Django way to protect from that. I created a repo (djposfunc) where you can test any solution. <code>  from django.db.models import Funcclass Position(Func): function = 'POSITION' template = ""%(function)s(LOWER('%(substring)s') in LOWER(%(expressions)s))"" template_sqlite = ""instr(lower(%(expressions)s), lower('%(substring)s'))"" def __init__(self, expression, substring): super(Position, self).__init__(expression, substring=substring) def as_sqlite(self, compiler, connection): return self.as_sql(compiler, connection, template=self.template_sqlite) class A(models.Model): title = models.CharField(max_length=30)data = ['Port 2', 'port 1', 'A port', 'Bport', 'Endport']for title in data: A.objects.create(title=title)search = 'port'qs = A.objects.filter( title__icontains=search ).annotate( pos=Position('title', search) ).order_by('pos').values_list('title', flat=True)# result is# ['Port 2', 'port 1', 'Bport', 'A port', 'Endport'] ",Django custom for complex Func (sql function)
Converting Pands DatetimeIndex to float format," I want to convert the DatetimeIndex in my DataFrame to float format,which can be analysed in my model.Could someone tell me how to do it? Do I need to use date2num()function?Many thanks! <code> ",Converting Pandas DatetimeIndex to a numeric format
Plot hyperplane Linear SVM o=python," I am trying to plot the hyperplane for the model I trained with LinearSVC and sklearn. Note that I am working with natural languages; before fitting the model I extracted features with CountVectorizer and TfidfTransformer.Here the classifier: Then I tried to plot as suggested on the Scikit-learn website: This example uses svm.SVC(kernel='linear'), while my classifier is LinearSVC. Therefore, I get this error: How can I successfully plot the hyperplan of my LinearSVC classifier? <code>  from sklearn.svm import LinearSVCfrom sklearn import svmclf = LinearSVC(C=0.2).fit(X_train_tf, y_train) # get the separating hyperplanew = clf.coef_[0]a = -w[0] / w[1]xx = np.linspace(-5, 5)yy = a * xx - (clf.intercept_[0]) / w[1]# plot the parallels to the separating hyperplane that pass through the# support vectorsb = clf.support_vectors_[0]yy_down = a * xx + (b[1] - a * b[0])b = clf.support_vectors_[-1]yy_up = a * xx + (b[1] - a * b[0])# plot the line, the points, and the nearest vectors to the planeplt.plot(xx, yy, 'k-')plt.plot(xx, yy_down, 'k--')plt.plot(xx, yy_up, 'k--')plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80, facecolors='none')plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)plt.axis('tight')plt.show() AttributeError Traceback (most recent call last)<ipython-input-39-6e231c530d87> in <module>() 7 # plot the parallels to the separating hyperplane that pass through the 8 # support vectors----> 9 b = clf.support_vectors_[0] 1 yy_down = a * xx + (b[1] - a * b[0]) 11 b = clf.support_vectors_[-1]AttributeError: 'LinearSVC' object has no attribute 'support_vectors_'",Plot hyperplane Linear SVM python
Pandas: comparing two series for floating point near-equality?, I can compare two Pandas series for exact equality using pandas.Series.equals. Is there a corresponding function or parameter that will check if the elements are equal to some of precision? <code> ,Comparing two pandas series for floating point near-equality?
Finding common names between three data frames - pandas," Hope you could help me. I am new to python and pandas, so please bear with me. I am trying to find the common word between three data frames and I am using Jupiter Notebook.Just for example: There is only one column in all data frames that is A. I would like to find the common word among all columnsthe words that are unique to their own columns and not in common. Example:duck is unique to df1, snail is unique to df2 and monkey is unique to df3. I am using the below code to some use but not getting what I want straightforward, Kindly let me know where I am going wrong. Cheers <code>  df1=Adogcatcow ducksnakedf2=Apigsnailbirddogdf3=Aeagledog snailmonkey df1[df1['A'].isin(df2['A']) & (df2['A']) & (df3['A'])]",Finding common elements between multiple dataframe columns
How to make np.unique() faster for large arrays?," When running np.unique(), it first flattens the array, sorts the array, then finds the unique values. When I have arrays with shape (10, 3000, 3000), it takes about a second to find the uniques, but this quickly adds up as I need to call np.unique() multiple times. Since I only care about the total number of unique numbers in an array, sorting seems like a waste of time.Is there a faster method of find the total number of unique values in a large array other than np.unique()? <code> ",Efficiently counting number of unique elements - NumPy / Python
Import CSV file into Python," I have a line of code in a script that imports data from a text file with lots of spaces between values into an array for use later. I need to change this from a text file to a csv file. I don't want to just change this text to split on commas (since some values can have commas if they're in quotes). Luckily I saw there is a csv library I can import that can handle this. How can I load the csv file into the data array?If it makes a difference, this is how the data will be used: <code>  textfile = open('file.txt')data = []for line in textfile: row_data = line.strip(""\n"").split() for i, item in enumerate(row_data): try: row_data[i] = float(item) except ValueError: pass data.append(row_data) import csvwith open('file.csv', 'rb') as csvfile: ??? row = 0for row_data in (data): worksheet.write_row(row, 0, row_data) row += 1",How to import a csv-file into a data array?
How to control the source IP address of a zeromq packet on a machine with multiple IPs?," The Python standard library's socket.create_connection()method has a source address option, for controlling which source IP a connection uses. How do I do the same thing with a Python ZeroMQ socket, given a machine that has multiple addresses?In this case, I've been using Linux's iproute2 ip addr add to create the addresses and the ZeroMQ PUB/SUB socket-archetypes. <code> ",How to control the source IP address of a ZeroMQ packet on a machine with multiple IPs?
"tuple takes less space whereas list takes more space in memory, Why?"," A tuple takes less memory space in Python: whereas lists takes more memory space: What happens internally on the Python memory management?  <code>  >>> a = (1,2,3)>>> a.__sizeof__()48 >>> b = [1,2,3]>>> b.__sizeof__()64",Why do tuples take less space in memory than lists?
Groupby + Apply generates unwanted MultiIndex," Based off this question. For each unique Name, I would like to keep the row with the largest Year value. In the above example I would like to get the table I tried solving this question with groupby + (apply): Not the best approach, but I'm more interested in what is happening, and why. The result has a MultiIndex that looks like this: I'm not looking for a workaround. I'm actually more interested to know why this happens, and how I can prevent it without changing my approach. <code>  df = pandas.DataFrame([[2001, ""Jack"", 77], [2005, ""Jack"", 44], [2001, ""Jill"", 93]],columns=['Year','Name','Value']) Year Name Value0 2001 Jack 771 2005 Jack 442 2001 Jill 93 Year Name Value0 2005 Jack 441 2001 Jill 93 df.groupby('Name', as_index=False)\ .apply(lambda x: x.sort_values('Value').head(1)) Year Name Value0 0 2001 Jack 441 2 2001 Jill 93 MultiIndex(levels=[[0, 1], [0, 2]], labels=[[0, 1], [0, 1]])",How to remove the multiindex from GroupBy.apply()?
Keras: Fit Image augmentations to training data using flow_from_directory," I want to use Image augmentation in Keras. My current code looks like this: When I run a model with this, I get the following error: But I didn't find clear information about how to use train_dataget.fit() together with flow_from_directory. <code>  # define image augmentationstrain_datagen = ImageDataGenerator(featurewise_center=True,featurewise_std_normalization=True,zca_whitening=True)# generate image batches from directorytrain_datagen.flow_from_directory(train_dir) ""ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data.""",Fit Image augmentations to training data using flow_from_directory
Select columns in Pyspark Dataframe," I am looking for a way to select columns of my dataframe in PySpark. For the first row, I know I can use df.first(), but not sure about columns given that they do not have column names.I have 5 columns and want to loop through each one of them. <code>  +--+---+---+---+---+---+---+|_1| _2| _3| _4| _5| _6| _7|+--+---+---+---+---+---+---+|1 |0.0|0.0|0.0|1.0|0.0|0.0||2 |1.0|0.0|0.0|0.0|0.0|0.0||3 |0.0|0.0|1.0|0.0|0.0|0.0|",Select columns in PySpark dataframe
How to pass current_user.userID to WTForms Flask," I'm trying to pass userID variable to WTForms with FlaskForms. First I'll show code that works fine and then what I need to modify(that the part I don't know how). I'm adding new Name associated with some group. FlaskForm model: View model: Template: I see correct list in dropdown, and on submit gives me correct numbers.Task: I need to pass different list based on current_user.userID.I'm forming list using SQLAlchemy, by making query from table from DB, so My Flask view is: How can i pass my groups_list to the form? I tried to implement forming procedure in the FlaskForm model, but it doesn't see current_user objectDo I need to transform groupID to string and then back to int when I need to pass it to the SelectField like tuples? <code>  class AddName(FlaskForm): name =StringField('Device name', validators=[InputRequired(),Length(min=4, max=30)]) groupID = SelectField('Payload Type', choices=[(1,""Group1""),(2,""Group2"")], validators=[InputRequired]) @app.route('/dashboard/addname', methods=['GET', 'POST'])def addname(): form=AddName() if form.validate_on_submit(): name=Name(form.name.data,form.groupID.data) db.session.add(name) db.session.commit() return ""New name added"" <form method=""POST"" action=""/dashboard/addname""> <h2>Add name</h2> {{ form.hidden_tag() }} {{ wtf.form_field(form.name) }} {{ wtf.form_field(form.groupID) }} <button type=""submit"">Add name</button> </form> @app.route('/dashboard/addname', methods=['GET', 'POST'])def addname(): available_groups=db.session.query(Groups).filter(Groups.userID == currend_user.userID).all() #Now forming the list of tuples, so it's ok for SelectField groups_list=[(i.groupID, i.groupName) for i in available_groups] form=AddName() if form.validate_on_submit(): name=Name(form.name.data,form.groupID.data) db.session.add(name) db.session.commit() return ""New name added""",Dynamic choices WTForms Flask SelectField
open couchdb data JSON to CSV," I have downloaded Twitter data on local couchdb server.And it was saved as json files.I use this code to enter the database in python.1st import libraries next connect to server and choose the database I want to enter. I could create and delete databases with python however, I don't know how I can put the data from the server to jupyter notebook.I would like to get the text and time with retweets to analyze it.I can only see one JSON file from python.If possible I would like to add the all JSON data in the db to pandas dataframe in python so I can analyze it in R too.The question is: How to query the documents and load them into pandas dataframe?  <code>  import couchdbimport pandas as pdfrom couchdbkit import Serverimport jsonimport cloudant dbname = couchdb.Server('http://localhost:5984')db = dbname['Test']server = couchdb.Server('http://localhost:5984')",How to query the documents from couchdb and load them into pandas dataframe?
strange use of python 'and' operator," I'm trying to learn python and came across some code that is nice and short but doesn't totally make sensethe context was: I get what it's doing, but why does python do this - ie return the value rather than True/False? returns 5. Similarly, changing the and to or will result in a change in functionality. So Would return 10.Is this legit/reliable style, or are there any gotchas on this? <code>  def fn(*args): return len(args) and max(args)-min(args) 10 and 7-2 10 or 7 - 2","How do ""and"" and ""or"" act with non-boolean values?"
Strange use of python's 'and' operator," I'm trying to learn python and came across some code that is nice and short but doesn't totally make sensethe context was: I get what it's doing, but why does python do this - ie return the value rather than True/False? returns 5. Similarly, changing the and to or will result in a change in functionality. So Would return 10.Is this legit/reliable style, or are there any gotchas on this? <code>  def fn(*args): return len(args) and max(args)-min(args) 10 and 7-2 10 or 7 - 2","How do ""and"" and ""or"" act with non-boolean values?"
Strange use of python's and / or operator," I'm trying to learn python and came across some code that is nice and short but doesn't totally make sensethe context was: I get what it's doing, but why does python do this - ie return the value rather than True/False? returns 5. Similarly, changing the and to or will result in a change in functionality. So Would return 10.Is this legit/reliable style, or are there any gotchas on this? <code>  def fn(*args): return len(args) and max(args)-min(args) 10 and 7-2 10 or 7 - 2","How do ""and"" and ""or"" act with non-boolean values?"
"Strange use of ""and"" / ""or"" operator"," I'm trying to learn python and came across some code that is nice and short but doesn't totally make sensethe context was: I get what it's doing, but why does python do this - ie return the value rather than True/False? returns 5. Similarly, changing the and to or will result in a change in functionality. So Would return 10.Is this legit/reliable style, or are there any gotchas on this? <code>  def fn(*args): return len(args) and max(args)-min(args) 10 and 7-2 10 or 7 - 2","How do ""and"" and ""or"" act with non-boolean values?"
"Can you overload the Python3.6 f-string's ""operator""?"," In Python 3.6, you can use f-strings like: I want to overload the method receiving the '%A' above. Can it be done? For example, if I wanted to write a dumb wrapper around datetime, I might expect this overloading to look something like: <code>  >>> date = datetime.date(1991, 10, 12)>>> f'{date} was on a {date:%A}''1991-10-12 was on a Saturday' class MyDatetime: def __init__(self, my_datetime, some_other_value): self.dt = my_datetime self.some_other_value = some_other_value def __fstr__(self, format_str): return ( self.dt.strftime(format_str) + 'some other string' + str(self.some_other_value )","Can you overload the Python 3.6 f-string's ""operator""?"
How to send FIX logon message with Python to GDAX," I'm trying to establish a FIX 4.2 session to fix.gdax.com (docs: https://docs.gdax.com/#fix-api or https://docs.prime.coinbase.com/?python#logon-a) using Python 3.5 and stunnel. Everything is working apart from my logon message which is rejected and the session is closed by the server with no response making it difficult to debug what's going wrong. My Python code is as follows: The results of those two print statements are: There are a lot of variables here that could be wrong and the process of trial and error is getting a bit tedious. Can anyone see what is wrong with the logon message? <code>  s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)s.connect((""127.0.0.1"", 4197)) # address and port specified in stunnel config file# generate a signature according to the gdax protocol for signing a message:timestamp = str(time.time())message = [timestamp, ""A"", ""0"", ""f3e85389ffb809650c367d42b37e0a80"", ""Coinbase"", ""password-goes-here""] # these are the components of the pre-hash string as specified in the docs for a logon messagemessage = bytes(""|"".join(message), 'utf-8') # add the field separatorhmac_key = base64.b64decode(r""api-secret-goes-here"")signature = hmac.new(hmac_key, message, hashlib.sha256)sign_b64 = base64.b64encode(signature.digest()).decode()# in the above line the .decode() is not included when used to authenticate messages to the REST API and those are working successfully.#The reason I've included it here is to allow a string to be passed into the variable 'body' below:msgType = ""A""t = str(datetime.utcnow()).replace(""-"","""").replace("" "", ""-"")[:-3] # format the timestamp into YYYYMMDD-HH:MM:SS.sss as per the FIX standardbody = '34=1|52=%s|49=f3e85389ffb809650c367d42b37e0a80|56=Coinbase|98=0|108=30|554=password-goes-here|96=%s|8013=Y|' % (t, sign_b64)bodyLength = len(body.encode('utf-8')) # length of the message in bytesheader = '8=FIX.4.2|9=%s|35=%s|' % (bodyLength, msgType)msg = header + body# generate the checksum:def check_sum(s): sum = 0 for char in msg: sum += ord(char) sum = str(sum % 256) while len(sum) < 3: sum = '0' + sum return sumc_sum = check_sum(msg)logon = msg + ""10=%s"" % c_sum # append the check sum onto the messagelogon = logon.encode('ascii') # create a bytes object to send over the socketprint(logon)s.sendall(logon)print(s.recv(4096)) b'8=FIX.4.2|9=159|35=A|34=1|52=20171104-11:13:53.331|49=f3e85389ffb809650c367d42b37e0a80|56=Coinbase|98=0|108=30|554=password-goes-here|96=G7yeX8uQqsCEhAjWDWHoBiQz9lZuoE0Q8+bLJp4XnPY=|8013=Y|10=212'b''",How to send FIX logon message with Python to GDAX/Coinbase
Understand apyori's output," I'm well familiar with the apriori algorithm, and the meaning of support/confidence/lift.I'm currently using the apyori apriori implementation, and I'm not sure I understand the output of an apyori.apriori() call.It comes out like this What is the rule? There are multiple support/confidence/lift, what each one denotes?I'd appreciate a dictionary style explanation of each part of the output <code>  > RelationRecord(items=frozenset({'item1', 'item2'}),> support=0.15365410803449842,> ordered_statistics=[OrderedStatistic(items_base=frozenset({'item1'}),> items_add=frozenset({'item2'}), confidence=0.6203420891875382,> lift=2.2233410344037092),> OrderedStatistic(items_base=frozenset({'item2'}),> items_add=frozenset({'item1'}), confidence=0.5507049891540131,> lift=2.2233410344037097)])",Understanding apyori's output
Forcing Python dict keys to be used as strings when interpolating, I have the following string interpolation: It obviously fails because format is literally trying to access the object test1 and its attribute 1. Is there a way to format this string and force the key values to be taken as strings? (Looking for a Python 2 and 3 solution.) <code>  >>> a = {'test1.1': 5}>>> 'test: {test1.1}'.format(**a)KeyError: 'test1',Forcing dict keys to be used as argument specifiers with str.format
Matplotlib path.contains_points returns false for point on edge," I'm attempting to use Matplotlib to find all points contained within a polygonal path, but it seems to be missing a few. More specifically, my path is a rectangle, and the points are on an underlying uniform grid. In the following test script it will not consider the points laying on the top line of the polygon as being part of the polygon, but will consider the points on the rest of the edges.Code: As is, the code above will return I would expect row 5 of the result to contain True values like the ones following it. If I change the coordinates in the polygon from 5 to 4.9, then I do get the result I expect.I'm assuming this has something to do with misusing or misunderstanding the function, but I'm not quite sure what or how that might be.EDIT: It was brought up that contains_points should be returning False for points that fall on the edges of the polygon. In my example, we see this behaviour for the top edge [5,5]-[10,5], but not for the other edges (i.e. [5,5]-[5,10], [5,10]-[10,10], and [10,10]-[10,5]). These three other edges correspond to the first and last columns with True values and the last row containing True values in the example output above. It is this apparent inconsistency that's the problem. <code>  import matplotlib.path as mpltPathpolygon = [(5,5),(10,5),(10,10),(5,10)]width =11height =11points = [[0,0],[1,0],[2,0],[3,0],[4,0],[5,0],[6,0],[7,0],[8,0],[9,0],[10,0],[11,0], \ [0,1],[1,1],[2,1],[3,1],[4,1],[5,1],[6,1],[7,1],[8,1],[9,1],[10,1],[11,1],\ [0,2],[1,2],[2,2],[3,2],[4,2],[5,2],[6,2],[7,2],[8,2],[9,2],[10,2],[11,2],\ [0,3],[1,3],[2,3],[3,3],[4,3],[5,3],[6,3],[7,3],[8,3],[9,3],[10,3],[11,3],\ [0,4],[1,4],[2,4],[3,4],[4,4],[5,4],[6,4],[7,4],[8,4],[9,4],[10,4],[11,4],\ [0,5],[1,5],[2,5],[3,5],[4,5],[5,5],[6,5],[7,5],[8,5],[9,5],[10,5],[11,5],\ [0,6],[1,6],[2,6],[3,6],[4,6],[5,6],[6,6],[7,6],[8,6],[9,6],[10,6],[11,6],\ [0,7],[1,7],[2,7],[3,7],[4,7],[5,7],[6,7],[7,7],[8,7],[9,7],[10,7],[11,7],\ [0,8],[1,8],[2,8],[3,8],[4,8],[5,8],[6,8],[7,8],[8,8],[9,8],[10,8],[11,8],\ [0,9],[1,9],[2,9],[3,9],[4,9],[5,9],[6,9],[7,9],[8,9],[9,9],[10,9],[11,9],\ [0,10],[1,10],[2,10],[3,10],[4,10],[5,10],[6,10],[7,10],[8,10],[9,10],[10,10],[11,10],\ [0,11],[1,11],[2,11],[3,11],[4,11],[5,11],[6,11],[7,11],[8,11],[9,11],[10,11],[11,11]]path = mpltPath.Path(polygon)inside = path.contains_points(points)print(inside) [False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False False True True True True True True False False False False False False True True True True True True False False False False False False True True True True True True False False False False False False True True True True True True False False False False False False True True True True True True False False False False False False False False False False False False False]",Matplotlib path.contains_points returns false for points on some edges but not others
pyodbc: how to get the ID when Inserting a row of values into a table with an IDENTITY_COLUMN," I am trying to get the ID of a newly inserted row by using OUTPUT. However, I encountered the HY010 error. The following query/code is what I use: the last line id = cursor.fetchone()[0] led to a HY010 error (see below). Any advice would be greatly appreciated! <code>  string = """""" SET NOCOUNT ON; DECLARE @NEWID TABLE(ID INT); INSERT INTO dbo.t1 (Username, Age) OUTPUT inserted.id INTO @NEWID(ID) VALUES(?, ?) SELECT ID FROM @NEWID """"""cursor.execute(string, ""John Doe"", 35)cursor.commit()id = cursor.fetchone()[0] pyodbc.Error: ('HY010', '[HY010] [Microsoft][ODBC SQL Server Driver]Function sequence error (0) (SQLFetch)')",How to get the IDENTITY value when using INSERT ... OUTPUT with pyodbc
Removing sublists from a set of lists in python," I'm trying to find the fastest way to solve this problem, say I have a list of lists: I'd like to be able to remove all the lists that are sublists of one of the other lists, for example I'd like the following output: Where the lists [2,3] and [1,2,3] were removed because they are completely contained in one of the other lists, while [3,7] was not removed because no single list contained all those elements.I'm not restricted to any one data structure, if a list of lists or a set is easier to work with, that would be fine too.The best I could come up with was something like this but it doesn't really work because I'm trying to remove from a list while iterating over it. I tried to copy it into a new list but somehow I couldn't get it working right. Thanks. <code>  myList = [[1,2,3,4,5],[2,3],[4,5,6,7],[1,2,3],[3,7]] myList = [[1,2,3,4,5],[4,5,6,7],[3,7]] for outter in range(0,len(myList)):outterSet = set(myList[outter])for inner in range(outter,len(myList)): innerSet = set(myList[inner]) if innerSet.issubset(outterSet): myList.remove(innerSet)",Removing sublists from a list of lists
Removing sublists from a set of lists," I'm trying to find the fastest way to solve this problem, say I have a list of lists: I'd like to be able to remove all the lists that are sublists of one of the other lists, for example I'd like the following output: Where the lists [2,3] and [1,2,3] were removed because they are completely contained in one of the other lists, while [3,7] was not removed because no single list contained all those elements.I'm not restricted to any one data structure, if a list of lists or a set is easier to work with, that would be fine too.The best I could come up with was something like this but it doesn't really work because I'm trying to remove from a list while iterating over it. I tried to copy it into a new list but somehow I couldn't get it working right. Thanks. <code>  myList = [[1,2,3,4,5],[2,3],[4,5,6,7],[1,2,3],[3,7]] myList = [[1,2,3,4,5],[4,5,6,7],[3,7]] for outter in range(0,len(myList)):outterSet = set(myList[outter])for inner in range(outter,len(myList)): innerSet = set(myList[inner]) if innerSet.issubset(outterSet): myList.remove(innerSet)",Removing sublists from a list of lists
What is the difference between lxml and ElementTree?," When it comes to generating XML data in Python, there are two libraries I often see recommended: lxml and ElementTreeFrom what I can tell, the two libraries are very similar to each other. They both seem to have similar module names, usage guidelines, and functionality. Even the import statements are fairly similar. What are the differences between the lxml and ElementTree libraries for Python? <code>  # Importing lxml and ElementTreeimport lxml.etreeimport xml.etree.ElementTree",What are the differences between lxml and ElementTree?
"How do you count the most frequent letter in a string, Using python?"," I need to implement findMostFrequenctChar. The only hint she gave us was that we needed to make 2 lists. and this is where she lost me.Here's the code that calls the function: <code>  class MyString: def __init__(self, myString): self.__myString = myString def countWord(self): count = len(self.__myString.split()) return count def findMostFrequentChar(self): # ? def main(): aString = MyString(""This is a super long long long string. Please help count me"") print(""There are"", aString.countWord(), ""words in the string."") count, letter = aString.findMostFrequentChar() print(""The most frequent character is"", letter, ""which appeared"", count, ""times"")main()",How to count the most frequent letter in a string?
Python: Understanding dictionnary view objects," I've been trying to understand built-in view objects return by .items(), .values(), .keys() in Python 3 or similarly by .viewitems(), .viewvalues(), .viewkeys(). There are other threads on that subject but none (even the doc) seems to described how they work internally. The main gain here seems to be efficienty compared to the copy of type list returned in Python 2. There are often compared to a window to the dictionnary items (like in this thread).But what is that window and why is it more efficient ?The only thing I can see is that the view objects seems to be set-like objects, which are generally faster for membership testing. But is this the only factor ?Code sample So, my question is regarding this dict_items class. How does that work internally? <code>  >>> example_dict = {'test':'test'}>>> example_dict.items()dict_items([('test', 'test')])>>> type(example_dict.items())<class 'dict_items'>",Python: Understanding dictionary view objects
Parsing Mathematical Programming System files," In order not to reinvent the wheel I tried to find some code to parse Mathematical Programming System files but I didnt find any implementations in python.Is there any code allready available for this? UpdateReading Mathematical Prog. filesExample MPS (afiro.mps: link1, link2)Contains: objective function, one row, n columns table with restrictions, m rows, n columns right table, one column, m rowsMany languages have packages for reading and writing these files. <code> ",Loading/Parsing Mathematical Programming System files
Expected tensorFlow model size from learned variables," When training convolutional neural networks for image classification tasks we generally want our algorithm to learn the filters (and biases) that transform a given image to its correct label. I have a few models I'm trying to compare in terms of model size, number of operations, accuracy, etc. However, the size of the model outputed from tensorflow, concretely the model.ckpt.data file that stores the values of all the variables in the graph, is not the one I expected. In fact, it seems to be three times bigger. To go straight to the problem I'm gonna base my question on this Jupyter notebook. Below is the section where the variables (weights and biases) are defined: I've added a couple of lines in order to save the model at the end of the training process: Adding up all those variables we would expect to get a model.ckpt.data file of size 12.45Mb (I've obtained this by just computing the number of float elements that our model learns and then convert that value to MegaBytes). But! the .data file saved is 39.3Mb. Why is this?I've followed the same approach with a more complex network (a variation of ResNet) and my expected model.data size is also ~3x smaller than what the actual .data file is.The data type of all these variables is float32. <code>  # Store layers weight & biasweights = {# 5x5 conv, 1 input, 32 outputs'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32]),dtype=tf.float32),# 5x5 conv, 32 inputs, 64 outputs'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64]),dtype=tf.float32),# fully connected, 7*7*64 inputs, 1024 outputs'wd1': tf.Variable(tf.random_normal([7*7*64, 1024]),dtype=tf.float32),# 1024 inputs, 10 outputs (class prediction)'out': tf.Variable(tf.random_normal([1024, num_classes]),dtype=tf.float32)}biases = {'bc1': tf.Variable(tf.random_normal([32]),dtype=tf.float32),'bc2': tf.Variable(tf.random_normal([64]),dtype=tf.float32),'bd1': tf.Variable(tf.random_normal([1024]),dtype=tf.float32),'out': tf.Variable(tf.random_normal([num_classes]),dtype=tf.float32)} # Save the modelsave_path = saver.save(sess, logdir+""model.ckpt"")print(""Model saved in file: %s"" % save_path)",Expected tensorflow model size from learned variables
How to apply a magic methods 'mixin' to a typing.NamedTuple," I love the typing.NamedTuple in Python 3.6. But there's often the case where the namedtuple contains a non-hashable attribute and I want to use it as a dict key or set member. If it makes sense that a namedtuple class uses object identity (id() for __eq__ and __hash__) then adding those methods to the class works fine.However, I now have this pattern in my code in several places and I want to get rid of the boilerplate __eq__ and __hash__ method definitions. I know namedtuple's are not regular classes and I haven't been able to figure out how to get this working.Here's what I've tried: Is there a way I don't have to repeat these methods in each NamedTuple that I need 'object identity' in? <code>  from typing import NamedTupleclass ObjectIdentityMixin: def __eq__(self, other): return self is other def __hash__(self): return id(self)class TestMixinFirst(ObjectIdentityMixin, NamedTuple): a: intprint(TestMixinFirst(1) == TestMixinFirst(1)) # Prints True, so not using my __eq__class TestMixinSecond(NamedTuple, ObjectIdentityMixin): b: intprint(TestMixinSecond(2) == TestMixinSecond(2)) # Prints True as wellclass ObjectIdentityNamedTuple(NamedTuple): def __eq__(self, other): return self is other def __hash__(self): return id(self)class TestSuperclass(ObjectIdentityNamedTuple): c: intTestSuperclass(3) """"""Traceback (most recent call last): File ""test.py"", line 30, in <module> TestSuperclass(3)TypeError: __new__() takes 1 positional argument but 2 were given""""""",How to apply a special methods 'Mixin' to a typing.NamedTuple
SyntaxError: 'yield from' inside async function," In Python 3.6, I am able to use yield inside a coroutine. However I am not able to use yield from. Below is my code. On line 3 I await another coroutine. On line 4 I try to yield from a file. Why won't Python 3.6 allow me to do that? Here's the exception Python 3.6 raises for the above code: <code>  async def read_file(self, filename): with tempfile.NamedTemporaryFile(mode='r', delete=True, dir='/tmp', prefix='sftp') as tmp_file: await self.copy_file(filename, tmp_file) yield from open(tmp_file) File ""example.py"", line 4 yield from open(tmp_file) ^SyntaxError: 'yield from' inside async function",Why can't I 'yield from' inside an async function?
"How can I write data processing code using Pandas that gives me at least as high performance as handwritten, standard Python code?"," I have a machine learning application written in Python which includes a data processing step. When I wrote it, I initially did the data processing on Pandas DataFrames, but when this lead to abysmal performance I eventually rewrote it using vanilla Python, with for loops instead of vectorized operations and lists and dicts instead of DataFrames and Series. To my surprise, the performance of the code written in vanilla Python ended up being far higher than that of the code written using Pandas.As my handcoded data processing code is substantially bigger and messier than the original Pandas code, I haven't quite given up on using Pandas, and I'm currently trying to optimize the Pandas code without much success.The core of the data processing step consists of the following: I first divide the rows into several groups, as the data consists of several thousand time series (one for each ""individual""), and I then do the same data processing on each group: a lot of summarization, combining different columns into new ones, etc.I profiled my code using Jupyter Notebook's lprun, and the bulk of the time is spent on the following and other similar lines: ...a mix of vectorized and non-vectorized processing. I understand that the non-vectorized operations won't be faster than my handwritten for loops, since that's basically what they are under the hood, but how can they be so much slower? We're talking about a performance degradation of 10-20x between my handwritten code and the Pandas code.Am I doing something very, very wrong? <code>  grouped_data = data.groupby('pk')data[[v + 'Diff' for v in val_cols]] = grouped_data[val_cols].transform(lambda x: x - x.shift(1)).fillna(0)data[[v + 'Mean' for v in val_cols]] = grouped_data[val_cols].rolling(4).mean().shift(1).reset_index()[val_cols](...)",Improving the performance of pandas groupby
If it's possible to speed up a loop in python?," The normal way to map a function in a numpy.narray like np.array[map(some_func,x)] or vectorize(f)(x) can't provide an index.The following code is just a simple example that is commonly seen in many applications. Is there a way to speed it up?Thank you for your help! The quickest way to speed up this code is this, using the function that @user2357112 commented about: @Julien's method is also good if feature_mat is small, but when the feature_mat is 1000 by 2000, then it needs nearly 40 GB of memory. <code>  dis_mat = np.zeros([feature_mat.shape[0], feature_mat.shape[0]])for i in range(feature_mat.shape[0]): for j in range(i, feature_mat.shape[0]): dis_mat[i, j] = np.linalg.norm( feature_mat[i, :] - feature_mat[j, :] ) dis_mat[j, i] = dis_mat[i, j] from scipy.spatial.distance import pdist,squareform dis_mat = squareform(pdist(feature_mat))",Is it possible to speed up this loop in Python?
Is it possible to speed up a loop in python?," The normal way to map a function in a numpy.narray like np.array[map(some_func,x)] or vectorize(f)(x) can't provide an index.The following code is just a simple example that is commonly seen in many applications. Is there a way to speed it up?Thank you for your help! The quickest way to speed up this code is this, using the function that @user2357112 commented about: @Julien's method is also good if feature_mat is small, but when the feature_mat is 1000 by 2000, then it needs nearly 40 GB of memory. <code>  dis_mat = np.zeros([feature_mat.shape[0], feature_mat.shape[0]])for i in range(feature_mat.shape[0]): for j in range(i, feature_mat.shape[0]): dis_mat[i, j] = np.linalg.norm( feature_mat[i, :] - feature_mat[j, :] ) dis_mat[j, i] = dis_mat[i, j] from scipy.spatial.distance import pdist,squareform dis_mat = squareform(pdist(feature_mat))",Is it possible to speed up this loop in Python?
Can't load modules in sklearn," I suddenly can't load newly upgraded modules modules, e.g scikit-learn, zope, but I can find other packages. Even though the path links from the import points to the correct anaconda folder, which contains all the code. Any ideas what might be wrong and how to fix it? <code>  Python 2.7.13 |Anaconda custom (64-bit)| (default, Dec 20 2016, 23:09:15) [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2>>> import sklearn>>> from os import listdir>>> print(dir(sklearn))['_ASSUME_FINITE', '__SKLEARN_SETUP__', '__all__', '__builtins__', '__check_build', '__doc__', '__file__', '__name__', '__package__', '__path__', '__version__', '_contextmanager', 'base', 'clone', 'config_context', 'exceptions', 'externals', 'get_config', 'logger', 'logging', 'os', 're', 'set_config', 'setup_module', 'sys', 'utils', 'warnings']>>> print(listdir(sklearn.__path__[0]))['exceptions.py', 'cross_validation.pyc', 'lda.py', 'naive_bayes.pyc', 'isotonic.py', '_build_utils', 'neighbors', 'cluster', 'naive_bayes.py', '__init__.pyc', 'multiclass.py', 'dummy.pyc', 'grid_search.pyc', 'tests', '__init__.py', 'calibration.py', '_isotonic.so', 'neural_network', 'datasets', 'preprocessing', '__check_build', 'random_projection.py', 'multiclass.pyc', 'model_selection', 'calibration.pyc', 'pipeline.pyc', 'qda.py', 'learning_curve.py', 'ensemble', 'tree', 'isotonic.pyc', 'kernel_ridge.py', 'gaussian_process', 'decomposition', 'base.pyc', 'dummy.py', 'utils', 'pipeline.py', 'cross_decomposition', 'covariance', 'qda.pyc', 'multioutput.pyc', 'lda.pyc', 'feature_selection', 'linear_model', 'metrics', 'kernel_ridge.pyc', 'setup.py', 'semi_supervised', 'exceptions.pyc', 'multioutput.py', 'cross_validation.py', 'discriminant_analysis.py', 'kernel_approximation.pyc', 'base.py', 'random_projection.pyc', 'setup.pyc', 'kernel_approximation.py', 'grid_search.py', 'discriminant_analysis.pyc', 'mixture', 'manifold', 'externals', 'svm', 'feature_extraction', 'learning_curve.pyc']>>> import zope>>> print(dir(zope))['__doc__', '__name__', '__path__']>>> print(listdir(zope.__path__[0]))['interface']>>> zope.interfaceTraceback (most recent call last): File ""<stdin>"", line 1, in <module>AttributeError: 'module' object has no attribute 'interface'>>> sklearn.ldaTraceback (most recent call last): File ""<stdin>"", line 1, in <module>AttributeError: 'module' object has no attribute 'lda'",Suddenly I can't load some newly upgraded modules in Python
Address sanitising Boost.Python modules," My project includes a large C++ library and Python bindings (via Boost.Python). The test suite is mostly written on top of the Python bindings, and I would like to run it with sanitizers, starting with ASAN.I'm running macOS (10.13.1 FWIW, but I had the problem with previous versions too), and I can't seem to find a way to run ASAN on Python modules (I very much doubt this is related to Boost.Python, I suppose it's the same with other techniques).Here is a simple Python module: here is the Makefile I used, made for MacPorts: Without asan, everything works well (well, too well actually...). But with ASAN, I hit LD_PRELOAD like issues: Okay, let's do that: define DYLD_INSERT_LIBRARIES Let's be suspicious about SIP, so I have disabled SIP here, and let's resolve the symlinks: What's the right way to do that? I have also tried to load libasan with ctypes.PyDLL, and even with sys.setdlopenflags(os.RTLD_NOW | os.RTLD_GLOBAL) I can't get this to work. <code>  // hello_ext.cc#include <boost/python.hpp>char const* greet(){ auto* res = new char[100]; std::strcpy(res, ""Hello, world!""); delete [] res; return res;}BOOST_PYTHON_MODULE(hello_ext){ using namespace boost::python; def(""greet"", greet);} // MakefileCXX = clang++-mp-4.0CXXFLAGS = -g -std=c++14 -fsanitize=address -fno-omit-frame-pointerCPPFLAGS = -isystem/opt/local/include $$($(PYTHON_CONFIG) --includes)LDFLAGS = -L/opt/local/libPYTHON = python3.5PYTHON_CONFIG = python3.5-configLIBS = -lboost_python3-mt $$($(PYTHON_CONFIG) --ldflags)all: hello_ext.sohello_ext.so: hello_ext.cc $(CXX) $(CPPFLAGS) $(CXXFLAGS) $(LDFLAGS) -shared -o $@ $< $(LIBS)check: all $(ENV) $(PYTHON) -c 'import hello_ext; print(hello_ext.greet())'clean: -rm -f hello_ext.so $ make checkpython -c 'import hello_ext; print(hello_ext.greet())'==19013==ERROR: Interceptors are not working. This may be because AddressSanitizer is loaded too late (e.g. via dlopen). Please launch the executable with:DYLD_INSERT_LIBRARIES=/opt/local/libexec/llvm-4.0/lib/clang/4.0.1/lib/darwin/libclang_rt.asan_osx_dynamic.dylib""interceptors not installed"" && 0make: *** [check] Abort trap: 6 $ DYLD_INSERT_LIBRARIES=/opt/local/libexec/llvm-4.0/lib/clang/4.0.1/lib/darwin/libclang_rt.asan_osx_dynamic.dylib \ python -c 'import hello_ext; print(hello_ext.greet())'==19023==ERROR: Interceptors are not working. This may be because AddressSanitizer is loaded too late (e.g. via dlopen). Please launch the executable with:DYLD_INSERT_LIBRARIES=/opt/local/libexec/llvm-4.0/lib/clang/4.0.1/lib/darwin/libclang_rt.asan_osx_dynamic.dylib""interceptors not installed"" && 0zsh: abort DYLD_INSERT_LIBRARIES= python -c 'import hello_ext; print(hello_ext.greet())' $ DYLD_INSERT_LIBRARIES=/opt/local/libexec/llvm-4.0/lib/clang/4.0.1/lib/darwin/libclang_rt.asan_osx_dynamic.dylib \ /opt/local/Library/Frameworks/Python.framework/Versions/3.5/bin/python3.5 -c 'import hello_ext; print(hello_ext.greet())'==19026==ERROR: Interceptors are not working. This may be because AddressSanitizer is loaded too late (e.g. via dlopen). Please launch the executable with:DYLD_INSERT_LIBRARIES=/opt/local/libexec/llvm-4.0/lib/clang/4.0.1/lib/darwin/libclang_rt.asan_osx_dynamic.dylib""interceptors not installed"" && 0zsh: abort DYLD_INSERT_LIBRARIES= -c 'import hello_ext; print(hello_ext.greet())'",Address sanitizing Boost.Python modules
Add a vertical legend to matplotlib colormap," This code enables me to plot a colormap of a ""3d"" array [X,Y,Z] (they are 3 simple np.array of elements). But I can't succeed in adding a vertical written label at the right of the colorbar legend. It's anoying to not get an easy answer from google... can someone help me ? <code>  import numpy as npimport matplotlib.pyplot as pltfig = plt.figure(""Color MAP 2D+"")contour = plt.tricontourf(X, Y, Z, 100, cmap=""bwr"")plt.xlabel(""X"")plt.ylabel(""Y"")plt.title(""Color MAP 2D+"")#Legenddef fmt(x, pos): a, b = '{:.2e}'.format(x).split('e') b = int(b) return r'${} \times 10^{{{}}}$'.format(a, b)import matplotlib.ticker as tickerplt.colorbar(contour, format=ticker.FuncFormatter(fmt))plt.show()",Add a vertical label to matplotlib colormap legend
"Python: Matplotlib Animation FFmpeg, RuntimeError: No MovieWriters available"," The problem I am getting is in code similar to this example: https://matplotlib.org/examples/animation/basic_example_writer.htmlThe error:RuntimeError: No MovieWriters available occurs at Writer = animation.writers['ffmpeg'] in the example above.I am using mac, I have installed ffmpeg using brew, and even installed it with conda even though I am not using anaconda for this particular code.I am positive that it is installed - I have used it in terminal to change files but it is not working within the program.Thanks! <code> ",RuntimeError: No MovieWriters available in Matplotlib animation
Matplotlib animation with FFmpeg: RuntimeError: No MovieWriters available," The problem I am getting is in code similar to this example: https://matplotlib.org/examples/animation/basic_example_writer.htmlThe error:RuntimeError: No MovieWriters available occurs at Writer = animation.writers['ffmpeg'] in the example above.I am using mac, I have installed ffmpeg using brew, and even installed it with conda even though I am not using anaconda for this particular code.I am positive that it is installed - I have used it in terminal to change files but it is not working within the program.Thanks! <code> ",RuntimeError: No MovieWriters available in Matplotlib animation
How to read bytes object from csv python 3," I have used tweepy to store the text of tweets in a csv file using Python csv.writer(), but I had to encode the text in utf-8 before storing, otherwise tweepy throws a weird error.Now, the text data is stored like this: I tried to decode this using this code (there is more data in other columns, text is in 3rd column): But, it doesn't decode the text. I cannot use .decode('utf-8') as the csv reader reads data as strings i.e. type(row[3]) is 'str' and I can't seem to convert it into bytes, the data gets encoded once more!How can I decode the text data? Edit: Here's a sample line from the csv file: Note: If the solution is in the encoding process, please note that I cannot afford to download the entire data again. <code>  ""b'Lorem Ipsum\xc2\xa0Assignment '"" with open('data.csv','rt',encoding='utf-8') as f: reader = csv.reader(f,delimiter=',') for row in reader: print(row[3]) 67783591545656656999,3415844,1450443669.0,b'Virginia School District Closes After Backlash Over Arabic Assignment: The Augusta County school district in\xe2\x80\xa6 | @abcde',52,18",How to read bytes object from csv?
"""Unable to find a matching set of capabilities"" with Selenium and Python"," I must have some versions here that don't match up since I can't get Selenium with Python to fire up a Firefox web browser. I'm using an older version of Firefox because other people in here have the same old version of Python and for them the old version of Firefox works best.Code: Error: Version info:Python 2.7.10 Selenium 3.8.0 Firefox 46.0 GeckoDriver 0.19.1 (It's in a folder which is in my PATH environment variable)MacOS 10.12.6 <code>  from selenium import webdriverfrom selenium import commonfrom selenium.webdriver import ActionChainsfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.common.exceptions import TimeoutExceptionfrom selenium.common.exceptions import NoSuchElementExceptionfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilitiesdriver=webdriver.Firefox(capabilities=DesiredCapabilities.FIREFOX) Traceback (most recent call last): File ""scrapeCommunitySelenium.py"", line 13, in <module> driver=webdriver.Firefox(capabilities=DesiredCapabilities.FIREFOX) File ""/Library/Python/2.7/site-packages/selenium/webdriver/firefox/webdriver.py"", line 158, in __init__ keep_alive=True) File ""/Library/Python/2.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 154, in __init__ self.start_session(desired_capabilities, browser_profile) File ""/Library/Python/2.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 243, in start_session response = self.execute(Command.NEW_SESSION, parameters) File ""/Library/Python/2.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 311, in execute self.error_handler.check_response(response) File ""/Library/Python/2.7/site-packages/selenium/webdriver/remote/errorhandler.py"", line 237, in check_response raise exception_class(message, screen, stacktrace)selenium.common.exceptions.SessionNotCreatedException: Message: Unable to find a matching set of capabilities",selenium.common.exceptions.SessionNotCreatedException: Message: Unable to find a matching set of capabilities with Firefox 46 through Selenium
"""SessionNotCreatedException: Message: Unable to find a matching set of capabilities"" with Firefox 46.0 through Selenium and Python"," I must have some versions here that don't match up since I can't get Selenium with Python to fire up a Firefox web browser. I'm using an older version of Firefox because other people in here have the same old version of Python and for them the old version of Firefox works best.Code: Error: Version info:Python 2.7.10 Selenium 3.8.0 Firefox 46.0 GeckoDriver 0.19.1 (It's in a folder which is in my PATH environment variable)MacOS 10.12.6 <code>  from selenium import webdriverfrom selenium import commonfrom selenium.webdriver import ActionChainsfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.common.exceptions import TimeoutExceptionfrom selenium.common.exceptions import NoSuchElementExceptionfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilitiesdriver=webdriver.Firefox(capabilities=DesiredCapabilities.FIREFOX) Traceback (most recent call last): File ""scrapeCommunitySelenium.py"", line 13, in <module> driver=webdriver.Firefox(capabilities=DesiredCapabilities.FIREFOX) File ""/Library/Python/2.7/site-packages/selenium/webdriver/firefox/webdriver.py"", line 158, in __init__ keep_alive=True) File ""/Library/Python/2.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 154, in __init__ self.start_session(desired_capabilities, browser_profile) File ""/Library/Python/2.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 243, in start_session response = self.execute(Command.NEW_SESSION, parameters) File ""/Library/Python/2.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 311, in execute self.error_handler.check_response(response) File ""/Library/Python/2.7/site-packages/selenium/webdriver/remote/errorhandler.py"", line 237, in check_response raise exception_class(message, screen, stacktrace)selenium.common.exceptions.SessionNotCreatedException: Message: Unable to find a matching set of capabilities",selenium.common.exceptions.SessionNotCreatedException: Message: Unable to find a matching set of capabilities with Firefox 46 through Selenium
Python access to project root direcotry," Below folder structure of my application: my code inside of the subfolder3. But I want to write output of the code to subfolder1. I would like to know how can I do this wihout importing full path to directory. <code>  rootfolder /subfolder1/ /subfolder2 /subfolder3/test.py script_dir = os.path.dirname(__file__)full_path = os.path.join(script_dir,'/subfolder1/')",Python access to project root directory
what is dispatch used for in django?," I have been trying to wrap my head around the dispatch method, particularly in Django. However, I cannot seem to figure out exactly what it does. I tried to gain an understanding from the Django docs but didn't find them to informative on this topic. Per my understanding it is a listener that listens to all events happening on a page but I am not sure if this is the case? <code>  class OrderDetail(DetailView): model = Order def **dispatch**(self, request, *args, **kwargs): try: user_check_id = self.request.session.get(""user_checkout_id"") user_checkout = UserCheckout.objects.get(id=user_check_id) except UserCheckout.DoesNotExist: user_checkout = UserCheckout.objects.get(user=request.user) except: user_checkout = None obj = self.get_object() if obj.user == user_checkout and user_checkout is not None: return super(OrderDetail, self).dispatch(request, *args, **kwargs) else: raise Http404",What is dispatch used for in django?
Numpy equivalent of Matlab's magic()," In Ocatave / Matlab, I can use magic() to get a magic square, e.g., Definition: A magic square is an NN grid of numbers in which the entries in each row, column and main diagonal sum to the same number (equal to N(N^2+1)/2).How can I generate the same using NumPy? <code>  magic(4) 16 2 3 13 5 11 10 8 9 7 6 12 4 14 15 1",NumPy equivalent of Matlab's magic()
mongodb group average array," I'm trying to do PyMongo aggregate - $group averages of arrays, and I cannot find any examples that matches my problem.Data example Wanted results I have tried this approach but MongoDB is interpreting the arrays as Null? I've looked through the MongoDB documentation and I cannot find or understand if there is any way to do this? <code>  { Subject: ""Dave"", Strength: [1,2,3,4]},{ Subject: ""Dave"", Strength: [1,2,3,5]},{ Subject: ""Dave"", Strength: [1,2,3,6]},{ Subject: ""Stuart"", Strength: [4,5,6,7]},{ Subject: ""Stuart"", Strength: [6,5,6,7]},{ Subject: ""Kevin"", Strength: [1,2,3,4]},{ Subject: ""Kevin"", Strength: [9,4,3,4]} { Subject: ""Dave"", mean_strength = [1,2,3,5]},{ Subject: ""Stuart"", mean_strength = [5,5,6,7]},{ Subject: ""Kevin"", mean_strength = [5,3,3,4]} pipe = [{'$group': {'_id': 'Subject', 'mean_strength': {'$avg': '$Strength'}}}]results = db.Walk.aggregate(pipeline=pipe)Out: [{'_id': 'SubjectID', 'total': None}]",Mongodb group average array
How to display all images in a director flask," I am trying to display all images from a particular directory in static (static/plots). Here is my python: and my html: And when the template successfully renders, the templates are not fetched. Opening the image in a new tab yields: http://127.0.0.1:8080/static/%7B%7Bhist%7D%7DI imagine the problem is with this line, but I can't figure out what's correct: <img src=""{{url_for('static', filename='{{hist}}')}}"" alt=""{{hist}}""> <code>  hists = os.listdir('static/plots')hists = ['plots/' + file for file in hists]return render_template('report.html', hists = hists) <!DOCTYPE=html><html><head> <title> Store Report </title></head><body> {{first_event}} to {{second_event}} {% for hist in hists %} <img src=""{{url_for('static', filename='{{hist}}')}}"" alt=""{{hist}}""> {% endfor %}</body></html>`",How to display all images in a directory with flask
Pandas slicing warning with 0.21.0," I'm trying to select a subset of a subset of a dataframe, selecting only some columns, and filtering on the rows. However, I'm getting the error: What 's the correct way to slice and filter now? <code>  df.loc[df.a.isin(['Apple', 'Pear', 'Mango']), ['a', 'b', 'f', 'g']] Passing list-likes to .loc or [] with any missing label will raiseKeyError in the future, you can use .reindex() as an alternative.",Pandas slicing FutureWarning with 0.21.0
PyQt QGridLayout diffrent column width," I am trying to create a layout looking like this: Basically, I want cell number 1 the first row to be thinner that cell 2, but cells number 3 and 4 on the second row should have equal widths.Is it even possible to create a layout like this using QGridLayout in PyQt4?  <code>  _________| | ||1 | 2 ||__|______|| 3 | 4 ||____|____|",QGridLayout different column width
PyQt QGridLayout different column width," I am trying to create a layout looking like this: Basically, I want cell number 1 the first row to be thinner that cell 2, but cells number 3 and 4 on the second row should have equal widths.Is it even possible to create a layout like this using QGridLayout in PyQt4?  <code>  _________| | ||1 | 2 ||__|______|| 3 | 4 ||____|____|",QGridLayout different column width
How to poperly sample truncated distirbutions," I am trying to learn how to sample truncated distributions. To begin with I decided to try a simple example I found here exampleI didn't really understand the division by the CDF, therefore I decided to tweak the algorithm a bit. Being sampled is an exponential distribution for values x>0 Here is an example python code: Ant the output is:The code above seems to work fine only for when using the condition if a > 0. :, i.e. positive x, choosing another condition (e.g. if a > 0.5 :) produces wrong results. Since my final goal was to sample a 2D-Gaussian - pdf on a truncated interval I tried extending the simple example using the exponential distribution (see the code below). Unfortunately, since the simple case didn't work, I assume that the code given below would yield wrong results.I assume that all this can be done using the advanced tools of python. However, since my primary idea was to understand the principle behind, I would greatly appreciate your help to understand my mistake.Thank you for your help. EDIT: EDIT:I changed the code by norming the analytic pdf according to this scheme, and according to the answers given by, @Crazy Ivan and @Leandro Caniglia , for the case where the bottom of the pdf is removed. That is dividing by (1-CDF(0.5)) since my accept condition is x>0.5. This seems again to show some discrepancies. Again the mystery prevails .. It seems that this can be cured by choosing larger shift size which is in general an issue of the Metropolis - Hastings. See the graph below: I also checked shift=150Bottom line is that changing the shift size definitely improves the convergence. The misery is why, since the Gaussian is unbounded.  <code>  # Sample exponential distribution for the case x>0import numpy as npimport matplotlib.pyplot as pltfrom scipy.stats import normdef pdf(x): return x*np.exp(-x)xvec=np.zeros(1000000)x=1.for i in range(1000000): a=x+np.random.normal() xs=x if a > 0. : xs=a A=pdf(xs)/pdf(x) if np.random.uniform()<A : x=xs xvec[i]=xx=np.linspace(0,15,1000)plt.plot(x,pdf(x))plt.hist([x for x in xvec if x != 0],bins=150,normed=True)plt.show() # code updated according to the answer of CrazyIvan from scipy.stats import multivariate_normalRANGE=100000a=2.06072E-02b=1.10011E+00a_range=[0.001,0.5]b_range=[0.01, 2.5]cov=[[3.1313994E-05, 1.8013737E-03],[ 1.8013737E-03, 1.0421529E-01]]x=ay=bj=0for i in range(RANGE): a_t,b_t=np.random.multivariate_normal([a,b],cov)# accept if within bounds - all that is neded to truncate if a_range[0]<a_t and a_t<a_range[1] and b_range[0]<b_t and b_t<b_range[1]: print(dx,dy) import numpy as npimport matplotlib.pyplot as pltfrom scipy.stats import normdef pdf(x): return x*np.exp(-x)# included the corresponding cdfdef cdf(x): return 1. -np.exp(-x)-x*np.exp(-x)xvec=np.zeros(1000000)x=1.for i in range(1000000): a=x+np.random.normal() xs=x if a > 0.5 : xs=a A=pdf(xs)/pdf(x) if np.random.uniform()<A : x=xs xvec[i]=xx=np.linspace(0,15,1000)# new part norm the analytic pdf to fix the areaplt.plot(x,pdf(x)/(1.-cdf(0.5)))plt.hist([x for x in xvec if x != 0],bins=200,normed=True)plt.savefig(""test_exp.png"")plt.show() shift=15.a=x+np.random.normal()*shift.",How to properly sample truncated distributions?
how to create a dataframe from a table in a word file using pandas," I have a word file (.docx) with table of data, I am trying to create a pandas data frame using that table, I have used docx and pandas module. But I could not create a data frame. and also tried to read table as df pd.read_table(""path of the file"")I can read the data cell by cell but I want to read the entire table or any particular column. Thanks in advance <code>  from docx import Documentdocument = Document('req.docx')for table in document.tables: for row in table.rows: for cell in row.cells: print (cell.text)",how to create a dataframe from a table in a word document (.docx) file using pandas
Python3 - JSONDecodeError using Google Translate API, I've searched thoroughly on Stack Overflow but couldn't find an answer to this problem. I'm trying to use the Google Translate API (googletrans 2.2.0) for Python (3.6.2) and am trying to translate a set of non-English documents into English. I am letting Google Translate do the language detection. Here is my code: I am throttling my call to the API by waiting 10 seconds every time. I am also only feeding the API 15k characters at a time to remain within the character limit. Every time I run this code I get the following error message: Can anybody help? <code>  ## newcorpus is a corpus I have created consisting of non-english documentsfileids = newcorpus.fileidsfor f in fileids: p = newcorpus.raw(f) p = str(p[:15000]) translated_text = translator.translate(p) print(translated_text) sleep(10) json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0),JSONDecodeError using Google Translate API with Python3
Bash randomly sampling lines from file," I have a csv file which is ~40gb and 1800000 lines. I want to randomly sample 10,000 lines and print them to a new file.Right now, my approach is to use sed as: Where $vars is a randomly generated list of lines. (Eg: 1p;14p;1700p;...;10203p)While this works, it takes about 5 minutes per execution. It's not a huge time, but I was wondering if anybody had ideas on how to make it quicker? <code>  (sed -n '$vars' < input.txt) > output.txt",Randomly sampling lines from a file
Python: Counting occurrence of List element within List," I'm trying to count the number of occurrences of elements within a list, if such elements are also lists. The order is also important. One important factor is that ['a', 'b', 'c'] != ['c', 'b', 'a']I have tried: Which both resulted in ['a', 'b', 'c'] = ['c', 'b', 'a'], one thing i didn't wantI also tried: Which only resulted in error; since lists can't be used as keys in dicts.Is there a way to do this? <code>  [PSEUDOCODE]lst = [ ['a', 'b', 'c'], ['d', 'e', 'f'], ['a', 'b', 'c'], ['c', 'b', 'a'] ]print( count(lst) )> { ['a', 'b', 'c'] : 2, ['d', 'e', 'f']: 1, ['c', 'b', 'a']: 1 } from collections import counterprint( Counter([tuple(x) for x in lst]) )print( [[x, list.count(x)] for x in set(lst)] ) from collections import counterprint( Counter( lst ) )",Python: Counting occurrences of List element within List
wrap text matplot not working," I'm attempting to wrap text using wrap=True but it doesn't seem to be working for me. Running the example from matplotlib below: gets me this:Text wrapping gone wrongAny ideas on what's the issue? <code>  import matplotlib.pyplot as pltfig = plt.figure()plt.axis([0, 10, 0, 10])t = ""This is a really long string that I'd rather have wrapped so that it""\ "" doesn't go outside of the figure, but if it's long enough it will go""\ "" off the top or bottom!""plt.text(4, 1, t, ha='left', rotation=15, wrap=True)plt.text(6, 5, t, ha='left', rotation=15, wrap=True)plt.text(5, 5, t, ha='right', rotation=-15, wrap=True)plt.text(5, 10, t, fontsize=18, style='oblique', ha='center', va='top', wrap=True)plt.text(3, 4, t, family='serif', style='italic', ha='right', wrap=True)plt.text(-1, 0, t, ha='left', rotation=-15, wrap=True)plt.show()",Wrapping text not working in matplotlib
How to generalizing the creation of a list with many variables and conditions of `if`?," I create a list as follows: How to generalize the creation of such a list, so that it can be easily expanded by a larger number of variables and subsequent conditions? <code>  ['v0' if x%4==0 else 'v1' if x%4==1 else 'v2' if x%4==2 else 'v3' for x in list_1]",How do you generalise the creation of a list with many variables and conditions of `if`?
Plotting pandas multi-index dataframe with one index as Y-axis and other as X-axis," I have a multi-index dataframe that is sampled here: I tried to plot this so that each column ['Var1', 'Var2', 'Var3', 'Var4'] in a separate figure, the Country is a curve, and y-axis, and the Year is the x-axisthe requested figure would be like this Ms-Excel figureI tried to plot it using but it gives KeyError: 'Var1'I also tried the following but it returns 3 empty figures and one figure with all the data in itWhat is wrong with my trials and What can I do to get what I need? <code>  import pandas as pdimport numpy as npfrom matplotlib import pyplot as plt%matplotlib inlinedf = pd.read_csv('https://docs.google.com/uc?id=1mjmatO1PVGe8dMXBc4Ukzn5DkkKsbcWY&export=download', index_col=[0,1])df f, a = plt.subplots(nrows=2, ncols=2, figsize=(9, 12), dpi= 80)df.xs('Var1').plot(ax=a[0])df.xs('Var2').plot(ax=a[1])df.xs('Var3').plot(x=a[2])df.xs('Var4').plot(kax=a[3]) f, a = plt.subplots(nrows=2, ncols=2, figsize=(7, 10), dpi= 80)for indicator in indicators_list: for c, country in enumerate(in_countries): ax = df[indicator].plot() ax.title.set_text(country + "" "" + indicator) ",Plotting pandas multi-index DataFrame with one index as Y-axis and other as X-axis
Python PIL image Verify issue," I am developing a tool which retrieves a JPG from an API and processes it. The source of the image cannot be trusted and I want to test if the image is a valid JPG (which is the only image type allowed).I encountered an error with PIL that I am unable to fix. Below is my code: However, it seems that img.verify() returns None. I can call other functions on the open image like img.size() which returns the size. I get the following output when I try to debug the code: Has someone encountered the same issue? <code>  image = StringIO(base64.b64decode(download['file']))img = Image.open(image)if img.verify(): print 'Valid image'else: print 'Invalid image' img = Image.open(image)print imgprint img.size()print img.verify()[2018-01-09 20:56:43,715: WARNING/Worker-1] <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2577x1715 at 0x7F16E17DC9D0>[2018-01-09 20:56:43,716: WARNING/Worker-1] (2577, 1715)[2018-01-09 20:56:43,716: WARNING/Worker-1] None",Python PIL image Verify returns None
Reindexing pandas DataFrame with Multiple Indices," I have a DataFrame with two indices and would like to reindex it by one of the indices. The reindexing goes as follows: The last line gives the following error: This is because the DataFrame index is a list of tuples: Is it possible to reindex adj_close? By the way, if I don't convert the Panel object to a DataFrame using to_frame(), the reindexing works as it is. But it seems that Panel objects are deprecated... <code>  from pandas_datareader import dataimport matplotlib.pyplot as pltimport pandas as pd# Instruments to downloadtickers = ['AAPL']# Online source one should usedata_source = 'yahoo'# Data rangestart_date = '2000-01-01'end_date = '2018-01-09'# Load the desired datapanel_data = data.DataReader(tickers, data_source, start_date, end_date).to_frame()panel_data.head() # Get just the adjusted closing pricesadj_close = panel_data['Adj Close']# Gett all weekdays between start and end datesall_weekdays = pd.date_range(start=start_date, end=end_date, freq='B')# Align the existing prices in adj_close with our new set of datesadj_close = adj_close.reindex(all_weekdays, method=""ffill"") TypeError: '<' not supported between instances of 'tuple' and 'int' panel_data.index[0] (Timestamp('2018-01-09 00:00:00'), 'AAPL')",Reindexing a specific level of a MultiIndex dataframe
Python/Kivy : how to plus integer value in loop," I have two file demo.py and demo.kv.can someone help me?1. +Add More add row dynamic.After fill value when i click on Total Value then it shows string like 151012.Don't show 12+10+15=37.I am using code for it 2. Can anyone tell me how to put sum of value in Total value TextBox after fill value TextBox instead of click on Total Value Box.Means How to call def test(self) function from value TextBox?demo.py demo.kv it would be a great assistance if you could. <code>  test = '' for val in values: test = val[2]+test self.total_value.text = test from kivy.uix.screenmanager import Screenfrom kivy.app import Appfrom kivy.lang import Builderfrom kivy.core.window import Windowfrom kivy.uix.boxlayout import BoxLayoutfrom kivy.properties import BooleanProperty, ListProperty, StringProperty, ObjectProperty, NumericPropertyfrom kivy.uix.textinput import TextInputfrom kivy.uix.button import ButtonWindow.clearcolor = (0.5, 0.5, 0.5, 1)Window.size = (500, 400)class User(Screen): total_value = ObjectProperty(None) def add_more(self): self.ids.rows.add_row() def test(self): values = [] rows = self.ids.rows for row in reversed(rows.children): vals = [] for ch in reversed(row.children): if isinstance(ch, TextInput): vals.append(ch.text) if isinstance(ch, Button): vals.insert(0, ch.text) values.append(vals) test = '' for val in values: test = val[2]+test self.total_value.text = testclass Row(BoxLayout): col_data = ListProperty([""?"", ""?"", ""?"", ""?"", ""?""]) button_text = StringProperty("""") col_data3 = StringProperty("""") col_data4 = StringProperty("""") def __init__(self, **kwargs): super(Row, self).__init__(**kwargs)class Rows(BoxLayout): row_count = 0 def __init__(self, **kwargs): super(Rows, self).__init__(**kwargs) self.add_row() def add_row(self): self.row_count += 1 self.add_widget(Row(button_text=str(self.row_count)))class Test(App): def build(self): self.root = Builder.load_file('demo.kv') return self.rootif __name__ == '__main__': Test().run() <Row>: size_hint_y: None height: self.minimum_height height: 40 Button: text: root.button_text size_hint_x: None top: 200 TextInput: text: root.col_data3 width: 300 TextInput: text: root.col_data4 width: 300<Rows>: size_hint_y: None height: self.minimum_height orientation: ""vertical""User: total_value:total_value BoxLayout: orientation: ""vertical"" padding : 20, 5 BoxLayout: orientation: ""horizontal"" #padding : 10, 10 spacing: 10, 10 size: 450, 40 size_hint: None, None Label: size_hint_x: .2 text: ""Number"" text_size: self.size valign: 'bottom' halign: 'center' Label: size_hint_x: .4 text: ""name"" text_size: self.size valign: 'bottom' halign: 'center' Label: size_hint_x: .4 text: ""Value"" text_size: self.size valign: 'bottom' halign: 'center' ScrollView: Rows: id: rows BoxLayout: orientation: ""horizontal"" padding : 10, 5 spacing: 10, 10 size: 200, 40 size_hint: None, None Label: size_hint_x: .7 text: ""Total value"" TextInput: id: total_value on_focus:root.test() BoxLayout: orientation: ""horizontal"" size_hint_x: .2 size_hint_y: .2 Button: text: ""+Add More"" on_press: root.add_more()",how to plus integer value in loop
What would be the better way to remap at once Boolean values into 1 and 0 for many columns in Python pandas data frame, I need to have 1 and 0 instead of True and False in a pandas data frame for only columns starting with abc_. Is there any better way of doing this other than my loop: <code>  for col in df: if col[:4] =='abc_': df[col] = df[col].astype(int) ,Convert multiple boolean columns which names start with string `abc_` at once into integer dtype
Using ROIPooling layer in a pretrained ResNet34 in mxnet," Assume I have a Resnet34 pretained model in MXNet and I want to add to it the premade ROIPooling Layer included in the API:https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.ROIPoolingIf the code for initializing Resnet is the following, how can I add ROIPooling at the last layer of the Resnet features before the classifier?Actually, how can I utilize the ROIPooling function in my model in general?How can I incorporate multiple different ROIs in the ROIpooling layer? How should they be stored? How should the data iterator be changed in order to give me the Batch index required by the ROIPooling function ?Let us assume that I use this along with the VOC 2012 Dataset for the task of action recognition <code>  batch_size = 40num_classes = 11init_lr = 0.001step_epochs = [2]train_iter, val_iter, num_samples = get_iterators(batch_size,num_classes)resnet34 = vision.resnet34_v2(pretrained=True, ctx=ctx)net = vision.resnet34_v2(classes=num_classes)class ROIPOOLING(gluon.HybridBlock): def __init__(self): super(ROIPOOLING, self).__init__() def hybrid_forward(self, F, x): #print(x) a = mx.nd.array([[0, 0, 0, 7, 7]]).tile((40,1)) return F.ROIPooling(x, a, (2,2), 1.0)net_cl = nn.HybridSequential(prefix='resnetv20')with net_cl.name_scope(): for l in xrange(4): net_cl.add(resnet34.classifier._children[l]) net_cl.add(nn.Dense(num_classes, in_units=resnet34.classifier._children[-1]._in_units))net.classifier = net_clnet.classifier[-1].collect_params().initialize(mx.init.Xavier(rnd_type='gaussian', factor_type=""in"", magnitude=2), ctx=ctx)net.features = resnet34.featuresnet.features._children.append(ROIPOOLING())net.collect_params().reset_ctx(ctx)",Using ROIPooling layer with a pretrained ResNet34 model in MxNet-Gluon
Using ROIPooling layer with a pretrained ResNet34 model in mxnet," Assume I have a Resnet34 pretained model in MXNet and I want to add to it the premade ROIPooling Layer included in the API:https://mxnet.incubator.apache.org/api/python/ndarray/ndarray.html#mxnet.ndarray.ROIPoolingIf the code for initializing Resnet is the following, how can I add ROIPooling at the last layer of the Resnet features before the classifier?Actually, how can I utilize the ROIPooling function in my model in general?How can I incorporate multiple different ROIs in the ROIpooling layer? How should they be stored? How should the data iterator be changed in order to give me the Batch index required by the ROIPooling function ?Let us assume that I use this along with the VOC 2012 Dataset for the task of action recognition <code>  batch_size = 40num_classes = 11init_lr = 0.001step_epochs = [2]train_iter, val_iter, num_samples = get_iterators(batch_size,num_classes)resnet34 = vision.resnet34_v2(pretrained=True, ctx=ctx)net = vision.resnet34_v2(classes=num_classes)class ROIPOOLING(gluon.HybridBlock): def __init__(self): super(ROIPOOLING, self).__init__() def hybrid_forward(self, F, x): #print(x) a = mx.nd.array([[0, 0, 0, 7, 7]]).tile((40,1)) return F.ROIPooling(x, a, (2,2), 1.0)net_cl = nn.HybridSequential(prefix='resnetv20')with net_cl.name_scope(): for l in xrange(4): net_cl.add(resnet34.classifier._children[l]) net_cl.add(nn.Dense(num_classes, in_units=resnet34.classifier._children[-1]._in_units))net.classifier = net_clnet.classifier[-1].collect_params().initialize(mx.init.Xavier(rnd_type='gaussian', factor_type=""in"", magnitude=2), ctx=ctx)net.features = resnet34.featuresnet.features._children.append(ROIPOOLING())net.collect_params().reset_ctx(ctx)",Using ROIPooling layer with a pretrained ResNet34 model in MxNet-Gluon
solving inequality in sympy," I tried to solve the following inequality in sympy: So I issued the command: However, I got: However, my manual computations give either x < 0 or x > 10000.What am I missing? Due to the -1, I cannot represent it as a rational function.Thanks in advance! <code>  (10000 / x) - 1 < 0 solve_poly_inequality( Poly((10000 / x) - 1 ), '<') [Interval.open(-oo, 1/10000)]",Solving Inequalities in Sympy
Solving Ineualities in Sympy," I tried to solve the following inequality in sympy: So I issued the command: However, I got: However, my manual computations give either x < 0 or x > 10000.What am I missing? Due to the -1, I cannot represent it as a rational function.Thanks in advance! <code>  (10000 / x) - 1 < 0 solve_poly_inequality( Poly((10000 / x) - 1 ), '<') [Interval.open(-oo, 1/10000)]",Solving Inequalities in Sympy
Process finished with exit code 134 (interrupted by signal 6: SIGABRT)," I am working on node2vec in Python, which uses Gensim's Word2Vec internally.When I am using a small dataset, the code works well. But as soon as I try to run the same code on a large dataset, the code crashes:Error: Process finished with exit code 134 (interrupted by signal 6: SIGABRT).The line which is giving the error is I am using PyCharm and Python3.5.What is happening? I could not find any post which could solve my problem. <code>  model = Word2Vec(walks, size=args.dimensions, window=args.window_size, min_count=0, sg=1, workers=args.workers, iter=args.iter)","Python node2vec (Gensim Word2Vec) ""Process finished with exit code 134 (interrupted by signal 6: SIGABRT)"""
Strange Behaviour of cv2.imshow," Something weird is going on with cv2.imshow. I was writing a piece of code and wondering why one of my operations wasn't working (as diagnosed by observing cv2.imshow). In exasperation I ended up writing the very same image to a file wherein it looks fine. Why is cv2.imshow showing a binary image (first image below) while cv2.imwrite writing the grayscale image as intended (second image)? I've never had problems with displaying grayscale images before! This is the image as it's displayed by cv2.imshow:This is the image as it's saved by imwrite: <code>  cv2.imshow('Latest', image)cv2.waitKey(0)cv2.destroyAllWindows()distTransform = cv2.distanceTransform(src=image,distanceType=cv2.DIST_L2,maskSize=5)cv2.imwrite('distanceTransform.png', distTransform)cv2.imshow('Latest', distTransform)cv2.waitKey(0)cv2.destroyAllWindows()",How to use `cv2.imshow` correctly for the float image returned by `cv2.distanceTransform`?
How to copy a class instance in python?," I would like to make a copy of a class instance in python. I tried copy.deepcopy but I get the error message: RuntimeError: Only Variables created explicitly by the user (graph leaves) support the deepcopy protocol at the momentSo suppose I have something like: And now I want to make an identical deep copy of c, is there an easy way? <code>  class C(object): def __init__(self,a,b, **kwargs): self.a=a self.b=b for x, v in kwargs.items(): setattr(self, x, v)c = C(4,5,'r'=2)c.a = 11del c.b",How to copy a Python class instance if deepcopy() does not work?
"How to copy a Python class instance if deepcopy() throws ""Only Variables created explicitly by the user (graph leaves) support the deepcopy protocol""?"," I would like to make a copy of a class instance in python. I tried copy.deepcopy but I get the error message: RuntimeError: Only Variables created explicitly by the user (graph leaves) support the deepcopy protocol at the momentSo suppose I have something like: And now I want to make an identical deep copy of c, is there an easy way? <code>  class C(object): def __init__(self,a,b, **kwargs): self.a=a self.b=b for x, v in kwargs.items(): setattr(self, x, v)c = C(4,5,'r'=2)c.a = 11del c.b",How to copy a Python class instance if deepcopy() does not work?
FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated.," After updating my Numpy and Tensorflow I am getting these kind of warnings. I had already tried these, but nothing works, every suggestion will be appreciated. <code>  FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. from ._conv import register_converters as _register_converters2018-01-19 17:11:38.695932: I C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2",FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated
PyQt5: How to detect the type of widget?," This should be a stupid question. I am just curious and could not find the answer on my own.E.g. I define in PyQt5 some widgets: Is there any method to detect what kind of widget the self.lbl, self.btn, or self.txt are?I could imagine: by detecting the widget type, the input is self.lbl, the output should be QLabel... Or something like this.I have only found the isWidgetType() method. But it is not what I want to have. <code>  self.lbl = QLabel(""label"")self.btn = QPushButton(""click"")self.txt = QLineEdit(""text"")",How to detect the type of widget?
How an I use numpy svd to create a full diagonal matrix?," I am using Python with numpy to do linear algebra.I performed numpy SVD on a matrix to get the matrices U,i, and V. However the i matrix is expressed as a 1x4 matrix with 1 row. i.e.: [ 12.22151125 4.92815942 2.06380839 0.29766152].How can I get numpy to express the i matrix as a diagonal matrix like so:[[12.22151125, 0, 0, 0],[0,4.92815942, 0, 0],[0,0,2.06380839,0 ],[0,0,0,0.29766152]]Code I am using: So I want i to be a full diagonal matrix. How an I do this? <code>  A = np.matrix([[3, 4, 3, 1],[1,3,2,6],[2,4,1,5],[3,3,5,2]])U, i, V = np.linalg.svd(A,full_matrices=True)",How can I use numpy to create a diagonal matrix from a 1d array?
How an I use numpy to create a diagonal matrix from array?," I am using Python with numpy to do linear algebra.I performed numpy SVD on a matrix to get the matrices U,i, and V. However the i matrix is expressed as a 1x4 matrix with 1 row. i.e.: [ 12.22151125 4.92815942 2.06380839 0.29766152].How can I get numpy to express the i matrix as a diagonal matrix like so:[[12.22151125, 0, 0, 0],[0,4.92815942, 0, 0],[0,0,2.06380839,0 ],[0,0,0,0.29766152]]Code I am using: So I want i to be a full diagonal matrix. How an I do this? <code>  A = np.matrix([[3, 4, 3, 1],[1,3,2,6],[2,4,1,5],[3,3,5,2]])U, i, V = np.linalg.svd(A,full_matrices=True)",How can I use numpy to create a diagonal matrix from a 1d array?
Dropout layer in Keras is not working?," just to test whether dropout is working..I set dropout rate to be 1.0the state in each epoch should be freezed without any tuning to parametershowever the accuracy keep growing although i drop all the hidden nodeswhat's wrong? <code>  input0 = keras.layers.Input((32, 32, 3), name='Input0')flatten = keras.layers.Flatten(name='Flatten')(input0)relu1 = keras.layers.Dense(256, activation='relu', name='ReLU1')(flatten)dropout = keras.layers.Dropout(1., name='Dropout')(relu1)softmax2 = keras.layers.Dense(10, activation='softmax', name='Softmax2')(dropout)model = keras.models.Model(inputs=input0, outputs=softmax2, name='cifar')",Dropout behavior in Keras with rate=1 (dropping all input units) not as expected
dask read parquet from s3 with specific aws profile," How to read a parquet file on s3 using dask and specific AWS profile (stored in a credentials file). Dask uses s3fs which uses boto. This is what I have tried: <code>  >>>import os>>>import s3fs>>>import boto3>>>import dask.dataframe as dd>>>os.environ['AWS_SHARED_CREDENTIALS_FILE'] = ""~/.aws/credentials"">>>fs = s3fs.S3FileSystem(anon=False,profile_name=""some_user_profile"")>>>fs.exists(""s3://some.bucket/data/parquet/somefile"")True>>>df = dd.read_parquet('s3://some.bucket/data/parquet/somefile')NoCredentialsError: Unable to locate credentials",How to read parquet file from s3 using dask with specific AWS profile
how to read parquet file from s3 using dask with specific aws profile," How to read a parquet file on s3 using dask and specific AWS profile (stored in a credentials file). Dask uses s3fs which uses boto. This is what I have tried: <code>  >>>import os>>>import s3fs>>>import boto3>>>import dask.dataframe as dd>>>os.environ['AWS_SHARED_CREDENTIALS_FILE'] = ""~/.aws/credentials"">>>fs = s3fs.S3FileSystem(anon=False,profile_name=""some_user_profile"")>>>fs.exists(""s3://some.bucket/data/parquet/somefile"")True>>>df = dd.read_parquet('s3://some.bucket/data/parquet/somefile')NoCredentialsError: Unable to locate credentials",How to read parquet file from s3 using dask with specific AWS profile
access to f1-score in classification_report - sklearn," This is a simple example of classification_report in sklearn I want to have access to avg/total row. For instance, I want to extract f1-score from the report, which is 0.61.How can I have access to the number in classification_report? <code>  from sklearn.metrics import classification_reporty_true = [0, 1, 2, 2, 2]y_pred = [0, 0, 2, 2, 1]target_names = ['class 0', 'class 1', 'class 2']print(classification_report(y_true, y_pred, target_names=target_names))# precision recall f1-score support## class 0 0.50 1.00 0.67 1# class 1 0.00 0.00 0.00 1# class 2 1.00 0.67 0.80 3##avg / total 0.70 0.60 0.61 5",access to numbers in classification_report - sklearn
Efficient numpy argsort with condition," I'm wondering what the most efficient way to do an argsort of an array given a condition, while preserving the original index I want to argsort this array with condition that x>0.6. Since 0.5 < 0.6, index 1 should not be included. This is inefficient since its not vectorized. EDIT:The filter will eliminate most of elements. So ideally, it filter first, then sort, while preserving original index.  <code>  import numpy as npx = np.array([0.63, 0.5, 0.7, 0.65])np.argsort(x)#Corrected argsort(x) solutionOut[99]: array([1, 0, 3, 2]) x = np.array([0.63, 0.5, 0.7, 0.65])index = x.argsort()list(filter(lambda i: x[i] > 0.6, index))[0,3,2]",Efficient numpy argsort with condition while maintaining original indices
Providing key file and cert file to Flask application," I am trying to serve a Flask application over HTTPS using the flask command. I can do this using app.run by passing the ssl_context argument, but I can't figure out how to do this on the CLI. <code>  flask run --host='0.0.0.0' --port=80",Run Flask dev server over HTTPS using CLI
XlsxWriter - freezing top row and first column," I am exporting a pandas DataFrame to Excel, and since it contains a lot of rows and columns, it would be useful to keep the top row and the first column when browsing its contents.There is a feature present in Excel that allows for freezing the top row and the first column. Is accessible through XlsxWriter when exporting DataFrames to excel? <code> ",How to freeze the top row and the first column using XlsxWriter?
How does Python's types.FunctionType work create dynamic Functions?," I'm trying to strengthen my Python skills, and I came across Open-Source code for Saltstack that is using types.FunctionType, and I don't understand what's going on.salt.cloud.clouds.cloudstack.pyFunction create() has the following bit of code: The function get_image, and get_size are passed to a function 'namespaced_function' as so: salt.utils.functools.pyHas the namespaced function I can see that they are dynamically creating a function get_image, but I don't understand the benefit of doing it this way. Why not just create the function? <code>  kwargs = { 'name': vm_['name'], 'image': get_image(conn, vm_), 'size': get_size(conn, vm_), 'location': get_location(conn, vm_),} get_size = namespaced_function(get_size, globals())get_image = namespaced_function(get_image, globals()) def namespaced_function(function, global_dict, defaults=None, preserve_context=False): ''' Redefine (clone) a function under a different globals() namespace scope preserve_context: Allow keeping the context taken from orignal namespace, and extend it with globals() taken from new targetted namespace. ''' if defaults is None: defaults = function.__defaults__ if preserve_context: _global_dict = function.__globals__.copy() _global_dict.update(global_dict) global_dict = _global_dict new_namespaced_function = types.FunctionType( function.__code__, global_dict, name=function.__name__, argdefs=defaults, closure=function.__closure__ ) new_namespaced_function.__dict__.update(function.__dict__) return new_namespaced_function",How does Python's types.FunctionType create dynamic Functions?
Python - How to check JSON format validation?," My program gets a JSON file that has an information for service.Before run the service program, I want to check if the JSON file is valide(Only check whether all necessary keys exist).Below is the standard(necessary datas) JSON format for this program: Now I am checking the JSON file validation like this: Actually, the JSON standard format has more than 20 keys. Is there any other way to check the JSON format? <code>  { ""service"" : ""Some Service Name"" ""customer"" : { ""lastName"" : ""Kim"", ""firstName"" : ""Bingbong"", ""age"" : ""99"", }} import jsondef is_valid(json_file): json_data = json.load(open('data.json')) if json_data.get('service') == None: return False if json_data.get('customer').get('lastName') == None: return False if json_data.get('customer').get('firstName') == None: return False if json_data.get('customer').get('age') == None: return False return True",How to check JSON format validation?
Binance-API Timestamp how to compute to Date in Python," I received the servertime from the Binance-API,I try to work with and it looks like this: The question is, how can I compute the date out of this stamp?I tried But the date wasnt valid.Do you have ideas, or is it to specific?Thank you! <code>  { ""serverTime"": 1518440400000} import datetimeprint(datetime.datetime.fromtimestamp( int(""1518308894652"")).strftime('%Y-%m-%d %H:%M:%S'))",Compute Date out of Timestamp from Binance-API (Python)
Compute Date out of Timestamp from Binance-API," I received the servertime from the Binance-API,I try to work with and it looks like this: The question is, how can I compute the date out of this stamp?I tried But the date wasnt valid.Do you have ideas, or is it to specific?Thank you! <code>  { ""serverTime"": 1518440400000} import datetimeprint(datetime.datetime.fromtimestamp( int(""1518308894652"")).strftime('%Y-%m-%d %H:%M:%S'))",Compute Date out of Timestamp from Binance-API (Python)
updating dictionary in python using for loop," I have list of lists and would like to create data frame with count of all unique elements. Here is my test data: I can do something like this using Counter with for loop as: But how can I have result of this loop summed up into new data frame ?Expected output as data frame: <code>  test = [[""P1"", ""P1"", ""P1"", ""P2"", ""P2"", ""P1"", ""P1"", ""P3""], [""P1"", ""P1"", ""P1""], [""P1"", ""P1"", ""P1"", ""P2""], [""P4""], [""P1"", ""P4"", ""P2""], [""P1"", ""P1"", ""P1""]] from collections import Counterfor item in test: print(Counter(item)) P1 P2 P3 P415 4 1 2",Calculate count of all the elements in nested list
Calculate count of all the element in list of lists," I have list of lists and would like to create data frame with count of all unique elements. Here is my test data: I can do something like this using Counter with for loop as: But how can I have result of this loop summed up into new data frame ?Expected output as data frame: <code>  test = [[""P1"", ""P1"", ""P1"", ""P2"", ""P2"", ""P1"", ""P1"", ""P3""], [""P1"", ""P1"", ""P1""], [""P1"", ""P1"", ""P1"", ""P2""], [""P4""], [""P1"", ""P4"", ""P2""], [""P1"", ""P1"", ""P1""]] from collections import Counterfor item in test: print(Counter(item)) P1 P2 P3 P415 4 1 2",Calculate count of all the elements in nested list
Using attrs to turn JSONs into Python classes," I was wondering if it possible to use the attrs library to convert nested JSONs to Python class instances so that I can access attributes in that JSON via dot notation (object.attribute.nested_attribute). My JSONs have a fixed schema, and I would be fine with having to define the classes for that schema manually, but I'm not sure if it would be possible to turn the JSON into the nested class structure without having to instantiate every nested object individually. I'm basically looking for a fromdict() function that knows (based on the keys) which class to turn a JSON object into.(I also know that there are other ways to build 'DotDicts', but these seem always a bit hacky to me and would probably need thorough testing to verify that they work correctly.) <code> ",Using attrs to turn JSONs into Python classes
Using attrs library to turn JSONs into Python classes," I was wondering if it possible to use the attrs library to convert nested JSONs to Python class instances so that I can access attributes in that JSON via dot notation (object.attribute.nested_attribute). My JSONs have a fixed schema, and I would be fine with having to define the classes for that schema manually, but I'm not sure if it would be possible to turn the JSON into the nested class structure without having to instantiate every nested object individually. I'm basically looking for a fromdict() function that knows (based on the keys) which class to turn a JSON object into.(I also know that there are other ways to build 'DotDicts', but these seem always a bit hacky to me and would probably need thorough testing to verify that they work correctly.) <code> ",Using attrs to turn JSONs into Python classes
Understanding the difference between `self`and `cls` as they refer to the same attributes," I'm trying to understand if there are differences between self and cls but I'm struggling, even though a lot of discussion on this topic exists. For instance: which give me: So whether I use self or cls, it always refers to the same variable. When I add a self.A in the Init__, the cls.A is just replaced and I get: I don't understand the point of having two ways to call a class member if they are the same? I know this is a common question on this forum, yet I really don't understand why we would use different words to refer to the same thing (we even could use any variable name instead of self or cls).updateIn the following case: I get : So in this case, in maclass: cls.A and self.A do not refer to the same variable. <code>  class maclass(): A = ""class method"" def __init__(self): self.B = ""instance method"" def getA_s(self): print(self.A) def getA_c(cls): print(cls.A) def getB_s(self): print(self.B) def getB_c(cls): print(cls.B)C = maclass()C.getA_s()C.getA_c()C.getB_s()C.getB_c() class methodclass methodinstance methodinstance method def __init__(self): self.B = ""instance method"" self.A = ""new instance method"" new instance methodnew instance methodinstance methodinstance method class maclass(): A = ""class method, "" def __init__(self): self.A = ""instance method, "" def getA_s(self): print(self.A) #give me ""instance method, "" @classmethod def getA_c(cls): print(cls.A) #give me ""class method, ""C = maclass()C.getA_s()C.getA_c()print(' ')print(C.A) #give me ""instance method, "" instance method, class method, instance method, ",Understanding the difference between `self`and `cls` and whether they refer to the same attributes
How to setup entry_points in setup.cfg," I am moving my config from setup.py to setup.cfg and having issues setting up the entry_points parameter. At the moment I am using a hybrid approach which works, however, I would like to move the entry_points to setup.cfg.From to <code>  def setup_package(): setup(version=get_version(), entry_points={'console_scripts':['app=my_package.app.run:cli'],}) [metadata]name = my-packagedescription = my-packagelicense = unlicensedlong-description = README.mdplatforms = anyclassifiers = Programming Language :: Python[options]zip_safe = Falsepackages = my_package, my_package.appinclude_package_data = Truepackage_dir = = .tests_require = pytest; pytest-cov[entry_points]console_scripts = my-package = my_package.app.run:cli",How to set up entry_points in setup.cfg
Execute Python on hadoop ecosystem by hadoop streaming on windows 10 makes error as: java.lang.RuntimeException: Error in configuring object," I have a problem to execute mapreduce python files on Hadoop by using Hadoop streaming.jar.I use:Windows 10 64bitPython 3.6 and my IDE is spyder 3.2.6,Hadoop 2.3.0jdk1.8.0_161I can get answer while my maperducec code is written on java language, but my problem is when I want to mingle python libraries such as tensorflow or other useful machine learning libs on my data.Installing hadoop 2.3.0:1. hadoop-env export JAVA_HOME=C:\Java\jdk1.8.0_1612. I created data -> dfs in hadoop folderFor environmentUser VariableHadoop_Home = D:\hadoopJava_Home = C:\Java\jdk1.8.0_161M2_HOME = C:\apache-maven-3.5.2\apache-maven-3.5.2-bin\Maven-3.5.2Platform = x64System Varibales:Edit Path as: My MapReduce Python code:D:\digit\wordcount-mapper.py D:\digit\wordcount-reducer.py When I run my command prompt as administrator: I checked : localhost:8088/ and http://localhost:50070all is ok.Then when I enter: I have this error: I really donot know what is problem it tool my time a lot.Thank you in advanced for your help or any idea? <code>  D:\hadoop\binC:\java\jdk1.8.0_161\binC:\ProgramData\Anaconda3 #!/usr/bin/python3import sysfor line in sys.stdin: line = line.strip() # remove leading and trailing whitespace words = line.split() # split the line into words for word in words: print( '%s\t%s' % (word, 1)) #!/usr/bin/python3from operator import itemgetterimport syscurrent_word = Nonecurrent_count = 0word = Nonefor line in sys.stdin: line = line.strip() word, count = line.split('\t', 1) try: count = int(count) except ValueError: continue if current_word == word: current_count += count else: if current_word: print( '%s\t%s' % (current_word, current_count)) current_count = count current_word = wordif current_word == word: print( '%s\t%s' % (current_word, current_count)) D:\hadoop\bin> hadoop namenode -formatD:\hadoop\sbin>start-dfs.cmdD:\hadoop\sbin>start-yarn.cmd D:\hadoop\sbin>hadoop fs -mkdir -p /inputD:\hadoop\sbin>hadoop fs -copyFromLocal D:\digit\mahsa.txt /inputD:\hadoop\sbin>D:\hadoop\bin\hadoop jar D:\hadoop\share\hadoop\tools\lib\hadoop-streaming-2.3.0.jar -file D:\digit\wordcount-mapper.py -mapper D:\digit\wordcount-mapper.py -file D:\digit\wordcount-reducer.py -reducer D:\digit\wordcount-reducer.py -input /input/mahsa.txt/ -output /output/ 18/02/21 21:49:24 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.packageJobJar: [D:\digit\wordcount-mapper.py, D:\digit\wordcount-reducer.py, /D:/tmp/hadoop-Mahsa/hadoop-unjar7054071292684552905/] [] C:\Users\Mahsa\AppData\Local\Temp\streamjob2327207111481875361.jar tmpDir=null18/02/21 21:49:25 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:803218/02/21 21:49:25 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:803218/02/21 21:49:28 INFO mapred.FileInputFormat: Total input paths to process : 118/02/21 21:49:28 INFO mapreduce.JobSubmitter: number of splits:218/02/21 21:49:29 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1519235874088_000318/02/21 21:49:29 INFO impl.YarnClientImpl: Submitted application application_1519235874088_000318/02/21 21:49:29 INFO mapreduce.Job: The url to track the job: http://Mahsa:8088/proxy/application_1519235874088_0003/18/02/21 21:49:29 INFO mapreduce.Job: Running job: job_1519235874088_000318/02/21 21:49:42 INFO mapreduce.Job: Job job_1519235874088_0003 running in uber mode : false18/02/21 21:49:42 INFO mapreduce.Job: map 0% reduce 0%18/02/21 21:49:52 INFO mapreduce.Job: Task Id : attempt_1519235874088_0003_m_000001_0, Status : FAILEDError: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:426) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ... 9 moreCaused by: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38) ... 14 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ... 17 moreCaused by: java.lang.RuntimeException: configuration exception at org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:222) at org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66) ... 22 moreCaused by: java.io.IOException: Cannot run program ""D:\tmp\hadoop-Mahsa\nm-local-dir\usercache\Mahsa\appcache\application_1519235874088_0003\container_1519235874088_0003_01_000003\.\wordcount-mapper.py"": CreateProcess error=193, %1 is not a valid Win32 application at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048) at org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:209) ... 23 moreCaused by: java.io.IOException: CreateProcess error=193, %1 is not a valid Win32 application at java.lang.ProcessImpl.create(Native Method) at java.lang.ProcessImpl.<init>(ProcessImpl.java:386) at java.lang.ProcessImpl.start(ProcessImpl.java:137) at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029) ... 24 more18/02/21 21:49:52 INFO mapreduce.Job: Task Id : attempt_1519235874088_0003_m_000000_0, Status : FAILEDError: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:426) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ... 9 moreCaused by: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38) ... 14 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ... 17 moreCaused by: java.lang.RuntimeException: configuration exception at org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:222) at org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66) ... 22 moreCaused by: java.io.IOException: Cannot run program ""D:\tmp\hadoop-Mahsa\nm-local-dir\usercache\Mahsa\appcache\application_1519235874088_0003\container_1519235874088_0003_01_000002\.\wordcount-mapper.py"": CreateProcess error=193, %1 is not a valid Win32 application at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048) at org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:209) ... 23 moreCaused by: java.io.IOException: CreateProcess error=193, %1 is not a valid Win32 application at java.lang.ProcessImpl.create(Native Method) at java.lang.ProcessImpl.<init>(ProcessImpl.java:386) at java.lang.ProcessImpl.start(ProcessImpl.java:137) at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029) ... 24 more18/02/21 21:50:02 INFO mapreduce.Job: Task Id : attempt_1519235874088_0003_m_000001_1, Status : FAILEDError: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:426) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ... 9 moreCaused by: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38) ... 14 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ... 17 moreCaused by: java.lang.RuntimeException: configuration exception at org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:222) at org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66) ... 22 moreCaused by: java.io.IOException: Cannot run program ""D:\tmp\hadoop-Mahsa\nm-local-dir\usercache\Mahsa\appcache\application_1519235874088_0003\container_1519235874088_0003_01_000004\.\wordcount-mapper.py"": CreateProcess error=193, %1 is not a valid Win32 application at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048) at org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:209) ... 23 moreCaused by: java.io.IOException: CreateProcess error=193, %1 is not a valid Win32 application at java.lang.ProcessImpl.create(Native Method) at java.lang.ProcessImpl.<init>(ProcessImpl.java:386) at java.lang.ProcessImpl.start(ProcessImpl.java:137) at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029) ... 24 more18/02/21 21:50:03 INFO mapreduce.Job: Task Id : attempt_1519235874088_0003_m_000000_1, Status : FAILEDError: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:426) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ... 9 moreCaused by: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38) ... 14 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ... 17 moreCaused by: java.lang.RuntimeException: configuration exception at org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:222) at org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66) ... 22 moreCaused by: java.io.IOException: Cannot run program ""D:\tmp\hadoop-Mahsa\nm-local-dir\usercache\Mahsa\appcache\application_1519235874088_0003\container_1519235874088_0003_01_000005\.\wordcount-mapper.py"": CreateProcess error=193, %1 is not a valid Win32 application at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048) at org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:209) ... 23 moreCaused by: java.io.IOException: CreateProcess error=193, %1 is not a valid Win32 application at java.lang.ProcessImpl.create(Native Method) at java.lang.ProcessImpl.<init>(ProcessImpl.java:386) at java.lang.ProcessImpl.start(ProcessImpl.java:137) at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029) ... 24 more18/02/21 21:50:13 INFO mapreduce.Job: Task Id : attempt_1519235874088_0003_m_000001_2, Status : FAILEDError: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:426) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ... 9 moreCaused by: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38) ... 14 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ... 17 moreCaused by: java.lang.RuntimeException: configuration exception at org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:222) at org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66) ... 22 moreCaused by: java.io.IOException: Cannot run program ""D:\tmp\hadoop-Mahsa\nm-local-dir\usercache\Mahsa\appcache\application_1519235874088_0003\container_1519235874088_0003_01_000007\.\wordcount-mapper.py"": CreateProcess error=193, %1 is not a valid Win32 application at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048) at org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:209) ... 23 moreCaused by: java.io.IOException: CreateProcess error=193, %1 is not a valid Win32 application at java.lang.ProcessImpl.create(Native Method) at java.lang.ProcessImpl.<init>(ProcessImpl.java:386) at java.lang.ProcessImpl.start(ProcessImpl.java:137) at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029) ... 24 more18/02/21 21:50:14 INFO mapreduce.Job: Task Id : attempt_1519235874088_0003_m_000000_2, Status : FAILEDError: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:426) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ... 9 moreCaused by: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38) ... 14 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ... 17 moreCaused by: java.lang.RuntimeException: configuration exception at org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:222) at org.apache.hadoop.streaming.PipeMapper.configure(PipeMapper.java:66) ... 22 moreCaused by: java.io.IOException: Cannot run program ""D:\tmp\hadoop-Mahsa\nm-local-dir\usercache\Mahsa\appcache\application_1519235874088_0003\container_1519235874088_0003_01_000008\.\wordcount-mapper.py"": CreateProcess error=193, %1 is not a valid Win32 application at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048) at org.apache.hadoop.streaming.PipeMapRed.configure(PipeMapRed.java:209) ... 23 moreCaused by: java.io.IOException: CreateProcess error=193, %1 is not a valid Win32 application at java.lang.ProcessImpl.create(Native Method) at java.lang.ProcessImpl.<init>(ProcessImpl.java:386) at java.lang.ProcessImpl.start(ProcessImpl.java:137) at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029) ... 24 more18/02/21 21:50:24 INFO mapreduce.Job: map 100% reduce 100%18/02/21 21:50:34 INFO mapreduce.Job: Job job_1519235874088_0003 failed with state FAILED due to: Task failed task_1519235874088_0003_m_000001Job failed as tasks failed. failedMaps:1 failedReduces:018/02/21 21:50:34 INFO mapreduce.Job: Counters: 13 Job Counters Failed map tasks=7 Killed map tasks=1 Launched map tasks=8 Other local map tasks=6 Data-local map tasks=2 Total time spent by all maps in occupied slots (ms)=66573 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=66573 Total vcore-seconds taken by all map tasks=66573 Total megabyte-seconds taken by all map tasks=68170752 Map-Reduce Framework CPU time spent (ms)=0 Physical memory (bytes) snapshot=0 Virtual memory (bytes) snapshot=018/02/21 21:50:34 ERROR streaming.StreamJob: Job not Successful!Streaming Command Failed!","Python Hadoop streaming on windows, Script not a valid Win32 application"
Module object has no attribute relu," I am trying to run the code from here which is an implementatino of Generative Adversarial Networks using keras python. I followed the instructions and install all the requirements. Then i tried to run the code for DCGAN. However, it seems that there is some issue with the compatibility of the libraries. I am receiving the following message when i am running the code: AttributeError: 'module' object has no attribute 'leaky_relu' I am using kerasVersion: 2.1.3 while tensorflowVersion: 1.2.1 and TheanoVersion: 1.0.1+40.g757b4d5Any idea why am I receiving that issue? EDIT:The error is located in the line 84 in the build_discriminator: <code>  File ""main.py"", line 176, in <module>dcgan = DCGAN()File ""main.py"", line 25, in __init__self.discriminator = self.build_discriminator()File ""main.py"", line 84, in build_discriminatormodel.add(LeakyReLU(alpha=0.2))File ""/opt/libraries/anaconda2/lib/python2.7/site-packages/keras/models.py"", line 492, in addoutput_tensor = layer(self.outputs[0])File ""/opt/libraries/anaconda2/lib/python2.7/site-packages/keras/engine/topology.py"", line 617, in __call__output = self.call(inputs, **kwargs)File ""/opt/libraries/anaconda2/lib/python2.7/site-packages/keras/layers/advanced_activations.py"", line 46, in callreturn K.relu(inputs, alpha=self.alpha)File ""/opt/libraries/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py"", line 2918, in relux = tf.nn.leaky_relu(x, alpha) function:`model.add(LeakyReLU(alpha=0.2))`",Module object has no attribute leaky_relu
Jinja2 scope Issue," I am building a Flask app and am trying to loop through order lines to display the amount of items in a basket. After some research I've found it is a scope issue, i.e. the bottom {{ items }} can't see that I have added 5 then 2. How can I increment a value in a Jinja for loop? <code>  {% set items = 0 %}{% for line in current_order.order_lines %} #loops twice in current test {% set items = items + line.quantity %} #should add 5 then 2{% endfor %}{{ items }} #outputs 0",Jinja for loop scope is reset when incrementing variable
Terminal output text into pandas dataframe without creating external file," I am using ffmpeg's extract_mvs file to generate some text information. I would use a command like this in the terminal: I would like to use this command with Popen or other subprocess in python such that instead of output.txt, the data is passed straight to a pandas data frame without actually generating the text file.The idea is to automate this multiple times, so, I am trying to avoid many .txt files from being generated and thus having to open() them one by one. I thought of something like this: But then I get an error: OSError: Expected file path name or file-like object, got <class 'bytes'> typeCan it be fixed and extended so as to read straight from subprocess to pandas? <code>  /extract_mvs input.mp4 > output.txt import subprocesscmd = ['./extract_mvs', 'input.mp4']a = subprocess.Popen(cmd, stdout=subprocess.PIPE)df = pd.read_csv(a.communicate()[0], sep=',')",Capturing terminal output into pandas dataframe without creating external text file
Calling multiple instances of python scripts in parallel from matlab for Speech recognition cloud processing," I am running Matlab2017 on windows 10.I call a python script that runs some Speech Recognition task on cloud with something like this: When the above command is called, the python script runs the input audio file on the ASR cloud engine, and as it runs, I can see Speech Recognition scores for the audio file from Python in the Matlab console. I want to do the following:(1) Execute multiple such commands in parallel. Lets say, I have 2 input audio files (each having different audio segments), and I would like to run the above command 2 times, but in parallel, using separate processes. I was able to create a code snippet that should be able to do this: Now, when the above code is executed, I can see ASRE scores for data1 ,but not for data 2. The exit status in variable rc, is 0,1, which confirms this. The problem is I do not know the cause of the error, as nothing is printed in Matlab. How can I get error message from Python captured in a java/Matlab variable so that i could take a look?The issue could be that multiple Calls to ASRE in parallel (with different user accounts of course) may not be supported but I won't know unless I can see the error.(2) When I run a single command standalone, as mentioned at the start of the post, I am able to see Score messages for each audio segment being printed in the Matlab console, as they are obtained from Python. However, with multi-processing using java.lang.Runtime.getRuntime() and the associated code, no messages appears in the Matlab console. Is there a way to display those messages (I am assuming display might be asynchronous?)Thankssedy <code>  userAuthCode=1;% authentication code for user account to be run on cloud cmd = ['C:\Python27\python.exe runASR.py userAuthCode]; system(cmd); for i=1: 2 userAuthCode=i; cmd = ['C:\Python27\python.exe runASR.py userAuthCode]; runtime = java.lang.Runtime.getRuntime(); pid(i) = runtime.exec(cmd); end for i=1:2 pid(i).waitFor(); % get exit status rc(i) = pid(i).exitValue(); end",Calling multiple instances of python scripts in matlab using java.lang.Runtime.getRuntime not working
What is the difference between .Semaphore() and .BoundedSemaphore() ? [Python]," I know that threading.Lock() is equal to threading.Semaphore(1).Is also threading.Lock() equal to threading.BoundedSemaphore(1) ?And newly I saw threading.BoundedSemaphore(), what is the difference between them? For example in the following code snippet (applying limitation on threads): <code>  import threadingsem = threading.Semaphore(5)sem = threading.BoundedSemaphore(5)",What is the difference between .Semaphore() and .BoundedSemaphore()?
Python - What is the difference between .Semaphore() and .BoundedSemaphore()?," I know that threading.Lock() is equal to threading.Semaphore(1).Is also threading.Lock() equal to threading.BoundedSemaphore(1) ?And newly I saw threading.BoundedSemaphore(), what is the difference between them? For example in the following code snippet (applying limitation on threads): <code>  import threadingsem = threading.Semaphore(5)sem = threading.BoundedSemaphore(5)",What is the difference between .Semaphore() and .BoundedSemaphore()?
`in range` construction in python 2 --- working too slow," I wanted to check if a given x lies in the interval [0,a-1].As a lazy coder I wrote and (because that piece of code was in 4.5 nested loops) quickly run into performance issues. I tested it and indeed, it turns out runtime of n in range(n) lies in O(n), give or take. I actually thought my code would be optimized to x >= 0 and x < a but it seems this is not the case. Even if I fix the range(a) in advance the time doesn't become constant (though it improves a lot) - see side notes.So, my question is:Should I use x >= 0 and x < a and never write x in range(a) ever again? Is there an even better way of writing it?Side notes:I tried searching SO for range, python-2.7, performance tags put together, found nothing (same with python-2.x).If I try following: so that the range is fixed and I only measure runtime of x in i, I still get runtime in O(x) (assuming a is large enough).runtime of n in xrange(n) lies in O(n) as well.I found this post, which asks similar question for python 3. I decided to test same stuff on python 3 and it rolled through tests like it's nothing. I got sad for python 2. <code>  x in range(a) i = range(a)...x in i","`in range` construction in python 2 --- working too slow, possible substitutions?"
Python: str.format() with nested dictionary," I want to use the str.format() method like this: And apply it to items as shown below: Is this feasible, and if so, then how? If not, I'm interested in your recommended approach. <code>  my_str = ""Username: {username}, User data: {user_data.attribute}"".format(**items) items = { ""username"" : ""Peter"", ""user_data"" : { ""attribute"" : ""foo"" }}",str.format() with nested dictionary
Where does MANIFEST.in save the files?," I have this small program do it with Python3.5 with the following structure: setup.py: MANIFEST.in After execute the following line and create the .deb: $ python3 setup.py --command-packages=stdeb.command bdist_debAnd unpack with: $ sudo dpkg -i deb_dist/python3-awesomegui_1.0-1_all.deb The code (*.py) is saved in /usr/lib/python3/dist-packages/awesome_gui/. But I do not see where app.config is saved.Does anyone know where the data files that are non-code are stored?Thank you! <code>  awesome_gui/ app.config MANIFEST.in setup.py awesome_gui/ __init__.py main.py #!/usr/bin/env pythonimport osfrom setuptools import setupsetup( name = ""awesomegui"", version = ""1.0"", author = ""Me"", author_email = ""me@example.com"", description = ""Awesome GUI"", packages=['awesome_gui'], entry_points = { 'console_scripts': ['awesomegui=awesome_gui.main'], }, include_package_data=True,) include app.config",Where are the data files included in MANIFEST.IN stored?
Activating conda dlwin36 from c# code (or what is the differences between manually opening cmd and opening it from c#?)," I want to run a gpu accelerated python script on windows using conda environment (dlwin36).Im trying to activate dlwin36 and execute a script:1) activate dlwin362) set KERAS_BACKEND=tensorflow3) python myscript.pyIf I manually open cmd on my machine and write:""activate dlwin36""it works.But when I try opening a cmd from c# I get:activate is not recognized as an internal or external command, operable program or batch file.I tried using the following methods:Command chaining: (Ive tested several variations of UseShellExecute, LoadUserProfile and WorkingDirectory)Redirect standard input: (Ive tested several variations of LoadUserProfile and WorkingDirectory)In both cases, I got the same error.It seems that there is a difference between manually opening cmd and opening it from c#. <code>  var start = new ProcessStartInfo();start.FileName = ""cmd.exe"";start.Arguments = ""/c activate dlwin36&&set KERAS_BACKEND=tensorflow&&python myscript.py"";Process.Start(start).WaitForExit(); var commandsList = new List<string>();commandsList.Add(""activate dlwin36"");commandsList.Add(""set KERAS_BACKEND=tensorflow"");commandsList.Add(""python myscript.py"");var start = new ProcessStartInfo();start.FileName = ""cmd.exe"";start.UseShellExecute = false;start.RedirectStandardInput = true;var proc = Process.Start(start);commandsList.ForEach(command => proc.StandardInput.WriteLine(command));",Activating conda environment from c# code (or what is the differences between manually opening cmd and opening it from c#?)
remove backround of any image using opencv," I have been searching for a technique to remove the background of a any given image. The idea is to detect a face and remove the background of the detected face. I have finished the face part. Now removing the background part still exists.I used this code. But this code only works for only this image What should be changed into to use the code for different images <code>  import cv2import numpy as np#== Parameters BLUR = 21CANNY_THRESH_1 = 10CANNY_THRESH_2 = 200MASK_DILATE_ITER = 10MASK_ERODE_ITER = 10MASK_COLOR = (0.0,0.0,1.0) # In BGR format#-- Read imageimg = cv2.imread('SYxmp.jpg')gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)#-- Edge detection edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)edges = cv2.dilate(edges, None)edges = cv2.erode(edges, None)#-- Find contours in edges, sort by area contour_info = []_, contours, _ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)for c in contours: contour_info.append(( c, cv2.isContourConvex(c), cv2.contourArea(c), ))contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)max_contour = contour_info[0]#-- Create empty mask, draw filled polygon on it corresponding to largest contour ----# Mask is black, polygon is whitemask = np.zeros(edges.shape)cv2.fillConvexPoly(mask, max_contour[0], (255))#-- Smooth mask, then blur itmask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)mask_stack = np.dstack([mask]*3) # Create 3-channel alpha mask#-- Blend masked img into MASK_COLOR backgroundmask_stack = mask_stack.astype('float32') / 255.0 img = img.astype('float32') / 255.0 masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR) masked = (masked * 255).astype('uint8') cv2.imshow('img', masked) # Displaycv2.waitKey()cv2.imwrite(""WTF.jpg"",masked)",remove background of any image using opencv
finding the unique sequences from a dataframe," In my below data set, I need to find unique sequences and assign them a serial no .. DataSet : Expected output: After finding the unique values like shown above, if I pass a new user like below, it should return me the products as a list . if there is no sequence, like below it should return me an empty list <code>  user age maritalstatus productA Young married 111B young married 222C young Single 111D old single 222E old married 111F teen married 222G teen married 555H adult single 444I adult single 333 young married 0young single 1old single 2old married 3teen married 4adult single 5 user age maritalstatus X young married X : [111, 222] user age maritalstatus Y adult married Y : []",Finding unique combinations of columns from a dataframe
Vectorized string operations in numpy: why are they rather slow?," This is of those ""mostly asked out of pure curiosity (in possibly futile hope I will learn something)"" questions.I was investigating ways of saving memory on operations on massive numbers of strings, and for some scenarios it seems like string operations in numpy could be useful. However, I got somewhat surprising results: Memory consumption using memory_profiler: So far, so good; however, timing results are surprising for me: Why is that? I expected that since Numpy uses contiguous chunks of memory for its arrays AND its operations are vectorized (as the above numpy doc page says) AND numpy string arrays apparently use less memory so operating on them should at least potentially be more on-CPU cache-friendly, performance on arrays of strings would be at least similar to those in pure Python?Environment: Python 3.6.3 x64, Linuxnumpy==1.14.1 <code>  import randomimport stringmilstr = [''.join(random.choices(string.ascii_letters, k=10)) for _ in range(1000000)]npmstr = np.array(milstr, dtype=np.dtype(np.unicode_, 1000000)) %memit [x.upper() for x in milstr]peak memory: 420.96 MiB, increment: 61.02 MiB%memit np.core.defchararray.upper(npmstr)peak memory: 391.48 MiB, increment: 31.52 MiB %timeit [x.upper() for x in milstr]129 ms 926 s per loop (mean std. dev. of 7 runs, 10 loops each)%timeit np.core.defchararray.upper(npmstr)373 ms 2.36 ms per loop (mean std. dev. of 7 runs, 1 loop each)",Vectorized string operations in Numpy: why are they rather slow?
Why does the **kwargs mapping compare equal with an OrderedDict?," According to PEP 468: Starting in version 3.6 Python will preserve the order of keyword arguments as passed to a function. To accomplish this the collected kwargs will now be an ordered mapping. Note that this does not necessarily mean OrderedDict.In that case, why does this ordered mapping fail to respect equality comparison with Python's canonical ordered mapping type, the collections.OrderedDict: Although iteration order is now preserved, kwargs seems to be behaving just like a normal dict for the comparisons. Python has a C implemented ordered dict since 3.5, so it could conceivably have been used directly (or, if performance was still a concern, a faster implementation using a thin subclass of the 3.6 compact dict).Why doesn't the ordered mapping received by a function respect ordering in equality comparisons? <code>  >>> from collections import OrderedDict>>> data = OrderedDict(zip('xy', 'xy'))>>> def foo(**kwargs):... return kwargs == data... >>> foo(x='x', y='y') # expected result: TrueTrue>>> foo(y='y', x='x') # expected result: FalseTrue",Why does the **kwargs mapping compare equal with a differently ordered OrderedDict?
How to buils stuff via conda on colab.research?, So I want to use python-occ library. It requires conda-forge to be build. I try to install it in basic notebook Yet it will install a package into condas python. How to make oit install package into global python or use its python\libs folder for cels interpritation?So what one must do to build/install stuff with conda in colab? <code>  !wget -c https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh!chmod +x Anaconda3-5.1.0-Linux-x86_64.sh!bash ./Anaconda3-5.1.0-Linux-x86_64.sh -b -f -p=conda3!export PYTHONPATH=./conda3/lib/python!export PATH=./conda3/bin/:$PATH!conda install -y -c conda-forge -c dlr-sc -c pythonocc -c oce pythonocc-core,How to build libraries via conda on colab.research?
python why is subprocess.Popen blocking?," I have an python cgi script that handles login, this is because my website is three (school) websites combined and before my website can be used the data needs to be extracted from those websites. This extraction takes 2 minutes so I want to make a fancy (semi-fake) loading screen.My register code ends with: With the subprocess line stolen from: Run Process and Don't WaitBut the subprocess line is still blocking and I can't figure out why.I'm developing on ubuntu 16.04, and it's going to run on an raspbarry pi 3 (that explains the loading time) <code>  import subprocesstoken = """".join(random.choice( string.ascii_lowercase + string.digits + string.ascii_uppercase) for _ in range(5)) #generate 5 random characters#run initScriptsubprocess.Popen(""python {}/python/initUser.py {} {}"".format( os.getcwd(), uid,token), shell=True, stdin=None, stdout=None, stderr=None, close_fds=True)print ""Content-type: text/html""print ""Location: registerLoading.php?token={}"".format(token)printsys.exit(0)",Why is subprocess.Popen blocking?
Trying to code up a discord bot in Python but commands aren't really working," Basically, everything appears to work fine and start up, but for some reason I can't call any of the commands. I've been looking around for easily an hour now and looking at examples/watching videos and I can't for the life of me figure out what is wrong. Code below: The debug output I have in on_message actually does work and responds, and the whole bot runs wihout any exceptions, but it just won't call the commands. <code>  import discordimport asynciofrom discord.ext import commandsbot = commands.Bot(command_prefix = '-')@bot.eventasync def on_ready(): print('Logged in as') print(bot.user.name) print(bot.user.id) print('------')@bot.eventasync def on_message(message): if message.content.startswith('-debug'): await message.channel.send('d')@bot.command(pass_context=True)async def ping(ctx): await ctx.channel.send('Pong!')@bot.command(pass_context=True)async def add(ctx, *, arg): await ctx.send(arg)",Why does on_message stop commands from working?
Adding items to QlistView in pyqt4 with python 2.7," I'm using pyqt4 with python 2.7 and I have a list view widget that I can't add items to it As you guys can see i used But it gave me an error that is talking about arguments and data types: Also, is it right to use ListView here or I should use listwidget?In general, what is the difference between both !! <code>  # -*- coding: utf-8 -*-# Form implementation generated from reading ui file 'add_category.ui'## Created: Mon Mar 19 23:22:30 2018# by: PyQt4 UI code generator 4.10## WARNING! All changes made in this file will be lost!from PyQt4 import QtCore, QtGuitry: _fromUtf8 = QtCore.QString.fromUtf8except AttributeError: def _fromUtf8(s): return stry: _encoding = QtGui.QApplication.UnicodeUTF8 def _translate(context, text, disambig): return QtGui.QApplication.translate(context, text, disambig, _encoding)except AttributeError: def _translate(context, text, disambig): return QtGui.QApplication.translate(context, text, disambig)class Ui_Dialog1(object): def setupUi(self, Dialog): Dialog.setObjectName(_fromUtf8(""Dialog"")) Dialog.resize(608, 460) icon = QtGui.QIcon() icon.addPixmap(QtGui.QPixmap(_fromUtf8("":/media/media/in.png"")), QtGui.QIcon.Normal, QtGui.QIcon.Off) Dialog.setWindowIcon(icon) self.gridLayout = QtGui.QGridLayout(Dialog) self.gridLayout.setObjectName(_fromUtf8(""gridLayout"")) self.lineEdit = QtGui.QLineEdit(Dialog) self.lineEdit.setObjectName(_fromUtf8(""lineEdit"")) self.gridLayout.addWidget(self.lineEdit, 0, 1, 1, 1) self.pushButton = QtGui.QPushButton(Dialog) self.pushButton.setIcon(icon) self.pushButton.setObjectName(_fromUtf8(""pushButton"")) self.gridLayout.addWidget(self.pushButton, 0, 6, 1, 1) self.label = QtGui.QLabel(Dialog) font = QtGui.QFont() font.setFamily(_fromUtf8(""Adobe Arabic"")) font.setPointSize(24) self.label.setFont(font) self.label.setObjectName(_fromUtf8(""label"")) self.gridLayout.addWidget(self.label, 0, 0, 1, 1) self.listView = QtGui.QListView(Dialog) self.listView.setObjectName(_fromUtf8(""listView"")) entries = ['one','two', 'three'] for i in entries: item = QtGui.QListView(i) self.listView.addItem(item) self.gridLayout.addWidget(self.listView, 1, 0, 1, 2) self.pushButton_2 = QtGui.QPushButton(Dialog) icon1 = QtGui.QIcon() icon1.addPixmap(QtGui.QPixmap(_fromUtf8("":/media/media/ok.png"")), QtGui.QIcon.Normal, QtGui.QIcon.Off) self.pushButton_2.setIcon(icon1) self.pushButton_2.setObjectName(_fromUtf8(""pushButton_2"")) def go_back(self): Dialog.hide() self.pushButton_2.clicked.connect(go_back) self.gridLayout.addWidget(self.pushButton_2, 2, 1, 1, 1) self.retranslateUi(Dialog) QtCore.QMetaObject.connectSlotsByName(Dialog) def retranslateUi(self, Dialog): Dialog.setWindowTitle(_translate(""Dialog"", "" "", None)) self.lineEdit.setPlaceholderText(_translate(""Dialog"", "" "", None)) self.pushButton.setText(_translate(""Dialog"", """", None)) self.label.setText(_translate(""Dialog"", "" "", None)) self.pushButton_2.setText(_translate(""Dialog"", """", None))import resrcsif __name__ == ""__main__"": import sys app = QtGui.QApplication(sys.argv) Dialog = QtGui.QDialog() ui = Ui_Dialog1() ui.setupUi(Dialog) Dialog.show() sys.exit(app.exec_()) entries = ['one','two', 'three']for i in entries: item = QtGui.QListView(i) self.listView.addItem(item) Traceback (most recent call last): File ""C:\python\townoftechwarehouse\add_category.py"", line 84, in <module> ui.setupUi(Dialog) File ""C:\python\townoftechwarehouse\add_category.py"", line 54, in setupUi item = QtGui.QListView(i)TypeError: QListView(QWidget parent=None): argument 1 has unexpected type 'str'[Finished in 1.7s]",Adding items to QlistView
Keras: How to use fit_generator with multiple inputs," Is it possible to have two fit_generator?I'm creating a model with two inputs,The model configuration is shown below.Label Y uses the same labeling for X1 and X2 data.The following error will continue to occur. Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[[[0.75686276, 0.75686276, 0.75686276], [0.75686276, 0.75686276, 0.75686276], [0.75686276, 0.75686276, 0.75686276], ..., [0.65882355, 0.65882355, 0.65882355...My code looks like this: <code>  def generator_two_img(X1, X2, Y,batch_size): generator = ImageDataGenerator(rotation_range=15, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest') genX1 = generator.flow(X1, Y, batch_size=batch_size) genX2 = generator.flow(X2, Y, batch_size=batch_size) while True: X1 = genX1.__next__() X2 = genX2.__next__() yield [X1, X2], Y """""" ................................. """"""hist = model.fit_generator(generator_two_img(x_train, x_train_landmark, y_train, batch_size), steps_per_epoch=len(x_train) // batch_size, epochs=nb_epoch, callbacks = callbacks, validation_data=(x_validation, y_validation), validation_steps=x_validation.shape[0] // batch_size, `enter code here`verbose=1)",How to use fit_generator with multiple inputs
AssertionError: Gaps in blk ref_locs," I am trying to unstack() data in a Pandas dataframe, but I keep getting this error, and I'm not sure why. Here is my code so far with a sample of my data. My attempt to fix it was to remove all rows where voteId was not a number, which did not work with my actual dataset. This is happening both in an Anaconda notebook (where I am developing) and in my production env when I deploy the code.I could not figure out how to reproduce the error in my sample code... possibly due to a typecasting issue that doesnt exist when you instantiate the dataframe like I did in the sample? Full error message/stack trace: <code>  #dataset simulate likely input# d = {'vote': [100, 50,1,23,55,67,89,44], # 'vote2': [10, 2,18,26,77,99,9,40], # 'ballot1': ['a','b','a','a','b','a','c','c'],# 'voteId':[1,2,3,4,5,'aaa',7,'NaN']}# df1=pd.DataFrame(d)#########################################################df1=df1.drop_duplicates(['voteId','ballot1'],keep='last')s=df1[:10].set_index(['voteId','ballot1'],verify_integrity=True).unstack()s.columns=s.columns.map('(ballot1={0[1]}){0[0]}'.format) dflw=pd.DataFrame(s) ---------------------------------------------------------------------------AssertionError Traceback (most recent call last)<ipython-input-10-0a520180a8d9> in <module>() 22 df1=df1.drop_duplicates(['voteId','ballot1'],keep='last') 23 ---> 24 s=df1[:10].set_index(['voteId','ballot1'],verify_integrity=True).unstack() 25 s.columns=s.columns.map('(ballot1={0[1]}){0[0]}'.format) 26 dflw=pd.DataFrame(s)~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py in unstack(self, level, fill_value) 4567 """""" 4568 from pandas.core.reshape.reshape import unstack-> 4569 return unstack(self, level, fill_value) 4570 4571 _shared_docs['melt'] = (""""""~/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/reshape.py in unstack(obj, level, fill_value) 467 if isinstance(obj, DataFrame): 468 if isinstance(obj.index, MultiIndex):--> 469 return _unstack_frame(obj, level, fill_value=fill_value) 470 else: 471 return obj.T.stack(dropna=False)~/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/reshape.py in _unstack_frame(obj, level, fill_value) 480 unstacker = partial(_Unstacker, index=obj.index, 481 level=level, fill_value=fill_value)--> 482 blocks = obj._data.unstack(unstacker) 483 klass = type(obj) 484 return klass(blocks)~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in unstack(self, unstacker_func) 4349 new_columns = new_columns[columns_mask] 4350 -> 4351 bm = BlockManager(new_blocks, [new_columns, new_index]) 4352 return bm 4353 ~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in __init__(self, blocks, axes, do_integrity_check, fastpath) 3035 self._consolidate_check() 3036 -> 3037 self._rebuild_blknos_and_blklocs() 3038 3039 def make_empty(self, axes=None):~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in _rebuild_blknos_and_blklocs(self) 3127 3128 if (new_blknos == -1).any():-> 3129 raise AssertionError(""Gaps in blk ref_locs"") 3130 3131 self._blknos = new_blknosAssertionError: Gaps in blk ref_locs",AssertionError: Gaps in blk ref_locs when unstack() dataframe
how to use keras with gpu," I've successfully installed TensorFlow with GPU. When I run the following script I get this result: C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 2018-03-26 Found device 0 with properties: name: GeForce GTX 970 major: 5 minor: 2 memoryClockRate(GHz): 1.253 pciBusID: 0000:01:00.0 totalMemory: 4.00GiB freeMemory: 3.31GiB 2018-03-26 11:47:03.186046: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1312] Adding visible gpu devices: 0 2018-03-26 11:47:04.062049: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:993] Creating TensorFlow device (/device:GPU:0 with 3043 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2) [name: ""/device:CPU:0"" device_type: ""CPU"" memory_limit: 268435456 locality { } incarnation: 8082333747214375667 , name: ""/device:GPU:0"" device_type: ""GPU"" memory_limit: 3190865920 locality { bus_id: 1 } incarnation: 1190887510488091263 physical_device_desc: ""device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2"" ]If I run a CNN in Keras, for example, will it automatically use the GPU? Or do I have to write some code to force Keras into using the GPU?For example, with the MNIST dataset, how would I use the GPU? <code>  from tensorflow.python.client import device_libprint(device_lib.list_local_devices()) model = Sequential()model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))model.add(Conv2D(64, (3, 3), activation='relu'))model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Dropout(0.25))model.add(Flatten())model.add(Dense(128, activation='relu'))model.add(Dropout(0.5))model.add(Dense(num_classes, activation='softmax'))model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))",How to use Keras with GPU?
Importing numpy in cython file gives undefined symbol error," When I use any import statement within modules I compile with cython, I receive the following error on importing the modules (see full code below): Everything works fine on my own machine, but I get this error when trying to recompile and run my scripts on an external high-performance computer.Googling I saw similar errors pop up in several places, but in these discussions the problem is either related to an error in the own code, rather than an imported module (e.g. Undefined symbol error importing Cython module) or something goes wrong at building cython or compile time (e.g. Can cython be compiled with icc?). I don't see how to apply these to my case.From reading other discussions, I suspected the problem was in the formulation of my setup.py, but I really don't know much about that - on my own system the standard examples simply worked. I tried adding numpy to the setup (see setup2.py below), but that didn't solve the problem - I get the same error on import hw.Full examplehw.pyx: setup.py: setup2.py: System infoThe external system runs python 3.5.2. I run my code within a virtualenv, where I have Cython 0.28.1. I tried both numpy 1.13.0 and 1.14.2. For the virtuel environment in this example, I did not install any other packages (probably not related to my problem, but pip freeze also lists UNKNOWN==0.0.0)[IMPORTANT EDIT]I received the error for numpy (and since I had struggled before with cdef np.ndarray I figured that was the problem) - but actually it is any import statement that causes the error - numpy just happened to be the first module that was imported. Still no clue how to resolve this though.  <code>  ImportError: /.../hw.cpython-35m-x86_64-linux-gnu.so: undefined symbol: __intel_sse2_strchr import numpy # without this line, the example works fine cpdef testfun(): print(""Hello world!"") from distutils.core import setupfrom Cython.Build import cythonizesetup( ext_modules = cythonize('hw.pyx'),) from distutils.core import setupfrom Cython.Build import cythonizeimport numpy as npsetup( ext_modules = cythonize('hw.pyx'), include_dirs = [np.get_include()] # -> including this doesn't make a difference)",Importing any module in cython file gives undefined symbol error
How to sum a column group by other columns in a list?," I have a list as follows. I would like sum up the last column grouped by the other columns.The result is like this which is still a list.In real practice, I would always like to sum up the last column grouped by many other columns. Is there a way I can do this in Python? Much appreciated. <code>  [['Andrew', '1', '9'], ['Peter', '1', '10'], ['Andrew', '1', '8'], ['Peter', '1', '11'], ['Sam', '4', '9'], ['Andrew', '2', '2']] [['Andrew', '1', '17'], ['Peter', '1', '21'], ['Sam', '4', '9'], ['Andrew', '2', '2']]",How to sum a column grouped by other columns in a list?
jacobian matrix in python," I am reading about jacobian Matrix, trying to build one and from what I have read so far, this python code should be considered as jacobian. Am I understanding this right? <code>  import numpy as npa = np.array([[1,2,3], [4,5,6], [7,8,9]])b = np.array([[1,2,3]]).Tc = a.dot(b) #functionjacobian = a # as partial derivative of c w.r.t to b is a.",Compute the Jacobian matrix in Python
"Initializing Flask environment variables for development, automatically on start. Is my method problematic?"," I'm working on my Flask project in a virtualenv. Every time I start a new terminal, I have to reinitialize these Flask environment variables: My goal is to not have to type them in manually.I tried writing a Python script that set them, but couldn't make it work. I tried writing a shell script that set them, but Flask would raise an error that said my Python path was incorrect.Finally, I tried adding the the env vars to the bottom of the virtualenv's activate script. It worked! The env vars are set and Flask runs as expected. Is it OK to modify the activate script like this? This is just for development purposes. <code>  export FLASK_APP=""server.py""export FLASK_DEBUG=""1"" $ source venv/bin/activate$ flask run",Is adding Flask env vars to the virtualenv's activate script OK?
How to close and issue with Github API," I am trying to figure out how to close an issue through the Github API.Specifically I'm trying to do it through pyGitHub and python, but knowing how to close an issue through the GitHub API would be enough to let me figure it out.Can anyone point me in the right direction? I'm sure it's simple but I can't find it in the documentation <code> ",How to close an issue with GitHub API
How to pass arguemnts with slash in Django2 urls," I would like to call a Django URL with an argument containing a forward slash. For example, I would like to call mysite.com/test/ with 'test1/test2'. (e.g. naively, the url would be mysite.com/test/test1/test2)urls.py: Accessing either of the following fails:mysite.com/test/test1/test2mysite.com/test/test1%2Ftest2 (%2F is the standard URL encoding for forward slash)with the below error:Error: I would expect using the path in 1 to fail, but I am surprised the path in 2 fails.What is the correct way of going about this?Using Django 2.0.2 <code>  urlpatterns = [ path('test/<test_arg>/', views.test, name='test'),] Using the URLconf defined in test.urls, Django tried these URL patterns, in this order: reader/ test/<test_arg>/ admin/The current path, test/test1/test2/, didn't match any of these.",How to pass arguments with slash in Django2 urls
True and False in python can be reassgined to False and True respectively?," I think I just witnessed something really scary and hard to digest!So in my projects I came across this beautiful piece of codefrom CoreDefaults import FALSE, TRUEAfter looking into the CoreDefaults module I saw this But then it raised a question that when python gives default True and False why would anyone evaluate the value of True and False and then assign to such variables but then I got a hunch that the only reason anyone would do was if those values can be reassigned!So I tried the following Is this behavior normal ? Is this the way it's supposed to be implemented ? Why not make it non-editable ? Is this a security risk when using python or is it the same with other languages too ? Any clever way of making it more non-editable like any builtin I need to override ? How do I fall asleep tonight ?I'm using python 2.6.6.EDIT 1 :So can we use 0 and 1 instead of False and True instead ? I think it's more fail proof though ? <code>  TRUE = 1 == 1 # some part of my mind was blown hereFALSE = 0 == 1 # which I honestly thought was clever and it really is! >>> TrueTrue>>> FalseFalse>>> True = False # True is assigned to False and it is NOT OK ?>>> TrueFalse # Python, I think it's over between you and me. ",True and False in python can be reassigned to False and True respectively?
"In Python, what does '1 and 2' mean?"," So, I was playing with the interpreter, and typed in the following: Initially I thought that it has to do with False and True values, because: But that doesn't seem related, because False is always 0, and it seems from the trials above that it isn't the biggest value that is being outputted.I can't see the pattern here honestly, and couldn't find anything in the documentation (honestly, I didn't really know how to effectively look for it). So, how does work in Python?  <code>  In [95]: 1 and 2Out[95]: 2In [96]: 1 and 5Out[96]: 5In [97]: 234324 and 2Out[97]: 2In [98]: 234324 and 22343243242Out[98]: 22343243242LIn [99]: 1 or 2 and 9Out[99]: 1 In [101]: True + TrueOut[101]: 2In [102]: True * 5Out[102]: 5 int(x) [logical operation] int(y)",How does the logical `and` operator work with integers?
How does the logical `and` operator work with integers in Python?," So, I was playing with the interpreter, and typed in the following: Initially I thought that it has to do with False and True values, because: But that doesn't seem related, because False is always 0, and it seems from the trials above that it isn't the biggest value that is being outputted.I can't see the pattern here honestly, and couldn't find anything in the documentation (honestly, I didn't really know how to effectively look for it). So, how does work in Python?  <code>  In [95]: 1 and 2Out[95]: 2In [96]: 1 and 5Out[96]: 5In [97]: 234324 and 2Out[97]: 2In [98]: 234324 and 22343243242Out[98]: 22343243242LIn [99]: 1 or 2 and 9Out[99]: 1 In [101]: True + TrueOut[101]: 2In [102]: True * 5Out[102]: 5 int(x) [logical operation] int(y)",How does the logical `and` operator work with integers?
Python Tkinter Hovering over Button -> Color change, Is there a possibility to change the background-color of a Button after hovering on it? What is the code for this in Tkinter? <code> ,Tkinter Hovering over Button -> Color change
Finding the maximum values of a column in multi-index data-frame and return it's entire values- Python," Reproducible code for the dataset: I need to find ""For each player, Find the week where his energy was maximum and display all the categories, energy values for that week""So what I did was:1.Set Player and Week as Index2.Iterate over the index to find the max value of energy and return its value Output Obtained: This obtained output is for the maximum energy in the entire dataset, I would want the maximum for each player with the all other categories and its energy for that week.Expected Output: I have tried using groupby method as suggested in the comments, The output obtained is: <code>  df = {'player' : ['a','a','a','a','a','a','a','a','a','b','b','b','b','b','b','b','b','b','c','c','c','c','c','c','c','c','c'], 'week' : ['1','1','1','2','2','2','3','3','3','1','1','1','2','2','2','3','3','3','1','1','1','2','2','2','3','3','3'], 'category': ['RES','VIT','MATCH','RES','VIT','MATCH','RES','VIT','MATCH','RES','VIT','MATCH','RES','VIT','MATCH','RES','VIT','MATCH','RES','VIT','MATCH','RES','VIT','MATCH','RES','VIT','MATCH'], 'energy' : [75,54,87,65,24,82,65,42,35,25,45,87,98,54,82,75,54,87,65,24,82,65,42,35,25,45,98] }df = pd.DataFrame(data= df)df = df[['player', 'week', 'category','energy']] df = df.set_index(['player', 'week'])for index, row in df1.iterrows(): group = df1.ix[df1['energy'].idxmax()] category energy player week b 2 RES 98 2 VIT 54 2 MATCH 82 df.groupby(['player','week'])['energy'].max().groupby(level=['player','week']) energy category player week a 1 87 VIT 2 82 VIT 3 65 VIT b 1 87 VIT 2 98 VIT 3 87 VIT c 1 82 VIT 2 65 VIT 3 98 VIT",Find the maximum values of a column in multiindex dataframe and return all its values
Reading Excel Columns on Pandas," I have been searching for a long time to find a solution to my problem.I get the data from the column I want using the below code I get the output I want to convert this time into readable time so I can input it into the x axis of a matplotlib graph. I get the error how do I get this unix time to output into a readable format for a user?I am a little new to code so if you answer, could you please explain the reasoning if you can? <code>  import pandas as pddf = pd.read_excel(""Live_data_test.xlsx"",""Sheet1"")number_of_entries = len(df.loc[:, 'Time'])number_of_entries_last_3 = number_of_entries - 3unix_x1 = df.loc[number_of_entries_last_:number_of_entries, 'Time']print(unix_x1) 10 1.513753e+0911 1.513753e+0912 1.513753e+09Name: Time, dtype: float64 real_x1 = datetime.datetime.strptime(str(unix_x1), '%Y-%m-%d %H:%M:%S') ValueError: time data '10 1.513753e+09\n11 1.513753e+09\n12 1.513753e+09\nName: Time, dtype: float64' does not match format '%Y-%m-%d %H:%M:%S'",Convert Unix epoch time to datetime in Pandas
How to show graph in Visual Studio Code itself?," When I try to run this example: I see the result in a new window.Is there any way to see the result graphs in the Visual Studio Code itself directly?Thank you. <code>  import matplotlib.pyplot as pltimport matplotlib as mplimport numpy as npx = np.linspace(0, 20, 100)plt.plot(x, np.sin(x))plt.show()",Python - How to show graph in Visual Studio Code itself?
How to show graph in Visual Studio Code itself," When I try to run this example: I see the result in a new window.Is there any way to see the result graphs in the Visual Studio Code itself directly?Thank you. <code>  import matplotlib.pyplot as pltimport matplotlib as mplimport numpy as npx = np.linspace(0, 20, 100)plt.plot(x, np.sin(x))plt.show()",Python - How to show graph in Visual Studio Code itself?
Where is the .profile file on mac," I have a MacBook Pro 13` inch (without touch bar) and it is running mac os sierra. I have installed Xcode, command line tools and homebrew. Now I want to install python 3 and make it a default interpreter when calling it from the terminal. I have found this guide http://docs.python-guide.org/en/latest/starting/install3/osx/ and I get confused when they say I should add the path to the ~./profile but I don't know where to find the file. Can somebody help with step by step guide on how to create it if it not there?Thanks <code> ",Where is the .profile file on mac?
Python - Panda Check Multiple Columns," I have a sample dataframe show as below.For each line, I want to check the c1 first, if it is not null, then check c2. By this way, find the first notnull column and store that value to column result. I am using this way for now. but I would like to know if there is a better method.(The column name do not have any pattern, this is just sample) When there are lots of columns, this method looks not good.  <code>  ID c1 c2 c3 c4 result1 a b a2 cc dd cc3 ee ff ee4 gg gg df[""result""] = np.where(df[""c1""].notnull(), df[""c1""], None)df[""result""] = np.where(df[""result""].notnull(), df[""result""], df[""c2""])df[""result""] = np.where(df[""result""].notnull(), df[""result""], df[""c3""])df[""result""] = np.where(df[""result""].notnull(), df[""result""], df[""c4""])df[""result""] = np.where(df[""result""].notnull(), df[""result""], ""unknown)",Get first non-null value per row
"creating a new, scaled, pandas column"," I have a dataframe like: I'd like to create a newly scaled column in the dataframe called SIZE where SIZE is a number between 5 and 50.For Example: I've tried but got Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.I've tried other things, such as creating a list, transforming it, and appending it back to the dataframe, among other things.What is the easiest way to do this?Thanks! <code>  TOTAL | Name3232 Jane382 Jack8291 Jones TOTAL | Name | SIZE3232 Jane 24.413382 Jack 108291 Jones 50 from sklearn.preprocessing import MinMaxScalerimport pandas as pdscaler=MinMaxScaler(feature_range=(10,50))df[""SIZE""]=scaler.fit_transform(df[""TOTAL""])",Scaling / Normalizing pandas column
Normalizing string to date in cerbrus," I'm trying to coerce string as date so it can validate date data type, but it still returns False: I have tried to use the integer and it works. I'm not sure why date conversion is not working: Any help would be appreciated. <code>  from cerberus import Validatorfrom datetime import datetimev = Validator()v.schema = {'start_date': {'type': 'date','coerce':datetime.date}}v.validate({'start_date': '2017-10-01'})>>> False v = Validator()v.schema = {'amount': {'type': 'integer','coerce': int}}v.validate({'amount': '2'})>>> True",How to coerce string to datetime in Python Cerberus?
Normalizing string to date in cerberus," I'm trying to coerce string as date so it can validate date data type, but it still returns False: I have tried to use the integer and it works. I'm not sure why date conversion is not working: Any help would be appreciated. <code>  from cerberus import Validatorfrom datetime import datetimev = Validator()v.schema = {'start_date': {'type': 'date','coerce':datetime.date}}v.validate({'start_date': '2017-10-01'})>>> False v = Validator()v.schema = {'amount': {'type': 'integer','coerce': int}}v.validate({'amount': '2'})>>> True",How to coerce string to datetime in Python Cerberus?
SQL Multirow Insert or Update by Python," I am currently executing the simply query below with python using pyodbc to insert data in SQL server table: This should work as long as there are no duplicate keys (let's assume the first column contains the key). However for data with duplicate keys (data already existing in the table) it will raise an error.How can I, in one go, insert multiple rows in a SQL server table using pyodbc such that data with duplicate keys simply gets updated.Note: There are solutions proposed for single rows of data, however, I would like to insert multiple rows at once (avoid loops)! <code>  import pyodbctable_name = 'my_table'insert_values = [(1,2,3),(2,2,4),(3,4,5)]cnxn = pyodbc.connect(...)cursor = cnxn.cursor()cursor.execute( ' '.join([ 'insert into', table_name, 'values', ','.join( [str(i) for i in insert_values] ) ]))cursor.commit()",Multi-row UPSERT (INSERT or UPDATE) from Python
Generating indices while reading csv with pandas," Here is the .csv file : where the first column must be indices like (0,1,2,3,4 ...) but due to some reasons they are zeros. Is there any way to make them normal when reading the csv file with pandas.read_csv ? i use and getting something like: and it's nearly i need, but first column (indices) is still zeros. Can pandas for example ignore this first column of zeros and automatically generate new indices to get this:  <code>  0 0 1 1 1 0 1 1 0 1 1 1 10 1 1 0 1 0 1 1 0 1 0 0 10 0 1 1 0 0 1 1 1 0 1 1 10 1 1 1 1 1 1 1 1 1 1 1 20 1 1 1 0 1 1 1 1 1 1 1 10 0 0 1 1 1 0 1 0 0 0 1 10 0 0 0 1 1 0 0 1 0 1 0 20 1 1 0 1 1 1 1 0 1 1 1 10 0 1 0 0 0 0 0 0 1 1 0 10 1 1 1 0 1 1 0 0 0 0 1 1 df = pd.read_csv(file,delimiter='\t',header=None,names=[1,2,3,4,5,6,7,8,9,10,11,12]) 1 2 3 4 5 6 7 8 9 10 11 120 0 1 1 1 0 1 1 0 1 1 1 10 1 1 0 1 0 1 1 0 1 0 0 10 0 1 1 0 0 1 1 1 0 1 1 10 1 1 1 1 1 1 1 1 1 1 1 20 1 1 1 0 1 1 1 1 1 1 1 10 0 0 1 1 1 0 1 0 0 0 1 10 0 0 0 1 1 0 0 1 0 1 0 20 1 1 0 1 1 1 1 0 1 1 1 10 0 1 0 0 0 0 0 0 1 1 0 10 1 1 1 0 1 1 0 0 0 0 1 1 0 1 2 3 4 5 6 7 8 9 10 11 120 0 1 0 1 1 0 0 0 1 1 1 0 11 0 1 0 1 1 0 0 0 1 1 1 1 22 0 1 1 1 0 0 1 1 1 1 1 1 2",Pandas read data without header or index
QScintilla how to implement fold by level feature?," I'm trying to implement the fold_by_level SublimeText3 feature on a QScintilla component but I don't know very well how to do it, so far I've come up with this code: The docs I've followed are https://www.scintilla.org/ScintillaDoc.html#Folding and http://pyqt.sourceforge.net/Docs/QScintilla2/classQsciScintilla.html.As I said, the fold_by_level feature is intended to behave exactly like SublimeText but I'm unsure about ST's feature implementation details. In any case, let me post some screenshots after testing some basic sequences on SublimeText that could clarify a bit what I'm trying to achieve here:Sequence1: {ctrl+k, ctrl+5}, {ctrl+k, ctrl+j} {ctrl+k, ctrl+4}, {ctrl+k, ctrl+j} {ctrl+k, ctrl+3}, {ctrl+k, ctrl+j} {ctrl+k, ctrl+2}, {ctrl+k, ctrl+j} {ctrl+k, ctrl+1}, {ctrl+k, ctrl+j} Sequence2: {ctrl+k, ctrl+5}, {ctrl+k, ctrl+4}, {ctrl+k, ctrl+3}, {ctrl+k, ctrl+2}, {ctrl+k, ctrl+1} I'm sure there are more inner details on SublimeText behaviour but if my example behaved exactly like posted on those shots after testing the sequences you could say the feature has become quite handy to use. <code>  import sysimport reimport mathfrom PyQt5.Qt import * # noqafrom PyQt5.Qsci import QsciScintillafrom PyQt5 import Qscifrom PyQt5.Qsci import QsciLexerCPPclass Foo(QsciScintilla): def __init__(self, parent=None): super().__init__(parent) # http://www.scintilla.org/ScintillaDoc.html#Folding self.setFolding(QsciScintilla.BoxedTreeFoldStyle) # Indentation self.setIndentationsUseTabs(False) self.setIndentationWidth(4) self.setBackspaceUnindents(True) self.setIndentationGuides(True) # Set the default font self.font = QFont() self.font.setFamily('Consolas') self.font.setFixedPitch(True) self.font.setPointSize(10) self.setFont(self.font) self.setMarginsFont(self.font) # Margin 0 is used for line numbers fontmetrics = QFontMetrics(self.font) self.setMarginsFont(self.font) self.setMarginWidth(0, fontmetrics.width(""000"") + 6) self.setMarginLineNumbers(0, True) self.setMarginsBackgroundColor(QColor(""#cccccc"")) # Indentation self.setIndentationsUseTabs(False) self.setIndentationWidth(4) self.setBackspaceUnindents(True) lexer = QsciLexerCPP() lexer.setFoldAtElse(True) lexer.setFoldComments(True) lexer.setFoldCompact(False) lexer.setFoldPreprocessor(True) self.setLexer(lexer) QShortcut(QKeySequence(""Ctrl+K, Ctrl+J""), self, lambda level=-1: self.fold_by_level(level)) QShortcut(QKeySequence(""Ctrl+K, Ctrl+1""), self, lambda level=1: self.fold_by_level(level)) QShortcut(QKeySequence(""Ctrl+K, Ctrl+2""), self, lambda level=2: self.fold_by_level(level)) QShortcut(QKeySequence(""Ctrl+K, Ctrl+3""), self, lambda level=3: self.fold_by_level(level)) QShortcut(QKeySequence(""Ctrl+K, Ctrl+4""), self, lambda level=4: self.fold_by_level(level)) QShortcut(QKeySequence(""Ctrl+K, Ctrl+5""), self, lambda level=5: self.fold_by_level(level)) def fold_by_level(self, lvl): if lvl < 0: self.foldAll(True) else: for i in range(self.lines()): level = self.SendScintilla( QsciScintilla.SCI_GETFOLDLEVEL, i) & QsciScintilla.SC_FOLDLEVELNUMBERMASK level -= 0x400 print(f""line={i+1}, level={level}"") if lvl == level: self.foldLine(i)def main(): app = QApplication(sys.argv) ex = Foo() ex.setText(""""""\#include <iostream>using namespace std;void Function0() { cout << ""Function0"";}void Function1() { cout << ""Function1"";}void Function2() { cout << ""Function2"";}void Function3() { cout << ""Function3"";}int main(void) { if (1) { if (1) { if (1) { if (1) { int yay; } } } } if (1) { if (1) { if (1) { if (1) { int yay2; } } } } return 0;}\ """""") ex.resize(800, 600) ex.show() sys.exit(app.exec_())if __name__ == ""__main__"": main()",How to implement SublimeText fold-by-level feature in QScintilla
How to list models migrations from command line?, I had run makemigrations and after that migrate to apply the migration How to find out models in boards from command line? <code>  python manage.py showmigrationsadmin [X] 0001_initial [X] 0002_logentry_remove_auto_addauth [X] 0001_initial [X] 0002_alter_permission_name_max_length [X] 0003_alter_user_email_max_length [X] 0004_alter_user_username_opts [X] 0005_alter_user_last_login_null [X] 0006_require_contenttypes_0002 [X] 0007_alter_validators_add_error_messages [X] 0008_alter_user_username_max_length [X] 0009_alter_user_last_name_max_lengthboards [X] 0001_initialcontenttypes [X] 0001_initial [X] 0002_remove_content_type_namesessions [X] 0001_initial,How to list applied migrations from command line?
size difference between A[0] and A[0:1] numpy arrays in python," I have a numpy array like this: And I do not understand what is the difference between candidates[0]: And candidates[0:1]: because I believe the two should give the exact same results? I mean the later i.e. candidates[0:1] is supposed to represent the first element only, right? So, what is the difference between the two exactly? <code>  candidates = array([[1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1], [1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0]]) candidates[0] = array([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]candidates[0].shape = (34,) candidates[0:1] = array([[1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]])candidates[0:1].shape = (1, 34)",difference between A[0] and A[0:1] numpy arrays in python
RequestsDependencyWarning: urllib3 (1.9.1) or chardet (2.3.0) doesn't match a supported version," I found several pages about this issue but none of them solved my problem.Even if I do a : I get : What I did : but as explain up, it gaves me the same error.so I did : and unistalling all his dependecies.After I reinstall it -> the same :'(I did the same for python-pip. After reinstalling it -> the same. Here are the lines about urllib3 and chardet versions needed :extract of /usr/local/lib/python2.7/dist-packages/requests/__init__.py : My versions are : I don't have no more ideas... <code>  pip show /usr/local/lib/python2.7/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.9.1) or chardet (2.3.0) doesn't match a supported version! RequestsDependencyWarning)Traceback (most recent call last): File ""/usr/bin/pip"", line 9, in <module> load_entry_point('pip==1.5.6', 'console_scripts', 'pip')() File ""/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 480, in load_entry_point return get_distribution(dist).load_entry_point(group, name) File ""/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 2691, in load_entry_point return ep.load() File ""/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 2322, in load return self.resolve() File ""/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py"", line 2328, in resolve module = __import__(self.module_name, fromlist=['__name__'], level=0) File ""/usr/lib/python2.7/dist-packages/pip/__init__.py"", line 74, in <module> from pip.vcs import git, mercurial, subversion, bazaar # noqa File ""/usr/lib/python2.7/dist-packages/pip/vcs/mercurial.py"", line 9, in <module> from pip.download import path_to_url File ""/usr/lib/python2.7/dist-packages/pip/download.py"", line 22, in <module> import requests, six File ""/usr/local/lib/python2.7/dist-packages/requests/__init__.py"", line 90, in <module> from urllib3.exceptions import DependencyWarningImportError: cannot import name DependencyWarning pip install --upgrade chardet sudo apt remove python-chardet # Check urllib3 for compatibility. major, minor, patch = urllib3_version # noqa: F811 major, minor, patch = int(major), int(minor), int(patch) # urllib3 >= 1.21.1, <= 1.22 assert major == 1 assert minor >= 21 assert minor <= 22 # Check chardet for compatibility. major, minor, patch = chardet_version.split('.')[:3] major, minor, patch = int(major), int(minor), int(patch) # chardet >= 3.0.2, < 3.1.0 assert major == 3 assert minor < 1 assert patch >= 2# Check imported dependencies for compatibility.try: check_compatibility(urllib3.__version__, chardet.__version__)except (AssertionError, ValueError): warnings.warn(""urllib3 ({0}) or chardet ({1}) doesn't match a supported "" ""version!"".format(urllib3.__version__, chardet.__version__), RequestsDependencyWarning) ii python-urllib3 1.9.1-3 all HTTP library with thread-safe connection pooling for Python ii python-chardet 2.3.0-1 all universal character encoding detector for Python2",Python (pip) - RequestsDependencyWarning: urllib3 (1.9.1) or chardet (2.3.0) doesn't match a supported version
Python - how to group functions without side effects?," I have a function with several helper functions. That's fairly common case. I want to group them in a common context for readability and I'm wondering how to do it right.they take ~15 linesonly the main function is called from somewhere elseno plans on reusing the helper functions in the near futureSimplified example: Please note that I don't want the helper methods on the module level because they belong to only one method. Imagine having several such examples as above in the same module. Maany non-public functions on module level. A mess (and this happens many times). Also I'd like to give them context and use the context's name to simplify the naming inside.Solution #0: A moduleJust put it in another module: Problems:that's too little code to add a file, especially when there are many files already (this creates a mess)this is an action and now I'm forced to create a noun-based name for the module (or break the modules naming rule). Moreover making it simple will make it too broad (in this case creating a set that really is a singleton).Finally this is just too little code to add a module for it.Solution #1: A classCreate a class with no __init__ and only one public (by naming convention) method: This is what I did many times in the past. Problems:there are no side effects, so there's no need to have the class' instancethis is an action and now I'm forced to create a noun-based name for the class (or break the classes naming rule). This leads to many of those ""managers"" or ""creators"".this is a misuse of a class concept, this is just a little execution tree with a single function-interface, not a class of things. Misusing concepts slows down understanding and may lead to further blending between uses. I know that in OOP this is common because in some languages you can't really make a function outside of a class, but this is too radical approach to order in code. Objects are useful when they are the closest expression of your idea. This isn't the case. Forcing not fitting order paradoxically generates disorder of a different kind :)Solution #2: Static classTake solution #1, add @staticmethod everywhere. Possibly also ABC metaclass. Pro: there is a clear indication that this all is instance-independent. Con: there's more code.Solution #3: Class with a __call__Take solution #1, create a __call__ function with the main method, then create on module level a single instance called create_filled_template_in_temp. Pro: calls like a single function. Con: implementation is overblown, not really fit for the purpose.Solution #4: Insert helper functions into main functionAdd them inside. Pro: this looks well if total number of lines is small and there are very few helper functions. Con: it doesn't otherwise. <code>  def create_filled_template_in_temp(path, values_mapping): template_text = path.read_text() filled_template = _fill_template(template_text, values_mapping) result_path = _save_in_temp(filled_template) return result_pathdef _fill_template(template_text, values_mapping): ...def _save_in_temp(filled_template): _, pathname = tempfile.mkstemp(suffix='.ini', text=True) path = pathlib.Path(pathname) path.write_text(text) return path...create_filled_template_in_temp(path, values_mapping) template_fillers.create_in_temp(path, values_mapping) class TemplateFillerIntoTemp: def run(self, path, values_mapping): template_text = path.read_text() filled_template = self._fill_template(template_text, values_mapping) result_path = self._save_in_temp(filled_template) return result_path def _fill_template(self, template_text, values_mapping): ... def _save_in_temp(self, filled_template): _, pathname = tempfile.mkstemp(suffix='.ini', text=True) path = pathlib.Path(pathname) path.write_text(text) return path ... TemplateFillerIntoTemp().run(path, values_mapping) TemplateFillerIntoTemp.run(path, values_mapping) create_filled_template_in_temp(path, values_mapping) def create_filled_template_in_temp(path, values_mapping): def _fill_template(template_text, values_mapping): ... def _save_in_temp(filled_template): _, pathname = tempfile.mkstemp(suffix='.ini', text=True) path = pathlib.Path(pathname) path.write_text(text) return path template_text = path.read_text() filled_template = _fill_template(template_text, values_mapping) result_path = _save_in_temp(filled_template) return result_path...create_filled_template_in_temp(path, values_mapping)",How to group functions without side effects?
Pandas Series containing array," I have a pandas series that contains an array for each element, like so: I want to extract all the first elements, put them in another Series or list and do the same for the second element.I've tried doing regular expression: and also splitting: both give nan values. What am I doing wrong? <code>  0 [0, 0]1 [12, 15]2 [43, 45]3 [9, 10]4 [0, 0]5 [3, 3]6 [0, 0]7 [0, 0]8 [0, 0]9 [3, 3]10 [2, 2] mySeries.str.extract(r'\[(\d+), (\d+)\]', expand=True) mySeries.str.split(', ').tolist())",Explode column of lists into multiple columns
Save 1 bit deep binary image in Pytohn, I have a binary image in Python and I want to save it in my pc.I need it to be a 1 bit deep png image once stored in my computer.How can I do that? I tried with both PIL and cv2 but I'm not able to save it with 1 bit depth. <code> ,Save 1 bit deep binary image in Python
OpenCV error - cv2.cvtcolor," I am trying to convert an image from BGR to grayscale format using this code: This seems to be working fine: I checked the data type of the img variable which turns out to be numpy ndarray and shape to be (100,80,3). However if I give an image of a native numpy ndarray data type with same dimensions of the input of the cvtColor function, it gives me the following error: The code for the second case is (making a custom np.ndarray over here): Can anyone clarify what is the reason for this error and how to rectify it? <code>  img = cv2.imread('path//to//image//file')gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) Error: Assertion failed (depth == 0 || depth == 2 || depth == 5) in cv::cvtColor, file D:\Build\OpenCV\opencv-3.4.1\modules\imgproc\src\color.cpp, line 11109cv2.error: OpenCV(3.4.1) D:\Build\OpenCV\opencv-3.4.1\modules\imgproc\src\color.cpp:11109: error: (-215) depth == 0 || depth == 2 || depth == 5 in function cv::cvtColor img = np.full((100,80,3), 12)gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) ",Error in OpenCV color conversion from BGR to grayscale
Pandas groupby.size vs series.value_counts vs collections.Counter," There are many questions (1, 2, 3) dealing with counting values in a single series.However, there are fewer questions looking at the best way to count combinations of two or more series. Solutions are presented (1, 2), but when and why one should use each is not discussed.Below is some benchmarking for three potential methods. I have two specific questions:Why is grouper more efficient than count? I expected count to be the more efficient, as it is implemented in C. The superior performance of grouper persists even if number of columns is increased from 2 to 4.Why does value_counter underperform grouper by so much? Is this due to the cost of constructing a list, or series from list?I understand the outputs are different, and this should also inform choice. For example, filtering by count is more efficient with contiguous numpy arrays versus a dictionary comprehension: However, the focus of my question is on performance of building comparable results in a series versus dictionary. My C knowledge is limited, yet I would appreciate any answer which can point to the logic underlying these methods.Benchmarking code Benchmarking resultsRun on python 3.6.2, pandas 0.20.3, numpy 1.13.1Machine specs: Windows 7 64-bit, Dual-Core 2.5 GHz, 4GB RAM.Key: g = grouper, v = value_counter, c = count. 1 This is not a typo. <code>  x, z = grouper(df), count(df)%timeit x[x.values > 10] # 749s%timeit {k: v for k, v in z.items() if v > 10} # 9.37ms import pandas as pdimport numpy as npfrom collections import Counternp.random.seed(0)m, n = 1000, 100000df = pd.DataFrame({'A': np.random.randint(0, m, n), 'B': np.random.randint(0, m, n)})def grouper(df): return df.groupby(['A', 'B'], sort=False).size()def value_counter(df): return pd.Series(list(zip(df.A, df.B))).value_counts(sort=False)def count(df): return Counter(zip(df.A.values, df.B.values))x = value_counter(df).to_dict()y = grouper(df).to_dict()z = count(df)assert (x == y) & (y == z), ""Dictionary mismatch!""for m, n in [(100, 10000), (1000, 10000), (100, 100000), (1000, 100000)]: df = pd.DataFrame({'A': np.random.randint(0, m, n), 'B': np.random.randint(0, m, n)}) print(m, n) %timeit grouper(df) %timeit value_counter(df) %timeit count(df) m n g v c100 10000 2.91 18.30 8.411000 10000 4.10 27.20 6.98[1]100 100000 17.90 130.00 84.501000 100000 43.90 309.00 93.50",Pandas groupby.size vs series.value_counts vs collections.Counter with multiple series
How to find cluster centroid with Schikit-learn," I have a data set with (labeled) clusters. I'm trying to find the centroids of each cluster (a vector that his distance is the smallest from all data points of the cluster).I found many solutions to perform clustering and only then find the centroids, but I didn't find yet for existing ones. Python schikit-learn is preferred. Thanks. <code> ",How to find cluster centroid with Scikit-learn
what is the quickest way to increment date string YYYY-MM-DD in python?," In Pandas, I am using dates with string format YYYY-MM-DD What is the quickest way to increment the date with the result in YYYY-MM-DD format? I want to increment it by 1 and get the result back as a string: <code>  d1 = '2018-02-10' d1_inc = '2018-02-11'",What is the quickest way to increment date string YYYY-MM-DD in Python?
Pandas - day month swapping," I wrote a code that reads multiple files, however on some of my files datetime swaps day & month whenever the day is less than 13, and any day that is from day 13 or above i.e. 13/06/11 remains correct (DD/MM/YY). I tried to fix it by doing this,but it doesn't work.My data frame looks like this:The actual datetime is from 12june2015 to 13june2015when my I read my datetime column as a string the dates remain correct dd/mm/yyyy but when I change the type of my column to datetime column it swaps my day and month for each day that is less than 13.output: Here is my code :I loop through files : then when my code finish reading all my files I concatenat them, the problem is that my datetime column needs to be in a datetime type so when I change its type by pd_datetime() it swaps the day and month when the day is less than 13.Post converting my datetime column the dates are correct (string type) But when I change the column type I get this: The question is : What command should i use or change in order to stop day and month swapping when the day is less than 13?UPDATEThis command swaps all the days and months of my column So in order to swap only the incorrect dates, I wrote a condition: But it doesn't work either <code>  tmp p1 p2 11/06/2015 00:56:55.060 0 111/06/2015 04:16:38.060 0 112/06/2015 16:13:30.060 0 112/06/2015 21:24:03.060 0 113/06/2015 02:31:44.060 0 113/06/2015 02:37:49.060 0 1 print(df)tmp p1 p2 06/11/2015 00:56:55 0 106/11/2015 04:16:38 0 106/12/2015 16:13:30 0 106/12/2015 21:24:03 0 113/06/2015 02:31:44 0 113/06/2015 02:37:49 0 1 df = pd.read_csv(PATH+file, header = None,error_bad_lines=False , sep = '\t') print(tmp) # as a result I get 11.06.2015 12:56:05 (11june2015) tmp = pd.to_datetime(tmp, unit = ""ns"")tmp = temps_absolu.apply(lambda x: x.replace(microsecond=0))print(tmp) # I get 06-11-2016 12:56:05 (06november2015 its not the right date) tmp = pd.to_datetime(tmp, unit='s').dt.strftime('%#m/%#d/%Y %H:%M:%S') for t in tmp: if (t.day < 13): t = datetime(year=t.year, month=t.day, day=t.month, hour=t.hour, minute=t.minute, second = t.second)",Python Pandas : pandas.to_datetime() is switching day & month when day is less than 13
Pandas - day & month swapping when day is less than 13," I wrote a code that reads multiple files, however on some of my files datetime swaps day & month whenever the day is less than 13, and any day that is from day 13 or above i.e. 13/06/11 remains correct (DD/MM/YY). I tried to fix it by doing this,but it doesn't work.My data frame looks like this:The actual datetime is from 12june2015 to 13june2015when my I read my datetime column as a string the dates remain correct dd/mm/yyyy but when I change the type of my column to datetime column it swaps my day and month for each day that is less than 13.output: Here is my code :I loop through files : then when my code finish reading all my files I concatenat them, the problem is that my datetime column needs to be in a datetime type so when I change its type by pd_datetime() it swaps the day and month when the day is less than 13.Post converting my datetime column the dates are correct (string type) But when I change the column type I get this: The question is : What command should i use or change in order to stop day and month swapping when the day is less than 13?UPDATEThis command swaps all the days and months of my column So in order to swap only the incorrect dates, I wrote a condition: But it doesn't work either <code>  tmp p1 p2 11/06/2015 00:56:55.060 0 111/06/2015 04:16:38.060 0 112/06/2015 16:13:30.060 0 112/06/2015 21:24:03.060 0 113/06/2015 02:31:44.060 0 113/06/2015 02:37:49.060 0 1 print(df)tmp p1 p2 06/11/2015 00:56:55 0 106/11/2015 04:16:38 0 106/12/2015 16:13:30 0 106/12/2015 21:24:03 0 113/06/2015 02:31:44 0 113/06/2015 02:37:49 0 1 df = pd.read_csv(PATH+file, header = None,error_bad_lines=False , sep = '\t') print(tmp) # as a result I get 11.06.2015 12:56:05 (11june2015) tmp = pd.to_datetime(tmp, unit = ""ns"")tmp = temps_absolu.apply(lambda x: x.replace(microsecond=0))print(tmp) # I get 06-11-2016 12:56:05 (06november2015 its not the right date) tmp = pd.to_datetime(tmp, unit='s').dt.strftime('%#m/%#d/%Y %H:%M:%S') for t in tmp: if (t.day < 13): t = datetime(year=t.year, month=t.day, day=t.month, hour=t.hour, minute=t.minute, second = t.second)",Python Pandas : pandas.to_datetime() is switching day & month when day is less than 13
Pandas - datetime swapping day & month when day is less than 13," I wrote a code that reads multiple files, however on some of my files datetime swaps day & month whenever the day is less than 13, and any day that is from day 13 or above i.e. 13/06/11 remains correct (DD/MM/YY). I tried to fix it by doing this,but it doesn't work.My data frame looks like this:The actual datetime is from 12june2015 to 13june2015when my I read my datetime column as a string the dates remain correct dd/mm/yyyy but when I change the type of my column to datetime column it swaps my day and month for each day that is less than 13.output: Here is my code :I loop through files : then when my code finish reading all my files I concatenat them, the problem is that my datetime column needs to be in a datetime type so when I change its type by pd_datetime() it swaps the day and month when the day is less than 13.Post converting my datetime column the dates are correct (string type) But when I change the column type I get this: The question is : What command should i use or change in order to stop day and month swapping when the day is less than 13?UPDATEThis command swaps all the days and months of my column So in order to swap only the incorrect dates, I wrote a condition: But it doesn't work either <code>  tmp p1 p2 11/06/2015 00:56:55.060 0 111/06/2015 04:16:38.060 0 112/06/2015 16:13:30.060 0 112/06/2015 21:24:03.060 0 113/06/2015 02:31:44.060 0 113/06/2015 02:37:49.060 0 1 print(df)tmp p1 p2 06/11/2015 00:56:55 0 106/11/2015 04:16:38 0 106/12/2015 16:13:30 0 106/12/2015 21:24:03 0 113/06/2015 02:31:44 0 113/06/2015 02:37:49 0 1 df = pd.read_csv(PATH+file, header = None,error_bad_lines=False , sep = '\t') print(tmp) # as a result I get 11.06.2015 12:56:05 (11june2015) tmp = pd.to_datetime(tmp, unit = ""ns"")tmp = temps_absolu.apply(lambda x: x.replace(microsecond=0))print(tmp) # I get 06-11-2016 12:56:05 (06november2015 its not the right date) tmp = pd.to_datetime(tmp, unit='s').dt.strftime('%#m/%#d/%Y %H:%M:%S') for t in tmp: if (t.day < 13): t = datetime(year=t.year, month=t.day, day=t.month, hour=t.hour, minute=t.minute, second = t.second)",Python Pandas : pandas.to_datetime() is switching day & month when day is less than 13
Python Pandas : pd.to_datetime() is swapping day & month when day is less than 13," I wrote a code that reads multiple files, however on some of my files datetime swaps day & month whenever the day is less than 13, and any day that is from day 13 or above i.e. 13/06/11 remains correct (DD/MM/YY). I tried to fix it by doing this,but it doesn't work.My data frame looks like this:The actual datetime is from 12june2015 to 13june2015when my I read my datetime column as a string the dates remain correct dd/mm/yyyy but when I change the type of my column to datetime column it swaps my day and month for each day that is less than 13.output: Here is my code :I loop through files : then when my code finish reading all my files I concatenat them, the problem is that my datetime column needs to be in a datetime type so when I change its type by pd_datetime() it swaps the day and month when the day is less than 13.Post converting my datetime column the dates are correct (string type) But when I change the column type I get this: The question is : What command should i use or change in order to stop day and month swapping when the day is less than 13?UPDATEThis command swaps all the days and months of my column So in order to swap only the incorrect dates, I wrote a condition: But it doesn't work either <code>  tmp p1 p2 11/06/2015 00:56:55.060 0 111/06/2015 04:16:38.060 0 112/06/2015 16:13:30.060 0 112/06/2015 21:24:03.060 0 113/06/2015 02:31:44.060 0 113/06/2015 02:37:49.060 0 1 print(df)tmp p1 p2 06/11/2015 00:56:55 0 106/11/2015 04:16:38 0 106/12/2015 16:13:30 0 106/12/2015 21:24:03 0 113/06/2015 02:31:44 0 113/06/2015 02:37:49 0 1 df = pd.read_csv(PATH+file, header = None,error_bad_lines=False , sep = '\t') print(tmp) # as a result I get 11.06.2015 12:56:05 (11june2015) tmp = pd.to_datetime(tmp, unit = ""ns"")tmp = temps_absolu.apply(lambda x: x.replace(microsecond=0))print(tmp) # I get 06-11-2016 12:56:05 (06november2015 its not the right date) tmp = pd.to_datetime(tmp, unit='s').dt.strftime('%#m/%#d/%Y %H:%M:%S') for t in tmp: if (t.day < 13): t = datetime(year=t.year, month=t.day, day=t.month, hour=t.hour, minute=t.minute, second = t.second)",Python Pandas : pandas.to_datetime() is switching day & month when day is less than 13
how to apply a function to each row of the dataframe and write the output back to the datafram?," What is a more elegant way of implementing below?I want to apply a function: my_function to a dataframe where each row of the dataframe contains the parameters of the function. Then I want to write the output of the function back to the dataframe row. <code>  results = pd.DataFrame()for row in input_panel.iterrows(): (index, row_contents) = row row_contents['target'] = my_function(*list(row_contents)) results = pd.concat([results, row_contents])",apply a function to each row of the dataframe
How to covert 1d array to 2d," Is there any bulid-in function in python/numpy to convert an array = [1, 3, 1, 2] to something like this: <code>  array = [[0, 1, 0, 0], [0, 0, 0, 1], [0, 1, 0, 0], [0, 0, 1, 0]]",How to covert 1d array to Logical matrix
Shopify Multiple Pictures upload with Python API," I am trying to add more than one picture on Shopify with the Python API however, I am not able to upload 2 pictures to one product. At this time only one picture is being uploaded. How I can add more than 1 picture to Shopify API? <code>  import shopify API_KEY = 'dsfsdsdsdsdsad'PASSWORD = 'sadsdasdasdas'shop_url = ""https://%s:%s@teststore.myshopify.com/admin"" % (API_KEY, PASSWORD)shopify.ShopifyResource.set_site(shop_url)path = ""audi.jpg""path2 = ""audi2.jpg""new_product = shopify.Product()new_product.title = ""Audi pictures test ""new_product.body_html = ""body of the page <br/><br/> test <br/> test""variant = shopify.Variant({'price': 1.00, 'requires_shipping': False,'sku':'000007'})new_product.variants = [variant]image = shopify.Image()image2 = shopify.Image()with open(path, ""rb"") as f: filename = path.split(""/"")[-1:][0] filename2 = path2.split(""/"")[-1:][0] encoded = f.read() image.attach_image(encoded, filename=filename) image2.attach_image(encoded, filename=filename2)new_product.images = [image,image2]new_product.save()",Shopify API Python Multiple Pictures upload with Python API
How to specify options and arguements dynamically," I'd like to load arguments and options from a database. I am allowing users to define their own options and arguments. Users can call a remote api on the command line. They specify the URL and parameters to the end point. Here is what the data from database looks like These parameters align with what the remote end point needs. Each API endpoint has different parameters hence the reason they need to be dynamic. Here is how the command line will look. The param and value will be mapped in a URL. Is there a way to setup dynamic parameters in click? <code>  [ { ""name"": ""thename1"", ""short"": ""a"", ""long"": ""ace"" ""type"": ""string"", ""required"": false }, { ""name"": ""thename2"", ""short"": ""b"", ""long"": ""bravo"" ""type"": ""number"", ""required"": true }, { ""name"": ""thename3"", ""short"": ""c"", ""long"": ""candy"" ""type"": ""array"", ""required"": true }] command run www.mysite.com/api/get -a testparam --bravo testpara2 -c item1 item2",Specify options and arguments dynamically
How to write an algorithm that calculates 'initial lists' in O(m*log m)?," Goal is to write an algorithm that calculates 'initial lists' (a data-structure) in a complexity class better than O(m^2)What are initial list?Let U be a set of tuples (for example {(2,5), (5,1), (9,0), (6,4)} ).Step 1: L1 is ordered by the first element of the tuple: and L2 by the second: Step 2:Add the indices of tuple e in the second list to the tuple e in the first list: and the other way: L1 and L2 are called the initial lists of U now.The first implementation idea of course is an exhaustive algorithm in O(m^2) So this works. But calling index ( O(m) ) in a for-loop ( O(m) ) makes its complexity quadratic. But how to write an algorithm for that in O(m*log m)? <code>  L1 = [ (2,5), (5,1), (6,4), (9,0) ] L2 = [ (9,0), (5,1), (6,4), (2,5) ] L1 = [ (2,5,3), (5,1,1), (6,4,2), (9,0,0) ] L2 = [ (9,0,3), (5,1,1), (6,4,2), (2,5,0) ] U = {(2,5), (5,1), (9,0), (6,4)}m = len(U)#step 1:L1 = [e for e in U]L1.sort()L2 = [e for e in U]L2.sort(key=lambda tup: tup[1])#step 2:help = []*len(L1)for i in range(len(L1)): help[i] = L1[i][0], L1[i][1], L2.index(L1[i])for i in range(len(L2)): L2[i] = L2[i][0], L2[i][1], L1.index(L2[i])L1 = help# >>> L1# [(2, 5, 3), (5, 1, 1), (6, 4, 2), (9, 0, 0)]# >>> L2# [(9, 0, 3), (5, 1, 1), (6, 4, 2), (2, 5, 0)]",Algorithm to calculate 'initial lists' in O(m*log m)
Slicing a vertex in C++," Is there an equivalent of list slicing [1:] from Python in C++ with vectors? I simply want to get all but the first element from a vector.Python's list slicing operator: C++ Desired result: <code>  list1 = [1, 2, 3]list2 = list1[1:] print(list2) # [2, 3] std::vector<int> v1 = {1, 2, 3};std::vector<int> v2;v2 = v1[1:];std::cout << v2 << std::endl; //{2, 3}",Slicing a vector in C++
I am getting value error on using roc_auc_score from sklearn?," I am using logistic regression for prediction. My predictions are 0's and 1's. After training my model on given data and also when training on important features i.e X_important_train see screenshot. I am getting score around 70% but when I use roc_auc_score(X,y) or roc_auc_score(X_important_train, y_train) I am getting value error: ValueError: multiclass-multioutput format is not supportedCode: Screenshot: <code>  # Load librariesfrom sklearn.linear_model import LogisticRegressionfrom sklearn import datasetsfrom sklearn.preprocessing import StandardScalerfrom sklearn.metrics import roc_auc_score# Standarize featuresscaler = StandardScaler()X_std = scaler.fit_transform(X)# Train the model using the training sets and check scoremodel.fit(X, y)model.score(X, y)model.fit(X_important_train, y_train)model.score(X_important_train, y_train)roc_auc_score(X_important_train, y_train)",ValueError: multiclass-multioutput format is not supported using sklearn roc_auc_score function
"Python, Bokeh ; ValueError('window must be an integer',)"," I seem to be having this issue with Pandas code inside a Bokeh callback.Here's part of the output before the error. My dataframe seems normal and I'm not sure why it's upset And here's the code I changed from the flask embed example (link here): I can also include the full code if that help, but the main change I have is just a doc.add_periodic_callback() that fetches new data periodically. <code>  time temperature0 2016-03-17 11:00:00 4.6761 2016-03-17 11:30:00 4.6332 2016-03-17 12:00:00 4.6393 2016-03-17 12:30:00 4.6034 2016-03-17 13:00:00 4.6155 2016-03-17 13:30:00 4.6506 2016-03-17 14:00:00 4.6787 2016-03-17 14:30:00 4.6988 2016-03-17 15:00:00 4.7539 2016-03-17 15:30:00 4.847ERROR:bokeh.server.protocol_handler:error handling message Message 'PATCH-DOC' (revision 1): ValueError('window must be an integer',) def callback(attr, old, new): df = pd.DataFrame.from_dict(source.data.copy()) print df[:10] if new == 0: data = df else: data = df.rolling('{0}D'.format(new)).mean() source.data = ColumnDataSource(data=data).data slider = Slider(start=0, end=30, value=0, step=1, title=""Smoothing by N Days"") slider.on_change('value', callback)","Python, Pandas ; ValueError('window must be an integer',)"
Django: overriding form's method to set queryset not working?," I want the queryset of my coin field to change when a user selects ""Sell"" in the ""BuySell"" dropdown option with jquery. Once the dropdown is changed I send a Get Request with AJAX, pick that request up in my view and then reload the form, which is where I override the default coin field queryset in my TransactionForm's init method.This isn't working as expected, nothing happens to change the coin dropdown options and I get no errors (including in the Network tab when I inspect element).I wonder if this is something to do with the way I'm calling my form here: and the form init method: FULL CODE: Forms Views snippet jquery <code>  form = TransactionForm(user = request.user, coin_price = GetCoin(""Bitcoin"").price) def __init__(self, coin_price = None, user = None, *args, **kwargs): super(TransactionForm, self).__init__(*args, **kwargs) if user: self.user = user qs_coin = Portfolio.objects.filter(user = self.user).values('coin').distinct() print(""qs_coin test: {}"".format(qs_coin)) self.fields['coin'].queryset = qs_coin class TransactionForm(forms.ModelForm): CHOICES = (('Buy', 'Buy'), ('Sell', 'Sell'),) coin = forms.ModelChoiceField(queryset = Coin.objects.all()) buysell = forms.ChoiceField(choices = CHOICES) field_order = ['buysell', 'coin', 'amount', 'trade_price'] class Meta: model = Transaction fields = {'buysell', 'coin', 'amount', 'trade_price'} def __init__(self, coin_price = None, user = None, *args, **kwargs): super(TransactionForm, self).__init__(*args, **kwargs) print(""Transaction form init: "", user, coin_price) if user: self.user = user qs_coin = Portfolio.objects.filter(user = self.user).values('coin').distinct() print(""qs_coin test: {}"".format(qs_coin)) self.fields['coin'].queryset = qs_coin def add_transaction(request): if request.method == ""GET"": if request.is_ajax(): print(""ajax test"") #print(request.GET.get) print(request.GET.get('coin')) print(request.GET.get('buysell')) view_coin = None if request.GET.get('coin'): view_coin = GetCoin(request.GET.get('coin')).price data = { 'view_buysell': request.GET.get('buysell'), 'view_coin': request.GET.get('coin'), 'view_amount': ""test"", 'view_price': view_coin } form = TransactionForm(user = request.user, coin_price = GetCoin(""Bitcoin"").price) return JsonResponse(data) $('#id_buysell').on('change', function(){ console.log(""buysell""); var $id_buysell = $('#id_buysell').val(); console.log($id_buysell); $.ajax({ method: ""GET"", url: ""/myportfolio/add_transaction"", dataType: 'json', data: { buysell: $id_buysell }, success: function(data, status) { console.log(""SUCCESS:""); console.log(data); console.log(data['view_buysell']); }, error: function(response) { } }); });$('#id_coin').on('change', function(){ console.log(""test"") console.log(""coin change"") var $id_coin = $('#id_coin').find(""option:selected"").text(); console.log($id_coin); $.ajax({ method: ""GET"", url: ""/myportfolio/add_transaction"", dataType: 'json', data: {coin: $id_coin}, success: function(data, status) { console.log(""SUCCESS:""); console.log(data); console.log(data['view_buysell']); $(""#id_trade_price"").val(data['view_price']); }, error: function(response) { } });","Django: How to change form field select options with different Querysets, based on other form field options selected?"
How to plot aggregated by date pandas dataframe?," I have this dataframe To aggregate payout_value by date I use: How do I plot (bar chart) dates on x-axis and aggregated payout sum on y axis? I tried using df.plot(x='date', y='payout_value',kind=""bar"") approach, but there is no 'date' column in df_daily dataframe, print(list(df_daily)) gives [('payout_value', 'sum')] <code>  df=pd.DataFrame([[""2017-01-14"",1], [""2017-01-14"",30], [""2017-01-16"",216], [""2017-02-17"",23], [""2017-02-17"",2], [""2017-03-19"",745], [""2017-03-19"",32], [""2017-03-20"",11], [""2017-03-20"",222], [""2017-03-21"",4]],columns=[""date"",""payout_value""]) df_daily=df.groupby('date').agg(['sum'])payout_valuesumdate 2017-01-14 312017-01-16 2162017-02-17 252017-03-19 7772017-03-20 2332017-03-21 4",How to plot aggregated by date pandas dataframe
Pandas filtering DataFrame by values in a certain column," I am trying to get a DataFrame from an existing DataFrame containing only the rows where values in a certain column(whose values are strings) do not contain a certain character.i.e. If the character we don't want is a '('Original dataframe: New dataframe: I have tried df.loc['(' not in df['my_column']], but this does not work since df['my_column'] is a Series object.I have also tried: df.loc[not df.my_column.str.contains('(')], which also does not work. <code>  some_col my_column0 1 some1 2 word2 3 hello( some_col my_column0 1 some1 2 word",Filtering out rows with non-alphanumeric characters
Modify the composition of a batch in Tensorflow," Is there a way to modify the composition of my images within a batch? At the moment, when I'm creating e.g. a batch with the size of 4, my batches will look like that:Batch1: [Img0 Img1 Img2 Img3]Batch2: [Img4 Img5 Img6 Img7]I need to modify the composition of my batches so that it will only shift once to the next image. Then it should look like that:Batch1: [Img0 Img1 Img2 Img3]Batch2: [Img1 Img2 Img3 Img4]Batch3: [Img2 Img3 Img4 Img5]Batch4: [Img3 Img4 Img5 Img6]Batch5: [Img4 Img5 Img6 Img7]I'm using in my code the Dataset API of Tensorflow which looks as follows: <code>  def tfrecords_train_input(input_dir, examples, epochs, nsensors, past, future, features, batch_size, threads, shuffle, record_type): filenames = sorted( [os.path.join(input_dir, f) for f in os.listdir(input_dir)]) num_records = 0 for fn in filenames: for _ in tf.python_io.tf_record_iterator(fn): num_records += 1 print(""Number of files to use:"", len(filenames), ""/ Total records to use:"", num_records) dataset = tf.data.TFRecordDataset(filenames) # Parse records read_proto = partial(record_type().read_proto, nsensors=nsensors, past=past, future=future, features=features) # Parallelize Data Transformation on available GPU dataset = dataset.map(map_func=read_proto, num_parallel_calls=threads) # Cache data dataset = dataset.cache() # repeat after shuffling dataset = dataset.repeat(epochs) # Batch data dataset = dataset.batch(batch_size) # Efficient Pipelining dataset = dataset.prefetch(2) iterator = dataset.make_one_shot_iterator() return iterator",Sliding window of a batch in Tensorflow using Dataset API
determine from which file a function is called in python," I am programmatically printing out a list of function in python.I can get the name from name How to get the source filename where the function is defined as well?and in case the function it is attribute of a object, how to get the type of parent object?portability python2/3 is a must <code>  for ifunc,func in enumerate(list_of_functions): print(str(ifunc)+func.__name__)",determine from which file a function is defined in python
PCapy Instalationg," I tried to install pcapy using pip install pcapy, but I encoutered an error stating that the file pcap.h does not exist as following: I tried to upgrade setuptools but I got the same result. I tried to install libcap by running pip install libcap but I also got the same problem. How can I fix this problem? <code>  Installing collected packages: pcapy Running setup.py install for pcapy ... error Complete output from command c:\python27\python.exe -u -c ""import setuptoolstokenize;__file__='c:\\users\\username\\appdata\\local\\temp\\pip-install-1tykyr\\pcapy\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record c:\users\username\appdata\local\temp\pip-record-u_q6qm\install-record.txt --single-version-externally-managed --compile: running install running build running build_ext building 'pcapy' extension creating build creating build\temp.win-amd64-2.7 creating build\temp.win-amd64-2.7\Release creating build\temp.win-amd64-2.7\Release\win32 C:\Users\UserName\AppData\Local\Programs\Common\Microsoft\Visual C++ forPython\9.0\VC\Bin\amd64\cl.exe /c /nologo /Ox /MD /W3 /GS- /DNDEBUG -DWIN32=1 -Ic:\wpdpack\Include -Ic:\python27\include -Ic:\python27\PC /Tppcapdumper.cc /Fobuild\temp.win-amd64-2.7\Release\pcapdumper.obj pcapdumper.cc pcapdumper.cc(11) : fatal error C1083: Cannot open include file: 'pcap.h': No such file or directory error: command 'C:\\Users\\UserName\\AppData\\Local\\Programs\\Common\\Microsoft\\Visual C++ for Python\\9.0\\VC\\Bin\\amd64\\cl.exe' failed with exit status 2Command ""c:\python27\python.exe -u -c ""import setuptools, tokenize;__file__='c:\\users\\username\\appdata\\local\\temp\\pip-install-1tykyr\\pcapy\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record c:\users\username\appdata\local\temp\pip-record-u_q6qm\install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in c:\users\username\appdata\local\temp\pip-install-1tykyr\pcapy\",pip install pcapy cannot open include file 'pcap.h'
`shutil.rmtree` does not work on `templib.TemporaryDirectory()`," Consider this test In tearDown however an error is raised which refers to self.test_dir.name.According to the source code for tempfile, both elements are the same. And I'm not using it within a context, so __exit__() shouldn't be called as far as I understand.What is happening? <code>  import shutil, tempfilefrom os import pathimport unittestfrom pathlib import Pathclass TestExample(unittest.TestCase): def setUp(self): # Create a temporary directory self.test_dir = tempfile.TemporaryDirectory() self.test_dir2 = tempfile.mkdtemp() def tearDown(self): # Remove the directory after the test shutil.rmtree(self.test_dir2) shutil.rmtree(self.test_dir.name) #throws error def test_something(self): self.assertTrue(Path(self.test_dir.name).is_dir()) self.assertTrue(Path(self.test_dir2).is_dir())if __name__ == '__main__': unittest.main() FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmpxz7ts7a7' def __init__(self, suffix=None, prefix=None, dir=None): self.name = mkdtemp(suffix, prefix, dir) self._finalizer = _weakref.finalize( self, self._cleanup, self.name, warn_message=""Implicitly cleaning up {!r}"".format(self))",`shutil.rmtree` does not work on `tempfile.TemporaryDirectory()`
"Python numpy array: select N evenly spaced out elements in array, including first and last"," I have an array of arbitrary length, and I want to select N elements of it, evenly spaced out (approximately, as N may be even, array length may be prime, etc) that includes the very first arr[0] element and the very last arr[len-1] element.Example: Then I want to make a function like the following to grab numElems evenly spaced out within the array, which must include the first and last element: Does this make sense?I've tried arr[0:len:numElems] (i.e. using the array start:stop:skip notation) and some slight variations, but I'm not getting what I'm looking for here: or I don't care exactly what the middle elements are, as long as they're spaced evenly apart, off by an index of 1 let's say. But getting the right number of elements, including the index zero and last index, are critical. <code>  >>> arr = np.arange(17)>>> arrarray([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]) GetSpacedElements(numElems = 4)>>> returns 0, 5, 11, 16 >>> arr[0:len:numElems]array([ 0, 4, 8, 12, 16]) >>> arr[0:len:numElems+1]array([ 0, 5, 10, 15])","Select N evenly spaced out elements in array, including first and last"
Why do python module names have some uppercase letters but are always imported in lowercase, Whenever I see references to modules like SciPy and NumPy the first letter of each part is capitalized. However they must be imported with all lowercase letters. Why is there this difference? <code> ,Why do python module names have some uppercase letters but are always imported in lowercase?
Python - Include submodules on click," I am trying to make a kind of recursive call on my first Click CLI app.The main point is to have sub-commands associated to the first and, so, I was trying to separate it all in different files/modules to improve it's maintainability.I have the current directory: This is my main file: My projects __init__ file: My commands.project.command1 file: The main point here is that, every time I want to add a new subcommand, I need to:Add .py file with all code to the command, in respective subcommand/submodule folder (obviously!)Add it's import statement on it's __init__ fileRelate this new command to it's parent (project/database, in this case)Is there any way to do a circular/dynamic load to avoid step no.2 and 3?EDITAfter tried Stephen Rauch way, it successfully includes all provided files, but none of the commands works with - only with function name (eg: update-project -> update_project). main.py commands/project/install_project.py CLI result main project --help (note the install_project instead install-project sub command) <code>  root|-commands|-project|---__init__|---command1|---command2|-database|---__init__|---command1|---command2 import clickfrom commands.project import projectfrom commands.database import database@click.group(help=""Main command"")def main(): passmain.add_command(project)main.add_command(database) from commands.project.command1 import *from commands.project.command2 import *import click@click.group(help=""Projects"")def project(): passproject.add_command(command1)project.add_command(command2) import click@click.command()def command1(): """""" Execute all the steps required to update the project. """""" pass root|-commands|-project|---update|---install_project|-database|---command_one|---command_two # main command ----------------------------------------------------------- ###@click.group(help=""CLI tool!"", context_settings=dict(max_content_width=120))def main(): pass# PROJECT command group -------------------------------------------------------- ###@main.group(cls=group_from_folder(""commands/project""), short_help=""Project installation and upgrade utils."", help=""Project installation and upgrade."")def project(): pass import click @click.command(name=""install-project"", help=""This options allows you to easily install project"", short_help=""Install a brand new project"")@click.pass_contextdef install_project(ctx): Usage: main project [OPTIONS] COMMAND [ARGS]... Project installation and upgrade.Options: --help Show this message and exit.Commands: install_project Install a brand new project one",Include submodules on click
what does data.norm() < 1000 do in pytorch?," I am following the PyTorch tutorial here.It says that Could someone explain what data.norm() does here?When I change .randn to .ones its output is tensor([ 1024., 1024., 1024.]). <code>  x = torch.randn(3, requires_grad=True)y = x * 2while y.data.norm() < 1000: y = y * 2print(y)Out: tensor([-590.4467, 97.6760, 921.0221])",What does data.norm() < 1000 do in PyTorch?
Amazon lamda does not show python logs, My API(Python) is deployed on Amazon Lambda. The problem is when I request my API I get the internal server error. I can tail the Lambda logs but I don't see the actual error or stack trace where the code crashed. When I tail the logs I just get the following output. How can I see the actual stack trace of my python api for debugging? <code>  START RequestId: 62341bgd-6231-11e8-8c5b-25793532a32u Version: $LATESTEND RequestId: 62341b0d-6231-1128-8r5b-2b793032a3edREPORT RequestId: 6234te0b-6rte-aaa8-au5a-21t93132r3rt Duration: 0.46 ms,Amazon lambda does not show python logs
"python cv2 putText (chinese texts) draw messy code in image, but normal when use English text"," I use python cv2(window10, python2.7) to write text in image, when the text is English it works, but when I use Chinese text it write messy code in the image.Below is my code: When text = ""Hello world"" # just work, below is the output image:When text = """" # Chinese text, draw messy text in the image, below is the output image:What's wrong? Does opencv putText don't support other language text? <code>  # coding=utf-8import cv2import numpy as nptext = ""Hello world"" # just work# text = """" # messy text in the imagecv2.putText(img, text, cord, font, fontScale, fontColor, lineType)# Display the imagecv2.imshow(""img"", img)cv2.waitKey(0)cv2.destroyAllWindows()",How to draw Chinese text on the image using `cv2.putText`correctly? (Python+OpenCV)
"Different between `Series.str.contains(""|"")` and `Series.apply(lambda x:""|"" in x)` in pandas?"," This is the code for testing: After executing the code above, you will get this three output: print(a.apply(lambda x:""|"" in x)) output is: print(a) output is:You will see in 7 and 8 in Series a do not have |. However the return of print(a.str.contains(""|"")) is all True. What is wrong here? <code>  import numpy as np # maybe you should download the packageimport pandas as pd # maybe you should download the packagedata = ['Romance|Fantasy|Family|Drama', 'War|Adventure|Science Fiction', 'Action|Family|Science Fiction|Adventure|Mystery', 'Action|Drama', 'Action|Drama|Thriller', 'Drama|Romance', 'Comedy|Drama', 'Action', 'Comedy', 'Crime|Comedy|Action|Adventure', 'Drama|Thriller|History', 'Action|Science Fiction|Thriller']a = pd.Series(data)print(a.str.contains(""|""))print(a.apply(lambda x:""|"" in x))print(a) 0 True1 True2 True3 True4 True5 True6 True7 True8 True9 True10 True11 Truedtype: bool 0 True1 True2 True3 True4 True5 True6 True7 False8 False9 True10 True11 Truedtype: bool","Difference between `Series.str.contains(""|"")` and `Series.apply(lambda x:""|"" in x)` in pandas?"
Opencv : How to correctly use cv2.approxPolyDP() to remove all the rounded rectangle boxes?," I am having a problem regarding the kernel size for morphologyEx. I have some captcha images and I want to do the same operation on them and get the same final result.code : The perfect result would beInput:Output:But the problem appears when I apply it to other images with the same structure output is distorted.Example 1 :Input:Output:Example 2 :Input:Output:Example 3:Input:Output: <code>  image = cv2.imread(""Image.jpg"")gray = cv2.cvtColor(image , cv2.COLOR_BGR2GRAY)ret, thresh1 = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)k1 = np.ones((3,3))k2 = np.ones((5,5))bottom_image = cv2.morphologyEx(thresh1, cv2.MORPH_CLOSE, k1)bottom_image = 255-bottom_imagebottom_image = remove_boxes(bottom_image , True)ret,thresh2 = cv2.threshold(bottom_image,127,255,cv2.THRESH_BINARY_INV)opening = cv2.morphologyEx(thresh2, cv2.MORPH_OPEN, k1)#closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, k)# cv2.imshow('opening', opening)dilate = cv2.morphologyEx(opening, cv2.MORPH_DILATE, k2)dilate = cv2.bitwise_not(dilate)# cv2.imshow('dilation', dilate)bottom_image = cv2.morphologyEx(bottom_image, cv2.MORPH_CLOSE, k1)",OpenCV: How to correctly apply morphologyEx operation?
Opencv : How to correctly apply morphologyEx operation ?," I am having a problem regarding the kernel size for morphologyEx. I have some captcha images and I want to do the same operation on them and get the same final result.code : The perfect result would beInput:Output:But the problem appears when I apply it to other images with the same structure output is distorted.Example 1 :Input:Output:Example 2 :Input:Output:Example 3:Input:Output: <code>  image = cv2.imread(""Image.jpg"")gray = cv2.cvtColor(image , cv2.COLOR_BGR2GRAY)ret, thresh1 = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)k1 = np.ones((3,3))k2 = np.ones((5,5))bottom_image = cv2.morphologyEx(thresh1, cv2.MORPH_CLOSE, k1)bottom_image = 255-bottom_imagebottom_image = remove_boxes(bottom_image , True)ret,thresh2 = cv2.threshold(bottom_image,127,255,cv2.THRESH_BINARY_INV)opening = cv2.morphologyEx(thresh2, cv2.MORPH_OPEN, k1)#closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, k)# cv2.imshow('opening', opening)dilate = cv2.morphologyEx(opening, cv2.MORPH_DILATE, k2)dilate = cv2.bitwise_not(dilate)# cv2.imshow('dilation', dilate)bottom_image = cv2.morphologyEx(bottom_image, cv2.MORPH_CLOSE, k1)",OpenCV: How to correctly apply morphologyEx operation?
"Error: could not determine PostgresSQL version from ""10.4"""," Complete output from command python setup.py egg_info: runningegg_info creating pip-egg-info/psycopg2.egg-info writingpip-egg-info/psycopg2.egg-info/PKG-INFO writing top-level names topip-egg-info/psycopg2.egg-info/top_level.txt writing dependency_linksto pip-egg-info/psycopg2.egg-info/dependency_links.txt writingmanifest file 'pip-egg-info/psycopg2.egg-info/SOURCES.txt' Error:could not determine PostgreSQL version from '10.4'Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-install-lR9u0X/psycopg2/Does anyone know what's the issue? Trying to run pgadmin in virtualenv and can't figure out because of this error. <code> ","Error: could not determine PostgreSQL version from ""10.4"""
Python plot candlesticks with automatic Y zoom," I am looking for a Python plotting library that allows me to plot candlesticks (preferably the OHLC bars variant) with X zoom via mousewheel scrolling (or similar) and an automatically scaled Y axis when zooming.As an example of what I am looking for, tradingview.com does this perfectly. See https://uk.tradingview.com/chart/?symbol=NASDAQ:NDX. OHLC bars can be seen by clicking the candlestick icon near the top left and selecting 'Bars'.Plotly is almost able to do this. The Ohlc class in plotly.graph_objs give the OHLC bars, and the default rangeslider is a nice feature for X zoom (mousewheel scrolling can also be easily enabled). However automatic Y scaling is not available in Python as far as I can see (Y-axis autoscaling with x-range sliders in plotly), thus zooming in on a section of data makes it appear flat. Example code - https://plot.ly/python/ohlc-charts/Another option I am familiar with is PyQtGraph, which has nice zoom features but does not have support for candlestick plots. Using this would involve coding my own candlestick object.There are a wide range of Python plotting libraries out there that I don't know. Is there anything out there that has out of the box support for this? Can anyone provide example code to do this cleanly? <code> ",Python: Plot candlesticks with automatic Y zoom
Receive POST data from Trumbowyg URL image upload," I'm using Trumbowyg, a WYSIWYG JavaScript editor which has a feature of rendering images from URLs pasted in. It also has an upload plugin which enables uploading local images and custom server side handling. My python/django function upload_image() can successfully detect the uploaded image - however when I use the URL image input, my python function cannot detect it. Trumbowyg simply renders the image without going through my python backend.Here's my code: Why can in detect the uploaded image but not the URL input? <code>  $('#id_content').trumbowyg({ btnsDef: { // Create a new dropdown image: { dropdown: ['insertImage', 'upload'], ico: 'insertImage' } }, // Redefine the button pane btns: [ ['strong', 'em', 'del'], ['link'], ['image'], // Our fresh created dropdown ], plugins: { // Add imagur parameters to upload plugin for demo purposes upload: { serverPath: '/upload_image/', fileFieldName: 'content_image', urlPropertyName: 'url' } }}); def upload_image(request): print('Success') #only prints when I use the upload input, not the URL input",Trumbowyg: Django server can detect file upload but not image URL input
Receive GET data from Trumbowyg URL image upload," I'm using Trumbowyg, a WYSIWYG JavaScript editor which has a feature of rendering images from URLs pasted in. It also has an upload plugin which enables uploading local images and custom server side handling. My python/django function upload_image() can successfully detect the uploaded image - however when I use the URL image input, my python function cannot detect it. Trumbowyg simply renders the image without going through my python backend.Here's my code: Why can in detect the uploaded image but not the URL input? <code>  $('#id_content').trumbowyg({ btnsDef: { // Create a new dropdown image: { dropdown: ['insertImage', 'upload'], ico: 'insertImage' } }, // Redefine the button pane btns: [ ['strong', 'em', 'del'], ['link'], ['image'], // Our fresh created dropdown ], plugins: { // Add imagur parameters to upload plugin for demo purposes upload: { serverPath: '/upload_image/', fileFieldName: 'content_image', urlPropertyName: 'url' } }}); def upload_image(request): print('Success') #only prints when I use the upload input, not the URL input",Trumbowyg: Django server can detect file upload but not image URL input
Installing ansible python package on Windows," I'm struggling to install Ansible Python package on my Windows 10 machine.I don't need Ansible to run on my machine, this is purely for development purpose on my Windows host. All commands will later be issued on a Linux machine.After running: I get the following exception: Command ""c:\users\evaldas.buinauskas\appdata\local\programs\python\python37-32\python.exe -u -c ""import setuptools, tokenize;__file__='C:\Users\evaldas.buinauskas\AppData\Local\Temp\pip-install-hpay_le9\ansible\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\evaldas.buinauskas\AppData\Local\Temp\pip-record-dvfgngpp\install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in C:\Users\evaldas.buinauskas\AppData\Local\Temp\pip-install-hpay_le9\ansible\Also there's a repetetive exception that I think is the root cause: error: can't copy 'lib\ansible\module_utils\ansible_release.py': doesn't exist or not a regular fileThis GitHub issue says that installing should be possible, not running it. That's basically all I really need.I tried running CMD/PowerShell/Cygwin as Administrator, didn't help. Also, there's an answer that tells how to install it on Windows: How to overcome - pip install ansible on windows failing with filename or extension too long on windowsBut I don't really understand how to get a wheel file for Ansible package. <code>  pip install ansible",Installing Ansible Python package on Windows
Seaborn Jointplot colors for each point according to class," I want to plot the correlation plot of 2 variables using seaborn jointplot. I have tried a lot of different things but I am not able to add colors to the points according to class.Here is my code: Now, I want to add colors for each point according to a class variable classes. <code>  import numpy as npimport seaborn as snsimport matplotlib.pyplot as pltsns.set()X = np.array([5.2945 , 3.6013 , 3.9675 , 5.1602 , 4.1903 , 4.4995 , 4.5234 , 4.6618 , 0.76131, 0.42036, 0.71092, 0.60899, 0.66451, 0.55388, 0.63863, 0.62504, 0. , 0. , 0.49364, 0.44828, 0.43066, 0.57368, 0. , 0. , 0.64824, 0.65166, 0.64968, 0. , 0. , 0.52522, 0.58259, 1.1309 , 0. , 0. , 1.0514 , 0.7519 , 0.78745, 0.94873, 1.0169 , 0. , 0. , 1.0416 , 0. , 0. , 0.93648, 0.92801, 0. , 0. , 0.89594, 0. , 0.80455, 1.0103 ])y = np.array([ 93, 115, 107, 115, 110, 107, 102, 113, 95, 101, 116, 74, 102, 102, 78, 85, 108, 110, 109, 80, 91, 88, 99, 110, 108, 96, 105, 93, 107, 98, 88, 75, 106, 92, 82, 84, 84, 92, 115, 107, 97, 115, 85, 133, 100, 65, 96, 105, 112, 107, 107, 105])ax = sns.jointplot(X, y, kind='reg' )ax.set_axis_labels(xlabel='Brain scores', ylabel='Cognitive scores')plt.tight_layout()plt.show()",Seaborn Jointplot add colors for each class
ROS image topic framerate is extremely low.," I'm trying to get VREP vision sensor output processed with opencv via ROS api. I did manage to set up scene and get scripts running, but the problem is that I get somewhat like 4-5 fps even without actual processing (currently I just push images directly to output). This issue does not seem to depend on image resolution, since both 1024*512 and 128*128 captures result in exactly the same fps. This also is not a matter of blocking calls, although I'm posting single-theaded code, I do have quite complex multithreaded processing pipeline which performs rather well with actual cameras (~50 fps). Lua scripts on VREP's side do not seem to be a problem also, since I've tried to play with video retranslation examples provided by vrep, and they seem to achieve rather satisfying fps. So it seems like image streaming is a bottleneck. Here's my sample script: I also have to mention that I run it with ros bridge, since I need processing done with python3, which is supported by ROS2 only, and VREP seems to work only with ROS1 (although I'm just starting to work with these systems, so I'm not confident in that case).  <code>  # coding=utf-8import rclpyimport rclpy.node as nodeimport cv2import numpy as npimport sensor_msgs.msg as msgimport third_party.ros.ros as rosclass TestDisplayNode(node.Node): def __init__(self): super().__init__('IProc_TestDisplayNode') self.__window_name = ""img"" self.sub = self.create_subscription(msg.Image, 'Vision_sensor', self.msg_callback) def msg_callback(self, m : msg.Image): np_img = np.reshape(m.data, (m.height, m.width, 3)).astype(np.uint8) self.display(np_img) def display(self, img : np.ndarray): cv2.imshow(self.__window_name, cv2.cvtColor(img, cv2.COLOR_RGB2BGR)) cv2.waitKey(1)def main(): ros_core = Ros2CoreWrapper() node = TestDisplayNode() rclpy.spin(node) node.destroy_node() rclpy.shutdown()if __name__ == ""__main__"": main()",ROS image topic framerate is extremely low
Phython Pip package RequestsDependencyWarning when installing elastic-search-curator," I installed elastic search curator via the following command. It all installed OK.However now when I do the following I get the following dependency warning. How do I either fix the warning or hide it? <code>  sudo pip install -U elasticsearch-curator curator_cli --version /usr/local/lib/python2.7/dist-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown. warnings.warn(warning, RequestsDependencyWarning)curator_cli, version 5.5.4",Python pip package RequestsDependencyWarning when installing elastic-search-curator
applying standardscaler in pipeline to test set," In the example below, I am using StandardScaler(), is this the correct way to apply it to test set as well? <code>  pipe = Pipeline([ ('scale', StandardScaler()), ('reduce_dims', PCA(n_components=4)), ('clf', SVC(kernel = 'linear', C = 1))])param_grid = dict(reduce_dims__n_components=[4,6,8], clf__C=np.logspace(-4, 1, 6), clf__kernel=['rbf','linear'])grid = GridSearchCV(pipe, param_grid=param_grid, cv=3, n_jobs=1, verbose=2)grid.fit(X_train, y_train)print(grid.score(X_test, y_test))",How to apply StandardScaler in Pipeline in scikit-learn (sklearn)?
Apply StandardScaler in Pipeline in scikit-learn (sklearn)," In the example below, I am using StandardScaler(), is this the correct way to apply it to test set as well? <code>  pipe = Pipeline([ ('scale', StandardScaler()), ('reduce_dims', PCA(n_components=4)), ('clf', SVC(kernel = 'linear', C = 1))])param_grid = dict(reduce_dims__n_components=[4,6,8], clf__C=np.logspace(-4, 1, 6), clf__kernel=['rbf','linear'])grid = GridSearchCV(pipe, param_grid=param_grid, cv=3, n_jobs=1, verbose=2)grid.fit(X_train, y_train)print(grid.score(X_test, y_test))",How to apply StandardScaler in Pipeline in scikit-learn (sklearn)?
Curios memory consumtion of pandas.unique()," While profiling the memory consumption of my algorithm, I was surprised that sometimes for smaller inputs more memory was needed.It all boils down to the following usage of pandas.unique(): with N=6*10^7 it needs 3.7GB peak memory, but with N=8*10^7 ""only"" 3GB. Scanning different input-size yields the following graph:Out of curiosity and for self-education: How can the counterintuitive behavior (i.e. more memory for smaller input size) around N=5*10^7, N=1.3*10^7 be explained?Here are the scripts for producing the memory consumption graph on Linux:pandas_unique_test.py: show_memory.py: run_perf_test.sh: And now: <code>  import numpy as npimport pandas as pdimport sysN=int(sys.argv[1])a=np.arange(N, dtype=np.int64)b=pd.unique(a) import numpy as npimport pandas as pdimport sysN=int(sys.argv[1]) a=np.arange(N, dtype=np.int64)b=pd.unique(a) import sysimport matplotlib.pyplot as plt ns=[]mems=[]for line in sys.stdin.readlines(): n,mem = map(int, line.strip().split("" "")) ns.append(n) mems.append(mem)plt.plot(ns, mems, label='peak-memory')plt.xlabel('n')plt.ylabel('peak memory in KB')ymin, ymax = plt.ylim()plt.ylim(0,ymax)plt.legend()plt.show() WRAPPER=""/usr/bin/time -f%M"" #peak memory in KbN=1000000while [ $N -lt 100000000 ]do printf ""$N "" $WRAPPER python pandas_unique_test.py $N N=`expr $N + 1000000`done sh run_perf_tests.sh 2>&1 | python show_memory.py",Curious memory consumption of pandas.unique()
Curious memory consumtion of pandas.unique()," While profiling the memory consumption of my algorithm, I was surprised that sometimes for smaller inputs more memory was needed.It all boils down to the following usage of pandas.unique(): with N=6*10^7 it needs 3.7GB peak memory, but with N=8*10^7 ""only"" 3GB. Scanning different input-size yields the following graph:Out of curiosity and for self-education: How can the counterintuitive behavior (i.e. more memory for smaller input size) around N=5*10^7, N=1.3*10^7 be explained?Here are the scripts for producing the memory consumption graph on Linux:pandas_unique_test.py: show_memory.py: run_perf_test.sh: And now: <code>  import numpy as npimport pandas as pdimport sysN=int(sys.argv[1])a=np.arange(N, dtype=np.int64)b=pd.unique(a) import numpy as npimport pandas as pdimport sysN=int(sys.argv[1]) a=np.arange(N, dtype=np.int64)b=pd.unique(a) import sysimport matplotlib.pyplot as plt ns=[]mems=[]for line in sys.stdin.readlines(): n,mem = map(int, line.strip().split("" "")) ns.append(n) mems.append(mem)plt.plot(ns, mems, label='peak-memory')plt.xlabel('n')plt.ylabel('peak memory in KB')ymin, ymax = plt.ylim()plt.ylim(0,ymax)plt.legend()plt.show() WRAPPER=""/usr/bin/time -f%M"" #peak memory in KbN=1000000while [ $N -lt 100000000 ]do printf ""$N "" $WRAPPER python pandas_unique_test.py $N N=`expr $N + 1000000`done sh run_perf_tests.sh 2>&1 | python show_memory.py",Curious memory consumption of pandas.unique()
Get path of calling script," Say I have two files, foo.py and bar.py. It does not matter where they are located, only that foo.py somehow manages to import bar.py and call a function func() defined in the latter. How would I get the absolute path of foo.py from bar.func()? <code> ",Get path of script containing the calling function
How to remote sub-list from list," I have two lists: I want to remove all sub_list occurrences in big_list.result should be [2, 3, 4]For strings you could use this: But AFAIK this does not work for lists.This is not a duplicate of Removing a sublist from a list since I want to remove all sub-lists from the big-list. In the other question the result should be [5,6,7,1,2,3,4].Update: For simplicity I took integers in this example. But list items could be arbitrary objects.Update2:if big_list = [1, 2, 1, 2, 1] and sub_list = [1, 2, 1], I want the result to be [2, 1] (like '12121'.replace('121', ''))Update3:I don't like copy+pasting source code from StackOverflow into my code. That's why I created second question at software-recommendations: https://softwarerecs.stackexchange.com/questions/51273/library-to-remove-every-occurrence-of-sub-list-from-list-pythonUpdate4: if you know a library to make this one method call, please write it as answer, since this is my preferred solution. The test should pass this test: <code>  big_list = [2, 1, 2, 3, 1, 2, 4]sub_list = [1, 2] '2123124'.replace('12', '') def test_remove_sub_list(self): self.assertEqual([1, 2, 3], remove_sub_list([1, 2, 3], [])) self.assertEqual([1, 2, 3], remove_sub_list([1, 2, 3], [4])) self.assertEqual([1, 3], remove_sub_list([1, 2, 3], [2])) self.assertEqual([1, 2], remove_sub_list([1, 1, 2, 2], [1, 2])) self.assertEquals([2, 1], remove_sub_list([1, 2, 1, 2, 1], [1, 2, 1])) self.assertEqual([], remove_sub_list([1, 2, 1, 2, 1, 2], [1, 2]))",How to remove every occurrence of sub-list from list
How to remove sub-list from list," I have two lists: I want to remove all sub_list occurrences in big_list.result should be [2, 3, 4]For strings you could use this: But AFAIK this does not work for lists.This is not a duplicate of Removing a sublist from a list since I want to remove all sub-lists from the big-list. In the other question the result should be [5,6,7,1,2,3,4].Update: For simplicity I took integers in this example. But list items could be arbitrary objects.Update2:if big_list = [1, 2, 1, 2, 1] and sub_list = [1, 2, 1], I want the result to be [2, 1] (like '12121'.replace('121', ''))Update3:I don't like copy+pasting source code from StackOverflow into my code. That's why I created second question at software-recommendations: https://softwarerecs.stackexchange.com/questions/51273/library-to-remove-every-occurrence-of-sub-list-from-list-pythonUpdate4: if you know a library to make this one method call, please write it as answer, since this is my preferred solution. The test should pass this test: <code>  big_list = [2, 1, 2, 3, 1, 2, 4]sub_list = [1, 2] '2123124'.replace('12', '') def test_remove_sub_list(self): self.assertEqual([1, 2, 3], remove_sub_list([1, 2, 3], [])) self.assertEqual([1, 2, 3], remove_sub_list([1, 2, 3], [4])) self.assertEqual([1, 3], remove_sub_list([1, 2, 3], [2])) self.assertEqual([1, 2], remove_sub_list([1, 1, 2, 2], [1, 2])) self.assertEquals([2, 1], remove_sub_list([1, 2, 1, 2, 1], [1, 2, 1])) self.assertEqual([], remove_sub_list([1, 2, 1, 2, 1, 2], [1, 2]))",How to remove every occurrence of sub-list from list
Why two identical lists have different memory footprint?," I created two lists l1 and l2, but each one with a different creation method: But the output surprised me: The list created with a list comprehension is a bigger size in memory, but the two lists are identical in Python otherwise.Why is that? Is this some CPython internal thing, or some other explanation? <code>  import sysl1 = [None] * 10l2 = [None for _ in range(10)]print('Size of l1 =', sys.getsizeof(l1))print('Size of l2 =', sys.getsizeof(l2)) Size of l1 = 144Size of l2 = 192",Why do two identical lists have a different memory footprint?
Concatening Attention layer with decoder input seq2seq model on Keras," I am trying to implement a sequence 2 sequence model with attention using the Keras library. The block diagram of the model is as followsThe model embeds the input sequence into 3D tensors. Then a bidirectional lstm creates the encoding layer. Next the encoded sequences are sent to a custom attention layer that returns a 2D tensor having attention weights for each hidden node.The decoder input is injected in the model as one hot vector. Now in the decoder (another bi-lstm) both decoder input and the attention weight are passed as input. The output of the decoder is sent to time distributed dense layer with the softmax activation function to get the output for every time step in the means of probability. The code of the model is as follows: The problem is when I am concatenating the attention layer and decoder input. Since the decoder input is a 3D tensor whereas attention is a 2D tensor, it's showing the following error:ValueError: A Concatenate layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 1024), (None, 10, 8281)]How can I convert a 2D Attention tensor into a 3D tensor? <code>  encoder_input = Input(shape=(MAX_LENGTH_Input, ))embedded = Embedding(input_dim=vocab_size_input, output_dim= embedding_width, trainable=False)(encoder_input)encoder = Bidirectional(LSTM(units= hidden_size, input_shape=(MAX_LENGTH_Input,embedding_width), return_sequences=True, dropout=0.25, recurrent_dropout=0.25))(embedded)attention = Attention(MAX_LENGTH_Input)(encoder)decoder_input = Input(shape=(MAX_LENGTH_Output,vocab_size_output))merge = concatenate([attention, decoder_input])decoder = Bidirectional(LSTM(units=hidden_size, input_shape=(MAX_LENGTH_Output,vocab_size_output))(merge))output = TimeDistributed(Dense(MAX_LENGTH_Output, activation=""softmax""))(decoder)",Concatening an attention layer with decoder input seq2seq model on Keras
Getting all iterations of a string by removing any number of characters," I've seen many questions on getting all the possible substrings (i.e., adjacent sets of characters), but none on generating all possible strings including the combinations of its substrings.For example, let: I would like the output to be something like: The main point is that we can remove multiple characters that are not adjacent in the original string (as well as the adjacent ones).Here is what I have tried so far: However, this only removes sets of adjacent strings from the original string, and will not return the element 'ac' from the example above.Another example is if we use the string 'abcde', the output list should contain the elements 'ace', 'bd' etc. <code>  x = 'abc' ['abc', 'ab', 'ac', 'bc', 'a', 'b', 'c'] def return_substrings(input_string): length = len(input_string) return [input_string[i:j + 1] for i in range(length) for j in range(i, length)]print(return_substrings('abc'))",Getting all combinations of a string and its substrings
Getting all combinations of a string by removing any number of characters," I've seen many questions on getting all the possible substrings (i.e., adjacent sets of characters), but none on generating all possible strings including the combinations of its substrings.For example, let: I would like the output to be something like: The main point is that we can remove multiple characters that are not adjacent in the original string (as well as the adjacent ones).Here is what I have tried so far: However, this only removes sets of adjacent strings from the original string, and will not return the element 'ac' from the example above.Another example is if we use the string 'abcde', the output list should contain the elements 'ace', 'bd' etc. <code>  x = 'abc' ['abc', 'ab', 'ac', 'bc', 'a', 'b', 'c'] def return_substrings(input_string): length = len(input_string) return [input_string[i:j + 1] for i in range(length) for j in range(i, length)]print(return_substrings('abc'))",Getting all combinations of a string and its substrings
Getting all combinations of a string and its substrings in Python," I've seen many questions on getting all the possible substrings (i.e., adjacent sets of characters), but none on generating all possible strings including the combinations of its substrings.For example, let: I would like the output to be something like: The main point is that we can remove multiple characters that are not adjacent in the original string (as well as the adjacent ones).Here is what I have tried so far: However, this only removes sets of adjacent strings from the original string, and will not return the element 'ac' from the example above.Another example is if we use the string 'abcde', the output list should contain the elements 'ace', 'bd' etc. <code>  x = 'abc' ['abc', 'ab', 'ac', 'bc', 'a', 'b', 'c'] def return_substrings(input_string): length = len(input_string) return [input_string[i:j + 1] for i in range(length) for j in range(i, length)]print(return_substrings('abc'))",Getting all combinations of a string and its substrings
"Running a python script from jupyter notebook, passing arguments", I have this simple Python script which I run from my Jupyter Notebook. However the arguments I pass to it seemingly are ignored and this results in an exception:two_digits.py <code>  import sysinput = sys.stdin.read()tokens = input.split()a = int(tokens[0])b = int(tokens[1])print(a + b)%run two_digits 3 5ndexError Traceback (most recent call last)D:\Mint_ns\two_digits.py in <module>() 5 tokens = input.split() 6 ----> 7 a = int(tokens[0]) 8 9 b = int(tokens[1])IndexError: list index out of range,"Running a Python script in Jupyter Notebook, with arguments passing"
"Running a python script in jupyter notebook, with arguments passing", I have this simple Python script which I run from my Jupyter Notebook. However the arguments I pass to it seemingly are ignored and this results in an exception:two_digits.py <code>  import sysinput = sys.stdin.read()tokens = input.split()a = int(tokens[0])b = int(tokens[1])print(a + b)%run two_digits 3 5ndexError Traceback (most recent call last)D:\Mint_ns\two_digits.py in <module>() 5 tokens = input.split() 6 ----> 7 a = int(tokens[0]) 8 9 b = int(tokens[1])IndexError: list index out of range,"Running a Python script in Jupyter Notebook, with arguments passing"
Type error when trying to use list of integers in numpy array: only integer scalar arrays can be converted to a scalar index," The following code: produces the error: TypeError: only integer scalar arrays can be converted to a scalar indexNote that my problem is different than in the question, ""numpy array TypeError: only integer scalar arrays can be converted to a scalar index"", which is attempting something more complex.My mistake in this question is that I'm trying to use a list of indexes on an ordinary python list -- see my answer. I expect it happens much more broadly than just with using range and shuffle. <code>  x = list(range(0,10))random.shuffle(x)ind = np.argsort(x)x[ind]",TypeError when indexing a list with a NumPy array: only integer scalar arrays can be converted to a scalar index
Python 3.5 create .rpm with pinstaller generated executable," I've got a build generated with a pyinstaller.I need to create .rpm package which will put the executable into the /usr/bin/ and create a systemd service which will run that executable.I found this https://docs.python.org/3/distutils/builtdist.html and https://docs.python.org/2.0/dist/creating-rpms.htmlHowever it doesn't give me a full picture.Is it possible to make it?What toolset do i need to use? (Basically, how to make it).If possible - sample code <code> ",Python 3.5 create .rpm with pyinstaller generated executable
Keras dot/Dot layer output not acting like documentation specifies," The Keras documentation for the dot/Dot layer states that:""Layer that computes a dot product between samples in two tensors.E.g. if applied to a list of two tensors a and b of shape (batch_size, n), the output will be a tensor of shape (batch_size, 1) where each entry i will be the dot product between a[i] and b[i].Argumentsaxes: Integer or tuple of integers, axis or axes along which to take the dot product.""I am not getting this, here is a quick, reproducible example to demonstrate: Output: (99, 45000, 300) (99, 45000, 300) (99, 45000, 45000)Why is the element wise dot product shape not (99,45000,1) ? What am I doing wrong and how can i fix it? <code>  from keras.layers import Input, dotinput_a = Input(batch_shape=(99,45000,300))input_b = Input(batch_shape=(99,45000,300))element_wise_dot_product = dot([input_a,input_b], axes = -1)print(input_a.get_shape(),input_b.get_shape(),element_wise_dot_product.get_shape()) ",Keras dot/Dot layer behavior on 3D tensors
what's the usage of python virtual subclass?," Output: I am trying to understand python virtual subclass, example shows as above. instance a virtual subclass doesn't require implement abstract method at all.What's the real use cases of virtual subclass? it seems to me the virtual subclass works like something in the middle of duck type and object inherit.Duck type -- virtual subclass -- object inheritance <code>  class AnimalMeta(type): def __instancecheck__(cls, instance): return cls.__subclasscheck__(type(instance)) def __subclasscheck__(cls, sub): return (hasattr(sub, 'eat') and callable(sub.eat) and hasattr(sub, 'sleep') and callable(sub.sleep))class Animal(object): __metaclass__ = AnimalMeta passclass Dog(object): def eat(self): print ""eat"" def sleep(self): print ""sleep""dog = Dog()dog.eat()print isinstance(dog, Animal)print issubclass(dog, Animal) eatTrueTrue",What's the usage of a virtual subclass?
How to debug flask custom commands in PyCharm," I have created a custom CLI command in Flask, that I am able to run via flask my_command in the terminal. I want to run this command using PyCharm's debugger.I created a ""Flask server"" configuration, and running it with the PyCharm debugger stops at breakpoints I set inside view functions. But if I try to run my CLI command from PyCharm's terminal, it doesn't stop at breakpoints in the command.Do I need a custom configuration to debug custom CLI commands? I found a question about Django commands, but PyCharm's ""Flask server"" configuration doesn't have the same options. How can I configure PyCharm to debug a Flask CLI command? <code> ",Run Flask CLI command with PyCharm debugger
Pandas Merge two rows into a single row based columns," I have 2 rows that look like these, I'm looking to merge them into a single as: Not sure how to accomplish this in Pandas. Any pointers will be highly appreciated! Thanks in advance <code>  ------------------------------DealName | Target | Acquirer |-----------------------------ABC-XYZ | ABC | None |------------------------------ABC-XYZ | None | XYZ |------------------------------ ------------------------------DealName | Target | Acquirer |-----------------------------ABC-XYZ | ABC | XYZ |------------------------------",Pandas Merge two rows into a single row based on columns
Using numpy.vstack in Numba," So I have been trying to optimize some code that calculates a statistical error metric from some array data. The metric is called the Continuous Rank Probability Score (CRPS). I have been using Numba to try to speed up the double for loop required in this calculation, but I have been having an issue with the numpy.vstack function. From what I understand from the docs here, the vstack() function should be supported, but when I run the following code I get an error. The error I get is this: I just wanted to know where I am going wrong. It seems as though the vstack function should work but maybe I am missing something. <code>  def crps_hersbach_numba(obs, fcst_ens, remove_neg=False, remove_zero=False): """"""Calculate the the continuous ranked probability score (CRPS) as per equation 25-27 in Hersbach et al. (2000) Parameters ---------- obs: 1D ndarry Array of observations for each start date fcst_ens: 2D ndarray Array of ensemble forecast of dimension n x M, where n = number of start dates and M = number of ensemble members. remove_neg: bool If True, when a negative value is found at the i-th position in the observed OR ensemble array, the i-th value of the observed AND ensemble array are removed before the computation. remove_zero: bool If true, when a zero value is found at the i-th position in the observed OR ensemble array, the i-th value of the observed AND ensemble array are removed before the computation. Returns ------- dict Dictionary contains a number of *experimental* outputs including: - [""crps""] 1D ndarray of crps values per n start dates. - [""crpsMean1""] arithmetic mean of crps values. - [""crpsMean2""] mean crps using eqn. 28 in Hersbach (2000). Notes ----- **NaN and inf treatment:** If any value in obs or fcst_ens is NaN or inf, then the corresponding row in both fcst_ens (for all ensemble members) and in obs will be deleted. References ---------- - Hersbach, H. (2000) Decomposition of the Continuous Ranked Porbability Score for Ensemble Prediction Systems, Weather and Forecasting, 15, 559-570. """""" # Treating the Data obs, fcst_ens = treat_data(obs, fcst_ens, remove_neg=remove_neg, remove_zero=remove_zero) # Set parameters n = fcst_ens.shape[0] # number of forecast start dates m = fcst_ens.shape[1] # number of ensemble members # Create vector of pi's p = np.linspace(0, m, m + 1) pi = p / m crps_numba = np.zeros(n) @njit def calculate_crps(): # Loop fcst start times for i in prange(n): # Initialise vectors for storing output a = np.zeros(m - 1) b = np.zeros(m - 1) # Verifying analysis (or obs) xa = obs[i] # Ensemble fcst CDF x = np.sort(fcst_ens[i, :]) # Deal with 0 < i < m [So, will loop 50 times for m = 51] for j in prange(m - 1): # Rule 1 if xa > x[j + 1]: a[j] = x[j + 1] - x[j] b[j] = 0 # Rule 2 if x[j] < xa < x[j + 1]: a[j] = xa - x[j] b[j] = x[j + 1] - xa # Rule 3 if xa < x[j]: a[j] = 0 b[j] = x[j + 1] - x[j] # Deal with outliers for i = 0, and i = m, # else a & b are 0 for non-outliers if xa < x[0]: a1 = 0 b1 = x[0] - xa else: a1 = 0 b1 = 0 # Upper outlier (rem m-1 is for last member m, but python is 0-based indexing) if xa > x[m - 1]: am = xa - x[m - 1] bm = 0 else: am = 0 bm = 0 # Combine full a & b vectors including outlier a = np.concatenate((np.array([0]), a, np.array([am]))) # a = np.insert(a, 0, a1) # a = np.append(a, am) a = np.concatenate((np.array([0]), a, np.array([bm]))) # b = np.insert(b, 0, b1) # b = np.append(b, bm) # Populate a_mat and b_mat if i == 0: a_mat = a b_mat = b else: a_mat = np.vstack((a_mat, a)) b_mat = np.vstack((b_mat, b)) # Calc crps for individual start times crps_numba[i] = ((a * pi ** 2) + (b * (1 - pi) ** 2)).sum() return crps_numba, a_mat, b_mat crps, a_mat, b_mat = calculate_crps() print(crps) # Calc mean crps as simple mean across crps[i] crps_mean_method1 = np.mean(crps) # Calc mean crps across all start times from eqn. 28 in Hersbach (2000) abar = np.mean(a_mat, 0) bbar = np.mean(b_mat, 0) crps_mean_method2 = ((abar * pi ** 2) + (bbar * (1 - pi) ** 2)).sum() # Output array as a dictionary output = {'crps': crps, 'crpsMean1': crps_mean_method1, 'crpsMean2': crps_mean_method2} return output Cannot unify array(float64, 1d, C) and array(float64, 2d, C) for 'a_mat', defined at *pathFile ""test.py"", line 86: def calculate_crps(): <source elided> if i == 0: a_mat = a ^[1] During: typing of assignment at *pathFile ""test.py"", line 89: def calculate_crps(): <source elided> else: a_mat = np.vstack((a_mat, a)) ^This is not usually a problem with Numba itself but instead often caused bythe use of unsupported features or an issue in resolving types.",Using numpy.vstack in numba
Not able to skip testcase in a class via pytest fixture skipif," I am using pytest framework and want to skip testcase based on some condition. Below code is not skipping the test case. When running, it is saying 0 testcases executed. This func_fixture is very crucial for the testsuite. It performs many pre-requisites before starting the test. If I remove class, and add rest of the functions with same syntax (after removing self), it works. Not sure why it is failing in class <code>  import pytestclass TEST: @pytest.fixture(scope=""function"", autouse=True) def func_fixture(self): return ""fail"" @pytest.mark.skipif(""self.func_fixture=='fail'"") def test_setting_value(self): print(""Hello I am in testcase"")",Pytest not able to skip testcase in a class via marker skipif
`Python` Pytest not able to skip testcase in a class via fixture skipif," I am using pytest framework and want to skip testcase based on some condition. Below code is not skipping the test case. When running, it is saying 0 testcases executed. This func_fixture is very crucial for the testsuite. It performs many pre-requisites before starting the test. If I remove class, and add rest of the functions with same syntax (after removing self), it works. Not sure why it is failing in class <code>  import pytestclass TEST: @pytest.fixture(scope=""function"", autouse=True) def func_fixture(self): return ""fail"" @pytest.mark.skipif(""self.func_fixture=='fail'"") def test_setting_value(self): print(""Hello I am in testcase"")",Pytest not able to skip testcase in a class via marker skipif
Python Pytest not able to skip testcase in a class via marker skipif," I am using pytest framework and want to skip testcase based on some condition. Below code is not skipping the test case. When running, it is saying 0 testcases executed. This func_fixture is very crucial for the testsuite. It performs many pre-requisites before starting the test. If I remove class, and add rest of the functions with same syntax (after removing self), it works. Not sure why it is failing in class <code>  import pytestclass TEST: @pytest.fixture(scope=""function"", autouse=True) def func_fixture(self): return ""fail"" @pytest.mark.skipif(""self.func_fixture=='fail'"") def test_setting_value(self): print(""Hello I am in testcase"")",Pytest not able to skip testcase in a class via marker skipif
Python Faster way to perform this list comprehension," Suppose we have a list of numbers, l. I need to COUNT all tuples of length 3 from l, (l_i,l_j,l_k) such that l_i evenly divides l_j, and l_j evenly divides l_k. With the stipulation that the indices i,j,k have the relationship i<j<k I.e.;If l=[1,2,3,4,5,6], then the tuples would be [1,2,6], [1,3,6],[1,2,4], so the COUNT would be 3.If l=[1,1,1], then the only tuple would be [1,1,1], so the COUNT would be 1.Here's what I've done so far, using list comprehensions: This works, but as l gets longer (and it can be as large as 2000 elements long), the time it takes increases too much. Is there a faster/better way to do this? <code>  def myCOUNT(l): newlist=[[x,y,z] for x in l for y in l for z in l if (z%y==0 and y%x==0 and l.index(x)<l.index(y) and l.index(y)<l.index(z))] return len(newlist)>>>l=[1,2,3,4,5,6]>>>myCOUNT(l)3",Faster Python technique to count triples from a list of numbers that are multiples of each other
Extending a class by parameter in Python," I want to expand the class Foo by the class Bar, the issue that I have is that I can't expand it in the usual way (class Foo(Bar)) because the class Bar is somewhat dynamically generated.I made this small example to illustrate my desired outcome: Again this is not what I'm looking for: <code>  class Bar: def super_cool_function(): print(""Cool"")class Foo: def __init__(self, another_class): # I want to extend Foo by another_class# Desired resultfoobar = Foo(Bar)foobar.super_cool_function() class Foo(Bar): passfoobar = Foo()foobar.super_cool_function()",Is it possible to dynamically inherit from a class that is only known at runtime in python?
Installing Tensorflow on El Capitan 10.11.6," I am trying to install tensorflow 1.10 on my old mac, but I run into the same problem every time. As soon as I start the python shell and I do get the error below.I did try to install it in a virtualenv first, after that, I tried to install it just using pip and got the same error. Also when I tried to install it with conda, the same issue again. I googled and looked that up here, but couldn't solve it yet, maybe due to my noobish acting. I did uninstall anaconda and pip already and tried again, but I always run into the same error.I do have CUDA installed, although I tried to install tensorflow with and without GPU.The error results in: Failed to load the native TensorFlow runtime.I hope someone is able to help, as I would really like to learn this. (tensorflow) abc:~ me$ python During handling of the above exception, another exception occurred: Failed to load the native TensorFlow runtime.See https://www.tensorflow.org/install/install_sources#common_installation_problemsFor some common reasons and solutions. Include the entire stack traceabove this error message when asking for help. <code>  import tensorflow as tfTraceback (most recent call last): File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module> from tensorflow.python.pywrap_tensorflow_internal import * File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module> _pywrap_tensorflow_internal = swig_import_helper() File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File ""/Users/me/tensorflow/lib/python3.6/imp.py"", line 243, in load_module return load_dynamic(name, filename, file) File ""/Users/me/tensorflow/lib/python3.6/imp.py"", line 343, in load_dynamic return _load(spec)ImportError: dlopen(/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation Referenced from: /Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so (which was built for Mac OS X 10.12) Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security in /Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so >Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/__init__.py"", line 22, in <module> from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module> from tensorflow.python import pywrap_tensorflow File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module> raise ImportError(msg)ImportError: Traceback (most recent call last): File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module> from tensorflow.python.pywrap_tensorflow_internal import * File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module> _pywrap_tensorflow_internal = swig_import_helper() File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File ""/Users/me/tensorflow/lib/python3.6/imp.py"", line 243, in load_module return load_dynamic(name, filename, file) File ""/Users/me/tensorflow/lib/python3.6/imp.py"", line 343, in load_dynamic return _load(spec)ImportError: dlopen(/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation Referenced from: /Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so (which was built for Mac OS X 10.12) Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security in /Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so",Installing Tensorflow 1.10 on El Capitan 10.11.6
Installing Tensorflow on El Capitan 10.11.6," I am trying to install tensorflow 1.10 on my old mac, but I run into the same problem every time. As soon as I start the python shell and I do get the error below.I did try to install it in a virtualenv first, after that, I tried to install it just using pip and got the same error. Also when I tried to install it with conda, the same issue again. I googled and looked that up here, but couldn't solve it yet, maybe due to my noobish acting. I did uninstall anaconda and pip already and tried again, but I always run into the same error.I do have CUDA installed, although I tried to install tensorflow with and without GPU.The error results in: Failed to load the native TensorFlow runtime.I hope someone is able to help, as I would really like to learn this. (tensorflow) abc:~ me$ python During handling of the above exception, another exception occurred: Failed to load the native TensorFlow runtime.See https://www.tensorflow.org/install/install_sources#common_installation_problemsFor some common reasons and solutions. Include the entire stack traceabove this error message when asking for help. <code>  import tensorflow as tfTraceback (most recent call last): File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module> from tensorflow.python.pywrap_tensorflow_internal import * File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module> _pywrap_tensorflow_internal = swig_import_helper() File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File ""/Users/me/tensorflow/lib/python3.6/imp.py"", line 243, in load_module return load_dynamic(name, filename, file) File ""/Users/me/tensorflow/lib/python3.6/imp.py"", line 343, in load_dynamic return _load(spec)ImportError: dlopen(/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation Referenced from: /Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so (which was built for Mac OS X 10.12) Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security in /Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so >Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/__init__.py"", line 22, in <module> from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module> from tensorflow.python import pywrap_tensorflow File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module> raise ImportError(msg)ImportError: Traceback (most recent call last): File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module> from tensorflow.python.pywrap_tensorflow_internal import * File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module> _pywrap_tensorflow_internal = swig_import_helper() File ""/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File ""/Users/me/tensorflow/lib/python3.6/imp.py"", line 243, in load_module return load_dynamic(name, filename, file) File ""/Users/me/tensorflow/lib/python3.6/imp.py"", line 343, in load_dynamic return _load(spec)ImportError: dlopen(/Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _SecKeyCopyExternalRepresentation Referenced from: /Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so (which was built for Mac OS X 10.12) Expected in: /System/Library/Frameworks/Security.framework/Versions/A/Security in /Users/me/tensorflow/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so",Installing Tensorflow 1.10 on El Capitan 10.11.6
Failed to start the kernel on jupyter notebook," I have python versions 3.6.5_1 and 3.7.0installed via Homebrew.jupyter needs python3.6 for launching. It wouldn't start if I switch to python3.7.After launching, it fails to start the kernel. Giving this error: From what I understand, the kernel is looking for python3.7. My kernel list has just python3 I looked at this link on github, but it wasn't helpful. How do I make jupyter and the kernel running on the same python version? <code>  Traceback (most recent call last):File ""/usr/local/Cellar/jupyter/1.0.0_4/libexec/vendor/lib/python3.6/site-packages/tornado/web.py"", line 1543, in _executeresult = yield resultFile ""/usr/local/Cellar/jupyter/1.0.0_4/libexec/vendor/lib/python3.6/site-packages/tornado/gen.py"", line 1099, in runvalue = future.result()File ""/usr/local/Cellar/jupyter/1.0.0_4/libexec/vendor/lib/python3.6/site-packages/tornado/gen.py"", line 1107, in runyielded = self.gen.throw(*exc_info)File ""/usr/local/Cellar/jupyter/1.0.0_4/libexec/lib/python3.6/site-packages/notebook/services/sessions/handlers.py"", line 73, in posttype=mtype))File ""/usr/local/Cellar/jupyter/1.0.0_4/libexec/vendor/lib/python3.6/site-packages/tornado/gen.py"", line 1099, in runvalue = future.result()File ""/usr/local/Cellar/jupyter/1.0.0_4/libexec/vendor/lib/python3.6/site-packages/tornado/gen.py"", line 1107, in runyielded = self.gen.throw(*exc_info)File ""/usr/local/Cellar/jupyter/1.0.0_4/libexec/lib/python3.6/site-packages/notebook/services/sessions/sessionmanager.py"", line 79, in create_sessionkernel_id = yield self.start_kernel_for_session(session_id, path, name, type, kernel_name)File ""/usr/local/Cellar/jupyter/1.0.0_4/libexec/vendor/lib/python3.6/site-packages/tornado/gen.py"", line 1099, in runvalue = future.result()File ""/usr/local/Cellar/jupyter/1.0.0_4/libexec/vendor/lib/python3.6/site-packages/tornado/gen.py"", line 1107, in runyielded = self.gen.throw(*exc_info)File ""/usr/local/Cellar/jupyter/1.0.0_4/libexec/lib/python3.6/site-packages/notebook/services/sessions/sessionmanager.py"", line 92, in start_kernel_for_sessionself.kernel_manager.start_kernel(path=kernel_path, kernel_name=kernel_name)File ""/usr/local/Cellar/jupyter/1.0.0_4/libexec/vendor/lib/python3.6/site-packages/tornado/gen.py"", line 1099, in runvalue = future.result()File ""/usr/local/Cellar/jupyter/1.0.0_4/libexec/vendor/lib/python3.6/site-packages/tornado/gen.py"", line 315, in wrapperyielded = next(result)File ""/usr/local/Cellar/jupyter/1.0.0_4/libexec/lib/python3.6/site-packages/notebook/services/kernels/kernelmanager.py"", line 148, in start_kernelsuper(MappingKernelManager, self).start_kernel(**kwargs)File ""/usr/local/Cellar/jupyter/1.0.0_4/libexec/lib/python3.6/site-packages/jupyter_client/multikernelmanager.py"", line 110, in start_kernelkm.start_kernel(**kwargs)File ""/usr/local/Cellar/jupyter/1.0.0_4/libexec/lib/python3.6/site-packages/jupyter_client/manager.py"", line 259, in start_kernel**kw)File ""/usr/local/Cellar/jupyter/1.0.0_4/libexec/lib/python3.6/site-packages/jupyter_client/manager.py"", line 204, in _launch_kernelreturn launch_kernel(kernel_cmd, **kw)File ""/usr/local/Cellar/jupyter/1.0.0_4/libexec/lib/python3.6/site-packages/jupyter_client/launcher.py"", line 128, in launch_kernelproc = Popen(cmd, **kwargs)File ""/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/subprocess.py"", line 709, in __init__restore_signals, start_new_session)File ""/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/subprocess.py"", line 1344, in _execute_childraise child_exception_type(errno_num, err_msg, err_filename)FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/opt/python/bin/python3.7': '/usr/local/opt/python/bin/python3.7' $jupyter kernelspec list Available kernels: python3 /usr/local/etc/jupyter/kernels/python3 ",SOLVED: Failed to start the kernel on jupyter notebook
"Block LMI constrained optmization in python, cvxpy"," I want to translate a LMI-constrained optimization problem from Matlab to Python. While reading the CVXPY documentation, I found that I can define an LMI-constrained problem by creating a matrix variable and adding the corresponding constraint. However, instead of constraining the problem by a simple LMI, I need to use the following block LMI:Where P, L are matrix variables and gamma is a scalar. The other symbols are state space matrices of a dynamic system.Is it possible to use this kind of LMI as a constraint in CVXPY? If not, is there any other tool that would allow me to solve this problem in Python? <code> ",Block LMI in CVXPY
Does bias need to be added in convolutional layer?," I understand that bias are required in small networks, to shift the activation function. But in the case of Deep network that has multiple layers of CNN, pooling, dropout and other non -linear activations, is Bias really making a difference? The convolutional filter is learning local features and for a given conv output channel same bias is used. This is not a dupe of this link. The above link only explains role of bias in small neural network and does not attempt to explain role of bias in deep-networks containing multiple CNN layers, drop-outs, pooling and non-linear activation functions. I ran a simple experiment and the results indicated that removing bias from conv layer made no difference in final test accuracy.There are two models trained and the test-accuracy is almost same (slightly better in one without bias.)model_with_bias, model_without_bias( bias not added in conv layer)Are they being used only for historical reasons?If using bias provides no gain in accuracy, shouldn't we omit them? Less parameters to learn.I would be thankful if someone who have deeper knowledge than me, could explain the significance(if- any) of these bias in deep networks.Here is the complete code and the experiment result bias-VS-no_bias experiment Output:InitializedTest accuracy(with bias): 90.5%Test accuracy(without bias): 90.6% <code>  batch_size = 16patch_size = 5depth = 16num_hidden = 64graph = tf.Graph()with graph.as_default(): # Input data. tf_train_dataset = tf.placeholder( tf.float32, shape=(batch_size, image_size, image_size, num_channels)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # Variables. layer1_weights = tf.Variable(tf.truncated_normal( [patch_size, patch_size, num_channels, depth], stddev=0.1)) layer1_biases = tf.Variable(tf.zeros([depth])) layer2_weights = tf.Variable(tf.truncated_normal( [patch_size, patch_size, depth, depth], stddev=0.1)) layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth])) layer3_weights = tf.Variable(tf.truncated_normal( [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1)) layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden])) layer4_weights = tf.Variable(tf.truncated_normal( [num_hidden, num_labels], stddev=0.1)) layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels])) # define a Model with bias . def model_with_bias(data): conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME') hidden = tf.nn.relu(conv + layer1_biases) conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME') hidden = tf.nn.relu(conv + layer2_biases) shape = hidden.get_shape().as_list() reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]]) hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) return tf.matmul(hidden, layer4_weights) + layer4_biases # define a Model without bias added in the convolutional layer. def model_without_bias(data): conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME') hidden = tf.nn.relu(conv ) # layer1_ bias is not added conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME') hidden = tf.nn.relu(conv) # + layer2_biases) shape = hidden.get_shape().as_list() reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]]) # bias are added only in Fully connected layer(layer 3 and layer 4) hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) return tf.matmul(hidden, layer4_weights) + layer4_biases # Training computation. logits_with_bias = model_with_bias(tf_train_dataset) loss_with_bias = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_with_bias)) logits_without_bias = model_without_bias(tf_train_dataset) loss_without_bias = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_without_bias)) # Optimizer. optimizer_with_bias = tf.train.GradientDescentOptimizer(0.05).minimize(loss_with_bias) optimizer_without_bias = tf.train.GradientDescentOptimizer(0.05).minimize(loss_without_bias) # Predictions for the training, validation, and test data. train_prediction_with_bias = tf.nn.softmax(logits_with_bias) valid_prediction_with_bias = tf.nn.softmax(model_with_bias(tf_valid_dataset)) test_prediction_with_bias = tf.nn.softmax(model_with_bias(tf_test_dataset)) # Predictions for without train_prediction_without_bias = tf.nn.softmax(logits_without_bias) valid_prediction_without_bias = tf.nn.softmax(model_without_bias(tf_valid_dataset)) test_prediction_without_bias = tf.nn.softmax(model_without_bias(tf_test_dataset))num_steps = 1001with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print('Initialized') for step in range(num_steps): offset = (step * batch_size) % (train_labels.shape[0] - batch_size) batch_data = train_dataset[offset:(offset + batch_size), :, :, :] batch_labels = train_labels[offset:(offset + batch_size), :] feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels} session.run(optimizer_with_bias, feed_dict=feed_dict) session.run(optimizer_without_bias, feed_dict = feed_dict) print('Test accuracy(with bias): %.1f%%' % accuracy(test_prediction_with_bias.eval(), test_labels)) print('Test accuracy(without bias): %.1f%%' % accuracy(test_prediction_without_bias.eval(), test_labels))",Does bias in the convolutional layer really make a difference to the test accuracy?
Significance of bias in the convolutional layer?," I understand that bias are required in small networks, to shift the activation function. But in the case of Deep network that has multiple layers of CNN, pooling, dropout and other non -linear activations, is Bias really making a difference? The convolutional filter is learning local features and for a given conv output channel same bias is used. This is not a dupe of this link. The above link only explains role of bias in small neural network and does not attempt to explain role of bias in deep-networks containing multiple CNN layers, drop-outs, pooling and non-linear activation functions. I ran a simple experiment and the results indicated that removing bias from conv layer made no difference in final test accuracy.There are two models trained and the test-accuracy is almost same (slightly better in one without bias.)model_with_bias, model_without_bias( bias not added in conv layer)Are they being used only for historical reasons?If using bias provides no gain in accuracy, shouldn't we omit them? Less parameters to learn.I would be thankful if someone who have deeper knowledge than me, could explain the significance(if- any) of these bias in deep networks.Here is the complete code and the experiment result bias-VS-no_bias experiment Output:InitializedTest accuracy(with bias): 90.5%Test accuracy(without bias): 90.6% <code>  batch_size = 16patch_size = 5depth = 16num_hidden = 64graph = tf.Graph()with graph.as_default(): # Input data. tf_train_dataset = tf.placeholder( tf.float32, shape=(batch_size, image_size, image_size, num_channels)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # Variables. layer1_weights = tf.Variable(tf.truncated_normal( [patch_size, patch_size, num_channels, depth], stddev=0.1)) layer1_biases = tf.Variable(tf.zeros([depth])) layer2_weights = tf.Variable(tf.truncated_normal( [patch_size, patch_size, depth, depth], stddev=0.1)) layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth])) layer3_weights = tf.Variable(tf.truncated_normal( [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1)) layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden])) layer4_weights = tf.Variable(tf.truncated_normal( [num_hidden, num_labels], stddev=0.1)) layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels])) # define a Model with bias . def model_with_bias(data): conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME') hidden = tf.nn.relu(conv + layer1_biases) conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME') hidden = tf.nn.relu(conv + layer2_biases) shape = hidden.get_shape().as_list() reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]]) hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) return tf.matmul(hidden, layer4_weights) + layer4_biases # define a Model without bias added in the convolutional layer. def model_without_bias(data): conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME') hidden = tf.nn.relu(conv ) # layer1_ bias is not added conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME') hidden = tf.nn.relu(conv) # + layer2_biases) shape = hidden.get_shape().as_list() reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]]) # bias are added only in Fully connected layer(layer 3 and layer 4) hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) return tf.matmul(hidden, layer4_weights) + layer4_biases # Training computation. logits_with_bias = model_with_bias(tf_train_dataset) loss_with_bias = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_with_bias)) logits_without_bias = model_without_bias(tf_train_dataset) loss_without_bias = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_without_bias)) # Optimizer. optimizer_with_bias = tf.train.GradientDescentOptimizer(0.05).minimize(loss_with_bias) optimizer_without_bias = tf.train.GradientDescentOptimizer(0.05).minimize(loss_without_bias) # Predictions for the training, validation, and test data. train_prediction_with_bias = tf.nn.softmax(logits_with_bias) valid_prediction_with_bias = tf.nn.softmax(model_with_bias(tf_valid_dataset)) test_prediction_with_bias = tf.nn.softmax(model_with_bias(tf_test_dataset)) # Predictions for without train_prediction_without_bias = tf.nn.softmax(logits_without_bias) valid_prediction_without_bias = tf.nn.softmax(model_without_bias(tf_valid_dataset)) test_prediction_without_bias = tf.nn.softmax(model_without_bias(tf_test_dataset))num_steps = 1001with tf.Session(graph=graph) as session: tf.global_variables_initializer().run() print('Initialized') for step in range(num_steps): offset = (step * batch_size) % (train_labels.shape[0] - batch_size) batch_data = train_dataset[offset:(offset + batch_size), :, :, :] batch_labels = train_labels[offset:(offset + batch_size), :] feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels} session.run(optimizer_with_bias, feed_dict=feed_dict) session.run(optimizer_without_bias, feed_dict = feed_dict) print('Test accuracy(with bias): %.1f%%' % accuracy(test_prediction_with_bias.eval(), test_labels)) print('Test accuracy(without bias): %.1f%%' % accuracy(test_prediction_without_bias.eval(), test_labels))",Does bias in the convolutional layer really make a difference to the test accuracy?
python: which types support weak references?," Code: Output: Doc: Several built-in types such as list and dict do not directly support weak references but can add support through subclassing: ... Other built-in types such as tuple and int do not support weak references even when subclassed (This is an implementation detail and may be different across various Python implementations.).This isn't expressive enough to explain:Why some built-in types don't support weak references?What are exactly those types that support weak references?To add some thoughts:For the above example you can wrap the int within a user-defined wrapper class, and that wrapper class supports weak references (Those who are familiar with Java will recall int and Integer): I'm not sure why Python doesn't provide auto-wrapping for commonly used built-in types (int, str, etc.) but instead simply say they don't support weak references. It might be due to performance issues, but not being able to weakref these built-in types greatly reduced its usage.  <code>  from weakref import WeakSetse = WeakSet()se.add(1) TypeError: cannot create weak reference to 'int' object from weakref import WeakSetse = WeakSet()class Integer: def __init__(self, n=0): self.n = ni = 1I = Integer(1)se.add(i) # failse.add(I) # ok",Python: which types support weak references?
ssl/aiohttp: traceback even when error is handled," Trying to download and process jpegs from URLs. My issue isn't that certificate verification fails for some URLs, as these URLs are old and may no longer be trustworthy, but that when I try...except... the SSLCertVerificationError, I still get the traceback.System:Linux 4.17.14-arch1-1-ARCH, python 3.7.0-3, aiohttp 3.3.2Minimal example: Output: <code>  import asyncioimport aiohttpfrom ssl import SSLCertVerificationErrorasync def fetch_url(url, client): try: async with client.get(url) as resp: print(resp.status) print(await resp.read()) except SSLCertVerificationError as e: print('Error handled')async def main(urls): tasks = [] async with aiohttp.ClientSession(loop=loop) as client: for url in urls: task = asyncio.ensure_future(fetch_url(url, client)) tasks.append(task) return await asyncio.gather(*tasks)loop = asyncio.get_event_loop()loop.run_until_complete(main(['https://images.photos.com/'])) SSL handshake failed on verifying the certificateprotocol: <asyncio.sslproto.SSLProtocol object at 0x7ffbecad8ac8>transport: <_SelectorSocketTransport fd=6 read=polling write=<idle, bufsize=0>>Traceback (most recent call last): File ""/usr/lib/python3.7/asyncio/sslproto.py"", line 625, in _on_handshake_complete raise handshake_exc File ""/usr/lib/python3.7/asyncio/sslproto.py"", line 189, in feed_ssldata self._sslobj.do_handshake() File ""/usr/lib/python3.7/ssl.py"", line 763, in do_handshake self._sslobj.do_handshake()ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'images.photos.com'. (_ssl.c:1045)SSL error in data receivedprotocol: <asyncio.sslproto.SSLProtocol object at 0x7ffbecad8ac8>transport: <_SelectorSocketTransport closing fd=6 read=idle write=<idle, bufsize=0>>Traceback (most recent call last): File ""/usr/lib/python3.7/asyncio/sslproto.py"", line 526, in data_received ssldata, appdata = self._sslpipe.feed_ssldata(data) File ""/usr/lib/python3.7/asyncio/sslproto.py"", line 189, in feed_ssldata self._sslobj.do_handshake() File ""/usr/lib/python3.7/ssl.py"", line 763, in do_handshake self._sslobj.do_handshake()ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'images.photos.com'. (_ssl.c:1045)Error handled",ssl/asyncio: traceback even when error is handled
Keras - printing intermediate tensors in loss function (tf.Print and K.print_tensor do not work...)," I have written a rather complex loss function for a Keras model and it keeps returning nan while training. Therefore, I need to print the intermediate tensors while training. I understand that you cannot do K.eval in your loss function because the tensors are not initialized. However, I have tried both K.print_tensor() and tf.Print() and neither work. Pretty much I want to do something like this: In practice, I would replace mean_squared_error with my custom loss. ""mean_squared_error"" would get printed, but not the values I try to print using TensorFlow print (nor Keras print). I also tried the exact same code as in How do I print inside the loss function during training in Keras? and I still don't see anything getting printed in the console. In addition, I have written a separate file to test something. Nothing gets printed either. Am I using TensorFlow's Print and Keras print_tensor wrongly? Or are the results printed elsewhere? I have tried to test for my console's stderr using print(""test"", file=sys.stderr) and got the correct output test. For clarification, I know that you can use K.eval to make the test code print out values of the tensor, but since I cannot use K.eval in my loss function, I need to make tf.Print or K.print_tensor work.  <code>  def mean_squared_error(y_true, y_pred): print(""mean_squared_error"") loss = K.mean(K.square(y_pred - y_true), axis=-1) loss = tf.Print(loss, [loss]) return lossmodel.compile(optimizer=self.optimizer, loss=mean_squared_error) import tensorflow as tfimport keras.backend as Kinput1 = K.constant(1)input2 = K.constant(2)input3 = K.constant(3)node1 = tf.add(input1, input2)print_output = K.print_tensor(node1)output = tf.multiply(print_output, input3)",TensorFlow's Print or K.print_tensor are not printing intermediate tensors in loss function
pytest disable multiple plugins in pytest.ini," I have tests within the same repository (separate pytest.ini files) that require different pytest plugins. How can I disable multiple plugins in pytest.ini without uninstalling them?https://docs.pytest.org/en/latest/plugins.html#findpluginname works fine, but I also want to disable pytest-django and pytest-bdd for one of the test suites. How can I do that? I've tried: all fail, and the documentation doesn't describe how this is done. Any pointers greatly appreciated, thanks! <code>  addopts = --nomigrations --reuse-db -s -p no:pytest-splinter addopts = --nomigrations --reuse-db -s -p no:pytest-splinter -p no:pytest-django addopts = --nomigrations --reuse-db -s -p no:pytest-splinter no:pytest-django addopts = --nomigrations --reuse-db -s -p no:pytest-splinter pytest-django ",How to disable multiple plugins in pytest.ini?
Keras Dense layer's input is 3D(including batch size)," This is my test code: The output is: But What Happend?The documentation says: Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.While the output is reshaped? <code>  from keras import layersinput1 = layers.Input((2,3))output = layers.Dense(4)(input1)print(output) <tf.Tensor 'dense_2/add:0' shape=(?, 2, 4) dtype=float32>",Keras Dense layer's input is not flattened
Insert element into array into all possible positions," Is there a better way to insert, one by one, elements in an array to all posible positions (n+1 positions).E.g inserting [1] to [6 7 8 9] should produce: So if I insert A = [1 2 3] one by one to B = [6 7 8 9] it should produce: Currently I use numpy.roll like this: <code>  [1 6 7 8 9][9 1 6 7 8][8 9 1 6 7][7 8 9 1 6][6 7 8 9 1] [1 6 7 8 9][9 1 6 7 8][8 9 1 6 7][7 8 9 1 6][6 7 8 9 1]--------------------[2 6 7 8 9][9 2 6 7 8][8 9 2 6 7][7 8 9 2 6][6 7 8 9 2]--------------------[3 6 7 8 9][9 3 6 7 8][8 9 3 6 7][7 8 9 3 6][6 7 8 9 3]-------------------- import numpy as npimport timeitA = np.array([1, 2, 3, 4, 5])B = np.array([6, 7, 8, 9])def inject_one(Ad, Bd): for i, _ in enumerate(Ad): C = np.append(Ad[i], Bd) for _ in range(len(C) - 1): C = np.roll(C, 1)t = timeit.Timer(lambda: inject_one(A, B))print(""{:.3f}secs for 1000 iterations"".format(t.timeit(number=1000))) # > 0.160 secs",Insert element into numpy array and get all rolled permutations
"Create Toeplitz matrix, (insert element into array into all possible positions)"," Is there a better way to insert, one by one, elements in an array to all posible positions (n+1 positions).E.g inserting [1] to [6 7 8 9] should produce: So if I insert A = [1 2 3] one by one to B = [6 7 8 9] it should produce: Currently I use numpy.roll like this: <code>  [1 6 7 8 9][9 1 6 7 8][8 9 1 6 7][7 8 9 1 6][6 7 8 9 1] [1 6 7 8 9][9 1 6 7 8][8 9 1 6 7][7 8 9 1 6][6 7 8 9 1]--------------------[2 6 7 8 9][9 2 6 7 8][8 9 2 6 7][7 8 9 2 6][6 7 8 9 2]--------------------[3 6 7 8 9][9 3 6 7 8][8 9 3 6 7][7 8 9 3 6][6 7 8 9 3]-------------------- import numpy as npimport timeitA = np.array([1, 2, 3, 4, 5])B = np.array([6, 7, 8, 9])def inject_one(Ad, Bd): for i, _ in enumerate(Ad): C = np.append(Ad[i], Bd) for _ in range(len(C) - 1): C = np.roll(C, 1)t = timeit.Timer(lambda: inject_one(A, B))print(""{:.3f}secs for 1000 iterations"".format(t.timeit(number=1000))) # > 0.160 secs",Insert element into numpy array and get all rolled permutations
Create Toeplitz matrix in Python," Is there a better way to insert, one by one, elements in an array to all posible positions (n+1 positions).E.g inserting [1] to [6 7 8 9] should produce: So if I insert A = [1 2 3] one by one to B = [6 7 8 9] it should produce: Currently I use numpy.roll like this: <code>  [1 6 7 8 9][9 1 6 7 8][8 9 1 6 7][7 8 9 1 6][6 7 8 9 1] [1 6 7 8 9][9 1 6 7 8][8 9 1 6 7][7 8 9 1 6][6 7 8 9 1]--------------------[2 6 7 8 9][9 2 6 7 8][8 9 2 6 7][7 8 9 2 6][6 7 8 9 2]--------------------[3 6 7 8 9][9 3 6 7 8][8 9 3 6 7][7 8 9 3 6][6 7 8 9 3]-------------------- import numpy as npimport timeitA = np.array([1, 2, 3, 4, 5])B = np.array([6, 7, 8, 9])def inject_one(Ad, Bd): for i, _ in enumerate(Ad): C = np.append(Ad[i], Bd) for _ in range(len(C) - 1): C = np.roll(C, 1)t = timeit.Timer(lambda: inject_one(A, B))print(""{:.3f}secs for 1000 iterations"".format(t.timeit(number=1000))) # > 0.160 secs",Insert element into numpy array and get all rolled permutations
How to convert list of dictionaries into Spark DataFrame," I want to convert my list of dictionaries into DataFrame. This is the list: This is my code: I assume that I should provide some mapping and types for each column, but I don't know how to do it.Update:I also tried this: But then I get null values: <code>  mylist = [ {""type_activity_id"":1,""type_activity_name"":""xxx""}, {""type_activity_id"":2,""type_activity_name"":""yyy""}, {""type_activity_id"":3,""type_activity_name"":""zzz""}] from pyspark.sql.types import StringTypedf = spark.createDataFrame(mylist, StringType())df.show(2,False)+-----------------------------------------+| value|+-----------------------------------------+|{type_activity_id=1,type_activity_id=xxx}||{type_activity_id=2,type_activity_id=yyy}||{type_activity_id=3,type_activity_id=zzz}|+-----------------------------------------+ schema = ArrayType( StructType([StructField(""type_activity_id"", IntegerType()), StructField(""type_activity_name"", StringType()) ]))df = spark.createDataFrame(mylist, StringType())df = df.withColumn(""value"", from_json(df.value, schema)) +-----+|value|+-----+| null|| null|+-----+",How to convert list of dictionaries into Pyspark DataFrame
Convert DatetimeIndex to datetime.date in pandas?," I am trying to subtract today's date from a column in pandas to get the number of days(as an integer).I first converted the date's in column(ex: 27-Sep-2018) using pd.to_datetime.df['Date'] - datetime.datetime.now().date()I got the following error:TypeError: unsupported operand type(s) for -: 'DatetimeIndex' and 'datetime.date'I am trying to figure out how to get this to work, also converting the days to integer? <code> ",Convert DatetimeIndex to datetime.date in pandas
Python: Use of if else inside a dict to set a value to key," I am looking for a way to use if else condition in a dictionary to set a value to a key. Is there a way?The example below might help you understand what I want to do. This is not a functional Python code. Just an example to give you an idea.  <code>  age = 22di = { 'name': 'xyz', if age>=18: 'access_grant': 'yes', else: 'access_grant': 'no', } ",Use of if else inside a dict to set a value to key using Python
Python Programatically Change Console font size," I found the code below which is supposed to programmatically change the console font size. I'm on Windows 10.However, whatever values I tweak, I can't seem to get any control over the font size, and also for some reason the console which gets opened when I run this script is very wide.I have no idea how ctypes works - all I want is to modify the size of the console font from inside Python.Any actual working solutions? <code>  import ctypesLF_FACESIZE = 32STD_OUTPUT_HANDLE = -11class COORD(ctypes.Structure): _fields_ = [(""X"", ctypes.c_short), (""Y"", ctypes.c_short)]class CONSOLE_FONT_INFOEX(ctypes.Structure): _fields_ = [(""cbSize"", ctypes.c_ulong), (""nFont"", ctypes.c_ulong), (""dwFontSize"", COORD), (""FontFamily"", ctypes.c_uint), (""FontWeight"", ctypes.c_uint), (""FaceName"", ctypes.c_wchar * LF_FACESIZE)]font = CONSOLE_FONT_INFOEX()font.cbSize = ctypes.sizeof(CONSOLE_FONT_INFOEX)font.nFont = 12font.dwFontSize.X = 11font.dwFontSize.Y = 18font.FontFamily = 54font.FontWeight = 400font.FaceName = ""Lucida Console""handle = ctypes.windll.kernel32.GetStdHandle(STD_OUTPUT_HANDLE)ctypes.windll.kernel32.SetCurrentConsoleFontEx( handle, ctypes.c_long(False), ctypes.pointer(font))print(""Foo"")",Python Programmatically Change Console font size
How to run a layer on input data quickly in Keras?," Is there an easy way to give data to a layer in Keras (over TF) and see the return values, for test purposes, without actually building a full model and fitting data to it?If not, how can one test a customized layer that they develop? <code> ",How to apply (call) a single layer on data in Keras?
Nested cross-validation with XGBoost," One way to do nested cross-validation with a XGB model would be: However, regarding the tuning of XGB parameters, several tutorials (such as this one) take advantage of the Python hyperopt library. I would like to be able to do nested cross-validation (as above) using hyperopt to tune the XGB parameters.To do so, I wrote my own Scikit-Learn estimator: My questions are:Is this a valid approach? For instance, in the fit method of my OptimizedXGB, best.fit(X, y) will train a XGB model on X, y. However, this might lead to overfitting as no eval_set is specified to ensure early stopping.On a toy example (the famous iris dataset), this OptimizedXGB performs worse than a basic LogisticRegression classifier. Why is that? Is it because the example is to simplistic? See below for the code of the example.Example: Outputs: Although the scores are close, I would have expected the XGB model to score at least as well as a LogisticRegression classifier. EDIT:similar post <code>  from sklearn.model_selection import GridSearchCV, cross_val_scorefrom xgboost import XGBClassifier# Let's assume that we have some data for a binary classification# problem : X (n_samples, n_features) and y (n_samples,)...gs = GridSearchCV(estimator=XGBClassifier(), param_grid={'max_depth': [3, 6, 9], 'learning_rate': [0.001, 0.01, 0.05]}, cv=2)scores = cross_val_score(gs, X, y, cv=2) from hyperopt import fmin, tpe, hp, Trials, STATUS_OKfrom sklearn.base import BaseEstimator, ClassifierMixinfrom sklearn.model_selection import train_test_splitfrom sklearn.exceptions import NotFittedErrorfrom sklearn.metrics import roc_auc_scorefrom xgboost import XGBClassifierdef optimize_params(X, y, params_space, validation_split=0.2): """"""Estimate a set of 'best' model parameters."""""" # Split X, y into train/validation X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=validation_split, stratify=y) # Estimate XGB params def objective(_params): _clf = XGBClassifier(n_estimators=10000, max_depth=int(_params['max_depth']), learning_rate=_params['learning_rate'], min_child_weight=_params['min_child_weight'], subsample=_params['subsample'], colsample_bytree=_params['colsample_bytree'], gamma=_params['gamma']) _clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric='auc', early_stopping_rounds=30) y_pred_proba = _clf.predict_proba(X_val)[:, 1] roc_auc = roc_auc_score(y_true=y_val, y_score=y_pred_proba) return {'loss': 1. - roc_auc, 'status': STATUS_OK} trials = Trials() return fmin(fn=objective, space=params_space, algo=tpe.suggest, max_evals=100, trials=trials, verbose=0)class OptimizedXGB(BaseEstimator, ClassifierMixin): """"""XGB with optimized parameters. Parameters ---------- custom_params_space : dict or None If not None, dictionary whose keys are the XGB parameters to be optimized and corresponding values are 'a priori' probability distributions for the given parameter value. If None, a default parameters space is used. """""" def __init__(self, custom_params_space=None): self.custom_params_space = custom_params_space def fit(self, X, y, validation_split=0.3): """"""Train a XGB model. Parameters ---------- X : ndarray, shape (n_samples, n_features) Data. y : ndarray, shape (n_samples,) or (n_samples, n_labels) Labels. validation_split : float (default: 0.3) Float between 0 and 1. Corresponds to the percentage of samples in X which will be used as validation data to estimate the 'best' model parameters. """""" # If no custom parameters space is given, use a default one. if self.custom_params_space is None: _space = { 'learning_rate': hp.uniform('learning_rate', 0.0001, 0.05), 'max_depth': hp.quniform('max_depth', 8, 15, 1), 'min_child_weight': hp.quniform('min_child_weight', 1, 5, 1), 'subsample': hp.quniform('subsample', 0.7, 1, 0.05), 'gamma': hp.quniform('gamma', 0.9, 1, 0.05), 'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 0.7, 0.05) } else: _space = self.custom_params_space # Estimate best params using X, y opt = optimize_params(X, y, _space, validation_split) # Instantiate `xgboost.XGBClassifier` with optimized parameters best = XGBClassifier(n_estimators=10000, max_depth=int(opt['max_depth']), learning_rate=opt['learning_rate'], min_child_weight=opt['min_child_weight'], subsample=opt['subsample'], gamma=opt['gamma'], colsample_bytree=opt['colsample_bytree']) best.fit(X, y) self.best_estimator_ = best return self def predict(self, X): """"""Predict labels with trained XGB model. Parameters ---------- X : ndarray, shape (n_samples, n_features) Returns ------- output : ndarray, shape (n_samples,) or (n_samples, n_labels) """""" if not hasattr(self, 'best_estimator_'): raise NotFittedError('Call `fit` before `predict`.') else: return self.best_estimator_.predict(X) def predict_proba(self, X): """"""Predict labels probaiblities with trained XGB model. Parameters ---------- X : ndarray, shape (n_samples, n_features) Returns ------- output : ndarray, shape (n_samples,) or (n_samples, n_labels) """""" if not hasattr(self, 'best_estimator_'): raise NotFittedError('Call `fit` before `predict_proba`.') else: return self.best_estimator_.predict_proba(X) import numpy as npfrom sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFoldfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalerX, y = load_iris(return_X_y=True)X = X[:, :2]X = X[y < 2]y = y[y < 2]skf = StratifiedKFold(n_splits=2, random_state=42)# With a LogisticRegression classifierpipe = Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression())])gs = GridSearchCV(estimator=pipe, param_grid={'lr__C': [1., 10.]})lr_scores = cross_val_score(gs, X, y, cv=skf)# With OptimizedXGBxgb_scores = cross_val_score(OptimizedXGB(), X, y, cv=skf)# Print resultsprint('Accuracy with LogisticRegression = %.4f (+/- %.4f)' % (np.mean(lr_scores), np.std(lr_scores)))print('Accuracy with OptimizedXGB = %.4f (+/- %.4f)' % (np.mean(xgb_scores), np.std(xgb_scores))) Accuracy with LogisticRegression = 0.9900 (+/- 0.0100)Accuracy with OptimizedXGB = 0.9100 (+/- 0.0300)",Cross-validation and parameters tuning with XGBoost and hyperopt
Nested cross-validation with XGBoost and hyperopt," One way to do nested cross-validation with a XGB model would be: However, regarding the tuning of XGB parameters, several tutorials (such as this one) take advantage of the Python hyperopt library. I would like to be able to do nested cross-validation (as above) using hyperopt to tune the XGB parameters.To do so, I wrote my own Scikit-Learn estimator: My questions are:Is this a valid approach? For instance, in the fit method of my OptimizedXGB, best.fit(X, y) will train a XGB model on X, y. However, this might lead to overfitting as no eval_set is specified to ensure early stopping.On a toy example (the famous iris dataset), this OptimizedXGB performs worse than a basic LogisticRegression classifier. Why is that? Is it because the example is to simplistic? See below for the code of the example.Example: Outputs: Although the scores are close, I would have expected the XGB model to score at least as well as a LogisticRegression classifier. EDIT:similar post <code>  from sklearn.model_selection import GridSearchCV, cross_val_scorefrom xgboost import XGBClassifier# Let's assume that we have some data for a binary classification# problem : X (n_samples, n_features) and y (n_samples,)...gs = GridSearchCV(estimator=XGBClassifier(), param_grid={'max_depth': [3, 6, 9], 'learning_rate': [0.001, 0.01, 0.05]}, cv=2)scores = cross_val_score(gs, X, y, cv=2) from hyperopt import fmin, tpe, hp, Trials, STATUS_OKfrom sklearn.base import BaseEstimator, ClassifierMixinfrom sklearn.model_selection import train_test_splitfrom sklearn.exceptions import NotFittedErrorfrom sklearn.metrics import roc_auc_scorefrom xgboost import XGBClassifierdef optimize_params(X, y, params_space, validation_split=0.2): """"""Estimate a set of 'best' model parameters."""""" # Split X, y into train/validation X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=validation_split, stratify=y) # Estimate XGB params def objective(_params): _clf = XGBClassifier(n_estimators=10000, max_depth=int(_params['max_depth']), learning_rate=_params['learning_rate'], min_child_weight=_params['min_child_weight'], subsample=_params['subsample'], colsample_bytree=_params['colsample_bytree'], gamma=_params['gamma']) _clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric='auc', early_stopping_rounds=30) y_pred_proba = _clf.predict_proba(X_val)[:, 1] roc_auc = roc_auc_score(y_true=y_val, y_score=y_pred_proba) return {'loss': 1. - roc_auc, 'status': STATUS_OK} trials = Trials() return fmin(fn=objective, space=params_space, algo=tpe.suggest, max_evals=100, trials=trials, verbose=0)class OptimizedXGB(BaseEstimator, ClassifierMixin): """"""XGB with optimized parameters. Parameters ---------- custom_params_space : dict or None If not None, dictionary whose keys are the XGB parameters to be optimized and corresponding values are 'a priori' probability distributions for the given parameter value. If None, a default parameters space is used. """""" def __init__(self, custom_params_space=None): self.custom_params_space = custom_params_space def fit(self, X, y, validation_split=0.3): """"""Train a XGB model. Parameters ---------- X : ndarray, shape (n_samples, n_features) Data. y : ndarray, shape (n_samples,) or (n_samples, n_labels) Labels. validation_split : float (default: 0.3) Float between 0 and 1. Corresponds to the percentage of samples in X which will be used as validation data to estimate the 'best' model parameters. """""" # If no custom parameters space is given, use a default one. if self.custom_params_space is None: _space = { 'learning_rate': hp.uniform('learning_rate', 0.0001, 0.05), 'max_depth': hp.quniform('max_depth', 8, 15, 1), 'min_child_weight': hp.quniform('min_child_weight', 1, 5, 1), 'subsample': hp.quniform('subsample', 0.7, 1, 0.05), 'gamma': hp.quniform('gamma', 0.9, 1, 0.05), 'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 0.7, 0.05) } else: _space = self.custom_params_space # Estimate best params using X, y opt = optimize_params(X, y, _space, validation_split) # Instantiate `xgboost.XGBClassifier` with optimized parameters best = XGBClassifier(n_estimators=10000, max_depth=int(opt['max_depth']), learning_rate=opt['learning_rate'], min_child_weight=opt['min_child_weight'], subsample=opt['subsample'], gamma=opt['gamma'], colsample_bytree=opt['colsample_bytree']) best.fit(X, y) self.best_estimator_ = best return self def predict(self, X): """"""Predict labels with trained XGB model. Parameters ---------- X : ndarray, shape (n_samples, n_features) Returns ------- output : ndarray, shape (n_samples,) or (n_samples, n_labels) """""" if not hasattr(self, 'best_estimator_'): raise NotFittedError('Call `fit` before `predict`.') else: return self.best_estimator_.predict(X) def predict_proba(self, X): """"""Predict labels probaiblities with trained XGB model. Parameters ---------- X : ndarray, shape (n_samples, n_features) Returns ------- output : ndarray, shape (n_samples,) or (n_samples, n_labels) """""" if not hasattr(self, 'best_estimator_'): raise NotFittedError('Call `fit` before `predict_proba`.') else: return self.best_estimator_.predict_proba(X) import numpy as npfrom sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFoldfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalerX, y = load_iris(return_X_y=True)X = X[:, :2]X = X[y < 2]y = y[y < 2]skf = StratifiedKFold(n_splits=2, random_state=42)# With a LogisticRegression classifierpipe = Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression())])gs = GridSearchCV(estimator=pipe, param_grid={'lr__C': [1., 10.]})lr_scores = cross_val_score(gs, X, y, cv=skf)# With OptimizedXGBxgb_scores = cross_val_score(OptimizedXGB(), X, y, cv=skf)# Print resultsprint('Accuracy with LogisticRegression = %.4f (+/- %.4f)' % (np.mean(lr_scores), np.std(lr_scores)))print('Accuracy with OptimizedXGB = %.4f (+/- %.4f)' % (np.mean(xgb_scores), np.std(xgb_scores))) Accuracy with LogisticRegression = 0.9900 (+/- 0.0100)Accuracy with OptimizedXGB = 0.9100 (+/- 0.0300)",Cross-validation and parameters tuning with XGBoost and hyperopt
Nested cross-validation and parameter tuning with XGBoost and hyperopt," One way to do nested cross-validation with a XGB model would be: However, regarding the tuning of XGB parameters, several tutorials (such as this one) take advantage of the Python hyperopt library. I would like to be able to do nested cross-validation (as above) using hyperopt to tune the XGB parameters.To do so, I wrote my own Scikit-Learn estimator: My questions are:Is this a valid approach? For instance, in the fit method of my OptimizedXGB, best.fit(X, y) will train a XGB model on X, y. However, this might lead to overfitting as no eval_set is specified to ensure early stopping.On a toy example (the famous iris dataset), this OptimizedXGB performs worse than a basic LogisticRegression classifier. Why is that? Is it because the example is to simplistic? See below for the code of the example.Example: Outputs: Although the scores are close, I would have expected the XGB model to score at least as well as a LogisticRegression classifier. EDIT:similar post <code>  from sklearn.model_selection import GridSearchCV, cross_val_scorefrom xgboost import XGBClassifier# Let's assume that we have some data for a binary classification# problem : X (n_samples, n_features) and y (n_samples,)...gs = GridSearchCV(estimator=XGBClassifier(), param_grid={'max_depth': [3, 6, 9], 'learning_rate': [0.001, 0.01, 0.05]}, cv=2)scores = cross_val_score(gs, X, y, cv=2) from hyperopt import fmin, tpe, hp, Trials, STATUS_OKfrom sklearn.base import BaseEstimator, ClassifierMixinfrom sklearn.model_selection import train_test_splitfrom sklearn.exceptions import NotFittedErrorfrom sklearn.metrics import roc_auc_scorefrom xgboost import XGBClassifierdef optimize_params(X, y, params_space, validation_split=0.2): """"""Estimate a set of 'best' model parameters."""""" # Split X, y into train/validation X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=validation_split, stratify=y) # Estimate XGB params def objective(_params): _clf = XGBClassifier(n_estimators=10000, max_depth=int(_params['max_depth']), learning_rate=_params['learning_rate'], min_child_weight=_params['min_child_weight'], subsample=_params['subsample'], colsample_bytree=_params['colsample_bytree'], gamma=_params['gamma']) _clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric='auc', early_stopping_rounds=30) y_pred_proba = _clf.predict_proba(X_val)[:, 1] roc_auc = roc_auc_score(y_true=y_val, y_score=y_pred_proba) return {'loss': 1. - roc_auc, 'status': STATUS_OK} trials = Trials() return fmin(fn=objective, space=params_space, algo=tpe.suggest, max_evals=100, trials=trials, verbose=0)class OptimizedXGB(BaseEstimator, ClassifierMixin): """"""XGB with optimized parameters. Parameters ---------- custom_params_space : dict or None If not None, dictionary whose keys are the XGB parameters to be optimized and corresponding values are 'a priori' probability distributions for the given parameter value. If None, a default parameters space is used. """""" def __init__(self, custom_params_space=None): self.custom_params_space = custom_params_space def fit(self, X, y, validation_split=0.3): """"""Train a XGB model. Parameters ---------- X : ndarray, shape (n_samples, n_features) Data. y : ndarray, shape (n_samples,) or (n_samples, n_labels) Labels. validation_split : float (default: 0.3) Float between 0 and 1. Corresponds to the percentage of samples in X which will be used as validation data to estimate the 'best' model parameters. """""" # If no custom parameters space is given, use a default one. if self.custom_params_space is None: _space = { 'learning_rate': hp.uniform('learning_rate', 0.0001, 0.05), 'max_depth': hp.quniform('max_depth', 8, 15, 1), 'min_child_weight': hp.quniform('min_child_weight', 1, 5, 1), 'subsample': hp.quniform('subsample', 0.7, 1, 0.05), 'gamma': hp.quniform('gamma', 0.9, 1, 0.05), 'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 0.7, 0.05) } else: _space = self.custom_params_space # Estimate best params using X, y opt = optimize_params(X, y, _space, validation_split) # Instantiate `xgboost.XGBClassifier` with optimized parameters best = XGBClassifier(n_estimators=10000, max_depth=int(opt['max_depth']), learning_rate=opt['learning_rate'], min_child_weight=opt['min_child_weight'], subsample=opt['subsample'], gamma=opt['gamma'], colsample_bytree=opt['colsample_bytree']) best.fit(X, y) self.best_estimator_ = best return self def predict(self, X): """"""Predict labels with trained XGB model. Parameters ---------- X : ndarray, shape (n_samples, n_features) Returns ------- output : ndarray, shape (n_samples,) or (n_samples, n_labels) """""" if not hasattr(self, 'best_estimator_'): raise NotFittedError('Call `fit` before `predict`.') else: return self.best_estimator_.predict(X) def predict_proba(self, X): """"""Predict labels probaiblities with trained XGB model. Parameters ---------- X : ndarray, shape (n_samples, n_features) Returns ------- output : ndarray, shape (n_samples,) or (n_samples, n_labels) """""" if not hasattr(self, 'best_estimator_'): raise NotFittedError('Call `fit` before `predict_proba`.') else: return self.best_estimator_.predict_proba(X) import numpy as npfrom sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFoldfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalerX, y = load_iris(return_X_y=True)X = X[:, :2]X = X[y < 2]y = y[y < 2]skf = StratifiedKFold(n_splits=2, random_state=42)# With a LogisticRegression classifierpipe = Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression())])gs = GridSearchCV(estimator=pipe, param_grid={'lr__C': [1., 10.]})lr_scores = cross_val_score(gs, X, y, cv=skf)# With OptimizedXGBxgb_scores = cross_val_score(OptimizedXGB(), X, y, cv=skf)# Print resultsprint('Accuracy with LogisticRegression = %.4f (+/- %.4f)' % (np.mean(lr_scores), np.std(lr_scores)))print('Accuracy with OptimizedXGB = %.4f (+/- %.4f)' % (np.mean(xgb_scores), np.std(xgb_scores))) Accuracy with LogisticRegression = 0.9900 (+/- 0.0100)Accuracy with OptimizedXGB = 0.9100 (+/- 0.0300)",Cross-validation and parameters tuning with XGBoost and hyperopt
Nested cross-validation and parameters tuning with XGBoost and hyperopt," One way to do nested cross-validation with a XGB model would be: However, regarding the tuning of XGB parameters, several tutorials (such as this one) take advantage of the Python hyperopt library. I would like to be able to do nested cross-validation (as above) using hyperopt to tune the XGB parameters.To do so, I wrote my own Scikit-Learn estimator: My questions are:Is this a valid approach? For instance, in the fit method of my OptimizedXGB, best.fit(X, y) will train a XGB model on X, y. However, this might lead to overfitting as no eval_set is specified to ensure early stopping.On a toy example (the famous iris dataset), this OptimizedXGB performs worse than a basic LogisticRegression classifier. Why is that? Is it because the example is to simplistic? See below for the code of the example.Example: Outputs: Although the scores are close, I would have expected the XGB model to score at least as well as a LogisticRegression classifier. EDIT:similar post <code>  from sklearn.model_selection import GridSearchCV, cross_val_scorefrom xgboost import XGBClassifier# Let's assume that we have some data for a binary classification# problem : X (n_samples, n_features) and y (n_samples,)...gs = GridSearchCV(estimator=XGBClassifier(), param_grid={'max_depth': [3, 6, 9], 'learning_rate': [0.001, 0.01, 0.05]}, cv=2)scores = cross_val_score(gs, X, y, cv=2) from hyperopt import fmin, tpe, hp, Trials, STATUS_OKfrom sklearn.base import BaseEstimator, ClassifierMixinfrom sklearn.model_selection import train_test_splitfrom sklearn.exceptions import NotFittedErrorfrom sklearn.metrics import roc_auc_scorefrom xgboost import XGBClassifierdef optimize_params(X, y, params_space, validation_split=0.2): """"""Estimate a set of 'best' model parameters."""""" # Split X, y into train/validation X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=validation_split, stratify=y) # Estimate XGB params def objective(_params): _clf = XGBClassifier(n_estimators=10000, max_depth=int(_params['max_depth']), learning_rate=_params['learning_rate'], min_child_weight=_params['min_child_weight'], subsample=_params['subsample'], colsample_bytree=_params['colsample_bytree'], gamma=_params['gamma']) _clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric='auc', early_stopping_rounds=30) y_pred_proba = _clf.predict_proba(X_val)[:, 1] roc_auc = roc_auc_score(y_true=y_val, y_score=y_pred_proba) return {'loss': 1. - roc_auc, 'status': STATUS_OK} trials = Trials() return fmin(fn=objective, space=params_space, algo=tpe.suggest, max_evals=100, trials=trials, verbose=0)class OptimizedXGB(BaseEstimator, ClassifierMixin): """"""XGB with optimized parameters. Parameters ---------- custom_params_space : dict or None If not None, dictionary whose keys are the XGB parameters to be optimized and corresponding values are 'a priori' probability distributions for the given parameter value. If None, a default parameters space is used. """""" def __init__(self, custom_params_space=None): self.custom_params_space = custom_params_space def fit(self, X, y, validation_split=0.3): """"""Train a XGB model. Parameters ---------- X : ndarray, shape (n_samples, n_features) Data. y : ndarray, shape (n_samples,) or (n_samples, n_labels) Labels. validation_split : float (default: 0.3) Float between 0 and 1. Corresponds to the percentage of samples in X which will be used as validation data to estimate the 'best' model parameters. """""" # If no custom parameters space is given, use a default one. if self.custom_params_space is None: _space = { 'learning_rate': hp.uniform('learning_rate', 0.0001, 0.05), 'max_depth': hp.quniform('max_depth', 8, 15, 1), 'min_child_weight': hp.quniform('min_child_weight', 1, 5, 1), 'subsample': hp.quniform('subsample', 0.7, 1, 0.05), 'gamma': hp.quniform('gamma', 0.9, 1, 0.05), 'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 0.7, 0.05) } else: _space = self.custom_params_space # Estimate best params using X, y opt = optimize_params(X, y, _space, validation_split) # Instantiate `xgboost.XGBClassifier` with optimized parameters best = XGBClassifier(n_estimators=10000, max_depth=int(opt['max_depth']), learning_rate=opt['learning_rate'], min_child_weight=opt['min_child_weight'], subsample=opt['subsample'], gamma=opt['gamma'], colsample_bytree=opt['colsample_bytree']) best.fit(X, y) self.best_estimator_ = best return self def predict(self, X): """"""Predict labels with trained XGB model. Parameters ---------- X : ndarray, shape (n_samples, n_features) Returns ------- output : ndarray, shape (n_samples,) or (n_samples, n_labels) """""" if not hasattr(self, 'best_estimator_'): raise NotFittedError('Call `fit` before `predict`.') else: return self.best_estimator_.predict(X) def predict_proba(self, X): """"""Predict labels probaiblities with trained XGB model. Parameters ---------- X : ndarray, shape (n_samples, n_features) Returns ------- output : ndarray, shape (n_samples,) or (n_samples, n_labels) """""" if not hasattr(self, 'best_estimator_'): raise NotFittedError('Call `fit` before `predict_proba`.') else: return self.best_estimator_.predict_proba(X) import numpy as npfrom sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFoldfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalerX, y = load_iris(return_X_y=True)X = X[:, :2]X = X[y < 2]y = y[y < 2]skf = StratifiedKFold(n_splits=2, random_state=42)# With a LogisticRegression classifierpipe = Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression())])gs = GridSearchCV(estimator=pipe, param_grid={'lr__C': [1., 10.]})lr_scores = cross_val_score(gs, X, y, cv=skf)# With OptimizedXGBxgb_scores = cross_val_score(OptimizedXGB(), X, y, cv=skf)# Print resultsprint('Accuracy with LogisticRegression = %.4f (+/- %.4f)' % (np.mean(lr_scores), np.std(lr_scores)))print('Accuracy with OptimizedXGB = %.4f (+/- %.4f)' % (np.mean(xgb_scores), np.std(xgb_scores))) Accuracy with LogisticRegression = 0.9900 (+/- 0.0100)Accuracy with OptimizedXGB = 0.9100 (+/- 0.0300)",Cross-validation and parameters tuning with XGBoost and hyperopt
Get a Discord Role by Id with Discord.py," I'm making a Discord bot but just ran into a problem.I want to modify a role. A specific role. I know how to do that with edit_role, but I need to get the Role object to edit it. Now, that's the problem.How do I get a Role object by the role's id? Or can I use the id in the Role argument? <code> ",Get a Discord Role by Id
Basic auth autentication in Bottle," How can i perform basic authentication in bottle framework? in flask i used to: and called as: How can i achieve the same in Bottle framework? <code>  def check( username, password ): # This function is called to check if a username/password combination is valid return username == 'nikos' and password == '******'def authenticate(): # Sends a 401 response that enables basic auth return Response( 'Credentials of a registered user required!', 401, {'WWW-Authenticate': 'Basic realm=""User!""'} ) auth = request.authorizationif not auth or not counters.check( auth.username, auth.password ): return counters.authenticate()",Basic auth authentication in Bottle
How to get column name for a given row value (in a Pandas dataframe)?," I have a pretty simple question - I think - but it seems I can't wrap my head around this one. I am a beginner with Python and Pandas. I searched the forum but couldn't get a (recent) answer that fits my need.I have a data frame such as this one: Which gives: My question is simple : I would like to add a column that gives the column name of the second max value of each row.I have written a simple function which returns the second max value for each row Which gives: But I can't find how to display the column name in the 'value' column, instead of the value... I'm thinking about boolean indexing (comparing the 'value' column values with each row), but I haven't figured out how to do it. To be clearer, I would like it to be: Any help (and explanation) appreciated! <code>  df = pd.DataFrame({'A': [1.1, 2.7, 5.3], 'B': [2, 10, 9], 'C': [3.3, 5.4, 1.5], 'D': [4, 7, 15]}, index = ['a1', 'a2', 'a3']) A B C D a1 1.1 2 3.3 4 a2 2.7 10 5.4 7 a3 5.3 9 1.5 15 def get_second_best(x): return sorted(x)[-2]df['value'] = df.apply(lambda row: get_second_best(row), axis=1) A B C D valuea1 1.1 2 3.3 4 3.3a2 2.7 10 5.4 7 7.0a3 5.3 9 1.5 15 9.0 A B C D valuea1 1.1 2 3.3 4 Ca2 2.7 10 5.4 7 Da3 5.3 9 1.5 15 B",How to get column name for second largest row value in pandas DataFrame
How to get foreignkey field name insted of id in django rest framework," Here is my models.py here is my serializers.py here is my views.py I am getting api like this..Here i am getting id instead of company_name. But i am expecting output--like this: I am expecting my api like this. with company_name.And i wants to post the data from same api in rest frameworkthanks,, <code>  from __future__ import unicode_literalsfrom django.db import modelsclass User(models.Model): name = models.CharField(max_length=200) company_name = models.ForeignKey('Company',on_delete=models.CASCADE,related_name='user') def __str__(self): return self.nameclass Company(models.Model): name = models.CharField(max_length=200) phone_number = models.IntegerField(null=True,blank=True) def __str__(self): return self.nameclass Catalog(models.Model): name = models.CharField(max_length=200) no_of_pcs = models.IntegerField(null=True,blank=True) per_piece_price = models.DecimalField(null=True,blank=True,max_digits=10,decimal_places=2) company_name = models.ForeignKey(Company,on_delete=models.CASCADE,related_name='catalog') def __str__(self): return self.name from rest_framework import serializersfrom .models import *from django.db.models import Sum,Countclass UserSerializer(serializers.ModelSerializer): # name = serializers.StringRelatedField() # company_name = serializers.CharField() class Meta: model = User fields = '__all__' from __future__ import unicode_literalsfrom django.http import HttpResponsefrom .models import *import jsonfrom django.http import JsonResponse, HttpResponsefrom .serializers import *from rest_framework.views import APIViewfrom rest_framework.response import Responsefrom rest_framework import statusfrom rest_framework import viewsetsclass UserView(viewsets.ModelViewSet): queryset = User.objects.all() serializer_class = UserSerializer [ { ""id"": 1, ""name"": ""soubhagya"", ""company_name"": 1 }, { ""id"": 2, ""name"": ""nihar"", ""company_name"": 2 }] [ { ""id"": 1, ""name"": ""soubhagya"", ""company_name"": ""google"" }, { ""id"": 2, ""name"": ""nihar"", ""company_name"": ""facebook"" }]",How to get foreignkey field name instead of id in django rest framework
Python operator precedence with +=, It seems this question only answered for Java but I would like to know how it works in Python. So are these the same? and <code>  a += b / 2 a += (b / 2),Python operator precedence with augmented assignment
Python list sort based on another shorter list," I need to sort a list based on the order of the elements in another list which is shorter ie, doesn't have all the elements compared to the list I'm sorting. I run into this error when using the sort(key=short_list): Is there another way to sort the long_list to result in a list that maintains the order of short_list followed by the order of the elements in the long_list? <code>  long_list = ['y', 'z', 'x', 'c', 'a', 'b']short_list = ['b', 'c', 'a']long_list.sort(key=short_list.index)ValueError: 'x' is not in list ['b', 'c', 'a', 'y', 'z', 'x']",List sort based on another shorter list
feature_names mistmach in xgboost despite having same columns," I have training (X) and test data (test_data_process) set with the same columns and order, as indicated below:But when I do It gives the following error: ValueError: feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34'] ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'YrMoSold'] expected f22, f25, f0, f34, f32, f5, f20, f3, f33, f15, f24, f31, f28, f9, f8, f19, f14, f18, f17, f2, f13, f4, f27, f16, f1, f29, f11, f26, f10, f7, f21, f30, f23, f6, f12 in input data training data did not have the following fields: OpenPorchSF, BsmtFinSF1, LotFrontage, GrLivArea, YrMoSold, FullBath, TotRmsAbvGrd, GarageCars, YearRemodAdd, BedroomAbvGr, PoolArea, KitchenAbvGr, LotArea, HalfBath, MiscVal, EnclosedPorch, BsmtUnfSF, MSSubClass, BsmtFullBath, YearBuilt, 1stFlrSF, ScreenPorch, 3SsnPorch, TotalBsmtSF, GarageYrBlt, MasVnrArea, OverallQual, Fireplaces, WoodDeckSF, 2ndFlrSF, BsmtFinSF2, BsmtHalfBath, LowQualFinSF, OverallCond, GarageAreaSo it complains that the training data (X) does not have those fields, whereas it has.How to solve this issue?[UPDATE]:My code: <code>  predictions = my_model.predict(test_data_process) X = data.select_dtypes(exclude=['object']).drop(columns=['Id'])X['YrMoSold'] = X['YrSold'] * 12 + X['MoSold']X = X.drop(columns=['YrSold', 'MoSold', 'SalePrice'])X = X.fillna(0.0000001)train_X, val_X, train_y, val_y = train_test_split(X.values, y.values, test_size=0.2)my_model = XGBRegressor(n_estimators=100, learning_rate=0.05, booster='gbtree')my_model.fit(train_X, train_y, early_stopping_rounds=5, eval_set=[(val_X, val_y)], verbose=False)test_data_process = test_data.select_dtypes(exclude=['object']).drop(columns=['Id'])test_data_process['YrMoSold'] = test_data_process['YrSold'] * 12 + test_data['MoSold']test_data_process = test_data_process.drop(columns=['YrSold', 'MoSold'])test_data_process = test_data_process.fillna(0.0000001)test_data_process = test_data_process[X.columns]predictions = my_model.predict(test_data_process) ",feature_names mismach in xgboost despite having same columns
Scatter 3D using GL in Plotly," I have a 3D scatter which wanna plot using Plotly in python. The problem is size of the dataframe is too large and I want to use webgl to plot the graph. As I know plotly has go.Scatter3d function to plot scatters. Also, there is a go.Scattergl to plot large datasets. However, U can't find something like go.Scatter3Dgl. What should I do? <code> ",Scatter 3D for Large Data-Set in Plotly
"pip._vendor.pkg_resources.RequirementParseError: Invalid requirement, parse error at ""'; extra '"""," I'm trying to spin up an nvidia-docker (2.0) container in Ubuntu 16.04 running Conda with a few python libraries (GPU-enabled tensorflow, opencv, and gdal) and their various dependencies. General explanation of the problemI have a few libraries that I need to install using pip within that environment (e.g. tensorflow-gpu 1.10.0 and a couple of other custom libraries), but anytime I try to install a package using pip either in my Dockerfile or after, I get the following error: This same error occurs if I try to install other packages using pip instead of conda install (others I've tried are setuptools, h5py, and several more I can't remember)DockerfileHere's the Dockerfile used to generate this, up to where the error is thrown (while installing tensorflow): The last RUN command throws the error.Things I've tried:Different versions of pip (10.0.1 and 18.0)pip upgrade pip or conda upgrade pip (trying to update pip this way throws the same error)updating setuptools (throws the same error if I try to do it with pip)I've checked to make sure that the correct pip is being used by prepending echo $(which pip) && before my pip install commands - it returns the virtual environment's pip.I would just install everything using conda and ignore it, but there are a couple of libraries internal to my group which aren't available for conda. <code>  Exception:Traceback (most recent call last): File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py"", line 2869, in _dep_map return self.__dep_map File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py"", line 2663, in __getattr__ raise AttributeError(attr)AttributeError: _DistInfoDistribution__dep_mapDuring handling of the above exception, another exception occurred:Traceback (most recent call last): File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_vendor/packaging/requirements.py"", line 93, in __init__ req = REQUIREMENT.parseString(requirement_string) File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_vendor/pyparsing.py"", line 1632, in parseString raise exc File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_vendor/pyparsing.py"", line 1622, in parseString loc, tokens = self._parse( instring, 0 ) File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_vendor/pyparsing.py"", line 1379, in _parseNoCache loc,tokens = self.parseImpl( instring, preloc, doActions ) File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_vendor/pyparsing.py"", line 3395, in parseImpl loc, exprtokens = e._parse( instring, loc, doActions ) File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_vendor/pyparsing.py"", line 1383, in _parseNoCache loc,tokens = self.parseImpl( instring, preloc, doActions ) File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_vendor/pyparsing.py"", line 3183, in parseImpl raise ParseException(instring, loc, self.errmsg, self)pip._vendor.pyparsing.ParseException: Expected stringEnd (at char 33), (line:1, col:34)During handling of the above exception, another exception occurred:Traceback (most recent call last): File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py"", line 2949, in __init__ super(Requirement, self).__init__(requirement_string) File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_vendor/packaging/requirements.py"", line 97, in __init__ requirement_string[e.loc:e.loc + 8]))pip._vendor.packaging.requirements.InvalidRequirement: Invalid requirement, parse error at ""'; extra '""During handling of the above exception, another exception occurred:Traceback (most recent call last): File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_internal/basecommand.py"", line 141, in main status = self.run(options, args) File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_internal/commands/install.py"", line 330, in run self._warn_about_conflicts(to_install) File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_internal/commands/install.py"", line 456, in _warn_about_conflicts package_set, _dep_info = check_install_conflicts(to_install) File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_internal/operations/check.py"", line 98, in check_install_conflicts package_set = create_package_set_from_installed() File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_internal/operations/check.py"", line 41, in create_package_set_from_installed package_set[name] = PackageDetails(dist.version, dist.requires()) File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py"", line 2607, in requires dm = self._dep_map File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py"", line 2871, in _dep_map self.__dep_map = self._compute_dependencies() File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py"", line 2881, in _compute_dependencies reqs.extend(parse_requirements(req)) File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py"", line 2942, in parse_requirements yield Requirement(line) File ""/opt/conda/envs/tf_keras/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py"", line 2951, in __init__ raise RequirementParseError(str(e))pip._vendor.pkg_resources.RequirementParseError: Invalid requirement, parse error at ""'; extra '"" FROM nvidia/cuda:9.0-devel-ubuntu16.04LABEL maintainer ""[deleted]""# get correct version of CUDNN for my system's CUDAENV CUDNN_VERSION 7.3.0.29LABEL com.nvidia.cudnn.version=""${CUDNN_VERSION}""RUN apt-get update && apt-get install -y --no-install-recommends \ libcudnn7=$CUDNN_VERSION-1+cuda9.0 \ libcudnn7-dev=$CUDNN_VERSION-1+cuda9.0 && \ apt-mark hold libcudnn7 && \ rm -rf /var/lib/apt/lists/*# install underlying requirementsRUN apt-get update \ && apt-get install -y --no-install-recommends \ bc \ bzip2 \ ca-certificates \ curl \ git \ libgl1 \ jq \ nfs-common \ parallel \ python-pip \ python-wheel \ python-setuptools \ unzip \ wget \ build-essential \ && apt-get clean \ && rm -rf /var/lib/apt/lists/*# install anacondaRUN wget --quiet https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh -O ~/miniconda.sh && \ /bin/bash ~/miniconda.sh -b -p /opt/conda && \ rm ~/miniconda.sh && \ /opt/conda/bin/conda clean -tipsy && \ ln -s /opt/conda/etc/profile.d/conda.sh /etc/profile.d/conda.sh && \ echo "". /opt/conda/etc/profile.d/conda.sh"" >> ~/.bashrc && \ echo ""conda activate base"" >> ~/.bashrcENV TINI_VERSION v0.16.1ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /usr/bin/tiniRUN chmod +x /usr/bin/tini# add conda to $PATH and create a conda environmentENV PATH /opt/conda/bin:$PATHRUN conda update conda && \ conda config --remove channels defaults && \ conda config --add channels conda-forge && \ conda create -n tf_keras python=3.6 \ && echo ""source activate tf_keras"" > ~/.bashrcENV PATH /opt/conda/envs/tf_keras/bin:$PATHSHELL [""/bin/bash"", ""-c""]# install required libraries (and some dependencies)RUN conda install -n tf_keras \ osmnx=0.7.3 \ affine \ pyproj \ pyhamcrest=1.9.0 \ cython \ fiona \ h5py \ ncurses \ jupyter \ jupyterlab \ ipykernel \ libgdal \ matplotlib \ numpy \ opencv \ pandas \ pillow \ pip \ scipy \ scikit-image \ scikit-learn \ shapely \ gdal \ rtree \ tqdm \ pandas \ geopandas \ rasterio# get tensorflowARG TENSORFLOW_VERSION=1.10.0ARG TENSORFLOW_DEVICE=gpuARG TENSORFLOW_APPEND=_gpuRUN source activate tf_keras && \ pip --no-cache-dir install https://storage.googleapis.com/tensorflow/linux/${TENSORFLOW_DEVICE}/tensorflow${TENSORFLOW_APPEND}-${TENSORFLOW_VERSION}-cp36-cp36m-linux_x86_64.whl",pip install AttributeError: _DistInfoDistribution__dep_map
why is pickle needed for multiprocessing module in python," I was doing multiprocessing in python and hit a pickling error. Which makes me wonder why do we need to pickle the object in order to do multiprocessing? isn't fork() enough? Edit: I kind of get why we need pickle to do interprocess communication, but that is just for the data you want to transfer right? why does the multiprocessing module also try to pickle stuff like functions etc? <code> ",Why is pickle needed for multiprocessing module in python
ModuleNotFoundError: No module named 'prompt_toolkit.formatted_text'," I am using Pycharm 2018.2 version in ubuntu 18.04 and I am trying to use the JupyterNoteBook inside the pycharm it's been loading and creating a new notebook. But the cell in the Jupyter always shows busy and it throws some error like ModuleNotFoundError: No module named 'prompt_toolkit.formatted_text'. Eventhough, I restarted the kernel again and again it throws the same error. Even I uninstalled and after the re-installation it throws the same error.Error it throwsCan aynone please help me to resolve this error?Thanks in advance ! <code> ",How to solve the ModuleNotFoundError: No module named 'prompt_toolkit.formatted_text' in Jupyter Notebook inside the Pycharm IDE?
python 2.7 multiprocessing parallelization for and arguments," I have defined this function Where a is a string containing the path of the file and seed is an integer seed.I want to parallelize a simple program in such a way that each core takes one of the available paths that I give in, seeds its random generator and write some random numbers on that files, so, for example, if I pass the vector and the seeds it gives to the first available core the function and to the second one the same function with different arguments: I have looked through a lot of similar questions here on Stackoverflow, but I cannot make any solution work. What I tried is: and gives me TypeError: writeonfiles() takes exactly 2 arguments (1 given).I tried also But it gives me File ""/usr/lib/python2.7/random.py"", line 120, in seed super(Random, self).seed(a)TypeError: unhashable type: 'list'Finally, I tried the contextmanager and it results in File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get raise self._valueTypeError: 'module' object is not callable <code>  def writeonfiles(a,seed): random.seed(seed) f = open(a, ""w+"") for i in range(0,10): j = random.randint(0,10) #print j f.write(j) f.close() vector = [Test/file1.txt, Test/file2.txt] seeds = (123412, 989898), writeonfiles(Test/file1.txt, 123412) writeonfiles(Test/file2.txt, 989898) def writeonfiles_unpack(args): return writeonfiles(*args)if __name__ == ""__main__"": folder = [""Test/%d.csv"" %i for i in range(0,4)] seed = [234124, 663123, 12345 ,123833] p = multiprocessing.Pool() p.map(writeonfiles, (folder,seed)) if __name__ == ""__main__"": folder = [""Test/%d.csv"" %i for i in range(0,4)] seed = [234124, 663123, 12345 ,123833] p = multiprocessing.Process(target=writeonfiles, args= [folder,seed]) p.start() @contextmanager def poolcontext(*args, **kwargs): pool = multiprocessing.Pool(*args, **kwargs) yield pool pool.terminate()if __name__ == ""__main__"": folder = [""Test/%d"" %i for i in range(0,4)] seed = [234124, 663123, 12345 ,123833] a = zip(folder, seed) with poolcontext(processes = 3) as pool: results = pool.map(writeonfiles_unpack,a )",Python 2.7: How to compensate for missing pool.starmap?
How to get back a list from bytes in python?," I have a list in Python that I converted to bytes using bytes(). I want to get back that list and I am unsure how to do that. I searched around and it seemed to me like I can get back integers or strings not really the list itself. I am new to byte manipulation and maybe I am missing something obvious? I want to do something like arr1= *list*.from_bytes(arr2), which I am expecting to return arr1= [1,2,3,4,5]. <code>  arr=[1,2,3,4,5]arr2= bytes(arr)",How to get back a list from bytes in Python?
PyQt5 qTableView Selecion Change, Been googling around for some time on this one but I can't seem to find anything. Need the signal for QTableView on a selection change. Tried tbl_view.itemSelectionChanged.connect(self.select_row) but the compiler complains this doesn't exist. I also need to retrieve the data from the row that is selected. Can someone please point me in the right direction? <code> ,QTableView Selecion Change
python chromedriver detected from the first time," I am trying to use selenium chromedriver in python for the website www.mouser.co.uk. However, it is detected as bot from the first shot . Does any one has an explanation for this ?. hereafter the code i am using :  <code>  options = Options()options.add_argument(""--start-maximized"")browser = webdriver.Chrome('chromedriver.exe',chrome_options=options)wait = WebDriverWait(browser, 30)browser.get('https://www.mouser.co.uk')",Chrome browser initiated through ChromeDriver gets detected
round() in Python," While using the round() function I noticed that I get two different results depending on whether I don't explicitly choose the number of decimal places to include or choosing the number to be 0. It prints the following: What is the difference? <code>  x = 4.1print(round(x))print(round(x, 0)) 44.0",round() returns different result depending on the number of arguments
Find group of consecutive dates in pandas dataframe," I am trying to get the chunks of data where there's consecutive dates from the Pandas DataFrame. My df looks like below. In this df, I want to get the first 3 rows, do some processing and then get the last 3 rows and do processing on that.I calculated the difference with 1 lag by applying following code. But after then I can't figure out that how to get the groups of consecutive rows without iterating. <code>  DateAnalyzed Val1 2018-03-18 0.4702532 2018-03-19 0.4702533 2018-03-20 0.4702534 2018-09-25 0.4677295 2018-09-26 0.4677296 2018-09-27 0.467729 df['Delta']=(df['DateAnalyzed'] - df['DateAnalyzed'].shift(1))",Find group of consecutive dates in Pandas DataFrame
Why middleware mixin declear in django.utils.deprecation.py," in path django.utils.deprecation.py we have some class about deprecation warning for methods.in that file we have a class called MiddlewareMixin. this class used to write middleware classes. Although not related to deprecation, Why this class wrote in this path? <code> ",Why middleware mixin declared in django.utils.deprecation.py
caling c++ function from python," I'm trying to call a C++ function from my Python code, if I pass a Boolean or an int it works perfectly, but if I send a string, it only prints the first character.I am compiling with: Here is the C++ and Python code:Python: c++: I'm aware of the Boost Library, but i couldn't manage to download it, and this way works well excepts for strings. Thank you for your help <code>  g++ -c -fPIC foo.cpp -Wextra -Wall -o foo.og++ -shared -Wl,-soname,libfoo.so -o libfoo.so foo.opython3 fooWrapper.py from ctypes import cdlllib = cdll.LoadLibrary(""./libfoo.so"")lib.Foo_bar(""hello"") #include <iostream>#include <string>#include <unistd.h>void bar(char* string){ printf(""%s"", string);}extern ""C"" { void Foo_bar(char* aString){ bar(aString); }}",Calling c++ function from python
how to start a new project in django in virtualenv," I just installed virtualenv and in it I installed django. However, when I go to the django-admin terminal in the bin file, I wrote I thought that would start a new project but it just returned Note that only Django core commands are listed as settings are not properly configured (error: Requested setting INSTALLED_APPS, but settings are not configured. You must either define the environment variable DJANGO_SETTINGS_MODULE or call settings.configure() before accessing settings.). <code>  django-admin startproject mysite ",How to start a new project in django using virtualenv
Apache Beam / Dataflow / Python," I am pretty new working on Apache Beam , where in I am trying to write a pipeline to extract the data from Google BigQuery and write the data to GCS in CSV format using Python.Using beam.io.read(beam.io.BigQuerySource()) I am able to read the data from BigQuery but not sure how to write it to GCS in CSV format.Is there a custom function to achieve the same , could you please help me? <code>  import loggingimport apache_beam as beamfrom apache_beam.io.BigQueryDisposition import CREATE_IF_NEEDEDfrom apache_beam.io.BigQueryDisposition import WRITE_TRUNCATEPROJECT='project_id'BUCKET='project_bucket'def run(): argv = [ '--project={0}'.format(PROJECT), '--job_name=readwritebq', '--save_main_session', '--staging_location=gs://{0}/staging/'.format(BUCKET), '--temp_location=gs://{0}/staging/'.format(BUCKET), '--runner=DataflowRunner' ] with beam.Pipeline(argv=argv) as p: # Execute the SQL in big query and store the result data set into given Destination big query table. BQ_SQL_TO_TABLE = p | 'read_bq_view' >> beam.io.Read( beam.io.BigQuerySource(query = 'Select * from `dataset.table`', use_standard_sql=True)) # Extract data from Bigquery to GCS in CSV format. # This is where I need your help BQ_SQL_TO_TABLE | 'Write_bq_table' >> beam.io.WriteToBigQuery( table='tablename', dataset='datasetname', project='project_id', schema='name:string,gender:string,count:integer', create_disposition=CREATE_IF_NEEDED, write_disposition=WRITE_TRUNCATE)if __name__ == '__main__': logging.getLogger().setLevel(logging.INFO) run()",Write BigQuery results to GCS in CSV format using Apache Beam
Python 3 upper/lowercase a letter in string," I want to randomly capitalize or lowercase each letter in a string. I'm new to working with strings in python, but I think because strings are immutable that I can't do the following: And get the error: TypeError: 'str' object does not support item assignmentBut then how else could I do this? <code>  i =0 for c in sentence: case = random.randint(0,1) print(""case = "", case) if case == 0: print(""here0"") sentence[i] = sentence[i].lower() else: print(""here1"") sentence[i] = sentence[i].upper() i += 1print (""new sentence = "", sentence)",Randomly capitalize letters in string
"Command Line: Python program says ""Killed""."," I'm extracting xml data from 465 webpages ,and parsing and storing it in "".csv"" file using python dataframe. After running the program for 30 mins, the program saves ""200.csv"" files and kills itself. The command line execution says ""Killed"". But when I run the program for first 200 pages and rest of 265 pages for extraction separately, it works well. I had thoroughly searched on the internet, no proper answer for this issue. Could you please tell me what could be the reason? Thanks in advance! <code>  for i in list: addr = str(url + i + '?&$format=json') response = requests.get(addr, auth=(self.user_, self.pass_)) # print (response.content) json_data = response.json() if ('d' in json_data): df = json_normalize(json_data['d']['results']) paginate = 'true' while paginate == 'true': if '__next' in json_data['d']: addr_next = json_data['d']['__next'] response = requests.get(addr_next, auth=(self.user_, self.pass_)) json_data = response.json() df = df.append(json_normalize(json_data['d']['results'])) else: paginate = 'false' try: if(not df.empty): storage = '/usr/share/airflow/documents/output/' + i + '_output.csv' df.to_csv(storage, sep=',', encoding='utf-8-sig') else: pass except: pass","Command Line: Python program says ""Killed"""
Django GIS : Using location__dwithin gives Unable to get repr for <class 'django.db.models.query.QuerySet'> however location__distance_lte works fine," I have the following two queries. The first one works fine however the last one which uses location__dwithin returns back Unable to get repr for . Any suggestions on why the last one fails ? and the other one is: This is what my modelEmployee looks like The error I am getting is this .Here is the traceback <code>  querySet = modelEmployee.objects.filter(location__distance_lte=(modelemp.location, D(mi=150))) querySet = modelEmployee.objects.filter(location__dwithin=(modelemp.location, D(mi=150))) class modelEmployee(models.Model): user = models.ForeignKey(User, on_delete=models.CASCADE, null=True, blank=True) title = models.CharField(max_length=200, unique=False, blank=False, null=True) skills = models.ManyToManyField(modelSkill, blank=True) location = models.PointField(srid=32148,max_length=40, blank=True,null=True) objects = GeoManager() def __str__(self): return ""Employee name : "" + self.user.first_name raise ValueError('Only numeric values of degree units are ' ValueError: Only numeric values of degree units are allowed on geographic DWithin queries Traceback (most recent call last): File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/django/core/handlers/base.py"", line 126, in _get_response response = self.process_exception_by_middleware(e, request) File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/channels/handler.py"", line 243, in process_exception_by_middleware return super(AsgiHandler, self).process_exception_by_middleware(exception, request) File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/django/core/handlers/base.py"", line 124, in _get_response response = wrapped_callback(request, *callback_args, **callback_kwargs) File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/django/views/decorators/csrf.py"", line 54, in wrapped_view return view_func(*args, **kwargs) File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/django/views/generic/base.py"", line 68, in view return self.dispatch(request, *args, **kwargs) File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/rest_framework/views.py"", line 483, in dispatch response = self.handle_exception(exc) File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/rest_framework/views.py"", line 443, in handle_exception self.raise_uncaught_exception(exc) File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/rest_framework/views.py"", line 480, in dispatch response = handler(request, *args, **kwargs) File ""/Users/admin/Development/TestWeb/virtual/TestWeb/Employer/views.py"", line 42, in post employeesJson = Serializer_Employee_TX(querySet,many=True,context={""request"": request,shared.LOGGED_IN_EMPLOYER_SHARED:modelemp}).data File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/rest_framework/serializers.py"", line 765, in data ret = super(ListSerializer, self).data File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/rest_framework/serializers.py"", line 262, in data self._data = self.to_representation(self.instance) File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/rest_framework/serializers.py"", line 683, in to_representation self.child.to_representation(item) for item in iterable File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/django/db/models/query.py"", line 268, in __iter__ self._fetch_all() File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/django/db/models/query.py"", line 1186, in _fetch_all self._result_cache = list(self._iterable_class(self)) File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/django/db/models/query.py"", line 54, in __iter__ results = compiler.execute_sql(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size) File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/django/db/models/sql/compiler.py"", line 1052, in execute_sql sql, params = self.as_sql() File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/django/db/models/sql/compiler.py"", line 464, in as_sql where, w_params = self.compile(self.where) if self.where is not None else ("""", []) File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/django/db/models/sql/compiler.py"", line 390, in compile sql, params = node.as_sql(self, self.connection) File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/django/db/models/sql/where.py"", line 81, in as_sql sql, params = compiler.compile(child) File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/django/db/models/sql/compiler.py"", line 390, in compile sql, params = node.as_sql(self, self.connection) File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/django/contrib/gis/db/models/lookups.py"", line 78, in as_sql rhs_sql, rhs_params = self.process_rhs(compiler, connection) File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/django/contrib/gis/db/models/lookups.py"", line 307, in process_rhs dist_sql, dist_params = self.process_distance(compiler, connection) File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/django/contrib/gis/db/models/lookups.py"", line 297, in process_distance ('%s', connection.ops.get_distance(self.lhs.output_field, self.rhs_params, self.lookup_name)) File ""/Users/admin/Development/TestWeb/virtual/lib/python3.5/site-packages/django/contrib/gis/db/backends/postgis/operations.py"", line 264, in get_distance raise ValueError('Only numeric values of degree units are 'ValueError: Only numeric values of degree units are allowed on geographic DWithin queries.[2018/10/31 00:24:31] HTTP POST /api/employer/login/ 500 [8.10, 127.0.0.1:58046]","Django GIS : Using location__dwithin gives ""Only numeric values of degree units are allowed"" however location__distance_lte works fine"
align text in the putText() in OpenCV python," I have a list of strings like this I want to print it in the following manner on my video: I have tried this: The label is a list of lists mentioned at the top. I am confused with the org argument in the putText(). <code>  ['standUp', 'front', 'lookU', 'lookF', 'lookDF', 'HandOnHipR'] ['standUp', 'front', 'lookU', 'lookF', 'lookDF', 'HandOnHipR'] offset = 1x, y = 5, 400for idx, list in enumerate(lbls): cv2.putText(frame, str(lbls), (x, y+offset*idx), font, 1, (0, 0, 255), 1)",Align text in the putText() in OpenCV
Tkinter - Maximizing and minimizing the window initially hides the widgets," This is the code that I have written. As you can see, it has a button which when clicked opens up a Text where one can enter something. It works, the window resizes if you click the button.But I have noticed that, if I maximize and minimize the window just after starting the program (i.e. the first thing I do is to maximize and minimize the window), then the window doesn't resize to fit the whole frame (on clicking the button) and my widgets get hidden. Any help as to why this is happening and how I can prevent this? <code>  from tkinter import *def Home(): global homeP homeP = Tk() homeP.title('Grades') enterButton = Button(homeP, text='Enter Grades', bg='blue', fg='white', command=enterG) enterButton.grid(row=1, column=0, padx=(5,5), pady=(5,2), sticky=""e"") homeP.mainloop()curFrame = ''def enterG(): global homeP global eGrade global curFrame if curFrame == 'e': return if 'eGrade' not in globals(): #Prevent frames from stacking up on one another eGrade = Frame(homeP) enterGrades = Text(eGrade, width=64, height=10) enterGrades.grid(row=0, column=0) eGrade.grid(row=2, column=0, columnspan=2, padx=(10,10), pady=(1,5)) else: eGrade.grid(row=2, column=0, columnspan=2, padx=(10,10), pady=(1,5)) curFrame = 'e' # for name, value in globals().copy().items(): # print(name, value)Home()","Tkinter - Maximizing and minimizing the window before using any widgets, hides the widgets when I use them"
Implementing a custom loss function for object detection in keras," I am new to keras and tensorflow . How do I go about implementing a custom loss function while doing object detection , right now I have 5 parameters - 4 for bounding box coordinates and 1 for whether the object is present or not . Loss function should return square of difference between coordinates if object is present else if object is absent it should return a huge value as loss . This is the code I am tring right now: Here I am masking other values to obtain the last value which is 1000--> object present 0 --> object absent.Is there any other better way to do this?The value of loss when I am running this in Kaggle decreases rapidly , by 2nd epoch the loss becomes 0. <code>  def loss_func(y_true,y_pred): mask = np.array([False, False, False,False,True]) # check column of the class of object mask1 = np.array([True, True, True,True,False]) # get the columns of the coordinates of B box check_class = K.mean(K.square(tf.subtract(tf.boolean_mask(y_true,mask),tf.boolean_mask(y_pred,mask)))) mean_square = K.mean(K.square(tf.subtract(tf.boolean_mask(y_true,mask1),tf.boolean_mask(y_pred,mask1)))) value=K.mean(tf.boolean_mask(y_pred,mask)) return value*mean_square + check_class",Implementing a custom loss function for object detection
django datetime not validating right," I'm using the HTML5 datetime-local input type to try and get some datetime data into my database.The ModelForm class Meta: looks like the following: I keep failing on form validation and it's causing me to pull my hair out.This is the documentation page that shows it should work, but it looks like I'm missing something?EDIT FOR CLARIFICATION:The error message is for both start and end and it's Enter a valid date/time <code>  class Meta: model = ScheduleEntry fields = ['calendar', 'title', 'start', 'end', 'assets', 'users'] widgets = { 'calendar': forms.Select(attrs={ 'class': 'fom-control chosen-select' }), 'title': forms.TextInput(attrs={ 'class': 'form-control' }), 'start': forms.DateTimeInput(attrs={ 'type':'datetime-local', 'class':'form-control' }, format='%Y-%m-%dT%H:%M'), 'end': forms.DateTimeInput(attrs={ 'type': 'datetime-local', 'class': 'form-control' }, format='%Y-%m-%dT%H:%M'), 'assets': forms.SelectMultiple(attrs={ 'class': 'form-control chosen-select' }), 'users': forms.SelectMultiple(attrs={ 'class': 'form-control chosen-select', }) }",Django datetime not validating right
Python - Insert and array of zeros after each element of an array," My code is: The output I want is: The error I get is: ValueError: shape mismatch: value array of shape (3,) could not be broadcast to indexing result of shape (4,)When I do: The out is: Why it doesn't work when I try to insert an array?P.S. I I cannot use loops <code>  x=np.linspace(1,5,5)a=np.insert(x,np.arange(1,5,1),np.zeros(3)) [1,0,0,0,2,0,0,0,3,0,0,0,4,0,0,0,5] x=np.linspace(1,5,5)a=np.insert(x,np.arange(1,5,1),0) array([1., 0., 2., 0., 3., 0., 4., 0., 5.])",NumPy - Insert an array of zeros after specified indices
Python - Insert and array of zeros after each element an," My code is: The output I want is: The error I get is: ValueError: shape mismatch: value array of shape (3,) could not be broadcast to indexing result of shape (4,)When I do: The out is: Why it doesn't work when I try to insert an array?P.S. I I cannot use loops <code>  x=np.linspace(1,5,5)a=np.insert(x,np.arange(1,5,1),np.zeros(3)) [1,0,0,0,2,0,0,0,3,0,0,0,4,0,0,0,5] x=np.linspace(1,5,5)a=np.insert(x,np.arange(1,5,1),0) array([1., 0., 2., 0., 3., 0., 4., 0., 5.])",NumPy - Insert an array of zeros after specified indices
How can I minimize/maximize windows in macOS from a Python script?," How can I minimize/maximize windows in macOS from a Python script? On Windows, there's a win32 api (the ShowWindow() function) that can do this. I'd like the macOS equivalent. I'd like to have a script be able to find a window from its title, then minimize or maximize it.Is this possible? I assume I need to use the pyobjc module for this. <code> ",How can I minimize/maximize windows in macOS with the Cocoa API from a Python script?
Python function that identifies if a number is closer to 0 or 1, I have a numpy array of numbers. Below is an example: I want to know if each of the numbers is closer to 0 or 1. Is there a function in Python that could do it or do I have to do it manually? <code>  [[-2.10044520e-04 1.72314372e-04 1.77235336e-04 -1.06613465e-046.76617611e-07 2.71623057e-03 -3.32789944e-05 1.44899758e-055.79249863e-05 4.06502549e-04 -1.35823707e-05 -4.13955189e-045.29862793e-05 -1.98286005e-04 -2.22829175e-04 -8.88758230e-045.62228710e-05 1.36249752e-05 -2.00474996e-05 -2.10090068e-051.00007518e+00 1.00007569e+00 -4.44597417e-05 -2.93724453e-041.00007513e+00 1.00007496e+00 1.00007532e+00 -1.22357142e-033.27903892e-06 1.00007592e+00 1.00007468e+00 1.00007558e+002.09869172e-05 -1.97610235e-05 1.00007529e+00 1.00007530e+001.00007503e+00 -2.68725642e-05 -3.00372853e-03 1.00007386e+001.00007443e+00 1.00007388e+00 5.86993822e-05 -8.69989983e-061.00007590e+00 1.00007488e+00 1.00007515e+00 8.81850779e-042.03875532e-05 1.00007480e+00 1.00007425e+00 1.00007517e+00-2.44678912e-05 -4.36556267e-08 1.00007436e+00 1.00007558e+001.00007571e+00 -5.42990711e-04 1.45517859e-04 1.00007522e+001.00007469e+00 1.00007575e+00 -2.52271817e-05 -7.46339417e-051.00007427e+00]],Python function that identifies if the numbers in a list or array are closer to 0 or 1
Inheritance - a defined method becomes None," I managed to reproduce this on both Python 3.4 and 3.7.Consider: Clearly one would expect b.__hash__ to be defined here, since it is defined under Comparable which B is a subclass of.Lo and behold, it is defined, but evaluates to None. What gives? The same behavior is reproduced if implementing __init__ as super().__init__() in Comparable and A. <code>  class Comparable: def _key(self): raise NotImplementedError def __hash__(self): return hash(self._key()) def __eq__(self, other): ... def __lt__(self, other): ...class A(Comparable): passclass B(A): def __str__(self): return ""d"" def __eq__(self, other): return isinstance(self, type(other)) def _key(self): return str(self),b = B() >> b<__main__.B object at 0x00000183C9734978>>> '__hash__' in dir(b)True>> b.__hash__>> b.__hash__ is NoneTrue>> B.__mro__(<class '__main__.B'>, <class '__main__.A'>, <class '__main__.Comparable'>, <class 'object'>)>> isinstance(b, Comparable)True",Inheritance - __hash__ sets to None in a subclass
Python3 type hint: Why can't List contain multiple types?," You can mix types inside tuples or lists. Why can't you specify that in typing hints? <code>  >>> from typing import Tuple, List>>> t = ('a', 1)>>> l = ['a', 1]>>> t2: Tuple[str, int] = ('a', 1)>>> l2: List[str, int] = ['a', 1]TypeError: Too many parameters for typing.List; actual 2, expected 1",Why can't List contain multiple types?
How to select all but the 2 last columns of a dataframe n Python," I want to select all but the 3 last columns of my dataframe.I tried : But it does not workEdit : title <code>  df.loc[:,-3]",How to select all but the 3 last columns of a dataframe in Python
How to select all but the 3 last columns of a dataframe n Python," I want to select all but the 3 last columns of my dataframe.I tried : But it does not workEdit : title <code>  df.loc[:,-3]",How to select all but the 3 last columns of a dataframe in Python
Why changing values in a column of a pandas data-frame is fast in one case and slow in another one?," I have two pieces of code that seem to do the same thing but one is almost a thousand times faster than the other one.This is the first piece: In ts I have values like: In contrast, this part of the code: Creates ts populated with the values like: I cannot figure out what the essential difference is between the first and second assignments.In both cases df should be the same.ADDEDIt turned out that the essential difference was not in the place where I was looking. In the fast version of the code I had: in the beginning of the class method (where inp_df was the input data frame of the method). In the slow version, I was operating directly on the input data frame. It became fast after copying the input data frame and operating on it. <code>  t1 = time.time()df[new_col] = np.where(df[col] < j, val_1, val_2)t2 = time.time()ts.append(t2 - t1) 0.0007321834564208984, 0.0002918243408203125, 0.0002799034118652344 t1 = time.time()df['new_col'] = np.where((df[col] >= i1) & (df[col] < i2), val, df.new_col)t2 = time.time()ts.append(t2 - t1) 0.11008906364440918, 0.09556794166564941, 0.08580684661865234 df = inp_df.copy()",Why is changing values in a column of a pandas data frame fast in one case and slow in another one?
Is there a reliable way to print from a python application to a CUPS print server?," I have two linux computers with fixed IP addresses:A print server, whereby the connected printer is shared via CUPS.(The server has the IP address ""192.168.1.2"" and the printer is called ""test_printer"".)A computer, on which a python application is running, that should be able to use this print server.Unfortunately, the printer propagation via CUPS seems not to work reliably (possibly due to the structure of the network).Can I send print jobs directly from a python program to the CUPS print server?If so, can you please provide a small example?In theory, I would just send correctly formatted data to the IP address + port, but I didn't get it to work ...Here are the approaches I have found so far and my problems with them:command 'lpr' Relies on the printer propagation via CUPS.python module pycups Before I can use a printer, I'll have to add it first, which in turn relies on the propagation via CUPS.Also I didn't get conn.addPrinter() to work.python module python-escpos / python-printer-escpos Probably the most promising approach ... unfortunately it doesn't print anything and throws an exception on closing. python module pkipplib / pyipptoolUnfortunately, there seems not to be a working python3 library that implements the Internet Printing Protocol (IPP) in 2018.I use python 3.6.7.The print server uses CUPS 2.2.1. <code>  import subprocesslpr = subprocess.Popen(""usr/bin/lpr"", stdin=subprocess.PIPE) # on some distros the command is 'lp'lpr.stdin.write(""hello world\n"")lpr.stdin.close() import cupswith open(""/home/user/Documents/some.txt"", ""w"") as f: f.write(""hello world\n"")conn = cups.Connection()conn.printFile(""test_printer"", ""/home/user/Documents/some.txt"", ""some_title"", {}) import escpos.printerp = escpos.printer.Network(""192.168.1.2"", port=631) # port 9100 seems not to work.p.text(""hello world\n"")p.close() # The traceback was produced in the interactive shell.Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/home/user/.local/lib/python3.6/site-package/escpos/printer.py"", line 214, in close self.device.shutdown(socket.SHUT_RDWR)OSError: [Errno 107] Transport endpoint is not connected",Can I send print jobs directly from a python program to the IP address of a CUPS print server?
Is `list()` a considered a function?," list is obviously a built-in type in Python. I saw a comment under this question which calls list() a built-in function. And when we check the documentation, it is, indeed, included in Built-in functions list but the documentation again states: Rather than being a function, list is actually a mutable sequence typeWhich brings me to my question: Is list() considered a function? Can we refer to it as a built-in function? If we were talking about C++, I'd say we are just calling the constructor, but I am not sure if the term constructor applies to Python (never encountered its use in this context). <code> ",Is `list()` considered a function?
Panda : Find the max value in one column," I have a dataframe like this: If I want to find the max value in each entry in the day column.For example: What should I do?Thanks for your help. <code>  fly_frame: day plcae0 [1,2,3,4,5] A1 [1,2,3,4] B2 [1,2] C3 [1,2,3,4] D fly_frame: day plcae0 5 A1 4 B2 2 C3 4 D",Pandas: Find the max value in one column containing lists
Performant cartesian product (CROSS JOIN) of two pandas DataFrames," The contents of this post were originally meant to be a part of Pandas Merging 101, but due to the nature and size of the content required to fully do justice to this topic, it has been moved to its own QnA.Given two simple DataFrames; The cross product of these frames can be computed, and will look something like: What is the most performant method of computing this result? <code>  left = pd.DataFrame({'col1' : ['A', 'B', 'C'], 'col2' : [1, 2, 3]})right = pd.DataFrame({'col1' : ['X', 'Y', 'Z'], 'col2' : [20, 30, 50]})left col1 col20 A 11 B 22 C 3right col1 col20 X 201 Y 302 Z 50 A 1 X 20A 1 Y 30A 1 Z 50B 2 X 20B 2 Y 30B 2 Z 50C 3 X 20C 3 Y 30C 3 Z 50",Performant cartesian product (CROSS JOIN) with pandas
Python 3 float() object id creation order," I've isolated the float() oddity here to the object creation order, because Note: I've done checks on the precision of the floats and it is not the reason why this is happening.I wish to understand why and how is Python deciding to create float objects.Why is float(1.0) pointing to the same object while float(1) points to 2 different objects when either are created twice?Also, for further reference: <code>  float(1.0) is float(1.0) #Truefloat(1) is float(1) #False x1 = float(1)x2 = float(1)x1 is x2 #Falseid(x1) == id(x2) #Falsey1 = float(1.0)y2 = float(1.0)y1 is y2 #Trueid(y1) == id(y2) #True float(1) is float(1) #Falseid(float(1)) == id(float(1)) #Truefloat(1.0) is float(1.0) #Trueid(float(1.0)) == id(float(1.0)) #True",float() object id creation order
spaCy named entity recognition (NER) to return unique entity ID's?," Perhaps I've skipped over a part of the docs, but what I am trying to determine is a unique ID for each entity in the standard NER toolset. For example: returns: I have been looking at ent.ent_id and ent.ent_id_ but these are inactive according to the docs. I couldn't find anything in ent.root either. For example, in GCP NLP each entity is returned with an entitynumber that enables you to identify multiple instances of the same entity within a text. This is a text2 about Apple Inc1 based in San Fransisco4. And here is some text3 about Samsung Corp6. Now, here is some more text8 about Apple1 and its products5 for customers7 in Norway9""Does spaCy support something similar? Or is there a way using NLTK or Stanford? <code>  import spacyfrom spacy import displacyimport en_core_web_smnlp = en_core_web_sm.load()text = ""This is a text about Apple Inc based in San Fransisco. ""\ ""And here is some text about Samsung Corp. ""\ ""Now, here is some more text about Apple and its products for customers in Norway""doc = nlp(text)for ent in doc.ents: print('ID:{}\t{}\t""{}""\t'.format(ent.label,ent.label_,ent.text,))displacy.render(doc, jupyter=True, style='ent') ID:381 ORG ""Apple Inc"" ID:382 GPE ""San Fransisco"" ID:381 ORG ""Samsung Corp."" ID:381 ORG ""Apple"" ID:382 GPE ""Norway""",spaCy coreference resolution - named entity recognition (NER) to return unique entity ID's?
pipenv: get path of virtual enviremtnt in pipenv, How to can get the path of virtualenv in pipenv?can configure it to use a custom path for newly created virtualenv? <code> ,Get path of virtual environment in pipenv
pipenv: get path of virtual environment in pipenv, How to can get the path of virtualenv in pipenv?can configure it to use a custom path for newly created virtualenv? <code> ,Get path of virtual environment in pipenv
Python Iterate through columns," I am working with a DataFrame which looks like this and I am trying to compute the following output. In my current approach I'm trying to iterate through the columns, then replace values with the contents of a third column.For example, if List[0][1] is equal to Numb[1][1] replace column List[0][1] with 'one'.How could I make an iteration like this work, or alternatively solve the problem without explicitly iterating at all? <code>  List Numb Name1 1 one1 2 two2 3 three4 4 four3 5 five List Numb Nameone 1 oneone 2 twotwo 3 threefour 4 fourthree 5 five",pandas map one column to the combination of two columns
Python3 Pygame - Complete drawing a circle in a specific amount of time," I am trying to create a countdown with a circle. The circle will show how much time has passed compared to the amount of time given. So if the countdown is 10 seconds and 5 have passed, half circle will have been drawn. I've come up with this code: So fps = 60 and in each frame I am drawing 2 * pi / countdown * fps, dividing a full circle into frames. And then in each frame, drawing a part of the circle. It seems to draw the circle fine, but it can't be used for a timer for two reasons:The circle seems to be completed in less time than given.While drawing the circle, it seems like the last part of the circle is drawn faster.Can anyone point out what I am doing wrong? <code>  from math import piimport pygamepygame.init()(x, y) = (800, 600)screen = pygame.display.set_mode((x, y))clock = pygame.time.Clock()radius = 50# Just to place the circle in the center later ontop_left_corner = ((x / 2) - (radius / 2), (y / 2) - (radius / 2))outer_rect = pygame.Rect(top_left_corner, (radius, radius))countdown = 1000 # secondsangle_per_frame = 2 * pi / (countdown * 60)angle_drawn = 0new_angle = angle_per_framewhile True: if angle_drawn < 2 * pi: new_angle += angle_drawn + angle_per_frame pygame.draw.arc(screen, (255, 255, 255), outer_rect, angle_drawn, new_angle, 10) angle_drawn += angle_per_frame clock.tick(60) pygame.display.update(outer_rect)",Complete drawing a circle in a specific amount of time
How to get weights and inputs of a keras layer while creating customized activation function," I am creating a customized activation function, RBF activation function in particular: The function rbf2 receives the previous layer as input: What should I do to get the inputs from layer1 and weights from layer2 to create the customized activation function?What I am actually trying to do is, implementing the output layer for LeNet5 neural network. The output layer of LeNet-5 is a bit special, instead of computing the dot product of the inputs and the weight vector, each neuron outputs the square of the Euclidean distance between its input vector and its weight vector.For example, layer1 has 84 neurons and layer2 has 10 neurons. In general cases, for calculating output for each of 10 neurons of layer2, we do the dot product of 84 neurons of layer1 and 84 weights in between layer1 and layer2. We then apply softmax activation function over it.But here, instead of doing dot product, each neuron of the layer2 outputs the square of the Euclidean distance between its input vector and its weight vector (I want to use this as my activation function). Any help on creating RBF activation function (calculating euclidean distance from inputs the layer receives and weights) and using it in the layer is also helpful. <code>  from keras import backend as Kfrom keras.layers import Lambdal2_norm = lambda a,b: K.sqrt(K.sum(K.pow((a-b),2), axis=0, keepdims=True))def rbf2(x):X = #here i need inputs that I receive from previous layer Y = # here I need weights that I should apply for this layerl2 = l2_norm(X,Y)res = K.exp(-1 * gamma * K.pow(l2,2))return res #some keras layersmodel.add(Dense(84, activation='tanh')) #layer1model.add(Dense(10, activation = rbf2)) #layer2",How to implement RBF activation function in Keras?
How to include multiple interactive widgets in the same cell in Jupyter notebook (Python)," My goal is to have one cell in Jupyter notebook displaying multiple interactive widgets. Specifically, I would like to have four slider for cropping an image and then another separate slider for rotating this cropped image. Of course, both plots should be displayed when I run the code. Here is what I have. I can have this in two cells (the first one crops while the second one rotates), but not in one. <code>  def image_crop(a,b,c,d): img_slic=frame[a:b,c:d] plt.figure(figsize=(8,8)) plt.imshow(img_slic,cmap='RdBu') return a,b,c,dinteractive_plot = interactive(image_crop, a = widgets.IntSlider(min=0,max=2000,step=10,value=500,description='Vertical_Uppper'), b = widgets.IntSlider(min=0,max=2000,step=10,value=500,description='Vertical_Lower'), c = widgets.IntSlider(min=0,max=1000,step=10,value=500,description='Horizontal_Left'), d = widgets.IntSlider(min=0,max=1000,step=10,value=500,description='Horizontal_Right') )interactive_plotdef image_rot(i): img_rot=scipy.ndimage.rotate(frame_slic.T,i) plt.figure(figsize=(8,8)) plt.imshow(img_rot,cmap='RdBu') return iinteractive_plot_2 = interactive(image_rot, i = widgets.IntSlider(min=-180,max=180,step=1,value=0,description='Rotation'))",How to include multiple interactive widgets in the same cell in Jupyter notebook
Why does Python empty string have size of 51 bytes instead of 49 bytes?," I tested sys.getsize('') and sys.getsize(' ') in three environments, and in two of them sys.getsize('') gives me 51 bytes (one byte more than the second) instead of 49 bytes:Screenshots:Win8 + Spyder + CPython 3.6:Win8 + Spyder + IPython 3.6:Win10 (VPN remote) + PyCharm + CPython 3.7:First editI did a second test in Python.exe instead of Spyder and PyCharm (These two are still showing 51), and everything seems to be good. Apparently I don't have the expertise to solve this problem so I'll leave it to you guys :)Win10 + Python 3.7 console versus PyCharm using same interpreter:Win8 + IPython 3.6 + Spyder using same interpreter: <code> ",Why does an empty string in Python sometimes take up 49 bytes and sometimes 51?
Slicing a 2D numpy array based on a boolean array," I have 2D numpy array something like this: and a boolean array: Now, when I try to slice arr based on boolarr, it gives me Output: But I am looking to have a 2D array output instead. The desired output is <code>  arr = np.array([[1,2,4], [2,1,1], [1,2,3]]) boolarr = np.array([[True, True, False], [False, False, True], [True, True,True]]) arr[boolarr] array([1, 2, 1, 1, 2, 3]) [[1, 2], [1], [1, 2, 3]]",Mask 2D array preserving shape
READING GMAIL MESSAGES USING PYTHON IMAP," Though I did most of it after searching a lot from lots of sites I am still not able to get the correct output which I wanted.Code: Output: Bharath Joshi subj: hehe From nobody Tue Dec 25 15:48:52 2018 Content-Type: text/plain; charset=""UTF-8"" hello444444444 Bharath Joshi subj: From nobody Tue Dec 25 15:48:52 2018 Content-Type: text/plain; charset=""UTF-8"" 33333 Bharath Joshi subj: From nobody Tue Dec 25 15:48:53 2018 Content-Type: text/plain; charset=""UTF-8"" hello--22The thing which is bothering me is the extra thing I'm getting i.e.""From nobody ......"" and ""Content type ....""How can I get those removed? <code>  import imaplibimport smtplibimport emailmail=imaplib.IMAP4_SSL(""imap.gmail.com"")mail.login(""**************@gmail.com"",""********"")mail.select('inbox')type,data=mail.search(None,'ALL')mail_ids=data[0]id_list=mail_ids.split()for i in range(int(id_list[-1]),int(id_list[0])-1,-1): typ,data=mail.fetch(i,'(RFC822)') for response_part in data : if isinstance(response_part,tuple): msg=email.message_from_string(response_part[1]) email_from=msg['from'] email_subj=msg['subject'] c=msg.get_payload(0) print email_from print ""subj:"",email_subj print c",Reading Gmail messages using Python IMAP
Reading gmail messaged using Python IMAP," Though I did most of it after searching a lot from lots of sites I am still not able to get the correct output which I wanted.Code: Output: Bharath Joshi subj: hehe From nobody Tue Dec 25 15:48:52 2018 Content-Type: text/plain; charset=""UTF-8"" hello444444444 Bharath Joshi subj: From nobody Tue Dec 25 15:48:52 2018 Content-Type: text/plain; charset=""UTF-8"" 33333 Bharath Joshi subj: From nobody Tue Dec 25 15:48:53 2018 Content-Type: text/plain; charset=""UTF-8"" hello--22The thing which is bothering me is the extra thing I'm getting i.e.""From nobody ......"" and ""Content type ....""How can I get those removed? <code>  import imaplibimport smtplibimport emailmail=imaplib.IMAP4_SSL(""imap.gmail.com"")mail.login(""**************@gmail.com"",""********"")mail.select('inbox')type,data=mail.search(None,'ALL')mail_ids=data[0]id_list=mail_ids.split()for i in range(int(id_list[-1]),int(id_list[0])-1,-1): typ,data=mail.fetch(i,'(RFC822)') for response_part in data : if isinstance(response_part,tuple): msg=email.message_from_string(response_part[1]) email_from=msg['from'] email_subj=msg['subject'] c=msg.get_payload(0) print email_from print ""subj:"",email_subj print c",Reading Gmail messages using Python IMAP
"Python 3.7: None.__eq__(""a"") evaluates as True? (Not Quite)"," If you execute the following statement in Python 3.7, it will (from my testing) print b: However, None.__eq__(""a"") evaluates to NotImplemented.Naturally, ""a"".__eq__(""a"") evaluates to True, and ""b"".__eq__(""a"") evaluates to False.I initially discovered this when testing the return value of a function, but didn't return anything in the second case -- so, the function returned None.What's going on here? <code>  if None.__eq__(""a""): print(""b"")","Why does `if None.__eq__(""a"")` seem to evaluate to True (but not quite)?"
"Why does `if None.__eq__(""a"")` evaluate to True?"," If you execute the following statement in Python 3.7, it will (from my testing) print b: However, None.__eq__(""a"") evaluates to NotImplemented.Naturally, ""a"".__eq__(""a"") evaluates to True, and ""b"".__eq__(""a"") evaluates to False.I initially discovered this when testing the return value of a function, but didn't return anything in the second case -- so, the function returned None.What's going on here? <code>  if None.__eq__(""a""): print(""b"")","Why does `if None.__eq__(""a"")` seem to evaluate to True (but not quite)?"
"Why does `if None.__eq__(""a"")` evaluate to a truthy value?"," If you execute the following statement in Python 3.7, it will (from my testing) print b: However, None.__eq__(""a"") evaluates to NotImplemented.Naturally, ""a"".__eq__(""a"") evaluates to True, and ""b"".__eq__(""a"") evaluates to False.I initially discovered this when testing the return value of a function, but didn't return anything in the second case -- so, the function returned None.What's going on here? <code>  if None.__eq__(""a""): print(""b"")","Why does `if None.__eq__(""a"")` seem to evaluate to True (but not quite)?"
What is netloc means..?," I'm learning to make login function with Flask-login, and I'm facing with this code in my tutorial that I'm following: But I'm not sure what's the code above that I commented means..?, especially in netloc word, what is that..?, I know that is stand for network locality, but what is the purpose on that line..? <code>  @app.route('/login', methods = ['GET', 'POST'])def login(): if current_user.is_authenticated: return redirect(url_for('index')) form = LoginForm() if form.validate_on_submit(): user = User.query.filter_by(username=form.username.data).first() if user is None or not user.check_password(form.password.data): flash('Invalid username or password') return redirect(url_for('login')) login_user(user, remember=form.remember_me.data) next_page = request.args.get('next') if not next_page or url_parse(next_page).netloc != '': # what is it means in this line..? next_page = url_for('index') return redirect(next_page) return render_template('login.html', title='Sign In', form=form)",What does netloc mean?
vscode autopep8 allow statements before imports," I'm using Visual Studio Code with the Python plugin and autopep8 with: I have local packages I need to import, so I have something like: but when I save, Visual Studio Code/autopep8 moves all import statements before the code, so Python can't find my local package. How can I tell Visual Studio Code/autopep8 that it's okay to put a statement before imports, or is there a more correct way of importing local packages?As a workaround, it looks like it's fine if you import in an if statement: <code>  ""editor.formatOnSave"": true import syssys.path.insert(0, '/path/to/packages')import localpackage import sysimport localpackagesys.path.insert(0, '/path/to/packages') import syssys.path.insert(0, '/path/to/packages')if 'localpackage' not in sys.modules: import localpackage",Allow statements before imports with Visual Studio Code and autopep8
Can a Tensorflow variable be trained using the Tensorflow keras functional API?," I am wondering if Keras model compile/training with the functional API train variables defined by tf.get_variable? Can Keras training also incorporate Tensorflow operations?So basically I am looking to define a Keras model with Tensorflow variables and operations, then use To train the model. The reason for this is that Google's TPUs require either a Keras or TF.Estimator API, with Keras being more recommended, so I am looking to see how easily I can convert my model.BackGroundIt looks like since Tensorflow is the backend, there are ways to mix Keras/Tensorflow variables. This blog post shows how Keras variables are trained using a Tensorflow graph/sessionhttps://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html And also here it shows that Tensorflow variables can be used as input to a Keras modelHow to set the input of a Keras layer of a functional model, with a Tensorflow tensor? So I am wondering if Keras can train Tensorflow variables.ExampleI would like to train the embedding and softmax variables in the Tensorflow architecture below Since Tensorflow Keras uses a Tensorflow backend, I'm guessing it's somehow possible to use and train Tensorflow variables and use Tensorflow operations in training.Why do I want to do this?Google's TPUs require that your architecture be implemented via the Estimator API or Keras API. Since the Keras API is more recommended, there is probably interest in converting a regular Tensorflow Graph/Session to use the Keras API with as few alterations to their code as possible.Knowing how to incorporate Tensorflow operations and train Tensorflow variables using the Keras model compile/train would greatly help with this. <code>  model = tf.keras.Model(inputs=inputs, outputs=predictions)model.compile(optimizer=optimizer, loss=loss)model.fit(data, labels, batch_size=batch_size, epochs=epochs) from keras.layers import Dropoutfrom keras import backend as Kimg = tf.placeholder(tf.float32, shape=(None, 784))labels = tf.placeholder(tf.float32, shape=(None, 10))x = Dense(128, activation='relu')(img)x = Dropout(0.5)(x)x = Dense(128, activation='relu')(x)x = Dropout(0.5)(x)preds = Dense(10, activation='softmax')(x)loss = tf.reduce_mean(categorical_crossentropy(labels, preds))train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)with sess.as_default(): for i in range(100): batch = mnist_data.train.next_batch(50) train_step.run(feed_dict={img: batch[0], labels: batch[1], K.learning_phase(): 1})acc_value = accuracy(labels, preds)with sess.as_default(): print acc_value.eval(feed_dict={img: mnist_data.test.images, labels: mnist_data.test.labels, K.learning_phase(): 0}) tf_embedding_input = ... # pre-processing output tensor# Keras modelmodel = Sequential()model.add(Input(tensor=tf_embedding_input)) model.add(Embedding(max_features, 128, input_length=maxlen)) embeddings = tf.get_variable( 'embeddings', initializer= tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) softmax_weights = tf.get_variable( 'softmax_weights', initializer= tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size))) softmax_biases = tf.get_variable('softmax_biases', initializer= tf.zeros([vocabulary_size]), trainable=False ) embed = tf.nn.embedding_lookup(embeddings, train_dataset) #train data set is embed_reshaped = tf.reshape( embed, [batch_size*num_inputs, embedding_size] ) segments= np.arange(batch_size).repeat(num_inputs) averaged_embeds = tf.segment_mean(embed_reshaped, segments, name=None) loss = tf.reduce_mean( tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds, labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))",Can a Tensorflow variable be trained using the Tensorflow Keras functional API model? Can a Tensorflow operation be used in the functional API Model?
"Python, existing connection error after script restart"," I do HTTP requests on Python 3.7, but if I have some errors during the execution and the script stops, all new HTTP requests (even after restart the script) have the errors: [Errno 10054] An existing connection was forcibly closed by the remote host.I have to disable/enable my network, to remove the error. It's probably due to my PC/OS because the script works on RaspberryPi but not on my Windows 10. But I don't know how to fix it.Here the minimum code to generate the error: Once I have the error, even pip.exe return the same error if I want to install a new module.edit1:I tried something else:I have another script which does only SQL requests, it works perfectly.But once I have the error even this one has the problem:""Lost connection to MySQL server during query (%s)"" % (e,))pymysql.err.OperationalError: (2013, 'Lost connection to MySQL server during query ([WinError 10054] An existing connection was forcibly closed by the remote host)')edit2: I installed bash on Ubuntu on Windows, and I do the same thing on the same PC but ""OS different"", I get ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))I tried on other PC (Windows 7), same problem as mine.@djvg I checked openSSL version (1.1.0i) and TSL version (1.2), so it looks compatible with the link you give. And this is the output of pip freeze (thanks for the tips, I didn't know btw): certifi==2018.11.29 chardet==3.0.4 cycler==0.10.0 idna==2.8 kiwisolver==1.0.1 matplotlib==3.0.2 numpy==1.15.4 PyMySQL==0.9.3 pyparsing==2.3.0 python-dateutil==2.7.5 requests==2.21.0 six==1.12.0 urllib3==1.24.1@M.Spiller I don't use a ""special network"", if I test on my professional network, it doesn't work but with another error (probably proxy issue), so I use another network with no particular security. But I don't think is a security issue, because this error appears once the script stops. And the script works well on raspberry pi. Could be an OS issue ? Like the network board doesn't close the connection correctly the first time. But I don't know how to verify that...@UserX I can't disable the firewall: company politic. I'm agree with you, it's probably the cause. Is it possible to open another connection ? Or to force the closing even when the script crash ? <code>  import requestsimport jsonimport urllib.requestimport socket if __name__ == '__main__': params = json.dumps({""toto"": ""ABCD""}).encode('utf-8') try: head = {'content-type': 'application/json'} #replace http by https, to generate the error, re-writte http, and it will never work again url = 'http://www.google.com' with requests.post(url, data=params, headers=head) as response: print(""All is OK: "" + str(response)) except (urllib.error.URLError, socket.timeout) as e: print(""Error time out: "" + str(e.args)) except Exception as e: print(""Uknown error: "" + str(e.args))",Existing connection error after script restart
Python from urllib3.util.ssl_ import ( ImportError: cannot import name ssl," My resources:Python 2.7, Ubunutu 18.04, Pycharm, virtual box oracleI have an automation solution built in python.The solution can be run from both cmd or pycharm of course.2 options to run automation solution. Once jenkinsRun.py is runnig it will execute each main.py like this: Note that this is how I implemented it 3 years ago..could be better ways like using __import__, but need way to pass arguments, etc...Anyway, when run: All good.When run: which should run main each time with diff args I get exception: This happend only when I run the code on my new environment (see resources above)last week I had old virtul box with ubuntu 15.04 (old) which everything worked well (didn't touch the vode ever since).I have installed on new virtual box from scratch libaries, drivers, etc, etc.Any ideas? <code>  python main.py args a,b,c...(run 1 suite of tests)python jenkinsRun.py arg a,b,c...(run main.py with diff args each time -lets say 5 time for instance) os.system('python main.py %s %s %s %s %s %s'%(STD,config.VpcStackName, '-dryrun', 'false', '-tenant' ,config.PROD_STAGE_Tenant)) python main.py arg a,b,c.. jenkinsRun.py ""/home/ohad/.local/lib/python2.7/site-packages/botocore/httpsession.py"", line 7, in <module> from urllib3.util.ssl_ import (ImportError: cannot import name ssl",from urllib3.util.ssl_ import ( ImportError: cannot import name ssl
How to check which Python installation Spyder is running?," The question:This post explains how to change which Python installation to run in Spyder under Tools > Preferences > Python Interpreter. One of the options is Default (i.e. same as Spyder's):But how do you find out what that is?Some details:I'm trying to help a colleague run a Python script using a batch file. The script runs just fine from Spyder. But when I'm setting up the batch file, I'm getting the error error importing module Seaborn. So I'm clearly able to run Python (the only python.exe I've found on the system) with the batch setup, but the importing error makes me think that there are more Python installations on the system that I'm not able to detect for some reason, but is being used by Spyder since the very same script does not raise the same error when run from Spyder. I'm a bit confused here, so thank you for any suggestions! <code> ",How to check which Python interpreter Spyder is running on its console?
"How to reproduce ""indexing past lexsort depth"" warning in Pandas"," I'm indexing a large multi-index Pandas df using df.loc[(key1, key2)]. Sometimes I get a series back (as expected), but other times I get a dataframe. I'm trying to isolate the cases which cause the latter, but so far all I can see is that it's correlated with getting a PerformanceWarning: indexing past lexsort depth may impact performance warning.I'd like to reproduce it to post here, but I can't generate another case that gives me the same warning. Here's my attempt: So my question is: what causes this warning? How do I artificially induce it? <code>  def random_dates(start, end, n=10): start_u = start.value//10**9 end_u = end.value//10**9 return pd.to_datetime(np.random.randint(start_u, end_u, n), unit='s')np.random.seed(0)df = pd.DataFrame(np.random.random(3255000).reshape(465000,7)) # same shape as my datadf['date'] = random_dates(pd.to_datetime('1990-01-01'), pd.to_datetime('2018-01-01'), 465000)df = df.set_index([0, 'date'])df = df.sort_values(by=[3]) # unsort indices, just in casedf.index.lexsort_depth> 0df.index.is_monotonic> Falsedf.loc[(0.9987185534991936, pd.to_datetime('2012-04-16 07:04:34'))]# no warning","What causes ""indexing past lexsort depth"" warning in Pandas?"
Parallel loading of Input Files in Pandas Data rame," I have a Requirement, where I have three Input files and need to load them inside the Pandas Data Frame, before merging two of the files into one single Data Frame.The File extension always changes, it could be .txt one time and .xlsx or .csv another time.How Can I run this process parallel, in order to save the waiting/ loading time ?This is my code at the moment, It takes around 20 minutes for me to load my primary_df and secondary_df. So, I am looking for an efficient solution possibly using parallel processing to save time.I timed by Reading operation and it takes most of the time approximately 18 minutes 45 seconds.Hardware Config :- Intel i5 Processor, 16 GB Ram and 64-bit OS Question Made Eligible for bounty :- As I am looking for a working code with detailed steps - using a package with in anaconda environment that supports loading my input files Parallel and storing them in a pandas data frame separately. This should eventually save time. <code>  from time import time # to measure the time taken to run the codestart_time = time()Primary_File = ""//ServerA/Testing Folder File Open/Report.xlsx""Secondary_File_1 = ""//ServerA/Testing Folder File Open/Report2.csv""Secondary_File_2 = ""//ServerA/Testing Folder File Open/Report2.csv""import pandas as pd # to work with the data framesPrimary_df = pd.read_excel (Primary_File)Secondary_1_df = pd.read_csv (Secondary_File_1)Secondary_2_df = pd.read_csv (Secondary_File_2)Secondary_df = Secondary_1_df.merge(Secondary_2_df, how='inner', on=['ID'])end_time = time()print(end_time - start_time)",Parallel loading of Input Files in Pandas Dataframe
What is the fasted way to count occurences of sublists into a nested list?," I have a list of lists in Python and I want to (as fastly as possible : very important...) append to each sublist the number of time it appear into the nested list.I have done that with some pandas data-frame, but this seems to be very slow and I need to run this lines on very very large scale. I am completely willing to sacrifice nice-reading code to efficient one. So for instance my nested list is here: I need to have: EDITOrder in res does not matter at all. <code>  l = [[1, 3, 2], [1, 3, 2] ,[1, 3, 5]] res = [[1, 3, 2, 2], [1, 3, 5, 1]]",What is the faster way to count occurrences of equal sublists in a nested list?
setting python json encoder for number of float digits," I am trying to set the python json library up in order to save to file a dictionary having as elements other dictionaries. There are many float numbers and I would like to limit the number of digits to, for example, 7.According to other posts on SO encoder.FLOAT_REPR shall be used. However it is not working.For example the code below, run in Python3.7.1, prints all the digits: How can I solve that?It might be irrelevant but I am on macOS.EDITThis question was marked as duplicated. However in the accepted answer (and until now the only one) to the original post it is clearly stated: Note: This solution doesn't work on python 3.6+So that solution is not the proper one. Plus it is using the library simplejson not the library json. <code>  import jsonjson.encoder.FLOAT_REPR = lambda o: format(o, '.7f' )d = dict()d['val'] = 5.78686876876089075543d['name'] = 'kjbkjbkj'f = open('test.json', 'w')json.dump(d, f, indent=4)f.close()",How to limit the number of float digits JSONEncoder produces?
How to set the number of float digits JSONEncoder produces?," I am trying to set the python json library up in order to save to file a dictionary having as elements other dictionaries. There are many float numbers and I would like to limit the number of digits to, for example, 7.According to other posts on SO encoder.FLOAT_REPR shall be used. However it is not working.For example the code below, run in Python3.7.1, prints all the digits: How can I solve that?It might be irrelevant but I am on macOS.EDITThis question was marked as duplicated. However in the accepted answer (and until now the only one) to the original post it is clearly stated: Note: This solution doesn't work on python 3.6+So that solution is not the proper one. Plus it is using the library simplejson not the library json. <code>  import jsonjson.encoder.FLOAT_REPR = lambda o: format(o, '.7f' )d = dict()d['val'] = 5.78686876876089075543d['name'] = 'kjbkjbkj'f = open('test.json', 'w')json.dump(d, f, indent=4)f.close()",How to limit the number of float digits JSONEncoder produces?
Error as I do a HoughTransform after replacing red color channel with green," Before detecting circles, I am replacing the red channel with the green channel. After replacing the channel, I pass it through a blur filter and then do a Hough transform to detect circles. But as I do this, I get a crappy error message: I could not make any sense from it and thus could not understand what I could be doing incorrectly. Here is the snippet to what I did. The error happens when I call HoughCircles function. What is it that I might be doing incorrectly? <code>  OpenCV(3.4.1) Error: Assertion failed (!_image.empty() && _image.type() == (((0) & ((1 << 3) - 1)) + (((1)-1) << 3)) && (_image.isMat() || _image.isUMat())) in HoughCircles, file /io/opencv/modules/imgproc/src/hough.cpp, line 1659Traceback (most recent call last): File ""circle_light.py"", line 44, in <module> param1=param1,param2=param2,minRadius=minRadius,maxRadius=maxRadius)cv2.error: OpenCV(3.4.1) /io/opencv/modules/imgproc/src/hough.cpp:1659: error: (-215) !_image.empty() && _image.type() == (((0) & ((1 << 3) - 1)) + (((1)-1) << 3)) && (_image.isMat() || _image.isUMat()) in function HoughCircles import cv2img = cv2.imread(""images/{}"".format(""img.png""), 1)b,g,r = cv2.split(img)img = cv2.merge([b,g,g])img = cv2.GaussianBlur(img,(5,5),0)minDist = 11param1 = 20param2 = 20minRadius = 10maxRadius = 20circles = cv2.HoughCircles( img, cv2.HOUGH_GRADIENT, 1, minDist, param1=param1, param2=param2, minRadius=minRadius, maxRadius=maxRadius)",Error using HoughCircles with 3-channel input
How to specify return type in an async python function?," In TypeScript, you would do something like How can I do the same in Python? I tried the following: And got this traceback: So Coroutine expects 3 types. But why? And what should they be in this case?This is also specified in the docs, but I still don't understand <code>  async function getString(word: string): Promise<string> { return word;} async def get_string(word: str) -> Coroutine[str]: return word TypeError: Too few parameters for typing.Coroutine; actual 1, expected 3",How to specify return type in an async Python function?
Python: validating input when mutating a dataclass," In Python 3.7 there are these new ""dataclass"" containers that are basically like mutable namedtuples. Suppose I make a dataclass that is meant to represent a person. I can add input validation via the __post_init__() function like this: This will let good inputs through: While all of these bad inputs will throw an error: However, since dataclasses are mutable, I can do this: Thus bypassing the input validation.So, what is the best way to make sure that the fields of a dataclass aren't mutated to something bad, after initialization?  <code>  @dataclassclass Person: name: str age: float def __post_init__(self): if type(self.name) is not str: raise TypeError(""Field 'name' must be of type 'str'."") self.age = float(self.age) if self.age < 0: raise ValueError(""Field 'age' cannot be negative."") someone = Person(name=""John Doe"", age=30)print(someone)Person(name='John Doe', age=30.0) someone = Person(name=[""John Doe""], age=30)someone = Person(name=""John Doe"", age=""thirty"")someone = Person(name=""John Doe"", age=-30) someone = Person(name=""John Doe"", age=30)someone.age = -30print(someone)Person(name='John Doe', age=-30)",Validating input when mutating a dataclass
class with only class methods," I have a class with only class methods. Is it a Pythonic way of namespacing? If not, what is the best way to group similar kinds of methods?. <code>  class OnlyClassMethods(object): @classmethod def method_1(cls): pass @classmethod def method_2(cls): pass",Class with only class methods
How to properly do gradient clipping in pytorch?," What is the correct way to perform gradient clipping in pytorch?I have an exploding gradients problem, and I need to program my way around it. <code> ",How to do gradient clipping in pytorch?
Pandas: how to get dataframe subsets having similar values on some columns?," I'm working on the following dataframe: and discovered this Relaxed Functional Dependency (RFD): meaning that for each couple of rows having a difference <=2 on the weight they would have a difference <=1 on the height too.I need to find all the subsets of rows on which this RFD holds, and show the one with more rows.In this case the best (biggest) subset would be: How do I get all such subsets from a dataframe or at least the biggest subset for which this RFD holds?UpdateI implemented the solution suggested by @DYZ, but it seems like the threshold is respected only for single edges and not for the full path from one node to the other of the connected component in the graph.To better explain what I'm talking about, here an example of the subset found with the following RFD Subset This subset is wrong because the rows and have a distance of 11 > 6, on the age, as well as a distance of 5 > 4 on the weight.I think this may be because it's considered the adjacency matrix instead of a distance matrix for the graph, so a row is added if there is at least one edge respecting the threshold, but it should actually be there if all the paths in the connected components respect the thresholds. <code>  height weight shoe_size age0 175 70 40 301 175 75 39 412 175 69 40 333 176 71 40 354 178 81 41 275 169 73 38 496 170 65 39 30 ('weight': 2.0) ==> ('height': 1.0) height weight shoe_size age2 175 69 40 330 175 70 40 303 176 71 40 35 ('height': 1.0, 'age': 6.0) ==> ('weight': 4.0) height weight shoe_size age0 175 70 40 301 175 75 39 412 175 69 40 333 176 71 40 35 height weight shoe_size age0 175 70 40 30 height weight shoe_size age1 175 75 39 41",how to get dataframe subsets having similar values on some columns?
Remove Twitter mentions from Python Pandas column," I have a dataset that includes Tweets from Twitter. Some of them also have user mentions such as @thisisauser. I try to remove that text at the same time I do other cleaning processes. However, when I run the above code, all the Twitter mentions are still on the text. I verified with a Regex online tool that my Regex is working correctly, so the problem should be on the Pandas's code. <code>  def clean_text(row, options): if options['lowercase']: row = row.lower() if options['decode_html']: txt = BeautifulSoup(row, 'lxml') row = txt.get_text() if options['remove_url']: row = row.replace('http\S+|www.\S+', '') if options['remove_mentions']: row = row.replace('@[A-Za-z0-9]+', '') return rowclean_config = { 'remove_url': True, 'remove_mentions': True, 'decode_utf8': True, 'lowercase': True }df['tweet'] = df['tweet'].apply(clean_text, args=(clean_config,))",Remove Twitter mentions from Pandas column
"Pygame: Bullets shot in a laser, forever, unintentionally. Plus a side order of a transparent image"," I have a sprite which shoots bullets. Sometimes bullets also shoot out of invisible shooters.The switch from opponent to shooter mid-program works but when I want the bullets to shoot in a certain way, with delays between each shot, the bullets seem to become a single line (the purple thing in the image is the bullets):The mechanic works when it's the opponent shooting, but not when it's the invisible square.Why is this happening? Is there a small bug I need to fix?Here's my code: <code>  import pygameimport timeimport itertoolsimport ospygame.init()SCREENWIDTH = 1000SCREENHEIGHT = 650screen = pygame.display.set_mode([SCREENWIDTH, SCREENHEIGHT])screen.fill((255, 123, 67))pygame.draw.rect(screen, (230, 179, 204), (0, 50, 1000, 650), 0)background = screen.copy()clock = pygame.time.Clock()stageon = Trueclass Spell: def __init__(self, bullet, pattern, speed, loop, tick_delay): self.bullet = bullet self.pattern = pattern self.speed = speed self.loop = loop self.tick_delay = tick_delayclass Shooter(pygame.sprite.Sprite): def __init__(self, spell, pos, *groups): super().__init__(*groups) self.image = pygame.image.load(""Sprites/transparent.jpg"") self.rect = self.image.get_rect(topleft=(pos)) self.pos = pygame.Vector2(pos) self.start_time = pygame.time.get_ticks() self.currentspell = spell self.speed = 3 self.ticks = 1000 def update(self): time_gone = pygame.time.get_ticks() - self.start_time if self.currentspell is not None and time_gone > self.currentspell.tick_delay: self.start_time = pygame.time.get_ticks() for bullet in self.currentspell.pattern: if bullet[0] <= time_gone: Bullet(self.rect.center, bullet[1], self.currentspell.bullet, sprites, bullets) self.currentspell.loop -= 1 if self.currentspell.loop <= 0: self.currentspell = None self.kill()class Opponent(pygame.sprite.Sprite): def __init__(self, sprite, sequence, *groups): super().__init__(*groups) self.image = sprite self.rect = self.image.get_rect(topleft=(425, 30)) self.pos = pygame.Vector2(self.rect.topleft) self.start_time = pygame.time.get_ticks() self.sequence = sequence self.spellno = 0 self.currentspell = sequence[self.spellno] self.speed = 3 self.ticks = 1000 self.shooters = 0 def update(self): time_gone = pygame.time.get_ticks() - self.start_time if type(self.currentspell) != Spell: Shooter(self.currentspell[0], self.currentspell[1], sprites) self.shooters += 1 if self.shooters != 0: return else: if self.currentspell is not None and time_gone > self.currentspell.tick_delay: self.start_time = pygame.time.get_ticks() for bullet in self.currentspell.pattern: if bullet[0] <= time_gone: Bullet(self.rect.center, bullet[1], self.currentspell.bullet, sprites, bullets) self.currentspell.loop -= 1 if self.currentspell.loop <= 0: self.spellno += 1 if self.spellno >= len(self.sequence): self.currentspell = None else: self.currentspell = self.sequence[self.spellno]sprites = pygame.sprite.Group()class Bullet(pygame.sprite.Sprite): def __init__(self, pos, direction, image, *groups): super().__init__(*groups) self.image = image self.rect = self.image.get_rect(topleft=pos) self.direction = direction self.pos = pygame.Vector2(self.rect.topleft) def update(self): self.pos += self.direction self.rect.topleft = (self.pos.x, self.pos.y) if not screen.get_rect().colliderect(self.rect): self.kill()bullets = pygame.sprite.Group()opponentgroup = pygame.sprite.Group()img4 = pygame.image.load(""Sprites/utd.png"")ut1 = Spell(pygame.image.load(""Sprites/purple-glowey.png""),((0, pygame.Vector2(-1, 1) * 4), (0, pygame.Vector2(-0.5, 1) * 4.5), (0, pygame.Vector2(0, 1) * 5), (0, pygame.Vector2(0.5, 1) * 4.5), (0, pygame.Vector2(1, 1) * 4),),4, 1, 400)ut2 = Spell(pygame.image.load(""Sprites/purple-glowey.png""),((0, pygame.Vector2(1, 0) * 5),),4, 8, 400)op_spells = [ut1, (ut2, (10, 395))]OP = Opponent(img4, op_spells, opponentgroup)sprites.add(OP)def main(): while stageon: for events in pygame.event.get(): if events.type == pygame.QUIT or stageon == False: time.sleep(1) pygame.quit() return sprites.update() screen.blit(background, (0, 0)) sprites.draw(screen) pygame.display.update() clock.tick(100) if stageon == False: returnif __name__ == '__main__': main()","""Bullets"" shot from two Pygame sprites combine into a single long line"
Why does Python copy numpy arrays where the length of the dimensions are the same?," I have a problem with referencing to a NumPy array.I have an array of the form If I now create a new variable, and do then a is not changing. But if I do the same thing with: so I removed one number in the end of the last dimension. Then I do this again: Now a is changing, what I thought is the normal behavior in Python. Can anybody explain me this? <code>  import numpy as npa = [np.array([0.0, 0.2, 0.4, 0.6, 0.8]), np.array([0.0, 0.2, 0.4, 0.6, 0.8]), np.array([0.0, 0.2, 0.4, 0.6, 0.8])] b = np.array(a) b[0] += 1print(a) a = [array([0. , 0.2, 0.4, 0.6, 0.8]), array([0. , 0.2, 0.4, 0.6, 0.8]), array([0. , 0.2, 0.4, 0.6, 0.8])] a = [np.array([0.0, 0.2, 0.4, 0.6, 0.8]), np.array([0.0, 0.2, 0.4, 0.6, 0.8]), np.array([0.0, 0.2, 0.4, 0.6])] b = np.array(a)b[0] += 1print(a) a = [array([1. , 1.2, 1.4, 1.6, 1.8]), array([0. , 0.2, 0.4, 0.6, 0.8]), array([0. , 0.2, 0.4, 0.6])]",Why does Python copy NumPy arrays where the length of the dimensions are the same?
How do I get an average 'spread' from Python list of numbers," Lets say I have a list of numbers: but the list contains a little too much data, and I want to convey the spread. I want to print 10 of those numbers. With 1 being the highest number and 10 being the lowest number, how could I sort the list and print a range of 10 numbers to represent the spread from highest to lowest?Lets say the list is [1,2,3,4,5,6,7,8,9,10,11,12,13] and I wanted a spread of four numbers in that range, the spread would be [1,5,9,13]. <code>  some_numbers = [16.0, 16.01, 24.53, 22.99, 22.72, 22.71, 22.2, 21.36, 21.34, 21.0, 22.67, 22.62, 15.89, 23.54, 27.0, 21.35, 26.99, 25.46, 22.54, 22.53, 17.99, 22.13, 17.97, 17.96, 17.95, 22.4, 22.32, 22.25, 22.19, 22.16, 20.68, 21.74, 15.38, 11.13, 15.82, 22.33, 22.31, 22.23, 22.15, 22.12, 22.11, 22.07, 18.99, 18.94, 18.86, 18.85, 18.82, 18.81, 16.79, 15.98, 15.96, 15.94, 15.9, 15.86, 15.85, 15.83, 11.47, 11.46, 11.36, 11.34, 11.32, 11.28, 11.26, 11.25, 11.21, 11.19, 11.18, 9.99]",How to get an average 'spread' from a list of numbers?
How to use the pickle for k-means clustering, I want to dump and load my Sklearn trained model using Pickle. How to do that? <code> ,How to use pickle to save sklearn model
How to use the pickle to save sklearn model, I want to dump and load my Sklearn trained model using Pickle. How to do that? <code> ,How to use pickle to save sklearn model
Viewing a Graph on Tensorboard, I just have a graph.pbtxt file. I want to view the graph in tensorboard. But I am not aware of how to do that. Do I have to write any python script or can I do it from the terminal itself? Kindly help me to know the steps involved. <code> ,Viewing Graph from saved .pbtxt file on Tensorboard
How do I kill a process in python 3 using the multiprocessing module?," I have a process that is essentially just an infinite loop and I have a second process that is a timer. How can I kill the loop process once the timer is done? I want the python script to end once the timer is done. <code>  def action(): x = 0 while True: if x < 1000000: x = x + 1 else: x = 0def timer(time): time.sleep(time) exit() loop_process = multiprocessing.Process(target=action)loop_process.start()timer_process = multiprocessing.Process(target=timer, args=(time,))timer_process.start()",How to kill a process using the multiprocessing module?
Is there a simple way to print a python class's hierarchy in tree form?," Is there a function that can print a python class' heirarchy in tree form, like git log --graph does for git commits?Example of what I'd like to do: Example of what the output might look like (but variations are fine). Bonus points if the mro can also be read off directly from the graph, as I've done here from top to bottom, but if not that's fine as well. <code>  class A(object): passclass B(A): passclass C(B): passclass D(A): passclass E(C, D): passprinttree(E) E|\C || DB ||/A|object",Is there a simple way to print a class' hierarchy in tree form?
Seabon & Matplotlib Adding Text Relative to Axes," Trying to use seaborn and matplotlib to plot some data, need to add some descriptive text to my plot, normally I'd just use the matplotlib command text, and place it where I wanted relative to the axes, but it doesn't appear to work at all, I get no text showing beyond the default stuff on the axes, ticks, etc. What I want is some custom text showing in the top left corner of the plot area. df is my pandas dataframe, it just contains some columns of time and coordinate data with a tag ""p"" which is an identifier. Anyone know how I can get some text to show, relatively positioned, the ""value"" items are just the variables with the data I want to print. Thanks. <code>  import seaborn as snsimport matplotlib.pyplot as pltimport pandas as pd ax2 = sns.scatterplot(""t"",""x"", data = df, hue = ""p"")ax2.text(0.1, 0.9, r""$s = {}, F = {}, N = {}$"".format(value1, valu2, value3))plt.show()",Seaborn & Matplotlib Adding Text Relative to Axes
How to multi-thread with for loop in Python," Similar questions may have been asked a couple of times before, but none of them seem to have my case/scenario or it does not work.I am trying to multithread a for loop as showed in an example. This for loop will do a function as it loops through an array. I would like to multithread it.Example: This should loop through the array and do the function dosomething with the variables a, then b, c, etc.Any idea on how I can do that? <code>  array = [""a"", ""b"", ""c"", ""d"", ""e""]def dosomething(var): #dosomething this is just an example as my actual code is not relevant to this questionfor arrayval in array: dosomething(arrayval)","How to multi-thread with ""for"" loop?"
Why does Google Colab uses 2 spaces per indentation level - and how to change this default setting to the PEP-8 compliant 4?," I am using Google Colaboratory to write Python code in their notebook. Whenever I hit an enter after a for loop definition or in a try-except block, the new line is automatically indented, which is good, but it uses only 2 whitespaces by default. This is contrary to PEP-8 standard.Why is this the case and how can I change this setting? <code> ",Change indentation level in Google Colab
Change indentation level on Google Colab," I am using Google Colaboratory to write Python code in their notebook. Whenever I hit an enter after a for loop definition or in a try-except block, the new line is automatically indented, which is good, but it uses only 2 whitespaces by default. This is contrary to PEP-8 standard.Why is this the case and how can I change this setting? <code> ",Change indentation level in Google Colab
Selenium using too much ram with Firefox, I am using selenium with Firefox to automate some tasks on Instagram. It basically goes back and forth between user profiles and notifications page and does tasks based on what it finds.It has one infinite loop that makes sure that the task keeps on going. I have sleep() function every few steps but the memory usage keeps increasing. I have something like this in Python: I never close the driver because that will slow down the program by a lot as it has a lot of queries to process. Is there any way to keep the memory usage from increasing overtime without closing or quitting the driver?EDIT: Added explicit conditions but that did not help either. I am using headless mode of Firefox. <code>  while(True): expected_conditions() ...doTask() driver.back() expected_conditions() ...doAnotherTask() driver.forward() expected_conditions(),Selenium using too much RAM with Firefox
Django append function result to object item," I am using django-haystack and I am trying to implement a way to append the page number to a pdf link in order to open it in the specific page. My goal is to open the pdf in the page where the first hit is found. I know the position of the hit in my document and the position where the page changes. For example i know that the first hit starts at character 2067 and the second page changes at character 3000, so i have to open the pdf at the second page.My question is: how can i get the result of the function that finds the number of the page where the pdf should open and render it ? I am thinking that the result should be something like that <a href=""{% static 'img/sample.pdf#page={{ pageNumber }}' %}""> but i am open to any other suggestions. P.S. I am not asking you to solve my problem. I am asking for suggestions or a constructive discussion as i am new to django.Thank you in advanceEDITSo after researching for a bit i did the following. I found that the highlighter class has a function that finds the position of the hits. I added a getter to that class in order to get the positions (I will change it later. For now i want to see if it is working the way i think it should). Then in my views.py file i added the following and in my html i added this just to print the position and see that everything works ok. But the list is empty, meaning that i get no results. Maybe something is not working the way i think it should. Any ideas?Edit #2I think that it is impossible to get the positions from the highlighter since I don't have the actual Highlighter object in order to get the positions using a getter.Is there any other way to pass arguments between highlighter and a view? I managed to get the query term in my view but i don't have the text block where the query term was found nor the full text to search again for the position. In addition, i think that this approach would be slow when the program scales up. <code>  from django.shortcuts import renderfrom haystack.utils.highlighting import Highlighterdef getPage(request): pos = Highlighter.getPos() print (pos) return render(request, 'search/_result_object.html', {'pos': pos}) <ul> {% for element in pos %} <li>{{ element }}</li> {% endfor %}</ul>",Django pass Haystack highlighter result to a view
How to rename categories after after using pandas.cut with IntervalIndex?," I discretized a column in my dataframe using pandas.cut with bins created by IntervalIndex.from_tuples.The cut works as intended however the categories are shown as the tuples I specified in the IntervalIndex. Is there any way to rename the categories into a different label e.g. (Small, Medium, Large)?Example: The resulting categories will be: I am trying to change [(0, 1] < (2, 3] < (4, 5]] into something like 1, 2 ,3 or small, medium ,large. Sadly, the labels parameter arguments of pd.cut is ignored when using IntervalIndex.Thanks!UPDATE:Thanks to @SergeyBushmanov I noticed that this issue only exist when trying to change category labels inside a dataframe (which is what I am trying to do). Updated example: <code>  bins = pd.IntervalIndex.from_tuples([(0, 1), (2, 3), (4, 5)])pd.cut([0, 0.5, 1.5, 2.5, 4.5], bins) [NaN, (0, 1], NaN, (2, 3], (4, 5]]Categories (3, interval[int64]): [(0, 1] < (2, 3] < (4, 5]] In [1]: df = pd.DataFrame([0, 0.5, 1.5, 2.5, 4.5], columns = ['col1'])In [2]: bins = pd.IntervalIndex.from_tuples([(0, 1), (2, 3), (4, 5)])In [3]: df['col1'] = pd.cut(df['col1'], bins)In [4]: df['col1'].categories = ['small','med','large']In [5]: df['col1']Out [5]:0 NaN1 (0, 1]2 NaN3 (2, 3]4 (4, 5]Name: col1, dtype: categoryCategories (3, interval[int64]): [(0, 1] < (2, 3] < (4, 5]]",How to rename categories after using pandas.cut with IntervalIndex?
Wrapping homogenous python objects," I'm looking for a way to have a collection of homogeneous objects, wrap them in another object, but have the wrapper object have the same API as the original and forward the corresponding API call to its object members. Some possible solutions that have been considered, but are inadequate:Rewriting the whole API Why not? Not adequate because the API is fairly large and expanding. Having to maintain the API in multiple spots is not realistic.Code example: Using a list and a for-loop This changes the API on the object. That said, this is the backup solution in the event I can't find something more elegant. In this case, the WrappedApi class would not exist.Code example: I tried usingPython Object Wrapper, but I could not see how to have it call multiple sub-objects with the same arguments.And for anyone curious about the use case, it's actually a collection of several matplotlib axes objects. I don't want to reimplement to entire axes API (it's big), and I don't want to change all the code that makes calls on axes (like plot, step, etc.) <code>  class OriginalApi: def __init__(self): self.a = 1 self.b = ""bee"" def do_something(self, new_a, new_b, put_them_together=None): self.a = new_a or self.a self.b = new_b or self.b if put_them_together is not None: self.b = ""{}{}"".format(self.a, self.b) # etc.class WrappedApi: def __init__(self): self.example_1 = OriginalApi() self.example_2 = OriginalApi() class WrappedApi: def __init__(self): self.example_1 = OriginalApi() self.example_2 = OriginalApi() def do_something(self, new_a, new_b, put_them_together=None): self.example_1.do_something(new_a, new_b, put_them_together) self.example_2.do_something(new_a, new_b, put_them_together) wrapped_apis = [OriginalApi(), OriginalApi()]for wrapped_api in wrapped_apis: wrapped_api.do_something(1, 2, True)",Wrapping homogeneous Python objects
Wrapping homogeneous python objects," I'm looking for a way to have a collection of homogeneous objects, wrap them in another object, but have the wrapper object have the same API as the original and forward the corresponding API call to its object members. Some possible solutions that have been considered, but are inadequate:Rewriting the whole API Why not? Not adequate because the API is fairly large and expanding. Having to maintain the API in multiple spots is not realistic.Code example: Using a list and a for-loop This changes the API on the object. That said, this is the backup solution in the event I can't find something more elegant. In this case, the WrappedApi class would not exist.Code example: I tried usingPython Object Wrapper, but I could not see how to have it call multiple sub-objects with the same arguments.And for anyone curious about the use case, it's actually a collection of several matplotlib axes objects. I don't want to reimplement to entire axes API (it's big), and I don't want to change all the code that makes calls on axes (like plot, step, etc.) <code>  class OriginalApi: def __init__(self): self.a = 1 self.b = ""bee"" def do_something(self, new_a, new_b, put_them_together=None): self.a = new_a or self.a self.b = new_b or self.b if put_them_together is not None: self.b = ""{}{}"".format(self.a, self.b) # etc.class WrappedApi: def __init__(self): self.example_1 = OriginalApi() self.example_2 = OriginalApi() class WrappedApi: def __init__(self): self.example_1 = OriginalApi() self.example_2 = OriginalApi() def do_something(self, new_a, new_b, put_them_together=None): self.example_1.do_something(new_a, new_b, put_them_together) self.example_2.do_something(new_a, new_b, put_them_together) wrapped_apis = [OriginalApi(), OriginalApi()]for wrapped_api in wrapped_apis: wrapped_api.do_something(1, 2, True)",Wrapping homogeneous Python objects
electron python not getting meassge printed," I am learning to use electron js with python and I am using python-shell so I have the following simple python script: and in my main.js: but the hi is not getting printed, what is wrong? <code>  import sys, json# simple JSON echo scriptfor line in sys.stdin: print(json.dumps(json.loads(line))) let {PythonShell} = require('python-shell')let pyshell = new PythonShell('/home/bassel/electron_app/pyapp/name.py', {mode : 'json'});pyshell.send({name:""mark""})pyshell.on('message', function (message) { // received a message sent from the Python script (a simple ""print"" statement) console.log(""hi"");});",Python not printing output
Not able to fetch Href inside nested Divs with Scrapy," I am trying to fetch reddit account name from reddit feed window, from the following link : Now, here I am able to fetch twitter account details successfully using following code: However, I am not able to get reddit account using similar method ?? Even I have tried fetching directly using simple xpath but it doesn't work : Output for : shows But there are many more tags inside this div tag?? why am I not able to get those tags?? Unfortunately, Scrapy is unable to find what is inside this div. This reddit feed even doesn't have an iframe. Is there any separate URL I should be calling??Edit<\b> :I did show(response) in shell. and it have twitter data but not reddit ?? why it should be ? <code>  fetch('https://coinmarketcap.com/currencies/ripple/') #fetch the tweet account of cointweet_account = response.xpath('//a[starts-with(@href, ""https://twitter.com"")]/@href').extract()tweet_account = [s for s in tweet_account if s != 'https://twitter.com/CoinMarketCap']tweet_account = [s for s in tweet_account if len(s) < 60 ]print(tweet_account) reddit_account = response.xpath('//a[starts-with(@href, ""https://www.reddit.com"")]/@href').extract()reddit_account = [s for s in reddit_account if s != 'https://www.reddit.com/r/CoinMarketCap'']reddit_account = [s for s in reddit_account if len(s) < 60 ]print(reddit_account) response.xpath('//*[@id=""reddit""]/div/div[1]/h4/a[2]/@href') response.xpath('//*[@id=""reddit""]').extract() <b>['<div id=""reddit"" class=""col-sm-6 text-left"">\n</div>']</b>",Unable to fetch `href` from Reddit embedded feed window using scrapy
Unable to fetch `href` in nested `Div` tag in Scrapy," I am trying to fetch reddit account name from reddit feed window, from the following link : Now, here I am able to fetch twitter account details successfully using following code: However, I am not able to get reddit account using similar method ?? Even I have tried fetching directly using simple xpath but it doesn't work : Output for : shows But there are many more tags inside this div tag?? why am I not able to get those tags?? Unfortunately, Scrapy is unable to find what is inside this div. This reddit feed even doesn't have an iframe. Is there any separate URL I should be calling??Edit<\b> :I did show(response) in shell. and it have twitter data but not reddit ?? why it should be ? <code>  fetch('https://coinmarketcap.com/currencies/ripple/') #fetch the tweet account of cointweet_account = response.xpath('//a[starts-with(@href, ""https://twitter.com"")]/@href').extract()tweet_account = [s for s in tweet_account if s != 'https://twitter.com/CoinMarketCap']tweet_account = [s for s in tweet_account if len(s) < 60 ]print(tweet_account) reddit_account = response.xpath('//a[starts-with(@href, ""https://www.reddit.com"")]/@href').extract()reddit_account = [s for s in reddit_account if s != 'https://www.reddit.com/r/CoinMarketCap'']reddit_account = [s for s in reddit_account if len(s) < 60 ]print(reddit_account) response.xpath('//*[@id=""reddit""]/div/div[1]/h4/a[2]/@href') response.xpath('//*[@id=""reddit""]').extract() <b>['<div id=""reddit"" class=""col-sm-6 text-left"">\n</div>']</b>",Unable to fetch `href` from Reddit embedded feed window using scrapy
Cube root of a very large number without importing Decimal or gmpy2," I want to compute the cube root of an extremely huge number in Python3.I've tried the function below, as well the Python syntax x ** (1 / n), but they both yield an error: I really need to compute the cube-root to solve a problem in cryptography. I can't use any modules other than math.Binary search: An example number that causes the error is: Result should be this (which is what gmpy2 finds and its correct - I've validated): <code>  OverflowError: (34, 'Numerical result out of range') def find_invpow(x,n): """"""Finds the integer component of the n'th root of x, an integer such that y ** n <= x < (y + 1) ** n. """""" high = 1 while high ** n < x: high *= 2 low = high/2 while low < high: mid = (low + high) // 2 if low < mid and mid**n < x: low = mid elif high > mid and mid**n > x: high = mid else: return mid return mid + 1 num = 68057481137876648248241485864416419482650225630788641878663638907856305801266787545152598107424503316701887749720220603415974959561242770647206405075854693761748645436474693912889174270087450524201874301881144063774246565393171209785613106940896565658550145896382997905000280819929717554126192912435958881333015570058980589421883357999956417864406416064784421639624577881872069579492555550080496871742644626220376297153908107132546228975057498201139955163867578898758090850986317974370013630474749530052454762925065538161450906977368449669946613816 408280486712458018941011423246208684000839238529670746836313590220206147266723174123590947072617862777039701335841276608156219318663582175921048087813907313165314488199897222817084206",Cube root of a very large number using only math library
Python Wget: Check for duplicate files and skip if it exsists?," So I'm downloading files with WGET and I want to check if the file exsists before I download it. I know with the CLI version it has an option to: (see example). With WGET it downloads the file without needing to name it. This is important because I don't want to rename the files when they already have a name. If there is an alternative file downloading method that allows for checking for exsisting files please tell me! Thanks!!! <code>  # check if file exsists# if not, downloadwget.download(url, path)",Python Wget: Check for duplicate files and skip if it exists?
How to deploy pyside2 applications?," I need to deploy a pyside2 application based on Qt 5.12.1 for all major 3 Operative Systems (Windows, Linux and MacOS).I already checked on How to make a Python script standalone executable to run without ANY dependency? but that is not what I want because I need a Qt-related approach like windeployqt, macdeployqt, linuxdeployqt (separate project).As pointed by eyllanesc: ""python is a scripting language that does not generate a binary"". However, the The Qt Company should figure that too and make easier for us to deploy pyside2 applications. At least as easier as deploying C++/QML applications.So I want a tool like windowsdeployqt, macdeployqt, linuxdeployqt... That works with pyside2 applications.[UPDATE]eyllanesc recommended fbs (fman build system) as a start point as there is not an official tool to deploy pyside2 applications. That should work as a workaround. New answers are welcome too.Please answer as soon as possible when The Qt Company releases an official tool.[NOTE]: I'm using Qt Creator 4.8.1 based on Qt 5.12.1 <code> ",How to deploy pyside2 applications? - The Qt way
How to search and play a video on YouTube using selenium in python?," I want to search and play a video using Selenium in Python.Here is the code: But the problem is, it plays the first video on the home page before it completes searching.Can you guys suggest how to play the video? <code>  import timefrom selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECdriver = webdriver.Chrome()driver.maximize_window()driver.get(www.youtube.com)wait = WebDriverWait(driver, 3)presence = EC.presence_of_element_locatedvisible = EC.visibility_of_element_located# Search for the video.time.sleep(1)wait.until(visible((By.NAME, ""search_query"")))driver.find_element_by_name(""search_query"").click()driver.find_element_by_name(""search_query"").send_keys(video)driver.find_element_by_id(""search-icon-legacy"").click()# Play the video.wait.until(visible((By.ID, ""video-title"")))driver.find_element_by_id(""video-title"").click()",How to search and play a video on YouTube using Selenium in Python?
"How to config ""HTTPS"" schemes with drf-yasg auto-generated swagger page?"," I know in a traditional swagger YAML file, we can define the schemes with: However, how can I do the same thing with auto-generated swagger page with the drf-yasg library?Now, the generated swagger page only contains HTTP schemes, but HTTPS is missing. I've tried set the DEFAULT_API_URL in setting.py to https://mybaseurl.com, but it seems not to be working. <code>  schemes: - http - https//ORschemes: [http, https]","How can I configure ""HTTPS"" schemes with the drf-yasg auto-generated swagger page?"
python-click: dependant options with require manipulation," This question is about the click package: I want to setup my command so that some optional options are dependent on a specific option value and are required based on its value.Required options:input (input file)doe (integer , represents algo name)Sub options:if doe isequal to 1 then option generator_string should become required=Trueequal to 2 then option number_of_sample_pointsshould become required=Trueequal to 3 then option number_of_center_pointsshould become required=TrueValid Examples:--input ./input.txt --doe 1 --generator_string 1234--input ./input.txt --doe 2 --number_of_sample_points 3--input ./input.txt --doe 3 --number_of_center_points 2CODE:  <code>  import clickdef check_output(ctx, param, value): if value == 1: if not ctx.params['generator_string']: setOptionAsRequired(ctx, 'generator_string') return valuedef setOptionAsRequired(ctx, name): for p in ctx.command.params: if isinstance(p, click.Option) and p.name == name: p.required = True@click.option('--input', required=True, type=click.Path(exists=True) )@click.option('--doe', required=True, type=int, callback=check_output )@click.option('--generator_string', required=False, type=str, is_eager=True)@click.option('--number_of_sample_points', required=False, type=int, is_eager=True)@click.option('--number_of_center_points', required=False, type=int, is_eager=True)@click.command(context_settings=dict(max_content_width=800))def main(input, doe, generator_string, number_of_sample_points, number_of_center_points): click.echo('is valid command')if __name__ == '__main__': main()",python-click: dependent options on another option
How can I make a 2d pyramidal array?," I've been boggling my head to make this array for some time but having no success doing it in a vectorized way.I need a function that takes in the 2d array size n and produces a 2d array of size (n, n) looking like this: (and can take odd number arguments)Any suggestions would be much appreciated, thanks! <code>  n = 6np.array([[0,0,0,0,0,0], [0,1,1,1,1,0], [0,1,2,2,1,0], [0,1,2,2,1,0], [0,1,1,1,1,0], [0,0,0,0,0,0],",Build 2d pyramidal array - Python / NumPy
pyenv won't build new python version (hangs)," I followed the tutorial from Northwestern to install pyenv and it seems the commands work. But when I run (or whatever version) it just hangs. I get: I have run tail -F /tmp/python-build.somenumber.log and get until I kill the install. Then the log file has Testing on another build version everything looked the same the tail of the log file looks different Maybe it just takes that long, but I ran both of these over 15 hrs each and so I assume something is going very wrong. I can't find a good SO or other post where someone else is experiencing this problem. Any advice? Doubt it'll be the difference, but I'm running zsh.Edit: Have tried in bash as well. Placing the export functions in ~/.bash_profile and ~/.bashrc with no success. I didn't think these would be the errors since I can call the program just fine and that's all these exports are doing, but I can't figure out what the two commenters are trying to suggest by linking different documentations. Edit 2:Running with verbose mode I still see nothing after the Installing line <code>  pyenv install 3.7.0 Downloading Python-3.7.0.tar.xz...-> https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tar.xzInstalling Python-3.7.0... /tmp/python-build.numbers.number ~/Path/tmp/python-build.numbers.number/Python-3.7.0 /tmp/python-build.numbers.number ~/path $ tail -n 20 /tmp/python-build.numbers.logchecking for ieeefp.h... nochecking io.h usability... nochecking io.h presence... nochecking for io.h... nochecking langinfo.h usability... yeschecking langinfo.h presence... yeschecking for langinfo.h... yeschecking libintl.h usability... yeschecking libintl.h presence... yeschecking for libintl.h... yeschecking process.h usability... nochecking process.h presence... nochecking for process.h... nochecking pthread.h usability... yeschecking pthread.h presence... yeschecking for pthread.h... yeschecking sched.h usability... yeschecking sched.h presence... yeschecking for sched.h... yeschecking shadow.h usability... % checking for a BSD-compatible install... /usr/bin/install -cchecking for a thread-safe mkdir -p... /bin/mkdir -pchecking for --with-pydebug... nochecking for --with-assertions... nochecking for --enable-optimizations... nochecking for --with-lto... nochecking target system type... x86_64-pc-linux-gnuchecking for -llvm-profdata... nochecking for llvm-profdata... ''checking for -Wextra... yeschecking whether gcc accepts and needs -fno-strict-aliasing... nochecking if we can turn off gcc unused result warning... yeschecking if we can turn off gcc unused parameter warning... yeschecking if we can turn off gcc missing field initializers warning... yeschecking if we can turn off gcc invalid function cast warning... nochecking if we can turn on gcc mixed sign comparison warning... yeschecking if we can turn on gcc unreachable code warning... nochecking if we can turn on gcc strict-prototypes warning... nochecking if we can make implicit function declaration an error in gcc... yeschecking whether pthreads are available without options... % $ uname -aLinux foo 4.15.0-47-generic #50~16.04.1-Ubuntu SMP Fri Mar 15 16:06:21 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux",pyenv won't build new python version (hangs) [SOLVED]
Flask-migration and SQLAlchemy error when you work with different branch," I'm using Flask-Migrate (Alembic) to manage SQLAlchemy database migrations. I'm working on two different branches with different migrations. If I switch branches, I get an error that the migration is not found.If i merge this branches into the parent branch, I need downgrade migration's on both multiple branches and create new one. If I will not, I get migration's conflict error. How can I do it easier? Maybe another tool that more like Django's migrations, but for Flask? <code> ",Work on multiple branches with Flask-Migrate
Work on multiple branches with different Alembic migrations," I'm using Flask-Migrate (Alembic) to manage SQLAlchemy database migrations. I'm working on two different branches with different migrations. If I switch branches, I get an error that the migration is not found.If i merge this branches into the parent branch, I need downgrade migration's on both multiple branches and create new one. If I will not, I get migration's conflict error. How can I do it easier? Maybe another tool that more like Django's migrations, but for Flask? <code> ",Work on multiple branches with Flask-Migrate
Converting a text document with special format to pandas data frame," I have a text file with the following format: I need to covert this text to a DataFrame with the following format: How I can do it? <code>  1: frack 0.733, shale 0.700, 10: space 0.645, station 0.327, nasa 0.258, 4: celebr 0.262, bahar 0.345 Id Term weight1 frack 0.7331 shale 0.70010 space 0.64510 station 0.32710 nasa 0.2584 celebr 0.2624 bahar 0.345",Converting a text document with special format to Pandas DataFrame
YAML - Dumping a nested object without types," I'm trying to dump some Python objects out into YAML.Currently, regardless of YAML library (pyyaml, oyaml, or ruamel) I'm having an issue where calling .dump(MyObject) gives me correct YAML, but seems to add a lot of metadata about the Python objects that I don't want, in a form that looks like:!!python/object:MyObject and other similar strings.I do not need to be able to rebuild the objects from the YAML, so I am fine for this metadata to be removed completelyOther questions on SO indicate that the common solution to this is to use safe_dump instead of dump. However, safe_dump does not seem to work for nested objects (or objects at all), as it throws this error: I see that the common workaround here is to manually specify Representers for the objects that I am trying to dump. My issue here is that my Objects are generated code that I don't have control over. I will also be dumping a variety of different objects. Bottom line: Is there a way to dump nested objects using .dump, but where the metadata isn't added? <code>  yaml.representer.RepresenterError: ('cannot represent an object', MyObject)",YAML - Dumping a nested object without types/tags
How to open selenium tab in existing browser instance," I need to download a massive amount of excel-files (estimated: 500 - 1000) from sellercentral.amazon.de. Manually downloading is not an option, as every download needs several clicks until the excel pops up.Since amazon cannot provide me a simple xml with its structure, I decided to automate this on my own. The first thing coming to mind was Selenium and Firefox.The Problem:A login to sellercentral is required, as well as 2-factor-authentication (2FA). So if I login once, i can open another tab, enter sellercentral.amazon.de and am instantly logged in. I can even open another instance of the browser, and be instantly logged in there too. They might be using session-cookies. The target URL to ""scrape"" is https://sellercentral.amazon.de/listing/download?ref=ag_dnldinv_apvu_newapvu .But when I open the URL from my python-script with selenium webdrive, a new instance of the browser is launched, in which I am not logged in. Even though, there are instances of firefox running at the same time, in which I am logged in. So I guess the instances launched by selenium are somewhat different.What I've tried:I tried setting a timedelay after the first .get() (to open site), then I'll manually login, and after that redoing the .get(), which makes the script go on for forever. What am I looking for?Need solution to use the two factor authentication token from google authenticator.I want the selenium to be opened up as a tab in the existing instance of the firefox browser, where I will have already logged in beforehand. Therefore no login (should be) required and the ""scraping"" and downloading can be done.If there's no direct way, maybe someone comes up with a workaround?I know selenium cannot download the files itself, as the popups are no longer part of the browser. I'll fix that when I get there.Important Side-Notes: Firefox is not a given! I'll gladly accept a solution for any browser. <code>  from selenium import webdriverimport timebrowser = webdriver.Firefox()# Wait for website to fire onload eventbrowser.get(""https://sellercentral.amazon.de/listing/download?ref=ag_dnldinv_apvu_newapvu"")time.sleep(30000)browser.get(""https://sellercentral.amazon.de/listing/download?ref=ag_dnldinv_apvu_newapvu"")elements = browser.find_elements_by_tag_name(""browse-node-component"")print(str(elements))",How to handle Google Authenticator with Selenium
How to Google Authenticator with Selenium," I need to download a massive amount of excel-files (estimated: 500 - 1000) from sellercentral.amazon.de. Manually downloading is not an option, as every download needs several clicks until the excel pops up.Since amazon cannot provide me a simple xml with its structure, I decided to automate this on my own. The first thing coming to mind was Selenium and Firefox.The Problem:A login to sellercentral is required, as well as 2-factor-authentication (2FA). So if I login once, i can open another tab, enter sellercentral.amazon.de and am instantly logged in. I can even open another instance of the browser, and be instantly logged in there too. They might be using session-cookies. The target URL to ""scrape"" is https://sellercentral.amazon.de/listing/download?ref=ag_dnldinv_apvu_newapvu .But when I open the URL from my python-script with selenium webdrive, a new instance of the browser is launched, in which I am not logged in. Even though, there are instances of firefox running at the same time, in which I am logged in. So I guess the instances launched by selenium are somewhat different.What I've tried:I tried setting a timedelay after the first .get() (to open site), then I'll manually login, and after that redoing the .get(), which makes the script go on for forever. What am I looking for?Need solution to use the two factor authentication token from google authenticator.I want the selenium to be opened up as a tab in the existing instance of the firefox browser, where I will have already logged in beforehand. Therefore no login (should be) required and the ""scraping"" and downloading can be done.If there's no direct way, maybe someone comes up with a workaround?I know selenium cannot download the files itself, as the popups are no longer part of the browser. I'll fix that when I get there.Important Side-Notes: Firefox is not a given! I'll gladly accept a solution for any browser. <code>  from selenium import webdriverimport timebrowser = webdriver.Firefox()# Wait for website to fire onload eventbrowser.get(""https://sellercentral.amazon.de/listing/download?ref=ag_dnldinv_apvu_newapvu"")time.sleep(30000)browser.get(""https://sellercentral.amazon.de/listing/download?ref=ag_dnldinv_apvu_newapvu"")elements = browser.find_elements_by_tag_name(""browse-node-component"")print(str(elements))",How to handle Google Authenticator with Selenium
Python - What if the end-user didn't have the required library?, My Python project uses various kind of libraries. What is the protocol if the end-user didn't have one or any of them?Should a window pop up and notify him/her which package and version to download in his/her environment? Or I should include the libraries within my project?What's the proper action? <code> ,What if the end-user didn't have the required library?
"Merge 'left', but override 'right values where possible"," ObjectiveI've reviewed pandas documentation on merge but have a question on overriding values efficiently in a 'left' merge. I can do this simply for one pair of values (as seen here), but it becomes cluttered when trying to do multiple pairs.SetupIf I take the following dataframes: I can merge them: to get I want to keep left values where no right value exists, but where possible overwrite with the right values.My desired outcome is: My AttemptI know I can accomplish this with a few lines of code: Or I can use the logic from this question.But this becomes cluttered when there are multiple column pairings where I want to apply this logic. For example, using a and b below: Is there a quicker, cleaner way to get my desired outcome? <code>  a = pd.DataFrame({ 'id': [0,1,2,3,4,5,6,7,8,9], 'val': [100,100,100,100,100,100,100,100,100,100]})b = pd.DataFrame({ 'id':[0,2,7], 'val': [500, 500, 500]}) df = a.merge(b, on=['id'], how='left', suffixes=('','_y')) id val val_y0 0 100 500.01 1 100 NaN2 2 100 500.03 3 100 NaN4 4 100 NaN5 5 100 NaN6 6 100 NaN7 7 100 500.08 8 100 NaN9 9 100 NaN id val0 0 500.01 1 100.02 2 500.03 3 100.04 4 100.05 5 100.06 6 100.07 7 500.08 8 100.09 9 100.0 df.loc[df.val_y.notnull(), 'val'] = df[df.val_y.notnull()].val_ydf = df.drop(['val_y'], axis = 1) a = pd.DataFrame({ 'id': [0,1,2,3,4,5,6,7,8,9], 'val': [100,100,100,100,100,100,100,100,100,100], 'val_2':[200, 200, 200, 200, 200, 200, 200, 200, 200, 200]})b = pd.DataFrame({ 'id':[0,2,7], 'val': [500, 500, 500], 'val_2': [500,500,500]})","Merge 'left', but override 'right' values where possible"
Shortening a long URL with bit.ly v4 and python 3 (migrate from bit.ly v3)," I do have some tutorial code that makes use of the bit.ly API v3 for shortening an URL response. I would like to migrate this code to the v4 API but I do not understand how to set up the correct API endpoint. At least I think this is my error and the API documentation is difficult to understand and to adapt for me, as I'm a beginner.I do have 2 files to work with:bitlyhelper.py (this needs to be migrated to bit.ly API v4)views.pyThis is the relevant code that relates to the URL shortening.bitlyhelper.py views.py The code does work fine when using the v3 API.I tried a few combinations of the API endpoint, but couldn't get it to work.for example simply changing the version number does not work.SHORTEN = ""/v4/shorten?access_token={}&longUrl={}""It would be great if someone could help with setting up the proper API endpoint.Here is the API documentation: https://dev.bitly.com/v4_documentation.htmlThis is the relevant part I think: <code>  import requests#import jsonTOKEN = ""my_general_access_token""ROOT_URL = ""https://api-ssl.bitly.com""SHORTEN = ""/v3/shorten?access_token={}&longUrl={}""class BitlyHelper: def shorten_url(self, longurl): try: url = ROOT_URL + SHORTEN.format(TOKEN, longurl) r = requests.get(url) jr = r.json() return jr['data']['url'] except Exception as e: print (e) from .bitlyhelper import BitlyHelperBH = BitlyHelper()@usr_account.route(""/account/createtable"", methods=[""POST""])@login_requireddef account_createtable(): form = CreateTableForm(request.form) if form.validate(): tableid = DB.add_table(form.tablenumber.data, current_user.get_id()) new_url = BH.shorten_url(config.base_url + ""newrequest/"" + tableid) DB.update_table(tableid, new_url) return redirect(url_for('account.account')) return render_template(""account.html"", createtableform=form, tables=DB.get_tables(current_user.get_id()))",Shortening a long URL with bit.ly v4 (migrate from bit.ly v3) and python 3.7 and bitlyshortener package
Comparing two list in python," I have two list I need to compare the corresponding values only. I have used the below code and getting 36 results as the 1st element in first is comparing with all the six elements of last list. output: I need results by comparing the corresponding elements only. Which means there should be only six outputs. <code>  first= (1,2,3,4,5,6)last=(6,5,4,3,2,1) for x in first: for y in last: if x>y: print(""first is greater then L2"",y) elif x==y: print(""equal"") else: print(""first is less then L2"",y)irst= (1,2,3,4,5,6)last=(6,5,4,3,2,1)for x in first: for y in last: if x>y: print(""first is greater then L2"",y) elif x==y: print(""equal"") else: print(""first is less then L2"",y) L1 is less then L2 6L1 is less then L2 5L1 is less then L2 4L1 is less then L2 3L1 is less then L2 2go dadaL1 is less then L2 6L1 is less then L2 5L1 is less then L2 4L1 is less then L2 3go dadaL1 is greater then L2 1L1 is less then L2 6L1 is less then L2 5L1 is less then L2 4go dadaL1 is greater then L2 2L1 is greater then L2 1L1 is less then L2 6L1 is less then L2 5go dadaL1 is greater then L2 3L1 is greater then L2 2L1 is greater then L2 1L1 is less then L2 6go dadaL1 is greater then L2 4L1 is greater then L2 3L1 is greater then L2 2L1 is greater then L2 1go dadaL1 is greater then L2 5L1 is greater then L2 4L1 is greater then L2 3L1 is greater then L2 2L1 is greater then L2 1y",Comparing two lists element-wise in python
Force sphinx to interpret markdown in python docstrings instead of restructured text," I'm using Sphinx to document a python project. I would like to use Markdown in my docstrings to format them. Even if I use the recommonmark extension, it only covers the .md files written manually, not the docstrings. I use autodoc, napoleon and recommonmark in my extensions.How can I make sphinx parse markdown in my docstrings? <code> ",Force Sphinx to interpret Markdown in Python docstrings instead of reStructuredText
Tf 2.0 : Why are two gradient tapes required for DC Gan example in tensorflow 2.0 guide?," In tf 2.0 DC Gan example in tensorflow 2.0 guide, there are two gradient tapes . See below. As you can see clearly that there are two gradient tapes. I was wondering what difference does using a single tape make and changed it to the following This gives me the following error : I would like to know why two tapes are necessary.As of now the documentation on tf2.0 APIs is scanty. Can anyone explain or point me to the right docs/tutorials?  <code>  @tf.functiondef train_step(images): noise = tf.random.normal([BATCH_SIZE, noise_dim]) with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: generated_images = generator(noise, training=True) real_output = discriminator(images, training=True) fake_output = discriminator(generated_images, training=True) gen_loss = generator_loss(fake_output) disc_loss = discriminator_loss(real_output, fake_output) gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables) gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables) generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables)) discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables)) @tf.functiondef train_step(images): noise = tf.random.normal([BATCH_SIZE, noise_dim]) with tf.GradientTape() as tape: generated_images = generator(noise, training=True) real_output = discriminator(images, training=True) fake_output = discriminator(generated_images, training=True) gen_loss = generator_loss(fake_output) disc_loss = discriminator_loss(real_output, fake_output) gradients_of_generator = tape.gradient(gen_loss, generator.trainable_variables) gradients_of_discriminator = tape.gradient(disc_loss, discriminator.trainable_variables) generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables)) discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables)) RuntimeError: GradientTape.gradient can only be called once on non-persistent tapes.",Tf 2.0 : RuntimeError: GradientTape.gradient can only be called once on non-persistent tapes
Test methods from inner class in Python with unittest," In Java when doing unit testing it is common to have a test class that contains multiple inner classes for each method of the class to test. Each inner class can have multiple testing methods to test a specific functionality of the related method.I am trying to do the same thing in Python with unittest, but it seems that the inner classes' methods are not executed. For example: Expected behavior: Actual behavior: Is it possible to execute the inner class methods as well with unittest ? <code>  import unittestclass OuterTestClass(unittest.TestCase): print(""start outer class"") def test_should_do_something(self): self.assertTrue( True ) print(""outer method test completed"") class InnerTestClass(unittest.TestCase): print(""start inner class"") def test_should_do_something(self): self.assertTrue( True ) print(""inner method test completed"") > start outer class> start inner class> inner method test completed> outer method test completed > start outer class> start inner class> outer method test completed",How to write inner class test method in Python with Unittest
PyQt Qt WebEngine seems to be initialized, When I run my Qt application I get the message Qt WebEngine seems to be initialized from a plugin. Please set Qt::AA_ShareOpenGLContexts using QCoreApplication::setAttribute before constructing QGuiApplication.The app runs fine regardless of the fact that this is getting dumped to the terminal. I cant seem to find the root cause or really understand what this message is trying to tell me. What is this message saying and how can I fix it? <code> ,Qt WebEngine seems to be initialized
How to speed up symbolic derivatives of long functions using sympy?," I am writing a program in Python to solve the Schrdinger equation using the Free ICI Method (well, SICI method right now... but Free ICI is what it will turn into). If this does not sound familiar, that is because there is very little information out there on the subject, and absolutely no sample code to work from.This process involves iteratively arriving at a solution to the partial differential equation. In doing this, there are a lot of symbolic derivatives that need to be performed. The problem is, as the program runs, the functions that need to be differentiated continue to get larger and larger so that by the fifth iteration it takes a very large amount of time to compute the symbolic derivatives.I need to speed this up because I'd like to be able to achieve at least 30 iterations, and I'd like to have it do that before I retire.I've gone through and removed unnecessary repeats of calculations (or at least the ones I know of), which has helped quite a bit. Beyond this, I have absolutely no clue how to speed things up.Here is the code where containing the function that is computing the derivatives (the inf_integrate function is just the composite Simpsons method, as it is way faster than using SymPys integrate, and doesnt raise errors due to oscillatory functions): The program is working and converges to exactly what the expected result is, but it is way too slow. Any help on speeding this up is very much appreciated. <code>  from sympy import *def inf_integrate(fun, n, a, b): f = lambdify(r, fun) h = (b-a)/n XI0 = f(a) + f(b) XI1 = 0 XI2 = 0 for i in range(1, n): X = a + i*h if i % 2 == 0: XI2 = XI2 + f(X) else: XI1 = XI1 + f(X) XI = h*(XI0 + 2*XI2 + 4*XI1)/3 return XIr = symbols('r')def H(fun): return (-1/2)*diff(fun, r, 2) - (1/r)*diff(fun, r) - (1/r)*funE1 = symbols('E1')low = 10**(-5)high = 40n = 5000g = Lambda(r, r)psi0 = Lambda(r, exp(-1.5*r))I1 = inf_integrate(4*pi*(r**2)*psi0(r)*H(psi0(r)), n, low, high)I2 = inf_integrate(4*pi*(r**2)*psi0(r)*psi0(r), n, low, high)E0 = I1/I2print(E0)for x in range(10): f1 = Lambda(r, psi0(r)) f2 = Lambda(r, g(r)*(H(psi0(r)) - E0*psi0(r))) Hf1 = Lambda(r, H(f1(r))) Hf2 = Lambda(r, H(f2(r))) H11 = inf_integrate(4*pi*(r**2)*f1(r)*Hf1(r), n, low, high) H12 = inf_integrate(4*pi*(r**2)*f1(r)*Hf2(r), n, low, high) H21 = inf_integrate(4*pi*(r**2)*f2(r)*Hf1(r), n, low, high) H22 = inf_integrate(4*pi*(r**2)*f2(r)*Hf2(r), n, low, high) S11 = inf_integrate(4*pi*(r**2)*f1(r)*f1(r), n, low, high) S12 = inf_integrate(4*pi*(r**2)*f1(r)*f2(r), n, low, high) S21 = S12 S22 = inf_integrate(4*pi*(r**2)*f2(r)*f2(r), n, low, high) eqn = Lambda(E1, (H11 - E1*S11)*(H22 - E1*S22) - (H12 - E1*S12)*(H21 - E1*S21)) roots = solve(eqn(E1), E1) E0 = roots[0] C = -(H11 - E0*S11)/(H12 - E0*S12) psi0 = Lambda(r, f1(r) + C*f2(r)) print(E0)",How to speed up symbolic derivatives of long functions using SymPy?
Is it possible to specify the max amount of time to wait for code to run with Python?," I have a section of code that I need to get the output from: Though if it takes too long to get the output from get_gps_data(), I would like to cancel the process and set gps to None. Modifying the function is impossible, so is there a way to specify the max amount of time to wait for some code to run and abort if it reaches that time, say 5 seconds? <code>  gps = get_gps_data()",Is it possible to specify the max amount of time to wait for code to run?
How to assign groups based on a mximum sum?," I have a dataframe like this: Further, I have a variable max_sum = 10.I want to assign a group to each row (i) based on the value in keys and (ii) the max_sum which should not be exceeded per group.My expected outcome looks like this: So, the first two values in the a group (1 and 5) sum up to 6 which is less than 10, so they are in the same group. If we now added also 6, max_sum would be exceeded and therefore this value goes into group 2. We cannot add 8 to this group as then again max_sum would be exceeded, therefore we define a group 3. Same then for the values b and c.One can do but I don't know how to get the group info from this. <code>  df = pd.DataFrame({'keys': list('aaaabbbbccccc'), 'values': [1, 5, 6, 8, 2, 4, 7, 7, 1, 1, 1, 1, 5]}) keys values0 a 11 a 52 a 63 a 84 b 25 b 46 b 77 b 78 c 19 c 110 c 111 c 112 c 5 keys values group0 a 1 11 a 5 12 a 6 23 a 8 34 b 2 45 b 4 46 b 7 57 b 7 68 c 1 79 c 1 710 c 1 711 c 1 712 c 5 7 df['cumsum'] = df.groupby('keys')['values'].cumsum() keys values cumsum0 a 1 11 a 5 62 a 6 123 a 8 204 b 2 25 b 4 66 b 7 137 b 7 208 c 1 19 c 1 210 c 1 311 c 1 412 c 5 9",How to assign groups based on a maximum sum?
Python pandas reindex `inplace`," From the reindex docs: Conform DataFrame to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.Therefore, I thought that I would get a reordered Dataframe by setting copy=False in place (!). It appears, however, that I do get a copy and need to assign it to the original object again. I don't want to assign it back, if I can avoid it (the reason comes from this other question).This is what I am doing: Outs: Reindex gives me the correct output, but I'd need to assign it back to the original object, which is what I wanted to avoid by using copy=False: The desired output after that line is: Why is copy=False not working in place? Is it possible to do that at all?Working with python 3.5.3, pandas 0.23.3 <code>  import numpy as npimport pandas as pddf = pd.DataFrame(np.random.rand(5, 5))df.columns = [ 'a', 'b', 'c', 'd', 'e' ]df.head() a b c d e0 0.234296 0.011235 0.664617 0.983243 0.1776391 0.378308 0.659315 0.949093 0.872945 0.3830242 0.976728 0.419274 0.993282 0.668539 0.9702283 0.322936 0.555642 0.862659 0.134570 0.6758974 0.167638 0.578831 0.141339 0.232592 0.976057 df.reindex( columns=['e', 'd', 'c', 'b', 'a'], copy=False ) e d c b a0 0.177639 0.983243 0.664617 0.011235 0.2342961 0.383024 0.872945 0.949093 0.659315 0.3783082 0.970228 0.668539 0.993282 0.419274 0.9767283 0.675897 0.134570 0.862659 0.555642 0.3229364 0.976057 0.232592 0.141339 0.578831 0.167638",Why doesn't pandas reindex() operate in-place?
ModuleNotFoundError: No module named 'frontend'," I have installed PymuPDF/fitz because am trying to extract images from PDF files. However, upon running the code below, I am seeing No module named 'frontend'. I have searched but there isn't single report of this kind of error. I have installed PyMuPDF, muPDF and fitz modulesHere is the error in full: <code>  doc = fitz.open(pdf_path) for i in range(len(doc)): for img in doc.getPageImageList(i): xref = img[0] pix = fitz.Pixmap(doc, xref) if pix.n < 5: # this is GRAY or RGB pix.writePNG(""p%s-%s.png"" % (i, xref)) else: # CMYK: convert to RGB first pix1 = fitz.Pixmap(fitz.csRGB, pix) pix1.writePNG(""p%s-%s.png"" % (i, xref)) pix1 = None pix = None Traceback (most recent call last): File ""/home/waqar/PycharmProjects/predator/ExtractFileImage.py"", line 1, in <module> import fitz File ""/home/waqar/anaconda3/envs/retinanet/lib/python3.6/site-packages/fitz/__init__.py"", line 1, in <module> from frontend import * ModuleNotFoundError: No module named 'frontend'","How do I resolve ""No module named 'frontend'"" error message?"
Holding shift key + mouse click," Hello I'm trying to simulate a mouse click while holding the SHIFT key. I have been trying to do this with the pynput module.This is my code so far: I know the code for holding the shift key is working (If I try to press the ""a"" button in the code I see an ""A""). Also I know the mouse click is working. However, together it does not work.Also I tried another code from a StackOverflow post: Pyautogui - Need to hold shift and clickI tried the following code from it: This worked for a minute then it stopped working! Very strange. It fails like 9 out of 10 times. <code>  from pynput.keyboard import Keyfrom pynput.keyboard import Controller as Contfrom pynput.mouse import Button, Controllerimport timemouse = Controller()keyboard = Cont()with keyboard.pressed(Key.shift): mouse.position = (1892, 838) mouse.click(Button.left) import pyautoguipyautogui.keyDown('shift')pyautogui.click()pyautogui.keyUp('shift')",How to simulate a mouse click while holding the SHIFT key in Windows?
Element wise concatenate list of (list of strings)," i have a list of list of strings as below I want to concatenate each string inside the lists element wise, expected output as below: The size of lst can vary. is there any way to accomplish this without going through for loops.I tried to use map, but its not working. please help. <code>  lst = [['a','b','c'],['@','$','#'],['1','2','3']] ['a@1','b$2','c#3'] map(str.__add__,(x for x in list))",Element wise concatenate multiple lists (list of list of strings)
"pandas df ""average if"" on groups from a second df with same shape/index/column names"," I have 2 dataframes like this... I'd like to find the average of values in a for the 4 groups in b. This... ...works for doing one group at a time, but I was wondering if anyone could think of a cleaner method.My expected result is Thanks. <code>  np.random.seed(0)a = pd.DataFrame(np.random.randn(20,3))b = pd.DataFrame(np.random.randint(1,5,size=(20,3))) a[b==1].sum().sum() / a[b==1].count().sum() 1 -0.0887152 -0.3400433 -0.0455964 0.582136dtype: float64",GroupBy operation using an entire dataframe to group values
How to calculate the groupby mean of the previous rows with pandas," I have a dataframe which looks like this: I would like to create a new column which contains the mean of the previous times of the same category. How can I create it ?The new column should look like this: Note: If it is the first time, the mean should be NaN.EDIT: as stated by cs95, my question was not really the same as this one since here, expanding is required. <code>  pd.DataFrame({'category': [1,1,1,2,2,2,3,3,3,4], 'order_start': [1,2,3,1,2,3,1,2,3,1], 'time': [1, 4, 3, 6, 8, 17, 14, 12, 13, 16]})Out[40]: category order_start time0 1 1 11 1 2 42 1 3 33 2 1 64 2 2 85 2 3 176 3 1 147 3 2 128 3 3 139 4 1 16 pd.DataFrame({'category': [1,1,1,2,2,2,3,3,3,4], 'order_start': [1,2,3,1,2,3,1,2,3,1], 'time': [1, 4, 3, 6, 8, 17, 14, 12, 13, 16], 'mean': [np.nan, 1, 2.5, np.nan, 6, 7, np.nan, 14, 13, np.nan]})Out[41]: category order_start time mean0 1 1 1 NaN1 1 2 4 1.0 = 1 / 12 1 3 3 2.5 = (4+1)/23 2 1 6 NaN4 2 2 8 6.0 = 6 / 15 2 3 17 7.0 = (8+6) / 26 3 1 14 NaN7 3 2 12 14.08 3 3 13 13.09 4 1 16 NaN",pandas GroupBy and cumulative mean of previous rows in group
GroupBy and cumulative mean of the previous rows with pandas," I have a dataframe which looks like this: I would like to create a new column which contains the mean of the previous times of the same category. How can I create it ?The new column should look like this: Note: If it is the first time, the mean should be NaN.EDIT: as stated by cs95, my question was not really the same as this one since here, expanding is required. <code>  pd.DataFrame({'category': [1,1,1,2,2,2,3,3,3,4], 'order_start': [1,2,3,1,2,3,1,2,3,1], 'time': [1, 4, 3, 6, 8, 17, 14, 12, 13, 16]})Out[40]: category order_start time0 1 1 11 1 2 42 1 3 33 2 1 64 2 2 85 2 3 176 3 1 147 3 2 128 3 3 139 4 1 16 pd.DataFrame({'category': [1,1,1,2,2,2,3,3,3,4], 'order_start': [1,2,3,1,2,3,1,2,3,1], 'time': [1, 4, 3, 6, 8, 17, 14, 12, 13, 16], 'mean': [np.nan, 1, 2.5, np.nan, 6, 7, np.nan, 14, 13, np.nan]})Out[41]: category order_start time mean0 1 1 1 NaN1 1 2 4 1.0 = 1 / 12 1 3 3 2.5 = (4+1)/23 2 1 6 NaN4 2 2 8 6.0 = 6 / 15 2 3 17 7.0 = (8+6) / 26 3 1 14 NaN7 3 2 12 14.08 3 3 13 13.09 4 1 16 NaN",pandas GroupBy and cumulative mean of previous rows in group
Transpose dataset using pandas," There are a lot of questions out there with similar titles but I'm unable to solve the issues that I'm having with my dataset.Dataset: Required output: In the Dataset I've columns which are names as IA[X]_NAME where X = 1..9 and NAME = Raw, Class1 and Class2.What I am trying to do is to just transpose these columns so that it looks like the table shown in Required output i.e. IA will show X value and just like this raw and classes will show their perspective values.So in order to achieve it I sliced the columns as: I don't know if this step was necessary but this gave me the perfect slices of columns but when I put it in melt it is not working properly. I have tried almost every method that is available in other questions. I've also tried this: but didn't work and here is Wide to long implementation which also didn't work: The error I got for wide to long is: ValueError: operands could not be broadcast together with shapes (95,) (431,)As my dataset has 526 columns in real, so that is why I've divided them into two lists one contains 95 column names which will be the i and the rest 431 are the one that I need to show in the row as shown in the sample data set. <code>  ID Country Type Region Gender IA01_Raw IA01_Class1 IA01_Class2 IA02_Raw IA02_Class1 IA02_Class2 QA_Include QA_CommentsSC1 France A Europe Male 4 8 1 J 4 1 yes N/ASC2 France A Europe Female 2 7 2 Q 6 4 yes N/ASC3 France B Europe Male 3 7 2 K 8 2 yes N/ASC4 France A Europe Male 4 8 2 A 2 1 yes N/ASC5 France B Europe Male 1 7 1 F 1 3 yes N/AID6 France A Europe Male 2 8 1 R 3 7 yes N/AID7 France B Europe Male 2 8 1 Q 4 6 yes N/AUC8 France B Europe Male 4 8 2 P 4 2 yes N/A ID Country Type Region Gender IA Raw Class1 Class2 QA_Include QA_CommentsSC1 France A Europe Male 01 K 8 1 yes N/ASC1 France A Europe Male 01 L 8 1 yes N/ASC1 France A Europe Male 01 P 8 1 yes N/ASC1 France A Europe Male 02 Q 8 1 yes N/ASC1 France A Europe Male 02 R 8 1 yes N/ASC1 France A Europe Male 02 T 8 1 yes N/ASC1 France A Europe Male 03 G 8 1 yes N/ASC1 France A Europe Male 03 R 8 1 yes N/ASC1 France A Europe Male 03 G 8 1 yes N/ASC1 France A Europe Male 04 K 8 1 yes N/ASC1 France A Europe Male 04 A 8 1 yes N/ASC1 France A Europe Male 04 P 8 1 yes N/ASC1 France A Europe Male 05 R 8 1 yes N/A.... idVars = list(excel_df_final.columns[0:40]) + list(excel_df_final.columns[472:527]) #These contain columns like ID, Country, Type etcvalueVars = excel_df_final.columns[41:472].tolist() #All the IA_ columns pd.melt(excel_df_final, id_vars=idVars,value_vars=valueVars) excel_df_final.set_index(idVars)[41:472].unstack() pd.wide_to_long(excel_df_final, stubnames = ['IA', 'Raw', 'Class1', 'Class2'], i=idVars, j=valueVars)",Wide to long dataset using pandas
Transpose some rows in dataset using pandas," There are a lot of questions out there with similar titles but I'm unable to solve the issues that I'm having with my dataset.Dataset: Required output: In the Dataset I've columns which are names as IA[X]_NAME where X = 1..9 and NAME = Raw, Class1 and Class2.What I am trying to do is to just transpose these columns so that it looks like the table shown in Required output i.e. IA will show X value and just like this raw and classes will show their perspective values.So in order to achieve it I sliced the columns as: I don't know if this step was necessary but this gave me the perfect slices of columns but when I put it in melt it is not working properly. I have tried almost every method that is available in other questions. I've also tried this: but didn't work and here is Wide to long implementation which also didn't work: The error I got for wide to long is: ValueError: operands could not be broadcast together with shapes (95,) (431,)As my dataset has 526 columns in real, so that is why I've divided them into two lists one contains 95 column names which will be the i and the rest 431 are the one that I need to show in the row as shown in the sample data set. <code>  ID Country Type Region Gender IA01_Raw IA01_Class1 IA01_Class2 IA02_Raw IA02_Class1 IA02_Class2 QA_Include QA_CommentsSC1 France A Europe Male 4 8 1 J 4 1 yes N/ASC2 France A Europe Female 2 7 2 Q 6 4 yes N/ASC3 France B Europe Male 3 7 2 K 8 2 yes N/ASC4 France A Europe Male 4 8 2 A 2 1 yes N/ASC5 France B Europe Male 1 7 1 F 1 3 yes N/AID6 France A Europe Male 2 8 1 R 3 7 yes N/AID7 France B Europe Male 2 8 1 Q 4 6 yes N/AUC8 France B Europe Male 4 8 2 P 4 2 yes N/A ID Country Type Region Gender IA Raw Class1 Class2 QA_Include QA_CommentsSC1 France A Europe Male 01 K 8 1 yes N/ASC1 France A Europe Male 01 L 8 1 yes N/ASC1 France A Europe Male 01 P 8 1 yes N/ASC1 France A Europe Male 02 Q 8 1 yes N/ASC1 France A Europe Male 02 R 8 1 yes N/ASC1 France A Europe Male 02 T 8 1 yes N/ASC1 France A Europe Male 03 G 8 1 yes N/ASC1 France A Europe Male 03 R 8 1 yes N/ASC1 France A Europe Male 03 G 8 1 yes N/ASC1 France A Europe Male 04 K 8 1 yes N/ASC1 France A Europe Male 04 A 8 1 yes N/ASC1 France A Europe Male 04 P 8 1 yes N/ASC1 France A Europe Male 05 R 8 1 yes N/A.... idVars = list(excel_df_final.columns[0:40]) + list(excel_df_final.columns[472:527]) #These contain columns like ID, Country, Type etcvalueVars = excel_df_final.columns[41:472].tolist() #All the IA_ columns pd.melt(excel_df_final, id_vars=idVars,value_vars=valueVars) excel_df_final.set_index(idVars)[41:472].unstack() pd.wide_to_long(excel_df_final, stubnames = ['IA', 'Raw', 'Class1', 'Class2'], i=idVars, j=valueVars)",Wide to long dataset using pandas
Efficient way to read 15 M lines text files in python," For my application, I need to read multiple files with 15 M lines each, store them in a DataFrame, and save the DataFrame in HDFS5 format. I've already tried different approaches, notably pandas.read_csv with chunksize and dtype specifications, and dask.dataframe. They both take around 90 seconds to treat 1 file, and so I'd like to know if there's a way to efficiently treat these files in the described way. In the following, I show some code of the tests I've done. Here is what the files look like (whitespace consists of a literal tab): <code>  import pandas as pdimport dask.dataframe as ddimport numpy as npimport re # First approachstore = pd.HDFStore('files_DFs.h5')chunk_size = 1e6df_chunk = pd.read_csv(file, sep=""\t"", chunksize=chunk_size, usecols=['a', 'b'], converters={""a"": lambda x: np.float32(re.sub(r""[^\d.]"", """", x)),\ ""b"": lambda x: np.float32(re.sub(r""[^\d.]"", """", x))}, skiprows=15 ) chunk_list = [] for chunk in df_chunk: chunk_list.append(chunk)df = pd.concat(chunk_list, ignore_index=True)store[dfname] = dfstore.close()# Second approachdf = dd.read_csv( file, sep=""\t"", usecols=['a', 'b'], converters={""a"": lambda x: np.float32(re.sub(r""[^\d.]"", """", x)),\ ""b"": lambda x: np.float32(re.sub(r""[^\d.]"", """", x))}, skiprows=15 )store.put(dfname, df.compute())store.close() a b599.998413 14.142895599.998413 20.105534599.998413 6.553850599.998474 27.116098599.998474 13.060312599.998474 13.766775599.998596 1.826706599.998596 18.275938599.998718 20.797491599.998718 6.132450)599.998718 41.646194599.998779 19.145775",Efficient way to read 15 M lines csv files in python
What does mean red triangle in VS Code?, I'm using Scrapy to crawl information and get my JSON file.What does a red rectangle and red background mean? <code> ,What does a red triangle mean in Visual Studio Code?
How to avoid tokenize words with underline?," I am trying to tokenize my texts by using ""nltk.word_tokenize()"" function, but it would split words connected by ""_"".For example, the text ""A,_B_C! is a movie!"" would be split into: The result I want is: My code: Any help would be appreciated! <code>  ['a', ',', '_b_c', '!', 'is','a','movie','!'] ['a,_b_c!', 'is', 'a', 'movie', '!'] import nltktext = ""A,_B_C! is a movie!""nltk.tokenize(text.lower())",How to avoid tokenize words with underscore?
Is there a shortcut to shift back an entire block of code in jupyter notebook?," Are there any shortcuts for code indentation editing in a jupyter notebook like available in VScode, sublime, etc? Specifically, I need help for shifting back a selected piece of code by one tab space. <code> ",shortcuts for editing code indentation in jupyter notebook
I can't locate an element using selenium," I'm trying to find an element using Selenium and I'm not getting it. Follow the HTML code: I've tried the following codes: But I got the same error: <code>  <div aria-disabled=""false"" data-tb-test-id=""DownloadCrosstab-Button"" role=""button"" tabindex=""0"" style=""font-size: 12px; font-weight: normal; color: rgba(0, 0, 0, 0.7); display: inline-block; padding: 0px 24px; position: relative; text-align: center; border-style: solid; border-width: 1px; border-radius: 1px; height: 24px; line-height: 22px; min-width: 90px; box-sizing: border-box; outline: none; white-space: nowrap; user-select: none; cursor: default; background-color: rgba(0, 0, 0, 0); border-color: rgb(203, 203, 203); margin-top: 8px; width: 100%; -webkit-tap-highlight-color: transparent;"">Tabela de referncia cruzada</div> x = browser.find_element_by_id(""Downloadcrosstab"")x = browser.find_element_by_link_text(""Downloadcrosstab-Button"")x = browser.find_element_by_class_name('Crosstab') NoSuchElementException: no such element: Unable to locate element: {""method"":""css selector"",""selector"":"".Crosstab""} (Session info: chrome=75.0.3770.142)",Find an element where data-tb-test-id attribute is present instead of id using Selenium and Python
"Find an element where the id isn't called ""id"" in selenium"," I'm trying to find an element using Selenium and I'm not getting it. Follow the HTML code: I've tried the following codes: But I got the same error: <code>  <div aria-disabled=""false"" data-tb-test-id=""DownloadCrosstab-Button"" role=""button"" tabindex=""0"" style=""font-size: 12px; font-weight: normal; color: rgba(0, 0, 0, 0.7); display: inline-block; padding: 0px 24px; position: relative; text-align: center; border-style: solid; border-width: 1px; border-radius: 1px; height: 24px; line-height: 22px; min-width: 90px; box-sizing: border-box; outline: none; white-space: nowrap; user-select: none; cursor: default; background-color: rgba(0, 0, 0, 0); border-color: rgb(203, 203, 203); margin-top: 8px; width: 100%; -webkit-tap-highlight-color: transparent;"">Tabela de referncia cruzada</div> x = browser.find_element_by_id(""Downloadcrosstab"")x = browser.find_element_by_link_text(""Downloadcrosstab-Button"")x = browser.find_element_by_class_name('Crosstab') NoSuchElementException: no such element: Unable to locate element: {""method"":""css selector"",""selector"":"".Crosstab""} (Session info: chrome=75.0.3770.142)",Find an element where data-tb-test-id attribute is present instead of id using Selenium and Python
Count the identical pairs in two lists Python," My list has I need to count if a[i]==b[i].For the above example, the answer should be Detail description of answer is <code>  a = [1,2,3,4,2,7,3,5,6,7]b = [1,2,3,1,2,5,6,2,6,7] 6 a[0]==b[0] (1==1)a[1]==b[1] (2==2)a[2]==b[0] (3==3)a[4]==b[4] (2==2)a[8]==b[8] (6==6)a[9]==b[9] (7==7)",Count the identical pairs in two lists
Understanding CTC loss keras," I am trying to understand how CTC loss is working for speech recognition and how it can be implemented in Keras.What i think i understood (please correct me if i'm wrong!)Grossly, the CTC loss is added on top of a classical network in order to decode a sequential information element by element (letter by letter for text or speech) rather than directly decoding an element block directly (a word for example).Let's say we're feeding utterances of some sentences as MFCCs.The goal in using CTC-loss is to learn how to make each letter match the MFCC at each time step. Thus, the Dense+softmax output layer is composed by as many neurons as the number of elements needed for the composition of the sentences:alphabet (a, b, ..., z)a blank token (-)a space (_) and an end-character (>)Then, the softmax layer has 29 neurons (26 for alphabet + some special characters).To implement it, i found that i can do something like this: With ALPHABET_LENGTH = 29 (alphabet length + special characters)And:y_true: tensor (samples, max_string_length) containing the truth labels.y_pred: tensor (samples, time_steps, num_categories) containing the prediction, or output of the softmax.input_length: tensor (samples, 1) containing the sequence length for each batch item in y_pred.label_length: tensor (samples, 1) containing the sequence length for each batch item in y_true.(source)Now, i'm facing some problems:What i don't understandIs this implantation the right way to code and use CTC loss?I do not understand what are concretely y_true, input_length and label_length. Any examples?In what form should I give the labels to the network? Again, Any examples? <code>  # CTC implementation from Keras example found at https://github.com/keras- # team/keras/blob/master/examples/image_ocr.pydef ctc_lambda_func(args): y_pred, labels, input_length, label_length = args # the 2 is critical here since the first couple outputs of the RNN # tend to be garbage: # print ""y_pred_shape: "", y_pred.shape y_pred = y_pred[:, 2:, :] # print ""y_pred_shape: "", y_pred.shape return K.ctc_batch_cost(labels, y_pred, input_length, label_length)input_data = Input(shape=(1000, 20))#let's say each MFCC is (1000 timestamps x 20 features)x = Bidirectional(lstm(...,return_sequences=True))(input_data)x = Bidirectional(lstm(...,return_sequences=True))(x)y_pred = TimeDistributed(Dense(units=ALPHABET_LENGTH, activation='softmax'))(x)loss_out = Lambda(function=ctc_lambda_func, name='ctc', output_shape=(1,))( [y_pred, y_true, input_length, label_length])model = Model(inputs=[input_data, y_true, input_length,label_length], outputs=loss_out)",Understanding CTC loss for speech recognition in Keras
Why does os.system() not recommended way to execute shell commands?," According to this: In the previous section, we saw that os.system() function works fine. But its not recommended way to execute shell commands. We will use Python subprocess module to execute system commands.The writer never mentions why os.system() is not the recommended way in his/her post. May I know why it's not recommended?Is there any security bug in os.system() that makes it not recommended way to execute shell commands? <code> ",Why is os.system() not the recommended way to execute shell commands?
How to get the first day of the next month in Python?," How can I get the first date of the next month in Python? For example, if it's now 2019-12-31, the first day of the next month is 2020-01-01. If it's now 2019-08-01, the first day of the next month is 2019-09-01.I came up with this: Is it correct? Is there a better way? <code>  import datetimedef first_day_of_next_month(dt): '''Get the first day of the next month. Preserves the timezone. Args: dt (datetime.datetime): The current datetime Returns: datetime.datetime: The first day of the next month at 00:00:00. ''' if dt.month == 12: return datetime.datetime(year=dt.year+1, month=1, day=1, tzinfo=dt.tzinfo) else: return datetime.datetime(year=dt.year, month=dt.month+1, day=1, tzinfo=dt.tzinfo)# Example usage (assuming that today is 2021-01-28):first_day_of_next_month(datetime.datetime.now())# Returns: datetime.datetime(2021, 2, 1, 0, 0)",How can I get the first day of the next month in Python?
Pandas dataframe join on overlapped timestamps," I have two dataframes. Each one has a timestamp index representing the start time and a duration value (in seconds) which could be used to calculate the end time. The time interval and duration is different for each dataframe, and could vary within each dataframe as well. I want to join these two dataframes such the index and columns of the first are maintained but the parameter values from the second are copied to it using the following scheme:For each row in the first dataframe, assign the param2 value from the first row in the (sorted) second dataframe which contains 50% or more of the time range.Example output below: <code>  duration param1Start Time (UTC) 2017-10-14 02:00:31 60 952017-10-14 02:01:31 60 342017-10-14 02:02:31 60 102017-10-14 02:03:31 60 442017-10-14 02:04:31 60 632017-10-14 02:05:31 60 52... duration param2Start Time (UTC)2017-10-14 02:00:00 300 932017-10-14 02:05:00 300 952017-10-14 02:10:00 300 91... duration param1 param2Start Time (UTC) 2017-10-14 02:00:31 60 95 932017-10-14 02:01:31 60 34 932017-10-14 02:02:31 60 10 932017-10-14 02:03:31 60 44 932017-10-14 02:04:31 60 63 952017-10-14 02:05:31 60 52 95...",Pandas dataframe join on overlapped time ranges
Modifying FFmpeg and OpenCV source code to capture the RTP timestamp for each packet in NTP format," I was trying a little experiment in order to get the timestamps of the RTP packets using the VideoCapture class from Opencv's source code in python, also had to modify FFmpeg to accommodate the changes in Opencv.Since I read about the RTP packet format.Wanted to fiddle around and see if I could manage to find a way to get the NTP timestamps. Was unable to find any reliable help in trying to get RTP timestamps. So tried out this little hack.Credits to ryantheseer on github for the modified code.Version of FFmpeg: 3.2.3Version of Opencv: 3.2.0In Opencv source code:modules/videoio/include/opencv2/videoio.hpp:Added two getters for the RTP timestamp: modules/videoio/src/cap.cpp:Added an import and added the implementation of the timestamp getter: Added the C++ timestamp getters in the VideoCapture class: modules/videoio/src/cap_ffmpeg.cpp:Added an import: Added a method reference definition: Added the method to the module initializer method: Implemented the getter interface: In FFmpeg's source code:libavcodec/avcodec.h:Added the NTP timestamp definition to the AVPacket struct: libavformat/rtpdec.c:Store the ntp time stamp in the struct in the finalize_packet method: libavformat/utils.c:Copy the ntp time stamp from the packet to the frame in the read_frame_internal method: My python code to utilise these changes: What I am getting as my output: What astounded me the most was that when i powered off the ip camera and powered it back on, timestamp would start from 0 then quickly increments. I read NTP time format is relative to January 1, 1900 00:00. Even when I tried calculating the offset, and accounting between now and 01-01-1900, I still ended up getting a crazy high number for the date.Don't know if I calculated it wrong. I have a feeling it's very off or what I am getting is not the timestamp. <code>  ..... /** @brief Gets the upper bytes of the RTP time stamp in NTP format (seconds). */ CV_WRAP virtual int64 getRTPTimeStampSeconds() const; /** @brief Gets the lower bytes of the RTP time stamp in NTP format (fraction of seconds). */ CV_WRAP virtual int64 getRTPTimeStampFraction() const;..... ....#include <cstdint>........static inline uint64_t icvGetRTPTimeStamp(const CvCapture* capture){ return capture ? capture->getRTPTimeStamp() : 0;}... ..../**@brief Gets the upper bytes of the RTP time stamp in NTP format (seconds).*/int64 VideoCapture::getRTPTimeStampSeconds() const{ int64 seconds = 0; uint64_t timestamp = 0; //Get the time stamp from the capture object if (!icap.empty()) timestamp = icap->getRTPTimeStamp(); else timestamp = icvGetRTPTimeStamp(cap); //Take the top 32 bytes of the time stamp seconds = (int64)((timestamp & 0xFFFFFFFF00000000) / 0x100000000); return seconds;}/**@brief Gets the lower bytes of the RTP time stamp in NTP format (seconds).*/int64 VideoCapture::getRTPTimeStampFraction() const{ int64 fraction = 0; uint64_t timestamp = 0; //Get the time stamp from the capture object if (!icap.empty()) timestamp = icap->getRTPTimeStamp(); else timestamp = icvGetRTPTimeStamp(cap); //Take the bottom 32 bytes of the time stamp fraction = (int64)((timestamp & 0xFFFFFFFF)); return fraction;}... ...#include <cstdint>... ...static CvGetRTPTimeStamp_Plugin icvGetRTPTimeStamp_FFMPEG_p = 0;... ...if( icvFFOpenCV )...... icvGetRTPTimeStamp_FFMPEG_p = (CvGetRTPTimeStamp_Plugin)GetProcAddress(icvFFOpenCV, ""cvGetRTPTimeStamp_FFMPEG"");......icvWriteFrame_FFMPEG_p != 0 &&icvGetRTPTimeStamp_FFMPEG_p !=0)...icvGetRTPTimeStamp_FFMPEG_p = (CvGetRTPTimeStamp_Plugin)cvGetRTPTimeStamp_FFMPEG; ...virtual uint64_t getRTPTimeStamp() const { return ffmpegCapture ? icvGetRTPTimeStamp_FFMPEG_p(ffmpegCapture) : 0; } ... typedef struct AVPacket {......uint64_t rtp_ntp_time_stamp;} static void finalize_packet(RTPDemuxContext *s, AVPacket *pkt, uint32_t timestamp){ uint64_t offsetTime = 0; uint64_t rtp_ntp_time_stamp = timestamp;....../*RM: Sets the RTP time stamp in the AVPacket */ if (!s->last_rtcp_ntp_time || !s->last_rtcp_timestamp) offsetTime = 0; else offsetTime = s->last_rtcp_ntp_time - ((uint64_t)(s->last_rtcp_timestamp) * 65536); rtp_ntp_time_stamp = ((uint64_t)(timestamp) * 65536) + offsetTime; pkt->rtp_ntp_time_stamp = rtp_ntp_time_stamp; static int read_frame_internal(AVFormatContext *s, AVPacket *pkt){ ... uint64_t rtp_ntp_time_stamp = 0;... while (!got_packet && !s->internal->parse_queue) { ... //COPY OVER the RTP time stamp TODO: just create a local copy rtp_ntp_time_stamp = cur_pkt.rtp_ntp_time_stamp; ... #if FF_API_LAVF_AVCTX update_stream_avctx(s); #endif if (s->debug & FF_FDEBUG_TS) av_log(s, AV_LOG_DEBUG, ""read_frame_internal stream=%d, pts=%s, dts=%s, "" ""size=%d, duration=%""PRId64"", flags=%d\n"", pkt->stream_index, av_ts2str(pkt->pts), av_ts2str(pkt->dts), pkt->size, pkt->duration, pkt->flags);pkt->rtp_ntp_time_stamp = rtp_ntp_time_stamp; #Just added this line in the if statement.return ret; import cv2uri = 'rtsp://admin:password@192.168.1.67:554'cap = cv2.VideoCapture(uri)while True: frame_exists, curr_frame = cap.read() # if frame_exists: k = cap.getRTPTimeStampSeconds() l = cap.getRTPTimeStampFraction() time_shift = 0x100000000 #because in the getRTPTimeStampSeconds() #function, seconds was multiplied by 0x10000000 seconds = time_shift * k m = (time_shift * k) + l print(""Imagetimestamp: %i"" % m)cap.release() Imagetimestamp: 0 Imagetimestamp: 212041451700224 Imagetimestamp: 212041687629824 Imagetimestamp: 212041923559424 Imagetimestamp: 212042159489024 Imagetimestamp: 212042395418624 Imagetimestamp: 212042631348224 ...",Capturing RTP Timestamps
How to convert a QByteArray to a python string in PySide2," I have a PySide2.QtCore.QByteArray object called roleName which I got encoding a python string: How can I get my decoded string back? <code>  propName = metaProp.name() // this is call of [const char *QMetaProperty::name() ](https://doc.qt.io/qt-5/qmetaproperty.html#name)// encode the objectroleName = QByteArray(propName.encode())print(roleName) // this gives b'myname'// now I would like to get just ""myname"" without the ""b"" roleString = str(roleName)print(roleString) // this gives the same output as above",Convert a QByteArray to a Python text string in PySide2
How do i change the position of the legend in Hvplot / Holoviews (python)?," Hvplot has default the position of the legend on the right outside of the plot. How can I change this default legend position?  <code>  import numpy as npimport pandas as pdimport hvplotimport hvplot.pandasimport holoviews as hvdata = np.random.normal(size=[50, 2])df = pd.DataFrame(data, columns=['a', 'b'])df.hvplot.line()",Change legend position using holoviews / hvplot
Python Hvplot / Holoviews: How do i change the (default) position of the legend?," Hvplot has default the position of the legend on the right outside of the plot. How can I change this default legend position?  <code>  import numpy as npimport pandas as pdimport hvplotimport hvplot.pandasimport holoviews as hvdata = np.random.normal(size=[50, 2])df = pd.DataFrame(data, columns=['a', 'b'])df.hvplot.line()",Change legend position using holoviews / hvplot
How do i change the (default) position of the legend? (hvplot holoviews python)," Hvplot has default the position of the legend on the right outside of the plot. How can I change this default legend position?  <code>  import numpy as npimport pandas as pdimport hvplotimport hvplot.pandasimport holoviews as hvdata = np.random.normal(size=[50, 2])df = pd.DataFrame(data, columns=['a', 'b'])df.hvplot.line()",Change legend position using holoviews / hvplot
Path to execute Python script from Azure WebJob," I'm trying to run python script from Azure webjob. This is what I've done following this linkAccess the kudu tool via the url https://<webapp name>.scm.azurewebsites.net and installed Python 364x86 via Site Extensions tab Confirmed Python 364x86 is installed in the following path: D:\home\python364x86Added my script trading.py in D:\home\python364x86Created run.bat file with this line of code D:\home\python364x86\python.exe trading.pyIncluded run.bat and trading.py in the webjob zip fileDeployed, but getting error Functions.cs Above code executes python script. It works locally, but once I deploy it to production it fails. Tried so many times, spent plethora of hours, but still unsure why prod doesn't work. Help is appreciated.trading.py <code>  [09/07/2019 07:02:00 > 0dd02c: SYS INFO] Status changed to Initializing[09/07/2019 07:02:00 > 0dd02c: SYS INFO] Run script 'run.bat' with script host - 'WindowsScriptHost'[09/07/2019 07:02:00 > 0dd02c: SYS INFO] Status changed to Running[09/07/2019 07:02:00 > 0dd02c: ERR ] The filename, directory name, or volume label syntax is incorrect.[09/07/2019 07:02:00 > 0dd02c: INFO] [09/07/2019 07:02:00 > 0dd02c: INFO] D:\local\Temp\jobs\triggered\z\2az54ret.wh4>D:\home\python364x86\python.exe trading.py [09/07/2019 07:02:00 > 0dd02c: SYS INFO] Status changed to Failed[09/07/2019 07:02:00 > 0dd02c: SYS ERR ] Job failed due to exit code 1 public void StartTheBot() { // Local //var fileName = @""C:\Users\robert\AppData\Local\Programs\Python\Python37-32\python.exe""; //var script = @""C:\python-scripts\trading.py""; // Production var fileName = @""D:\home\python364x86\python.exe""; var script = @""D:\home\python364x86\trading.py""; var errors = """"; var results = """"; ProcessStartInfo psi = new ProcessStartInfo { FileName = fileName, Arguments = script, UseShellExecute = false, RedirectStandardOutput = true, RedirectStandardError = true, CreateNoWindow = true }; using (Process process = Process.Start(psi)) { errors = process.StandardError.ReadToEnd(); results = process.StandardOutput.ReadToEnd(); } Console.WriteLine(""Errors:""); Console.WriteLine(errors); Console.WriteLine(); Console.WriteLine(""Results:""); Console.WriteLine(results); } import telegrammy_token = 'mytoken'bot = telegram.Bot(token = my_token)chat_id = 'mychatid'message = 'Hellobot.sendMessage(chat_id=chat_id, text=message)",Running Python script from Azure WebJob
How to display <IPython.core.display.HTML object>?," I try to run the below codes but I have a problem in showing the results.also, I use pycharm IDE. The output while I run the line ""data.show_batch()"" is: <code>  from fastai.text import *data = pd.read_csv(""data_elonmusk.csv"", encoding='latin1')data.head()data = (TextList.from_df(data, cols='Tweet') .split_by_rand_pct(0.1) .label_for_lm() .databunch(bs=48))data.show_batch() IPython.core.display.HTML object",How to display <IPython.core.display.HTML object>? (SOLVED)
"Air flow, enable dag on creation"," When I create a DAG with air flow in python I can pass some parameters. Yet when I do so, I still have to go on the interface and to enable the DAG with a click. I would like to know if there is a settings to pass to do it directly on creation. I think it has something to do with ""pause"" but can't find the name of the parameter. <code>  SETTINGS = { 'owner': 'hello', 'depends_on_past': False, 'start_date': datetime(2019, 1, 1), 'email_on_failure': False, 'email_on_retry': False, 'retries': 1, 'retry_delay': timedelta(minutes=5), }dag = DAG(dag_id, schedule_interval='@daily', catchup=False, default_args=SETTINGS)","Airflow, enable dag on creation"
Easiest way to copy all fields from one Python dataclass instance to another instance," Let's assume you have defined a Python dataclass: What's the easiest way to copy the values from an instance marker_a to another instance marker_b?Here's an example of what I try to achieve: As a boundary condition, I don't want to create and assign a new instance to marker_b. OK, I could loop through all defined fields and copy the values one by one, but there has to be a simpler way, I guess. <code>  @dataclassclass Marker: a: float b: float = 1.0 marker_a = Marker(1.0, 2.0)marker_b = Marker(11.0, 12.0)# now some magic happens which you hopefully can fill inprint(marker_b)# result: Marker(a=1.0, b=2.0)",Easiest way to copy all fields from one dataclass instance to another?
how to succesfully compile python 3.7," Upon attempting to compile python 3.7 I hit Could not import runpy module: Any advice on how to overcome this and install python3.7 appreciated.Edit - the solution listed below seems to work for various other python versions, so I changed title to python 3.x from 3.7 <code>  jeremyr@b88:$ wget https://www.python.org/ftp/python/3.7.3/Python-3.7.3.tar.xz....jeremyr@b88:~/Python-3.7.3$ ./configure --enable-optimizations jeremyr@b88:~/Python-3.7.3$ make clean jeremyr@b88:~/Python-3.7.3$ make -j32 .... gcc -pthread -Xlinker -export-dynamic -o Programs/_testembed Programs/_testembed.o libpython3.7m.a -lcrypt -lpthread -ldl -lutil -lm ./python -E -S -m sysconfig --generate-posix-vars ;\if test $? -ne 0 ; then \ echo ""generate-posix-vars failed"" ; \ rm -f ./pybuilddir.txt ; \ exit 1 ; \fiCould not import runpy moduleTraceback (most recent call last): File ""/home/jeremyr/Python-3.7.3/Lib/runpy.py"", line 15, in <module> import importlib.util File ""/home/jeremyr/Python-3.7.3/Lib/importlib/util.py"", line 14, in <module> from contextlib import contextmanager File ""/home/jeremyr/Python-3.7.3/Lib/contextlib.py"", line 4, in <module> import _collections_abcSystemError: <built-in function compile> returned NULL without setting an errorgenerate-posix-vars failedMakefile:603: recipe for target 'pybuilddir.txt' failedmake[1]: *** [pybuilddir.txt] Error 1make[1]: Leaving directory '/home/jeremyr/Python-3.7.3'Makefile:531: recipe for target 'profile-opt' failedmake: *** [profile-opt] Error 2jeremyr@88:~/Python-3.7.3$ lsb_release -aNo LSB modules are available.Distributor ID: DebianDescription: Debian GNU/Linux 8.11 (jessie)Release: 8.11Codename: jessiejeremyr@88:~/Python-3.7.3$ gcc --version gcc (Debian 4.9.2-10+deb8u2) 4.9.2Copyright (C) 2014 Free Software Foundation, Inc.This is free software; see the source for copying conditions. There is NOwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.jeremyr@88:~/Python-3.7.3$ sudo apt upgrade gccReading package lists... DoneBuilding dependency tree Reading state information... DoneCalculating upgrade... gcc is already the newest version.jeremyr@b88:~/Python-3.7.3$ echo $PYTHONPATH",how to succesfully compile python 3.x
how to list all function names of a python module in c++," I have a C++ program, I want to import a Python module and list all function names in this module. How can I do it?I used the following code to get the dict from a module: But how to list the functions names? <code>  PyDictObject* pDict = (PyDictObject*)PyModule_GetDict(pModule);",How to list all function names of a Python module in C++?
Target transformation in RFECV scikit-learn," I am using RFECV for feature selection in scikit-learn. I would like to compare the result of a simple linear model (X,y) with that of a log transformed model (using X, log(y))Simple Model:RFECV and cross_val_score provide the same result (we need to compare the average score of cross-validation across all folds with the score of RFECV for all features: 0.66 = 0.66, no problem, results are reliable)Log Model:the Problem: it seems that RFECV does not provide a way to trasnform the y. the scores in this case are 0.55 vs 0.53. This is quite expected though, because I had to manually apply np.log to fit the data: log_seletor = log_selector.fit(X,np.log(y)). This r2 score is for y = log(y), with no inverse_func, while what we need is a way to fit the model on the log(y_train) and calculate the score using exp(y_test). Alternatively, if I try to use the TransformedTargetRegressor, I get the error shown in the code: The classifier does not expose ""coef_"" or ""feature_importances_"" attributesHow do I resolve the problem and make sure that the feature selection process is reliable? Output:  <code>  from sklearn.datasets import make_friedman1from sklearn.feature_selection import RFECVfrom sklearn import linear_modelfrom sklearn.model_selection import cross_val_scorefrom sklearn.compose import TransformedTargetRegressorimport numpy as npX, y = make_friedman1(n_samples=50, n_features=10, random_state=0)estimator = linear_model.LinearRegression()log_estimator = TransformedTargetRegressor(regressor=linear_model.LinearRegression(), func=np.log, inverse_func=np.exp)selector = RFECV(estimator, step=1, cv=5, scoring='r2')selector = selector.fit(X, y)#### log_selector = RFECV(log_estimator, step=1, cv=5, scoring='r2')# log_seletor = log_selector.fit(X,y) # #RuntimeError: The classifier does not expose ""coef_"" or ""feature_importances_"" attributes###log_selector = RFECV(estimator, step=1, cv=5, scoring='r2')log_seletor = log_selector.fit(X,np.log(y))print(""**Simple Model**"")print(""RFECV, r2 scores: "", np.round(selector.grid_scores_,2))scores = cross_val_score(estimator, X, y, cv=5)print(""cross_val, mean r2 score: "", round(np.mean(scores),2), "", same as RFECV score with all features"") print(""no of feat: "", selector.n_features_ )print(""**Log Model**"")log_scores = cross_val_score(log_estimator, X, y, cv=5)print(""RFECV, r2 scores: "", np.round(log_selector.grid_scores_,2))print(""cross_val, mean r2 score: "", round(np.mean(log_scores),2)) print(""no of feat: "", log_selector.n_features_ ) **Simple Model**RFECV, r2 scores: [0.45 0.6 0.63 0.68 0.68 0.69 0.68 0.67 0.66 0.66]cross_val, mean r2 score: 0.66 , same as RFECV score with all featuresno of feat: 6**Log Model**RFECV, r2 scores: [0.39 0.5 0.59 0.56 0.55 0.54 0.53 0.53 0.53 0.53]cross_val, mean r2 score: 0.55no of feat: 3",Target transformation and feature selection in scikit-learn
How to break this Python line?," How to break a long line with multiple bracket pairs to follow PEP 8s 79-character limit? <code>  config[""network""][""connection""][""client_properties""][""service""] = config[""network""][""connection""][""client_properties""][""service""].format(service=service)",How to break a long line with multiple bracket pairs?
How to break this line of code in Python?," How to break a long line with multiple bracket pairs to follow PEP 8s 79-character limit? <code>  config[""network""][""connection""][""client_properties""][""service""] = config[""network""][""connection""][""client_properties""][""service""].format(service=service)",How to break a long line with multiple bracket pairs?
How MaxAbsScaler in Scikit-learn works," I was reading the docs for MaxAbsScaler.https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-rangeI can't understand what exactly it does.Here is an example: It says that it scales in a way that the training data lies within the range [-1, 1] by dividing through the largest maximum value in each feature.I think it works per column when it says in each feature.A simpler explanation would be great. <code>  >>> X_train = np.array([[ 1., -1., 2.],... [ 2., 0., 0.],... [ 0., 1., -1.]])...>>> max_abs_scaler = preprocessing.MaxAbsScaler()>>> X_train_maxabs = max_abs_scaler.fit_transform(X_train)>>> X_train_maxabs # doctest +NORMALIZE_WHITESPACE^array([[ 0.5, -1. , 1. ], [ 1. , 0. , 0. ], [ 0. , 1. , -0.5]])>>> X_test = np.array([[ -3., -1., 4.]])>>> X_test_maxabs = max_abs_scaler.transform(X_test)>>> X_test_maxabs array([[-1.5, -1. , 2. ]])>>> max_abs_scaler.scale_ array([2., 1., 2.])",Can someone explain MaxAbsScaler in Scikit-learn?
How to remove or hide x-axis label from Seaborn Boxplot?," I have a boxplot and need to remove the x-axis ('user_type' and 'member_gender') label. How do I do this given the below format? <code>  sb.boxplot(x=""user_type"", y=""Seconds"", data=df, color = default_color, ax = ax[0,0], sym='').set_title('User-Type (0=Non-Subscriber, 1=Subscriber)')sb.boxplot(x=""member_gender"", y=""Seconds"", data=df, color = default_color, ax = ax[1,0], sym='').set_title('Gender (0=Male, 1=Female, 2=Other)')",How to remove or hide x-axis labels from a seaborn / matplotlib plot
How to remove or hide x-axis labels from a seaborn / matplotlib plot?," I have a boxplot and need to remove the x-axis ('user_type' and 'member_gender') label. How do I do this given the below format? <code>  sb.boxplot(x=""user_type"", y=""Seconds"", data=df, color = default_color, ax = ax[0,0], sym='').set_title('User-Type (0=Non-Subscriber, 1=Subscriber)')sb.boxplot(x=""member_gender"", y=""Seconds"", data=df, color = default_color, ax = ax[1,0], sym='').set_title('Gender (0=Male, 1=Female, 2=Other)')",How to remove or hide x-axis labels from a seaborn / matplotlib plot
tf.reshape is not giving ? for first element?," I am new to tensorflow, I have tensor like below, Output of a.shape is TensorShape([Dimension(2), Dimension(3)])For my computational process I want to reshape the tensor to (?, 2, 3)I am unable to reshape it to desire format.I tried, But it returns, further I tried, it returns, How do I get the desired result?Sorry if it sounds simple problem.  <code>  a = tf.constant([[1, 2, 3], [4, 5, 6]]) tf.reshape(a, [-1, 2, 3]) <tf.Tensor 'Reshape_18:0' shape=(1, 2, 3) dtype=int32> # 1 has to be replaced by ? tf.reshape(a, [-1, -1, 2, 3]) <tf.Tensor 'Reshape_19:0' shape=(?, ?, 2, 3) dtype=int32> # two ? are there",tf.reshape is not giving ?(None) for first element
add points to line chart using plotly.express," plotly.express is very convenient to produce nice interactive plots. The code below generates a line chart colored by country. Now what I need is to add points to the plot. Does anyone know how I can add points to the line chart? <code>  import plotly.express as pxgapminder = px.data.gapminder().query(""continent=='Oceania'"")fig = px.line(gapminder, x=""year"", y=""lifeExp"", color='country')fig.show()",How to add points or markers to line chart using plotly express?
Hwo to add points to line chart using plotly express?," plotly.express is very convenient to produce nice interactive plots. The code below generates a line chart colored by country. Now what I need is to add points to the plot. Does anyone know how I can add points to the line chart? <code>  import plotly.express as pxgapminder = px.data.gapminder().query(""continent=='Oceania'"")fig = px.line(gapminder, x=""year"", y=""lifeExp"", color='country')fig.show()",How to add points or markers to line chart using plotly express?
How to add points to line chart using plotly express?," plotly.express is very convenient to produce nice interactive plots. The code below generates a line chart colored by country. Now what I need is to add points to the plot. Does anyone know how I can add points to the line chart? <code>  import plotly.express as pxgapminder = px.data.gapminder().query(""continent=='Oceania'"")fig = px.line(gapminder, x=""year"", y=""lifeExp"", color='country')fig.show()",How to add points or markers to line chart using plotly express?
"Pytorch CNN won't learn, need help finding out why"," I'm doing a CNN with Pytorch for a task, but it won't learn and improve the accuracy. I made a version working with the MNIST dataset so I could post it here. I'm just looking for an answer as to why it's not working. The architecture is fine, I implemented it in Keras and I had over 92% accuracy after 3 epochs. Note: I reshaped the MNIST into 60x60 pictures because that's how the pictures are in my ""real"" problem. <code>  import numpy as npfrom PIL import Imageimport torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torch.utils.data import DataLoaderfrom torch.autograd import Variablefrom keras.datasets import mnist(x_train, y_train), (x_test, y_test) = mnist.load_data()def resize(pics): pictures = [] for image in pics: image = Image.fromarray(image).resize((dim, dim)) image = np.array(image) pictures.append(image) return np.array(pictures)dim = 60x_train, x_test = resize(x_train), resize(x_test) # because my real problem is in 60x60x_train = x_train.reshape(-1, 1, dim, dim).astype('float32') / 255x_test = x_test.reshape(-1, 1, dim, dim).astype('float32') / 255y_train, y_test = y_train.astype('float32'), y_test.astype('float32') if torch.cuda.is_available(): x_train = torch.from_numpy(x_train)[:10_000] x_test = torch.from_numpy(x_test)[:4_000] y_train = torch.from_numpy(y_train)[:10_000] y_test = torch.from_numpy(y_test)[:4_000]class ConvNet(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 32, 3) self.conv2 = nn.Conv2d(32, 64, 3) self.conv3 = nn.Conv2d(64, 128, 3) self.fc1 = nn.Linear(5*5*128, 1024) self.fc2 = nn.Linear(1024, 2048) self.fc3 = nn.Linear(2048, 1) def forward(self, x): x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2)) x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2)) x = x.view(x.size(0), -1) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = F.dropout(x, 0.5) x = torch.sigmoid(self.fc3(x)) return xnet = ConvNet()optimizer = optim.Adam(net.parameters(), lr=0.03)loss_function = nn.BCELoss()class FaceTrain: def __init__(self): self.len = x_train.shape[0] self.x_train = x_train self.y_train = y_train def __getitem__(self, index): return x_train[index], y_train[index].unsqueeze(0) def __len__(self): return self.lenclass FaceTest: def __init__(self): self.len = x_test.shape[0] self.x_test = x_test self.y_test = y_test def __getitem__(self, index): return x_test[index], y_test[index].unsqueeze(0) def __len__(self): return self.lentrain = FaceTrain()test = FaceTest()train_loader = DataLoader(dataset=train, batch_size=64, shuffle=True)test_loader = DataLoader(dataset=test, batch_size=64, shuffle=True)epochs = 10steps = 0train_losses, test_losses = [], []for e in range(epochs): running_loss = 0 for images, labels in train_loader: optimizer.zero_grad() log_ps = net(images) loss = loss_function(log_ps, labels) loss.backward() optimizer.step() running_loss += loss.item() else: test_loss = 0 accuracy = 0 with torch.no_grad(): for images, labels in test_loader: log_ps = net(images) test_loss += loss_function(log_ps, labels) ps = torch.exp(log_ps) top_p, top_class = ps.topk(1, dim=1) equals = top_class.type('torch.LongTensor') == labels.type(torch.LongTensor).view(*top_class.shape) accuracy += torch.mean(equals.type('torch.FloatTensor')) train_losses.append(running_loss/len(train_loader)) test_losses.append(test_loss/len(test_loader)) print(""[Epoch: {}/{}] "".format(e+1, epochs), ""[Training Loss: {:.3f}] "".format(running_loss/len(train_loader)), ""[Test Loss: {:.3f}] "".format(test_loss/len(test_loader)), ""[Test Accuracy: {:.3f}]"".format(accuracy/len(test_loader)))",Loss doesn't decrease in Pytorch CNN
how to fix module 'platform' has no attribute 'linux_distribution' error for recently installed python 3.8," I had Python versions of 2.7 and 3.5. I wanted the install a newer version of Python which is python 3.8. I am using Ubuntu 16.04 and I can not just uninstall Python 3.5 due to the dependencies. So in order to run my scripts, I use python3.8 app.py. No problem so far. But when I want to install new packages via pip: It throws an error: So far, I tried: and chose python3.8 and run command by starting with python3 but no luck.Then: I also tried running the command by starting with python3 but it did not work either.How can I fix it so that I can install new packages to my new version of Python? <code>  python3.8 -m pip install pylint AttributeError: module 'platform' has no attribute 'linux_distribution' sudo update-alternatives --config python3 sudo ln -sf /usr/bin/python3.5 /usr/bin/python3","How to fix ""module 'platform' has no attribute 'linux_distribution'"" when installing new packages with Python3.8?"
"How to fix ""module 'platform' has no attribute 'linux_distribution'"" when installing pylint with Python3.8?"," I had Python versions of 2.7 and 3.5. I wanted the install a newer version of Python which is python 3.8. I am using Ubuntu 16.04 and I can not just uninstall Python 3.5 due to the dependencies. So in order to run my scripts, I use python3.8 app.py. No problem so far. But when I want to install new packages via pip: It throws an error: So far, I tried: and chose python3.8 and run command by starting with python3 but no luck.Then: I also tried running the command by starting with python3 but it did not work either.How can I fix it so that I can install new packages to my new version of Python? <code>  python3.8 -m pip install pylint AttributeError: module 'platform' has no attribute 'linux_distribution' sudo update-alternatives --config python3 sudo ln -sf /usr/bin/python3.5 /usr/bin/python3","How to fix ""module 'platform' has no attribute 'linux_distribution'"" when installing new packages with Python3.8?"
Pandas : filter the rows based on a condition," How to filter the rows in a data frame based on another column value?I have a data frame which is, Based on the column values of ""min_subject"" and ""min_marks"", the row should be filtered. For index 0, the ""min_subjects"" is ""2"", at least 2 elements in ""marks"" column should be greater than 80 i.e., ""min_marks"" column then a new column named ""flag"" has to be added as 1For index 1, the ""min_subjects"" is ""1"", at least 1 element in ""marks"" column should be greater than 85 i.e., ""min_marks"" column then a new column named ""flag"" has to be added as 0 (i.e., flag=0 as the condition didnt satisfy here)The final outcome should be, Can anyone help me to achieve the same in the data frame? <code>  ip_df: class name marks min_marks min_subjects0 I tom [89,85,80,74] 80 21 II sam [65,72,43,40] 85 1 op_df: class name marks min_marks min_subjects flag0 I tom [89,85,80,74] 80 2 11 II sam [65,72,43,40] 85 1 0",Pandas : filter the rows based on a column containing lists
Understanding Develop an Multichannel CNN Model for Text Classification," I am going through this link to understand Multi-channel CNN Model for Text Classification.The code is based on this tutorial.I have understood most of the things, however I can't understand how Keras defines the output shapes of certain layers.Here is the code:define a model with three input channels for processing 4-grams, 6-grams, and 8-grams of movie review text. Running the code:Running the example first prints a summary of the prepared training dataset.Max document length: 1380Vocabulary size: 44277(1800, 1380) And My interpretation of the Layer and output shape are as follows:Please help me understand if its correct as I am lost in multi-dimension.input_1 (InputLayer) (None, 1380) : ---> 1380 is the total number of features ( that is 1380 input neurons) per data point. 1800 is the total number of documents or data points.embedding_1 (Embedding) (None, 1380, 100) 4427700 ----> Embedding layer is : 1380 as features(words) and each feature is a vector of dimension 100.How the number of parameters here is 4427700??conv1d_1 (Conv1D) (None, 1377, 32) 12832 ------> Conv1d is of kernel size=4. Is it 1*4 filter which is used 32 times. Then how the dimension became (None, 1377, 32) with 12832 parameters?max_pooling1d_1 (MaxPooling1D) (None, 688, 32) with MaxPooling1D(pool_size=2) how the dimension became (None, 688, 32)? flatten_1 (Flatten) (None, 22016) This is just multiplication of 688, 32?** Does every epoch trains 1800 data points at once?**Please let me know how output dimensions is calculated. Any reference or help would be appreciated. <code>  #Skipped keras imports# load a clean datasetdef load_dataset(filename): return load(open(filename, 'rb'))# fit a tokenizerdef create_tokenizer(lines): tokenizer = Tokenizer() tokenizer.fit_on_texts(lines) return tokenizer# calculate the maximum document lengthdef max_length(lines): return max([len(s.split()) for s in lines])# encode a list of linesdef encode_text(tokenizer, lines, length): # integer encode encoded = tokenizer.texts_to_sequences(lines) # pad encoded sequences padded = pad_sequences(encoded, maxlen=length, padding='post') return padded# define the modeldef define_model(length, vocab_size): # channel 1 inputs1 = Input(shape=(length,)) embedding1 = Embedding(vocab_size, 100)(inputs1) conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1) drop1 = Dropout(0.5)(conv1) pool1 = MaxPooling1D(pool_size=2)(drop1) flat1 = Flatten()(pool1) # channel 2 inputs2 = Input(shape=(length,)) embedding2 = Embedding(vocab_size, 100)(inputs2) conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2) drop2 = Dropout(0.5)(conv2) pool2 = MaxPooling1D(pool_size=2)(drop2) flat2 = Flatten()(pool2) # channel 3 inputs3 = Input(shape=(length,)) embedding3 = Embedding(vocab_size, 100)(inputs3) conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3) drop3 = Dropout(0.5)(conv3) pool3 = MaxPooling1D(pool_size=2)(drop3) flat3 = Flatten()(pool3) # merge merged = concatenate([flat1, flat2, flat3]) # interpretation dense1 = Dense(10, activation='relu')(merged) outputs = Dense(1, activation='sigmoid')(dense1) model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs) # compile model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # summarize print(model.summary()) plot_model(model, show_shapes=True, to_file='multichannel.png') return model# load training datasettrainLines, trainLabels = load_dataset('train.pkl')# create tokenizertokenizer = create_tokenizer(trainLines)# calculate max document lengthlength = max_length(trainLines)# calculate vocabulary sizevocab_size = len(tokenizer.word_index) + 1print('Max document length: %d' % length)print('Vocabulary size: %d' % vocab_size)# encode datatrainX = encode_text(tokenizer, trainLines, length)print(trainX.shape)# define modelmodel = define_model(length, vocab_size)# fit modelmodel.fit([trainX,trainX,trainX], array(trainLabels), epochs=10, batch_size=16)# save the modelmodel.save('model.h5') ____________________________________________________________________________________________________Layer (type) Output Shape Param # Connected to====================================================================================================input_1 (InputLayer) (None, 1380) 0____________________________________________________________________________________________________input_2 (InputLayer) (None, 1380) 0____________________________________________________________________________________________________input_3 (InputLayer) (None, 1380) 0____________________________________________________________________________________________________embedding_1 (Embedding) (None, 1380, 100) 4427700 input_1[0][0]____________________________________________________________________________________________________embedding_2 (Embedding) (None, 1380, 100) 4427700 input_2[0][0]____________________________________________________________________________________________________embedding_3 (Embedding) (None, 1380, 100) 4427700 input_3[0][0]____________________________________________________________________________________________________conv1d_1 (Conv1D) (None, 1377, 32) 12832 embedding_1[0][0]____________________________________________________________________________________________________conv1d_2 (Conv1D) (None, 1375, 32) 19232 embedding_2[0][0]____________________________________________________________________________________________________conv1d_3 (Conv1D) (None, 1373, 32) 25632 embedding_3[0][0]____________________________________________________________________________________________________dropout_1 (Dropout) (None, 1377, 32) 0 conv1d_1[0][0]____________________________________________________________________________________________________dropout_2 (Dropout) (None, 1375, 32) 0 conv1d_2[0][0]____________________________________________________________________________________________________dropout_3 (Dropout) (None, 1373, 32) 0 conv1d_3[0][0]____________________________________________________________________________________________________max_pooling1d_1 (MaxPooling1D) (None, 688, 32) 0 dropout_1[0][0]____________________________________________________________________________________________________max_pooling1d_2 (MaxPooling1D) (None, 687, 32) 0 dropout_2[0][0]____________________________________________________________________________________________________max_pooling1d_3 (MaxPooling1D) (None, 686, 32) 0 dropout_3[0][0]____________________________________________________________________________________________________flatten_1 (Flatten) (None, 22016) 0 max_pooling1d_1[0][0]____________________________________________________________________________________________________flatten_2 (Flatten) (None, 21984) 0 max_pooling1d_2[0][0]____________________________________________________________________________________________________flatten_3 (Flatten) (None, 21952) 0 max_pooling1d_3[0][0]____________________________________________________________________________________________________concatenate_1 (Concatenate) (None, 65952) 0 flatten_1[0][0] flatten_2[0][0] flatten_3[0][0]____________________________________________________________________________________________________dense_1 (Dense) (None, 10) 659530 concatenate_1[0][0]____________________________________________________________________________________________________dense_2 (Dense) (None, 1) 11 dense_1[0][0]====================================================================================================Total params: 14,000,337Trainable params: 14,000,337Non-trainable params: 0____________________________________________________________________________________________________ Epoch 6/101800/1800 [==============================] - 30s - loss: 9.9093e-04 - acc: 1.0000Epoch 7/101800/1800 [==============================] - 29s - loss: 5.1899e-04 - acc: 1.0000Epoch 8/101800/1800 [==============================] - 28s - loss: 3.7958e-04 - acc: 1.0000Epoch 9/101800/1800 [==============================] - 29s - loss: 3.0534e-04 - acc: 1.0000Epoch 10/101800/1800 [==============================] - 29s - loss: 2.6234e-04 - acc: 1.0000",Understanding shapes of Keras layers
How to add axis layouts into a subplot?," Given that i have the below code from this link: The problem in this code is, the xaxis and yaxis does not have any label. Beside this, the current code applies only one title to all the plots, however I want to apply different titles to each scatter plot.How can i do that? <code>  from plotly.subplots import make_subplotsimport plotly.graph_objects as gofig = make_subplots(rows=1, cols=2)fig.add_trace( go.Scatter(x=[1, 2, 3], y=[4, 5, 6]), row=1, col=1)fig.add_trace( go.Scatter(x=[20, 30, 40], y=[50, 60, 70]), row=1, col=2)fig.update_layout(height=600, width=800, title_text=""Subplots"")fig.show()",Plotly: How to apply different titles for each different subplots?
Plotly: How to add axis layouts into a subplot?," Given that i have the below code from this link: The problem in this code is, the xaxis and yaxis does not have any label. Beside this, the current code applies only one title to all the plots, however I want to apply different titles to each scatter plot.How can i do that? <code>  from plotly.subplots import make_subplotsimport plotly.graph_objects as gofig = make_subplots(rows=1, cols=2)fig.add_trace( go.Scatter(x=[1, 2, 3], y=[4, 5, 6]), row=1, col=1)fig.add_trace( go.Scatter(x=[20, 30, 40], y=[50, 60, 70]), row=1, col=2)fig.update_layout(height=600, width=800, title_text=""Subplots"")fig.show()",Plotly: How to apply different titles for each different subplots?
train the model in keras if the input data and output data are all complex number," I am trying to train a very simple model which only have one convolution layer. But the input(X), prediction output(y_pred) and true_output(y_true) are all complex number. When I call the function model.fit(X,y_true)There is the error TypeError: Gradients of complex tensors must set grad_ys (y.dtype = tf.complex64)Does that means I have to write the back-propagation by hand?What should I do to solve this problem? thanks <code>  def kernel_model(filters=1, kernel_size=3): input_layer = Input(shape=(250,1)) conv_layer = Conv1D(filters=filters,kernel_size=kernel_size,padding='same',use_bias = False)(input_layer) model = Model(inputs=input_layer,output=conv_layer) return model ",Can I train a Tensorflow keras model with complex input/output?
How to install opencv-python in pycharm," I'm having problem during the installation of opencv-python in pycharm.After opening pycharm I click on settings and then project interpreter, I click on '+' and search for the correct module, I started the installation but it fails with Additionally I tried installing it through console but I get the same error.I also have updated to the last pip version, how can I solve this? <code>  Could not find a version that satisfies the requirement opencv-python (from versions: )No matching distribution found for opencv-python",How to install opencv-python in python 3.8
What is an elegant way for calling robot framework tests with automatically generated arguments?," Problem summary:I am currently trying to migrate existing tests from pure python to robot framework in order to benefit from the nice reporting features. These system tests have to be re-run using multiple parameter sets consisting of many parameters. That's why I already have a python generator yielding dictionaries with all possible parameter configurations as well as methods generating readable descriptions for each parameter.I would like to achieve:A report where every set of parameters corresponds to one test case, like in data-driven RF styleReadable test cases, without the need to abuse the generators to generate ugly ""hard code""Data driven appproach:I used the generators I have to write a data driven test-file in the following format, which gives me pretty much exactly the output I would like to have. The problem I have with this approach is that my descriptions that I'd like to use as test case names are pretty long and there are far more than three parameters to be messed with, most of them having more than two states. That renders the .robot file I created that way unreadable. The one thing I dislike about the output is that I don't see the names of the parameters used for the test cases, so the test title really must carry all the information about all the parameters. Other than that I think if there is no better solution, this is what I'll go with despite the unreadable intermediate step. The code of the following keyword-driven approach is much more to my taste, but the output doesn't look nice. Using the following code, there is just one test-case represented in the report, which then calls one keyword multiple times. Even more disadvantages: The test setup and teardown are both only called once. Also, the describe parameters keyword logs the information about the test case, but it's not visible right away, but hidden in the keyword call. Using the dictionary as parameter seems pretty nice though because that way I get to see the dictionary keys next to the values in the report for each for loop iteration. I also tried using [Template] within a test case, but that solution just combines the bad parts of both approaches I'm showing here. One thing I haven't tried yet is giving the whole test case arguments and running it from python. I feel like there should be an elegant solution somehow using a test template in combination with a for loop and maybe embeded arguments, but I could not figure it out yet. Thanks for your help in advance! <code>  *** Settings ***Test Template Check Result With Args*** Keywords ***Check Result With Args [Arguments] ${par1} ... ${par2} ... ${par3} Set par par1 ${par1} Set par par2 ${par2} Set par par3 ${par3} Evaluation Check result*** Test Cases *** par1 par2 par3description000 0 0 0description001 0 0 1description010 0 1 0description011 0 1 1description100 1 0 0description101 1 0 1description110 1 1 0description111 1 1 1 *** Keywords ***Set Parameters And Check Result [Arguments] ${parameter dictionary} Describe Parameters ${parameter dictionary} Set Parameters ${parameter dictionary} Evaluation Check result*** Test Cases ***Check Result For All Possible Configurations ${all configs} = Yield All Possible Configurations :FOR ${configuration} IN @{all configs} \ Set Parameters And Check Result ${configuration}",Is there an elegant way for calling robot framework tests with automatically generated arguments?
How to split a compound word split by hyphen into two individual words in python," I have the following list I used the following to remove the hyphen The result I got The result that Ideally want is How can I accomplish this in the most pythonic (efficient way) <code>  list1= ['Dodd-Frank', 'insurance', 'regulation'] new1 =[j.replace('-', ' ') for j in list1] new1= ['Dodd Frank', 'insurance', 'regulation'] new1= ['Dodd', 'Frank', 'insurance', 'regulation']",How to split a compound word split by hyphen into two individual words
Saving PyTorch model with no access to your code," How can I save a PyTorch model without a need for the model class to be defined somewhere?Disclaimer:In Best way to save a trained model in PyTorch?, there are no solutions (or a working solution) for saving the model without access to the model class code. <code> ",Saving PyTorch model with no access to model class code
DFS on an adjacency matrix using Numpy or Scipy," I've the following adjacency matrix: Which can be drawn like that:My goal is to identify the connected graph ABC and DEFG. It's seems that Depth-First Search algorithm is what I need and that Scipy implemented it. So here is my code: But I don't get the result: what's that array([-9999, 0, 1, -9999, -9999, -9999, -9999]) ? Also, in the documentation, they talk about a sparse matrix not about an adjacency one. But an adjacency matrix seems to be a sparse matrix by definition so it's not clear for me.  <code>  array([[0, 1, 1, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 1], [0, 0, 0, 1, 0, 1, 0], [0, 0, 0, 0, 1, 0, 1], [0, 0, 0, 1, 0, 1, 0]]) from scipy.sparse import csr_matrixfrom scipy.sparse.csgraph import depth_first_orderimport numpy as nptest = np.asarray([ [0, 1, 1, 0, 0, 0, 0], [1, 0, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 1], [0, 0, 0, 1, 0, 1, 0], [0, 0, 0, 0, 1, 0, 1], [0, 0, 0, 1, 0, 1, 0]])graph = csr_matrix(test)result = depth_first_order(graph, 0) >>> result(array([0, 1, 2]), array([-9999, 0, 1, -9999, -9999, -9999, -9999]))",Connected components from an adjacency matrix using Numpy or Scipy
How to fit a line using RANSAC in cartesian cordinates," I am using a 2D Lidar and getting the data as angle and distance with respect to lidar Position. I have to create a floor plan using Lidar and the data is given bellow is represent a room. I want to use the RANSAC algorithm to find the wall of the room. If I could fit RANSAC then I believe somehow I could find the floor flan. I have written a code but it's not fitting into my wall. what modification should I do so that it will fit into my room wall? I have converted this data into cartesian and plotted this using Matplotlib.After I have done the RANSAC, it shows a blue line in the picture that is not detecting the wall properly. I should find out 4 walls. I do not know which modification I should do in my code. Here is the code: I will be so glad if you could suggest me to achieve the floor plane and surface area. If you have any questions, please drop it into the comment section. <code>  #(angle, distance)0,9420.62,34691.25,33502.5,34103.12,34043.75,34034.37,34645,34415.62,34456.25,34446.87,34557.5,34648.12,34648.75,34779.37,347010,350410.62,350511.25,350511.87,351612.5,352913.12,354113.75,354314.37,355215,355915.62,356516.25,357816.87,359117.5,360718.12,362418.75,363419.37,363020,365120.62,367321.25,367821.87,369722.5,371123.12,372623.75,374424.37,376525,378025.62,379626.25,380926.87,383027.5,386728.12,388128.75,385429.37,375130,368930.62,363731.25,358931.87,402937.5,348350,273453.75,168654.37,165655,163155.62,162156.25,160856.87,160057.5,159558.12,159858.75,159659.37,160460,161160.62,162261.25,164461.87,167365,221265.62,222166.25,103766.87,129967.5,208667.5,13083.12,200283.75,199584.37,199385.62,206186.25,204386.87,204687.5,204089.37,208290,271390.62,224591.25,204693.75,2092102.5,327109.37,1349110,4279110.62,2177111.25,2175111.87,2136113.12,2151113.75,2170114.37,2186123.12,2066123.75,2080124.37,2087125,2110125.62,1778126.25,1732126.87,428127.5,1650128.12,1093128.75,2206129.37,2219130,2243130.62,2276131.25,2317131.87,2319132.5,2305133.12,2276133.75,2253135,2224135.62,2202136.25,2181136.87,2156137.5,2131138.12,2108138.75,2081139.37,2068140,2046140.62,2028141.25,1982141.87,2001142.5,1985143.12,2030152.5,1727153.12,1728153.75,1722154.37,1711155,1700155.62,1691156.25,1683156.87,1672157.5,1666158.12,1655158.75,1645159.37,1637160,1633160.62,1622161.25,1621161.87,1611162.5,1602163.12,1597163.75,1592164.37,1583165,1579165.62,1578166.25,1571166.87,1564167.5,1558168.12,1552168.75,1551169.37,1550170,1545170.62,1543171.25,1540171.87,1537172.5,1529173.12,1527173.75,1527174.37,1524175,1522175.62,1518176.25,1519176.87,1517177.5,1513178.12,1510178.75,1514180,1514180.62,1511181.25,1519181.87,1513182.5,1514183.12,1513183.75,1513184.37,1514185,1518185.62,1517186.25,1519186.87,1517187.5,1526188.12,1522188.75,1526189.37,1527190,1530190.62,1536191.25,1536191.87,1541192.5,1544193.12,1549193.75,1553194.37,1555195,1553195.62,1563196.25,1569196.87,1570197.5,1581198.12,1583198.75,1591199.37,1597200,1601200.62,1606202.5,1615203.12,1626203.75,1625204.37,1644205,1646205.62,1658206.25,1663206.87,1674207.5,1685208.12,1703208.75,1703209.37,1717210,1732210.62,1743211.25,1750211.87,1766212.5,1776213.12,1791213.75,1808214.37,1814215,1835215.62,1844216.25,1854216.87,1870217.5,1892218.12,1909218.75,1918219.37,1934220,1952220.62,1972221.87,2023222.5,2039223.12,2059223.75,2082225,2101225.62,2122226.25,2148226.87,2173227.5,2190228.12,2214228.75,2241229.37,2275230,2295230.62,2324231.25,2348231.87,2160232.5,2416233.12,2445233.75,2479234.37,2520235.62,2607236.25,2649236.87,2156237.5,2726238.12,2768238.75,2806239.37,2865240,2912240.62,2962241.25,3026241.87,3078242.5,3147243.12,3210243.75,3276244.37,3315245,3307245.62,3288246.25,3267246.87,3253247.5,3153248.75,4678249.37,4563250,4560250.62,4504251.25,4523251.87,4478252.5,4452253.12,4465253.75,4434254.37,4421255,4391255.62,4371256.25,4361256.87,4356257.5,4348258.12,4326258.75,4326259.37,4331260,4341260.62,4270261.25,4263261.87,4281262.5,2992263.12,2984263.75,2976264.37,2983265,2971265.62,2963266.25,2963266.87,2967267.5,2968268.12,2951268.75,2951270,2959270.62,2953271.25,2500271.87,5514272.5,2839273.12,2706273.75,2721274.37,2693288.12,3010288.75,2999289.37,2998290,3020290.62,3036291.25,3083291.87,3169292.5,3170293.12,3196293.75,3212294.37,3230295,3234295.62,3262296.25,3273296.87,3298297.5,3318298.12,3333298.75,3356299.37,3374300,3394300.62,3417301.25,3427301.87,3453302.5,3474303.12,3490303.75,3516304.37,3552305,3571305.62,3581307.5,5224308.12,5271308.75,5316309.37,5411310,3843310.62,3892311.25,3907311.87,3922312.5,3985313.12,4016313.75,4058315,4081315.62,4143316.25,4190316.87,4230317.5,4291318.12,4353318.75,4406319.37,4460320,4512320.62,4563321.25,4507321.87,4473322.5,4426323.12,4398323.75,4371324.37,4321325,4274325.62,4256326.25,4215326.87,4194327.5,4148328.12,4104328.75,4077329.37,4051330,4024330.62,3995331.25,3971331.87,3932332.5,3909333.12,3898333.75,3884334.37,3858335,3840335.62,3818336.25,3791337.5,3765338.12,3747338.75,3720339.37,3715340,3689340.62,3687341.25,3635341.87,3632342.5,3624343.12,3613343.75,3613344.37,3594345,3595345.62,3560346.25,3570346.87,3543347.5,3555348.12,3527348.75,3512349.37,3512350,3521350.62,3486351.25,3496351.87,3477352.5,3487353.12,3461353.75,3460354.37,3458355,3453355.62,3460356.25,3448357.5,998358.12,3442 import numpy as npfrom matplotlib import pyplot as pltimport pandas as pdfrom sklearn import linear_model, datasetsimport math# scan data is stored in a txt file and getting data from that text filedf = pd.read_csv('scanData.txt',delimiter=',')angle = df.values[:,0]distance = df.values[:,1]cartesian = [(r*math.cos(phi*math.pi/180), r*math.sin(phi*math.pi/180)) for r, phi in zip(distance, angle)]x, y = map(list, zip(*cartesian))#print(x)# coverting this into 2d arrayx= np.array(x)y= np.array(y)x=x.reshape(-1, 1)y=y.reshape(-1, 1)lr = linear_model.LinearRegression()lr.fit(x, y)ransac = linear_model.RANSACRegressor(max_trials=1000,min_samples=300)ransac.fit(x, y)# Predict data of estimated modelsline_X = np.arange(x.min(), x.max())[:, np.newaxis]print(line_X)line_y = lr.predict(line_X)line_y_ransac = ransac.predict(line_X)print(line_y_ransac)plt.scatter(x,y, color='yellowgreen', marker='.', label='Inliers')plt.plot(line_X, line_y_ransac, color='cornflowerblue', linewidth=1, label='RANSAC regressor')plt.legend(loc='lower right')plt.xlabel(""Input"")plt.ylabel(""Response"")plt.show()",How to fit a line using RANSAC in Cartesian coordinates?
How can I orient a geopandas map east/west instead of north/south?," Starting with a shapefile I obtained from https://s3.amazonaws.com/nyc-tlc/misc/taxi_zones.zip, I'd like to plot the borough of Manhattan, and have outlines for each taxi-zone.This code rotates each individual taxi zone individually instead of all at once. [EDIT]I have further refined this to be able to rotate the image programmatically. However, if I add a legend, then that is also rotated, which is not desirable. Still looking for a better solution.Note, there is also this stackoverflow post (How can I rotate a matplotlib plot through 90 degrees?), however, the solutions that rotate the plot, and not the image, only work with 90 degree rotations. [EDIT2]A simple modification to the answer given below by @PMende solved it. The key was rotating all of the objects around a single point, instead of around their individual origins.[EDIT 3] If anyone is trying to do this, and needs to save the resulting rotated geoseries to a dataframe (say for instance, to color the geometry based on an additional column), you need to create a new one, simply writing does not work. I'm not sure why at the moment. However, the following code worked for me.  <code>  import geopandas as gpdfrom matplotlib import pyplot as pltfname = ""path_to_shapefile.shp""df = gpd.read_file(fname)df = df[df['borough'] == ""Manhattan""]glist = gpd.GeoSeries([g for g in df['geometry']])glist = glist.rotate(90)glist.plot() import geopandas as gpdfrom matplotlib import pyplot as pltimport numpy as npfrom scipy import ndimagefrom matplotlib import transformsfname = ""path_to_shapefile.shp""df = gpd.read_file(fname)df = df[df['borough'] == ""Manhattan""]df.plot()plt.axis(""off"")plt.savefig(""test.png"")img = plt.imread('test.png')rotated_img = ndimage.rotate(img, -65)plt.imshow(rotated_img, cmap=plt.cm.gray)plt.axis('off')plt.show() df = gpd.read_file(fname)df = df[df['borough'] == ""Manhattan""]glist = gpd.GeoSeries([g for g in df['geometry']])glist = glist.rotate(-65, origin=(0,0))glist.plot() df['geometry'] = glist new_dataframe = gpd.GeoDataFrame(glist)new_dataframe = new_dataframe.rename(columns={0:'geometry'}).set_geometry('geometry')new_dataframe.plot()",How can I rotate a matplotlib map?
"One liner to ""assign if not null"""," Is there a way to do an assignment only if the assigned value is not None, and otherwise do nothing?Of course we can do: but this will read the value twice. We can cache it to a local variable: but now we have made two statements for a simple thing.We could write a function: And then do x = return_if_not_none(get_value(), x). But surely there is already a Python idiom to accomplish this, without accessing x or get_value() twice and without creating variables?Put in another way, let's say =?? is a Python operator similar to the C# null coalesce operator. Unlike the C# ??=, our fictional operator checks if the right hand side is None: Such a =?? operator would do exactly what my question is asking. <code>  x = get_value() if get_value() is not None v = get_value()x = v if v is not None def return_if_not_none(v, default): if v is not None: return v else: return default x = 1y = 2z = Nonex =?? yprint(x) # Prints ""2""x =?? zprint(x) # Still prints ""2""","One liner to ""assign if not None"""
How to install Tensorflow with Python3.8," Whenever I try to install TensorFlow with pip on Python 3.8, I get the error that TensorFlow is not found. I have realized later on that it is not supported by Python 3.8.How can I install TensorFlow on Python 3.8? <code> ",How to install TensorFlow with Python 3.8
Open CV write video file not opening and corrupted," This is my code to save web_cam streaming. It is working but the problem with output video file. <code>  import numpy as npimport cv2cap = cv2.VideoCapture(0)# Define the codec and create VideoWriter object#fourcc = cv2.cv.CV_FOURCC(*'DIVX')#out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))out = cv2.VideoWriter('output.avi', -1, 20.0, (640,480))while(cap.isOpened()): ret, frame = cap.read() if ret==True: frame = cv2.flip(frame,0) # write the flipped frame out.write(frame) cv2.imshow('frame',frame) if cv2.waitKey(1) & 0xFF == ord('q'): break else: break# Release everything if job is finishedcap.release()out.release()cv2.destroyAllWindows()",VideoWriter outputs corrupted video file
x | y ^ z 3 different results," I had seen this test question on Pluralsight:Given these sets: What is the value of x | y ^ z?The expected answer is: Combines the sets (automatically discarding duplicates), and orders them from lowest to greatest.My questions are:What is this expression called?Why do I get 3 different results from 3 different Python versions?Result on Python 3.7.5 on Ubuntu 18.04: Result on Python 2.17.17rc1 on Ubuntu 18.04: Result on Python 3.7.2 on Windows 10: Here is a repl of the same code I'm using for this:https://repl.it/repls/RudeMoralWorkplaceI'd like to understand what happens behind the scenes with these expressions so I can debunk why I get different results. <code>  x = {'a', 'b', 'c', 'd'}y = {'c', 'e', 'f'}z = {'a', 'g', 'h', 'i'} {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i'} {'c', 'h', 'f', 'd', 'b', 'i', 'g', 'a', 'e'} set(['a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h']) {'a', 'd', 'h', 'f', 'b', 'g', 'e', 'c', 'i'}","What are these set operations, and why do they give different results?"
"What do these set operations do, and why do they give different results?"," I had seen this test question on Pluralsight:Given these sets: What is the value of x | y ^ z?The expected answer is: Combines the sets (automatically discarding duplicates), and orders them from lowest to greatest.My questions are:What is this expression called?Why do I get 3 different results from 3 different Python versions?Result on Python 3.7.5 on Ubuntu 18.04: Result on Python 2.17.17rc1 on Ubuntu 18.04: Result on Python 3.7.2 on Windows 10: Here is a repl of the same code I'm using for this:https://repl.it/repls/RudeMoralWorkplaceI'd like to understand what happens behind the scenes with these expressions so I can debunk why I get different results. <code>  x = {'a', 'b', 'c', 'd'}y = {'c', 'e', 'f'}z = {'a', 'g', 'h', 'i'} {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i'} {'c', 'h', 'f', 'd', 'b', 'i', 'g', 'a', 'e'} set(['a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h']) {'a', 'd', 'h', 'f', 'b', 'g', 'e', 'c', 'i'}","What are these set operations, and why do they give different results?"
"Does the ""with"" syntax in Python 3 support type hinting?"," Can you define the type hint for a variable defined with the with syntax? I would like to type hint the above to say that x is a str (as an example).The only work around that I've found is to use an intermediate variable, but this feels hacky. I can't find an example in the typing documentation. <code>  with example() as x: print(x) with example() as x: y: str = x print(y)","Does a ""with"" statement support type hinting?"
DRF throw django.core.exceptions.ImproperlyConfigured why override get_queryset," Django Rest Framework throw: django.core.exceptions.ImproperlyConfigured: Could not resolve URL for hyperlinked relationship using view name ""customuser-detail"". You may have failed to include the related model in your API, or incorrectly configured the lookup_field attribute on this field.when I try to override get_queryset . My User serializer: users/models.py: My views.py: My urls.py: I realized, that it can be fixed if I will override url for page which show user model details.But i think there are more correct answer. <code>  class UserSerializer(serializers.HyperlinkedModelSerializer): """""" Represent User Serializer class. """""" teacher_account = TeacherSerializer(required=False) student_account = StudentSerializer(required=False) account_type = serializers.IntegerField(required=True) class Meta: model = CustomUser fields = ['url', 'username', ""password"", 'email', 'first_name', 'last_name', ""account_type"", 'teacher_account', 'student_account'] email_validator = UniqueValidator(queryset=CustomUser.objects.all(), message=""A user with that email already exists."") extra_kwargs = { ""password"": {""write_only"": True}, ""email"": {""required"": True, ""validators"": [email_validator]} } @staticmethod def setup_eager_loading(queryset): queryset = queryset.select_related('teacher_account', 'student_account') return queryset class StudentAccount(models.Model): """""" Represent student's account model. """""" classes = models.ManyToManyField('classroom.Class', related_name=""students"")class TeacherAccount(models.Model): """""" Represent teacher's account. use get_subject_name from {root}/utils.py for get name of the subject. """""" subject_id = models.PositiveSmallIntegerField("""", choices=SUBJECTS_CHOICES, blank=False, default=0)class CustomUser(AbstractUser): """""" Represent Custom user model, inherited from AbstractUser account_type = 0(teacher) or 1(student) """""" student_account = models.OneToOneField(StudentAccount, on_delete=models.CASCADE, blank=True, null=True, related_name=""user"") teacher_account = models.OneToOneField(TeacherAccount, on_delete=models.CASCADE, blank=True, null=True, related_name=""user"") account_type = models.PositiveSmallIntegerField(default=1) first_name = models.CharField(""-"", max_length=30, blank=False) last_name = models.CharField(""-"", max_length=150, blank=False) class UserViewSet(viewsets.ModelViewSet): """""" API endpoint that allows users to be viewed or edited. """""" permission_classes = (permissions.AllowAny,) # queryset = CustomUser.objects.all() # queryset_raw = CustomUser.objects.all() # queryset = UserSerializer.setup_eager_loading(queryset_raw) model = CustomUser serializer_class = UserSerializer def get_queryset(self): queryset = CustomUser.objects.all() # queryset = self.get_serializer_class().setup_eager_loading(queryset) return queryset from django.urls import path, includefrom rest_framework import routersfrom knox import views as knox_viewsfrom .views import UserViewSet, ClassViewSet, LoginView, LessonViewSetrouter = routers.DefaultRouter()router.register('users', UserViewSet, basename=""CustomUser"")router.register('classes', ClassViewSet)router.register('lessons', LessonViewSet)urlpatterns = [ path('login/', LoginView.as_view(), name='knox-login'), path('logout/', knox_views.LogoutView.as_view(), name=""knox_logout""), path('logoutall/', knox_views.LogoutAllView.as_view(), name=""knox_logoutall""), path('', include(router.urls)), path('api-auth/', include('rest_framework.urls', namespace='rest_framework'))] path('api/users/<int:pk>/', UserViewSet.as_view({""get"": ""retrieve"", ""put"": ""update"", ""delete"": ""destroy""}), name=""customuser-detail"")",DRF throw django.core.exceptions.ImproperlyConfigured when override get_queryset
Find Question Text Blok In Image with Python Opencv," How can I select question blocks in a jpg file with questions in Python code? The codes below select texts. I want to select question blocks with their choices. Desired result:I drew the rectangles in the picture with the mouse. There are no rectangles in the original picture.The original file is here: <code>  import cv2image = cv2.imread('test2.jpg')gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)blur = cv2.GaussianBlur(gray, (9,9), 0)thresh = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,11,30)kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9,9))dilate = cv2.dilate(thresh, kernel, iterations=4)cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts\[0\] if len(cnts) == 2 else cnts\[1\]ROI_number = 0for c in cnts: area = cv2.contourArea(c) if area > 10000: x,y,w,h = cv2.boundingRect(c) cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 3) ROI = image\[y:y+h, x:x+w\] cv2.imwrite('ROI_{}.png'.format(ROI_number), ROI) ROI_number += 1cv2.imshow('thresh', thresh)cv2.imshow('dilate', dilate)cv2.imshow('image', image)cv2.waitKey()",Find question text block in image with Python Opencv
unable to execute the python script," I have multiple images diagram, all of which contains labels as alphanumeric characters instead of just the text label itself. I want my YOLO model to identify all the numbers & alphanumeric characters present in it.How can I train my YOLO model to do the same. The dataset can be found here. https://drive.google.com/open?id=1iEkGcreFaBIJqUdAADDXJbUrSj99bvoiFor example : see the bounding boxes. I want YOLO to detect wherever the text are present. However currently its not necessary to identify the text inside it.Also the same needs to be done for these type of imagesThe images can be downloaded hereThis is what I have tried using opencv but it does not work for all the images in the dataset. Is there any model or any opencv technique or some pre trained model that can do the same for me?I just need the bounding boxes around all the alphanumeric characters present in the images. After that I need to identify whats present in it. However the second part is not important currently. <code>  import cv2import numpy as npimport pytesseractpytesseract.pytesseract.tesseract_cmd = r""C:\Users\HPO2KOR\AppData\Local\Tesseract-OCR\tesseract.exe""image = cv2.imread(r'C:\Users\HPO2KOR\Desktop\Work\venv\Patent\PARTICULATE DETECTOR\PD4.png')gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]clean = thresh.copy()horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15,1))detect_horizontal = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)cnts = cv2.findContours(detect_horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts[0] if len(cnts) == 2 else cnts[1]for c in cnts: cv2.drawContours(clean, [c], -1, 0, 3)vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,30))detect_vertical = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, vertical_kernel, iterations=2)cnts = cv2.findContours(detect_vertical, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts[0] if len(cnts) == 2 else cnts[1]for c in cnts: cv2.drawContours(clean, [c], -1, 0, 3)cnts = cv2.findContours(clean, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts[0] if len(cnts) == 2 else cnts[1]for c in cnts: area = cv2.contourArea(c) if area < 100: cv2.drawContours(clean, [c], -1, 0, 3) elif area > 1000: cv2.drawContours(clean, [c], -1, 0, -1) peri = cv2.arcLength(c, True) approx = cv2.approxPolyDP(c, 0.02 * peri, True) x,y,w,h = cv2.boundingRect(c) if len(approx) == 4: cv2.rectangle(clean, (x, y), (x + w, y + h), 0, -1)open_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2,2))opening = cv2.morphologyEx(clean, cv2.MORPH_OPEN, open_kernel, iterations=2)close_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,2))close = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, close_kernel, iterations=4)cnts = cv2.findContours(close, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts[0] if len(cnts) == 2 else cnts[1]for c in cnts: x,y,w,h = cv2.boundingRect(c) area = cv2.contourArea(c) if area > 500: ROI = image[y:y+h, x:x+w] ROI = cv2.GaussianBlur(ROI, (3,3), 0) data = pytesseract.image_to_string(ROI, lang='eng',config='--psm 6') if data.isalnum(): cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 2) print(data)cv2.imwrite('image.png', image)cv2.imwrite('clean.png', clean)cv2.imwrite('close.png', close)cv2.imwrite('opening.png', opening)cv2.waitKey()",Using YOLO or other image recognition techniques to identify all alphanumeric text present in images
Using YOLO to identify all the text/alphanumerics present in images," I have multiple images diagram, all of which contains labels as alphanumeric characters instead of just the text label itself. I want my YOLO model to identify all the numbers & alphanumeric characters present in it.How can I train my YOLO model to do the same. The dataset can be found here. https://drive.google.com/open?id=1iEkGcreFaBIJqUdAADDXJbUrSj99bvoiFor example : see the bounding boxes. I want YOLO to detect wherever the text are present. However currently its not necessary to identify the text inside it.Also the same needs to be done for these type of imagesThe images can be downloaded hereThis is what I have tried using opencv but it does not work for all the images in the dataset. Is there any model or any opencv technique or some pre trained model that can do the same for me?I just need the bounding boxes around all the alphanumeric characters present in the images. After that I need to identify whats present in it. However the second part is not important currently. <code>  import cv2import numpy as npimport pytesseractpytesseract.pytesseract.tesseract_cmd = r""C:\Users\HPO2KOR\AppData\Local\Tesseract-OCR\tesseract.exe""image = cv2.imread(r'C:\Users\HPO2KOR\Desktop\Work\venv\Patent\PARTICULATE DETECTOR\PD4.png')gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]clean = thresh.copy()horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15,1))detect_horizontal = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)cnts = cv2.findContours(detect_horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts[0] if len(cnts) == 2 else cnts[1]for c in cnts: cv2.drawContours(clean, [c], -1, 0, 3)vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,30))detect_vertical = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, vertical_kernel, iterations=2)cnts = cv2.findContours(detect_vertical, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts[0] if len(cnts) == 2 else cnts[1]for c in cnts: cv2.drawContours(clean, [c], -1, 0, 3)cnts = cv2.findContours(clean, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts[0] if len(cnts) == 2 else cnts[1]for c in cnts: area = cv2.contourArea(c) if area < 100: cv2.drawContours(clean, [c], -1, 0, 3) elif area > 1000: cv2.drawContours(clean, [c], -1, 0, -1) peri = cv2.arcLength(c, True) approx = cv2.approxPolyDP(c, 0.02 * peri, True) x,y,w,h = cv2.boundingRect(c) if len(approx) == 4: cv2.rectangle(clean, (x, y), (x + w, y + h), 0, -1)open_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2,2))opening = cv2.morphologyEx(clean, cv2.MORPH_OPEN, open_kernel, iterations=2)close_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,2))close = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, close_kernel, iterations=4)cnts = cv2.findContours(close, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts[0] if len(cnts) == 2 else cnts[1]for c in cnts: x,y,w,h = cv2.boundingRect(c) area = cv2.contourArea(c) if area > 500: ROI = image[y:y+h, x:x+w] ROI = cv2.GaussianBlur(ROI, (3,3), 0) data = pytesseract.image_to_string(ROI, lang='eng',config='--psm 6') if data.isalnum(): cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 2) print(data)cv2.imwrite('image.png', image)cv2.imwrite('clean.png', clean)cv2.imwrite('close.png', close)cv2.imwrite('opening.png', opening)cv2.waitKey()",Using YOLO or other image recognition techniques to identify all alphanumeric text present in images
Using YOLO or other Image recognition techniques to identify all the text/alphanumerics present in images," I have multiple images diagram, all of which contains labels as alphanumeric characters instead of just the text label itself. I want my YOLO model to identify all the numbers & alphanumeric characters present in it.How can I train my YOLO model to do the same. The dataset can be found here. https://drive.google.com/open?id=1iEkGcreFaBIJqUdAADDXJbUrSj99bvoiFor example : see the bounding boxes. I want YOLO to detect wherever the text are present. However currently its not necessary to identify the text inside it.Also the same needs to be done for these type of imagesThe images can be downloaded hereThis is what I have tried using opencv but it does not work for all the images in the dataset. Is there any model or any opencv technique or some pre trained model that can do the same for me?I just need the bounding boxes around all the alphanumeric characters present in the images. After that I need to identify whats present in it. However the second part is not important currently. <code>  import cv2import numpy as npimport pytesseractpytesseract.pytesseract.tesseract_cmd = r""C:\Users\HPO2KOR\AppData\Local\Tesseract-OCR\tesseract.exe""image = cv2.imread(r'C:\Users\HPO2KOR\Desktop\Work\venv\Patent\PARTICULATE DETECTOR\PD4.png')gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]clean = thresh.copy()horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15,1))detect_horizontal = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)cnts = cv2.findContours(detect_horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts[0] if len(cnts) == 2 else cnts[1]for c in cnts: cv2.drawContours(clean, [c], -1, 0, 3)vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,30))detect_vertical = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, vertical_kernel, iterations=2)cnts = cv2.findContours(detect_vertical, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts[0] if len(cnts) == 2 else cnts[1]for c in cnts: cv2.drawContours(clean, [c], -1, 0, 3)cnts = cv2.findContours(clean, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts[0] if len(cnts) == 2 else cnts[1]for c in cnts: area = cv2.contourArea(c) if area < 100: cv2.drawContours(clean, [c], -1, 0, 3) elif area > 1000: cv2.drawContours(clean, [c], -1, 0, -1) peri = cv2.arcLength(c, True) approx = cv2.approxPolyDP(c, 0.02 * peri, True) x,y,w,h = cv2.boundingRect(c) if len(approx) == 4: cv2.rectangle(clean, (x, y), (x + w, y + h), 0, -1)open_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2,2))opening = cv2.morphologyEx(clean, cv2.MORPH_OPEN, open_kernel, iterations=2)close_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,2))close = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, close_kernel, iterations=4)cnts = cv2.findContours(close, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)cnts = cnts[0] if len(cnts) == 2 else cnts[1]for c in cnts: x,y,w,h = cv2.boundingRect(c) area = cv2.contourArea(c) if area > 500: ROI = image[y:y+h, x:x+w] ROI = cv2.GaussianBlur(ROI, (3,3), 0) data = pytesseract.image_to_string(ROI, lang='eng',config='--psm 6') if data.isalnum(): cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 2) print(data)cv2.imwrite('image.png', image)cv2.imwrite('clean.png', clean)cv2.imwrite('close.png', close)cv2.imwrite('opening.png', opening)cv2.waitKey()",Using YOLO or other image recognition techniques to identify all alphanumeric text present in images
Is np.ndarray.tobytes() a deterministic operation?," I encountered a strange behavior of np.ndarray.tobytes() that makes me doubt that it is working deterministically, at least for arrays of dtype=object. In the sample code, a list of mixed python objects ([1, [2]]) is first converted to a numpy array, and then transformed to a byte sequence using tobytes(). Why do the resulting byte-representations differ for repeated instantiations of the same data? The documentation just states that it converts an ndarray to raw python bytes, but it does not refer to any limitations. So far, I observed this just for dtype=object. Numeric arrays always yield the same byte sequence: Have I missed an elementar thing about python's/numpy's memory architecture? I tested with numpy version 1.17.2 on a Mac.Context: I encountered this problem when trying to compute a hash for arbitrary data structures. I hoped that I can rely on the basic serialization capabilities of tobytes(), but this appears to be a wrong premise. I know that pickle is the standard for serialization in python, but since I don't require portability and my data structures only contain numbers, I first sought help with numpy. <code>  import numpy as npprint(np.array([1,[2]]).dtype)#=> objectprint(np.array([1,[2]]).tobytes())#=> b'0h\xa3\t\x01\x00\x00\x00H{!-\x01\x00\x00\x00'print(np.array([1,[2]]).tobytes())# => b'0h\xa3\t\x01\x00\x00\x00\x88\x9d)-\x01\x00\x00\x00' np.random.seed(42); print(np.random.rand(3).tobytes())# b'\xecQ_\x1ew\xf8\xd7?T\xd6\xbbh@l\xee?Qg\x1e\x8f~l\xe7?'np.random.seed(42); print(np.random.rand(3).tobytes())# b'\xecQ_\x1ew\xf8\xd7?T\xd6\xbbh@l\xee?Qg\x1e\x8f~l\xe7?'","How does np.ndarray.tobytes() work for dtype ""object""?"
How does [*a] overallocate?," Apparently list(a) doesn't overallocate, [x for x in a] overallocates at some points, and [*a] overallocates all the time?Here are sizes n from 0 to 12 and the resulting sizes in bytes for the three methods: Computed like this, reproducable at repl.it, using Python 3.8: So: How does this work? How does [*a] overallocate? Actually, what mechanism does it use to create the result list from the given input? Does it use an iterator over a and use something like list.append? Where is the source code?(Colab with data and code that produced the images.)Zooming in to smaller n:Zooming out to larger n: <code>  0 56 56 561 64 88 882 72 88 963 80 88 1044 88 88 1125 96 120 1206 104 120 1287 112 120 1368 120 120 1529 128 184 18410 136 184 19211 144 184 20012 152 184 208 from sys import getsizeoffor n in range(13): a = [None] * n print(n, getsizeof(list(a)), getsizeof([x for x in a]), getsizeof([*a]))",What causes [*a] to overallocate?
weird MRO result when inheriting directly from NamedTuple," I am confused why FooBar.__mro__ doesn't show <class '__main__.Parent'> like the above two.I still don't know why after some digging into the CPython source code. <code>  from typing import NamedTuplefrom collections import namedtupleA = namedtuple('A', ['test'])class B(NamedTuple): test: strclass Parent: passclass Foo(Parent, A): passclass Bar(Parent, B): passclass FooBar(Parent, NamedTuple): passprint(Foo.__mro__)# prints (<class '__main__.Foo'>, <class '__main__.Parent'>, <class '__main__.A'>, <class 'tuple'>, <class 'object'>)print(Bar.__mro__)# prints (<class '__main__.Bar'>, <class '__main__.Parent'>, <class '__main__.B'>, <class 'tuple'>, <class 'object'>)print(FooBar.__mro__)# prints (<class '__main__.FooBar'>, <class 'tuple'>, <class 'object'>)# expecting: (<class '__main__.FooBar'>, <class '__main__.Parent'>, <class 'tuple'>, <class 'object'>) ",Weird MRO result when inheriting directly from typing.NamedTuple
row of actions for creating venv in python and clone a git repo," I'm relatively new in all that and I have a problem with the row of the actions. Say that you created a directory and you want a python virtual environment for some project and clone a few git repos (say, from GitHub). Then you cd in that directory and create a virtual environment using the venv module (for python3). To do so you run the following command, which will create in your directory a virtual environment called my_env. To activate this environment you run the following. If in addition inside that directory you have a requirements.txt file you can run, to install your various dependencies and packages with pip3. Now this is where I'm getting confused. If you want to clone git repos where exactly you do that? In the same directory you just run git clone and creates the git repos or you need to cd in another folder. In order to let python venv pick up the cloned repos is the above enough, or venv must be installed after you have cloned the repos in your directory? <code>  python3 -m venv my_venv source ./my_env/bin/activate pip3 install -r ./requirements.txt",Actions for creating venv in python and clone a git repo
How to convert a rgb image into a cmyk with python?," I want to convert a rgb image into cmyk. This is my code the first problem is, when I divide each pixel by 255 the value closes to zero so the result image is approximately black! The second problem is, I don't know how to convert the 1 channel resulted image to 4 channel.Of course I'm not sure the made CMYK in the following code is correct.Thank you for your attention <code>  import cv2import numpy as npimport timeimg = cv2.imread('image/dr_trump.jpg')B = img[:, :, 0]G = img[:, :, 1]R = img[:, :, 2]B_ = np.copy(B) G_ = np.copy(G)R_ = np.copy(R)K = np.zeros_like(B) C = np.zeros_like(B) M = np.zeros_like(B) Y = np.zeros_like(B) ts = time.time()for i in range(B.shape[0]): for j in range(B.shape[1]): B_[i, j] = B[i, j]/255 G_[i, j] = G[i, j]/255 R_[i, j] = R[i, j]/255 K[i, j] = 1 - max(B_[i, j], G_[i, j], R_[i, j]) if (B_[i, j] == 0) and (G_[i, j] == 0) and (R_[i, j] == 0): # black C[i, j] = 0 M[i, j] = 0 Y[i, j] = 0 else: C[i, j] = (1 - R_[i, j] - K[i, j])/float((1 - K[i, j])) M[i, j] = (1 - G_[i, j] - K[i, j])/float((1 - K[i, j])) Y[i, j] = (1 - B_[i, j] - K[i, j])/float((1 - K[i, j]))CMYK = C + M + Y + K t = (time.time() -ts)print(""Loop: {:} ms"".format(t*1000))cv2.imshow('CMYK by loop',CMYK)cv2.waitKey(0)cv2.destroyAllWindows()",How to convert a rgb image into a cmyk?
Python round to higher potency of 10," How would I manage to perform math.ceil such that a number is assigned to the next highest power of 10? My current solution is a dictionary that checks the range of the input number, but it's hardcoded and I would prefer a one-liner solution. Maybe I am missing a simple mathematical trick or a corresponding numpy function here? <code>  # 0.04 -> 0.1# 0.7 -> 1# 1.1 -> 10 # 90 -> 100 # ...",Python round to next highest power of 10
Python nested loops to just one loop making," I have an input abcde. I'm trying to output something like this: I can't make a code which is without nested loops. My question is what is the solution of this problem with O(n) time complexity? My code is given below: <code>  aababcabcdabcdebbcbcdbcdeccdcdeddee s = ""abcde"" for i in range(len(s)): for x in range(i, len(s) + 1): a = s[i:x] if a != """": print(a)",Is there a way to print all substrings of a string in O(n) time?
More efficient way to print all substrings of a string?," I have an input abcde. I'm trying to output something like this: I can't make a code which is without nested loops. My question is what is the solution of this problem with O(n) time complexity? My code is given below: <code>  aababcabcdabcdebbcbcdbcdeccdcdeddee s = ""abcde"" for i in range(len(s)): for x in range(i, len(s) + 1): a = s[i:x] if a != """": print(a)",Is there a way to print all substrings of a string in O(n) time?
Is there a way to print all substrings of s string in O(n) time?," I have an input abcde. I'm trying to output something like this: I can't make a code which is without nested loops. My question is what is the solution of this problem with O(n) time complexity? My code is given below: <code>  aababcabcdabcdebbcbcdbcdeccdcdeddee s = ""abcde"" for i in range(len(s)): for x in range(i, len(s) + 1): a = s[i:x] if a != """": print(a)",Is there a way to print all substrings of a string in O(n) time?
I don't understand Python for-loop flow," Trying to understand the Python for-loop, I thought this would give the result {1} for one iteration, or just get stuck in an infinite loop, depending on if it does the iteration like in C or other languages. But actually it did neither. Why does it do 16 iterations? Where does the result {16} come from?This was using Python 3.8.2. On pypy it makes the expected result {1}. <code>  >>> s = {0}>>> for i in s:... s.add(i + 1)... s.remove(i)...>>> print(s){16}",Why do I get this many iterations when adding to and removing from a set while iterating over it?
Why do I get this many iterations when adding to and removing from a set I'm iterating over?," Trying to understand the Python for-loop, I thought this would give the result {1} for one iteration, or just get stuck in an infinite loop, depending on if it does the iteration like in C or other languages. But actually it did neither. Why does it do 16 iterations? Where does the result {16} come from?This was using Python 3.8.2. On pypy it makes the expected result {1}. <code>  >>> s = {0}>>> for i in s:... s.add(i + 1)... s.remove(i)...>>> print(s){16}",Why do I get this many iterations when adding to and removing from a set while iterating over it?
How to check if connection is established in Kafka-Python," I'm trying to consume messages from a Kafka topic. I'm using a wrapper around confluent_kafka consumer. I need to check if connection is established before I start consuming messages.I read that the consumer is lazy, so I need to perform some action for the connection to get established. But I want to check the connection establishment without doing a consume or poll operation.Also, I tried giving some bad configurations to see what the response on a poll would be. The response I got was: So, how do I decide if the connection parameters are faulty, the connection is broken, or there actually are no messages in the topic? <code>  b'Broker: No more messages'",How to programmatically check if Kafka Broker is up and running in Python
Reproduce Matlab's imgaborfilt in Python," I'm trying to reproduce the behaviour of the following MATLAB code in Python: Unfortunately, I don't have a license and cannot run the code. Also, the official Matlab documentation of imgaborfilt does not specify precisely what the functions do.For lack of an obvious alternative, I'm trying to use OpenCV in Python (open to other suggestions). I have no experience working with OpenCV. I'm trying to use cv2.getGaborKernel and cv2.filter2D. I could not find detailed documentation of the behaviour of these functions, either. Afaik there is no official documentation of the Python wrapper for OpenCV. The docstrings of the functions provide some information but it is incomplete and imprecise. I found this question, where OpenCV is used in C++ to solve the problem. I assume the functions work in a very similar way (also note the official C++ documentation). However, they have a number of additional parameters. How can I find out what the matlab functions really do to reproduce the behaviour? EDIT:After receiving the great answer by Cris Luengo, I used it to make two functions, using OpenCV and scikit-image respectively, to (hopefully) reproduce MATLAB imgaborfit function behaviour. I include them here. Note that the scikit implementation is a lot slower than OpenCV.I still have further questions about these functions: To what precision dothe results of the OpenCV solution and the MATLAB solution agree? For people not wanting to use OpenCV, I also include a scikit-image solutionhere. Ifound parameters, such that the magnitudes are almost equal. However, it seems the phase of the scikit-image solution is different from the OpenCV solution. Why is this? Code to test above functions: <code>  % Matlab codewavelength = 10orientation = 45image = imread('filename.tif') % grayscale image[mag,phase] = imgaborfilt(image, wavelength, orientation)gabor_im = mag .* sin(phase) # python 3.6import numpy as npimport cv2wavelength = 10orientation = 45shape = (500, 400) # arbitrary values to get running example code...sigma = 100 # what to put for Matlab behaviour?gamma = 1 # what to put for Matlab behaviour?gabor_filter = cv2.getGaborKernel(shape, sigma, orientation, wavelength, gamma)print(gabor_filter.shape) # =(401, 501). Why flipped?image = np.random.random(shape) # create some random data.out_buffer = np.zeros(shape)destination_depth = -1 # like dtype for filter2D. Apparantly, -1=""same as input"".thing = cv2.filter2D(image, destination_depth, gabor_filter, out_buffer)print(out_buffer.shape, out_buffer.dtype, out_buffer.max()) # =(500, 400) float64 65.2..print(thing.shape, thing.dtype, thing.max()) # =(500, 400) float64 65.2.. import numpy as npimport mathimport cv2def gaborfilt_OpenCV_likeMATLAB(image, wavelength, orientation, SpatialFrequencyBandwidth=1, SpatialAspectRatio=0.5): """"""Reproduces (to what accuracy in what MATLAB version??? todo TEST THIS!) the behaviour of MATLAB imgaborfilt function using OpenCV."""""" orientation = -orientation / 180 * math.pi # for OpenCV need radian, and runs in opposite direction sigma = 0.5 * wavelength * SpatialFrequencyBandwidth gamma = SpatialAspectRatio shape = 1 + 2 * math.ceil(4 * sigma) # smaller cutoff is possible for speed shape = (shape, shape) gabor_filter_real = cv2.getGaborKernel(shape, sigma, orientation, wavelength, gamma, psi=0) gabor_filter_imag = cv2.getGaborKernel(shape, sigma, orientation, wavelength, gamma, psi=math.pi / 2) filtered_image = cv2.filter2D(image, -1, gabor_filter_real) + 1j * cv2.filter2D(image, -1, gabor_filter_imag) mag = np.abs(filtered_image) phase = np.angle(filtered_image) return mag, phase import numpy as npimport mathfrom skimage.filters import gabordef gaborfilt_skimage_likeMATLAB(image, wavelength, orientation, SpatialFrequencyBandwidth=1, SpatialAspectRatio=0.5): """"""TODO (does not quite) reproduce the behaviour of MATLAB imgaborfilt function using skimage."""""" sigma = 0.5 * wavelength * SpatialFrequencyBandwidth filtered_image_re, filtered_image_im = gabor( image, frequency=1 / wavelength, theta=-orientation / 180 * math.pi, sigma_x=sigma, sigma_y=sigma/SpatialAspectRatio, n_stds=5, ) full_image = filtered_image_re + 1j * filtered_image_im mag = np.abs(full_image) phase = np.angle(full_image) return mag, phase from matplotlib import pyplot as pltimport numpy as npdef show(im, title=""""): plt.figure() plt.imshow(im) plt.title(f""{title}: dtype={im.dtype}, shape={im.shape},\n max={im.max():.3e}, min= {im.min():.3e}"") plt.colorbar()image = np.zeros((400, 400))image[200, 200] = 1 # a delta impulse image to visualize the filtering kernelwavelength = 10orientation = 33 # in degrees (for MATLAB)mag_cv, phase_cv = gaborfilt_OpenCV_likeMATLAB(image, wavelength, orientation)show(mag_cv, ""mag"") # normalized by maximum, non-zero noise even outside filter window regionshow(phase_cv, ""phase"") # all over the placemag_sk, phase_sk = gaborfilt_skimage_likeMATLAB(image, wavelength, orientation)show(mag_sk, ""mag skimage"") # small values, zero outside filter regionshow(phase_sk, ""phase skimage"") # and hence non-zero only inside filter window regionshow(mag_cv - mag_sk/mag_sk.max(), ""cv - normalized(sk)"") # approximately zero-image.show(phase_sk - phase_cv, ""phase_sk - phase_cv"") # phases do not agree at all! Not even in the window region!plt.show()",Reproduce MATLAB's imgaborfilt in Python
Recommended use of `with open()` in python3," Historically I have always used the following for reading files in python: Is this still the recommend approach? Are there any drawbacks to using the following: Most of the references I found are using the with keyword for opening files for the convenience of not having to explicitly close the file. Is this applicable for the iterator approach here?with open() docs <code>  with open(""file"", ""r"") as f: for line in f: # do thing to line from pathlib import Pathpath = Path(""file"")for line in path.open(): # do thing to line",Recommended way of closing files using pathlib module?
Line plots with shaded standard deviation using plotly, How can I use Plotly to produce a line plot with a shaded standard deviation? I am trying to achieve something similar to seaborn.tsplot. Any help is appreciated. <code> ,Plotly: How to make a figure with multiple lines and shaded area for standard deviations?
bottlepy : how to set a cookie inside decorator," There are some operations that needs to be done before running some routes. For example :check if we recognise the user,check the language,check the location,set variables in the navbar (here after named header) of the htmland so on, then make decisions based on the outcome and lastly run the requested route.I find it hard to use the respose.set_cookie(""cookie_name"", actual_cookie) inside a decorator. It seems flask has a ""make_response"" object that works well (see here on stack overflow issue 34543157 : Python Flask - Setting a cookie using a decorator), but I find it difficult to reproduce the same thing with bottle.any how here is my attempt that is not working : and the main file where the routing goes to I also tried set the cookie like that : But got an error :)Here is the response doc : Response documentationDo you have any clues ?Thanks <code>  #python3#/decorator_cookie.pyfrom bottle import request, response, redirectfrom other_module import datamodel, db_pointer, secret_value #custom_moduleimport jsoncookie_value = Nonesurfer_email_exist_in_db = None header = None db_pointer = instanciation_of_a_db_connexion_to_tablessurfer = db_pointer.get(request.get_cookie('surfer')) if db_pointer.get(request.get_cookie('surfer')) != None else ""empty""def set_header(func): def header_manager(): global cookie_value, surfer_email_exist_in_db, header, db_pointer cookie_value = True #for stack-overflow question convenience surfer_email_exist_in_db = True #for stack-overflow question convenience if not all([cookie_value, surfer_email_exist_in_db]): redirect('/login') else: header = json.dumps(db_pointer.get('header_fr')) response.set_cookie(""header"", header, secret = secret_value, path = ""/"", httponly = True) return func() return header_manager #python3#/main.pyfrom bottle import route, requestfrom decorator_cookie import set_headerfrom other_module secret_value@route('/lets_try')@set_headerdef lets_try(): header = request.get_cookie('header', secret = secret_value) print(header) #here I get None return template('lets_try.tpl', headers = header) make_response = response(func).set_cookie(""header"", header, secret = secret_value, path = ""/"", httponly = True)",bottle : how to set a cookie inside a python decorator?
Computing least squares in python," I am using the below code to display x and y values on plotly dash. But then i want to be be able to add a another text field below the ""value"" textfield.The text field would be called ""Category"" so that if the y value displayed is:5k then category = not pricey or if value is 20k then category = average price and if value is30k then category = too pricey.How would i implement this? Here's the running code that displays the values hovered on <code>  import jsonfrom textwrap import dedent as dimport pandas as pdimport plotly.graph_objects as goimport numpy as npimport dashimport dash_core_components as dccimport dash_html_components as htmlimport plotly.express as pxfrom dash.dependencies import Input, Outputfrom jupyter_dash import JupyterDash# app infoapp = JupyterDash(__name__)styles = { 'pre': { 'border': 'thin lightgrey solid', 'overflowX': 'scroll' }}# data and basic figurex = np.arange(20)+10fig = go.Figure(data=go.Scatter(x=x, y=x**2, mode = 'lines+markers'))fig.add_traces(go.Scatter(x=x, y=x**2.2, mode = 'lines+markers'))app.layout = html.Div([ dcc.Graph( id='basic-interactions', figure=fig, ), html.Div(className='row', children=[ html.Div([ dcc.Markdown(d("""""" Click on points in the graph. """""")), html.Pre(id='hover-data', style=styles['pre']), ], className='three columns'), ])])@app.callback( Output('hover-data', 'children'), [Input('basic-interactions', 'hoverData')])def display_hover_data(hoverData): return json.dumps(hoverData, indent=2)app.run_server(mode='external', port = 8070, dev_tools_ui=True, dev_tools_hot_reload =True, threaded=True)",Plotly: How to edit text output based on value retrieved by hovering?
how do i edit text based on plotly value," I am using the below code to display x and y values on plotly dash. But then i want to be be able to add a another text field below the ""value"" textfield.The text field would be called ""Category"" so that if the y value displayed is:5k then category = not pricey or if value is 20k then category = average price and if value is30k then category = too pricey.How would i implement this? Here's the running code that displays the values hovered on <code>  import jsonfrom textwrap import dedent as dimport pandas as pdimport plotly.graph_objects as goimport numpy as npimport dashimport dash_core_components as dccimport dash_html_components as htmlimport plotly.express as pxfrom dash.dependencies import Input, Outputfrom jupyter_dash import JupyterDash# app infoapp = JupyterDash(__name__)styles = { 'pre': { 'border': 'thin lightgrey solid', 'overflowX': 'scroll' }}# data and basic figurex = np.arange(20)+10fig = go.Figure(data=go.Scatter(x=x, y=x**2, mode = 'lines+markers'))fig.add_traces(go.Scatter(x=x, y=x**2.2, mode = 'lines+markers'))app.layout = html.Div([ dcc.Graph( id='basic-interactions', figure=fig, ), html.Div(className='row', children=[ html.Div([ dcc.Markdown(d("""""" Click on points in the graph. """""")), html.Pre(id='hover-data', style=styles['pre']), ], className='three columns'), ])])@app.callback( Output('hover-data', 'children'), [Input('basic-interactions', 'hoverData')])def display_hover_data(hoverData): return json.dumps(hoverData, indent=2)app.run_server(mode='external', port = 8070, dev_tools_ui=True, dev_tools_hot_reload =True, threaded=True)",Plotly: How to edit text output based on value retrieved by hovering?
PyGame substantially slower on macOS than on Ubuntu or Raspbian," I use PyGame for creating games, but I noticed that the programs ran a lot slower on macOS than on my Raspberry Pi. My original solution was to install Ubuntu alongside macOS on my computer, and that worked. However, I would rather only have one operating system on my computer. Does anyone know why PyGame is so much slower on my mac when running macOS?If it would help, I can send code. However, I have multiple PyGame programs and they all do the exact same thing, so I figured that it was most likely not the fault of the code, but I could be wrong.Any help is appreciated, thanks.P.S. When I say slower, I mean that it is running at about 30% of the speed on macOS than it would on Ubuntu. <code> ",PyGame slower on macOS than on Ubuntu or Raspbian
Pairwise Distance (Chunking?)," Problem:I have a vector that is approximately [350000, 1] and I wish to calculate the pair wise distance. This results in a [350000, 350000] matrix of integer datatype that does not fit into RAM. I eventually want to end up with a boolean (which fits into RAM) so I am currently doing this one element at a time but this is not very time efficient.Edit: Standard sklearn and scipy functions do not work because of the size of the data -- but if I can chunk it somehow to use hard disk then I should be able to use these.Problem Visualised:[a_1, a_2, a_3]^t -> [[a_1 - a_1, a_1 - a_2, a_1 - a_3], [a_2 - a_1, a_2 - a_2, a_2 - a_3], [a_3 - a_1, a_3 - a_2, a_3 - a_3]]Note that only the upper triangle needs to be calculated as it is symmetric when taking the abs value.Vectorised Code that Needs Chunking or Alternative Solution:I have found a way to compute the distance (subtraction) between all points that works on a small matrix using broadcasting but need a way to be able to do this on larger matrices without hitting RAM limitations.Or maybe a better way to the MWE below that is quicker could be suggested? Other Attempts:I have tried using dask and memmap but still get memory errors so must be doing something wrong. I have also tried memmap and manually chunking the data but do not obtain a full set of results so any help would be most appreciated.MWE of Current Method: Reason:I have time, x_coordinate, y_coordinate vectors for approx 350000 points. I want to calculate the distance between all time points (simple subtraction) and the Euclidean distance between each (x,y) point. I then want to be able to identify all point pairs that are within a time and distance occurrence threshold of each other producing a boolean. <code>  distMatrix = np.absolute((points[np.newaxis, :, :] - points[:, np.newaxis, :])[:, :, 0]) ## Data ###Note that the datatype and code may not match up exactly as just creating to demonstrate. Essentially want to take first column and create distance matrix with itself through subtracting, and then take 2nd and 3rd column and create euclidean distance matrix.data = np.random.randint(1, 5, size=[350001,3])minTime = 3maxTime = 4minDist = 1maxDist = 2### CODE ###n = len(data)for i in trange(n): for j in range(i+1, n): #Within time threshold? if minTime <= (data[j][idxT] - data[i][idxT]) <= maxTime: #Within distance threshold? xD = math.pow(data[j][idxX] - data[i][idxX], 2) yD = math.pow(data[j][idxY] - data[i][idxY], 2) d = math.sqrt(xD + yD) #If within threshold then if minDist <= d <= maxDist: #DO SOMETHING",Pairwise Distance with Large NumPy Arrays (Chunking?)
Problem With Colliding My Mouse On A Symbol And Displaying Text Image On My Screen How Do I Fix This? Pygame," I am having an issue where my bullets dont look like they are coming out of my gun they look like they are coming out of the players body VIDEO as you can see in the video it shoots somewhere else or its the gun its the same thing for the left side it shoots good going up but it shoots bad going down VIDEOI tried angeling my gun to 120 but what happens is everything good works for the right side not for the left side VIDEO as you can see it just glitchesmy projectile class how my projectiles append my full gun class I think what I am trying to say is how could I make my gun rotate at exactly at my mouse poisition without any problemsmy full code script <code>  class projectile(object): def __init__(self, x, y, dirx, diry, color): self.x = x self.y = y self.dirx = dirx self.diry = diry self.slash = pygame.image.load(""round.png"") self.slash = pygame.transform.scale(self.slash,(self.slash.get_width()//2,self.slash.get_height()//2)) self.rect = self.slash.get_rect() self.rect.topleft = ( self.x, self.y ) self.speed = 18 self.color = color self.hitbox = (self.x + -18, self.y, 46,60) if event.type == pygame.MOUSEBUTTONDOWN: # this is for the bullets if len(bullets) < 3: if box1.health > 25: mousex, mousey = pygame.mouse.get_pos() playerman.isJump = True start_x, start_y = playerman.x - 30, playerman.y - 65 mouse_x, mouse_y = event.pos dir_x, dir_y = mouse_x - start_x, mouse_y - start_y distance = math.sqrt(dir_x**2 + dir_y**2) if distance > 0: new_bullet = projectile(start_x, start_y, dir_x/distance, dir_y/distance, (0,0,0)) bullets.append(new_bullet) # this is displaying the bullets for the player for bullet in bullets[:]: bullet.move() if bullet.x < 0 or bullet.x > 900 or bullet.y < 0 or bullet.y > 900: bullets.pop(bullets.index(bullet)) def draw(self,drawX,drawY): self.rect.topleft = (drawX,drawY) # the guns hitbox # rotatiing the gun dx = self.look_at_pos[0] - self.rect.centerx dy = self.look_at_pos[1] - self.rect.centery angle = (190/math.pi) * math.atan2(-dy, dx) gun_size = self.image.get_size() pivot = (8, gun_size[1]//2) blitRotate(window, self.image, self.rect.center, pivot, angle) if((angle > 90 or angle < -90) and self.gunDirection != ""left""): self.gunDirection = ""left"" self.image = pygame.transform.flip(self.image, False, True) if((angle < 90 and angle > -90) and self.gunDirection != ""right""): self.gunDirection = ""right"" self.image = pygame.transform.flip(self.image, False, True) class handgun(): def __init__(self,x,y,height,width,color): self.x = x self.y = y self.height = height self.width = width self.color = color self.rect = pygame.Rect(x,y,height,width) # LOL THESE IS THE HAND self.shootsright = pygame.image.load(""hands.png"") self.image = self.shootsright self.rect = self.image.get_rect(center = (self.x, self.y)) self.look_at_pos = (self.x, self.y) self.isLookingAtPlayer = False self.look_at_pos = (x,y) self.hitbox = (self.x + -18, self.y, 46,60) self.gunDirection = ""right"" def draw(self,drawX,drawY): self.rect.topleft = (drawX,drawY) # the guns hitbox # rotatiing the gun dx = self.look_at_pos[0] - self.rect.centerx dy = self.look_at_pos[1] - self.rect.centery angle = (120/math.pi) * math.atan2(-dy, dx) gun_size = self.image.get_size() pivot = (8, gun_size[1]//2) blitRotate(window, self.image, self.rect.center, pivot, angle) if((angle > 90 or angle < -90) and self.gunDirection != ""left""): self.gunDirection = ""left"" self.image = pygame.transform.flip(self.image, False, True) if((angle < 90 and angle > -90) and self.gunDirection != ""right""): self.gunDirection = ""right"" self.image = pygame.transform.flip(self.image, False, True) def lookAt( self, coordinate ): self.look_at_pos = coordinate white = (255,255,255)handgun1 = handgun(300,300,10,10,white)how my images are blitted ```def blitRotate(surf, image, pos, originPos, angle): # calcaulate the axis aligned bounding box of the rotated image w, h = image.get_size() sin_a, cos_a = math.sin(math.radians(angle)), math.cos(math.radians(angle)) min_x, min_y = min([0, sin_a*h, cos_a*w, sin_a*h + cos_a*w]), max([0, sin_a*w, -cos_a*h, sin_a*w - cos_a*h]) # calculate the translation of the pivot pivot = pygame.math.Vector2(originPos[0], -originPos[1]) pivot_rotate = pivot.rotate(angle) pivot_move = pivot_rotate - pivot # calculate the upper left origin of the rotated image origin = (pos[0] - originPos[0] + min_x - pivot_move[0], pos[1] - originPos[1] - min_y + pivot_move[1]) # get a rotated image rotated_image = pygame.transform.rotate(image, angle) # rotate and blit the image surf.blit(rotated_image, origin) ",How Can I Make My Bullets Look LIke They Are Comming Out Of My Guns Tip?
Problem With Colliding My Mouse On A Symbol And Displaying Text Image On My Screen How Do I Fix This?," I am having an issue where my bullets dont look like they are coming out of my gun they look like they are coming out of the players body VIDEO as you can see in the video it shoots somewhere else or its the gun its the same thing for the left side it shoots good going up but it shoots bad going down VIDEOI tried angeling my gun to 120 but what happens is everything good works for the right side not for the left side VIDEO as you can see it just glitchesmy projectile class how my projectiles append my full gun class I think what I am trying to say is how could I make my gun rotate at exactly at my mouse poisition without any problemsmy full code script <code>  class projectile(object): def __init__(self, x, y, dirx, diry, color): self.x = x self.y = y self.dirx = dirx self.diry = diry self.slash = pygame.image.load(""round.png"") self.slash = pygame.transform.scale(self.slash,(self.slash.get_width()//2,self.slash.get_height()//2)) self.rect = self.slash.get_rect() self.rect.topleft = ( self.x, self.y ) self.speed = 18 self.color = color self.hitbox = (self.x + -18, self.y, 46,60) if event.type == pygame.MOUSEBUTTONDOWN: # this is for the bullets if len(bullets) < 3: if box1.health > 25: mousex, mousey = pygame.mouse.get_pos() playerman.isJump = True start_x, start_y = playerman.x - 30, playerman.y - 65 mouse_x, mouse_y = event.pos dir_x, dir_y = mouse_x - start_x, mouse_y - start_y distance = math.sqrt(dir_x**2 + dir_y**2) if distance > 0: new_bullet = projectile(start_x, start_y, dir_x/distance, dir_y/distance, (0,0,0)) bullets.append(new_bullet) # this is displaying the bullets for the player for bullet in bullets[:]: bullet.move() if bullet.x < 0 or bullet.x > 900 or bullet.y < 0 or bullet.y > 900: bullets.pop(bullets.index(bullet)) def draw(self,drawX,drawY): self.rect.topleft = (drawX,drawY) # the guns hitbox # rotatiing the gun dx = self.look_at_pos[0] - self.rect.centerx dy = self.look_at_pos[1] - self.rect.centery angle = (190/math.pi) * math.atan2(-dy, dx) gun_size = self.image.get_size() pivot = (8, gun_size[1]//2) blitRotate(window, self.image, self.rect.center, pivot, angle) if((angle > 90 or angle < -90) and self.gunDirection != ""left""): self.gunDirection = ""left"" self.image = pygame.transform.flip(self.image, False, True) if((angle < 90 and angle > -90) and self.gunDirection != ""right""): self.gunDirection = ""right"" self.image = pygame.transform.flip(self.image, False, True) class handgun(): def __init__(self,x,y,height,width,color): self.x = x self.y = y self.height = height self.width = width self.color = color self.rect = pygame.Rect(x,y,height,width) # LOL THESE IS THE HAND self.shootsright = pygame.image.load(""hands.png"") self.image = self.shootsright self.rect = self.image.get_rect(center = (self.x, self.y)) self.look_at_pos = (self.x, self.y) self.isLookingAtPlayer = False self.look_at_pos = (x,y) self.hitbox = (self.x + -18, self.y, 46,60) self.gunDirection = ""right"" def draw(self,drawX,drawY): self.rect.topleft = (drawX,drawY) # the guns hitbox # rotatiing the gun dx = self.look_at_pos[0] - self.rect.centerx dy = self.look_at_pos[1] - self.rect.centery angle = (120/math.pi) * math.atan2(-dy, dx) gun_size = self.image.get_size() pivot = (8, gun_size[1]//2) blitRotate(window, self.image, self.rect.center, pivot, angle) if((angle > 90 or angle < -90) and self.gunDirection != ""left""): self.gunDirection = ""left"" self.image = pygame.transform.flip(self.image, False, True) if((angle < 90 and angle > -90) and self.gunDirection != ""right""): self.gunDirection = ""right"" self.image = pygame.transform.flip(self.image, False, True) def lookAt( self, coordinate ): self.look_at_pos = coordinate white = (255,255,255)handgun1 = handgun(300,300,10,10,white)how my images are blitted ```def blitRotate(surf, image, pos, originPos, angle): # calcaulate the axis aligned bounding box of the rotated image w, h = image.get_size() sin_a, cos_a = math.sin(math.radians(angle)), math.cos(math.radians(angle)) min_x, min_y = min([0, sin_a*h, cos_a*w, sin_a*h + cos_a*w]), max([0, sin_a*w, -cos_a*h, sin_a*w - cos_a*h]) # calculate the translation of the pivot pivot = pygame.math.Vector2(originPos[0], -originPos[1]) pivot_rotate = pivot.rotate(angle) pivot_move = pivot_rotate - pivot # calculate the upper left origin of the rotated image origin = (pos[0] - originPos[0] + min_x - pivot_move[0], pos[1] - originPos[1] - min_y + pivot_move[1]) # get a rotated image rotated_image = pygame.transform.rotate(image, angle) # rotate and blit the image surf.blit(rotated_image, origin) ",How Can I Make My Bullets Look LIke They Are Comming Out Of My Guns Tip?
How to forcibly free memory used by dictionary in Python," I am working on a Python script which queries several different databases to collate data and persist said data to another database. This script collects data from potentially millions of records across about 15 different databases. To attempt to speed up the script I have included some caching functionality, which boils down to having a dictionary which holds some frequently queried data. The dictionary holds key value pairs where the key is a hash generated based on the database name, collection name and query conditions and the value is the data retrieved from the database. For example:{123456789: {_id: '1', someField: 'someValue'}} where 123456789 is the hash and {_id: '1', someField: 'someValue'} is the data retrieved from the database.Holding this data in a local dictionary means that instead of having to query the databases each time, which is likely slow, I can access some frequently queried data locally. As mentioned, there are a lot of queries so the dictionary can grow pretty large (several gigabytes). I have some code which uses psutil to look at how much memory is available on the machine running the script and if the available memory gets below a certain threshold I clear the dictionary. The code to clear the dictionary is: I should point out that cached_documents is a local variable which gets passed into all the methods that either access or add to the cache. Unfortunately, it seems that this isn't enough to free the memory properly as Python is still holding onto a lot of extra memory, even after calling the above code. You can see a profile of the memory usage here:Of note is the fact that the first few times the dictionary is cleared, we release a lot of memory back the system, but each subsequent time seems to be less, at which point the memory usage flatlines because the cache gets cleared extremely frequently since the available memory is within the threshold since Python is holding onto a lot of memory.Is there a way to force Python to free the memory properly when clearing the dictionary so that I avoid flat lining? Any tips are appreciated. <code>  cached_documents.clear()cached_documents = Nonegc.collect()cached_documents = {}",How to forcibly free memory used by dictionary?
Python Seaborn: Plot multiple distplot in Facetgrid," I have a dataframe which looks like below:df: I want to plot the distribution of the value over the given years. So I used distplot of seaborn by using following code: In the next step I want to plot the same value distribution over the given year w.r.t MAJ_CAT. So I decided to use Facetgrid of seaborn, below is the code : However, when it ran the above command, it throws the following error: I am not sure where am I making the mistake. Could anyone please help me in fixing the issue? <code>  RY MAJ_CAT Value2016 Cause Unknown 0.002272016 Vegetation 0.042172016 Vegetation 0.043932016 Vegetation 0.078782016 Defective Equip 0.001372018 Cause Unknown 0.004842018 Defective Equip 0.015462020 Defective Equip 0.051692020 Defective Equip 0.005152020 Cause Unknown 0.00050 year_2016 = df[df['RY']==2016]year_2018 = df[df['RY']==2018]year_2020 = df[df['RY']==2020]sns.distplot(year_2016['value'].values, hist=False,rug=True) sns.distplot(year_2018['value'].values, hist=False,rug=True) sns.distplot(year_2020['value'].values, hist=False,rug=True) g = sns.FacetGrid(df,col='MAJ_CAT')g = g.map(sns.distplot,df[df['RY']==2016]['value'].values, hist=False,rug=True)) g = g.map(sns.distplot,df[df['RY']==2018]['value'].values, hist=False,rug=True)) g = g.map(sns.distplot,df[df['RY']==2020]['value'].values, hist=False,rug=True)) KeyError: ""None of [Index([(0.00227, 0.04217, 0.043930000000000004, 0.07877999999999999, 0.00137, 0.0018800000000000002, 0.00202, 0.00627, 0.00101, 0.07167000000000001, 0.01965, 0.02775, 0.00298, 0.00337, 0.00088, 0.04049, 0.01957, 0.01012, 0.12065, 0.23699, 0.03639, 0.00137, 0.03244, 0.00441, 0.06748, 0.00035, 0.0066099999999999996, 0.00302, 0.015619999999999998, 0.01571, 0.0018399999999999998, 0.03425, 0.08046, 0.01695, 0.02416, 0.08975, 0.0018800000000000002, 0.14743, 0.06366000000000001, 0.04378, 0.043, 0.02997, 0.0001, 0.22799, 0.00611, 0.13960999999999998, 0.38871, 0.018430000000000002, 0.053239999999999996, 0.06702999999999999, 0.14103, 0.022719999999999997, 0.011890000000000001, 0.00186, 0.00049, 0.13947, 0.0067, 0.00503, 0.00242, 0.00137, 0.00266, 0.38638, 0.24068, 0.0165, 0.54847, 1.02545, 0.01889, 0.32750999999999997, 0.22526, 0.24516, 0.12791, 0.00063, 0.0005200000000000001, 0.00921, 0.07665, 0.00116, 0.01042, 0.27046, 0.03501, 0.03159, 0.46748999999999996, 0.022090000000000002, 2.2972799999999998, 0.69021, 0.22529000000000002, 0.00147, 0.1102, 0.03234, 0.05799, 0.11744, 0.00896, 0.09556, 0.03202, 0.01347, 0.00923, 0.0034200000000000003, 0.041530000000000004, 0.04848, 0.00062, 0.0031100000000000004, ...)], dtype='object')] are in the [columns]""",Python Seaborn: Plot multiple distplot in Facetgrid
Python Seaborn: Plot multiple distplot in Facetgrid (TypeError: 'AxesSubplot' object is not callable)," I have a dataframe which looks like below:df: I want to plot the distribution of the value over the given years. So I used distplot of seaborn by using following code: In the next step I want to plot the same value distribution over the given year w.r.t MAJ_CAT. So I decided to use Facetgrid of seaborn, below is the code : However, when it ran the above command, it throws the following error: I am not sure where am I making the mistake. Could anyone please help me in fixing the issue? <code>  RY MAJ_CAT Value2016 Cause Unknown 0.002272016 Vegetation 0.042172016 Vegetation 0.043932016 Vegetation 0.078782016 Defective Equip 0.001372018 Cause Unknown 0.004842018 Defective Equip 0.015462020 Defective Equip 0.051692020 Defective Equip 0.005152020 Cause Unknown 0.00050 year_2016 = df[df['RY']==2016]year_2018 = df[df['RY']==2018]year_2020 = df[df['RY']==2020]sns.distplot(year_2016['value'].values, hist=False,rug=True) sns.distplot(year_2018['value'].values, hist=False,rug=True) sns.distplot(year_2020['value'].values, hist=False,rug=True) g = sns.FacetGrid(df,col='MAJ_CAT')g = g.map(sns.distplot,df[df['RY']==2016]['value'].values, hist=False,rug=True)) g = g.map(sns.distplot,df[df['RY']==2018]['value'].values, hist=False,rug=True)) g = g.map(sns.distplot,df[df['RY']==2020]['value'].values, hist=False,rug=True)) KeyError: ""None of [Index([(0.00227, 0.04217, 0.043930000000000004, 0.07877999999999999, 0.00137, 0.0018800000000000002, 0.00202, 0.00627, 0.00101, 0.07167000000000001, 0.01965, 0.02775, 0.00298, 0.00337, 0.00088, 0.04049, 0.01957, 0.01012, 0.12065, 0.23699, 0.03639, 0.00137, 0.03244, 0.00441, 0.06748, 0.00035, 0.0066099999999999996, 0.00302, 0.015619999999999998, 0.01571, 0.0018399999999999998, 0.03425, 0.08046, 0.01695, 0.02416, 0.08975, 0.0018800000000000002, 0.14743, 0.06366000000000001, 0.04378, 0.043, 0.02997, 0.0001, 0.22799, 0.00611, 0.13960999999999998, 0.38871, 0.018430000000000002, 0.053239999999999996, 0.06702999999999999, 0.14103, 0.022719999999999997, 0.011890000000000001, 0.00186, 0.00049, 0.13947, 0.0067, 0.00503, 0.00242, 0.00137, 0.00266, 0.38638, 0.24068, 0.0165, 0.54847, 1.02545, 0.01889, 0.32750999999999997, 0.22526, 0.24516, 0.12791, 0.00063, 0.0005200000000000001, 0.00921, 0.07665, 0.00116, 0.01042, 0.27046, 0.03501, 0.03159, 0.46748999999999996, 0.022090000000000002, 2.2972799999999998, 0.69021, 0.22529000000000002, 0.00147, 0.1102, 0.03234, 0.05799, 0.11744, 0.00896, 0.09556, 0.03202, 0.01347, 0.00923, 0.0034200000000000003, 0.041530000000000004, 0.04848, 0.00062, 0.0031100000000000004, ...)], dtype='object')] are in the [columns]""",Python Seaborn: Plot multiple distplot in Facetgrid
How to overlay two plots in same figure in plotly?," I was trying to plot barplot and scatterplot in the same plot in plotly, but it shows only scatterplot.How to show both the plots?data Code OutputRequiredshow both barchart and scatterplotbarchart y-ticks on left y-axisscatterplot y-ticks on right y-axisxticklabels rotate 90 degree <code>  import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pyplot as pltfrom matplotlib.ticker import PercentFormatterimport plotlyimport plotly.offline as pyimport plotly.graph_objs as goimport plotly.figure_factory as ffimport plotly.tools as tlsfrom plotly.subplots import make_subplotsfrom plotly.offline import plot, iplot, init_notebook_modeinit_notebook_mode(connected=False)df = pd.DataFrame({ 'price': [ 4.0, 17.0, 7.0, 7.0, 2.0, 1.0, 1.0], 'item': ['apple', 'banana', 'carrot', 'plum', 'orange', 'date', 'cherry']})df = df.sort_values(num,ascending=False)df['cumulative_sum'] = df[num].cumsum()df['cumulative_perc'] = 100*df['cumulative_sum']/df[num].sum()df['demarcation'] = 80num = 'price'cat = 'item'title = 'Pareto Chart' trace1 = go.Bar( x=df[cat], y=df[num], name=num, marker=dict( color='rgb(34,163,192)' ))trace2 = go.Scatter( x=df[cat], y=df['cumulative_perc'], name='Cumulative Percentage', yaxis='y2',)data = [trace1,trace2]fig = dict(data=data)iplot(fig)",How to overlay two plots in same figure in plotly ( Create Pareto chart in plotly )?
Accessing data in tensorflow PrefetchDataset," I am still learning tensorflow and keras, and I suspect this question has a very easy answer I'm just missing due to lack of familiarity.I have a PrefetchDataset object: ...made up of features and a target. I can iterate over it using a for loop: However, this is very slow. What I'd like to do is access the tensor corresponding to the class labels and turn that into a numpy array, or a list, or any sort of iterable that can be fed into scikit-learn's classification report and/or confusion matrix: ...OR access the data such that it could be used in tensorflow's confusion matrix: In both cases, the general ability to grab the target data from the original object in a manner that isn't computationally expensive would be very helpful (and might help with my underlying intuitions re: tensorflow and keras).Any advice would be greatly appreciated. <code>  > print(tf_test)$ <PrefetchDataset shapes: ((None, 99), (None,)), types: (tf.float32, tf.int64)> > for example in tf_test:> print(example[0].numpy())> print(example[1].numpy())> exit()$ [[-0.31 -0.94 -1.12 ... 0.18 -0.27] [-0.22 -0.54 -0.14 ... 0.33 -0.55] [-0.60 -0.02 -1.41 ... 0.21 -0.63] ... [-0.03 -0.91 -0.12 ... 0.77 -0.23] [-0.76 -1.48 -0.15 ... 0.38 -0.35] [-0.55 -0.08 -0.69 ... 0.44 -0.36]] [0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 ... 0 1 1 0] > y_pred = model.predict(tf_test)> print(y_pred)$ [[0.01] [0.14] [0.00] ... [0.32] [0.03] [0.00]]> y_pred_list = [int(x[0]) for x in y_pred] # assumes value >= 0.5 is positive prediction> y_true = [] # what I need help with> print(sklearn.metrics.confusion_matrix(y_true, y_pred_list) > labels = [] # what I need help with> predictions = y_pred_list # could we just use a tensor?> print(tf.math.confusion_matrix(labels, predictions)",Extract target from Tensorflow PrefetchDataset
"For multi-class classification problem, is pytorch cross-entropy loss function needs target to be one hot encoded?"," For example, if I want to solve the MNIST classification problem, we have 10 output classes. With PyTorch, I would like to use the torch.nn.CrossEntropyLoss function. Do I have to format the targets so that they are one-hot encoded or can I simply use their class labels that come with the dataset? <code> ",Is One-Hot Encoding required for using PyTorch's Cross Entropy Loss Function?
Plotly dropdown won't show plots," I am trying to plot three graphs (day, month, year) and give the user the option to pick which graph they want to see with a dropdown menu. When I do it for (day, month) it works perfectly (with month showing as the default graph), but when I add (year), then (day, month) don't show up (in this scenario, I want year to be the default graph).This is the working code: This is the code that doesn't work and I can't figure out why: Here's the data:  <code>  # Plot Daytemp_day = pd.DataFrame(df.day.value_counts())temp_day.reset_index(inplace=True)temp_day.columns = ['day', 'tweet_count']temp_day.sort_values(by=['day'], inplace=True)temp_day.reset_index(inplace=True, drop=True)trace_day = go.Scatter( x=temp_day.day.values, y=temp_day.tweet_count.values, text = [f""{humanize.naturaldate(day)}: {count} tweets"" for day,count in zip(temp_day.day.values,temp_day.tweet_count.values)], hoverinfo='text', mode='lines', line = { 'color': my_color, 'width': 1.2 }, visible=False, name=""Day"" )# Plot Monthtemp_month = pd.DataFrame(df.YYYYMM.value_counts())temp_month.reset_index(inplace=True)temp_month.columns = ['YYYYMM', 'tweet_count']temp_month['YYYYMM'] = temp_month['YYYYMM'].dt.strftime('%Y-%m')temp_month.sort_values(by=['YYYYMM'], inplace=True)temp_month.reset_index(inplace=True, drop=True)trace_month = go.Scatter( x=temp_month.YYYYMM.values, y=temp_month.tweet_count.values, mode='lines', line = { 'color': my_color, 'width': 1.2 }, visible=True, name=""Month"" ) # Menusupdatemenus = list([ dict( active=0, buttons=list([ dict(label = 'Month', method = 'update', args = [{'visible': [True, False]}, {'title': 'Number of Tweets per Month'}]), dict(label = 'Day', method = 'update', args = [{'visible': [False, True]}, {'title': 'Number of Tweets per Day'}]), ]), )])# Layoutlayout = go.Layout(title=""Number of Tweets -- Pick a scale"", updatemenus=updatemenus, )fig = go.Figure(data=[trace_month, trace_day], layout=layout)iplot(fig) # Plot Daytemp_day = pd.DataFrame(df.day.value_counts())temp_day.reset_index(inplace=True)temp_day.columns = ['day', 'tweet_count']temp_day.sort_values(by=['day'], inplace=True)temp_day.reset_index(inplace=True, drop=True)trace_day = go.Scatter( x=temp_day.day.values, y=temp_day.tweet_count.values, text = [f""{humanize.naturaldate(day)}: {count} tweets"" for day,count in zip(temp_day.day.values,temp_day.tweet_count.values)], hoverinfo='text', mode='lines', line = { 'color': my_color, 'width': 1.2 }, visible=False, name=""Day"" )# Plot Monthtemp_month = pd.DataFrame(df.YYYYMM.value_counts())temp_month.reset_index(inplace=True)temp_month.columns = ['YYYYMM', 'tweet_count']temp_month['YYYYMM'] = temp_month['YYYYMM'].dt.strftime('%Y-%m')temp_month.sort_values(by=['YYYYMM'], inplace=True)temp_month.reset_index(inplace=True, drop=True)trace_month = go.Scatter( x=temp_month.YYYYMM.values, y=temp_month.tweet_count.values, mode='lines', line = { 'color': my_color, 'width': 1.2 }, visible=False, name=""Month"" ) # Plot yeartemp_year = pd.DataFrame(df.year.value_counts())temp_year.reset_index(inplace=True)temp_year.columns = ['year', 'tweet_count']temp_year.sort_values(by=['year'], inplace=True)temp_year.reset_index(inplace=True, drop=True)trace_year = go.Scatter( x=temp_year.year.values, y=temp_year.tweet_count.values, text = [f""Year {year}: {count:,.0f} tweets"" for year,count in zip(temp_year.year.values,temp_year.tweet_count.values)], hoverinfo='text', mode='lines+markers', line = { 'color': my_color, 'width': 1.2 }, visible=True, name=""Year"" ) # Menusupdatemenus = list([ dict( active=0, buttons=list([ dict(label = 'Year', method = 'update', args = [{'visible': [True, False, False]}, {'title': 'Number of Tweets per Month'}]), dict(label = 'Month', method = 'update', args = [{'visible': [False, True, False]}, {'title': 'Number of Tweets per Month'}]), dict(label = 'Day', method = 'update', args = [{'visible': [False, False, True]}, {'title': 'Number of Tweets per Day'}]), ]), )])# Layoutlayout = go.Layout(title=""Number of Tweets -- Pick a scale"", updatemenus=updatemenus, )fig = go.Figure(data=[trace_year, trace_month, trace_day], layout=layout)iplot(fig) # YearScatter({ 'hoverinfo': 'text', 'line': {'color': '#ff00a7', 'width': 1.2}, 'mode': 'lines+markers', 'name': 'Year', 'text': [Year 2011: 73 tweets, Year 2012: 562 tweets, Year 2013: 1,153 tweets, Year 2014: 700 tweets, Year 2015: 2,104 tweets, Year 2016: 1,816 tweets, Year 2017: 1,691 tweets, Year 2018: 1,082 tweets, Year 2019: 914 tweets, Year 2020: 482 tweets], 'visible': False, 'x': array([2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]), 'y': array([ 73, 562, 1153, 700, 2104, 1816, 1691, 1082, 914, 482])})# Month Scatter({ 'line': {'color': '#ff00a7', 'width': 1.2}, 'mode': 'lines', 'name': 'Month', 'visible': False, 'x': array(['2011-06', '2011-07', '2011-08', '2011-09', '2011-10', '2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05', '2012-06', '2012-07', '2012-08', '2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02', '2016-03', '2016-04', '2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11', '2016-12', '2017-01', '2017-02', '2017-03', '2017-04', '2017-05', '2017-06', '2017-07', '2017-08', '2017-09', '2017-10', '2017-11', '2017-12', '2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06', '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12', '2019-01', '2019-02', '2019-03', '2019-04', '2019-05', '2019-06', '2019-08', '2019-09', '2019-10', '2019-11', '2019-12', '2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06'], dtype=object), 'y': array([ 1, 1, 2, 8, 4, 20, 37, 79, 16, 13, 8, 12, 2, 5, 68, 139, 57, 64, 99, 182, 63, 60, 74, 128, 59, 109, 126, 86, 77, 112, 77, 78, 44, 32, 22, 33, 46, 61, 66, 109, 81, 78, 50, 140, 151, 297, 173, 225, 69, 119, 213, 177, 134, 217, 189, 255, 149, 114, 127, 154, 116, 110, 150, 184, 179, 117, 161, 48, 115, 147, 153, 199, 174, 195, 154, 162, 114, 140, 90, 156, 81, 107, 62, 64, 49, 128, 127, 60, 89, 115, 44, 58, 86, 65, 102, 93, 82, 78, 158, 65, 50, 77, 55, 71, 70, 105, 124, 57])})# DayScatter({ 'hoverinfo': 'text', 'line': {'color': '#ff00a7', 'width': 1.2}, 'mode': 'lines', 'name': 'Day', 'text': [Jun 04 2011: 1 tweets, Jul 17 2011: 1 tweets, Aug 11 2011: 1 tweets, ..., Jun 17: 4 tweets, Jun 18: 1 tweets, Jun 19: 3 tweets], 'visible': False, 'x': array([datetime.date(2011, 6, 4), datetime.date(2011, 7, 17), datetime.date(2011, 8, 11), ..., datetime.date(2020, 6, 17), datetime.date(2020, 6, 18), datetime.date(2020, 6, 19)], dtype=object), 'y': array([1, 1, 1, ..., 4, 1, 3])})","Plotly: How to subset data by year, month and day using dropdown menus?"
Plotly: Dropdown menu won't show plots," I am trying to plot three graphs (day, month, year) and give the user the option to pick which graph they want to see with a dropdown menu. When I do it for (day, month) it works perfectly (with month showing as the default graph), but when I add (year), then (day, month) don't show up (in this scenario, I want year to be the default graph).This is the working code: This is the code that doesn't work and I can't figure out why: Here's the data:  <code>  # Plot Daytemp_day = pd.DataFrame(df.day.value_counts())temp_day.reset_index(inplace=True)temp_day.columns = ['day', 'tweet_count']temp_day.sort_values(by=['day'], inplace=True)temp_day.reset_index(inplace=True, drop=True)trace_day = go.Scatter( x=temp_day.day.values, y=temp_day.tweet_count.values, text = [f""{humanize.naturaldate(day)}: {count} tweets"" for day,count in zip(temp_day.day.values,temp_day.tweet_count.values)], hoverinfo='text', mode='lines', line = { 'color': my_color, 'width': 1.2 }, visible=False, name=""Day"" )# Plot Monthtemp_month = pd.DataFrame(df.YYYYMM.value_counts())temp_month.reset_index(inplace=True)temp_month.columns = ['YYYYMM', 'tweet_count']temp_month['YYYYMM'] = temp_month['YYYYMM'].dt.strftime('%Y-%m')temp_month.sort_values(by=['YYYYMM'], inplace=True)temp_month.reset_index(inplace=True, drop=True)trace_month = go.Scatter( x=temp_month.YYYYMM.values, y=temp_month.tweet_count.values, mode='lines', line = { 'color': my_color, 'width': 1.2 }, visible=True, name=""Month"" ) # Menusupdatemenus = list([ dict( active=0, buttons=list([ dict(label = 'Month', method = 'update', args = [{'visible': [True, False]}, {'title': 'Number of Tweets per Month'}]), dict(label = 'Day', method = 'update', args = [{'visible': [False, True]}, {'title': 'Number of Tweets per Day'}]), ]), )])# Layoutlayout = go.Layout(title=""Number of Tweets -- Pick a scale"", updatemenus=updatemenus, )fig = go.Figure(data=[trace_month, trace_day], layout=layout)iplot(fig) # Plot Daytemp_day = pd.DataFrame(df.day.value_counts())temp_day.reset_index(inplace=True)temp_day.columns = ['day', 'tweet_count']temp_day.sort_values(by=['day'], inplace=True)temp_day.reset_index(inplace=True, drop=True)trace_day = go.Scatter( x=temp_day.day.values, y=temp_day.tweet_count.values, text = [f""{humanize.naturaldate(day)}: {count} tweets"" for day,count in zip(temp_day.day.values,temp_day.tweet_count.values)], hoverinfo='text', mode='lines', line = { 'color': my_color, 'width': 1.2 }, visible=False, name=""Day"" )# Plot Monthtemp_month = pd.DataFrame(df.YYYYMM.value_counts())temp_month.reset_index(inplace=True)temp_month.columns = ['YYYYMM', 'tweet_count']temp_month['YYYYMM'] = temp_month['YYYYMM'].dt.strftime('%Y-%m')temp_month.sort_values(by=['YYYYMM'], inplace=True)temp_month.reset_index(inplace=True, drop=True)trace_month = go.Scatter( x=temp_month.YYYYMM.values, y=temp_month.tweet_count.values, mode='lines', line = { 'color': my_color, 'width': 1.2 }, visible=False, name=""Month"" ) # Plot yeartemp_year = pd.DataFrame(df.year.value_counts())temp_year.reset_index(inplace=True)temp_year.columns = ['year', 'tweet_count']temp_year.sort_values(by=['year'], inplace=True)temp_year.reset_index(inplace=True, drop=True)trace_year = go.Scatter( x=temp_year.year.values, y=temp_year.tweet_count.values, text = [f""Year {year}: {count:,.0f} tweets"" for year,count in zip(temp_year.year.values,temp_year.tweet_count.values)], hoverinfo='text', mode='lines+markers', line = { 'color': my_color, 'width': 1.2 }, visible=True, name=""Year"" ) # Menusupdatemenus = list([ dict( active=0, buttons=list([ dict(label = 'Year', method = 'update', args = [{'visible': [True, False, False]}, {'title': 'Number of Tweets per Month'}]), dict(label = 'Month', method = 'update', args = [{'visible': [False, True, False]}, {'title': 'Number of Tweets per Month'}]), dict(label = 'Day', method = 'update', args = [{'visible': [False, False, True]}, {'title': 'Number of Tweets per Day'}]), ]), )])# Layoutlayout = go.Layout(title=""Number of Tweets -- Pick a scale"", updatemenus=updatemenus, )fig = go.Figure(data=[trace_year, trace_month, trace_day], layout=layout)iplot(fig) # YearScatter({ 'hoverinfo': 'text', 'line': {'color': '#ff00a7', 'width': 1.2}, 'mode': 'lines+markers', 'name': 'Year', 'text': [Year 2011: 73 tweets, Year 2012: 562 tweets, Year 2013: 1,153 tweets, Year 2014: 700 tweets, Year 2015: 2,104 tweets, Year 2016: 1,816 tweets, Year 2017: 1,691 tweets, Year 2018: 1,082 tweets, Year 2019: 914 tweets, Year 2020: 482 tweets], 'visible': False, 'x': array([2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]), 'y': array([ 73, 562, 1153, 700, 2104, 1816, 1691, 1082, 914, 482])})# Month Scatter({ 'line': {'color': '#ff00a7', 'width': 1.2}, 'mode': 'lines', 'name': 'Month', 'visible': False, 'x': array(['2011-06', '2011-07', '2011-08', '2011-09', '2011-10', '2011-11', '2011-12', '2012-01', '2012-02', '2012-03', '2012-04', '2012-05', '2012-06', '2012-07', '2012-08', '2012-09', '2012-10', '2012-11', '2012-12', '2013-01', '2013-02', '2013-03', '2013-04', '2013-05', '2013-06', '2013-07', '2013-08', '2013-09', '2013-10', '2013-11', '2013-12', '2014-01', '2014-02', '2014-03', '2014-04', '2014-05', '2014-06', '2014-07', '2014-08', '2014-09', '2014-10', '2014-11', '2014-12', '2015-01', '2015-02', '2015-03', '2015-04', '2015-05', '2015-06', '2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02', '2016-03', '2016-04', '2016-05', '2016-06', '2016-07', '2016-08', '2016-09', '2016-10', '2016-11', '2016-12', '2017-01', '2017-02', '2017-03', '2017-04', '2017-05', '2017-06', '2017-07', '2017-08', '2017-09', '2017-10', '2017-11', '2017-12', '2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06', '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12', '2019-01', '2019-02', '2019-03', '2019-04', '2019-05', '2019-06', '2019-08', '2019-09', '2019-10', '2019-11', '2019-12', '2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06'], dtype=object), 'y': array([ 1, 1, 2, 8, 4, 20, 37, 79, 16, 13, 8, 12, 2, 5, 68, 139, 57, 64, 99, 182, 63, 60, 74, 128, 59, 109, 126, 86, 77, 112, 77, 78, 44, 32, 22, 33, 46, 61, 66, 109, 81, 78, 50, 140, 151, 297, 173, 225, 69, 119, 213, 177, 134, 217, 189, 255, 149, 114, 127, 154, 116, 110, 150, 184, 179, 117, 161, 48, 115, 147, 153, 199, 174, 195, 154, 162, 114, 140, 90, 156, 81, 107, 62, 64, 49, 128, 127, 60, 89, 115, 44, 58, 86, 65, 102, 93, 82, 78, 158, 65, 50, 77, 55, 71, 70, 105, 124, 57])})# DayScatter({ 'hoverinfo': 'text', 'line': {'color': '#ff00a7', 'width': 1.2}, 'mode': 'lines', 'name': 'Day', 'text': [Jun 04 2011: 1 tweets, Jul 17 2011: 1 tweets, Aug 11 2011: 1 tweets, ..., Jun 17: 4 tweets, Jun 18: 1 tweets, Jun 19: 3 tweets], 'visible': False, 'x': array([datetime.date(2011, 6, 4), datetime.date(2011, 7, 17), datetime.date(2011, 8, 11), ..., datetime.date(2020, 6, 17), datetime.date(2020, 6, 18), datetime.date(2020, 6, 19)], dtype=object), 'y': array([1, 1, 1, ..., 4, 1, 3])})","Plotly: How to subset data by year, month and day using dropdown menus?"
What is the time complexity of checking membership operation in dict.items()?," What is the time complexity of checking membership in dict.items()?According to the documentation:Keys views are set-like since their entries are unique and hashable.If all values are hashable, so that (key, value) pairs are unique andhashable, then the items view is also set-like. (Values views are nottreated as set-like since the entries are generally not unique.) Forset-like views, all of the operations defined for the abstract baseclass collections.abc.Set are available (for example, ==, <, or ^).So I did some testing with the following code: With the output: As you can see, when the dict.values are all hashable (int),the execution time for the membership is similar to that of a set or d_keys,because items view is set-like.The last two examples are on the dict.values with unhashable objects (list).So I assumed the execution time would be similar to that of a list.However, they are still similar to that of a set.Does this mean that even though dict.values are unhashable objects,the implementation of items view is still very efficient,resulting O(1) time complexity for checking the membership?Am I missing something here?EDITEDper @chepner's comment: dict.fromkeys(r, [1]) -> {k: [1] for k in r}EDITEDper @MarkRansom's comment: another test case list(d2.items()) <code>  from timeit import timeitdef membership(val, container): val in containerr = range(100000)s = set(r)d = dict.fromkeys(r, 1)d2 = {k: [1] for k in r}items_list = list(d2.items())print('set'.ljust(12), end='')print(timeit(lambda: membership(-1, s), number=1000))print('dict'.ljust(12), end='')print(timeit(lambda: membership(-1, d), number=1000))print('d_keys'.ljust(12), end='')print(timeit(lambda: membership(-1, d.keys()), number=1000))print('d_values'.ljust(12), end='')print(timeit(lambda: membership(-1, d.values()), number=1000))print('\n*With hashable dict.values')print('d_items'.ljust(12), end='')print(timeit(lambda: membership((-1, 1), d.items()), number=1000))print('*With unhashable dict.values')print('d_items'.ljust(12), end='')print(timeit(lambda: membership((-1, 1), d2.items()), number=1000))print('d_items'.ljust(12), end='')print(timeit(lambda: membership((-1, [1]), d2.items()), number=1000))print('\nitems_list'.ljust(12), end='')print(timeit(lambda: membership((-1, [1]), items_list), number=1000)) set 0.00034419999999998896dict 0.0003307000000000171d_keys 0.0004200000000000037d_values 2.4773092*With hashable dict.valuesd_items 0.0004413000000003109*With unhashable dict.valuesd_items 0.00042879999999989593d_items 0.0005549000000000248items_list 3.5529328",What is the time complexity of checking membership in dict.items()?
Can someone explain this weird behaviour in Python?," I am not sure I quite understand what's happening in the below mini snippet (on Py v3.6.7). It would be great if someone can explain to me as to how can we mutate the list successfully even though there's an error thrown by Python.I know that we can mutate a list and update it, but whats with the error? Like I was under the impression that if there's an error, then the x should remain the same. The Traceback thrown at line (1) is I understand what the error means but I am unable to get the context of it.But now if I try to print the value of my variable x, Python says it's, As far as I can understand, the exception has happened after Python allowed the mutation of the list to happen and then hopefully it tried re-assigning it back. It blew there I think as Tuples are immutable.Can someone explain what's happening under the hood?Edit - 1Error From ipython console as an image; <code>  x = ([1, 2], )x[0] += [3,4] # ------ (1) > TypeError: 'tuple' object doesn't support item assignment.. print(x) # returns ([1, 2, 3, 4])",Why does mutating a list in a tuple raise an exception but mutate it anyway?
Why does mutating a list in a tuple raises an exception but mutates it anyway?," I am not sure I quite understand what's happening in the below mini snippet (on Py v3.6.7). It would be great if someone can explain to me as to how can we mutate the list successfully even though there's an error thrown by Python.I know that we can mutate a list and update it, but whats with the error? Like I was under the impression that if there's an error, then the x should remain the same. The Traceback thrown at line (1) is I understand what the error means but I am unable to get the context of it.But now if I try to print the value of my variable x, Python says it's, As far as I can understand, the exception has happened after Python allowed the mutation of the list to happen and then hopefully it tried re-assigning it back. It blew there I think as Tuples are immutable.Can someone explain what's happening under the hood?Edit - 1Error From ipython console as an image; <code>  x = ([1, 2], )x[0] += [3,4] # ------ (1) > TypeError: 'tuple' object doesn't support item assignment.. print(x) # returns ([1, 2, 3, 4])",Why does mutating a list in a tuple raise an exception but mutate it anyway?
Discord.py Glitch or random error: TypeError: __new__() got an unexpected keyword argument 'deny_new'," Yesterday, my code was perfectly fine. Everything was running... and it was going great. All of a sudden, this error: pops up in my PyCharm console. I've looked it up on the internet but I've only found a similiar questions with zero answers to it. I hope the stackoverflow community will be able to help me. I did not change my code, all I did was, I tried to host my bot on heroku, and it did not go well. And after my first few attempts, I gave up. But, I found out my bot started going crazy and I couldn't run it anymore :<. Has anyone else experienced this and know how to fix it?UPDATEI just found out that for some reason, it only works on my test server but not any other servers. I tried it with a different file and bot and I got the same results, this is like a problem with discord.py.This is literally my entire code <code>  TypeError: __new__() got an unexpected keyword argument 'deny_new' Traceback (most recent call last): File ""C:/Users/danie/PyCharmProjects/skybot/skybotgaming.py"", line 21, in <module> client.run('TOKEN') File ""C:\Users\danie\anaconda3\envs\discordbottest\lib\site-packages\discord\client.py"", line 640, in run return future.result() File ""C:\Users\danie\anaconda3\envs\discordbottest\lib\site-packages\discord\client.py"", line 621, in runner await self.start(*args, **kwargs) File ""C:\Users\danie\anaconda3\envs\discordbottest\lib\site-packages\discord\client.py"", line 585, in start await self.connect(reconnect=reconnect) File ""C:\Users\danie\anaconda3\envs\discordbottest\lib\site-packages\discord\client.py"", line 499, in connect await self._connect() File ""C:\Users\danie\anaconda3\envs\discordbottest\lib\site-packages\discord\client.py"", line 463, in _connect await self.ws.poll_event() File ""C:\Users\danie\anaconda3\envs\discordbottest\lib\site-packages\discord\gateway.py"", line 471, in poll_event await self.received_message(msg) File ""C:\Users\danie\anaconda3\envs\discordbottest\lib\site-packages\discord\gateway.py"", line 425, in received_message func(data) File ""C:\Users\danie\anaconda3\envs\discordbottest\lib\site-packages\discord\state.py"", line 750, in parse_guild_create guild = self._get_create_guild(data) File ""C:\Users\danie\anaconda3\envs\discordbottest\lib\site-packages\discord\state.py"", line 725, in _get_create_guild guild._from_data(data) File ""C:\Users\danie\anaconda3\envs\discordbottest\lib\site-packages\discord\guild.py"", line 297, in _from_data self._sync(guild) File ""C:\Users\danie\anaconda3\envs\discordbottest\lib\site-packages\discord\guild.py"", line 328, in _sync self._add_channel(CategoryChannel(guild=self, data=c, state=self._state)) File ""C:\Users\danie\anaconda3\envs\discordbottest\lib\site-packages\discord\channel.py"", line 726, in __init__ self._update(guild, data) File ""C:\Users\danie\anaconda3\envs\discordbottest\lib\site-packages\discord\channel.py"", line 737, in _update self._fill_overwrites(data) File ""C:\Users\danie\anaconda3\envs\discordbottest\lib\site-packages\discord\abc.py"", line 294, in _fill_overwrites self._overwrites.append(_Overwrites(id=overridden_id, **overridden))TypeError: __new__() got an unexpected keyword argument 'deny_new' import discordimport randomfrom discord.ext import commandsimport asyncioclient = commands.Bot(command_prefix='{')client.remove_command('help')@client.eventasync def on_ready(): print(""Signed in"")@client.command()async def dm(ctx): await ctx.author.send(""What up chump?"")client.run('TOKEN')",Discord.py error: TypeError: __new__() got an unexpected keyword argument 'deny_new'
How to scale sklearn's pipeline output for Autoencoders?," I'm using sklearn pipelines to build a Keras autoencoder model and use gridsearch to find the best hyperparameters. This works fine if I use a Multilayer Perceptron model for classification; however, in the autoencoder I need the output values to be the same as input. In other words, I am using a StandardScalar instance in the pipeline to scale the input values and therefore this leads to my question: how can I make the StandardScalar instance inside the pipeline to work on both the input data as well as target data, so that they end up to be the same?I'm providing a code snippet as an example. <code>  from sklearn.datasets import make_classificationfrom sklearn.preprocessing import StandardScalerfrom sklearn.pipeline import Pipelinefrom sklearn.model_selection import GridSearchCV, KFoldfrom keras.models import Sequentialfrom keras.layers import Dense, Dropoutfrom keras.optimizers import RMSprop, Adamfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressorX, y = make_classification (n_features = 50, n_redundant = 0, random_state = 0, scale = 100, n_clusters_per_class = 1)# Define wrapperdef create_model (learn_rate = 0.01, input_shape, metrics = ['mse']): model = Sequential () model.add (Dense (units = 64, activation = 'relu', input_shape = (input_shape, ))) model.add (Dense (32, activation = 'relu')) model.add (Dense (8, activation = 'relu')) model.add (Dense (32, activation = 'relu')) model.add (Dense (input_shape, activation = None)) model.compile (loss = 'mean_squared_error', optimizer = Adam (lr = learn_rate), metrics = metrics) return model# Create scalermy_scaler = StandardScaler ()steps = list ()steps.append (('scaler', my_scaler))standard_scaler_transformer = Pipeline (steps)# Create classifierclf = KerasRegressor (build_fn = create_model, verbose = 2)# Assemble pipeline# How to scale input and output??clf = Pipeline (steps = [('scaler', my_scaler), ('classifier', clf)], verbose = True)# Run grid searchparam_grid = {'classifier__input_shape' : [X.shape [1]], 'classifier__batch_size' : [50], 'classifier__learn_rate' : [0.001], 'classifier__epochs' : [5, 10]}cv = KFold (n_splits = 5, shuffle = False)grid = GridSearchCV (estimator = clf, param_grid = param_grid, scoring = 'neg_mean_squared_error', verbose = 1, cv = cv)grid_result = grid.fit (X, X)print ('Best: %f using %s' % (grid_result.best_score_, grid_result.best_params_))",How to scale target values of a Keras autoencoder model using a sklearn pipeline?
Efficient algorithm," I had this problem in one of my interview practices and had a problem getting this with a better time complexity other than O(N^2). At some level you'll have to visit each element in the list. I thought about using hash table but it would still have to conduct the hash table and populate it then do the calculation. Basically my solution was a nested for loop and I have my code included as well and it passed everything except time exception under 4 seconds.My Code: The problem description: <code>  def concatenationsSum(a): sum = 0 current_index_looking_at = 0 for i in a: for x in a: temp = str(i)+str(x) sum += int(temp) return sum Given an array of positive integers a, your task is to calculate the sumof every possible a[i] a[j], where a[i] a[j] is the concatenationof the string representations of a[i] and a[j] respectively. Example For a = [10, 2], the output should be concatenationsSum(a) = 1344. a[0] a[0] = 10 10 = 1010, a[0] a[1] = 10 2 = 102, a[1] a[0] = 2 10 = 210, a[1] a[1] = 2 2 = 22. So the sum is equal to 1010 + 102 + 210 + 22 = 1344. For a = [8], the output should be concatenationsSum(a) = 88. There is only one number in a, and a[0] a[0] = 8 8 = 88, so the answer is 88. Input/Output [execution time limit] 4 seconds (py3) [input] array.integer a A non-empty array of positive integers. Guaranteed constraints: 1 a.length 10^5, 1 a[i] 10^6. [output] integer64 The sum of all a[i] a[j]s. It's guaranteed that the answer is less than 2^53.",Efficient algorithm to find the sum of all concatenated pairs of integers in a list
Where does the Python's class attribute `__mro__` comes from?," Let's say there is some class: (1)Somewhere on SO and in documentation i read next:mro() called at class instantiation, and its result is stored in __mro__.Okay, that is still clear to me, cause in __mro__ something is really stored: Again, somewhere i read this:To look up an attribute name Python searches:a)Search the __dict__ of all metaclasses on the __mro__ found atCs __class__.b)If a data descriptor was found in step a, call its __get__()and exit.c)Else, call a descriptor or return a value in the __dict__of a class on Cs own __mro__.d)Call a non-data descriptor found in step a.e)Else, return Metaclass-tree valuesI can find out that there is no __mro__ in Test.__dict__: So accordingly to e clause from previous quote i guess that__mro__ should be taken from ""Metaclass-tree values"" and hence from type.__dict__Really, there is __mro__ in type.__dict__: So what was mentioned above in (1) about mro() result stored in __mro__ attribute from documentation doesn't really works this way?How does <member '__mro__' of 'type' objects> results to (__main__.Test, object)?Maybe you could show some source code to understand what really happens when i call Test.__mro__.. <code>  class Test(): pass Test.__mro__ Out[48]: (__main__.Test, object) '__mro__' in Test.__dict__ Out[49]: False [""mro:<method 'mro' of 'type' objects>"",""__mro__:<member '__mro__' of 'type' objects>""]",Where does the `__mro__` attribute of a Python's class come from?
Where does the Python's class `__mro__` attribute comes from?," Let's say there is some class: (1)Somewhere on SO and in documentation i read next:mro() called at class instantiation, and its result is stored in __mro__.Okay, that is still clear to me, cause in __mro__ something is really stored: Again, somewhere i read this:To look up an attribute name Python searches:a)Search the __dict__ of all metaclasses on the __mro__ found atCs __class__.b)If a data descriptor was found in step a, call its __get__()and exit.c)Else, call a descriptor or return a value in the __dict__of a class on Cs own __mro__.d)Call a non-data descriptor found in step a.e)Else, return Metaclass-tree valuesI can find out that there is no __mro__ in Test.__dict__: So accordingly to e clause from previous quote i guess that__mro__ should be taken from ""Metaclass-tree values"" and hence from type.__dict__Really, there is __mro__ in type.__dict__: So what was mentioned above in (1) about mro() result stored in __mro__ attribute from documentation doesn't really works this way?How does <member '__mro__' of 'type' objects> results to (__main__.Test, object)?Maybe you could show some source code to understand what really happens when i call Test.__mro__.. <code>  class Test(): pass Test.__mro__ Out[48]: (__main__.Test, object) '__mro__' in Test.__dict__ Out[49]: False [""mro:<method 'mro' of 'type' objects>"",""__mro__:<member '__mro__' of 'type' objects>""]",Where does the `__mro__` attribute of a Python's class come from?
Pandas dataframe replace function not working properly," Trying to group 23 different labels in second last column of ""KDDTest+.csv"" into four groups. Please note, I have deleted the last column of the csv prior to doing this.I have read the .csv file using where If I print out the first 5 rows of the dataframe, this is the output (please note the 'label' column):using print(df.head(5)) I've tried both these methods for grouping based on what I found online:Method 1: Method 2: However, this is still the output of printing the first 5 rows of the dataframe: I'm expecting the label column in row 2 to read 'dos' instead of 'neptune', but it's not happening.What am I doing wrong? Any help is appreciated. <code>  df = pd.read_csv('KDDTrain+.csv', header=None, names = col_names) col_names = [""duration"",""protocol_type"",""service"",""flag"",""src_bytes"", ""dst_bytes"",""land"",""wrong_fragment"",""urgent"",""hot"",""num_failed_logins"", ""logged_in"",""num_compromised"",""root_shell"",""su_attempted"",""num_root"", ""num_file_creations"",""num_shells"",""num_access_files"",""num_outbound_cmds"", ""is_host_login"",""is_guest_login"",""count"",""srv_count"",""serror_rate"", ""srv_serror_rate"",""rerror_rate"",""srv_rerror_rate"",""same_srv_rate"", ""diff_srv_rate"",""srv_diff_host_rate"",""dst_host_count"",""dst_host_srv_count"", ""dst_host_same_srv_rate"",""dst_host_diff_srv_rate"",""dst_host_same_src_port_rate"", ""dst_host_srv_diff_host_rate"",""dst_host_serror_rate"",""dst_host_srv_serror_rate"", ""dst_host_rerror_rate"",""dst_host_srv_rerror_rate"",""label""] duration protocol_type ... dst_host_srv_rerror_rate label0 0 tcp ... 0.00 normal1 0 udp ... 0.00 normal2 0 tcp ... 0.00 neptune3 0 tcp ... 0.01 normal4 0 tcp ... 0.00 normal df.replace(to_replace = ['ipsweep.', 'portsweep.', 'nmap.', 'satan.'], value = 'probe', inplace = True)df.replace(to_replace = ['ftp_write.', 'guess_passwd.', 'imap.', 'multihop.', 'phf.', 'spy.', 'warezclient.', 'warezmaster.'], value = 'r2l', inplace = True)df.replace(to_replace = ['buffer_overflow.', 'loadmodule.', 'perl.', 'rootkit.'], value = 'u2r', inplace = True)df.replace(to_replace = ['back.', 'land.' , 'neptune.', 'pod.', 'smurf.', 'teardrop.'], value = 'dos', inplace = True) df['label'] = df['label'].replace(['ipsweep.', 'portsweep.', 'nmap.', 'satan.'], 'probe',regex=True)df['label'] = df['label'].replace(['ftp_write.', 'guess_passwd.', 'imap.', 'multihop.', 'phf.', 'spy.', 'warezclient.', 'warezmaster.'], 'r2l',regex=True)df['label'] = df['label'].replace(['buffer_overflow.', 'loadmodule.', 'perl.', 'rootkit.'], 'u2r',regex=True)df['label'] = df['label'].replace(['back.', 'land.' , 'neptune.', 'pod.', 'smurf.', 'teardrop.'], 'dos',regex=True) After replacing, first 5 rows of df: duration protocol_type ... dst_host_srv_rerror_rate label0 0 tcp ... 0.00 normal1 0 udp ... 0.00 normal2 0 tcp ... 0.00 neptune3 0 tcp ... 0.01 normal4 0 tcp ... 0.00 normal",How to replace a value in pandas?
seaborn displot() is not plotting within defined subplots," I am trying to plot two displots side by side with this code It returns the following result (two empty subplots followed by one displot each on two lines)-If I try the same code with violinplot, it returns result as expected Why is displot returning a different kind of output and what can I do to output two plots on the same line? <code>  fig,(ax1,ax2) = plt.subplots(1,2)sns.displot(x =X_train['Age'], hue=y_train, ax=ax1)sns.displot(x =X_train['Fare'], hue=y_train, ax=ax2) fig,(ax1,ax2) = plt.subplots(1,2)sns.violinplot(y_train, X_train['Age'], ax=ax1)sns.violinplot(y_train, X_train['Fare'], ax=ax2)",seaborn is not plotting within defined subplots
Why can't Python's walrus operator be used on instance attributes?," I just learned that the new walrus operator (:=) can't be used to set instance attributes, it's supposedly invalid syntax (raises a SyntaxError).Why is this? (And can you provide a link to official docs mentioning this?)I looked through PEP 572, and couldn't find if/where this is documented.ResearchThis answer mentions this limitation without an explanation or source:you can't use the walrus operator on object attributesSample Code Trying to import Foo results in a SyntaxError: <code>  class Foo: def __init__(self): self.foo: int = 0 def bar(self, value: int) -> None: self.spam(self.foo := value) # Invalid syntax def baz(self, value: int) -> None: self.spam(temp := value) self.foo = temp def spam(self, value: int) -> None: """"""Do something with value."""""" self.spam(self.foo := value) ^SyntaxError: cannot use assignment expressions with attribute",Why can't Python's walrus operator be used to set instance attributes?
How to make feature activity line chart with matplotlib/seaborn in python?," I have retail beef ad counts time series data, and I intend to make stacked line chart aim to show On a three-week average basis, quantity of average ads that grocers posted per store last week. To do so, I managed to aggregate data for plotting and tried to make line chart that I want. The main motivation is based on context of the problem and desired plot. In my attempt, I couldn't get very nice line chart because it is not informative to understand. I am wondering how can I achieve this goal in matplotlib. Can anyone suggest me what should I do from my current attempt? Any thoughts?reproducible data and current attemptHere is minimal reproducible data that I used in my current attempt: Current Resultbut I couldn't get correct line chart that I expected, I want to reproduce the plot from this site. Is that doable to achieve this? Any idea?desired plothere is the example desired plot that I want to make from this minimal reproducible data:I don't know how should make changes for my current attempt to get my desired plot above. Can anyone know any possible way of doing this in matplotlib? what else should I do? Any possible help would be appreciated. Thanks <code>  import pandas as pdimport matplotlib.pyplot as pltimport matplotlib.dates as mdatesimport seaborn as snsfrom datetime import timedelta, datetimeurl = 'https://gist.githubusercontent.com/adamFlyn/96e68902d8f71ad62a4d3cda135507ad/raw/4761264cbd55c81cf003a4219fea6a24740d7ce9/df.csv'df = pd.read_csv(url, parse_dates=['date'])df.drop(columns=['Unnamed: 0'], inplace=True)df_grp = df.groupby(['date', 'retail_item']).agg({'number_of_ads': 'sum'})df_grp[""percentage""] = df_grp.groupby(level=0).apply(lambda x:100 * x / float(x.sum()))df_grp = df_grp.reset_index(level=[0,1])for item in df_grp['retail_item'].unique(): dd = df_grp[df_grp['retail_item'] == item].groupby(['date', 'percentage'])[['number_of_ads']].sum().reset_index(level=[0,1]) dd['weakly_change'] = dd[['percentage']].rolling(7).mean() fig, ax = plt.subplots(figsize=(8, 6), dpi=144) sns.lineplot(dd.index, 'weakly_change', data=dd, ax=ax) ax.set_xlim(dd.index.min(), dd.index.max()) ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))plt.gcf().autofmt_xdate()plt.style.use('ggplot')plt.xticks(rotation=90)plt.show()",How to create a min-max lineplot by month
How to create a min-max lineplot by month?," I have retail beef ad counts time series data, and I intend to make stacked line chart aim to show On a three-week average basis, quantity of average ads that grocers posted per store last week. To do so, I managed to aggregate data for plotting and tried to make line chart that I want. The main motivation is based on context of the problem and desired plot. In my attempt, I couldn't get very nice line chart because it is not informative to understand. I am wondering how can I achieve this goal in matplotlib. Can anyone suggest me what should I do from my current attempt? Any thoughts?reproducible data and current attemptHere is minimal reproducible data that I used in my current attempt: Current Resultbut I couldn't get correct line chart that I expected, I want to reproduce the plot from this site. Is that doable to achieve this? Any idea?desired plothere is the example desired plot that I want to make from this minimal reproducible data:I don't know how should make changes for my current attempt to get my desired plot above. Can anyone know any possible way of doing this in matplotlib? what else should I do? Any possible help would be appreciated. Thanks <code>  import pandas as pdimport matplotlib.pyplot as pltimport matplotlib.dates as mdatesimport seaborn as snsfrom datetime import timedelta, datetimeurl = 'https://gist.githubusercontent.com/adamFlyn/96e68902d8f71ad62a4d3cda135507ad/raw/4761264cbd55c81cf003a4219fea6a24740d7ce9/df.csv'df = pd.read_csv(url, parse_dates=['date'])df.drop(columns=['Unnamed: 0'], inplace=True)df_grp = df.groupby(['date', 'retail_item']).agg({'number_of_ads': 'sum'})df_grp[""percentage""] = df_grp.groupby(level=0).apply(lambda x:100 * x / float(x.sum()))df_grp = df_grp.reset_index(level=[0,1])for item in df_grp['retail_item'].unique(): dd = df_grp[df_grp['retail_item'] == item].groupby(['date', 'percentage'])[['number_of_ads']].sum().reset_index(level=[0,1]) dd['weakly_change'] = dd[['percentage']].rolling(7).mean() fig, ax = plt.subplots(figsize=(8, 6), dpi=144) sns.lineplot(dd.index, 'weakly_change', data=dd, ax=ax) ax.set_xlim(dd.index.min(), dd.index.max()) ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))plt.gcf().autofmt_xdate()plt.style.use('ggplot')plt.xticks(rotation=90)plt.show()",How to create a min-max lineplot by month
PyQt5: finplot as a widget in layout," I am trying to add finplot, https://pypi.org/project/finplot/, as a widget to one of the layouts in my UI. I created a widget for finplot and added it to the widgets in the layout but I get the following error: Here is the code that I used. <code>  self.tab1.layout.addWidget(self.tab1.fplt_widget)TypeError: addWidget(self, QWidget, stretch: int = 0, alignment: Union[Qt.Alignment, Qt.AlignmentFlag] = Qt.Alignment()): argument 1 has unexpected type 'PlotItem' import sysfrom PyQt5.QtWidgets import *import finplot as fpltimport yfinance# Creating the main windowclass App(QMainWindow): def __init__(self): super().__init__() self.title = 'PyQt5 - QTabWidget' self.left = 0 self.top = 0 self.width = 600 self.height = 400 self.setWindowTitle(self.title) self.setGeometry(self.left, self.top, self.width, self.height) self.tab_widget = MyTabWidget(self) self.setCentralWidget(self.tab_widget) self.show() # Creating tab widgetsclass MyTabWidget(QWidget): def __init__(self, parent): super(QWidget, self).__init__(parent) self.layout = QVBoxLayout(self) # Initialize tab screen self.tabs = QTabWidget() self.tabs.setTabPosition(QTabWidget.East) self.tabs.setMovable(True) self.tab1 = QWidget() self.tabs.resize(600, 400) # Add tabs self.tabs.addTab(self.tab1, ""tab1"") self.tab1.layout = QVBoxLayout(self) self.tab1.label = QLabel(""AAPL"") self.tab1.df = yfinance.download('AAPL') self.tab1.fplt_widget = fplt.create_plot_widget(self.window()) fplt.candlestick_ochl(self.tab1.df[['Open', 'Close', 'High', 'Low']]) self.tab1.layout.addWidget(self.tab1.label) self.tab1.layout.addWidget(self.tab1.fplt_widget) self.tab1.setLayout(self.tab1.layout) # Add tabs to widget self.layout.addWidget(self.tabs) self.setLayout(self.layout)if __name__ == '__main__': app = QApplication(sys.argv) ex = App() sys.exit(app.exec_())",finplot as a widget in layout
Algorithm: What set of tiles of length N can be used to generate the most amount of unique words?," I'm trying to create a function best_tiles which takes in the number of tiles in your hand and returns the set of tiles that allows you to produce the most number of unique English-valid words, assuming that you can only use each tile once.For example, with the set of tiles in your hand (A, B, C) you can produce the words, CAB, BAC, AB, and BA (all of these are English words), so you can spell 4 unique words with that set. With (B, A, A), you can spell 5 words: ABA, BAA, AA, AB, and BA. The goal is to find the set of letters which allows you to spell the most number of English-Valid words (without replacement).So if 5 was the maximum number of words that could be spelled with any combination of letters for N = 3, running best_tiles( n = 3 ) would return B, A, A.I'm wondering how to implement this efficiently? My current approach doesn't scale well at all with number of letters.I read in a wordlist. In this case, I'm using enable.txt here: https://www.wordgamedictionary.com/enable/ I create a function word_in_tiles h/t smack89 which returns whether it is possible to construct a word given a tile set: I then create a function get_all_words which produces a list of all the possible words one can spell from a word list and a tile set. The extremely naive approach for identifying which tileset is the ""best"" for three letters is the following:I first create a list of every possible combination for a given length. So for length 3, I'd do: Then remove the values with no length and sort by the length of the result: GetMaxLength(res)You get that, for three letters, the tile-set that produces the most english valid words is T A E which can produce the following words ['AE', 'AT', 'ATE', 'EAT', 'ET', 'ETA', 'TA', 'TAE', 'TEA']I'd like to be able to scale this up to as big as N = 15. What is the best procedure for doing this? <code>  import ospath = ""enable.txt""words = []with open(path , encoding='utf8') as f: for values in f: words.append(list(values.strip().upper())) def word_in_tiles(word, tiles): tiles_counter = collections.Counter(tiles) return all(tiles_counter.get(ch, 0) == cnt for ch,cnt in collections.Counter(word).items()) def get_all_words(tile_set, words): # words is a word list return [i for i in words if word_in_tiles(i, tile_set)] import stringimport itertools letters = string.ascii_lowercaseall_three_letter_combinations = list(itertools.combinations_with_replacement(letters, 3))# Create a list of only words that are three letters are lessthree_letter = [i for i in words if len(i) <= 3]sequence_dict = dict() for i in all_three_letter_combinations: string_test = """".join(i).upper() sequence_dict[i] = get_all_words(string_test, three_letter) res = {k: v for k, v in sequence_dict.items() if len(v) >= 1}def GetMaxLength(res): return max((len(v), v, k) for k, v in res.items())[1:]",Algorithm: What set of tiles of length N can be used to generate the most amount of Scrabble-valid words?
"Is there a way to find multiple varaibles in a String, without iterating over it multiple times?"," I need to find if items from a list appear in a string, and then add the items to a different list. This code works: However, the code iterates over line (which could be long) multiple times- as many times as there are item in _legal (which could be a lot). That's too slow for me, and I'm searching for a way to do it faster. line doesn't have any specific format, so using .split() couldn't work, as far as I know.Edit: changed line so that it better represents the problems. <code>  data =[]line = 'akhgvfalfhda.dhgfa.lidhfalihflaih**Thing1**aoufgyafkugafkjhafkjhflahfklh**Thing2**dlfkhalfhafli...'_legal = ['thing1', 'thing2', 'thing3', 'thing4',...] for i in _legal: if i in line: data.append(i)",Finding multiple substrings in a string without iterating over it multiple times
Pandas groupby and appending entire dataframes," I have a dataframe df and a column df['table'] such that each item in df['table'] is another dataframe with the same headers/number of columns. I was wondering if there's a way to do a groupby like this:Original dataframe: After groupby: I found this code snippet to do a groupby and lambda for strings in a dataframe, but haven't been able to figure out how to append entire dataframes in a groupby. I've also tried df['table'] = df.groupby(['name'])['HTML'].apply(list), but that gives me a df['table'] of all NaN.Thanks for your help!! <code>  name tableBob Pandas df1Joe Pandas df2Bob Pandas df3Bob Pandas df4Emily Pandas df5 name tableBob Pandas df containing the appended df1, df3, and df4Joe Pandas df2Emily Pandas df5 df['table'] = df.groupby(['name'])['table'].transform(lambda x : ' '.join(x)) ","How to aggregate, combining dataframes, with pandas groupby"
"Seasonal_decompose function processing a pandas data frame gives ""Index(...) must be called with..."" error"," Error Test Data <code>  from statsmodels.tsa.seasonal import seasonal_decompose def seasonal_decomp(df, model=""additive""): seasonal_df = None seasonal_df = seasonal_decompose(df, model='additive') return seasonal_df seasonal_decomp(df) ---------------------------------------------------------------------------TypeError Traceback (most recent call last)<ipython-input-93-00543113a58a> in <module>----> 1 seasonal_decompose(df, model='additive')e:\Anaconda3\lib\site-packages\pandas\util\_decorators.py in wrapper(*args, **kwargs) 197 else: 198 kwargs[new_arg_name] = new_arg_value--> 199 return func(*args, **kwargs) 200 201 return cast(F, wrapper)e:\Anaconda3\lib\site-packages\statsmodels\tsa\seasonal.py in seasonal_decompose(x, model, filt, period, two_sided, extrapolate_trend) 185 for s, name in zip((seasonal, trend, resid, x), 186 ('seasonal', 'trend', 'resid', None)):--> 187 results.append(pw.wrap(s.squeeze(), columns=name)) 188 return DecomposeResult(seasonal=results[0], trend=results[1], 189 resid=results[2], observed=results[3])e:\Anaconda3\lib\site-packages\statsmodels\tools\validation\validation.py in wrap(self, obj, columns, append, trim_start, trim_end) 216 new.append(append if c is None else str(c) + '_' + append) 217 columns = new--> 218 return pd.DataFrame(obj, columns=columns, index=index) 219 else: 220 raise ValueError('Can only wrap 1 or 2-d array_like')e:\Anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy) 495 mgr = init_dict({data.name: data}, index, columns, dtype=dtype) 496 else:--> 497 mgr = init_ndarray(data, index, columns, dtype=dtype, copy=copy) 498 499 # For data is list-like, or Iterable (will consume into list)e:\Anaconda3\lib\site-packages\pandas\core\internals\construction.py in init_ndarray(values, index, columns, dtype, copy) 201 202 # _prep_ndarray ensures that values.ndim == 2 at this point--> 203 index, columns = _get_axes( 204 values.shape[0], values.shape[1], index=index, columns=columns 205 )e:\Anaconda3\lib\site-packages\pandas\core\internals\construction.py in _get_axes(N, K, index, columns) 460 columns = ibase.default_index(K) 461 else:--> 462 columns = ensure_index(columns) 463 return index, columns 464 e:\Anaconda3\lib\site-packages\pandas\core\indexes\base.py in ensure_index(index_like, copy) 5612 index_like = copy_func(index_like) 5613 -> 5614 return Index(index_like) 5615 5616 e:\Anaconda3\lib\site-packages\pandas\core\indexes\base.py in __new__(cls, data, dtype, copy, name, tupleize_cols, **kwargs) 409 410 elif data is None or is_scalar(data):--> 411 raise cls._scalar_data_error(data) 412 elif hasattr(data, ""__array__""): 413 return Index(np.asarray(data), dtype=dtype, copy=copy, name=name, **kwargs)TypeError: Index(...) must be called with a collection of some kind, 'seasonal' was passed df = pd.DataFrame.from_dict(data, orient='index')data = {pd.Timestamp('2020-01-23 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-01-24 00:00:00'): {'LA': 1.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-01-25 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-01-26 00:00:00'): {'LA': 3.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-01-27 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-01-28 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-01-29 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-01-30 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 1.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-01-31 00:00:00'): {'LA': 2.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 2.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-01 00:00:00'): {'LA': 1.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-02 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 1.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-03 00:00:00'): {'LA': 3.0, 'NY': 0.0, 'Miami': 1.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-04 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-05 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-06 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-07 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-08 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-09 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-10 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-11 00:00:00'): {'LA': 1.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-12 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-13 00:00:00'): {'LA': 1.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-14 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-15 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-16 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-17 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-18 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-19 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-20 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-21 00:00:00'): {'LA': 2.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-22 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-23 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-24 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-25 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-26 00:00:00'): {'LA': 0.0, 'NY': 1.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-27 00:00:00'): {'LA': 1.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-28 00:00:00'): {'LA': 0.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-02-29 00:00:00'): {'LA': 8.0, 'NY': 1.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-03-01 00:00:00'): {'LA': 6.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-03-02 00:00:00'): {'LA': 23.0, 'NY': 0.0, 'Miami': 2.0, 'Seattle': 1.0, 'San Diego': 0.0}, pd.Timestamp('2020-03-03 00:00:00'): {'LA': 20.0, 'NY': 0.0, 'Miami': 0.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-03-04 00:00:00'): {'LA': 31.0, 'NY': 2.0, 'Miami': 23.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-03-05 00:00:00'): {'LA': 70.0, 'NY': 0.0, 'Miami': 2.0, 'Seattle': 1.0, 'San Diego': 1.0}, pd.Timestamp('2020-03-06 00:00:00'): {'LA': 48.0, 'NY': 9.0, 'Miami': 1.0, 'Seattle': 9.0, 'San Diego': 0.0}, pd.Timestamp('2020-03-07 00:00:00'): {'LA': 115.0, 'NY': 0.0, 'Miami': 3.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-03-08 00:00:00'): {'LA': 114.0, 'NY': 7.0, 'Miami': 5.0, 'Seattle': 4.0, 'San Diego': 2.0}, pd.Timestamp('2020-03-09 00:00:00'): {'LA': 68.0, 'NY': 5.0, 'Miami': 4.0, 'Seattle': 0.0, 'San Diego': 0.0}, pd.Timestamp('2020-03-10 00:00:00'): {'LA': 192.0, 'NY': 6.0, 'Miami': 13.0, 'Seattle': 3.0, 'San Diego': 4.0}, pd.Timestamp('2020-03-11 00:00:00'): {'LA': 398.0, 'NY': 7.0, 'Miami': 6.0, 'Seattle': 0.0, 'San Diego': 6.0}, pd.Timestamp('2020-03-12 00:00:00'): {'LA': 452.0, 'NY': 14.0, 'Miami': 11.0, 'Seattle': 8.0, 'San Diego': 4.0}, pd.Timestamp('2020-03-13 00:00:00'): {'LA': 596.0, 'NY': 99.0, 'Miami': 9.0, 'Seattle': 17.0, 'San Diego': 7.0}, pd.Timestamp('2020-03-14 00:00:00'): {'LA': 713.0, 'NY': 0.0, 'Miami': 20.0, 'Seattle': 14.0, 'San Diego': 14.0}, pd.Timestamp('2020-03-15 00:00:00'): {'LA': 98.0, 'NY': 11.0, 'Miami': 11.0, 'Seattle': 4.0, 'San Diego': 13.0}, pd.Timestamp('2020-03-16 00:00:00'): {'LA': 1392.0, 'NY': 38.0, 'Miami': 6.0, 'Seattle': 27.0, 'San Diego': 11.0}, pd.Timestamp('2020-03-17 00:00:00'): {'LA': 1781.0, 'NY': 121.0, 'Miami': 23.0, 'Seattle': 24.0, 'San Diego': 0.0}, pd.Timestamp('2020-03-18 00:00:00'): {'LA': 2776.0, 'NY': 51.0, 'Miami': 14.0, 'Seattle': 33.0, 'San Diego': 54.0}, pd.Timestamp('2020-03-19 00:00:00'): {'LA': 5240.0, 'NY': 249.0, 'Miami': 38.0, 'Seattle': 52.0, 'San Diego': 34.0}, pd.Timestamp('2020-03-20 00:00:00'): {'LA': 5322.0, 'NY': 172.0, 'Miami': 50.0, 'Seattle': 54.0, 'San Diego': 52.0}, pd.Timestamp('2020-03-21 00:00:00'): {'LA': 6346.0, 'NY': 228.0, 'Miami': 86.0, 'Seattle': 53.0, 'San Diego': 38.0}, pd.Timestamp('2020-03-22 00:00:00'): {'LA': 7936.0, 'NY': 525.0, 'Miami': 66.0, 'Seattle': 61.0, 'San Diego': 34.0}}",How to use statsmodels.tsa.seasonal.seasonal_decompose with a pandas dataframe
How can I trap StopIteration exception in the yield-calling function?," A generator-returning function (i.e. one with a yield statement in it) in one of our libraries fails some tests due to an unhandled StopIteration exception. For convenience, in this post I'll refer to this function as buggy.I have not been able to find a way for buggy to prevent the exception (without affecting the function's normal operation). Similarly, I have not found a way to trap the exception (with a try/except) within buggy.(Client code using buggy can trap this exception, but this happens too late, because the code that has the information necessary to properly handle the condition leading to this exception is the buggy function.)The actual code and test case I am working with are far too complicated to post here, so I have created a very simple, but also extremely artificial toy example that illustrates the problem.First, the module with the buggy function: As indicated by the comment, the use of the csv module (from the Python 3.x standard library) is an essential feature of this problem1.The next file for the example is a script that is meant to stand in for ""client code"". In other word, this script's ""real purpose"" beyond this example is largely irrelevant. Its role in the example is to provide a simple, reliable way to elicit the problem with the buggy function. (Some of its code could be repurposed for a test case in a test suite, for example.) The script takes the path to a CSV file as a mandatory argument, and an optional second argument. If the second argument is ommitted, or it is anything other than the string ""first"", the script will print to stdout the information in the CSV file, but in TSV format. If the second argument is the string ""first"", only the information in the first row will be so printed.The StopIteration exception I am trying to trap arises when myscript.py script is invoked with an empty file and the string ""first"" as arguments2.Here is an example of this code in action: Q: How can I prevent or trap this StopIteration exception in the lexical scope of the buggy function?IMPORTANT: Please keep in mind that, in the example given above, the myscript.py script is stand-in for ""client code"", and is therefore outside of our control. This means that any approach that would require changing the myscript.py script would not solve the actual real-world problem, and therefore it would not be an acceptable answer to this question.One important difference between the simple example shown above and our actual situation is that in our case, the problematic input stream does not come from an empty file. The problem arises in cases where buggy (or, rather, its real-world counterpart) reaches the end of this stream ""too early"", so to speak.I think it may be enough if I could test whether either stream is at its end, before the for row in reader: line, but I have not figured a way to do this either. Testing whether the value returned by stream.read(1) is 0 or 1 will tell me if stream is at its end, but in the latter case stream's internal pointer will be left pointing one byte too far into csvfile's content. (Neither stream.seek(-1, 1) nor stream.tell() work at this point.)Lastly, to anyone who would like post an answer to this question: it would be most efficient if you were to take advantage of the example code I have provided above to test your proposal before posting it.EDIT: One variation of mymod.py that I tried was this: This variation fails with pretty much the same error message as does the original version.When I first read @mcernak's proposal, I thought that it was pretty similar to the variation above, and therefore expected it to fail too. Then I was pleasantly surprised to discover that this is not the case! Therefore, as of now, there is one definite candidate to get bounty. That said, I would love to understand why the variation above fails to trap the exception, while @mcernak's succeeds.1 The actual case I'm dealing with is legacy code; switching from the csv module to some alternative is not an option for us in the short term.2 Please, disregard entirely the question of what this demonstration script's ""right response should be"" when it gets invoked with an empty file and the string ""first"" as arguments. The particular combination of inputs that elicits the StopIteration exception in this post's demonstration does not represent the real-world condition that causes our code to emit the problematic StopIteration exception. Therefore, the ""correct response"", whatever that may be, of the demonstration script to the empty file plus ""first"" string combination would be irrelevant to the real-world problem I am dealing with. <code>  # mymod.pyimport csv # essential!def buggy(csvfile): with open(csvfile) as stream: reader = csv.reader(stream) # how to test *here* if either stream is at its end? for row in reader: yield row #!/usr/bin/env python3# myscript.pyimport sysimport mymoddef print_row(row): print(*row, sep='\t')def main(csvfile, mode=None): if mode == 'first': print_row(next(mymod.buggy(csvfile))) else: for row in mymod.buggy(csvfile): print_row(row)if __name__ == '__main__': main(*sys.argv[1:]) % cat ok_input.csv1,2,34,5,67,8,9% ./myscript.py ok_input.csv1 2 34 5 67 8 9% ./myscript.py ok_input.csv first1 2 3% cat empty_input.csv# no output (of course)% ./myscript.py empty_input.csv# no output (as desired)% ./myscript.py empty_input.csv firstTraceback (most recent call last): File ""./myscript.py"", line 19, in <module> main(*sys.argv[1:]) File ""./myscript.py"", line 13, in main print_row(next(mymod.buggy(csvfile)))StopIteration import csv # essential!def buggy(csvfile): with open(csvfile) as stream: reader = csv.reader(stream) try: firstrow = next(reader) except StopIteration: firstrow = None if firstrow != None: yield firstrow for row in reader: yield row",How can I prevent or trap StopIteration exception in the yield-calling function?
find version of jupyter notebook from within notebook," I wish to return the version of Jupyter Notebook from within a cell of a notebook.For example, to get the python version, I run: or to get the pandas version: I have tried: and several other, related forms (including capitalizing the first letters), but get errors that (for example):NameError: name 'jupyter' is not definedI am aware of other ways (e.g. clicking on Help>About in the GUI menu; using the conda command line) but I want to automate documentation of all package versions.If it matters, I am running v6.1.1 of Notebook in a Python 3.7.3 environment. <code>  from platform import python_versionpython_version() pd.__version__ notebook.version()ipython.version()jupyter.version()",How to find the version of jupyter notebook from within the notebook
in seaborn kde plot what does levels mean?," I am trying to make a contour plot of my 2d data. However, I would like to input the contours manually. I found the ""levels"" option in seaborn.kde documentation, where I can define the levels for contours manually. However, I have no idea what these levels mean. The documentation gives this definition -Levels correspond to iso-proportions of the density.What does iso-proportions of density mean?Are there any references that I could read up on this? <code> ",What does levels mean in seaborn kde plot?
Counting vowels in an array - Python," I know I'm close to figuring this out but I've been wracking my brain and can't think of what's going wrong here. I need to count the number of vowels in the array of nameList using the vowelList array, and currently it's outputting 22, which is not the correct number of vowels.Incidentally, 22 is double the length of the array nameList, but I can't see any reason what I wrote would be outputting double the array length. Any help would be appreciated. Not looking for the answer, for a nudge in the right direction. <code>  nameList = [ ""Euclid"", ""Archimedes"", ""Newton"",""Descartes"", ""Fermat"", ""Turing"", ""Euler"", ""Einstein"", ""Boole"", ""Fibonacci"", ""Nash""]vowelList = ['A', 'a', 'E', 'e', 'I', 'i', 'O', 'o', 'U','u']z=0counter = 0for k in nameList: i = 0 for q in vowelList: counter+=nameList[z].count(vowelList[i]) i+=1 z=+1print(""The number of vowels in the list is"",counter)",Counting vowels in an array
Numpy - map string to ints based on ASCII," I have a string like this: Based on string.ascii_lowercase, I'd like to create a new array which looks like this: My solution for this problem was to do the following: Output: [15, 24, 19, 7, 14, 13]But I'm looking for a more efficient way, without using loops. How can I do that in a vectorized way? <code>  word = 'python' [15, 24, 19, 7, 14, 13] alphabet = {char: i for i, char in enumerate(string.ascii_lowercase)}indices = [alphabet[char] for char in word]print(indices)",Map string to integers with position in ASCII table
Map string to integers of position in ASCII table," I have a string like this: Based on string.ascii_lowercase, I'd like to create a new array which looks like this: My solution for this problem was to do the following: Output: [15, 24, 19, 7, 14, 13]But I'm looking for a more efficient way, without using loops. How can I do that in a vectorized way? <code>  word = 'python' [15, 24, 19, 7, 14, 13] alphabet = {char: i for i, char in enumerate(string.ascii_lowercase)}indices = [alphabet[char] for char in word]print(indices)",Map string to integers with position in ASCII table
pandas read csv ingore ending semicolon of last column," My data file looks like this: As can be seen, the last column ends with a semicolon, so when I read into pandas, the column is inferred as type object (ending with the semicolon. How do I make pandas ignore that semicolon? <code>  data.txtuser,activity,timestamp,x-axis,y-axis,z-axis0,33,Jogging,49105962326000,-0.6946376999999999,12.680544,0.50395286;1,33,Jogging,49106062271000,5.012288,11.264028,0.95342433;2,33,Jogging,49106112167000,4.903325,10.882658000000001,-0.08172209;3,33,Jogging,49106222305000,-0.61291564,18.496431,3.0237172; df = pd.read_csv('data.txt')df user activity timestamp x-axis y-axis z-axis0 33 Jogging 49105962326000 -0.694638 12.680544 0.50395286;1 33 Jogging 49106062271000 5.012288 11.264028 0.95342433;2 33 Jogging 49106112167000 4.903325 10.882658 -0.08172209;3 33 Jogging 49106222305000 -0.612916 18.496431 3.0237172;",pandas read csv ignore ending semicolon of last column
comparing csv files on local machine with those on server," I want to compare all CSV files kept on my local machine to files kept on a server. The folder structure is the same for both of them. I only want to do a data comparison and not metadata (like time of creation, etc). I am using filecmp but it seems to perform metadata comparison. Is there a way to do what I want? <code>  import filecmpcomparison = filecmp.dircmp(dir_local, dir_server)comparison.report_full_closure()",Compare CSV files content with filecmp and ignore metadata
Using a Data Converter to Display 3D Image Dataset," I would like to write a data converter tool. I need analyze the bitstream in a file to display the 2D cross-sections of a 3D volume.The dataset I am trying to view can be found here: https://figshare.com/articles/SSOCT_test_dataset_for_OCTproZ/12356705.It's the file titled: burned_wood_with_tape_1664x512x256_12bit.raw (832 MB)Would extremely appreciate some direction. Willing to award a bounty if I could get some software to display the dataset as images using a data conversion.As I'm totally new to this concept, I don't have code to show for this problem. However, here's a little something I tried using inspiration from other questions on SO: <code>  import rawpyimport imageiopath = ""Datasets/burned_wood_with_tape_1664x512x256_12bit.raw""for item in path: item_path = path + item raw = rawpy.imread(item_path) rgb = raw.postprocess() rawpy.imshow(rgb)",Using a Data Converter to Display 3D Volume as Images
Using a Data Converter to Display 3D Volume Dataset," I would like to write a data converter tool. I need analyze the bitstream in a file to display the 2D cross-sections of a 3D volume.The dataset I am trying to view can be found here: https://figshare.com/articles/SSOCT_test_dataset_for_OCTproZ/12356705.It's the file titled: burned_wood_with_tape_1664x512x256_12bit.raw (832 MB)Would extremely appreciate some direction. Willing to award a bounty if I could get some software to display the dataset as images using a data conversion.As I'm totally new to this concept, I don't have code to show for this problem. However, here's a little something I tried using inspiration from other questions on SO: <code>  import rawpyimport imageiopath = ""Datasets/burned_wood_with_tape_1664x512x256_12bit.raw""for item in path: item_path = path + item raw = rawpy.imread(item_path) rgb = raw.postprocess() rawpy.imshow(rgb)",Using a Data Converter to Display 3D Volume as Images
Using a Data Converter to Display 3D Volume Image," I would like to write a data converter tool. I need analyze the bitstream in a file to display the 2D cross-sections of a 3D volume.The dataset I am trying to view can be found here: https://figshare.com/articles/SSOCT_test_dataset_for_OCTproZ/12356705.It's the file titled: burned_wood_with_tape_1664x512x256_12bit.raw (832 MB)Would extremely appreciate some direction. Willing to award a bounty if I could get some software to display the dataset as images using a data conversion.As I'm totally new to this concept, I don't have code to show for this problem. However, here's a little something I tried using inspiration from other questions on SO: <code>  import rawpyimport imageiopath = ""Datasets/burned_wood_with_tape_1664x512x256_12bit.raw""for item in path: item_path = path + item raw = rawpy.imread(item_path) rgb = raw.postprocess() rawpy.imshow(rgb)",Using a Data Converter to Display 3D Volume as Images
How does pathlib Parallel deal with global variables?," My code looks something like this: My question is whether LARGE_MODEL will be pickled/unpickled with each iteration of the loop. And if so, how can I make sure each worker caches it instead (if that's possible)? <code>  from joblib import Parallel, delayed# prediction model - 10s of megabytes on diskLARGE_MODEL = load_model('path/to/model')file_paths = glob('path/to/files/*')def do_thing(file_path): pred = LARGE_MODEL.predict(load_image(file_path)) return predParallel(n_jobs=2)(delayed(do_thing)(fp) for fp in file_paths)",How does joblib.Parallel deal with global variables?
logical and with nested boolean arrays," I have one (very long) boolean array a with k True entries, and one boolean array b of length k. I would like to get a boolean array c that is True if and only if a ""and"" b are True: This here uses a loop, but I'm thinking there must a faster way with boolean indexing, but I can't quite get to it.Any hints? <code>  import numpya = numpy.array([False, False, True, False, True, False])b = numpy.array([True, False])assert numpy.sum(a) == len(b) # guaranteedc = numpy.zeros(len(a), dtype=bool)idx_b = 0for k in range(len(a)): if a[k]: if b[idx_b]: c[k] = True idx_b += 1print(c) [False False True False False False]",logical_and with nested boolean arrays
"What's is meant by the `dir()` built-in function returns ""(some of) the attributes of the given object""?"," I wonder whether the documentation is wrong for the dir() built-in function. In particular, what object attributes may not be part of the list returned by dir()?For both class objects and other objects, the documentation says that the list contains ""its attributes"", which means a full set (and not ""some of"") of attributes?Python v. 3.9 @ macOS 10.15.7 <code>  Help on built-in function dir in module builtins:dir(...) dir([object]) -> list of strings If called without an argument, return the names in the current scope. Else, return an alphabetized list of names comprising (some of) the attributes of the given object, and of attributes reachable from it. If the object supplies a method named __dir__, it will be used; otherwise the default dir() logic is used and returns: for a module object: the module's attributes. for a class object: its attributes, and recursively the attributes of its bases. for any other object: its attributes, its class's attributes, and recursively the attributes of its class's base classes.","What's is meant by the dir() built-in function returns ""(some of) the attributes of the given object""?"
Keras vertical ensemble model with condition after first model," I have trained two separate modelsModelA: Checks if the input text is related to my work (Binary Classifier [related/not-related])ModelB: Classifier of related texts (Classifier [good/normal/bad]). Only the related texts are relayed to this model from ModelAI wantModelC: Ensemble classifier that outputs [good/normal/bad/not-related]I'll be training in batches. And there can be mix of not-related and good/normal/bad in one batch. I need them separated.Some pseudo code of what i need I don't know how to include the if logic inside the models inference. How can I achieve this? <code>  # Output of modelA will be a vector I presume `(1, None)` where `None` is batchdef ModelC.predict(input): outputA = ModelA(input) if outputA == 'not-related': return outputA return ModelB(outputA)",Keras vertical ensemble model with condition in between
How to do you list local profiles with boto3 from ~/.aws/.credentials file and ~/.aws/.config file?," I would like to list all of my local profiles using boto3, as I think boto3 is not picking up my credentials correctly.I have tried the following: Which doesn't give me a list, but a property object. <code>  import boto3boto3.Session.available_profiles",How do you list local profiles with boto3 from ~/.aws/.credentials and ~/.aws/.config files?
"Catch correctly a ""No hostkey for host 1.2.3.4 found"" exception with pysftp and paramiko"," I'd like to catch nicely the error when ""No hostkey for host *** is found"" and give an appropriate message to the end user. I tried this: but unfortunately the output is not very nice: How to nicely avoid these last lines / this ""exception ignored"" with a try: except:? <code>  import pysftp, paramikotry: with pysftp.Connection('1.2.3.4', username='root', password='') as sftp: sftp.listdir()except paramiko.ssh_exception.SSHException as e: print('SSH error, you need to add the public key of your remote in your local known_hosts file first.', e) SSH error, you need to add the public key of your remote in your local known_hosts file first. No hostkey for host 1.2.3.4 found.Exception ignored in: <function Connection.__del__ at 0x00000000036B6D38>Traceback (most recent call last): File ""C:\Python37\lib\site-packages\pysftp\__init__.py"", line 1013, in __del__ self.close() File ""C:\Python37\lib\site-packages\pysftp\__init__.py"", line 784, in close if self._sftp_live:AttributeError: 'Connection' object has no attribute '_sftp_live'","""'Connection' object has no attribute '_sftp_live'"" when pysftp connection fails"
Image sequences training with CNN and RNN," I'm making my first steps learning Deep Learning. I am trying to do Activity Recognition from images sequences (frames) of videos. As a result i am facing a problem with the training procedure.Firstly i need to determine the architecture of my images folders: So the problem is that each folder can have a different number of frames so I get confused both with the input shape of the model and the timesteps which I should use.I am creating a model (as you see bellow) with time distirbuted CNN(pre trained VGG16) and LSTM that takes an input all the frames of all classes with the coresponding labels (in the above example making food would be the coresponding label to p1_rgb_frame1 etc.) and the final shape of x_train is (9000,200,200,3) where 9000 coresponds to all frames from all classes, 200 is height & width and 3 the channel of images. I am reshaping this data to (9000,1,200,200,3) in order to be used as input to the model.I am wondering and worried that I do not pass a proper timestep, as a result a wrong training , i have val_acc ~ 98% but when testing with different dataset is much lower. Can you suggest another way to do it more efficient? <code>  Making Food -> p1 -> rgb_frame1.png,rgb_frame2.png ... rgb_frame200.pngMaking Food -> p2 -> rgb_frame1.png,rgb_frame2.png ... rgb_frame280.png ... ... ...Taking Medicine -> p1 -> rgb_frame1.png,rgb_frame2.png...rgbframe500.png etc.. x = base_model.output x = Flatten()(x) features = Dense(64, activation='relu')(x) conv_model = Model(inputs=base_model.input, outputs=features) for layer in base_model.layers: layer.trainable = False model = Sequential() model.add(TimeDistributed(conv_model, input_shape=(None,200,200,3))) model.add(LSTM(32, return_sequences=True)) model.add(LSTM(16))",Image sequence training with CNN and RNN
Python Asyncio - what is the difference between async with lock vs await lock, I have seen two ways of acquiring the asyncio Lock: and What is the difference between them? <code>  async def main(lock): async with lock: async.sleep(100) async def main(lock): with await lock: async.sleep(100),"What is the difference between ""async with lock"" and ""with await lock""?"
What is the difference between async with lock vs await lock, I have seen two ways of acquiring the asyncio Lock: and What is the difference between them? <code>  async def main(lock): async with lock: async.sleep(100) async def main(lock): with await lock: async.sleep(100),"What is the difference between ""async with lock"" and ""with await lock""?"
Plotly Python Button to toggle traces similar to clicking them in legend," I'm using python and creating standalone html files with interactive plots (no Dash). I have been able to build a plotly plot with buttons that can toggle the visibility of traces in the plot. However, this functionality removes the traces from the legend as well. What I would like is to be able to keep the functionality of the legend (click a single trace to toggle visibility) but also have a set of buttons that extends that functionality to a group of traces that I define.The goal is to be able toggle everything (or a select group) to invisible but add individual items from that group back to visible as needed.Below is an example (using modified code from this answer by vestland) to show what I am currently attempting. That example does not work how I would like. Below is a screenshot of what it looks like at first.The problem is that if I use one of those buttons, it does hide all the other traces but it also removes them from the legend so they can't be toggled back to visible.So my question becomes, is there a different value in the args list/dictionary that can be given for the functionality to match that of simply clicking a trace in the legend?Sort of related, is there some way to get the current state of visibility for each trace? <code>  import numpy as npimport pandas as pdimport plotly.graph_objects as goimport datetime# mimic OP's datasampleNPERIODS = 200np.random.seed(123)df = pd.DataFrame(np.random.randint(-10, 12, size=(NPERIODS, 4)), columns=list('ABCD'))datelist = pd.date_range(datetime.datetime(2020, 1, 1).strftime('%Y-%m-%d'), periods=NPERIODS).tolist()df['dates'] = datelist df = df.set_index(['dates'])df.index = pd.to_datetime(df.index)df.iloc[0] = 0df = df.cumsum()# set up multiple tracestraces = []buttons = []for col in df.columns: traces.append(go.Scatter(x=df.index, y=df[col], visible=True, name=col) ) buttons.append(dict(method='update', label=col, visible=True, args=[{'visible':[x == col for x in df.columns]}], args2=[{'visible':[x != col for x in df.columns]}] ) )# create the layout layout = go.Layout( updatemenus=[ dict( type='buttons', direction='right', x=0.7, y=1.3, showactive=True, buttons=buttons ) ], title=dict(text='Toggle Traces',x=0.5), showlegend=True)fig = go.Figure(data=traces,layout=layout)# add dropdown menus to the figurefig.show()",Plotly: How to toggle traces with a button similar to clicking them in legend?
pandas: How to keep the top N (only N) value in a dataframe (Pandas)," It is pandas/Dataframe, for every row, I want to keep only the top N (N=3) values and set others to nan, output is I want to get Similar to pandas: Keep only top n values and set others to 0, but I need to keep only N highest available values, otherwise the average is not correctFor the result above I want to keep first 5 only <code>  import pandas as pdimport numpy as npdata = np.array([['','day1','day2','day3','day4','day5'], ['larry',1,4,4,3,5], ['gunnar',2,-1,3,4,4], ['tin',-2,5,5, 6,7]]) df = pd.DataFrame(data=data[1:,1:], index=data[1:,0], columns=data[0,1:])print(df) day1 day2 day3 day4 day5larry 1 4 4 3 5gunnar 2 -1 3 4 4tin -2 5 5 6 7 day1 day2 day3 day4 day5larry NaN 4 4 NaN 5gunnar NaN NaN 3 4 4tin NaN 5 NaN 6 7",How to keep the only the top N values in a dataframe
"is there a c/c++ equivalent for python's ""__init__.py""?"," In Python when you create a module you create an __init__.py file (at least, usually). Is there a C/C++ equivalent of this thing? I mean is there a way to #include a directory actually including a file inside it? <code> ","Is there a C/C++ equivalent for Python's ""__init__.py""?"
"python - how to compose a nested function g=fn(...(f3(f2(f1()))...) from a list of functions [f1, f2, f3,...fn]"," QuestionIs there a readily available Pythonic way to compose a multiple nested function g = f3(f2(f1())) from a list of functions [f1, f2, f3] where there are more functions in a list.If there are a few, I may do: However when I have dozens of functions e.g layers in a deep neural network, it is un-manageable. Prefer not creating another function to compose g but finding an available way.UpdateBased on the answer from @Chris. For sequential neural network layers [ batchnorm, matmul, activation, softmaxloss ], each of which has a forward(X) method to calculate its output to the next layer, the loss function L and loss would be: <code>  g = lambda x: f3(f2(f1(x))) L = reduce(lambda f, g: lambda X: g(f(X)), [ layer.forward for layer in layers ] ) # Loss functionnetwork_loss = L(X)","How to compose a nested function g=fn(...(f3(f2(f1()))...) from a list of functions [f1, f2, f3,...fn]"
Why aren't my list elements getting swapped?," I want to swap the minimum and maximum element in a list. My code is as follows: But, when I write: As you can see, in the second case, it shows the correct output. Why not in the first case? <code>  A=[5,1,3,4,2]a=max(A)b=min(A)A[A.index(a)],A[A.index(b)]=A[A.index(b)],A[A.index(a)]print(A) #prints [5,1,3,4,2] A[0],A[1]=A[1],A[0]print(A) #prints [1,5,3,4,2]",Why aren't my list elements being swapped?
what is the most conventional way to integrate C code into a Python library using distutils?," Many well-known python libraries are basically written in C (like tensorflow or numpy) because this apparently speeds things up a lot. I was able to very easily integrate a C function into python by reading this.Doing so I can finally use distutils to access the functions of the source.c file: so that when i run python setup.py install i can install my library.However, what if i want to create a python-coded wrapper object for the functions inside source.c? Is there a way to do this without polluting the installed modules? Wandering around the internet I have seen some seemingly simple solutions using shared libraries (.so). However I would need a solution that does not involve attaching the compiled C code, but one that compiles it the first time the program is run. <code>  # setup.pyfrom distutils.core import setup, Extensiondef main(): setup( # All the other parameters... ext_modules=[ Extension(""source"", [""source.c""]) ] )if __name__ == ""__main__"": main()",What is the most conventional way to integrate C code into a Python library using distutils?
Quickly read only specific timestamp (multiple rows) from Parquet file in Python?," I have a Pandas dataframe that looks similar to this: The real dataframe is very large and for each unique timestamp, there are a few thousand rows.I want to save this dataframe to a Parquet file so that I can quickly read all the rows that have a specific datetime index, without loading the whole file or looping through it. How do I save it correctly in Python and how do I quickly read only the rows for one specific datetime?After reading, I would like to have a new dataframe that contains all the rows for that specific datetime. For example, I want to read only the rows for datetime ""2021-01-23 00:00:31.140"" from the Parquet file and receive this dataframe: I am wondering it it may be first necessary to convert the data for each timestamp into a column, like this, so it can be accessed by reading a column instead of rows? I appreciate any help, thank you very much in advance! <code>  datetime data1 data22021-01-23 00:00:31.140 a1 a22021-01-23 00:00:31.140 b1 b2 2021-01-23 00:00:31.140 c1 c22021-01-23 00:01:29.021 d1 d22021-01-23 00:02:10.540 e1 e22021-01-23 00:02:10.540 f1 f2 datetime data1 data22021-01-23 00:00:31.140 a1 a22021-01-23 00:00:31.140 b1 b2 2021-01-23 00:00:31.140 c1 c2 2021-01-23 00:00:31.140 2021-01-23 00:01:29.021 2021-01-23 00:02:10.540 ['a1', 'a2'] ['d1', 'd2'] ['e1', 'e2'] ['b1', 'b2'] NaN ['f1', 'f2'] ['c1', 'c2'] NaN NaN ",Read group of rows from Parquet file in Python Pandas / Dask?
Read only specific timestamp (multiple rows) from Parquet file in Python Pandas?," I have a Pandas dataframe that looks similar to this: The real dataframe is very large and for each unique timestamp, there are a few thousand rows.I want to save this dataframe to a Parquet file so that I can quickly read all the rows that have a specific datetime index, without loading the whole file or looping through it. How do I save it correctly in Python and how do I quickly read only the rows for one specific datetime?After reading, I would like to have a new dataframe that contains all the rows for that specific datetime. For example, I want to read only the rows for datetime ""2021-01-23 00:00:31.140"" from the Parquet file and receive this dataframe: I am wondering it it may be first necessary to convert the data for each timestamp into a column, like this, so it can be accessed by reading a column instead of rows? I appreciate any help, thank you very much in advance! <code>  datetime data1 data22021-01-23 00:00:31.140 a1 a22021-01-23 00:00:31.140 b1 b2 2021-01-23 00:00:31.140 c1 c22021-01-23 00:01:29.021 d1 d22021-01-23 00:02:10.540 e1 e22021-01-23 00:02:10.540 f1 f2 datetime data1 data22021-01-23 00:00:31.140 a1 a22021-01-23 00:00:31.140 b1 b2 2021-01-23 00:00:31.140 c1 c2 2021-01-23 00:00:31.140 2021-01-23 00:01:29.021 2021-01-23 00:02:10.540 ['a1', 'a2'] ['d1', 'd2'] ['e1', 'e2'] ['b1', 'b2'] NaN ['f1', 'f2'] ['c1', 'c2'] NaN NaN ",Read group of rows from Parquet file in Python Pandas / Dask?
Read group of rows from Parquet file in Python Pandas?," I have a Pandas dataframe that looks similar to this: The real dataframe is very large and for each unique timestamp, there are a few thousand rows.I want to save this dataframe to a Parquet file so that I can quickly read all the rows that have a specific datetime index, without loading the whole file or looping through it. How do I save it correctly in Python and how do I quickly read only the rows for one specific datetime?After reading, I would like to have a new dataframe that contains all the rows for that specific datetime. For example, I want to read only the rows for datetime ""2021-01-23 00:00:31.140"" from the Parquet file and receive this dataframe: I am wondering it it may be first necessary to convert the data for each timestamp into a column, like this, so it can be accessed by reading a column instead of rows? I appreciate any help, thank you very much in advance! <code>  datetime data1 data22021-01-23 00:00:31.140 a1 a22021-01-23 00:00:31.140 b1 b2 2021-01-23 00:00:31.140 c1 c22021-01-23 00:01:29.021 d1 d22021-01-23 00:02:10.540 e1 e22021-01-23 00:02:10.540 f1 f2 datetime data1 data22021-01-23 00:00:31.140 a1 a22021-01-23 00:00:31.140 b1 b2 2021-01-23 00:00:31.140 c1 c2 2021-01-23 00:00:31.140 2021-01-23 00:01:29.021 2021-01-23 00:02:10.540 ['a1', 'a2'] ['d1', 'd2'] ['e1', 'e2'] ['b1', 'b2'] NaN ['f1', 'f2'] ['c1', 'c2'] NaN NaN ",Read group of rows from Parquet file in Python Pandas / Dask?
Optimizing a Python Aerodynamics Calculation," Problem:I am trying to increase the speed of an aerodynamics function in Python.Function Set: Context:This function is used by Ptera Software, an open-source solver for flapping wing aerodynamics. As shown by the profile output below, it is by far the largest contributor to Ptera Software's run time.Currently, Ptera Software takes just over 3 minutes to run a typical case, and my goal is to get this below 1 minute.The function takes in a group of points, origins, terminations, and strengths. At every point, it finds the induced velocity due to the line vortices, which are characterized by the groups of origins, terminations, and strengths. If collapse is true, then the output is the cumulative velocity induced at each point due to the vortices. If false, the function outputs each vortex's contribution to the velocity at each point.During a typical run, the velocity function is called approximately 2000 times. At first, the calls involve vectors with relatively small input arguments (around 200 points, origins, terminations, and strengths). Later calls involve large input arguments (around 400 points and around 6,000 origins, terminations, and strengths). An ideal solution would be fast for all size inputs, but increasing the speed of large input calls is more important.For testing, I recommend running the following script with your own implementation of the function: Previous Attempts:My prior attempts at speeding up this function involved vectorizing it (which worked great, so I kept those changes) and trying out Numba's JIT compiler. I had mixed results with Numba. When I tried to use Numba on a modified version of the entire velocity function, my results were much slower than before. However, I found that Numba significantly sped up the cross-product and norm functions, which I implemented above.Updates:Update 1:Based on Mercury's comment (which has since been deleted), I replaced with two calls to the following function: This resulted in a speed increase from 227 s to 220 s. This is better! However, it is still not fast enough.I also have tried setting the njit fastmath flag to true, and using a numba function instead of calls to np.einsum. Neither increased the speed.Update 2:With Jrme Richard's answer, the run time is now 156 s, which is a decrease of 29%! I'm satisfied enough to accept this answer, but feel free to make other suggestions if you think you can improve on their work! <code>  import numpy as npfrom numba import njitdef calculate_velocity_induced_by_line_vortices( points, origins, terminations, strengths, collapse=True): # Expand the dimensionality of the points input. It is now of shape (N x 1 x 3). # This will allow NumPy to broadcast the upcoming subtractions. points = np.expand_dims(points, axis=1) # Define the vectors from the vortex to the points. r_1 and r_2 now both are of # shape (N x M x 3). Each row/column pair holds the vector associated with each # point/vortex pair. r_1 = points - origins r_2 = points - terminations r_0 = r_1 - r_2 r_1_cross_r_2 = nb_2d_explicit_cross(r_1, r_2) r_1_cross_r_2_absolute_magnitude = ( r_1_cross_r_2[:, :, 0] ** 2 + r_1_cross_r_2[:, :, 1] ** 2 + r_1_cross_r_2[:, :, 2] ** 2 ) r_1_length = nb_2d_explicit_norm(r_1) r_2_length = nb_2d_explicit_norm(r_2) # Define the radius of the line vortices. This is used to get rid of any # singularities. radius = 3.0e-16 # Set the lengths and the absolute magnitudes to zero, at the places where the # lengths and absolute magnitudes are less than the vortex radius. r_1_length[r_1_length < radius] = 0 r_2_length[r_2_length < radius] = 0 r_1_cross_r_2_absolute_magnitude[r_1_cross_r_2_absolute_magnitude < radius] = 0 # Calculate the vector dot products. r_0_dot_r_1 = np.einsum(""ijk,ijk->ij"", r_0, r_1) r_0_dot_r_2 = np.einsum(""ijk,ijk->ij"", r_0, r_2) # Calculate k and then the induced velocity, ignoring any divide-by-zero or nan # errors. k is of shape (N x M) with np.errstate(divide=""ignore"", invalid=""ignore""): k = ( strengths / (4 * np.pi * r_1_cross_r_2_absolute_magnitude) * (r_0_dot_r_1 / r_1_length - r_0_dot_r_2 / r_2_length) ) # Set the shape of k to be (N x M x 1) to support numpy broadcasting in the # subsequent multiplication. k = np.expand_dims(k, axis=2) induced_velocities = k * r_1_cross_r_2 # Set the values of the induced velocity to zero where there are singularities. induced_velocities[np.isinf(induced_velocities)] = 0 induced_velocities[np.isnan(induced_velocities)] = 0 if collapse: induced_velocities = np.sum(induced_velocities, axis=1) return induced_velocities@njit def nb_2d_explicit_norm(vectors): return np.sqrt( (vectors[:, :, 0]) ** 2 + (vectors[:, :, 1]) ** 2 + (vectors[:, :, 2]) ** 2 )@njitdef nb_2d_explicit_cross(a, b): e = np.zeros_like(a) e[:, :, 0] = a[:, :, 1] * b[:, :, 2] - a[:, :, 2] * b[:, :, 1] e[:, :, 1] = a[:, :, 2] * b[:, :, 0] - a[:, :, 0] * b[:, :, 2] e[:, :, 2] = a[:, :, 0] * b[:, :, 1] - a[:, :, 1] * b[:, :, 0] return e import timeitimport matplotlib.pyplot as pltimport numpy as npn_repeat = 2n_execute = 10 ** 3min_oom = 0max_oom = 3times_py = []for i in range(max_oom - min_oom + 1): n_elem = 10 ** i n_elem_pretty = np.format_float_scientific(n_elem, 0) print(""Number of elements: "" + n_elem_pretty) # Benchmark Python. print(""\tBenchmarking Python..."") setup = '''import numpy as npthese_points = np.random.random((''' + str(n_elem) + ''', 3))these_origins = np.random.random((''' + str(n_elem) + ''', 3))these_terminations = np.random.random((''' + str(n_elem) + ''', 3))these_strengths = np.random.random(''' + str(n_elem) + ''')def calculate_velocity_induced_by_line_vortices(points, origins, terminations, strengths, collapse=True): pass ''' statement = '''results_orig = calculate_velocity_induced_by_line_vortices(these_points, these_origins, these_terminations, these_strengths) ''' times = timeit.repeat(repeat=n_repeat, stmt=statement, setup=setup, number=n_execute) time_py = min(times)/n_execute time_py_pretty = np.format_float_scientific(time_py, 2) print(""\t\tAverage Time per Loop: "" + time_py_pretty + "" s"") # Record the times. times_py.append(time_py)sizes = [10 ** i for i in range(max_oom - min_oom + 1)]fig, ax = plt.subplots()ax.plot(sizes, times_py, label='Python')ax.set_xscale(""log"")ax.set_xlabel(""Size of List or Array (elements)"")ax.set_ylabel(""Average Time per Loop (s)"")ax.set_title( ""Comparison of Different Optimization Methods\nBest of "" + str(n_repeat) + "" Runs, each with "" + str(n_execute) + "" Loops"")ax.legend()plt.show() points = np.expand_dims(points, axis=1)r_1 = points - originsr_2 = points - terminations @njitdef subtract(a, b): c = np.empty((a.shape[0], b.shape[0], 3)) for i in range(a.shape[0]): for j in range(b.shape[0]): for k in range(3): c[i, j, k] = a[i, k] - b[j, k] return c","Can I speed up this aerodynamics calculation with Numba, vectorization, or multiprocessing?"
"Can I Speed up This Aerodynamics Calculation With Numba, Vectorization, or Multiprocessing?"," Problem:I am trying to increase the speed of an aerodynamics function in Python.Function Set: Context:This function is used by Ptera Software, an open-source solver for flapping wing aerodynamics. As shown by the profile output below, it is by far the largest contributor to Ptera Software's run time.Currently, Ptera Software takes just over 3 minutes to run a typical case, and my goal is to get this below 1 minute.The function takes in a group of points, origins, terminations, and strengths. At every point, it finds the induced velocity due to the line vortices, which are characterized by the groups of origins, terminations, and strengths. If collapse is true, then the output is the cumulative velocity induced at each point due to the vortices. If false, the function outputs each vortex's contribution to the velocity at each point.During a typical run, the velocity function is called approximately 2000 times. At first, the calls involve vectors with relatively small input arguments (around 200 points, origins, terminations, and strengths). Later calls involve large input arguments (around 400 points and around 6,000 origins, terminations, and strengths). An ideal solution would be fast for all size inputs, but increasing the speed of large input calls is more important.For testing, I recommend running the following script with your own implementation of the function: Previous Attempts:My prior attempts at speeding up this function involved vectorizing it (which worked great, so I kept those changes) and trying out Numba's JIT compiler. I had mixed results with Numba. When I tried to use Numba on a modified version of the entire velocity function, my results were much slower than before. However, I found that Numba significantly sped up the cross-product and norm functions, which I implemented above.Updates:Update 1:Based on Mercury's comment (which has since been deleted), I replaced with two calls to the following function: This resulted in a speed increase from 227 s to 220 s. This is better! However, it is still not fast enough.I also have tried setting the njit fastmath flag to true, and using a numba function instead of calls to np.einsum. Neither increased the speed.Update 2:With Jrme Richard's answer, the run time is now 156 s, which is a decrease of 29%! I'm satisfied enough to accept this answer, but feel free to make other suggestions if you think you can improve on their work! <code>  import numpy as npfrom numba import njitdef calculate_velocity_induced_by_line_vortices( points, origins, terminations, strengths, collapse=True): # Expand the dimensionality of the points input. It is now of shape (N x 1 x 3). # This will allow NumPy to broadcast the upcoming subtractions. points = np.expand_dims(points, axis=1) # Define the vectors from the vortex to the points. r_1 and r_2 now both are of # shape (N x M x 3). Each row/column pair holds the vector associated with each # point/vortex pair. r_1 = points - origins r_2 = points - terminations r_0 = r_1 - r_2 r_1_cross_r_2 = nb_2d_explicit_cross(r_1, r_2) r_1_cross_r_2_absolute_magnitude = ( r_1_cross_r_2[:, :, 0] ** 2 + r_1_cross_r_2[:, :, 1] ** 2 + r_1_cross_r_2[:, :, 2] ** 2 ) r_1_length = nb_2d_explicit_norm(r_1) r_2_length = nb_2d_explicit_norm(r_2) # Define the radius of the line vortices. This is used to get rid of any # singularities. radius = 3.0e-16 # Set the lengths and the absolute magnitudes to zero, at the places where the # lengths and absolute magnitudes are less than the vortex radius. r_1_length[r_1_length < radius] = 0 r_2_length[r_2_length < radius] = 0 r_1_cross_r_2_absolute_magnitude[r_1_cross_r_2_absolute_magnitude < radius] = 0 # Calculate the vector dot products. r_0_dot_r_1 = np.einsum(""ijk,ijk->ij"", r_0, r_1) r_0_dot_r_2 = np.einsum(""ijk,ijk->ij"", r_0, r_2) # Calculate k and then the induced velocity, ignoring any divide-by-zero or nan # errors. k is of shape (N x M) with np.errstate(divide=""ignore"", invalid=""ignore""): k = ( strengths / (4 * np.pi * r_1_cross_r_2_absolute_magnitude) * (r_0_dot_r_1 / r_1_length - r_0_dot_r_2 / r_2_length) ) # Set the shape of k to be (N x M x 1) to support numpy broadcasting in the # subsequent multiplication. k = np.expand_dims(k, axis=2) induced_velocities = k * r_1_cross_r_2 # Set the values of the induced velocity to zero where there are singularities. induced_velocities[np.isinf(induced_velocities)] = 0 induced_velocities[np.isnan(induced_velocities)] = 0 if collapse: induced_velocities = np.sum(induced_velocities, axis=1) return induced_velocities@njit def nb_2d_explicit_norm(vectors): return np.sqrt( (vectors[:, :, 0]) ** 2 + (vectors[:, :, 1]) ** 2 + (vectors[:, :, 2]) ** 2 )@njitdef nb_2d_explicit_cross(a, b): e = np.zeros_like(a) e[:, :, 0] = a[:, :, 1] * b[:, :, 2] - a[:, :, 2] * b[:, :, 1] e[:, :, 1] = a[:, :, 2] * b[:, :, 0] - a[:, :, 0] * b[:, :, 2] e[:, :, 2] = a[:, :, 0] * b[:, :, 1] - a[:, :, 1] * b[:, :, 0] return e import timeitimport matplotlib.pyplot as pltimport numpy as npn_repeat = 2n_execute = 10 ** 3min_oom = 0max_oom = 3times_py = []for i in range(max_oom - min_oom + 1): n_elem = 10 ** i n_elem_pretty = np.format_float_scientific(n_elem, 0) print(""Number of elements: "" + n_elem_pretty) # Benchmark Python. print(""\tBenchmarking Python..."") setup = '''import numpy as npthese_points = np.random.random((''' + str(n_elem) + ''', 3))these_origins = np.random.random((''' + str(n_elem) + ''', 3))these_terminations = np.random.random((''' + str(n_elem) + ''', 3))these_strengths = np.random.random(''' + str(n_elem) + ''')def calculate_velocity_induced_by_line_vortices(points, origins, terminations, strengths, collapse=True): pass ''' statement = '''results_orig = calculate_velocity_induced_by_line_vortices(these_points, these_origins, these_terminations, these_strengths) ''' times = timeit.repeat(repeat=n_repeat, stmt=statement, setup=setup, number=n_execute) time_py = min(times)/n_execute time_py_pretty = np.format_float_scientific(time_py, 2) print(""\t\tAverage Time per Loop: "" + time_py_pretty + "" s"") # Record the times. times_py.append(time_py)sizes = [10 ** i for i in range(max_oom - min_oom + 1)]fig, ax = plt.subplots()ax.plot(sizes, times_py, label='Python')ax.set_xscale(""log"")ax.set_xlabel(""Size of List or Array (elements)"")ax.set_ylabel(""Average Time per Loop (s)"")ax.set_title( ""Comparison of Different Optimization Methods\nBest of "" + str(n_repeat) + "" Runs, each with "" + str(n_execute) + "" Loops"")ax.legend()plt.show() points = np.expand_dims(points, axis=1)r_1 = points - originsr_2 = points - terminations @njitdef subtract(a, b): c = np.empty((a.shape[0], b.shape[0], 3)) for i in range(a.shape[0]): for j in range(b.shape[0]): for k in range(3): c[i, j, k] = a[i, k] - b[j, k] return c","Can I speed up this aerodynamics calculation with Numba, vectorization, or multiprocessing?"
What does pip-compile do? What is it's use?," I am a beginner in programming and python. I read pip-compiles definition in pip-tools documentation but I could not understand. Can someone explain me this?More specifically, what does compiling requirements.in to produce requirements.txt mean? <code> ",What does pip-compile do? What is its use?
What would be the best way to solve the following using Python OOP?," I have written code to create a Secret Santa list from a .csv file with groups of names and corresponding emails. How can I unit test this code, since I am using random shuffle operations? I am not sure how I could write a test case for the same. <code>  import randomimport pandas as pddef compare(list_of_names, list_of_emails): zipped_lists = list(zip(list_of_emails, list_of_names)) random.shuffle(zipped_lists) # shuffle list of emails and names result = [] shuffled_emails = [i[0] for i in zipped_lists] for i, _ in enumerate(shuffled_emails): result.append(zipped_lists[i-1][1]) # shift email relatively one position to the right return list(zip(result, shuffled_emails))def main(): names = [""John"", ""Bob"", ""Alice""] emails = [""John@gmail.com"", ""Bob@gmail.com"", ""Alice@outlook.com""] print(compare(names,emails))if __name__ == '__main__': main()",How do I unit test random shuffle operations in Python?
Keep only bigest interval in coordinates row in pandas," I have a dataframe such as: I would like for each Groups to keep only one interval representant (an interval meaning that df.start and df.end overlaps between rows), I explain:For exemple, in the G1 there are 2 intervals groups :Interval 1 (with min = 451 and max = 969): Then I take the biggest df.sum (here 1420)andInterval2 (with min = 229 and max = 450) Then I take the biggest df.sum (here 681)If I do that for the whole dataframe I get: Does someone have an idea?Here are the data in dictionary format : <code>  Groups Name start end sum1 G1 A 451 954 14052 G1 B 451 951 14023 G1 C 451 969 14204 G1 D 463 870 13335 G1 E 463 888 13516 G1 X 230 450 6807 G1 Z 229 450 6818 G2 F 119 841 9609 G2 G 118 842 96010 G3 H 460 790 125011 G3 I 123 300 17712 G4 J 343 878 122113 G4 K 343 878 122114 G4 L 320 862 1182 Name start end sumA 451 954 1405B 451 951 1402C 451 969 1420D 463 870 1333E 463 888 1351 Name start end sumX 230 450 680Z 229 450 681 Groups Name start end sum3 G1 C 451 969 14207 G1 Z 229 450 6819 G2 G 118 842 96010 G3 H 460 790 125011 G3 I 123 300 17712 G4 J 343 878 1221 {'Groups Name start end sum': {0: 'G1 A 451 954 1405', 1: 'G1 B 451 951 1402', 2: 'G1 C 451 969 1420', 3: 'G1 D 463 870 1333', 4: 'G1 E 463 888 1351', 5: 'G1 X 230 450 680', 6: 'G1 Z 229 450 681', 7: 'G2 F 119 841 960', 8: 'G2 G 118 842 960', 9: 'G3 H 460 790 1250', 10: 'G3 I 123 300 177', 11: 'G4 J 343 878 1221', 12: 'G4 K 343 878 1221', 13: 'G4 L 320 862 1182'}}",Keep only largest interval in coordinates row in pandas
WARNING messages when I update PIP or install Packages, I have a M1 Mac and I just noticed that when I try to upgrade pip or install any packages I get a series of warnings: Please advise. <code>  user@mac01 ~ $python3 -m pip install --upgrade pipWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>distutils: /opt/homebrew/lib/python3.9/site-packagessysconfig: /opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packagesWARNING: Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>distutils: /opt/homebrew/lib/python3.9/site-packagessysconfig: /opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packagesWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>distutils: /opt/homebrew/include/python3.9/UNKNOWNsysconfig: /opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/include/python3.9WARNING: Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>distutils: /opt/homebrew/binsysconfig: /opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/binWARNING: Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>distutils: /opt/homebrewsysconfig: /opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9WARNING: Additional context:user = Falsehome = Noneroot = Noneprefix = NoneRequirement already satisfied: pip in /opt/homebrew/lib/python3.9/site-packages (21.1)WARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>distutils: /opt/homebrew/lib/python3.9/site-packagessysconfig: /opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packagesWARNING: Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>distutils: /opt/homebrew/lib/python3.9/site-packagessysconfig: /opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packagesWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>distutils: /opt/homebrew/include/python3.9/UNKNOWNsysconfig: /opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/include/python3.9WARNING: Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>distutils: /opt/homebrew/binsysconfig: /opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9/binWARNING: Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>distutils: /opt/homebrewsysconfig: /opt/homebrew/opt/python@3.9/Frameworks/Python.framework/Versions/3.9WARNING: Additional context:user = Falsehome = Noneroot = Noneprefix = Noneuser@mac01 ~ $,"""WARNING: Value for scheme.data does not match"" when I try to update pip or install packages"
how to prevent turtle from moving opposite direction," Code: I am not close to done with this project but I have run into some problems. I want to create a game in the turtle module. But I don't know how to prevent the block from moving backward. I have seen other people use t.direction or something. But I have tried that and it didn't really work, maybe I'm just stupid and I did something wrong. How can I prevent the square from moving in the opposite direction? <code>  import turtleimport randomimport times = turtle.getscreen()turtle.screensize(canvwidth=400, canvheight=400)t = turtle.Turtle()t.pensize(0)t.shape('square')t.color(""black"")t.speed(0)t.penup()def moveu(num): t.setheading(num) t.forward(20) s.onkey(lambda : moveu(90), 'w')s.onkey(lambda : moveu(270), 's')s.onkey(lambda : moveu(180), 'a')s.onkey(lambda : moveu(0), 'd') s.listen()",How to prevent turtle from moving in opposite direction
How to Prevent turtle from Moving Opposite Direction," Code: I am not close to done with this project but I have run into some problems. I want to create a game in the turtle module. But I don't know how to prevent the block from moving backward. I have seen other people use t.direction or something. But I have tried that and it didn't really work, maybe I'm just stupid and I did something wrong. How can I prevent the square from moving in the opposite direction? <code>  import turtleimport randomimport times = turtle.getscreen()turtle.screensize(canvwidth=400, canvheight=400)t = turtle.Turtle()t.pensize(0)t.shape('square')t.color(""black"")t.speed(0)t.penup()def moveu(num): t.setheading(num) t.forward(20) s.onkey(lambda : moveu(90), 'w')s.onkey(lambda : moveu(270), 's')s.onkey(lambda : moveu(180), 'a')s.onkey(lambda : moveu(0), 'd') s.listen()",How to prevent turtle from moving in opposite direction
Getting : WARNING: xcodeproj is not installed or is not configured properly. after setting up my nativescript environment," My nativescript project won't run, getting the error mentioned above. The Xcode command Line Tools is set.Things i tried but didn't fix the problem: As you might have guessed, I have Xcode installed from the app store and I've also installed cocoapods and xcodeproj. This is the only warning that I get.Any suggestions? <code>  sudo xcode-select -s /Applications/Xcode.app/Contents/Developersudo xcode-select --resetns doctor ios",WARNING: xcodeproj is not installed or is not configured properly
Why in-place operation like a *= b will be slower than simple operation like a = a * b for integers?," I know integers are immutable so the computed values do not modify the original integers. Therefore the in-place operations should do the same as the simple operations, 1. compute the value and 2. reassign the value back to the variable. But why are the in-place operations slower than the simple ones? Output: All the in-place operations are slower than the simple ones. Addition has the smallest difference while multiplication has the greatest. <code>  import timeitprint(""a = a + 1: "", end="""")print(timeit.timeit(""for i in range(100): a = a + 1"", setup=""a = 0""))print(""a += 1: "", end="""")print(timeit.timeit(""for i in range(100): a += 1"", setup=""a = 0""))print(""a = a - 1: "", end="""")print(timeit.timeit(""for i in range(100): a = a - 1"", setup=""a = 0""))print(""a -= 1: "", end="""")print(timeit.timeit(""for i in range(100): a -= 1"", setup=""a = 0""))print(""a = a * 1: "", end="""")print(timeit.timeit(""for i in range(100): a = a * 1"", setup=""a = 1""))print(""a *= 1: "", end="""")print(timeit.timeit(""for i in range(100): a *= 1"", setup=""a = 1""))print(""a = a // 1: "", end="""")print(timeit.timeit(""for i in range(100): a = a // 1"", setup=""a = 1""))print(""a //= 1: "", end="""")print(timeit.timeit(""for i in range(100): a //= 1"", setup=""a = 1"")) a = a + 1: 2.922127154a += 1: 2.9701245480000003a = a - 1: 2.9568866799999993a -= 1: 3.1065419050000003a = a * 1: 2.2483990140000003a *= 1: 2.703524648a = a // 1: 2.534561783000001a //= 1: 2.6582312889999997",Why is an in-place integer operation like a *= b slower than a = a * b?
module 'tensorflow._api.v1.compat.v2' has no attribute '__internal__'," I am running a tensorflow model on google colab. Today, I got this error: Previously, things had been running smoothly, so I'm not sure why this happened.I am using Python 3.7.10, and these are the packages I am supposed to use: Perhaps colab recently upgraded some libraries? I am sure that I followed the same installation steps as I usually do.EDIT:I think there may be an issue in the keras version.Here are the first few lines of the file I am running: If I remove all of the lines starting with ""from keras"", I don't get the error. However, I never touched these lines before, so I don't know why they would suddenly cause an error now. Also, it is not the python version causing this error, because colab changed it to 3.7.10 in April and I had no problem. <code>  Using TensorFlow backend. Traceback (most recent call last): File ""train.py"", line 6, in <module> from yolo import create_yolov3_model, dummy_loss File ""/content/drive/MyDrive/yolo/yolo_plz_work/yolo.py"", line 1, in <module> from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D, Lambda File ""/usr/local/lib/python3.7/dist-packages/keras/__init__.py"", line 3, in <module> from . import utils File ""/usr/local/lib/python3.7/dist-packages/keras/utils/__init__.py"", line 26, in <module> from .vis_utils import model_to_dot File ""/usr/local/lib/python3.7/dist-packages/keras/utils/vis_utils.py"", line 7, in <module> from ..models import Model File ""/usr/local/lib/python3.7/dist-packages/keras/models.py"", line 10, in <module> from .engine.input_layer import Input File ""/usr/local/lib/python3.7/dist-packages/keras/engine/__init__.py"", line 3, in <module> from .input_layer import Input File ""/usr/local/lib/python3.7/dist-packages/keras/engine/input_layer.py"", line 7, in <module> from .base_layer import Layer File ""/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py"", line 12, in <module> from .. import initializers File ""/usr/local/lib/python3.7/dist-packages/keras/initializers/__init__.py"", line 124, in <module> populate_deserializable_objects() File ""/usr/local/lib/python3.7/dist-packages/keras/initializers/__init__.py"", line 49, in populate_deserializable_objects LOCAL.GENERATED_WITH_V2 = tf.__internal__.tf2.enabled() File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/module_wrapper.py"", line 193, in __getattr__ attr = getattr(self._tfmw_wrapped_module, name) AttributeError: module 'tensorflow._api.v1.compat.v2' has no attribute '__internal__' absl-py==0.9.0astor==0.8.1gast==0.2.2google-pasta==0.1.8grpcio==1.26.0h5py==2.10.0Keras==2.3.1Keras-Applications==1.0.8Keras-Preprocessing==1.1.0Markdown==3.1.1numpy==1.18.1opencv-contrib-python==4.1.2.30opt-einsum==3.1.0protobuf==3.11.2PyYAML==5.3scipy==1.4.1six==1.14.0tensorboard==1.15.0tensorflow==1.15.0tensorflow-estimator==1.15.1termcolor==1.1.0tqdm==4.41.1Werkzeug==0.16.0wrapt==1.11.2 from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D, Lambdafrom keras.layers.merge import add, concatenatefrom keras.models import Modelfrom keras.engine.topology import Layerimport tensorflow as tf",module 'tensorflow._api.v1.compat.v2' has no attribute '__internal__' google colab error
Pandas co-occurrence count," Lets say I have the following table/data frame: I would like to find, for each pair of products the number of times they co-occur in a store.Since the data is very large (5M rows and about 50K individual products & 20K individual stores) and there are many potential co-occurrence pairs, I would just like to get the top n (example: 10) co-occurrences for each product and the count of the cooccurrence. The example result is below: An effective and efficient solution in SQL instead of pandas would also be acceptable <code>  d = {'store': ['s1', 's1', 's2', 's2',], 'product': ['a', 'c', 'a', 'c']} df = pd.DataFrame(data=d)print(df) store product0 s1 a 1 s1 c 3 s2 a 4 s2 c product_1 product_2 cooccurrence_count0 a c 2 1 c a 2",Pandas/SQL co-occurrence count
pip install py-find-1st fails on ubuntu 20 with python3.9," This is my process.I start a new aws t2.micro ec2 on ubuntu20 and run this script The last line is the problemI get this output (error is at the bottom, I included more output incase it's helpful, had to trim output a little bit) If you would like to reproduce the error, you can start a t2.micro ec2 running ubuntu20 and run the script listed above <code>  sudo apt-get updatesudo apt-get install gccsudo apt-get install python3.9sudo apt-get install python3.9-venvcurl --silent --show-error --retry 5 https://bootstrap.pypa.io/get-pip.py > get-pip.pypython3.9 get-pip.pysudo apt-get install python-devsudo apt-get install python3-devpython3.9 -m pip install --upgrade pippython3.9 -m pip install --upgrade pip setuptoolspython3.9 -m pip install --upgrade wheelpython3.9 -m pip install testresourcessudo apt-get install build-essentialsudo apt-get install libreadline-gplv2-dev libncursesw5-dev libssl-dev libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev libffi-dev wget liblzma-dev lzmasudo apt-get install python3-develpython3.9 -m pip install p5pypython3.9 -m pip install PEP517python3.9 -m pip install py-find-1st Preparing to unpack .../python3.9-venv_3.9.5-3~20.04.1_amd64.deb ...Unpacking python3.9-venv (3.9.5-3~20.04.1) ...Setting up python-pip-whl (20.0.2-5ubuntu1.5) ...Setting up python3.9-venv (3.9.5-3~20.04.1) ...Defaulting to user installation because normal site-packages is not writeableCollecting pip Downloading pip-21.1.3-py3-none-any.whl (1.5 MB) || 1.5 MB 5.0 MB/s Collecting wheel Downloading wheel-0.36.2-py2.py3-none-any.whl (35 kB)Installing collected packages: wheel, pip WARNING: The script wheel is installed in '/home/ubuntu/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location. WARNING: The scripts pip, pip3 and pip3.9 are installed in '/home/ubuntu/.local/bin' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.Successfully installed pip-21.1.3 wheel-0.36.2Reading package lists... DoneBuilding dependency tree Reading state information... DoneNote, selecting 'python-dev-is-python2' instead of 'python-dev'The following additional packages will be installed: libexpat1-dev libpython2-dev libpython2-stdlib libpython2.7 libpython2.7-dev libpython2.7-minimal libpython2.7-stdlib python-is-python2 python2 python2-dev python2-minimal python2.7 python2.7-dev python2.7-minimalSuggested packages: python2-doc python-tk python2.7-doc binfmt-supportThe following NEW packages will be installed: libexpat1-dev libpython2-dev libpython2-stdlib libpython2.7 libpython2.7-dev libpython2.7-minimal libpython2.7-stdlib python-dev-is-python2 python-is-python2 python2 python2-dev python2-minimal python2.7 python2.7-dev python2.7-minimal0 upgraded, 15 newly installed, 0 to remove and 78 not upgraded.Need to get 7744 kB of archives.After this operation, 35.1 MB of additional disk space will be used.Do you want to continue? [Y/n] YGet:1 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/universe amd64 libpython2.7-minimal amd64 2.7.18-1~20.04.1 [335 kB]Get:2 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/universe amd64 python2.7-minimal amd64 2.7.18-1~20.04.1 [1285 kB]Get:3 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/universe amd64 python2-minimal amd64 2.7.17-2ubuntu4 [27.5 kB]Get:4 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/universe amd64 libpython2.7-stdlib amd64 2.7.18-1~20.04.1 [1887 kB]Get:5 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/universe amd64 python2.7 amd64 2.7.18-1~20.04.1 [248 kB]Get:6 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/universe amd64 libpython2-stdlib amd64 2.7.17-2ubuntu4 [7072 B]Get:7 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/universe amd64 python2 amd64 2.7.17-2ubuntu4 [26.5 kB]Get:8 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 libexpat1-dev amd64 2.2.9-1build1 [116 kB]Get:9 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/universe amd64 libpython2.7 amd64 2.7.18-1~20.04.1 [1038 kB]Get:10 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/universe amd64 libpython2.7-dev amd64 2.7.18-1~20.04.1 [2475 kB]Get:11 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/universe amd64 libpython2-dev amd64 2.7.17-2ubuntu4 [7140 B]Get:12 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/universe amd64 python-is-python2 all 2.7.17-4 [2496 B]Get:13 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/universe amd64 python2.7-dev amd64 2.7.18-1~20.04.1 [287 kB]Get:14 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/universe amd64 python2-dev amd64 2.7.17-2ubuntu4 [1268 B]Get:15 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/universe amd64 python-dev-is-python2 all 2.7.17-4 [1396 B]Fetched 7744 kB in 0s (46.3 MB/s) Selecting previously unselected package libpython2.7-minimal:amd64.(Reading database ... 65082 files and directories currently installed.)Preparing to unpack .../0-libpython2.7-minimal_2.7.18-1~20.04.1_amd64.deb ...Unpacking libpython2.7-minimal:amd64 (2.7.18-1~20.04.1) ...Selecting previously unselected package python2.7-minimal.Preparing to unpack .../1-python2.7-minimal_2.7.18-1~20.04.1_amd64.deb ...Unpacking python2.7-minimal (2.7.18-1~20.04.1) ...Selecting previously unselected package python2-minimal.Preparing to unpack .../2-python2-minimal_2.7.17-2ubuntu4_amd64.deb ...Unpacking python2-minimal (2.7.17-2ubuntu4) ...Selecting previously unselected package libpython2.7-stdlib:amd64.Preparing to unpack .../3-libpython2.7-stdlib_2.7.18-1~20.04.1_amd64.deb ...Unpacking libpython2.7-stdlib:amd64 (2.7.18-1~20.04.1) ...Selecting previously unselected package python2.7.Preparing to unpack .../4-python2.7_2.7.18-1~20.04.1_amd64.deb ...Unpacking python2.7 (2.7.18-1~20.04.1) ...Selecting previously unselected package libpython2-stdlib:amd64.Preparing to unpack .../5-libpython2-stdlib_2.7.17-2ubuntu4_amd64.deb ...Unpacking libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...Setting up libpython2.7-minimal:amd64 (2.7.18-1~20.04.1) ...Setting up python2.7-minimal (2.7.18-1~20.04.1) ...Linking and byte-compiling packages for runtime python2.7...Setting up python2-minimal (2.7.17-2ubuntu4) ...Selecting previously unselected package python2.(Reading database ... 65829 files and directories currently installed.)Preparing to unpack .../0-python2_2.7.17-2ubuntu4_amd64.deb ...Unpacking python2 (2.7.17-2ubuntu4) ...Selecting previously unselected package libexpat1-dev:amd64.Preparing to unpack .../1-libexpat1-dev_2.2.9-1build1_amd64.deb ...Unpacking libexpat1-dev:amd64 (2.2.9-1build1) ...Selecting previously unselected package libpython2.7:amd64.Preparing to unpack .../2-libpython2.7_2.7.18-1~20.04.1_amd64.deb ...Unpacking libpython2.7:amd64 (2.7.18-1~20.04.1) ...Selecting previously unselected package libpython2.7-dev:amd64.Preparing to unpack .../3-libpython2.7-dev_2.7.18-1~20.04.1_amd64.deb ...Unpacking libpython2.7-dev:amd64 (2.7.18-1~20.04.1) ...Selecting previously unselected package libpython2-dev:amd64.Preparing to unpack .../4-libpython2-dev_2.7.17-2ubuntu4_amd64.deb ...Unpacking libpython2-dev:amd64 (2.7.17-2ubuntu4) ...Selecting previously unselected package python-is-python2.Preparing to unpack .../5-python-is-python2_2.7.17-4_all.deb ...Unpacking python-is-python2 (2.7.17-4) ...Selecting previously unselected package python2.7-dev.Preparing to unpack .../6-python2.7-dev_2.7.18-1~20.04.1_amd64.deb ...Unpacking python2.7-dev (2.7.18-1~20.04.1) ...Selecting previously unselected package python2-dev.Preparing to unpack .../7-python2-dev_2.7.17-2ubuntu4_amd64.deb ...Unpacking python2-dev (2.7.17-2ubuntu4) ...Selecting previously unselected package python-dev-is-python2.Preparing to unpack .../8-python-dev-is-python2_2.7.17-4_all.deb ...Unpacking python-dev-is-python2 (2.7.17-4) ...Setting up libpython2.7-stdlib:amd64 (2.7.18-1~20.04.1) ...Setting up libexpat1-dev:amd64 (2.2.9-1build1) ...Setting up libpython2.7:amd64 (2.7.18-1~20.04.1) ...Setting up libpython2.7-dev:amd64 (2.7.18-1~20.04.1) ...Setting up python2.7 (2.7.18-1~20.04.1) ...Setting up libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...Setting up python2 (2.7.17-2ubuntu4) ...Setting up libpython2-dev:amd64 (2.7.17-2ubuntu4) ...Setting up python-is-python2 (2.7.17-4) ...Setting up python2.7-dev (2.7.18-1~20.04.1) ...Setting up python2-dev (2.7.17-2ubuntu4) ...Setting up python-dev-is-python2 (2.7.17-4) ...Processing triggers for libc-bin (2.31-0ubuntu9.2) ...Processing triggers for man-db (2.9.1-1) ...Processing triggers for mime-support (3.64ubuntu1) ...Reading package lists... DoneBuilding dependency tree Reading state information... DoneThe following additional packages will be installed: libpython3-dev libpython3.8 libpython3.8-dev libpython3.8-minimal libpython3.8-stdlib python3.8 python3.8-dev python3.8-minimal zlib1g-devSuggested packages: python3.8-venv python3.8-doc binfmt-supportThe following NEW packages will be installed: libpython3-dev libpython3.8-dev python3-dev python3.8-dev zlib1g-devThe following packages will be upgraded: libpython3.8 libpython3.8-minimal libpython3.8-stdlib python3.8 python3.8-minimal5 upgraded, 5 newly installed, 0 to remove and 73 not upgraded.Need to get 10.9 MB of archives.After this operation, 21.2 MB of additional disk space will be used.Do you want to continue? [Y/n] YGet:1 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 python3.8 amd64 3.8.10-0ubuntu1~20.04 [387 kB]Get:2 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 libpython3.8 amd64 3.8.10-0ubuntu1~20.04 [1625 kB]Get:3 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 libpython3.8-stdlib amd64 3.8.10-0ubuntu1~20.04 [1675 kB]Get:4 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 python3.8-minimal amd64 3.8.10-0ubuntu1~20.04 [1898 kB]Get:5 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 libpython3.8-minimal amd64 3.8.10-0ubuntu1~20.04 [717 kB]Get:6 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 libpython3.8-dev amd64 3.8.10-0ubuntu1~20.04 [3943 kB]Get:7 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 libpython3-dev amd64 3.8.2-0ubuntu2 [7236 B]Get:8 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 zlib1g-dev amd64 1:1.2.11.dfsg-2ubuntu1.2 [155 kB]Get:9 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 python3.8-dev amd64 3.8.10-0ubuntu1~20.04 [510 kB]Get:10 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 python3-dev amd64 3.8.2-0ubuntu2 [1212 B]Fetched 10.9 MB in 0s (42.5 MB/s) (Reading database ... 66030 files and directories currently installed.)Preparing to unpack .../0-python3.8_3.8.10-0ubuntu1~20.04_amd64.deb ...Unpacking python3.8 (3.8.10-0ubuntu1~20.04) over (3.8.5-1~20.04.2) ...Preparing to unpack .../1-libpython3.8_3.8.10-0ubuntu1~20.04_amd64.deb ...Unpacking libpython3.8:amd64 (3.8.10-0ubuntu1~20.04) over (3.8.5-1~20.04.2) ...Preparing to unpack .../2-libpython3.8-stdlib_3.8.10-0ubuntu1~20.04_amd64.deb ...Unpacking libpython3.8-stdlib:amd64 (3.8.10-0ubuntu1~20.04) over (3.8.5-1~20.04.2) ...Preparing to unpack .../3-python3.8-minimal_3.8.10-0ubuntu1~20.04_amd64.deb ...Unpacking python3.8-minimal (3.8.10-0ubuntu1~20.04) over (3.8.5-1~20.04.2) ...Preparing to unpack .../4-libpython3.8-minimal_3.8.10-0ubuntu1~20.04_amd64.deb ...Unpacking libpython3.8-minimal:amd64 (3.8.10-0ubuntu1~20.04) over (3.8.5-1~20.04.2) ...Selecting previously unselected package libpython3.8-dev:amd64.Preparing to unpack .../5-libpython3.8-dev_3.8.10-0ubuntu1~20.04_amd64.deb ...Unpacking libpython3.8-dev:amd64 (3.8.10-0ubuntu1~20.04) ...Selecting previously unselected package libpython3-dev:amd64.Preparing to unpack .../6-libpython3-dev_3.8.2-0ubuntu2_amd64.deb ...Unpacking libpython3-dev:amd64 (3.8.2-0ubuntu2) ...Selecting previously unselected package zlib1g-dev:amd64.Preparing to unpack .../7-zlib1g-dev_1%3a1.2.11.dfsg-2ubuntu1.2_amd64.deb ...Unpacking zlib1g-dev:amd64 (1:1.2.11.dfsg-2ubuntu1.2) ...Selecting previously unselected package python3.8-dev.Preparing to unpack .../8-python3.8-dev_3.8.10-0ubuntu1~20.04_amd64.deb ...Unpacking python3.8-dev (3.8.10-0ubuntu1~20.04) ...Selecting previously unselected package python3-dev.Preparing to unpack .../9-python3-dev_3.8.2-0ubuntu2_amd64.deb ...Unpacking python3-dev (3.8.2-0ubuntu2) ...Setting up libpython3.8-minimal:amd64 (3.8.10-0ubuntu1~20.04) ...Setting up zlib1g-dev:amd64 (1:1.2.11.dfsg-2ubuntu1.2) ...Setting up python3.8-minimal (3.8.10-0ubuntu1~20.04) ...Setting up libpython3.8-stdlib:amd64 (3.8.10-0ubuntu1~20.04) ...Setting up python3.8 (3.8.10-0ubuntu1~20.04) ...Setting up libpython3.8:amd64 (3.8.10-0ubuntu1~20.04) ...Setting up libpython3.8-dev:amd64 (3.8.10-0ubuntu1~20.04) ...Setting up python3.8-dev (3.8.10-0ubuntu1~20.04) ...Setting up libpython3-dev:amd64 (3.8.2-0ubuntu2) ...Setting up python3-dev (3.8.2-0ubuntu2) ...Processing triggers for libc-bin (2.31-0ubuntu9.2) ...Processing triggers for man-db (2.9.1-1) ...Processing triggers for mime-support (3.64ubuntu1) ...Defaulting to user installation because normal site-packages is not writeableRequirement already satisfied: pip in ./.local/lib/python3.9/site-packages (21.1.3)Defaulting to user installation because normal site-packages is not writeableRequirement already satisfied: pip in ./.local/lib/python3.9/site-packages (21.1.3)Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (45.2.0)Collecting setuptools Downloading setuptools-57.4.0-py3-none-any.whl (819 kB) || 819 kB 5.0 MB/s Installing collected packages: setuptoolsERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.launchpadlib 1.10.13 requires testresources, which is not installed.Successfully installed setuptools-57.4.0Defaulting to user installation because normal site-packages is not writeableRequirement already satisfied: wheel in ./.local/lib/python3.9/site-packages (0.36.2)Reading package lists... DoneBuilding dependency tree Reading state information... DoneThe following additional packages will be installed: dpkg-dev fakeroot g++ g++-9 libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl libdpkg-perl libfakeroot libfile-fcntllock-perl libstdc++-9-dev makeSuggested packages: debian-keyring g++-multilib g++-9-multilib gcc-9-doc bzr libstdc++-9-doc make-docThe following NEW packages will be installed: build-essential dpkg-dev fakeroot g++ g++-9 libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl libdpkg-perl libfakeroot libfile-fcntllock-perl libstdc++-9-dev make0 upgraded, 13 newly installed, 0 to remove and 73 not upgraded.Need to get 11.4 MB of archives.After this operation, 52.2 MB of additional disk space will be used.Do you want to continue? [Y/n] YGet:1 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 libstdc++-9-dev amd64 9.3.0-17ubuntu1~20.04 [1714 kB]Get:2 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 g++-9 amd64 9.3.0-17ubuntu1~20.04 [8405 kB]Get:3 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 g++ amd64 4:9.3.0-1ubuntu2 [1604 B]Get:4 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 make amd64 4.2.1-1.2 [162 kB]Get:5 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 libdpkg-perl all 1.19.7ubuntu3 [230 kB]Get:6 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 dpkg-dev all 1.19.7ubuntu3 [679 kB]Get:7 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 build-essential amd64 12.8ubuntu1.1 [4664 B]Get:8 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 libfakeroot amd64 1.24-1 [25.7 kB]Get:9 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 fakeroot amd64 1.24-1 [62.6 kB]Get:10 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 libalgorithm-diff-perl all 1.19.03-2 [46.6 kB]Get:11 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 libalgorithm-diff-xs-perl amd64 0.04-6 [11.3 kB]Get:12 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 libalgorithm-merge-perl all 0.08-3 [12.0 kB]Get:13 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 libfile-fcntllock-perl amd64 0.22-3build4 [33.1 kB]Fetched 11.4 MB in 0s (54.8 MB/s) Selecting previously unselected package libstdc++-9-dev:amd64.(Reading database ... 66237 files and directories currently installed.)Preparing to unpack .../00-libstdc++-9-dev_9.3.0-17ubuntu1~20.04_amd64.deb ...Unpacking libstdc++-9-dev:amd64 (9.3.0-17ubuntu1~20.04) ...Selecting previously unselected package g++-9.Preparing to unpack .../01-g++-9_9.3.0-17ubuntu1~20.04_amd64.deb ...Unpacking g++-9 (9.3.0-17ubuntu1~20.04) ...Selecting previously unselected package g++.Preparing to unpack .../02-g++_4%3a9.3.0-1ubuntu2_amd64.deb ...Unpacking g++ (4:9.3.0-1ubuntu2) ...Selecting previously unselected package make.Preparing to unpack .../03-make_4.2.1-1.2_amd64.deb ...Unpacking make (4.2.1-1.2) ...Selecting previously unselected package libdpkg-perl.Preparing to unpack .../04-libdpkg-perl_1.19.7ubuntu3_all.deb Get:87 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 tcl8.6 amd64 8.6.10+dfsg-1 [14.8 kB]Get:88 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/universe amd64 tcl amd64 8.6.9+1 [5112 B]Get:89 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 tcl8.6-dev amd64 8.6.10+dfsg-1 [905 kB]Get:90 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/universe amd64 tcl-dev amd64 8.6.9+1 [5760 B]Get:91 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 tk8.6 amd64 8.6.10-1 [12.5 kB]Get:92 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/universe amd64 tk amd64 8.6.9+1 [3240 B]Get:93 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 tk8.6-dev amd64 8.6.10-1 [711 kB]Get:94 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/universe amd64 tk-dev amd64 8.6.9+1 [3076 B]Get:95 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 libglvnd0 amd64 1.3.2-1~ubuntu0.20.04.1 [51.4 kB]Get:96 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 libglx0 amd64 1.3.2-1~ubuntu0.20.04.1 [32.6 kB]Get:97 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 libgl1 amd64 1.3.2-1~ubuntu0.20.04.1 [86.9 kB]Get:98 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 x11-utils amd64 7.7+5 [199 kB]Get:99 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 xbitmaps all 1.1.1-2 [28.1 kB]Get:100 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/universe amd64 xterm amd64 353-1ubuntu1.20.04.2 [765 kB]Get:101 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 libffi-dev amd64 3.3-4 [57.0 kB]Get:102 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 liblzma-dev amd64 5.2.4-1ubuntu1 [147 kB]Get:103 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal/main amd64 libreadline-gplv2-dev amd64 5.2+dfsg-3build3 [125 kB]Fetched 47.5 MB in 1s (53.2 MB/s) Extracting templates from packages: 100%Preconfiguring packages ... Created wheel for p5py: filename=p5py-1.0.0-py2.py3-none-any.whl size=2333 sha256=e7417e00f0c9701b6889208ba42a352140b10d38748267029e0b3f5adb557857 Stored in directory: /home/ubuntu/.cache/pip/wheels/72/c3/3e/d2e21f7f687d90134f4774eee0b36f1b3303ef35d4ebf832c7Successfully built p5pyInstalling collected packages: p5pySuccessfully installed p5py-1.0.0Defaulting to user installation because normal site-packages is not writeableCollecting PEP517 Downloading pep517-0.11.0-py2.py3-none-any.whl (19 kB)Collecting tomli Downloading tomli-1.0.4-py3-none-any.whl (11 kB)Installing collected packages: tomli, PEP517Successfully installed PEP517-0.11.0 tomli-1.0.4Defaulting to user installation because normal site-packages is not writeableCollecting py-find-1st Downloading py_find_1st-1.1.5.tar.gz (8.8 kB) Installing build dependencies ... done Getting requirements to build wheel ... done Preparing wheel metadata ... doneCollecting numpy>=1.13.0 Downloading numpy-1.21.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.8 MB) || 15.8 MB 6.0 MB/s Building wheels for collected packages: py-find-1st Building wheel for py-find-1st (PEP 517) ... error ERROR: Command errored out with exit status 1: command: /usr/bin/python3.9 /home/ubuntu/.local/lib/python3.9/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /tmp/tmp47_3kk6k cwd: /tmp/pip-install-pvu0gblm/py-find-1st_36eb63373254485b8448e2b46d13c71f Complete output (20 lines): /tmp/pip-build-env-qm53q7xb/overlay/lib/python3.9/site-packages/setuptools/dist.py:697: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead warnings.warn( running bdist_wheel running build running build_py creating build creating build/lib.linux-x86_64-3.9 creating build/lib.linux-x86_64-3.9/utils_find_1st copying utils_find_1st/__init__.py -> build/lib.linux-x86_64-3.9/utils_find_1st running build_ext check for clang compiler ... no building 'find_1st' extension creating build/temp.linux-x86_64-3.9 creating build/temp.linux-x86_64-3.9/utils_find_1st x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DNPY_NO_DEPRECATED_API=NPY_1_13_API_VERSION -I/tmp/pip-build-env-qm53q7xb/overlay/lib/python3.9/site-packages/numpy/core/include -I/usr/include/python3.9 -c utils_find_1st/find_1st.cpp -o build/temp.linux-x86_64-3.9/utils_find_1st/find_1st.o utils_find_1st/find_1st.cpp:3:10: fatal error: Python.h: No such file or directory 3 | #include ""Python.h"" | ^~~~~~~~~~ compilation terminated. error: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1 ---------------------------------------- ERROR: Failed building wheel for py-find-1stFailed to build py-find-1stERROR: Could not build wheels for py-find-1st which use PEP 517 and cannot be installed directly",pip install py-find-1st fails on ubuntu20 & centos with python3.9
How to optimize pixel classifier for images?," I have built a pixel classifier for images, and for each pixel in the image, I want to define to which pre-defined color cluster it belongs. It works, but at some 5 minutes per image, I think I am doing something unpythonic that can for sure be optimized.How can we map the function directly over the list of lists? Then we define the clusters to map the colors to and the function to do so (which is taken from the PIL library) What remains is to match every color, and make a new list with matched indexes from the original Colors: Running a single image takes approximately 5 minutes, which is OK, but likely there is a method in which this time can be greatly reduced?36%| | 7691707/21499080 [01:50<03:18, 69721.86it/s]Expected outcome of Filt_lab: <code>  #First I convert my image to a list#Below list represents a true image sizelist1=[[255, 114, 70],[120, 89, 15],[247, 190, 6],[41, 38, 37],[102, 102, 10],[255,255,255]]*3583180 #Define colors of interest#Colors of interest RED=[255, 114, 70]DARK_YELLOW=[120, 89, 15]LIGHT_YELLOW=[247, 190, 6]BLACK=[41, 38, 37]GREY=[102, 102, 10]WHITE=[255,255,255]Colors=[RED, DARK_YELLOW, LIGHT_YELLOW, GREY, BLACK, WHITE]#Function to find closes cluster by root and squareroot distance of RGBdef distance(c1, c2): (r1,g1,b1) = c1 (r2,g2,b2) = c2 return math.sqrt((r1 - r2)**2 + (g1 - g2) ** 2 + (b1 - b2) **2) Filt_lab=[]#Match colors and make new list with indexed colorsfor pixel in tqdm(list1): closest_colors = sorted(Colors, key=lambda color: distance(color, pixel)) closest_color = closest_colors[0] for num, clust in enumerate(Colors): if list(clust) == list(closest_color): Filt_lab.append(num) [0, 1, 2, 4, 3, 5]*3583180",How to map function directly over list of lists?
Spark AttributeError: Can't get attribute 'new_block' on <module 'pandas.core.internals.blocks'," I was using pyspark on AWS EMR (4 r5.xlarge as 4 workers, each has one executor and 4 cores), and I got AttributeError: Can't get attribute 'new_block' on <module 'pandas.core.internals.blocks'. Below is a snippet of the code that threw this error: Below is the traceback, where line 102, in get_zip_b refers to pdf = brd_pdf.value: Some observations and thought process:1, After doing some search online, the AttributeError in pyspark seems to be caused by mismatched pandas versions between driver and workers?2, But I ran the same code on two different datasets, one worked without any errors but the other didn't, which seems very strange and undeterministic, and it seems like the errors may not be caused by mismatched pandas versions. Otherwise, neither two datasets would succeed.3, I then ran the same code on the successful dataset again, but this time with different spark configurations: setting spark.driver.memory from 2048M to 4192m, and it threw AttributeError.4, In conclusion, I think the AttributeError has something to do with driver. But I can't tell how they are related from the error message, and how to fix it: AttributeError: Can't get attribute 'new_block' on <module 'pandas.core.internals.blocks'. <code>  search = SearchEngine(db_file_dir = ""/tmp/db"")conn = sqlite3.connect(""/tmp/db/simple_db.sqlite"")pdf_ = pd.read_sql_query('''select zipcode, lat, lng, bounds_west, bounds_east, bounds_north, bounds_south from simple_zipcode''',conn)brd_pdf = spark.sparkContext.broadcast(pdf_) conn.close()@udf('string')def get_zip_b(lat, lng): pdf = brd_pdf.value out = pdf[(np.array(pdf[""bounds_north""]) >= lat) & (np.array(pdf[""bounds_south""]) <= lat) & (np.array(pdf['bounds_west']) <= lng) & (np.array(pdf['bounds_east']) >= lng) ] if len(out): min_index = np.argmin( (np.array(out[""lat""]) - lat)**2 + (np.array(out[""lng""]) - lng)**2) zip_ = str(out[""zipcode""].iloc[min_index]) else: zip_ = 'bad' return zip_df = df.withColumn('zipcode', get_zip_b(col(""latitude""),col(""longitude""))) 21/08/02 06:18:19 WARN TaskSetManager: Lost task 12.0 in stage 7.0 (TID 1814, ip-10-22-17-94.pclc0.merkle.local, executor 6): org.apache.spark.api.python.PythonException: Traceback (most recent call last): File ""/mnt/yarn/usercache/hadoop/appcache/application_1627867699893_0001/container_1627867699893_0001_01_000009/pyspark.zip/pyspark/worker.py"", line 605, in main process() File ""/mnt/yarn/usercache/hadoop/appcache/application_1627867699893_0001/container_1627867699893_0001_01_000009/pyspark.zip/pyspark/worker.py"", line 597, in process serializer.dump_stream(out_iter, outfile) File ""/mnt/yarn/usercache/hadoop/appcache/application_1627867699893_0001/container_1627867699893_0001_01_000009/pyspark.zip/pyspark/serializers.py"", line 223, in dump_stream self.serializer.dump_stream(self._batched(iterator), stream) File ""/mnt/yarn/usercache/hadoop/appcache/application_1627867699893_0001/container_1627867699893_0001_01_000009/pyspark.zip/pyspark/serializers.py"", line 141, in dump_stream for obj in iterator: File ""/mnt/yarn/usercache/hadoop/appcache/application_1627867699893_0001/container_1627867699893_0001_01_000009/pyspark.zip/pyspark/serializers.py"", line 212, in _batched for item in iterator: File ""/mnt/yarn/usercache/hadoop/appcache/application_1627867699893_0001/container_1627867699893_0001_01_000009/pyspark.zip/pyspark/worker.py"", line 450, in mapper result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs) File ""/mnt/yarn/usercache/hadoop/appcache/application_1627867699893_0001/container_1627867699893_0001_01_000009/pyspark.zip/pyspark/worker.py"", line 450, in <genexpr> result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs) File ""/mnt/yarn/usercache/hadoop/appcache/application_1627867699893_0001/container_1627867699893_0001_01_000009/pyspark.zip/pyspark/worker.py"", line 90, in <lambda> return lambda *a: f(*a) File ""/mnt/yarn/usercache/hadoop/appcache/application_1627867699893_0001/container_1627867699893_0001_01_000009/pyspark.zip/pyspark/util.py"", line 121, in wrapper return f(*args, **kwargs) File ""/mnt/var/lib/hadoop/steps/s-1IBFS0SYWA19Z/Mobile_ID_process_center.py"", line 102, in get_zip_b File ""/mnt/yarn/usercache/hadoop/appcache/application_1627867699893_0001/container_1627867699893_0001_01_000009/pyspark.zip/pyspark/broadcast.py"", line 146, in value self._value = self.load_from_path(self._path) File ""/mnt/yarn/usercache/hadoop/appcache/application_1627867699893_0001/container_1627867699893_0001_01_000009/pyspark.zip/pyspark/broadcast.py"", line 123, in load_from_path return self.load(f) File ""/mnt/yarn/usercache/hadoop/appcache/application_1627867699893_0001/container_1627867699893_0001_01_000009/pyspark.zip/pyspark/broadcast.py"", line 129, in load return pickle.load(file)AttributeError: Can't get attribute 'new_block' on <module 'pandas.core.internals.blocks' from '/mnt/miniconda/lib/python3.9/site-packages/pandas/core/internals/blocks.py'>",AttributeError: Can't get attribute 'new_block' on <module 'pandas.core.internals.blocks'>
Poetry: when poetry add SQLAlchemy get AttributeErrorr allows," When installing with pip, pip install sqlalchemy all is ok.When installing with poetry I am getting the error <code>  backend poetry add sqlalchemyUsing version ^1.4.23 for SQLAlchemyUpdating dependenciesResolving dependencies... (0.1s) AttributeError 'EmptyConstraint' object has no attribute 'allows' at ~/.poetry/lib/poetry/_vendor/py3.8/poetry/core/version/markers.py:291 in validate 287 288 if self._name not in environment: 289 return True 290 291 return self._constraint.allows(self._parser(environment[self._name])) 292 293 def without_extras(self): # type: () -> MarkerTypes 294 return self.exclude(""extra"") 295 backend",Installing SQLAlchemy with Poetry causes an AttributeErrorr
That choice is not one of the available choices django," I have a form to update user, the error is on the role field. I am filtering the role based on customer. I am getting the right values for role but anyways the error pops up.Select a valid choice. That choice is not one of the available choicesviews.py forms.py I am filtering all data using this class, each customer has its own row in the table. I am rendering the form using {{form|crispy}} <code>  class UserUpdateView(LoginRequiredMixin, SuccessMessageMixin, UpdateView): form_class = UserUpdateForm template_name = 'users/modals/update_profile_modal.html' success_message = ""User updated successfully."" def get_form_kwargs(self): kw = super().get_form_kwargs() kw['request'] = self.request return kw def get_object(self, *args, **kwargs): user_id = self.request.session['user_detail'] return TbUser.objects.get(id=user_id) def form_invalid(self, form): messages.error(self.request, form.errors) print(form.errors) return redirect('user-detail', pk=self.object.pk) def get_success_url(self): return reverse('user-detail', kwargs={'pk': self.object.pk}) class UserUpdateForm(forms.ModelForm): email = forms.EmailField() def __init__(self, request, *args, **kwargs): super().__init__(*args, **kwargs) self.request = request if request.user.customer: self.fields['department'].queryset = TbDepartment.objects.filter( customer=request.user.customer) self.fields['role'].queryset = TbRole.objects.filter( customer=request.user.customer) self.fields['username'].required = True self.fields['real_name'].required = True self.fields['email'].required = True self.fields['cellphone'].required = True self.fields['department'].required = True self.fields['role'].required = True class Meta: model = TbUser fields = ['username', 'real_name', 'email', 'cellphone', 'department', 'role'] class TbCustomer(models.Model): id = models.CharField(primary_key=True, max_length=50) short_name = models.CharField(max_length=255) names = models.CharField(max_length=255) descs = models.CharField(max_length=255, blank=True, null=True) creat_time = models.DateTimeField() creat_user = models.CharField(max_length=255) authenticationcode = models.CharField( db_column='authenticationCode', max_length=255, blank=True, null=True) is_available = models.IntegerField(blank=True, null=True) logo_img = models.CharField(max_length=40, blank=True, null=True) response_message = models.CharField(max_length=100, blank=True, null=True) language = models.CharField(max_length=20, blank=True, null=True) class Meta: managed = False db_table = 'tb_customer' def __str__(self): return '%s' % self.short_name {% block modal %}{% load static %}{% load crispy_forms_tags %}<!-- Modal --><div class=""modal fade"" data-backdrop=""static"" data-keyboard=""false"" id=""tb-user-profile-update-modal"" tabindex=""-1"" role=""dialog"" aria-labelledby=""exampleModalLabel"" aria-hidden=""true""> <div class=""modal-dialog modal-sm"" role=""document""> <div class=""modal-content""> <form enctype=""multipart/form-data"" action=""{% url 'tb-user-update' pk=user.id %}"" method=""POST""> <div class=""row d-flex justify-content-center""> <div class=""col-10""> <fieldset class=""form-group mt-2""> {{user.username}} {% csrf_token %} {{form|crispy}} </fieldset> <div class=""form-group""> <button class=""btn btn-secondary"" type=""submit""> <span>Update</span> </button> <button class=""btn btn-secondary"" type=""button"" data-dismiss=""modal""> <span>Close</span> </button> </div> </div> </div> </form> </div> </div></div>{% endblock modal %}",Django Forms - That choice is not one of the available choices
How to add a row in a special form (python)," I have a pandas.DataFrame of the form I want to create a dataframe in which column df repeats 0,1,2,3. But there is something missing in the data. I'm trying to fill in the blanks with 0 by appending row values.Here is my expected result: How can I achieve this?edit:What should I do if my input is as below? Here is my expected result: <code>  index df df10 0 1111 1 1112 2 1113 3 1114 0 1115 2 1116 3 1117 0 1118 2 1119 3 11110 0 11111 1 11112 2 11113 3 11114 0 11115 1 11116 2 11117 3 11118 1 11119 2 11120 3 111 index df df10 0 1111 1 1112 2 1113 3 1114 0 1115 1 06 2 1117 3 1118 0 1119 1 010 2 11111 3 11112 0 11113 1 11114 2 11115 3 11116 0 11117 1 11118 2 11119 3 11120 0 021 1 11122 2 11123 3 111 index df1 df20 0 1111 1 1112 2 1113 3 1114 0 1115 3 1116 1 1117 2 111 index df1 df20 0 1111 1 1112 2 1113 3 1114 0 1115 1 06 2 07 3 1118 0 0 9 1 11110 2 111 11 3 0 ",How to add a row in a special form
*args in cycles (Python)," In python, I can write something like this: I will get the next output: When we use *args as function arguments, it is unpacked into a tuple.Why do we receive a list in this situation? <code>  some_list = [(1, 2, 3), (3, 2, 1)]for i, *args in some_list: print(args) [2, 3][2, 1]",Why does starred assignment produce lists and not tuples?
*args in loops (Python)," In python, I can write something like this: I will get the next output: When we use *args as function arguments, it is unpacked into a tuple.Why do we receive a list in this situation? <code>  some_list = [(1, 2, 3), (3, 2, 1)]for i, *args in some_list: print(args) [2, 3][2, 1]",Why does starred assignment produce lists and not tuples?
Repeating triangle pattern in python 3.x," I need to make a triangle of triangle pattern of * depending on the integer input.For example:n = 2 n = 3 I've already figured out the code for a single triangle, but I don't know how to duplicate them so they'll appear like a triangle of triangles.Here's my code for one triangle: <code>  * *** * * ********** * *** ***** * * * *** *** *** *************** * * * * * *** *** *** *** **************************** rows = int(input())for i in range(rows): for j in range(i, rows): print("" "", end="""") for j in range(i): print(""*"", end="""") for j in range(i + 1): print(""*"", end="""") print()",Repeating triangle pattern in Python
Repeating triangle pattern in python," I need to make a triangle of triangle pattern of * depending on the integer input.For example:n = 2 n = 3 I've already figured out the code for a single triangle, but I don't know how to duplicate them so they'll appear like a triangle of triangles.Here's my code for one triangle: <code>  * *** * * ********** * *** ***** * * * *** *** *** *************** * * * * * *** *** *** *** **************************** rows = int(input())for i in range(rows): for j in range(i, rows): print("" "", end="""") for j in range(i): print(""*"", end="""") for j in range(i + 1): print(""*"", end="""") print()",Repeating triangle pattern in Python
127.0.0.1 works but localhost not in macOS monterey," I cannot access a web server on localhost port 5000 on macOS v12 (Monterey) (Flask or any other).E.g., use the built-in HTTP server, I cannot get onto port 5000: If you have Flask installed and you run the Flask web server, it does not fail on start. Let's take the minimum Flask example code: Then run it (provided you have Flask/Python 3 installed): Output: However, if you try to access this server (from a browser or with anything else), it is denied: <code>  python3 -m http.server 5000... (stack trace)File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socketserver.py"", line 466, in server_bindself.socket.bind(self.server_address)OSError: [Errno 48] Address already in use # Save as hello.py in the current working directory.from flask import Flaskapp = Flask(__name__)@app.route(""/"")def hello_world(): return ""<p>Hello, World!</p>"" export FLASK_APP=helloflask run * Running on http://127.0.0.1:5000/ curl -I localhost:5000HTTP/1.1 403 ForbiddenContent-Length: 0Server: AirTunes/595.13.1",localhost:5000 unavailable in macOS v12 (Monterey)
localhost:5000 unavailable in macOS Monterey," I cannot access a web server on localhost port 5000 on macOS v12 (Monterey) (Flask or any other).E.g., use the built-in HTTP server, I cannot get onto port 5000: If you have Flask installed and you run the Flask web server, it does not fail on start. Let's take the minimum Flask example code: Then run it (provided you have Flask/Python 3 installed): Output: However, if you try to access this server (from a browser or with anything else), it is denied: <code>  python3 -m http.server 5000... (stack trace)File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socketserver.py"", line 466, in server_bindself.socket.bind(self.server_address)OSError: [Errno 48] Address already in use # Save as hello.py in the current working directory.from flask import Flaskapp = Flask(__name__)@app.route(""/"")def hello_world(): return ""<p>Hello, World!</p>"" export FLASK_APP=helloflask run * Running on http://127.0.0.1:5000/ curl -I localhost:5000HTTP/1.1 403 ForbiddenContent-Length: 0Server: AirTunes/595.13.1",localhost:5000 unavailable in macOS v12 (Monterey)
Plot bar chart in multiple rows with Pandas," I have a simple long-form dataset I would like to generate bar charts from. The dataframe looks like this: I would like to plot a bar chart that has 3 rows, each for 2019, 2020 and 2021. X axis being month_diff and data goes on Y axis.How do I do this?If the data was in different columns, then I could have just used this code: But my data is in a single column and ideally, I'd like to have different row for each year. <code>  data = {'Year':[2019,2019,2019,2020,2020,2020,2021,2021,2021], 'Month_diff':[0,1,2,0,1,2,0,1,2], 'data': [12,10,13,16,12,18,19,45,34]}df = pd.DataFrame(data) df.plot(x=""X"", y=[""A"", ""B"", ""C""], kind=""bar"")",Plot bar chart in multiple subplot rows with Pandas
Sum dataframe values in for loop inside a for loop Python," I have a large polygon file, small polygon file and points file. What I do here is loop through large polygons to find which small polygons intersect. Then calculate the area of each small polygon within the large one. And then I loop through the small polygons to find points statistics in each of them.I have found number_of_somethin value in each small polygon. And the question would be how to can I sum all number_of_somethin small polygons values within the large polygon and store the results in original large_polygon file as a new column, let's say large_polygon['smth_sum']?With df_res_2.loc[idx, 'smth'] = number_of_somethin I get number_of_somethin values in each small polygon inside the large ones. Now I need to sum them in large_polygon['smth_sum']Note: FID is the id for large polygons and ID is the id for small polygons I had a few ideas how to do this, but none of them are not wokring, so I assume that there is some other way. <code>  import geopandas as gpdsmall_polygon = gpd.read_file(r'R:\...\small.shp')large_polygon = gpd.read_file(r'R:\...\large.shp')points = gpd.read_file(r'R:\...\points.shp')SmallJoin =gpd.sjoin(small_polygon, large_polygon)[['FID', 'ID', 'someValue','geometry']]for i in large_polygon.index: df_i = SmallJoin[SmallJoin['FID'] == i] # i do something here, f.e. calculate small polgyon area df_res = gpd.overlay(large_polygon, df_i, how='intersection') df_res['area'] = round((df_res.apply(lambda row: row.geometry.area, axis=1)), 4) # now i know area for each small polygon within large polygon df_res_2 = df_res[df_res['FID_1'] == i] # now point statistics in small polygons PointsJoin =gpd.sjoin(points, df_res)[['ID','someAttribute', 'someAttribute2','geometry']] for idx, val in df_res_2['ID'].items(): df_idx = PointsJoin[PointsJoin['ID'] == val] number_of_somethin = df_idx ['someAttribute'] + 121 + df_idx['someAttribute2'] df_res_2.loc[idx, 'smth'] = number_of_somethin large_polygon.loc[i, 'smth_sum'] = df_res_2['smth']large_polygon.loc[i, 'smth_sum'] = df_res_2['smth'].sum()large_polygon['smth_sum'] = large_polygon[large_polygon['FID'] == df_res_2['FID_1'].sum()]",Sum dataframe values in for loop inside a for loop
Python 3.10 Pattern matching (PEP 634) - wildcard in string," I got a large list of JSON objects that I want to parse depending on the start of one of the keys, and just wildcard the rest. A lot of the keys are similar, like ""matchme-foo"" and ""matchme-bar"". There is a builtin wildcard, but it is only used for whole values, kinda like an else.I might be overlooking something but I can't find a solution anywhere in the proposal:https://docs.python.org/3/whatsnew/3.10.html#pep-634-structural-pattern-matchingAlso a bit more about it in PEP-636:https://www.python.org/dev/peps/pep-0636/#going-to-the-cloud-mappingsMy data looks like this: I want to do something that can match the id without having to make a long list of |'s.Something like this: It's a relatively new addition to Python so there aren't many guides on how to use it yet. <code>  data = [{ ""id"" : ""matchme-foo"", ""message"": ""hallo this is a message"", },{ ""id"" : ""matchme-bar"", ""message"": ""goodbye"", },{ ""id"" : ""anotherid"", ""message"": ""completely diffrent event"" }, ...] for event in data: match event: case {'id':'matchme-*'}: # Match all 'matchme-' no matter what comes next log.INFO(event['message']) case {'id':'anotherid'}: log.ERROR(event['message'])",Python 3.10 pattern matching (PEP 634) - wildcard in string
Python/Selenium web scrap how to find hidden href value from a links?," Scrapping links should be a simple feat, usually just grabbing the src value of the a tag.I recently came across this website (https://sunteccity.com.sg/promotions) where the href value of a tags of each item cannot be found, but the redirection still works. I'm trying to figure out a way to grab the items and their corresponding links. My typical python selenium code looks something as such However, I can't seem to retrieve any href, onclick attributes, and I'm wondering if this is even possible. I noticed that I couldn't do a right-click, open link in new tab as well.Are there any ways around getting the links of all these items?Edit: Are there any ways to retrieve all the links of the items on the pages?i.e. Edit:Adding an image of one such anchor tag for better clarity: <code>  all_items = bot.find_elements_by_class_name('thumb-img')for promo in all_items: a = promo.find_elements_by_tag_name(""a"") print(""a[0]: "", a[0].get_attribute(""href"")) https://sunteccity.com.sg/promotions/724https://sunteccity.com.sg/promotions/731https://sunteccity.com.sg/promotions/751https://sunteccity.com.sg/promotions/752https://sunteccity.com.sg/promotions/754https://sunteccity.com.sg/promotions/280...",Python/Selenium web scrap how to find hidden src value from a links?
"Getting error when building conda package: ""site-packages/[PACKAGE_NAME]-[VERSION].dist-info/Icon\r' not in info/files"""," I honestly can't figure out what is happening with this error. I thought it was something in my manifest file but apparently it's not.Note, this directory is in my Google Drive.Here is my MANIFEST.in file: I'm running conda build . in the directory and get the following error: Here's the complete logI can confirm that the Icon files do not exist in my soothsayer_utils-2022.1.19.tar.gz file: Can someone help me get conda build to work for my package? <code>  graft soothsayer_utilsinclude setup.pyinclude LICENSE.txtinclude README.mdglobal-exclude Icon*global-exclude *.py[co]global-exclude .DS_Store Packaging soothsayer_utilsINFO:conda_build.build:Packaging soothsayer_utilsINFO conda_build.build:build(2214): Packaging soothsayer_utilsPackaging soothsayer_utils-2022.01.19-py_0INFO:conda_build.build:Packaging soothsayer_utils-2022.01.19-py_0INFO conda_build.build:bundle_conda(1454): Packaging soothsayer_utils-2022.01.19-py_0number of files: 11Fixing permissionsPackaged license file/s.INFO :: Time taken to mark (prefix) 0 replacements in 0 files was 0.11 seconds'site-packages/soothsayer_utils-2022.1.19.dist-info/Icon' not in tarball'site-packages/soothsayer_utils-2022.1.19.dist-info/Icon\r' not in info/filesTraceback (most recent call last): File ""/Users/jespinoz/anaconda3/bin/conda-build"", line 11, in <module> sys.exit(main()) File ""/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/cli/main_build.py"", line 474, in main execute(sys.argv[1:]) File ""/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/cli/main_build.py"", line 463, in execute outputs = api.build(args.recipe, post=args.post, test_run_post=args.test_run_post, File ""/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/api.py"", line 186, in build return build_tree( File ""/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/build.py"", line 3008, in build_tree packages_from_this = build(metadata, stats, File ""/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/build.py"", line 2291, in build newly_built_packages = bundlers[pkg_type](output_d, m, env, stats) File ""/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/build.py"", line 1619, in bundle_conda tarcheck.check_all(tmp_path, metadata.config) File ""/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/tarcheck.py"", line 89, in check_all x.info_files() File ""/Users/jespinoz/anaconda3/lib/python3.8/site-packages/conda_build/tarcheck.py"", line 53, in info_files raise Exception('info/files')Exception: info/files (base) jespinoz@x86_64-apple-darwin13 Downloads % tree soothsayer_utils-2022.1.19soothsayer_utils-2022.1.19 LICENSE.txt MANIFEST.in PKG-INFO README.md setup.cfg setup.py soothsayer_utils __init__.py soothsayer_utils.py soothsayer_utils.egg-info PKG-INFO SOURCES.txt dependency_links.txt requires.txt top_level.txt2 directories, 13 files","How to fix error when building conda package related to ""Icon"" file?"
How to make a 10 million requests in python," I want to download/scrape 50 million log records from a site. Instead of downloading 50 million in one go, I was trying to download it in parts like 10 million at a time using the following code but it's only handling 20,000 at a time (more than that throws an error) so it becomes time-consuming to download that much data. Currently, it takes 3-4 mins to download 20,000 records with the speed of 100%|| 20000/20000 [03:48<00:00, 87.41it/s] so how to speed it up? Traceback (most recent call last): <code>  import asyncioimport aiohttpimport timeimport tqdmimport nest_asyncionest_asyncio.apply()async def make_numbers(numbers, _numbers): for i in range(numbers, _numbers): yield in = 0q = 10000000async def fetch(): # example url = ""https://httpbin.org/anything/log?id="" async with aiohttp.ClientSession() as session: post_tasks = [] # prepare the coroutines that poat async for x in make_numbers(n, q): post_tasks.append(do_get(session, url, x)) # now execute them all at once responses = [await f for f in tqdm.tqdm(asyncio.as_completed(post_tasks), total=len(post_tasks))]async def do_get(session, url, x): headers = { 'Content-Type': ""application/x-www-form-urlencoded"", 'Access-Control-Allow-Origin': ""*"", 'Accept-Encoding': ""gzip, deflate"", 'Accept-Language': ""en-US"" } async with session.get(url + str(x), headers=headers) as response: data = await response.text() print(data)s = time.perf_counter()try: loop = asyncio.get_event_loop() loop.run_until_complete(fetch())except: print(""error"")elapsed = time.perf_counter() - s# print(f""{__file__} executed in {elapsed:0.2f} seconds."") File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py"", line 986, in _wrap_create_connection return await self._loop.create_connection(*args, **kwargs) # type: ignore[return-value] # noqa File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\base_events.py"", line 1056, in create_connection raise exceptions[0] File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\base_events.py"", line 1041, in create_connection sock = await self._connect_sock( File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\base_events.py"", line 955, in _connect_sock await self.sock_connect(sock, address) File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\proactor_events.py"", line 702, in sock_connect return await self._proactor.connect(sock, address) File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\tasks.py"", line 328, in __wakeup future.result() File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\windows_events.py"", line 812, in _poll value = callback(transferred, key, ov) File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\windows_events.py"", line 599, in finish_connect ov.getresult()OSError: [WinError 121] The semaphore timeout period has expiredThe above exception was the direct cause of the following exception:Traceback (most recent call last): File ""C:\Users\SGM\Desktop\xnet\x3stackoverflow.py"", line 136, in <module> loop.run_until_complete(fetch()) File ""C:\Users\SGM\AppData\Roaming\Python\Python39\site-packages\nest_asyncio.py"", line 81, in run_until_complete return f.result() File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\futures.py"", line 201, in result raise self._exception File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\tasks.py"", line 256, in __step result = coro.send(None) File ""C:\Users\SGM\Desktop\xnet\x3stackoverflow.py"", line 88, in fetch response = await f File ""C:\Users\SGM\Desktop\xnet\x3stackoverflow.py"", line 37, in _wait_for_one return f.result() File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\futures.py"", line 201, in result raise self._exception File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\tasks.py"", line 258, in __step result = coro.throw(exc) File ""C:\Users\SGM\Desktop\xnet\x3stackoverflow.py"", line 125, in do_get async with session.get(url + str(x), headers=headers) as response: File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\client.py"", line 1138, in __aenter__ self._resp = await self._coro File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\client.py"", line 535, in _request conn = await self._connector.connect( File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py"", line 542, in connect proto = await self._create_connection(req, traces, timeout) File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py"", line 907, in _create_connection _, proto = await self._create_direct_connection(req, traces, timeout) File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py"", line 1206, in _create_direct_connection raise last_exc File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py"", line 1175, in _create_direct_connection transp, proto = await self._wrap_create_connection( File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py"", line 992, in _wrap_create_connection raise client_error(req.connection_key, exc) from excaiohttp.client_exceptions.ClientConnectorError: Cannot connect to host example.com:80 ssl:default [The semaphore timeout period has expired]",How to speed up async requests in Python
How to speedup requests in python," I want to download/scrape 50 million log records from a site. Instead of downloading 50 million in one go, I was trying to download it in parts like 10 million at a time using the following code but it's only handling 20,000 at a time (more than that throws an error) so it becomes time-consuming to download that much data. Currently, it takes 3-4 mins to download 20,000 records with the speed of 100%|| 20000/20000 [03:48<00:00, 87.41it/s] so how to speed it up? Traceback (most recent call last): <code>  import asyncioimport aiohttpimport timeimport tqdmimport nest_asyncionest_asyncio.apply()async def make_numbers(numbers, _numbers): for i in range(numbers, _numbers): yield in = 0q = 10000000async def fetch(): # example url = ""https://httpbin.org/anything/log?id="" async with aiohttp.ClientSession() as session: post_tasks = [] # prepare the coroutines that poat async for x in make_numbers(n, q): post_tasks.append(do_get(session, url, x)) # now execute them all at once responses = [await f for f in tqdm.tqdm(asyncio.as_completed(post_tasks), total=len(post_tasks))]async def do_get(session, url, x): headers = { 'Content-Type': ""application/x-www-form-urlencoded"", 'Access-Control-Allow-Origin': ""*"", 'Accept-Encoding': ""gzip, deflate"", 'Accept-Language': ""en-US"" } async with session.get(url + str(x), headers=headers) as response: data = await response.text() print(data)s = time.perf_counter()try: loop = asyncio.get_event_loop() loop.run_until_complete(fetch())except: print(""error"")elapsed = time.perf_counter() - s# print(f""{__file__} executed in {elapsed:0.2f} seconds."") File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py"", line 986, in _wrap_create_connection return await self._loop.create_connection(*args, **kwargs) # type: ignore[return-value] # noqa File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\base_events.py"", line 1056, in create_connection raise exceptions[0] File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\base_events.py"", line 1041, in create_connection sock = await self._connect_sock( File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\base_events.py"", line 955, in _connect_sock await self.sock_connect(sock, address) File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\proactor_events.py"", line 702, in sock_connect return await self._proactor.connect(sock, address) File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\tasks.py"", line 328, in __wakeup future.result() File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\windows_events.py"", line 812, in _poll value = callback(transferred, key, ov) File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\windows_events.py"", line 599, in finish_connect ov.getresult()OSError: [WinError 121] The semaphore timeout period has expiredThe above exception was the direct cause of the following exception:Traceback (most recent call last): File ""C:\Users\SGM\Desktop\xnet\x3stackoverflow.py"", line 136, in <module> loop.run_until_complete(fetch()) File ""C:\Users\SGM\AppData\Roaming\Python\Python39\site-packages\nest_asyncio.py"", line 81, in run_until_complete return f.result() File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\futures.py"", line 201, in result raise self._exception File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\tasks.py"", line 256, in __step result = coro.send(None) File ""C:\Users\SGM\Desktop\xnet\x3stackoverflow.py"", line 88, in fetch response = await f File ""C:\Users\SGM\Desktop\xnet\x3stackoverflow.py"", line 37, in _wait_for_one return f.result() File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\futures.py"", line 201, in result raise self._exception File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\asyncio\tasks.py"", line 258, in __step result = coro.throw(exc) File ""C:\Users\SGM\Desktop\xnet\x3stackoverflow.py"", line 125, in do_get async with session.get(url + str(x), headers=headers) as response: File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\client.py"", line 1138, in __aenter__ self._resp = await self._coro File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\client.py"", line 535, in _request conn = await self._connector.connect( File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py"", line 542, in connect proto = await self._create_connection(req, traces, timeout) File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py"", line 907, in _create_connection _, proto = await self._create_direct_connection(req, traces, timeout) File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py"", line 1206, in _create_direct_connection raise last_exc File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py"", line 1175, in _create_direct_connection transp, proto = await self._wrap_create_connection( File ""C:\Users\SGM\AppData\Local\Programs\Python\Python39\lib\site-packages\aiohttp\connector.py"", line 992, in _wrap_create_connection raise client_error(req.connection_key, exc) from excaiohttp.client_exceptions.ClientConnectorError: Cannot connect to host example.com:80 ssl:default [The semaphore timeout period has expired]",How to speed up async requests in Python
